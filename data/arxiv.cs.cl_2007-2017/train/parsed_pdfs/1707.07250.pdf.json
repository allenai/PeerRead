{
  "name" : "1707.07250.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Tensor Fusion Network for Multimodal Sentiment Analysis",
    "authors" : [ "Amir Zadeh", "Minghai Chen", "Soujanya Poria" ],
    "emails" : [ "abagherz@cs.cmu.edu", "minghail@cs.cmu.edu", "sporia@ntu.edu.sg", "cambria@ntu.edu.sg", "morency@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Multimodal sentiment analysis (Morency et al., 2011; Zadeh et al., 2016b; Poria et al., 2015) is an increasingly popular area of affective computing research (Poria et al., 2017) that focuses on generalizing text-based sentiment analysis to opinionated videos, where three communicative modalities are present: language (spoken words), visual (gestures), and acoustic (voice).\nThis generalization is particularly vital to part of the NLP community dealing with opinion mining and sentiment analysis (Cambria et al., 2017) since there is a growing trend of sharing opinions in videos instead of text, specially in social media (Facebook, YouTube, etc.). The central challenge in multimodal sentiment analysis is to model the inter-modality dynamics: the interactions between\n† means equal contribution\nlanguage, visual and acoustic behaviors that change the perception of the expressed sentiment.\nFigure 1 illustrates these complex inter-modality dynamics. The utterance “This movie is sick” can be ambiguous (either positive or negative) by itself, but if the speaker is also smiling at the same time, then it will be perceived as positive. On the other hand, the same utterance with a frown would be perceived negatively. A person speaking loudly “This movie is sick” would still be ambiguous. These examples are illustrating bimodal interactions. Examples of trimodal interactions are shown in Figure 1 when loud voice increases the sentiment to strongly positive. The complexity of inter-modality dynamics is shown in the second trimodal example where the utterance “This movie is fair” is still weakly positive, given the strong influence of the word “fair”.\nA second challenge in multimodal sentiment analysis is efficiently exploring intra-modality dynamics of a specific modality (unimodal interaction). Intra-modality dynamics are particularly\nar X\niv :1\n70 7.\n07 25\n0v 1\n[ cs\n.C L\n] 2\n3 Ju\nl 2 01\n7\nchallenging for the language analysis since multimodal sentiment analysis is performed on spoken language. A spoken opinion such as “I think it was alright . . . Hmmm . . . let me think . . . yeah . . . no . . . ok yeah” almost never happens in written text. This volatile nature of spoken opinions, where proper language structure is often ignored, complicates sentiment analysis. Visual and acoustic modalities also contain their own intra-modality dynamics which are expressed through both space and time.\nPrevious works in multimodal sentiment analysis does not account for both intra-modality and intermodality dynamics directly, instead they either perform early fusion (a.k.a., feature-level fusion) or late fusion (a.k.a., decision-level fusion). Early fusion consists in simply concatenating multimodal features mostly at input level (Morency et al., 2011; Pérez-Rosas et al., 2013; Poria et al., 2016). This fusion approach does not allow the intra-modality dynamics to be efficiently modeled. This is due to the fact that inter-modality dynamics can be more complex at input level and can dominate the learning process or result in overfitting. Late fusion, instead, consists in training unimodal classifiers independently and performing decision voting (Wang et al., 2016; Zadeh et al., 2016a). This prevents the model from learning inter-modality dynamics in an efficient way by assuming that simple weighted averaging is a proper fusion approach.\nIn this paper, we introduce a new model, termed Tensor Fusion Network (TFN), which learns both the intra-modality and inter-modality dynamics end-to-end. Inter-modality dynamics are modeled with a new multimodal fusion approach, named Tensor Fusion, which explicitly aggregates unimodal, bimodal and trimodal interactions. Intramodality dynamics are modeled through three Modality Embedding Subnetworks, for language, visual and acoustic modalities, respectively.\nIn our extensive set of experiments, we show (a) that TFN outperforms previous state-of-the-art approaches for multimodal sentiment analysis, (b) the characteristics and capabilities of our Tensor Fusion approach for multimodal sentiment analysis, and (c) that each of our three Modality Embedding Subnetworks (language, visual and acoustic) are also outperforming unimodal state-of-the-art unimodal sentiment analysis approaches."
    }, {
      "heading" : "2 Related Work",
      "text" : "Sentiment Analysis is a well-studied research area in NLP (Pang et al., 2008). Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al., 2015).\nMultimodal Sentiment Analysis is an emerging research area that integrates verbal and nonverbal behaviors into the detection of user sentiment. There exist several multimodal datasets that include sentiment annotations, including the newly-introduced CMU-MOSI dataset (Zadeh et al., 2016b), as well as other datasets including ICT-MMMO (Wöllmer et al., 2013), YouTube (Morency et al., 2011), and MOUD (Pérez-Rosas et al., 2013), however CMUMOSI is the only English dataset with utterancelevel sentiment labels. The newest multimodal sentiment analysis approaches have used deep neural networks, including convolutional neural networks (CNNs) with multiple-kernel learning (Poria et al., 2015), SAL-CNN (Wang et al., 2016) which learns generalizable features across speakers, and support vector machines (SVMs) with a multimodal dictionary (Zadeh, 2015).\nAudio-Visual Emotion Recognition is closely tied to multimodal sentiment analysis (Poria et al., 2017). Both audio and visual features have been shown to be useful in the recognition of emotions (Ghosh et al., 2016a). Using facial expressions and audio cues jointly has been the focus of many recent studies (Glodek et al., 2011; Valstar et al., 2016; Nojavanasghari et al., 2016).\nMultimodal Machine Learning has been a growing trend in machine learning research that is closely tied to the studies in this paper. Creative and novel applications of using multiple modalities have been among successful recent research directions in machine learning (You et al., 2016; Donahue et al., 2015; Antol et al., 2015; Specia et al., 2016; Tong et al., 2017)."
    }, {
      "heading" : "3 CMU-MOSI Dataset",
      "text" : "Multimodal Opinion Sentiment Intensity (CMUMOSI) dataset is an annotated dataset of video\n0 100\n200\n300\n400\n500\n600\nHighly Negative Negative Weakly Negative Neutral Weakly Positive Positive Highly Positive\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90%\n100%\n5 10 15 20 25 30 35\nPe rc\nen ta\nge o\nf S en\ntim en\nt D eg\nre es\nUtterance Size\nHighly Positive\nPositive\nWeakly Positive\nNeutral\nWeakly Negative\nNegative\nHighly Negative\nN um\nbe r o\nf O pi\nni on\nS eg\nm en\nts\nFigure 2: Distribution of sentiment across different opinions (left) and opinion sizes (right) in CMU-MOSI.\nopinions from YouTube movie reviews (Zadeh et al., 2016a). Annotation of sentiment has closely followed the annotation scheme of the Stanford Sentiment Treebank (Socher et al., 2013), where sentiment is annotated on a seven-step Likert scale from very negative to very positive. However, whereas the Stanford Sentiment Treebank is segmented by sentence, the CMU-MOSI dataset is segmented by opinion utterances to accommodate spoken language where sentence boundaries are not as clear as text. There are 2199 opinion utterances for 93 distinct speakers in CMU-MOSI. There are an average 23.2 opinion segments in each video. Each video has an average length of 4.2 seconds. There are a total of 26,295 words in the opinion utterances. These utterance are annotated by five Mechanical Turk annotators for sentiment. The final agreement between the annotators is high in terms of Krippendorf’s alpha α = 0.77. Figure 2 shows the distribution of sentiment across different opinions and different opinion sizes. CMU-MOSI dataset facilitates three prediction tasks, each of which we address in our experiments: 1) Binary Sentiment Classification 2) Five-Class Sentiment Classification (similar to Stanford Sentiment Treebank fine-grained classification with seven scale being mapped to five) and 3) Sentiment Regression in range [−3, 3]. For sentiment regression, we report Mean-Absolute Error (lower is better) and correlation (higher is better) between the model predictions and regression ground truth."
    }, {
      "heading" : "4 Tensor Fusion Network",
      "text" : "Our proposed TFN consists of three major components: 1) Modality Embedding Subnetworks take as input unimodal features, and output a rich modality embedding. 2) Tensor Fusion Layer explicitly models the unimodal, bimodal and trimodal interactions using a 3-fold Cartesian product from modality embeddings. 3) Sentiment Inference Subnetwork is a\nnetwork conditioned on the output of the Tensor Fusion Layer and performs sentiment inference. Depending on the task from Section 3 the network output changes to accommodate binary classification, 5-class classification or regression. Input to the TFN is an opinion utterance which includes three modalities of language, visual and acoustic. The following three subsections describe the TFN subnetworks and their inputs in detail."
    }, {
      "heading" : "4.1 Modality Embedding Subnetworks",
      "text" : "Spoken Language Embedding Subnetwork: Spoken text is different than written text (reviews, tweets) in compositionality and grammar. We revisit the spoken opinion: “I think it was alright . . . Hmmm . . . let me think . . . yeah . . . no . . . ok yeah”. This form of opinion rarely happens in written language but variants of it are very common in spoken language. The first part conveys the actual message and the rest is speaker thinking out loud eventually agreeing with the first part. The key factor in dealing with this volatile nature of spoken language is to build models that are capable of operating in presence of unreliable and idiosyncratic speech traits by focusing on important parts of speech.\nOur proposed approach to deal with challenges of spoken language is to learn a rich representation of spoken words at each word interval and use it as input to a fully connected deep network (Figure 3). This rich representation for ith word contains information from beginning of utterance through time, as well as ith word. This way as the model is discovering the meaning of the utterance through time, if it encounters unusable information in word i+ 1 and arbitrary number of words after, the representation up until i is not diluted or lost. Also, if the model encounters usable information again, it can recover by embedding those in the long short-term memory (LSTM). The time-dependent\nencodings are usable by the rest of the pipeline by simply focusing on relevant parts using the nonlinear affine transformation of time-dependent embeddings which can act as a dimension reducing attention mechanism. To formally define our proposed Spoken Language Embedding Subnetwork (Ul), let l = {l1, l2, l3, . . . , lTl ; lt ∈ R300}, where Tl is the number of words in an utterance, be the set of spoken words represented as a sequence of 300-dimensional GloVe word vectors (Pennington et al., 2014).\nA LSTM network (Hochreiter and Schmidhuber, 1997) with a forget gate (Gers et al., 2000) is used to learn time-dependent language representations hl = {h1, h2, h3, . . . , hTl ;ht ∈ R128} for words according to the following LSTM formulation.\ni f o m  =  sigmoid sigmoid sigmoid tanh Wld (XtWleht−1 )\nct = f ct−1 + i m ht = o⊗ tanh(ct) hl = [h1;h2;h3; . . . ;hTl ]\nhl is a matrix of language representations formed from concatenation of h1, h2, h3, . . . hTl . hl is then used as input to a fully-connected network that generates language embedding zl:\nzl = Ul(l; Wl) ∈ R128\nwhere Wl is the set of all weights in the Ul network (including Wld , Wle ,Wlfc , and blfc), σ is the sigmoid function.\nVisual Embedding Subnetwork: Since opinion videos consist mostly of speakers talking to the audience through close-up camera, face is the most important source of visual information. The speaker’s face is detected for each frame (sampled at 30Hz) and indicators of the seven basic emotions\n(anger, contempt, disgust, fear, joy, sadness, and surprise) and two advanced emotions (frustration and confusion) (Ekman, 1992) are extracted using FACET facial expression analysis framework1. A set of 20 Facial Action Units (Ekman et al., 1980), indicating detailed muscle movements on the face, are also extracted using FACET. Estimates of head position, head rotation, and 68 facial landmark locations also extracted per frame using OpenFace (Baltrušaitis et al., 2016; Zadeh et al., 2017).\nLet the visual features v̂j = [v1j , v 2 j , v 3 j , . . . , v p j ] for frame j of utterance video contain the set of p visual features, with Tv the number of total video frames in utterance. We perform mean pooling over the frames to obtain the expected visual features v = [E[v1],E[v2],E[v3], . . . ,E[vl]]. v is then used as input to the Visual Embedding Subnetwork Uv. Since information extracted using FACET from videos is rich, using a deep neural network would be sufficient to produce meaningful embeddings of visual modality. We use a deep neural network with three hidden layers of 32 ReLU units and weights Wv. Empirically we observed that making the model deeper or increasing the number of neurons in each layer does not lead to better visual performance. The subnetwork output provides the visual embedding zv:\nzv = Uv(v; Wv) ∈ R32\nAcoustic Embedding Subnetwork: For each opinion utterance audio, a set of acoustic features are extracted using COVAREP acoustic analysis framework (Degottex et al., 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al., 2012; Alku, 1992; Alku et al., 2002, 1997; Titze and Sundberg, 1992; Childers and Lee, 1991)), peak slope parameters (Degottex et al., 2014), maxima dispersion quotients (MDQ) (Kane and Gobl, 2013), and estimations of the Rd shape parameter of the Liljencrants-Fant (LF) glottal model (Fujisaki and Ljungqvist, 1986). These extracted features capture different characteristics of human voice and have been shown to be related to emotions (Ghosh et al., 2016b).\n1http://goo.gl/1rh1JN\nFor each opinion segment with Ta audio frames (sampled at 100Hz; i.e., 10ms), we extract the set of q acoustic features âj = [a1j , a 2 j , a 3 j , . . . , a q j ] for audio frame j in utterance. We perform mean pooling per utterance on these extracted acoustic features to obtain the expected acoustic features a = [E[a1],E[a2],E[a3], . . . ,E[q]]. Here, a is the input to the Audio Embedding Subnetwork Ua. Since COVAREP also extracts rich features from audio, using a deep neural network is sufficient to model the acoustic modality. Similar to Uv, Ua is a network with 3 layers of 32 ReLU units with weights Wa.\nHere, we also empirically observed that making the model deeper or increasing the number of neurons in each layer does not lead to better performance. The subnetwork produces the audio embedding za:\nza = Ua(a;Wa) ∈ R32"
    }, {
      "heading" : "4.2 Tensor Fusion Layer",
      "text" : "While previous works in multimodal research has used feature concatenation as an approach for multimodal fusion, we aim to build a fusion layer in TFN that disentangles unimodal, bimodal and trimodal dynamics by modeling each of them explicitly. We call this layer Tensor Fusion, which is defined as the following vector field using three-fold Cartesian product:{\n(zl, zv, za) | zl ∈ [ zl\n1\n] , zv ∈ [ zv\n1\n] , za ∈ [ za\n1\n]}\nThe extra constant dimension with value 1 generates the unimodal and bimodal dynamics. Each neural coordinate (zl, zv, za) can be seen as a 3-D point in the 3-fold Cartesian space defined by the language, visual, and acoustic embeddings dimensions [zl1]T , [zv1]T , and [za1]T .\nThis definition is mathematically equivalent to a differentiable outer product between zl, the visual representation zv, and the acoustic representation za.\nzm =\n[ zl\n1\n] ⊗ [ zv\n1\n] ⊗ [ za\n1 ] Here⊗ indicates the outer product between vectors and zm ∈ R129×33×33 is the 3D cube of all possible combination of unimodal embeddings with seven semantically distinct subregions in Figure 4. The first three subregions zl, zv, and za are unimodal embeddings from Modality Embedding Subnetworks forming unimodal interactions in Tensor Fusion. Three subregions zl ⊗ zv, zl ⊗ za, and zv ⊗ za capture bimodal interactions in Tensor Fusion. Finally, zl ⊗ zv ⊗ za captures trimodal interactions.\nEarly fusion commonly used in multimodal research dealing with language, vision and audio, can be seen as a special case of Tensor Fusion with only unimodal interactions. Since Tensor Fusion is mathematically formed by an outer product, it has no learnable parameters and we empirically observed that although the output tensor is high dimensional, chances of overfitting are low.\nWe argue that this is due to the fact that the output neurons of Tensor Fusion are easy to interpret and semantically very meaningful (i.e., the manifold that they lie on is not complex but just high dimensional). Thus, it is easy for the subsequent layers of the network to decode the meaningful information."
    }, {
      "heading" : "4.3 Sentiment Inference Subnetwork",
      "text" : "After Tensor Fusion layer, each opinion utterance can be represented as a multimodal tensor zm. We use a fully connected deep neural network called Sentiment Inference Subnetwork Us with weights Ws conditioned on zm. The architecture of the network consists of two layers of 128 ReLU activation units connected to decision layer. The likelihood function of the Sentiment Inference Subnetwork is defined as follows, where φ is the sentiment prediction:\narg max φ p(φ | zm;Ws) = arg max φ Us(zm;Ws)\nIn our experiments, we use three variations of the Us network. The first network is trained for binary sentiment classification, with a single sigmoid output neuron using binary cross-entropy loss. The second network is designed for five-class sentiment classification, and uses a softmax probability function using categorical cross-entropy loss. The third network uses a single sigmoid output, using meansquarred error loss to perform sentiment regression."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this paper, we devise three sets of experiments each addressing a different research question:\nExperiment 1: We compare our TFN with previous state-of-the-art approaches in multimodal sentiment analysis.\nExperiment 2: We study the importance of the TFN subtensors and the impact of each individual modality (see Figure 4). We also compare with the commonly-used early fusion approach.\nExperiment 3: We compare the performance of our three modality-specific networks (language, visual and acoustic) with state-of-the-art unimodal approaches.\nSection 5.4 describes our experimental methodology which is kept constant across all experiments. Section 6 will discuss our results in more details with a qualitative analysis."
    }, {
      "heading" : "5.1 E1: Multimodal Sentiment Analysis",
      "text" : "In this section, we compare the performance of TFN model with previously proposed multimodal sentiment analysis models. We compare to the following baselines:\nC-MKL (Poria et al., 2015) Convolutional MKL-based model is a multimodal sentiment classification model which uses a CNN to extract textual features and uses multiple kernel learning for sentiment analysis. It is current SOTA (state of the art) on CMU-MOSI.\nSAL-CNN (Wang et al., 2016) Select-Additive Learning is a multimodal sentiment analysis model that attempts to prevent identity-dependent information from being learned in a deep neural network. We retrain the model for 5-fold cross-validation using the code provided by the authors on github.\nSVM-MD (Zadeh et al., 2016b) is a SVM model trained on multimodal features using early fusion. The model used in (Morency et al., 2011) and (Pérez-Rosas et al., 2013) also similarly use SVM on multimodal concatenated features. We also present the results of Random Forest RF-MD to compare to another non-neural approach.\nThe results first experiment are reported in Table 1. TFN outperforms previously proposed neural and non-neural approaches. This difference is specifically visible in the case of 5-class classification."
    }, {
      "heading" : "5.2 E2: Tensor Fusion Evaluation",
      "text" : "Table 4 shows the results of our ablation study. The first three rows are showing the performance of each modality, when no intermodality dynamics are modeled. From this first experiment, we observe that the language modality is the most predictive.\nAs a second set of ablation experiments, we test our TFN approach when only the bimodal subtensors are used (TFNbimodal) or when only the trimodal subtensor is used (TFNbimodal). We observe that bimodal subtensors are more informative when used without other subtensors. The most interesting comparison is between our full TFN model and a variant (TFNnotrimodal) where the trimodal subtensor is removed (but all the unimodal and bimodal subtensors are present). We observe a big improvement for the full TFN model, confirming the importance of the trimodal dynamics and the need for all components of the full tensor.\nWe also perform a comparison with the early fusion approach (TFNearly) by simply concatenating all three modality embeddings < zl, za, zv > and passing it directly as input to Us. This approach was depicted on the left side of Figure 4. When looking at Table 4 results, we see that our TFN approach outperforms the early fusion approach2."
    }, {
      "heading" : "5.3 E3: Modality Embedding Subnetworks Evaluation",
      "text" : "In this experiment, we compare the performance of our Modality Embedding Networks with stateof-the-art approaches for language-based, visualbased and acoustic-based sentiment analysis."
    }, {
      "heading" : "5.3.1 Language Sentiment Analysis",
      "text" : "We selected the following state-of-the-art approaches to include variety in their techniques,\n2We also performed other comparisons with variants of the early fusion model TFNearly where we increased the number of parameters and neurons to replicate the numbers from our TFN model. In all cases, the performances were similar to TFNearly (and lower than our TFN model). Because of space constraints, we could not include them in this paper.\nbased on dependency parsing (RNTN), distributional representation of text (DAN), and convolutional approaches (DynamicCNN). When possible, we retrain them on the CMU-MOSI dataset (performances of the original pre-trained models are shown in parenthesis in Table 3) and compare them to our language only TFNlanguage.\nRNTN (Socher et al., 2013)The Recursive Neural Tensor Network is among the most well-known sentiment analysis methods proposed for both binary and multi-class sentiment analysis that uses dependency structure.\nDAN (Iyyer et al., 2015) The Deep Average Network approach is a simple but efficient sentiment analysis model that uses information only from distributional representation of the words and not from the compositionality of the sentences.\nDynamicCNN (Kalchbrenner et al., 2014) DynamicCNN is among the state-of-the-art models in text-based sentiment analysis which uses a convolutional architecture adopted for the semantic modeling of sentences.\nCMK-L, SAL-CNN-L and SVM-MD-L are multimodal models from section using only language modality 5.1.\nResults in Table 3 show that our model using only language modality outperforms state-of-theart approaches for the CMU-MOSI dataset. While previous models are well-studied and suitable models for sentiment analysis in written language, they underperform in modeling the sentiment in spoken language. We suspect that this underperformance is due to: RNTN and similar approaches rely heavily on dependency structure, which may not be present\nin spoken language; DAN and similar sentence embeddings approaches can easily be diluted by words that may not relate directly to sentiment or meaning; D-CNN and similar convolutional approaches rely on spatial proximity of related words, which may not always be present in spoken language."
    }, {
      "heading" : "5.3.2 Visual Sentiment Analysis",
      "text" : "We compare the performance of our models using visual information (TFNvisual) with the following well-known approaches in visual sentiment analysis and emotion recognition (retrained for sentiment analysis):\n3DCNN (Byeon and Kwak, 2014) a network using 3D CNN is trained using the face of the speaker. Face of the speaker is extracted in every 6 frames and resized to 64× 64 and used as the input to the proposed network.\nCNN-LSTM (Ebrahimi Kahou et al., 2015) is a recurrent model that at each timestamp performs convolutions over facial region and uses output to an LSTM. Face processing is similar to 3DCNN.\nLSTM-FA similar to both baselines above, information extracted by FACET is used every 6 frames as input to an LSTM with a memory dimension of 100 neurons.\nSAL-CNN-V, SVM-MD-V, CMKL-V, RF-V use only visual modality in multimodal baselines from Section 5.1.\nThe results in Table 5 show that Uv is able to outperform state-of-the-art approaches on visual sentiment analysis."
    }, {
      "heading" : "5.3.3 Acoustic Sentiment Analysis",
      "text" : "We compare the performance of our models using visual information (TFNacoustic) with the following well-known approaches in audio sentiment analysis\nand emotion recognition (retrained for sentiment analysis):\nHL-RNN (Lee and Tashev, 2015) uses an LSTM on high-level audio features. We use the same features extracted for Ua averaged over time slices of every 200 intervals.\nAdieu-Net (Trigeorgis et al., 2016) is an endto-end approach for emotion recognition in audio using directly PCM features.\nSER-LSTM (Lim et al., 2016) is a model that uses recurrent neural networks on top of convolution operations on spectrogram of audio.\nSAL-CNN-A, SVM-MD-A, CMKL-A, RF-A use only acoustic modality in multimodal baselines from Section 5.1."
    }, {
      "heading" : "5.4 Methodology",
      "text" : "All the models in this paper are tested using five-fold cross-validation proposed by CMUMOSI (Zadeh et al., 2016a). All of our experiments are performed independent of speaker identity, as no speaker is shared between train and test sets for generalizability of the model to unseen speakers in real-world. The best hyperparameters are chosen using grid search based on model performance on a validation set (using last 4 videos in train fold). The TFN model is trained using the Adam optimizer (Kingma and Ba, 2014) with the learning rate 5e4. Uv and Ua, Us subnetworks are regularized using dropout on all hidden layers with p = 0.15 and L2 norm coefficient 0.01. The train, test and validation folds are exactly the same for all baselines."
    }, {
      "heading" : "6 Qualitative Analysis",
      "text" : "We analyze the impact of our proposed TFN multimodal fusion approach by comparing it with the\nearly fusion approach TFNearly and the three unimodal models. Table 6 shows examples taken from the CMU-MOSI dataset. Each example is described with the spoken words as well as the acoustic and visual behaviors. The sentiment predictions and the ground truth labels range between strongly negative (-3) and strongly positive (+3).\nAs a first general observation, we observe that the early fusion model TFNearly shows a strong preference for the language modality and seems to be neglecting the intermodality dynamics. We can see this trend by comparing it with the language unimodal model TFNlanguage. In comparison, our TFN approach seems to capture more complex interaction through bimodal and trimodal dynamics and thus performs better. Specifically, in the first example, the utterance is weakly negative where the speaker is referring to lack of funny jokes in the movie. This example contains a bimodal interaction where the visual modality shows a negative expression (frowning) which is correctly captured by our TFN approach.\nIn the second example, the spoken words are ambiguous since the model has no clue what a B is except a token, but the acoustic and visual modalities are bringing complementary evidences. Our TFN approach correctly identify this trimodal interaction and predicts a positive sentiment. The third example is interesting since it shows an interaction where language predicts a positive sentiment\nbut the strong negative visual behaviors bring the final prediction of our TFN approach almost to a neutral sentiment. The fourth example shows how the acoustic modality is also influencing our TFN predictions."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We introduced a new end-to-end fusion method for sentiment analysis which explicitly represents unimodal, bimodal, and trimodal interactions between behaviors. Our experiments on the publiclyavailable CMU-MOSI dataset produced state-ofthe-art performance when compared against both multimodal approaches. Furthermore, our approach brings state-of-the-art results for languageonly, visual-only and acoustic-only multimodal sentiment analysis on CMU-MOSI."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This project was partially supported by Oculus research grant. We would like to thank the reviewers for their valuable feedback."
    } ],
    "references" : [ {
      "title" : "Concept-level sentiment analysis with dependencybased semantic parsing: a novel approach",
      "author" : [ "Basant Agarwal", "Soujanya Poria", "Namita Mittal", "Alexander Gelbukh", "Amir Hussain." ],
      "venue" : "Cognitive Computation 7(4):487–499.",
      "citeRegEx" : "Agarwal et al\\.,? 2015",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2015
    }, {
      "title" : "Glottal wave analysis with pitch synchronous iterative adaptive inverse filtering",
      "author" : [ "Paavo Alku." ],
      "venue" : "Speech communication 11(2-3):109–118.",
      "citeRegEx" : "Alku.,? 1992",
      "shortCiteRegEx" : "Alku.",
      "year" : 1992
    }, {
      "title" : "Normalized amplitude quotient for parametrization of the glottal flow",
      "author" : [ "Paavo Alku", "Tom Bäckström", "Erkki Vilkman." ],
      "venue" : "the Journal of the Acoustical Society of America 112(2):701–710.",
      "citeRegEx" : "Alku et al\\.,? 2002",
      "shortCiteRegEx" : "Alku et al\\.",
      "year" : 2002
    }, {
      "title" : "Parabolic spectral parameter—a new method for quantification of the glottal flow",
      "author" : [ "Paavo Alku", "Helmer Strik", "Erkki Vilkman." ],
      "venue" : "Speech Communication 22(1):67–79.",
      "citeRegEx" : "Alku et al\\.,? 1997",
      "shortCiteRegEx" : "Alku et al\\.",
      "year" : 1997
    }, {
      "title" : "Vqa: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision. pages 2425–2433.",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Openface: an open source facial behavior analysis toolkit",
      "author" : [ "Tadas Baltrušaitis", "Peter Robinson", "Louis-Philippe Morency." ],
      "venue" : "Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on. IEEE, pages 1–10.",
      "citeRegEx" : "Baltrušaitis et al\\.,? 2016",
      "shortCiteRegEx" : "Baltrušaitis et al\\.",
      "year" : 2016
    }, {
      "title" : "Facial expression recognition using 3d convolutional neural network",
      "author" : [ "Young-Hyen Byeon", "Keun-Chang Kwak." ],
      "venue" : "International Journal of Advanced Computer Science and Applications 5(12).",
      "citeRegEx" : "Byeon and Kwak.,? 2014",
      "shortCiteRegEx" : "Byeon and Kwak.",
      "year" : 2014
    }, {
      "title" : "A Practical Guide to Sentiment Analysis",
      "author" : [ "Erik Cambria", "Dipankar Das", "Sivaji Bandyopadhyay", "Antonio Feraco." ],
      "venue" : "Springer, Cham, Switzerland.",
      "citeRegEx" : "Cambria et al\\.,? 2017",
      "shortCiteRegEx" : "Cambria et al\\.",
      "year" : 2017
    }, {
      "title" : "SenticNet 4: A semantic resource for sentiment analysis based on conceptual primitives",
      "author" : [ "Erik Cambria", "Soujanya Poria", "Rajiv Bajpai", "Björn Schuller." ],
      "venue" : "COLING. pages 2666–2677.",
      "citeRegEx" : "Cambria et al\\.,? 2016",
      "shortCiteRegEx" : "Cambria et al\\.",
      "year" : 2016
    }, {
      "title" : "Vocal quality factors: Analysis, synthesis, and perception",
      "author" : [ "Donald G Childers", "CK Lee." ],
      "venue" : "the Journal of the Acoustical Society of America 90(5):2394–2410.",
      "citeRegEx" : "Childers and Lee.,? 1991",
      "shortCiteRegEx" : "Childers and Lee.",
      "year" : 1991
    }, {
      "title" : "Covarep—a collaborative voice analysis repository for speech technologies",
      "author" : [ "Gilles Degottex", "John Kane", "Thomas Drugman", "Tuomo Raitio", "Stefan Scherer." ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference",
      "citeRegEx" : "Degottex et al\\.,? 2014",
      "shortCiteRegEx" : "Degottex et al\\.",
      "year" : 2014
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell." ],
      "venue" : "Proceedings of the IEEE",
      "citeRegEx" : "Donahue et al\\.,? 2015",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2015
    }, {
      "title" : "Joint robust voicing detection and pitch estimation based on residual harmonics",
      "author" : [ "Thomas Drugman", "Abeer Alwan." ],
      "venue" : "Interspeech. pages 1973– 1976.",
      "citeRegEx" : "Drugman and Alwan.,? 2011",
      "shortCiteRegEx" : "Drugman and Alwan.",
      "year" : 2011
    }, {
      "title" : "Detection of glottal closure instants from speech signals: A quantitative review",
      "author" : [ "Thomas Drugman", "Mark Thomas", "Jon Gudnason", "Patrick Naylor", "Thierry Dutoit." ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing 20(3):994–1006.",
      "citeRegEx" : "Drugman et al\\.,? 2012",
      "shortCiteRegEx" : "Drugman et al\\.",
      "year" : 2012
    }, {
      "title" : "Recurrent neural networks for emotion recognition in video",
      "author" : [ "Samira Ebrahimi Kahou", "Vincent Michalski", "Kishore Konda", "Roland Memisevic", "Christopher Pal." ],
      "venue" : "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction.",
      "citeRegEx" : "Kahou et al\\.,? 2015",
      "shortCiteRegEx" : "Kahou et al\\.",
      "year" : 2015
    }, {
      "title" : "An argument for basic emotions",
      "author" : [ "Paul Ekman." ],
      "venue" : "Cognition & emotion 6(3-4):169–200.",
      "citeRegEx" : "Ekman.,? 1992",
      "shortCiteRegEx" : "Ekman.",
      "year" : 1992
    }, {
      "title" : "Facial signs of emotional experience",
      "author" : [ "Paul Ekman", "Wallace V Freisen", "Sonia Ancoli." ],
      "venue" : "Journal of personality and social psychology 39(6):1125– 1134.",
      "citeRegEx" : "Ekman et al\\.,? 1980",
      "shortCiteRegEx" : "Ekman et al\\.",
      "year" : 1980
    }, {
      "title" : "Proposal and evaluation of models for the glottal source waveform",
      "author" : [ "Hiroya Fujisaki", "Mats Ljungqvist." ],
      "venue" : "Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP’86.. IEEE, volume 11, pages 1605–1608.",
      "citeRegEx" : "Fujisaki and Ljungqvist.,? 1986",
      "shortCiteRegEx" : "Fujisaki and Ljungqvist.",
      "year" : 1986
    }, {
      "title" : "Learning to forget: Continual prediction with lstm",
      "author" : [ "Felix A Gers", "Jürgen Schmidhuber", "Fred Cummins." ],
      "venue" : "Neural computation 12(10):2451–2471.",
      "citeRegEx" : "Gers et al\\.,? 2000",
      "shortCiteRegEx" : "Gers et al\\.",
      "year" : 2000
    }, {
      "title" : "Representation learning for speech emotion recognition",
      "author" : [ "Sayan Ghosh", "Eugene Laksana", "Louis-Philippe Morency", "Stefan Scherer." ],
      "venue" : "Interspeech 2016. pages 3603–3607. https://doi.org/10.21437/Interspeech.2016-692.",
      "citeRegEx" : "Ghosh et al\\.,? 2016a",
      "shortCiteRegEx" : "Ghosh et al\\.",
      "year" : 2016
    }, {
      "title" : "Representation learning for speech emotion recognition",
      "author" : [ "Sayan Ghosh", "Eugene Laksana", "Louis-Philippe Morency", "Stefan Scherer." ],
      "venue" : "Interspeech 2016 pages 3603–3607.",
      "citeRegEx" : "Ghosh et al\\.,? 2016b",
      "shortCiteRegEx" : "Ghosh et al\\.",
      "year" : 2016
    }, {
      "title" : "Multiple classifier systems for the classification of audio-visual emotional",
      "author" : [ "Michael Glodek", "Stephan Tschechne", "Georg Layher", "Martin Schels", "Tobias Brosch", "Stefan Scherer", "Markus Kächele", "Miriam Schmidt", "Heiko Neumann", "Günther Palm" ],
      "venue" : null,
      "citeRegEx" : "Glodek et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Glodek et al\\.",
      "year" : 2011
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, pages 168– 177.",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "Deep unordered composition rivals syntactic methods for text classification",
      "author" : [ "Mohit Iyyer", "Varun Manjunatha", "Jordan L BoydGraber", "Hal Daumé III." ],
      "venue" : "ACL (1). pages 1681–1691.",
      "citeRegEx" : "Iyyer et al\\.,? 2015",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2015
    }, {
      "title" : "A convolutional neural network for modelling sentences pages 656–666",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom" ],
      "venue" : null,
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Wavelet maxima dispersion for breathy to tense voice discrimination",
      "author" : [ "John Kane", "Christer Gobl." ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing 21(6):1170–1179.",
      "citeRegEx" : "Kane and Gobl.,? 2013",
      "shortCiteRegEx" : "Kane and Gobl.",
      "year" : 2013
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980 .",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "High-level feature representation using recurrent neural network for speech emotion recognition",
      "author" : [ "Jinkyu Lee", "Ivan Tashev." ],
      "venue" : "INTERSPEECH. pages 1537–1540.",
      "citeRegEx" : "Lee and Tashev.,? 2015",
      "shortCiteRegEx" : "Lee and Tashev.",
      "year" : 2015
    }, {
      "title" : "Speech emotion recognition using convolutional and recurrent neural networks",
      "author" : [ "Wootaek Lim", "Daeyoung Jang", "Taejin Lee." ],
      "venue" : "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2016 Asia-Pacific. IEEE,",
      "citeRegEx" : "Lim et al\\.,? 2016",
      "shortCiteRegEx" : "Lim et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "author" : [ "Louis-Philippe Morency", "Rada Mihalcea", "Payal Doshi." ],
      "venue" : "Proceedings of the 13th international conference on multimodal interfaces. ACM, pages 169–176.",
      "citeRegEx" : "Morency et al\\.,? 2011",
      "shortCiteRegEx" : "Morency et al\\.",
      "year" : 2011
    }, {
      "title" : "Emoreact: a multimodal approach and dataset for recognizing emotional responses in children",
      "author" : [ "Behnaz Nojavanasghari", "Tadas Baltrušaitis", "Charles E Hughes", "Louis-Philippe Morency." ],
      "venue" : "Proceedings of the 18th ACM International Conference on Multi-",
      "citeRegEx" : "Nojavanasghari et al\\.,? 2016",
      "shortCiteRegEx" : "Nojavanasghari et al\\.",
      "year" : 2016
    }, {
      "title" : "Opinion mining and sentiment analysis",
      "author" : [ "Bo Pang", "Lillian Lee" ],
      "venue" : "Foundations and Trends R",
      "citeRegEx" : "Pang and Lee,? \\Q2008\\E",
      "shortCiteRegEx" : "Pang and Lee",
      "year" : 2008
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "EMNLP. volume 14, pages 1532– 1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Utterance-level multimodal sentiment analysis",
      "author" : [ "Verónica Pérez-Rosas", "Rada Mihalcea", "LouisPhilippe Morency." ],
      "venue" : "ACL (1). pages 973– 982.",
      "citeRegEx" : "Pérez.Rosas et al\\.,? 2013",
      "shortCiteRegEx" : "Pérez.Rosas et al\\.",
      "year" : 2013
    }, {
      "title" : "Dependency-based semantic parsing for conceptlevel text analysis",
      "author" : [ "Soujanya Poria", "Basant Agarwal", "Alexander Gelbukh", "Amir Hussain", "Newton Howard." ],
      "venue" : "International Conference on Intelligent Text Processing and Computational Lin-",
      "citeRegEx" : "Poria et al\\.,? 2014a",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2014
    }, {
      "title" : "A review of affective computing: From unimodal analysis to multimodal fusion",
      "author" : [ "Soujanya Poria", "Erik Cambria", "Rajiv Bajpai", "Amir Hussain." ],
      "venue" : "Information Fusion 37:98–125.",
      "citeRegEx" : "Poria et al\\.,? 2017",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis",
      "author" : [ "Soujanya Poria", "Erik Cambria", "Alexander F Gelbukh." ],
      "venue" : "EMNLP. pages 2539–2544.",
      "citeRegEx" : "Poria et al\\.,? 2015",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2015
    }, {
      "title" : "Sentic patterns: Dependency-based rules for concept-level sentiment analysis",
      "author" : [ "Soujanya Poria", "Erik Cambria", "Gregoire Winterstein", "Guang-Bin Huang." ],
      "venue" : "Knowledge-Based Systems 69:45–63.",
      "citeRegEx" : "Poria et al\\.,? 2014b",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2014
    }, {
      "title" : "Convolutional mkl based multimodal emotion recognition and sentiment analysis",
      "author" : [ "Soujanya Poria", "Iti Chaturvedi", "Erik Cambria", "Amir Hussain." ],
      "venue" : "Data Mining (ICDM), 2016 IEEE 16th International Conference on. IEEE, pages 439–448.",
      "citeRegEx" : "Poria et al\\.,? 2016",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2016
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the conference on",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "A shared task on multimodal machine translation and crosslingual image description",
      "author" : [ "Lucia Specia", "Stella Frank", "Khalil Sima’an", "Desmond Elliott" ],
      "venue" : "In Proceedings of the First Conference on Machine Translation,",
      "citeRegEx" : "Specia et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Specia et al\\.",
      "year" : 2016
    }, {
      "title" : "Lexicon-based methods for sentiment analysis",
      "author" : [ "Maite Taboada", "Julian Brooke", "Milan Tofiloski", "Kimberly Voll", "Manfred Stede." ],
      "venue" : "Computational linguistics 37(2):267–307.",
      "citeRegEx" : "Taboada et al\\.,? 2011",
      "shortCiteRegEx" : "Taboada et al\\.",
      "year" : 2011
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks pages 1556–1566",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D Manning" ],
      "venue" : null,
      "citeRegEx" : "Tai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Vocal intensity in speakers and singers",
      "author" : [ "Ingo R Titze", "Johan Sundberg." ],
      "venue" : "the Journal of the Acoustical Society of America 91(5):2936–2946.",
      "citeRegEx" : "Titze and Sundberg.,? 1992",
      "shortCiteRegEx" : "Titze and Sundberg.",
      "year" : 1992
    }, {
      "title" : "Combating human trafficking with deep multimodal models",
      "author" : [ "Edmund Tong", "Amir Zadeh", "Louis-Philippe Morency." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Tong et al\\.,? 2017",
      "shortCiteRegEx" : "Tong et al\\.",
      "year" : 2017
    }, {
      "title" : "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "author" : [ "George Trigeorgis", "Fabien Ringeval", "Raymond Brueckner", "Erik Marchi", "Mihalis A Nicolaou", "Björn Schuller", "Stefanos Zafeiriou." ],
      "venue" : "Acous-",
      "citeRegEx" : "Trigeorgis et al\\.,? 2016",
      "shortCiteRegEx" : "Trigeorgis et al\\.",
      "year" : 2016
    }, {
      "title" : "Depression, mood, and emotion recognition workshop",
      "author" : [ "Michel Valstar", "Jonathan Gratch", "Björn Schuller", "Fabien Ringeval", "Dennis Lalanne", "Mercedes Torres Torres", "Stefan Scherer", "Giota Stratou", "Roddy Cowie", "Maja Pantic" ],
      "venue" : "Avec",
      "citeRegEx" : "Valstar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Valstar et al\\.",
      "year" : 2016
    }, {
      "title" : "Select-additive learning: Improving cross-individual generalization in multimodal sentiment analysis",
      "author" : [ "Haohan Wang", "Aaksha Meghawat", "Louis-Philippe Morency", "Eric P Xing." ],
      "venue" : "arXiv preprint arXiv:1609.05244 .",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Youtube movie reviews: Sentiment analysis in an audio-visual context",
      "author" : [ "Martin Wöllmer", "Felix Weninger", "Tobias Knaup", "Björn Schuller", "Congkai Sun", "Kenji Sagae", "LouisPhilippe Morency." ],
      "venue" : "IEEE Intelligent Systems 28(3):46–53.",
      "citeRegEx" : "Wöllmer et al\\.,? 2013",
      "shortCiteRegEx" : "Wöllmer et al\\.",
      "year" : 2013
    }, {
      "title" : "Extracting opinion expressions with semi-markov conditional random fields",
      "author" : [ "Bishan Yang", "Claire Cardie." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language",
      "citeRegEx" : "Yang and Cardie.,? 2012",
      "shortCiteRegEx" : "Yang and Cardie.",
      "year" : 2012
    }, {
      "title" : "Image captioning with semantic attention",
      "author" : [ "Quanzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 4651–4659.",
      "citeRegEx" : "You et al\\.,? 2016",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2016
    }, {
      "title" : "Micro-opinion sentiment intensity analysis and summarization in online videos",
      "author" : [ "Amir Zadeh." ],
      "venue" : "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction. ACM, pages 587– 591.",
      "citeRegEx" : "Zadeh.,? 2015",
      "shortCiteRegEx" : "Zadeh.",
      "year" : 2015
    }, {
      "title" : "Convolutional experts constrained local model for facial landmark detection",
      "author" : [ "Amir Zadeh", "Tadas Baltrušaitis", "Louis-Philippe Morency." ],
      "venue" : "Computer Vision and Pattern Recognition Workshop (CVPRW). IEEE.",
      "citeRegEx" : "Zadeh et al\\.,? 2017",
      "shortCiteRegEx" : "Zadeh et al\\.",
      "year" : 2017
    }, {
      "title" : "Mosi: Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "author" : [ "Amir Zadeh", "Rowan Zellers", "Eli Pincus", "LouisPhilippe Morency." ],
      "venue" : "arXiv preprint arXiv:1606.06259 .",
      "citeRegEx" : "Zadeh et al\\.,? 2016a",
      "shortCiteRegEx" : "Zadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "author" : [ "Amir Zadeh", "Rowan Zellers", "Eli Pincus", "LouisPhilippe Morency." ],
      "venue" : "IEEE Intelligent Systems 31(6):82–88.",
      "citeRegEx" : "Zadeh et al\\.,? 2016b",
      "shortCiteRegEx" : "Zadeh et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Multimodal sentiment analysis (Morency et al., 2011; Zadeh et al., 2016b; Poria et al., 2015) is an increasingly popular area of affective computing research (Poria et al.",
      "startOffset" : 30,
      "endOffset" : 93
    }, {
      "referenceID" : 55,
      "context" : "Multimodal sentiment analysis (Morency et al., 2011; Zadeh et al., 2016b; Poria et al., 2015) is an increasingly popular area of affective computing research (Poria et al.",
      "startOffset" : 30,
      "endOffset" : 93
    }, {
      "referenceID" : 37,
      "context" : "Multimodal sentiment analysis (Morency et al., 2011; Zadeh et al., 2016b; Poria et al., 2015) is an increasingly popular area of affective computing research (Poria et al.",
      "startOffset" : 30,
      "endOffset" : 93
    }, {
      "referenceID" : 36,
      "context" : ", 2015) is an increasingly popular area of affective computing research (Poria et al., 2017) that focuses on generalizing text-based sentiment analysis to opinionated videos, where three communicative modalities are present: language (spoken words), visual (gestures), and acoustic (voice).",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : "This generalization is particularly vital to part of the NLP community dealing with opinion mining and sentiment analysis (Cambria et al., 2017) since there is a growing trend of sharing opinions in videos instead of text, specially in social media (Facebook, YouTube, etc.",
      "startOffset" : 122,
      "endOffset" : 144
    }, {
      "referenceID" : 30,
      "context" : "Early fusion consists in simply concatenating multimodal features mostly at input level (Morency et al., 2011; Pérez-Rosas et al., 2013; Poria et al., 2016).",
      "startOffset" : 88,
      "endOffset" : 156
    }, {
      "referenceID" : 34,
      "context" : "Early fusion consists in simply concatenating multimodal features mostly at input level (Morency et al., 2011; Pérez-Rosas et al., 2013; Poria et al., 2016).",
      "startOffset" : 88,
      "endOffset" : 156
    }, {
      "referenceID" : 39,
      "context" : "Early fusion consists in simply concatenating multimodal features mostly at input level (Morency et al., 2011; Pérez-Rosas et al., 2013; Poria et al., 2016).",
      "startOffset" : 88,
      "endOffset" : 156
    }, {
      "referenceID" : 48,
      "context" : "Late fusion, instead, consists in training unimodal classifiers independently and performing decision voting (Wang et al., 2016; Zadeh et al., 2016a).",
      "startOffset" : 109,
      "endOffset" : 149
    }, {
      "referenceID" : 54,
      "context" : "Late fusion, instead, consists in training unimodal classifiers independently and performing decision voting (Wang et al., 2016; Zadeh et al., 2016a).",
      "startOffset" : 109,
      "endOffset" : 149
    }, {
      "referenceID" : 23,
      "context" : "Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al.",
      "startOffset" : 122,
      "endOffset" : 205
    }, {
      "referenceID" : 42,
      "context" : "Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al.",
      "startOffset" : 122,
      "endOffset" : 205
    }, {
      "referenceID" : 38,
      "context" : "Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al.",
      "startOffset" : 122,
      "endOffset" : 205
    }, {
      "referenceID" : 8,
      "context" : "Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al.",
      "startOffset" : 122,
      "endOffset" : 205
    }, {
      "referenceID" : 50,
      "context" : ", 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 40,
      "context" : ", 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al.",
      "startOffset" : 119,
      "endOffset" : 201
    }, {
      "referenceID" : 35,
      "context" : ", 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al.",
      "startOffset" : 119,
      "endOffset" : 201
    }, {
      "referenceID" : 0,
      "context" : ", 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al.",
      "startOffset" : 119,
      "endOffset" : 201
    }, {
      "referenceID" : 43,
      "context" : ", 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al.",
      "startOffset" : 119,
      "endOffset" : 201
    }, {
      "referenceID" : 24,
      "context" : ", 2015), and distributional representations for sentiment (Iyyer et al., 2015).",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 55,
      "context" : "There exist several multimodal datasets that include sentiment annotations, including the newly-introduced CMU-MOSI dataset (Zadeh et al., 2016b), as well as other datasets including ICT-MMMO (Wöllmer et al.",
      "startOffset" : 124,
      "endOffset" : 145
    }, {
      "referenceID" : 49,
      "context" : ", 2016b), as well as other datasets including ICT-MMMO (Wöllmer et al., 2013), YouTube (Morency et al.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 30,
      "context" : ", 2013), YouTube (Morency et al., 2011), and MOUD (Pérez-Rosas et al.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 34,
      "context" : ", 2011), and MOUD (Pérez-Rosas et al., 2013), however CMUMOSI is the only English dataset with utterancelevel sentiment labels.",
      "startOffset" : 18,
      "endOffset" : 44
    }, {
      "referenceID" : 37,
      "context" : "The newest multimodal sentiment analysis approaches have used deep neural networks, including convolutional neural networks (CNNs) with multiple-kernel learning (Poria et al., 2015), SAL-CNN (Wang et al.",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 48,
      "context" : ", 2015), SAL-CNN (Wang et al., 2016) which learns generalizable features across speakers, and support vector machines (SVMs) with a multimodal dictionary (Zadeh, 2015).",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 52,
      "context" : ", 2016) which learns generalizable features across speakers, and support vector machines (SVMs) with a multimodal dictionary (Zadeh, 2015).",
      "startOffset" : 125,
      "endOffset" : 138
    }, {
      "referenceID" : 36,
      "context" : "Audio-Visual Emotion Recognition is closely tied to multimodal sentiment analysis (Poria et al., 2017).",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 19,
      "context" : "Both audio and visual features have been shown to be useful in the recognition of emotions (Ghosh et al., 2016a).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "Using facial expressions and audio cues jointly has been the focus of many recent studies (Glodek et al., 2011; Valstar et al., 2016; Nojavanasghari et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 162
    }, {
      "referenceID" : 47,
      "context" : "Using facial expressions and audio cues jointly has been the focus of many recent studies (Glodek et al., 2011; Valstar et al., 2016; Nojavanasghari et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 162
    }, {
      "referenceID" : 31,
      "context" : "Using facial expressions and audio cues jointly has been the focus of many recent studies (Glodek et al., 2011; Valstar et al., 2016; Nojavanasghari et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 162
    }, {
      "referenceID" : 51,
      "context" : "Creative and novel applications of using multiple modalities have been among successful recent research directions in machine learning (You et al., 2016; Donahue et al., 2015; Antol et al., 2015; Specia et al., 2016; Tong et al., 2017).",
      "startOffset" : 135,
      "endOffset" : 235
    }, {
      "referenceID" : 11,
      "context" : "Creative and novel applications of using multiple modalities have been among successful recent research directions in machine learning (You et al., 2016; Donahue et al., 2015; Antol et al., 2015; Specia et al., 2016; Tong et al., 2017).",
      "startOffset" : 135,
      "endOffset" : 235
    }, {
      "referenceID" : 4,
      "context" : "Creative and novel applications of using multiple modalities have been among successful recent research directions in machine learning (You et al., 2016; Donahue et al., 2015; Antol et al., 2015; Specia et al., 2016; Tong et al., 2017).",
      "startOffset" : 135,
      "endOffset" : 235
    }, {
      "referenceID" : 41,
      "context" : "Creative and novel applications of using multiple modalities have been among successful recent research directions in machine learning (You et al., 2016; Donahue et al., 2015; Antol et al., 2015; Specia et al., 2016; Tong et al., 2017).",
      "startOffset" : 135,
      "endOffset" : 235
    }, {
      "referenceID" : 45,
      "context" : "Creative and novel applications of using multiple modalities have been among successful recent research directions in machine learning (You et al., 2016; Donahue et al., 2015; Antol et al., 2015; Specia et al., 2016; Tong et al., 2017).",
      "startOffset" : 135,
      "endOffset" : 235
    }, {
      "referenceID" : 54,
      "context" : "opinions from YouTube movie reviews (Zadeh et al., 2016a).",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 40,
      "context" : "Annotation of sentiment has closely followed the annotation scheme of the Stanford Sentiment Treebank (Socher et al., 2013), where sentiment is annotated on a seven-step Likert scale from very negative to very positive.",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 33,
      "context" : ", lTl ; lt ∈ R300}, where Tl is the number of words in an utterance, be the set of spoken words represented as a sequence of 300-dimensional GloVe word vectors (Pennington et al., 2014).",
      "startOffset" : 160,
      "endOffset" : 185
    }, {
      "referenceID" : 22,
      "context" : "A LSTM network (Hochreiter and Schmidhuber, 1997) with a forget gate (Gers et al.",
      "startOffset" : 15,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "A LSTM network (Hochreiter and Schmidhuber, 1997) with a forget gate (Gers et al., 2000) is used to learn time-dependent language representations",
      "startOffset" : 69,
      "endOffset" : 88
    }, {
      "referenceID" : 15,
      "context" : "The speaker’s face is detected for each frame (sampled at 30Hz) and indicators of the seven basic emotions (anger, contempt, disgust, fear, joy, sadness, and surprise) and two advanced emotions (frustration and confusion) (Ekman, 1992) are extracted using FACET facial expression analysis framework1.",
      "startOffset" : 222,
      "endOffset" : 235
    }, {
      "referenceID" : 16,
      "context" : "A set of 20 Facial Action Units (Ekman et al., 1980), indicating detailed muscle movements on the face, are also extracted using FACET.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : "Estimates of head position, head rotation, and 68 facial landmark locations also extracted per frame using OpenFace (Baltrušaitis et al., 2016; Zadeh et al., 2017).",
      "startOffset" : 116,
      "endOffset" : 163
    }, {
      "referenceID" : 53,
      "context" : "Estimates of head position, head rotation, and 68 facial landmark locations also extracted per frame using OpenFace (Baltrušaitis et al., 2016; Zadeh et al., 2017).",
      "startOffset" : 116,
      "endOffset" : 163
    }, {
      "referenceID" : 10,
      "context" : "Acoustic Embedding Subnetwork: For each opinion utterance audio, a set of acoustic features are extracted using COVAREP acoustic analysis framework (Degottex et al., 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al.",
      "startOffset" : 148,
      "endOffset" : 171
    }, {
      "referenceID" : 12,
      "context" : ", 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al.",
      "startOffset" : 162,
      "endOffset" : 187
    }, {
      "referenceID" : 13,
      "context" : ", 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al., 2012; Alku, 1992; Alku et al., 2002, 1997; Titze and Sundberg, 1992; Childers and Lee, 1991)), peak slope parameters (Degottex et al.",
      "startOffset" : 286,
      "endOffset" : 395
    }, {
      "referenceID" : 1,
      "context" : ", 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al., 2012; Alku, 1992; Alku et al., 2002, 1997; Titze and Sundberg, 1992; Childers and Lee, 1991)), peak slope parameters (Degottex et al.",
      "startOffset" : 286,
      "endOffset" : 395
    }, {
      "referenceID" : 44,
      "context" : ", 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al., 2012; Alku, 1992; Alku et al., 2002, 1997; Titze and Sundberg, 1992; Childers and Lee, 1991)), peak slope parameters (Degottex et al.",
      "startOffset" : 286,
      "endOffset" : 395
    }, {
      "referenceID" : 9,
      "context" : ", 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al., 2012; Alku, 1992; Alku et al., 2002, 1997; Titze and Sundberg, 1992; Childers and Lee, 1991)), peak slope parameters (Degottex et al.",
      "startOffset" : 286,
      "endOffset" : 395
    }, {
      "referenceID" : 10,
      "context" : ", 2002, 1997; Titze and Sundberg, 1992; Childers and Lee, 1991)), peak slope parameters (Degottex et al., 2014), maxima dispersion quotients (MDQ) (Kane and Gobl, 2013), and estimations of the Rd shape parameter of the Liljencrants-Fant (LF) glottal model (Fujisaki and Ljungqvist, 1986).",
      "startOffset" : 88,
      "endOffset" : 111
    }, {
      "referenceID" : 26,
      "context" : ", 2014), maxima dispersion quotients (MDQ) (Kane and Gobl, 2013), and estimations of the Rd shape parameter of the Liljencrants-Fant (LF) glottal model (Fujisaki and Ljungqvist, 1986).",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : ", 2014), maxima dispersion quotients (MDQ) (Kane and Gobl, 2013), and estimations of the Rd shape parameter of the Liljencrants-Fant (LF) glottal model (Fujisaki and Ljungqvist, 1986).",
      "startOffset" : 152,
      "endOffset" : 183
    }, {
      "referenceID" : 20,
      "context" : "These extracted features capture different characteristics of human voice and have been shown to be related to emotions (Ghosh et al., 2016b).",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 37,
      "context" : "C-MKL (Poria et al., 2015) Convolutional MKL-based model is a multimodal sentiment classification model which uses a CNN to extract textual features and uses multiple kernel learning for sentiment analysis.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 48,
      "context" : "SAL-CNN (Wang et al., 2016) Select-Additive Learning is a multimodal sentiment analysis model that attempts to prevent identity-dependent information from being learned in a deep neural network.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 55,
      "context" : "SVM-MD (Zadeh et al., 2016b) is a SVM model trained on multimodal features using early fusion.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 30,
      "context" : "The model used in (Morency et al., 2011) and (Pérez-Rosas et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 34,
      "context" : ", 2011) and (Pérez-Rosas et al., 2013) also similarly use SVM on multimodal concatenated features.",
      "startOffset" : 12,
      "endOffset" : 38
    }, {
      "referenceID" : 40,
      "context" : "RNTN (Socher et al., 2013)The Recursive Neural Tensor Network is among the most well-known sentiment analysis methods proposed for both binary and multi-class sentiment analysis that uses dependency structure.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 24,
      "context" : "DAN (Iyyer et al., 2015) The Deep Average Network approach is a simple but efficient sentiment analysis model that uses information only from distributional representation of the words and not from the compositionality of the sentences.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 25,
      "context" : "DynamicCNN (Kalchbrenner et al., 2014) DynamicCNN is among the state-of-the-art models in text-based sentiment analysis which uses a convolutional architecture adopted for the semantic modeling of sentences.",
      "startOffset" : 11,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "3DCNN (Byeon and Kwak, 2014) a network using 3D CNN is trained using the face of the speaker.",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 28,
      "context" : "and emotion recognition (retrained for sentiment analysis): HL-RNN (Lee and Tashev, 2015) uses an LSTM on high-level audio features.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 46,
      "context" : "Adieu-Net (Trigeorgis et al., 2016) is an endto-end approach for emotion recognition in audio using directly PCM features.",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 29,
      "context" : "SER-LSTM (Lim et al., 2016) is a model that uses recurrent neural networks on top of convolution operations on spectrogram of audio.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 54,
      "context" : "All the models in this paper are tested using five-fold cross-validation proposed by CMUMOSI (Zadeh et al., 2016a).",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 27,
      "context" : "The TFN model is trained using the Adam optimizer (Kingma and Ba, 2014) with the learning rate 5e4.",
      "startOffset" : 50,
      "endOffset" : 71
    } ],
    "year" : 2017,
    "abstractText" : "Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-ofthe-art approaches for both multimodal and unimodal sentiment analysis.",
    "creator" : "LaTeX with hyperref package"
  }
}