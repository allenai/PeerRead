{
  "name" : "1305.3981.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Binary Tree based Chinese Word Segmentation",
    "authors" : [ ],
    "emails" : [ "sunmaosong@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "language processing. The granularity mismatch problem is the main cause of the errors. This paper showed that the binary tree representation can store outputs with different granularity. A binary tree based framework is also designed to overcome the granularity mismatch problem. There are two steps in this framework, namely tree building and tree pruning. The tree pruning step is specially designed to focus on the granularity problem. Previous work for Chinese word segmentation such as the sequence tagging can be easily employed in this framework. This framework can also provide quantitative error analysis methods. The experiments showed that after using a more sophisticated tree pruning function for a state-of-the-art conditional random field based baseline, the error reduction can be up to 20%.\nKey words: Natural language processing; Chinese word segmentation; binary tree; Support vector machine.\nIntroduction\nEach Chinese word consists of one or more characters. But there are no delimiters between characters in the sentences to indicate words. Since words are the basic units for many natural language processing tasks, Chinese word segmentation (CWS) is considered as a fundamental task for Chinese language processing. Languages such as Japanese, Thai and Vietnamese have similar problems. The state-of-the-art methods treat the CWS as a character sequence tagging task like the POS-tagging task. A tag indicates the position of the corresponding character in the word. We point out that the state-of-the-art methods suffer from a problem called the granularity mismatch. In CWS, it means that the granularity of the output is hard to perfectly match the granularity of the gold standard (the correct result). Without such problem, the performance is claimed to be increased by Li and Sun [5] . For example, the string 老树(old tree) in the MSR corpus in SIGHAN bake-off 2005[1] is considered as two words, 老(old) and 树(tree), whereas the string 老人(old people) in the same corpus is a single multi-character word consists of two morphemes, 老(old) and 人(people). Similar examples are quite common in any Chinese corpus, and cause most of the errors for CWS models.\n1 Received: 2011-04-## Supported by the National Natural Science Foundation of China (No. ####). *To whom correspondence should be addressed. Tel: ####; E-mail: sunmaosong@gmail.com\nThe explanation of this language phenomenon is that the boundary between the Chinese morphology and the syntax is not clear. The Chinese morphology and syntax share rules and even units (many morphemes such as “老” can also be used as free words). Sometimes it is hard to determine that whether a structure is morphological or syntactical (However, the CWS model has to do this). Historically, a large number of multi-character words in modern Chinese are used to be phrases in ancient Chinese. In order to represent the structures of the unclear part between the morphology and syntax, and to overcome the granularity mismatch problem, we propose the binary tree representation, and a binary tree based CWS framework. In this framework, the CWS is divided into two steps, namely tree building and tree pruning. Fig. 2 shows the data flow in this framework. In Step 1, the raw Chinese sentence is parsed to a binary tree (see Fig. 4 for an example) based on a simple tree building function. After this step ``老树'' and ``老人'' will have similar binary trees:\nIn the tree building step, it is no need to determine that whether the structures are morphological or syntactical. Step 2 is designed to focus on the granularity problem. In Step 2, the tree is pruned based on a tree pruning function. The leaves of the pruned tree (see Fig. 6 for an example) form the output. This binary tree based framework is with several benefits. First, it is a simple framework that can employ many previous CWS methods such as the dictionary based methods, the association measure based methods, and the sequence tagging based methods. Though in the framework we build trees for sentences, the training data is not needed to have any extra manually annotations. We will describe the two steps of the framework in Section 2. Second, it provides quantitative error analysis methods which are described in Section 3. From such analysis, we see that the granularity mismatch problem is the primary cause of error for both “mono corpus” CWS and cross-corpus CWS 2 . The tree pruning step in our framework provides us a way to focus on the granularity problem. More sophisticated method can be employed in this step. We illustrated this idea in Section 4 by using an SVM-based model for the tree pruning. The experiments in Section 5 show that the errors reduced up to 20% comparing to the state-of-the-art CRF-based baseline.\n2 The definitions of Chinese word are not consistent between different corpora. The performance will drop a lot if we do cross-corpus CWS (i.e., train the CWS model from one corpus but test it on another one). This is also a research issue for CWS."
    }, {
      "heading" : "1 Related Work",
      "text" : "Dictionary Matching algorithms such as the forward maximum matching algorithm and the backward maximum matching algorithm are greedy algorithms. The forward maximum matching algorithm finds the longest word in the dictionary that the input sentence starts with, and do this matching recursively for the rest of the sentence. The backward maximum matching algorithm does similar process just from the end of a sentence. These algorithms will fail if the sentence contains any out-of-vocabulary (OOV) words (Words that do not appear in the training data or in the used dictionary are called OOV words. Otherwise they are called IV words). Association measures such as the pairwise mutual information (PMI) and the $t$-test are used for CWS \\cite{sun_chinese_1998}. These methods treat Chinese words as the “character collocations” and use collocation extraction methods to find them. Xue \\shortcite{xue:2003_j} proposed a character sequence tagging framework which is like the POS-tagging task. In such framework, the input is a raw Chinese sentence \uD835\uDC94, which can be seen as a sequence of characters \uD835\uDC50\uD835\uDC56.\n\uD835\uDC94 = \uD835\uDC501 ⋯ \uD835\uDC50\uD835\uDC5B\nThe output of the character sequence tagging is a sequence \uD835\uDC90 of labels \uD835\uDC61\uD835\uDC56 corresponding to the input characters.\n\uD835\uDC90 = \uD835\uDC611 ⋯ \uD835\uDC61\uD835\uDC5B\nwhere \uD835\uDC61\uD835\uDC56 ∈ *B, M, E, S+. The tag B / M / E indicates the corresponding character is at the beginning / middle / end of a multi-character word. The tag S indicates the corresponding character is a single character word. For example, if the gold standard result for the input ``材料利用率高'' is “材料 利用率 高”, the corresponding correct tag sequence will be B E B M E S. And in MSR corpus “老树” is tagged as S S, and ``老人'' is tagged as $\\textsf{B~E}$. This character sequence tagging framework can be implemented by a CRF\\cite{peng_chinese_2004} model, a perceptron\\cite{gao_chinese_2005} or other models. Some reranking methods \\cite{jiang_word_2008} are proposed to adjust the output.\nThe sequence tagging methods are considered to have the ability to identify the OOV words and make use of existent dictionary \\cite{kruengkrai_error-driven_2009}. We will show that they still suffer from the granularity mismatch problem. Some tree based methods for Chinese word segmentation were proposed\\cite{zhao_character-level_2009,liu_information_2008} in order to represent the morphological or syntactical structure for better CWS. But these methods are hard to be applied for they need the training data to be extra manually annotated."
    }, {
      "heading" : "2 Method",
      "text" : ""
    }, {
      "heading" : "2.1 Tree Building",
      "text" : "The first step of our framework is to build a binary tree for an input sentence \uD835\uDC501 ⋯ \uD835\uDC50\uD835\uDC5B. The process can be simply based on a single function \uD835\uDC4F(\uD835\uDC50\uD835\uDC56), which gives the confidence that there is a word boundary between \uD835\uDC50\uD835\uDC56 and c\uD835\uDC56+1. The function \uD835\uDC4F(\uD835\uDC50) could be derived from various previous works. For a PMI-based method\\cite{sun_chinese_1998} which is an association measure based method, we can define the function \uD835\uDC4FPMI(\uD835\uDC50) as:\n\uD835\uDC4FPMI(\uD835\uDC50\uD835\uDC56) = log \uD835\uDC43(\uD835\uDC50\uD835\uDC56\uD835\uDC50\uD835\uDC56+1)\n\uD835\uDC43(\uD835\uDC50\uD835\uDC56)\uD835\uDC43(\uD835\uDC50\uD835\uDC56+1)\nFor the CRF-based method, we define this function \uD835\uDC4FCRF(\uD835\uDC50) as the marginal probability that:\n\uD835\uDC4FCRF(\uD835\uDC50\uD835\uDC56) = \uD835\uDC43(\uD835\uDC61\uD835\uDC56 = S ∨ \uD835\uDC61\uD835\uDC56 = E|\uD835\uDC2C)\nThe algorithm to building the binary tree based on this function is described as the function [x] as the pseudo code.\nThis function \\ref{function:tb} recursively splits the sequence \uD835\uDC50\uD835\uDC56 ⋯ \uD835\uDC50\uD835\uDC57 at \uD835\uDC50\uD835\uDC5A which has a maximum \uD835\uDC4F(\uD835\uDC50), until all the leaves are single characters. An example of the binary tree is showed in Fig. \\ref{fig:tree}. Notice that this is a tree for the entire sentence, but it is not a tree used to represent the morphological or syntactical structure. It represents the structures of the unclear part between the Chinese morphology and syntax."
    }, {
      "heading" : "2.2 Tree Pruning",
      "text" : "The second step is to prune the binary tree, which focus on the granularity. The leaves of the pruned tree form the segmentation output. The example of a pruned binary tree is showed in Fig. \\ref{fig:pruned-tree}.\nThe pruning can also be applied by a single function \uD835\uDC5D(\uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC5A, \uD835\uDC50\uD835\uDC5A+1 … \uD835\uDC50\uD835\uDC57), where \uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC5A and \uD835\uDC50\uD835\uDC5A+1 … \uD835\uDC50\uD835\uDC57 are the roots of two subtrees of the node \uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC57. The function returns 1 if the subtrees of the node \uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC57 should be pruned, or 0 if not.\nThis binary function can be based on a threshold \uD835\uDC61 and the same \uD835\uDC4F(\uD835\uDC50) function used for the tree building:\n\uD835\uDC5Dthreshold\uD835\uDC61(\uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC5A, \uD835\uDC50\uD835\uDC5A+1 … \uD835\uDC50\uD835\uDC57) = { 1, \uD835\uDC4F(\uD835\uDC50\uD835\uDC5A) < \uD835\uDC61; 0, otherwise.\n([x])\nThis is a trivial pruning function. When we set \uD835\uDC61 to 0.5 and set the \uD835\uDC4F(\uD835\uDC50) to \uD835\uDC4FCRF(\uD835\uDC50\uD835\uDC56), the output is the same as the output by the original CRF-based method. Word dictionary can be easily employed in the pruning step as another pruning function:\n\uD835\uDC5Ddictionary(\uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC5A, \uD835\uDC50\uD835\uDC5A+1 … \uD835\uDC50\uD835\uDC57) = { 1, \uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC57 is in the dictionary;\n0, otherwise.\n([x])\nNotice that the values of particular pruning function may contain conflicts. For example, for the binary tree in Fig. \\ref{fig:tree}, we may have \uD835\uDC5D(材料,利用率) = 1 and \uD835\uDC5D(利利利) = 0. So, for the tree \uD835\uDC47 to be pruned, we could have a top-down tree pruning function TDTP described as the pseudo code, and a bottom-up tree pruning function BUTP."
    }, {
      "heading" : "2.3 A Granularity Based Explanation",
      "text" : "For the previous work like the CRF-based methods without the binary tree based framework, the outputted words can be directly determined by the function \uD835\uDC4F(\uD835\uDC50\uD835\uDC56) and a threshold \uD835\uDC61:\nThe \uD835\uDC61 -words of an input sentence \uD835\uDC501 … \uD835\uDC50\uD835\uDC5B are any substrings like \uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC57 , such that\nmin (\uD835\uDC4F(\uD835\uDC50\uD835\uDC56−1), \uD835\uDC4F(\uD835\uDC50\uD835\uDC57)) > \uD835\uDC61 > max (\uD835\uDC4F(\uD835\uDC50\uD835\uDC56), ⋯ , \uD835\uDC4F(\uD835\uDC50\uD835\uDC57−1))\nDefinition: The inequalities in this definition mean that there is a word boundary between \uD835\uDC50\uD835\uDC56 and \uD835\uDC50\uD835\uDC56+1 if and only if \uD835\uDC4F(\uD835\uDC50\uD835\uDC56) > \uD835\uDC61. Notice that different results can be got with different thresholds. The greater the threshold is, the more coarse-grained the segmentation result is, which means there are lesser number of words in the output. The threshold \uD835\uDC61 can be seen as a parameter to control the output granularity. It is better to store all the results with different granularity (by different thresholds). We can use an altered definition as: Definition: The word candidates of an input sentence \uD835\uDC501 … \uD835\uDC50\uD835\uDC5B are any substrings like \uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC57 such\nthat min (\uD835\uDC4F(\uD835\uDC50\uD835\uDC56−1), \uD835\uDC4F(\uD835\uDC50\uD835\uDC57)) > max (\uD835\uDC4F(\uD835\uDC50\uD835\uDC56), ⋯ , \uD835\uDC4F(\uD835\uDC50\uD835\uDC57−1))\nThe only difference is that there is no threshold \uD835\uDC61 in the inequalities. This definition is also natural. The explanation of this definition is that if the left and right boundaries of a string are more likely to be word boundaries than any character boundaries inside the string, this string may be a word.\nThe relation between the binary tree and the word candidates of an input sentence can be described as a proposition: Proposition: A string is a word candidate if and only if it is in the binary tree of the corresponding sentence. This proposition shows that the binary tree is a suitable representation to store all the word candidates with different granularity, which provides rich information for the tree pruning process to focus on the granularity problem."
    }, {
      "heading" : "3 Binary Tree Based Error Analysis",
      "text" : "The statistics-based CWS algorithms are lack of error analysis methods. In the SIGHAN bake-off, the errors are only been divided into IV word errors and OOV word errors. Here we propose a new way to classify the errors for methods such as the CRF-based ones, and a way to investigate the performance without the granularity mismatch problem. The granularity mismatch is an important cause for the errors. Since using binary trees is a way to maintain all the results for different granularity, the use of the binary trees can be helpful for the error analysis. If an error is only caused by the granularity mismatch, the corresponding word in the gold standard should be found in the binary tree (It also should be the word candidate as we discussed in Section 2), although it is not in the output of the pruned tree. Otherwise, the corresponding word cannot be found in the binary tree. According to this idea, in our framework, we divide the errors into tree errors and pruning errors, and the pruning errors can be caused by either over-pruning or less-pruning. We describe this as follows:  Tree error. If a word in the gold standard cannot be found in the binary tree, it is called a tree\nerror. It also means that the word is not in the output according to the threshold-based pruning method with any thresholds. This kind of errors cannot be corrected in the tree pruning step in our framework.\n Pruning error. If a word in the gold standard and can be found in the binary tree but it is not in\nthe output, it is called a pruning errors. This kind of errors is caused by the granularity mismatch and could be corrected in the tree pruning step. These errors can be further divided into two subcategories:  Over-pruning error. If a word in the gold standard is pruned by the tree pruning function,\nit is called a over-pruning error. This is because of that the segmentation granularity for this word is too coarse.\n Less-pruning error. If a word in the gold standard and its children are not pruned by the\ntree pruning function, it is called a less-pruning error. This is because of that the segmentation granularity for this word is too fine.\nBoth IV and OOV words may have tree errors and pruning errors. This is a new dimension to describe the errors besides the IV-OOV-based classification. In order to estimate the upper bound of the performance for the tree pruning step, we define the oracle pruning function based on the gold standard:\n\uD835\uDC5Doracle(\uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC5A, \uD835\uDC50\uD835\uDC5A+1 … \uD835\uDC50\uD835\uDC57) = { 1, there is no word boundary between \uD835\uDC50\uD835\uDC5A and \uD835\uDC50\uD835\uDC5A+1 in the gold standard; 0, otherwise.\n([x])\nThis function can be seen as a perfect pruning. By this pruning function, we can investigate the performance without the granularity mismatch problem for both ``mono corpus'' CWS and cross-corpus CWS."
    }, {
      "heading" : "4 An SVM-based Tree Pruning Function",
      "text" : "Here we introduce a more sophisticated SVM-based function \uD835\uDC5DSVM for the tree pruning in our framework. A state-of-the-art CRF-based model is used as the tree building function. We need two training sets. The training set \uD835\uDC12b is used to train a CRF-based CWS model. The training set \uD835\uDC12p is used to train the SVM-based tree pruning model. We need to train an SVM model as the binary pruning function \uD835\uDC5DSVM(). Sentences in \uD835\uDC12p will first be parsed to binary trees by the trained CRF-based tree building function. Any input pairs for \uD835\uDC5D(\uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC5A, \uD835\uDC50\uD835\uDC5A+1 … \uD835\uDC50\uD835\uDC57) that 0.95 > \uD835\uDC4F(\uD835\uDC50\uD835\uDC5A) > 0.05 are with less confidence for the CRF model and are used as the samples to train the SVM model. The oracle pruning \uD835\uDC5Doracle() values are used as the answers. There are some notations for describing the features. For the pruning function \uD835\uDC5DSVM(\uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC5A, \uD835\uDC50\uD835\uDC5A+1 … \uD835\uDC50\uD835\uDC57), we have \uD835\uDC8D = \uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC5A$, \uD835\uDC93 = \uD835\uDC50\uD835\uDC5A+1 … \uD835\uDC50\uD835\uDC57 and \uD835\uDC8E = \uD835\uDC50\uD835\uDC56 … \uD835\uDC50\uD835\uDC57. We also have \uD835\uDC8D−\uD835\uDFCF = \uD835\uDC50\uD835\uDC62 … \uD835\uDC50\uD835\uDC56−1 that \uD835\uDC4F(\uD835\uDC50\uD835\uDC62−1) > 0.5 and \uD835\uDC4F(\uD835\uDC50\uD835\uDC58) < 0.5 for $\uD835\uDC58 = \uD835\uDC62, … , \uD835\uDC56 − 2, which is the word on the left side of \uD835\uDC8D according to the CRF-based tree building function. Similarly we have the string \uD835\uDC93+\uD835\uDFCF. The operator ‖\uD835\uDC94‖ returns the frequency of the string \uD835\uDC94 in a corpus. The operator ‖\uD835\uDC94‖tree returns the frequency of the string \uD835\uDC94 in the binary trees. The features for the SVM are described as follows: CRF-based features: The CRF-based features include the probability \uD835\uDC43(\uD835\uDC61\uD835\uDC56 = S ∨ \uD835\uDC61\uD835\uDC56 = E|\uD835\uDC2C), and the marginal probabilities, \uD835\uDC43(\uD835\uDC61\uD835\uDC58 = \uD835\uDC61|\uD835\uDC94) for \uD835\uDC58 ∈ *\uD835\uDC5A, \uD835\uDC5A + 1+ and \uD835\uDC61 ∈ *B, M, E, S+. Length-based features: Each character is one syllable in Chinese. Since syllable-length is a factor for Chinese word forming, we use binary features to represent the length-based information for the strings \uD835\uDC8D and \uD835\uDC93. Dictionary-based features: We use the word unigrams and word bigrams in \uD835\uDC12b to construct a unigram dictionary and a bigram dictionary. We use binary features to represent whether the strings \uD835\uDC8D, \uD835\uDC93 and \uD835\uDC8E are IV or OOV, respectively. Similar features are for the string pairs (\uD835\uDC8D−\uD835\uDFCF, \uD835\uDC8D), (\uD835\uDC8D, \uD835\uDC93), (\uD835\uDC93, \uD835\uDC93+\uD835\uDFCF), (\uD835\uDC8D−\uD835\uDFCF, \uD835\uDC8E) and (\uD835\uDC8E, \uD835\uDC93+\uD835\uDFCF). Association-measure-based features: These features are designed to capture the global information of the strings. We define a chi 2 -like function to measure the association between two strings. For the string pair (\uD835\uDC8D, \uD835\uDC93), we set \uD835\uDC4E = ‖\uD835\uDC8E‖, \uD835\uDC4E + \uD835\uDC4F = ‖\uD835\uDC8D‖, \uD835\uDC4E + \uD835\uDC50 = ‖\uD835\uDC93‖ and \uD835\uDC4E + \uD835\uDC4F + \uD835\uDC50 + \uD835\uDC51 to be the total number of the characters. The frequencies are counted in training sets and the test set. The feature value is (\uD835\uDC4E\uD835\uDC51 − \uD835\uDC4F\uD835\uDC50)2 (\uD835\uDC4E + \uD835\uDC4F) (\uD835\uDC4E + \uD835\uDC50) (\uD835\uDC4F + \uD835\uDC51) (\uD835\uDC50 + \uD835\uDC51)⁄ . Similar feature values can be calculated for the string pairs (\uD835\uDC8D−\uD835\uDFCF, \uD835\uDC8D), (\uD835\uDC93, \uD835\uDC93+\uD835\uDFCF), (\uD835\uDC8D−\uD835\uDFCF, \uD835\uDC8E) and (\uD835\uDC8E, \uD835\uDC93+\uD835\uDFCF). Tree-based features: These features are designed to capture the local information of the strings in the context. The idea is that if a string appears frequently in a document, it is likely to be a word. The frequencies are counted in either \uD835\uDC12p or the test set. The used feature values are log‖\uD835\uDC8E‖tree and\nlog‖\uD835\uDC8E‖tree − max\uD835\uDC8E′∈pa(\uD835\uDC8E) log‖\uD835\uDC8E ′‖tree, where pa(\uD835\uDC8E) is the set of all the parent nodes of \uD835\uDC8E in the trees. We use the CRF-based tree building function \uD835\uDC4FCRF() and the SVM-based tree pruning function \uD835\uDC5DSVM() for the test."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets and the Baseline",
      "text" : "We use these four corpora of the SIGHAN bake-off 2005~\\cite{emerson_second_2005} for our evaluation. They are free and widely used for the evaluation by most of the previous work. The used measurements for the evaluation are the precision that \uD835\uDC5D = (# of words segmented correctly) (# of words in the output)⁄ , the recall that \uD835\uDC5F = (# of words segmented correctly) (# of words in the gold standard)⁄ and the F_measure = 2\uD835\uDC5D\uD835\uDC5F/(\uD835\uDC5D + \uD835\uDC5F). Besides, the OOV rate is also calculated for the analysis. We use a state-of-the-art CRF-based method as the baseline, and to define the tree building function \uD835\uDC4F(\uD835\uDC50\uD835\uDC56). The error analysis is also based on it. The CRF-based model is trained using the toolkit Pocket CRF 3 . The used feature templates for character \uD835\uDC50\uD835\uDC56 are:"
    }, {
      "heading" : "5.2 Binary Tree Based Error Analysis",
      "text" : "First, we investigate the numbers of different errors for the baseline method. The errors are not only divided into IV errors and OOV errors, but are also divided based on the binary trees as we discussed in the previous section.\nThe results are in Table \\ref{tab:error_types}. Among the four different copora, the phenomena are similar. Tree errors are much less than the pruning errors. This indicates that the granularity mismatch is the most primary cause of the errors. We can also see that there are more IV words for the over-pruning errors while there are OOV words for the less-pruning errors. This is due to the phenomenon that most of the OOV words are consist of IV words but not vice versa.\n3 http://pocket-crf-1.sourceforge.net/\nThen, with the help of the oracle pruning function, we can see what performance we can get without the granularity mismatch problem.\nThe F measures by the threshold based pruning function (the output will be the original output by the CRF-based baseline) and the oracle pruning function can be found in Table \\ref{tab:baseline}. These results show that the upper bound of the F_measure for the best pruning function is quite high. We see that a better pruning function is useful to improve the performance. Then we investigate the performances of cross-corpus CWS. The MSR and PKU corpus are both in simplified Chinese. We trained a CRF-based model using the training set of the MSR corpus and test it on the test set of the PKU corpus. This experiment is called `MSR to PKU'. Similar experiment `PKU to MSR' is also performed.\nThe results for these two cross-corpus CWS experiments are in Table \\ref{tab:cross_corpus}. Though the F measures of the threshold based pruning function (i.e. the original CRF-based model) are much poor, the F measures of the oracle pruning function are still high. Especially for the `MSR to PKU', there are more than 98\\% of the words in the gold standard which can be found in the binary trees. The morphological and syntax structures are the same in any corpora. The drop of the performance for the cross-corpus experiments are caused by the worse granularity mismatch problem."
    }, {
      "heading" : "5.3 SVM-based Tree Pruning",
      "text" : "In order to compare to the previous works based on the same training sets in SIGHAN bake-off 2005 and avoid using any other resources, we divided the original training set into two parts. Nine tenths of them are used as the training set $\\mathcal{S}_\\textup{b}$ to train the CRF model for the tree building, while the rest one tenth are used as the training set $\\mathcal{S}_\\textup{p}$ to train the SVM model for the tree pruning. We use LibSVM 4 for the training and testing for the SVM-based model, and use all the default\n4 http://www.csie.ntu.edu.tw/~cjlin/libsvm/\nparameters. Features are described in Section \\ref{section:SVM}. The results are in Table \\ref{tab:svm}. The first and second rows show the F measures of the original CRF-based baseline with 100% and 90% of the training set, respectively. The third row shows the F measures of the method that uses the SVM-based pruning function. We see that the performance only drops a little when we reduce 10\\% of the training data for the CRF model. After using them for the training of the more sophisticated SVM-based pruning model, the performances increase. If we define the error rate as $1-\\textup{F-measure}$, the error reduction is about 10\\% and up to 20\\%, which is significant for the CWS evaluation. Other experiments also show that all kinds of features help the performance."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We proposed a binary tree representation for the structures of the unclear part between the Chinese morphology and syntax. We also proposed a simple binary tree based two-step framework for CWS, namely tree building and tree pruning. Previous models for CWS can be employed in this framework. The binary tree representation provides a quantitative error analysis method for CWS, by which we see that the granularity mismatch problem is the primary cause of the errors for CWS and cross-corpus CWS. We also illustrated with an SVM-based tree pruning model for the Step 2, and reduce the error rate up to 20\\% from a state-of-the-art CRF-based baseline. The definition of Chinese word is not clear even for the linguists\\cite{xue_defining_2001}. The disagreements of the segmentation standard between different corpora such as the disagreement between MSR and PKU corpus are mainly on the granularity. Moreover, applications such as machine translation and information retrieve need CWS models with different granularity. Our binary tree representation not only provides a way to improve the performance of CWS, but also provides a way to solve these problems. We can have a unified tree building function and different tree pruning functions for different corpora and applications with different granularity."
    } ],
    "references" : [ {
      "title" : "The second international Chinese word segmentation bakeoff",
      "author" : [ "T Emerson" ],
      "venue" : "Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing. Jeju Island,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2005
    }, {
      "title" : "Chinese word segmentation and named entity recognition: A pragmatic approach",
      "author" : [ "Gao Jianfeng", "Li Mu", "Huang Chang-Ning", "Wu Andi" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Word lattice reranking for Chinese word segmentation and Part-of-Speech tagging",
      "author" : [ "Jiang Wenbin", "Mi Haitao", "Liu Qun" ],
      "venue" : "Proceedings of the 22 International Conference on Computational Linguistics Volume 1. Association for Computational Linguistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "An Error-Driven  Word-Character hybrid model for joint Chinese word segmentation and POS tagging",
      "author" : [ "C Kruengkrai", "K Uchimoto", "J Kazama", "Yiou Wang", "K Torisawa", "Hitoshi Isahara" ],
      "venue" : "Proc. of ACL-IJCNLP",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Punctuation as implicit annotations for Chinese word segmentation",
      "author" : [ "Li Zhongguo", "Sun Maosong" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Information retrieval oriented word segmentation based on character associative strength ranking",
      "author" : [ "Y Liu", "B Wang", "F Ding", "S Xu" ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "Chinese segmentation and new word detection using conditional random fields",
      "author" : [ "Peng Fuchun", "Feng Fangfang", "McCallum A" ],
      "venue" : "Proceedings of the 20th international conference on Computational Linguistics. Association for Computational Linguistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2004
    }, {
      "title" : "Chinese word segmentation without using lexicon and hand-crafted training data, In: Proceedings of the 17th international conference on Computational linguistics-Volume",
      "author" : [ "Sun Maosong", "Shen Dayang", "Tsou Benjamin K" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "Defining and automatically identifying words in Chinese: Ph.D",
      "author" : [ "Xue Nianwen" ],
      "venue" : "Dissertation, University of Delaware,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2001
    }, {
      "title" : "Chinese word segmentation as character tagging",
      "author" : [ "Xue Nianwen" ],
      "venue" : "Computational Linguistics and Chinese Language Processing,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2003
    }, {
      "title" : "Character-Level dependencies in Chinese: Usefulness and learning, In: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics",
      "author" : [ "Zhao Hai" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Without such problem, the performance is claimed to be increased by Li and Sun [5] .",
      "startOffset" : 79,
      "endOffset" : 82
    } ],
    "year" : 2011,
    "abstractText" : "Chinese word segmentation is a fundamental task for Chinese language processing. The granularity mismatch problem is the main cause of the errors. This paper showed that the binary tree representation can store outputs with different granularity. A binary tree based framework is also designed to overcome the granularity mismatch problem. There are two steps in this framework, namely tree building and tree pruning. The tree pruning step is specially designed to focus on the granularity problem. Previous work for Chinese word segmentation such as the sequence tagging can be easily employed in this framework. This framework can also provide quantitative error analysis methods. The experiments showed that after using a more sophisticated tree pruning function for a state-of-the-art conditional random field based baseline, the error reduction can be up to 20%.",
    "creator" : "Microsoft® Word 2010"
  }
}