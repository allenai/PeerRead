{
  "name" : "1702.01802.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "freitagm@us.ibm.com", "onaizan@us.ibm.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n01 80\n2v 1\n[ cs\n.C L\n] 6\nF eb\n2 01\n7"
    }, {
      "heading" : "1 Introduction",
      "text" : "Knowledge distillation describes the idea of enhancing a student network by matching its predictions to the ones of a stronger teacher network. There are two possible ways of using knowledge distillation for Neural Machine Translation (NMT): First, the student network can be a model with less layers and/or hidden units. The main purpose of this is to reduce the model size of the NMT system without significant loss in translation quality. Secondly, without changing the model architecture, one can get reasonable gains by combining different models of the same architecture with\nensemble. By using an ensemble, you also get the disadvantage of a much slower decoding speed. We show that the performance of a teacher composed of an ensemble of 6 models can be achieved with a student composed of a single model leading to a significantly faster decoding and smaller memory footprint. In addition to the ensemble teacher network, we also investigate a teacher network that is producing the best sentence BLEU translations from the final decoder beam. Further, we demonstrate how to improve an NMT system, if the student network has the same architecture and dimensions as the teacher network.\nTraining an NMT system on several millions of parallel sentences is already slow. When applying knowledge distillation, a second training phase on at least the same amount of data is needed. In this work, we show how to filter the training data with knowledge distillation without losing performance. Further by filtering the data, we get rid of noisy parallel data and sentences that are not reachable by the NMT system. We show that filtering the data does not only make the training faster, but also improves the translation quality."
    }, {
      "heading" : "2 Related Work",
      "text" : "(Buciluǎ et al., 2006) show how to compress the function that is learned by a complex ensemble model into a much smaller, faster model that has comparable performance. Results on eight test problems show that, on average, the loss in performance due to compression is usually negligible.\n(Ba and Caruana, 2014) demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets with knowledge distillation. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to deeper convolutional models.\n(Hinton et al., 2015) present knowledge distillation for image classification (MNIST) and acoustic modelling. They show that nearly all of the improvement that is achieved by training an ensemble of deep neural nets can be distilled into a single neural net of the same size.\n(Kim and Rush, 2016) use knowledge distillation for NMT to reduce the model size of their neural network. Their best student model runs 10 times faster with little loss in performance.\nTo the best of our knowledge, we are the first to apply knowledge distillation based on an ensemble model for Neural Machine Translation. Further, we use a teacher network that generates the best sentence BLEU hypothesis out of an NMT model and use these translations for knowledge distillation. We show how to use the techniques of knowledge distillation to benefit from a teacher network that has the same architecture and dimensions as the student network. We introduce a data filtering method using knowledge distillation that not only speed up the training process, but also improves the translation quality. We further investigate, if the parameters of the student model should be randomly initialize or if we start training from the final parameters of a baseline student network that has been trained on the given parallel data only."
    }, {
      "heading" : "3 Knowledge Distillation",
      "text" : "The idea of knowledge distillation is to match the predictions of a student network to that of a teacher network. In this work, we collect the predictions of the teacher network by translating the full training data with the teacher network. By doing this, we produce a new reference for the training data which can be used by the student network to simulate the teacher network. The idea is that the forward translation of the teacher network should be more reachable than the original reference. There are two ways of using the forward translation. First, we can train the student network only on the original source and the translations. Secondly, we can add the translations as additional training data to the original training data. This has the side effect that the final training data size of the student network is doubled."
    }, {
      "heading" : "4 Teacher Networks",
      "text" : "• Ensemble Teacher Model An ensemble of different NMT models can improve the translation performance of an\nNMT system. The idea is to train several models in parallel and combine their predictions by averaging the probabilities of the individual models at each time step during decoding. In this work, we use an ensemble of 6 models as a teacher model. All 6 individual systems are trained on the same parallel data and use the same optimization method. The only difference is the random initialization of the parameters.\n• Best BLEU Teacher Model We use a left-to-right beam-search decoder to build new translations that aims to maximize the conditional probability of a given model. It stops the search when it found a fix number of hypotheses that end with an end-of-sequence symbol and picks the translation with the highest log-probability out of the final candidate list. In our distillation approach, we produce the forward translation of our parallel data. Since we know the reference translation of all sentences, we choose instead of the highest log-probability the sentence with the highest sentence level BLEU from the final candidate list. We use the sentence level BLEU proposed by (Lin and Och, 2004) which adds 1 to both the matched and the total n-gram counts."
    }, {
      "heading" : "5 Data Filtering",
      "text" : "In machine translation, bilingual sentence pairs that serve as training data are mostly crawled from the web and contain many nonparallel sentence pairs. Furthermore, one source sentence can have several correct translations that differ in choice and order of words. The training of the network gets complicated, if the training corpus contains noisy sentence pairs or sentences with several correct translations. In our knowledge distillation approach, we translate the full parallel data with our teacher model. This gives us the option to score each translation with the original reference. We remove sentences with high TER scores (Snover et al., 2006) from our training data. By removing noisy or unreachable sentence pairs, the training algorithm is able to learn a stronger network."
    }, {
      "heading" : "6 Experiments",
      "text" : "We run our experiments on the German→English WMT 2016 translation task (Bojar et al., 2016)\n(3.9 parallel sentences). We use our in-house attention-based NMT implementation which is similar to (Bahdanau et al., 2014). We use sub-word units extracted by byte pair encoding (Sennrich et al., 2015) instead of words which shrinks the vocabulary to 40k sub-word symbols for both source and target. We use an embedding dimension of 620 and fix the RNN GRU layers to be of 1000 cells each. For the training procedure, we use SGD (Bishop, 1995) to update the model parameters with a mini-batch size of 64. Starting from the 4th epoch, we reduce the learning rate by half every epoch. The training data is shuffled after each epoch and we use beam 5 for all translations. All different setups are run twice: First, we train the student network from scratch with random parameter initialization. Secondly, we continue the training based on the final parameters of a fully trained baseline model."
    }, {
      "heading" : "7 Results",
      "text" : "• Single Teacher Model Instead of using a stronger teacher model, we use the same model for both student and teacher network. By using the forward translation, we can stabilize the student network and make its decision much stronger. Results are given in Table 1. Using only the forward translation does not improve the model. When combining both the reference and the forward translation, we improve the model by 1.4 points in both BLEU and TER. Pruning the training data and using only sentence pairs with a TER score less than 0.8 yields to similar translation quality while reducing the training data by 12% leading to a faster training.\n• Ensemble Teacher Model The results for using an ensemble of 6 models as a teacher model are summarized in Table 2. Using only the forward translation improves the single system by 1.4 points in BLEU and 1.9 points in TER. When using both the original reference and the forward translation, we get an additional improvement of 0.3 points in BLEU. When pruning the parallel data and using only sentences with a TER less than 0.8, we can improve the single system by 2 points in BLEU and 2.2 points in TER.\n• Best Sentence BLEU Teacher Model The teacher model is the same ensemble model as before, but instead of choosing the hypotheses with the highest log probability, it chooses the sentence with the highest sentence level BLEU from the final candidate list. The empirical results obtained with the best sentence level BLEU teacher model are summarized in Table 3. By using only the forward translation of the teacher network, we gain improvements of 1.1 points in BLEU and 1.2 points in TER. By combining both the forward translation and the reference, we obtain improvements of 1.5 points in both BLEU and TER. However, the results are slightly worse compared to the results obtained with an ensemble teacher network.\n• Reducing Model Size We use the ensemble teacher network to teach a student network with lower dimensions. Empirical results are given in Table 4. We reduced the original word embedding (Wemb) size of 620 to 150 and the original hidden layer size (hlayer) of 1000 to 300 without losing any translation quality compared to the single model. In fact the performance is even better by 0.4 points in BLEU and 0.6 points in TER."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this work, we applied knowledge distillation for several kinds of teacher networks. First we demonstrate how to benefit from a teacher network that is the same architecture as the student network. By combining both forward translation and the original reference, we get an improvement of 1.4 points in BLEU. Using an ensemble model of 6 single models as teacher model further improves the translation quality of the student network. We showed how to prune the parallel data based on the TER values obtained with the forward translations. The combination of an ensemble teacher network and pruning all sentences with a TER value higher than 0.8 leads us to the best setup which improves the baseline by 2 points in BLEU and 2.2 points in TER. Using a teacher model based on the best sentence level BLEU translations does improve the translation quality, but the results are slightly worse compared to the ensemble teacher model. Furthermore, we showed how to use the teacher ensemble model to significantly\nreduce the size of the student network while still getting gains in translation quality."
    } ],
    "references" : [ {
      "title" : "Do deep nets really need to be deep",
      "author" : [ "Ba", "Caruana2014] Jimmy Ba", "Rich Caruana" ],
      "venue" : null,
      "citeRegEx" : "Ba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2014
    }, {
      "title" : "and Y",
      "author" : [ "D. Bahdanau", "K. Cho" ],
      "venue" : "Bengio.",
      "citeRegEx" : "Bahdanau et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Neural networks for pattern recognition",
      "author" : [ "Christopher M Bishop" ],
      "venue" : null,
      "citeRegEx" : "Bishop.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 1995
    }, {
      "title" : "Christof Monz",
      "author" : [ "Ondrej Bojar", "Rajen Chatterjee", "Christian Federmann", "Yvette Graham", "Barry Haddow", "Matthias Huck", "Antonio Jimeno Yepes", "Philipp Koehn", "Varvara Logacheva" ],
      "venue" : "et al.",
      "citeRegEx" : "Bojar et al.2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Rich Caruana",
      "author" : [ "Cristian Buciluǎ" ],
      "venue" : "and Alexandru Niculescu-Mizil.",
      "citeRegEx" : "Buciluǎ et al.2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Oriol Vinyals",
      "author" : [ "Geoffrey Hinton" ],
      "venue" : "and Jeff Dean.",
      "citeRegEx" : "Hinton et al.2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sequence-level knowledge distillation",
      "author" : [ "Kim", "Rush2016] Yoon Kim", "Alexander M Rush" ],
      "venue" : "arXiv preprint arXiv:1606.07947",
      "citeRegEx" : "Kim et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics",
      "author" : [ "Lin", "Och2004] Chin-Yew Lin", "Franz Josef Och" ],
      "venue" : "In Proceedings of the 42nd Annual Meeting on Association",
      "citeRegEx" : "Lin et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2004
    }, {
      "title" : "Barry Haddow",
      "author" : [ "Rico Sennrich" ],
      "venue" : "and Alexandra Birch.",
      "citeRegEx" : "Sennrich et al.2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Linnea Micciulla",
      "author" : [ "Matthew Snover", "Bonnie Dorr", "Richard Schwartz" ],
      "venue" : "and John Makhoul.",
      "citeRegEx" : "Snover et al.2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Knowledge distillation describes a method for training a student network to perform better by learning from a stronger teacher network. In this work, we run experiments with different kinds of teacher networks to enhance the translation performance of a student Neural Machine Translation (NMT) network. We demonstrate techniques based on an ensemble and a best BLEU teacher network. We also show how to benefit from a teacher network that has the same architecture and dimensions of the student network. Furthermore, we introduce a data filtering technique based on the dissimilarity between the forward translation (obtained during knowledge distillation) of a given source sentence and its target reference. We use TER to measure dissimilarity. Finally, we show that an ensemble teacher model can significantly reduce the student model size while still getting performance improvements compared to the baseline student",
    "creator" : "LaTeX with hyperref package"
  }
}