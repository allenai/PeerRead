{
  "name" : "1505.01749.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Object detection via a multi-region & semantic segmentation-aware CNN model",
    "authors" : [ "Spyros Gidaris", "Nikos Komodakis" ],
    "emails" : [ "gidariss@imagine.enpc.fr", "nikos.komodakis@enpc.fr" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "One of the most studied problems of computer vision is that of object detection: given an image return all the instances of one or more type of objects in form of bounding boxes that tightly enclose them. The last two years, huge improvements have been observed on this task thanks to the recent advances of deep learning community [19, 1, 15]. Among them, most notable is the work of Sermanet et al. [30] with the Overfeat framework and the work of Girshick et al. [10] with the R-CNN framework.\nOverfeat [30] uses two CNN models that applies on a sliding window fashion on multiple scales of an image. The first is used to classify if a window contains an object and the second to predict the true bounding box location of the object. Finally, the dense class and location predictions are merged with a greedy algorithm in order to produce the final set of object detections.\nThis work was supported by the ANR SEMAPOLIS project. Its code will become available on <https://github.com/gidariss/ mrcnn-object-detection>.\nR-CNN [10] uses Alex Krizhevsky’s Net [18] to extract features from box proposals provided by selective search [34] and then classifies them with class specific linear SVMs. They manage to train networks with millions of parameters by first pre-training on the auxiliary task of image classification and then fine-tuning on a small set of images annotated for the detection task. This simple pipeline surpasses by a large margin the detection performance of all the previously published systems, such as deformable parts models [8] or non-linear multi-kernel approaches [35]. Their success comes from the fact that they replaced the hand-engineered features like HOG [3] or SIFT [24] with the high level object representations produced from the last layer of a CNN model. By employing an even deeper CNN model, such as the 16-layers VGG-Net [31], they boosted the performance another 7 points.\nIn this paper we aim to further advance the state-of-theart on object detection by improving on two key aspects that play a critical role in this task: object representation and object localization.\nObject representation. One of the lessons learned from the above-mentioned works is that indeed features matter a lot on object detection and our work is partly motivated from this observation. However, instead of proposing only a network architecture that is deeper, here we also opt for an architecture of greater width, i.e., one whose last hidden layers provide features of increased dimensionality. In doing so, our goal is to build a richer candidate box representation. This is accomplished at two levels:\nar X\niv :1\n50 5.\n01 74\n9v 3\n[ cs\n.C V\n] 2\n3 Se\np 20\n15\n(1). At a first level, we want our object representation to capture several different aspects of an object such as its pure appearance characteristics, the distinct appearance of its different regions (object parts), context appearance, the joint appearance on both sides of the object boundaries, and semantics. We believe that such a rich representation will further facilitate the problem of recognising (even difficult) object instances under a variety of circumstances (like, e.g., those depicted in figure 1). In order to achieve our goal, we propose a multi-component CNN model, called multiregion CNN hereafter, each component of which is steered to focus on a different region of the object thus enforcing diversification of the discriminative appearance factors captured by it.\nAdditionally, as we will explain shortly, by properly choosing and arranging some of these regions, we aim also to help our representation in being less invariant to inaccurate localization of an object. Note that this property, which is highly desirable for detection, contradicts with the builtin invariances of CNN models, which stem from the use of max-pooling layers.\n(2). At a second level, inspired by the close connection that exists between segmentation and detection, we wish to enrich the above representation so that it also captures semantic segmentation information. To that end, we extend the above CNN model such that it also learns novel CNNbased semantic segmentation-aware features. Importantly, learning these features (i.e., training the extended unified CNN model) does not require having ground truth object segmentations as training data.\nObject localization. Besides object representation, our work is also motivated from the observation that, due to the tremendous classification capability of the recent CNN models [18, 37, 31, 17, 14, 32], the bottleneck for good detection performance is now the accurate object localization. Indeed, it was noticed on R-CNN [10] that the most common type of false positives is the mis-localized detections. They fix some of them by employing a post processing step of bounding box regression that they apply on the final list of detections. However, their technique only helps on small localization errors. We believe that there is much more space for improvement on this aspect. In order to prove it, we attempt to built a more powerful localization system that relies on combining our multi-region CNN model with a CNN-model for bounding box regression, which are used within an iterative scheme that alternates between scoring candidate boxes and refining their coordinates.\nContributions. To summarize, our contributions are as follows:\n(1). We develop a multi-region CNN recognition model that yields an enriched object representation capable to capture a diversity of discriminative appearance factors and to exhibit localization sensitivity that is desired for the task of\naccurate object localization. (2). We furthermore extend the above model by proposing a unified neural network architecture that also learns semantic segmentation-aware CNN features for the task of object detection. These features are jointly learnt in a weakly supervised manner, thus requiring no additional annotation.\n(3). We show how to significantly improve the localization capability by coupling the aforementioned CNN recognition model with a CNN model for bounding box regression, adopting a scheme that alternates between scoring candidate boxes and refining their locations, as well as modifying the post-processing step of non-maximumsuppression.\n(4). Our detection system achieves mAP of 78.2% and 73.9% on VOC2007 [6] and VOC2012 [7] detection challenges respectively, thus surpassing the previous state-of-art by a very significant margin.\nThe remainder of the paper is structured as follows: We discuss related work in §2. We describe our multi-region CNN model in §3. We show how to extend it to also learn semantic segmentation-aware CNN features in §4. Our localization scheme is described in §5 and implementation details are provided in §6. We present experimental results in §7, qualitative results in §8 and conclude in §9."
    }, {
      "heading" : "2. Related Work",
      "text" : "Apart from Overfeat [30] and R-CNN [10], several other recent papers are dealing with the object detection problem using deep neural networks. One is the very recent work of Zhu et al. [38], which shares some conceptual similarities with ours. Specifically, they extract features from an additional region in order to capture the contextual appearance of a candidate box, they utilize a MRF inference framework to exploit object segmentation proposals (obtained through parametric min-cuts) in order to improve the object detection accuracy, and also use iterative box regression (based on ridge regression). More than them, we use multiple regions designed to diversify the appearance factors captured by our representation and to improve localization, we exploit CNN-based semantic segmentation-aware features (integrated in a unified neural network architecture), and make use of a deep CNN model for bounding box regression, as well as a box-voting scheme after nonmax-suppression. Feature extraction from multiple regions has also been exploited for performing object recognition in videos by Leordeanu et al. [20]. As features they use the outputs of HOG [3]+SVM classifiers trained on each region separately and the 1000-class predictions of a CNN pretrained on ImageNet. Instead, we fine-tune our deep networks on each region separately in order to accomplish our goal of learning deep features that will adequately capture their discriminative appearance characteristics. Furthermore, our regions exhibit more variety on their shape that,\nas we will see in section 3.1, helps on boosting the detection performance. On [33], they designed a deep CNN model for object proposals generation and they use contextual features extracted from the last hidden layer of a CNN model trained on ImageNet classification task after they have applied it on large crops of the image. On [26], they introduce a deep CNN with a novel deformation constrained pooling layer, a new strategy for pre-training that uses the bounding box annotations provided from ImageNet localization task, and contextual features derived by applying a pre-trained on ImageNet CNN on the whole image and treating the 1000- class probabilities for ImageNet objects as global contextual features. On SPP-Net [13] detection framework, instead of applying their deep CNN on each candidate box separately as R-CNN does, they extract the convolutional feature maps from the whole image, project the candidate boxes on them, and then with an adaptive max-pooling layer, which consists of multiple pooling levels, they produce fixed length feature vectors that they pass through the fully connected layers of the CNN model. Thanks to those modifications, they manage to speed up computation by a considerable factor while maintaining high detection accuracy. Our work adopts this paradigm of processing.\nContemporary to our work are the approaches of [29, 9, 28] that are also based on the SPP-Net framework. On [29], they improve the SPP framework by replacing the subnetwork component that is applied on the convolutional features extracted from the whole image with a deeper convolutional network. On [9], they focus on simplifying the training phase of SPP-Net and R-CNN and speeding up both the testing and the training phases. Also, by fine-tuning the whole network and adopting a multi-task objective that has both box classification loss and box regression loss, they\nmanage to improve the accuracy of their system. Finally, on [28] they extend [9] by adding a new sub-network component for predicting class-independent proposals and thus making the system both faster and independent of object proposal algorithms."
    }, {
      "heading" : "3. Multi-Region CNN Model",
      "text" : "The recognition model that we propose consists of a multi-component CNN network, each component of which is chosen so as to focus on a different region of an object. We call this a Multi-Region CNN model. We begin by describing first its overall architecture. To that end, in order to facilitate the description of our model we introduce a general CNN architecture abstraction that decomposes the computation into two different modules:\nActivation maps module. This part of the network gets as input the entire image and outputs activation maps (feature maps) by forwarding it through a sequence of convolutional layers.\nRegion adaptation module. Given a region R on the image and the activation maps of the image, this module projects R on the activation maps, crops the activations that lay inside it, pools them with a spatially adaptive (max-)pooling layer [13], and then forwards them through a multi-layer network.\nUnder this formalism, the architecture of the MultiRegion CNN model can be seen in figure 2. Initially, the entire image is forwarded through the activation maps module. Then, a candidate detection box B is analysed on a set of (possibly overlapping) regions {Ri}ki=1 each of which is assigned to a dedicated region adaptation module (note\nthat these regions are always defined relatively to the bounding box B). As mentioned previously, each of these region adaptation modules passes the activations pooled from its assigned region through a multilayer network that produces a high level feature. Finally, the candidate box representation is obtained by concatenating the last hidden layer outputs of all the region adaptation modules.\nBy steering the focus on different regions of an object, our aim is: (i) to force the network to capture various complementary aspects of the objects appearance (e.g., context, object parts, etc.), thus leading to a much richer and more robust object representation, and (ii) to also make the resulting representation more sensitive to inaccurate localization (e.g., by focusing on the border regions of an object), which is also crucial for object detection.\nIn the next section we describe how we choose the regions {Ri}ki=1 to achieve the above goals, and also discuss their role on object detection."
    }, {
      "heading" : "3.1. Region components and their role on detection",
      "text" : "We utilize 2 types of region shapes: rectangles and rectangular rings, where the latter type is defined in terms of an inner and outer rectangle. We describe below all of the regions that we employ, while their specifications are given in the caption of figure 3.\nOriginal candidate box: this is the candidate detection box itself as being used on R-CNN [10] (figure 3a). A network trained on this type of region is guided to capture the appearance information of the entire object. When it is used alone consists the baseline of our work.\nHalf boxes: those are the left/right/up/bottom half parts\nof a candidate box (figures 3b, 3c, 3d, and 3e). Networks trained on each of them, are guided to learn the appearance characteristics present only on each half part of an object or on each side of the objects borders, aiming also to make the representation more robust with respect to occlusions.\nCentral Regions: there are two type of central regions in our model (figures 3f and 3g). The networks trained on them are guided to capture the pure appearance characteristics of the central part of an object that is probably less interfered from other objects next to it or its background.\nBorder Regions: we include two such regions, with the shape of rectangular rings (figures 3h and 3i). We expect that the dedicated on them networks will be guided to focus on the joint appearance characteristics on both sides of the object borders, also aiming to make the representation more sensitive to inaccurate localization.\nContextual Region: there is one region of this type that has rectangular ring shape (figure 3j). Its assigned network is driven to focus on the contextual appearance that surrounds an object such as the appearance of its background or of other objects next to it.\nRole on detection. Concerning the general role of the regions on object detection, we briefly focus below on two of the reasons why using these regions helps:\nDiscriminative feature diversification. Our hypothesis is that having regions that render visible to their networkcomponents only a limited part of the object or only its immediate surrounding forces each network-component to discriminate image boxes solely based on the visual information that is apparent on them thus diversifying the discriminative factors captured by our overall recognition\nmodel. For example, if the border region depicted on figure 3i is replaced with one that includes its whole inner content, then we would expect that the network-component dedicated on it will not pay the desired attention on the visual content that is concentrated around the borders of an object. We tested such a hypothesis by conducting an experiment where we trained and tested two Multi-Region CNN models that consist of two regions each. Model A included the original box region (figure 3a) and the border region of figure 3i that does not contain the central part of the object. On model B, we replaced the latter region (figure 3i), which is a rectangular ring, with a normal box of the same size. Both of them were trained on PASCAL VOC2007 [6] trainval set and tested on the test set of the same challenge. Model A achieved 64.1% mAP while Model B achieved 62.9% mAP which is 1.2 points lower and validates our assumption.\nLocalization-aware representation. We argue that our multi-region architecture as well as the type of regions included, address to a certain extent one of the major problems on the detection task, which is the inaccurate object localization. We believe that having multiple regions with network-components dedicated on each of them imposes soft constraints regarding the visual content allowed on each type of region for a given candidate detection box. We experimentally justify this argument by referring to sections\n7.2 and 7.3."
    }, {
      "heading" : "4. Semantic Segmentation-Aware CNN Model",
      "text" : "To further diversify the features encoded by our representation, we extend the Multi-Region CNN model so that it also learns semantic segmentation-aware CNN features. The motivation for this comes from the close connection between segmentation and detection as well as from the fact that segmentation related cues are empirically known to often help object detection [5, 12, 25]. In the context of our multi-region CNN network, the incorporation of the semantic segmentation-aware features is done by adding properly adapted versions of the two main modules of the network, i.e., the ‘activation-maps’ and ‘region-adaptation’ modules (see architecture in figure 4). We hereafter refer to the resulting modules as:\n• Activation maps module for semantic segmentationaware features.\n• Region adaptation module for semantic segmentationaware features.\nIt is important to note that the modules for the semantic segmentation-aware features are trained without the use of any additional annotation. Instead, they are trained in a\nweakly supervised manner using only the provided bounding box annotations for detection.\nWe combine the Multi-Region CNN features and the semantic segmentation aware CNN features by concatenating them (see figure 4). The resulting network thus jointly learns deep features of both types during training."
    }, {
      "heading" : "4.1. Activation maps module for semantic",
      "text" : "segmentation-aware features\nFully Convolutional Nets. In order to serve the purpose of exploiting semantic segmentation aware features, for this module we adopt a Fully Convolutional Network [23], abbreviated hereafter as FCN, trained to predict class specific foreground probabilities (we refer the interested reader to [23] for more details about FCN where it is being used for the task of semantic segmentation).\nWeakly Supervised Training. To train the activation maps module for the class-specific foreground segmentation task, we only use the annotations provided on object detection challenges (so as to make the training of our overall system independent of the availability of segmentation annotations). To that end, we follow a weakly supervised training strategy and we create artificial foreground classspecific segmentation masks using bounding box annotations. More specifically, the ground truth bounding boxes of an image are projected on the spatial domain of the last hidden layer of the FCN, and the ”pixels” that lay inside the projected boxes are labelled as foreground while the rest are labelled as background (see left and middle column in figure 5). The aforementioned process is performed independently for each class and yields as many segmentation target images as the number of our classes. As can be seen in figure 5 right column, despite the weakly supervised way of training, the resulting activations still carry significant semantic segmentation information, enough even to delineate the boundaries of the object and separate the object from its background.\nActivation Maps. After the FCN has been trained on the auxiliary task of foreground segmentation, we drop the last classification layer and we use the rest of the FCN network in order to extract from images semantic segmentation aware activation maps."
    }, {
      "heading" : "4.2. Region adaptation module for semantic segmentation-aware features",
      "text" : "We exploit the above activation maps by treating them as mid-level features and adding on top of them a single region adaptation module trained for our primary task of object detection. In this case, we choose to use a single region obtained by enlarging the candidate detection box by a factor of 1.5 (such a region contains semantic information also from the surrounding of a candidate detection box). The reason that we do not repeat the same regions as in the ini-\ntial Multi-Region CNN architecture is for efficiency as these are already used for capturing the appearance cues of an object."
    }, {
      "heading" : "5. Object Localization",
      "text" : "As already explained, our Multi-Region CNN recognition model exhibits the localization awareness property that is necessary for accurate object localization. However, by itself it is not enough. In order to make full use of it, our recognition model needs to be presented with well localized candidate boxes that in turn will be scored with high confidence from it. The solution that we adopt consists of 3 main components:\nCNN region adaptation module for bounding box regression. We introduce an extra region adaptation module that, instead of being used for object recognition, is trained to predict the object bounding box. It is applied on top of the activation maps produced from the Multi-Region CNN model and, instead of a typical one-layer ridge regression model [10], consists of two hidden fully connected layers and one prediction layer that outputs 4 values (i.e., a bounding box) per category. In order to allow it to predict the location of object instances that are not in the close proximity of any of the initial candidate boxes, we use as region\na box obtained by enlarging the candidate box by a factor of 1.3. This combination offers a significant boost on the detection performance of our system by allowing it to make more accurate predictions and for more distant objects.\nIterative Localization. Our localization scheme starts from the selective search proposals [34] and works by iteratively scoring them and refining their coordinates. Specifically, let Btc = {Bti,c} Nc,t i=1 denote the set of Nc,t bounding boxes generated on iteration t for class c and image X . For each iteration1 t = 1, ..., T , the boxes from the previous iteration Bt−1c are scored with sti,c = Frec(Bt−1i,c |c,X) by our recognition model and refined into Bti,c = Freg(B t−1 i,c |c,X) by our CNN regression model, thus forming the set of candidate detections Dtc = {(sti,c, Bti,c)} Nc,t i=1 . For the first iteration t = 1, the box proposals B0c are coming from selective search [34] and are common between all the classes. Also, those with score s0i,c below a threshold τs are rejected\n2 in order to reduce the computational burden of the subsequent iterations. This way, we obtain a sequence of candidate detection sets {Dtc}Tt=1 that all-together both exhibit high recall of the objects on an image and are well localized on them.\nBounding box voting. After the last iteration T , the candidate detections {Dtc}Tt=1 produced on each iteration t are merged together Dc = ∪Tt=1Dtc. Because of the multiple regression steps, the generated boxes will be highly concentrated around the actual objects of interest. We exploit this ”by-product” of the iterative localization scheme by adding a step of bounding box voting. First, standard non-max suppression [10] is applied on Dc and produces the detections Yc = {(si,c, Bi,c)} using an IoU overlap threshold of 0.3. Then, the final bounding box coordinatesBi,c are further refined by having each box Bj,c ∈ N (Bi,c) (where N (Bi,c) denotes the set of boxes in Dc that overlap with Bi,c by more than 0.5 on IoU metric) to vote for the bounding box location using as weight its score wj,c = max(0, sj,c), or\nB ′\ni,c =\n∑ j:Bj,c∈N(Bi,c)\nwj,c ·Bj,c∑ j:Bj,c∈N(Bi,c) wj,c . (1)\nThe final set of object detections for class c will be Y ′\nc = {(si,c, B ′\ni,c)}. In figure 6 we provide a visual illustration of the object\nlocalization."
    }, {
      "heading" : "6. Implementation Details",
      "text" : "For all the CNN models involved in our proposed system, we used the publicly available 16-layers VGG\n1In practice T =2 iterations were enough for convergence. 2We use τs = −2.1, which was selected such that the average number of box proposals per image from all the classes together to be around 250.\nmodel [31] pre-trained on ImageNet [4] for the task of image classification3. For simplicity, we fine-tuned only the fully connected layers (fc6 and fc7) of each model while we preserved the pre-trained weights for the convolutional layers (conv1 1 to conv5 3), which are shared among all the\n3https://gist.github.com/ksimonyan/\nmodels of our system. Multi-Region CNN model. Its activation maps module consists of the convolutional part (layers conv1 1 to conv5 3) of the 16-layers VGG-Net that outputs 512 feature channels. The max-pooling layer right after the last convolutional layer is omitted on this module. Each region adaptation module inherits the fully connected layers of the 16-layers VGG-Net and is fine-tuned separately from the others. Regarding the regions that are rectangular rings, both the inner and outer box are projected on the activation maps and then the activations that lay inside the inner box are masked out by setting them to zero (similar to the Convolutional Feature Masking layer proposed on [2]). In order to train the region adaptation modules, we follow the guidelines of R-CNN [10]. As an optimization objective we use the softmax-loss and the minimization is performed with stochastic gradient descent (SGD). The momentum is set to 0.9, the learning rate is initially set to 0.001 and then reduced by a factor of 10 every 30k iterations, and the minibatch has 128 samples. The positive samples are defined as the selective search proposals [34] that overlap a groundtruth bounding box by at least 0.5. As negative samples we use the proposals that overlap with a ground-truth bounding box on the range [0.1, 0.5). The labelling of the training samples is relative to the original candidate boxes and is the same across all the different regions.\nActivation maps module for semantic segmentation aware features. Its architecture consists of the 16-layers VGG-Net without the last classification layer and transformed to a FCN [23] (by reshaping the fc6 and fc7 fully connected layers to convolutional ones with kernel size of 7× 7 and 1× 1 correspondingly). For efficiency purposes, we reduce the output channels of the fc7 layer from 4096 to 512. In order to learn the semantic segmentation aware features, we use an auxiliary fc8 convolutional classification layer (of kernel size 1 × 1) that outputs as many channels as our classes and a binary (foreground vs background) logistic loss applied on each spatial cell and for each class independently. Initially, we train the FCN with the 4096 channels on the fc7 layer until convergence. Then, we replace the fc7 layer with another one that has 512 output channels, which is initialized from a Gaussian distribution, and the training of the FCN starts from the beginning and is continued until convergence again. For loss minimization we use SGD with minibatch of size 10. The momentum is set to 0.9 and the learning rate is initialized to 0.01 and decreased by a factor of 10 every 20 epochs. For faster convergence, the learning rate of the randomly initialized fc7 layer with the 512 channels is multiplied by a factor of 10.\nRegion adaptation module for semantic segmentation aware features. Its architecture consists of a spatially adaptive max-pooling layer [23] that outputs feature maps of 512 channels on a 9 × 9 grid, and a fully connected layer with\n2096 channels. In order to train it, we use the same procedure as for the region components of the Multi-Region CNN model. During training, we only learn the weights of the region adaptation module layers that are randomly initialized from a Gaussian distribution.\nClassification SVMs. In order to train the SVMs we follow the same principles as in [10]. As positive samples are considered the ground truth bounding boxes and as negative samples are considered the selective search proposals [34] that overlap with the ground truth boxes by less than 0.3. We use hard negative mining the same way as in [10, 8].\nCNN region adaptation module for bounding box regression. The activation maps module used as input in this case is common with the Multi-Region CNN model. The region adaptation module for bounding box regression inherits the fully connected hidden layers of the 16-layers VGG-Net. As a loss function we use the euclidean distance between the target values and the network predictions. For training samples we use the box proposals [34] that overlap by at least 0.4 with the ground truth bounding boxes. The target values are defined the same way as in R-CNN [10]. The learning rate is initially set to 0.01 and reduced by a factor of 10 every 40k iterations. The momentum is set to 0.9 and the minibatch size is 128.\nMulti-Scale Implementation. In our system we adopt a similar multi-scale implementation as in SPP-Net [13]. More specifically, we apply the activation maps modules of our models on multiple scales of an image and then a single scale is selected for each region adaptation module independently.\n• Multi-Region CNN model: The activation maps module is applied on 7 scales of an image with their shorter dimension being in {480, 576, 688, 874, 1200, 1600, 2100}. For training, the region adaptation modules are applied on a random scale and for testing, a single scale is used such that the area of the scaled region is closest to 224 × 224 pixels. In the case of rectangular ring regions, the scale is selected based on the area of the scaled outer box of the rectangular ring.\n• Semantic Segmentation-Aware CNN model: The activation maps module is applied on 3 scales of an image with their shorter dimension being in {576, 874, 1200}. For training, the region adaptation module is applied on a random scale and for testing, a single scale is selected such that the area of the scaled region is closest to 288× 288 pixels.\n• Bounding Box Regression CNN model: The activation maps module is applied on 7 scales of an image with their shorter dimension being in {480, 576, 688, 874, 1200, 1600, 2100}. Both during\ntraining and testing, a single scale is used such that the area of the scaled region is closest to 224× 224 pixels.\nTraining/Test Time. On a Titan GPU and on PASCAL VOC2007 train+val dataset, the training time of each region adaptation module is approximately 12 hours, of the activation maps module for the semantic segmentation features is approximately 4 days, and of the linear SVM is approximately 16 hours. In order to speed up the above steps, the activation maps (conv5 3 features and the fc7 semantic segmentation aware features) were pre-cashed on a SSD. Finally, the per image runtime is around 30 seconds."
    }, {
      "heading" : "7. Experimental Evaluation",
      "text" : "We evaluate our detection system on PASCAL VOC2007 [6] and on PASCAL VOC2012 [7]. During the presentation of the results, we will use as baseline either the Original candidate box region alone (figure 3a) and/or the R-CNN framework with VGG-Net [31]. We note that, when the Original candidate box region alone is used then the resulted model is a realization of the SPP-Net [13] object detection framework with the 16-layers VGG-Net [31]. Except if otherwise stated, for all the PASCAL VOC2007 results, we trained our models on the trainval set and tested them on the test set of the same year."
    }, {
      "heading" : "7.1. Results on PASCAL VOC2007",
      "text" : "First, we asses the significance of each of the region adaptation modules alone on the object detection task. Results are reported in table 1. As we expected, the best performing component is the Original candidate box. What is surprising is the high detection performance of individual regions like the Border Region on figure 3i 54.8% or the Contextual Region on figure 3j 47.2%. Despite the fact that the area visible by them includes limited or not at all portion of the object, they outperform previous detection systems that were based on hand crafted features. Also interesting, is the high detection performance of the semantic segmentation aware region, 56.6%.\nIn table 2, we report the detection performance of our proposed modules. The Multi-Region CNN model without the semantic segmentation aware CNN features (MR-CNN), achieves 66.2% mAP, which is 4.2 points higher than RCNN with VGG-Net (62.0%) and 4.5 points higher than the Original candidate box region alone (61.7%). Moreover, its detection performance slightly exceeds that of R-CNN with VGG-Net and bounding box regression (66.0%). Extending the Multi-Region CNN model with the semantic segmentation aware CNN features (MR-CNN & S-CNN), boosts the performance of our recognition model another 1.3 points and reaches the total of 67.5% mAP. Comparing to the recently published method of Yuting et al. [36], our MR-CNN & S-CNN model scores 1 point higher than their best per-\nforming method that includes generation of extra box proposals via Bayesian optimization and structured loss during the fine-tuning of the VGG-Net. Significant is also the improvement that we get when we couple our recognition model with the CNN model for bounding box regression under the iterative localization scheme proposed (MR-CNN & S-CNN & Loc.). Specifically, the detection performance is raised from 67.5% to 74.9% setting the new state-of-theart on this test set and for this set of training data (VOC2007 train+val set).\nIn table 3, we report the detection performance of our system when the overlap threshold for considering a detection positive is set to 0.7. This metric was proposed from [36] in order to reveal the localization capability of their method. From the table we observe that each of our modules exhibit very good localization capability, which was our goal when designing them, and our overall system exceeds in that metric the approach of [36]."
    }, {
      "heading" : "7.2. Detection error analysis",
      "text" : "We use the tool of Hoiem et al. [16] to analyse the detection errors of our system. In figure 8, we plot pie charts with the percentage of detections that are false positive due to bad localization, confusion with similar category, confusion with other category, and triggered on the background or an unlabelled object. We use the tool of Hoiem et al. [16] to analyse the detection errors of our system. In figure 8, we plot pie charts with the percentage of detections that are false positive due to bad localization, confusion with similar category, confusion with other category, and triggered on the background or an unlabelled object. We observe that, by using the Multi-Region CNN model instead of the Original Candidate Box region alone, a considerable reduction in the percentage of false positives due to bad localization is achieved. This validates our argument that focusing on multiple regions of an object increases the localization sensitivity of our model. Furthermore, when our recognition model is integrated on the localization module developed for it, the reduction of false positives due to bad localization is huge. A similar observation can be deducted from figure 7 where we plot the top-ranked false positive types of the baseline and of our overall proposed system."
    }, {
      "heading" : "7.3. Localization awareness of Multi-Region CNN",
      "text" : "model\nTwo extra experiments are presented here that indicate the localization awareness of our Multi-Region CNN model without the semantic segmentation aware CNN features (MR-CNN) against the model that uses only the original candidate box (Baseline).\nCorrelation between the scores and the IoU overlap of box proposals. In this experiment, we estimate the correlation between the IoU overlap of box proposals [34] (with the\npe rc\nen ta\nge o\nf e ac\nh ty\npe\nboat\n25 50 100 200 400 800 1600 3200 0\n10\n20\n30\n40\n50\n60\n70\n80\n90 100 Loc Sim Oth BG\npe rc\nen ta\nge o\nf e ac\nh ty\npe\nbottle\n25 50 100 200 400 800 1600 3200 0\n10\n20\n30\n40\n50\n60\n70\n80\n90 100 Loc Sim Oth BG\npe rc\nen ta\nge o\nf e ac\nh ty\npe\nchair\n25 50 100 200 400 800 1600 3200 0\n10\n20\n30\n40\n50\n60\n70\n80\n90 100 Loc Sim Oth BG\npe rc\nen ta\nge o\nf e ac\nh ty\npe\npottedplant\n25 50 100 200 400 800 1600 3200 0\n10\n20\n30\n40\n50\n60\n70\n80\n90 100 Loc Sim Oth BG\nclosest ground truth bounding box) and the score assigned to them from the two examined models. High correlation\ncoefficient means that better localized box proposals will tend to be scored higher than mis-localized ones. We report\nboat\nboat\nboat\nthe correlation coefficients of the aforementioned quantities\nboth for the Baseline and MR-CNN models in table 4. Because with this experiment we want to emphasize on the localization aspect of the Multi-Region CNN model, we use proposals that overlap with the ground truth bounding boxes by at least 0.1 IoU.\nArea-Under-the-Curve of well-localized proposals against mis-localized proposals. The ROC curves are typically used to illustrate the capability of a classifier to distinguish between two classes. This discrimination capability can be measured by computing the Area-Underthe-Curve (AUC) metric. The higher the AUC measure is, the more discriminative is the classifier between the two classes. In our case, the set of well-localized box proposals is the positive class and the set of miss-localized box proposals is the negative class. As well-localized are considered the box proposals that overlap with a ground-truth bounding box in the range [0.5, 1.0] and as mis-localized are considered the box proposals that overlap with a ground truth bounding box in the range [0.1, 0.5). In table 5, we report the AUC measure for each class separately and both for the MR-CNN and the Baseline models."
    }, {
      "heading" : "7.4. Results on PASCAL VOC2012",
      "text" : "In table 6, we compare our detection system against other published work on the test set of PASCAL VOC2012 [7]. Our overall system involves the MultiRegion CNN model enriched with the semantic segmentation aware CNN features and coupled with the CNN based bounding box regression under the iterative localization scheme. We tested two instances of our system. Both of them have exactly the same components but they have being trained on different datasets. For the first one, the finetuning of the networks as well as the training of the detection SVMs was performed on VOC2007 train+val dataset that includes 5011 annotated images. For the second one, the fine-tuning of the networks was performed on VOC2012 train dataset that includes 5717 annotated images and the training of the detection SVMs was performed on VOC2012 train+val dataset that includes 11540 annotated images. As we observe from table 6, we achieve excellent mAP (69.1% and 70.7% correspondingly) in both cases setting the new state-of-the-art on this test set and for those training sets."
    }, {
      "heading" : "7.5. Training with extra data and comparison with",
      "text" : "contemporary work\nApproaches contemporary to ours [29, 9, 28, 27], train their models with extra data in order to improve the accuracy of their systems. We follow the same practice and we report results on tables 7 and 8. Specifically, we trained our models on VOC 2007 and 2012 train+val datasets using both selective search [34] and EdgeBox [39] proposals. During test time we only use EdgeBox proposals that are faster to be computed. From the tables, it is apparent that our methods outperforms the other approaches even when trained with less data. Currently (08/06/15), our entries are ranked 1st and 2nd on the leader board of PASCAL VOC2012 object detection comp4 benchmark (see table 8) and the difference of our top performing entry from the 3rd is 3.5 points."
    }, {
      "heading" : "8. Qualitative Results",
      "text" : "In figures 12 - 14 we present some object detections obtained by our approach. We use blue bounding boxes to mark the true positive detections and red bounding boxes to mark the false positive detections. The ground truth bounding boxes are marked with green color.\nFailure cases. Accurately detecting multiple adjacent object instances remains in many cases a difficult problem even for our approach. In figure 9 we present a few difficult examples of this type. In figure 10 we show some other failure cases.\nMissing annotations. There were also cases of object instances that were correctly detected by our approach but which were not in the ground truth annotation of PASCAL VOC2007. Figure 11 presents a few such examples of nonannotated object instances."
    }, {
      "heading" : "9. Conclusions",
      "text" : "We proposed a powerful CNN-based representation for object detection that relies on two key factors: (i) diversification of the discriminative appearance factors captured by it through steering its focus on different regions of the object, and (ii) the encoding of semantic segmentation-aware features. By using it in the context of a CNN-based localization refinement scheme, we show that it achieves excellent results that surpass the state-of-the art by a significant margin."
    } ],
    "references" : [ {
      "title" : "Greedy layer-wise training of deep networks",
      "author" : [ "Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Convolutional feature masking for joint object and stuff segmentation",
      "author" : [ "J. Dai", "K. He", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1412.1283,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Histograms of oriented gradients for human detection",
      "author" : [ "N. Dalal", "B. Triggs" ],
      "venue" : "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886–893. IEEE,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei" ],
      "venue" : "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Towards unified object detection and semantic segmentation",
      "author" : [ "J. Dong", "Q. Chen", "S. Yan", "A. Yuille" ],
      "venue" : "Computer Vision–ECCV 2014, pages 299–314. Springer,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The pascal visual object classes challenge",
      "author" : [ "M. Everingham", "L. Van Gool", "C. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : "(voc",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "The pascal visual object classes challenge",
      "author" : [ "M. Everingham", "L. Van Gool", "C. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Object detection with discriminatively trained partbased models",
      "author" : [ "P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(9):1627–1645,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Fast r-cnn",
      "author" : [ "R. Girshick" ],
      "venue" : "arXiv preprint arXiv:1504.08083,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 580–587. IEEE,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep CNN ensemble with data augmentation for object detection",
      "author" : [ "J. Guo", "S. Gould" ],
      "venue" : "CoRR, abs/1506.07224,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Simultaneous detection and segmentation",
      "author" : [ "B. Hariharan", "P. Arbeláez", "R. Girshick", "J. Malik" ],
      "venue" : "Computer Vision– ECCV 2014, pages 297–312. Springer,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Spatial pyramid pooling in deep convolutional networks for visual recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1406.4729,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1502.01852,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : "Science, 313(5786):504–507,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Diagnosing error in object detectors",
      "author" : [ "D. Hoiem", "Y. Chodpathumwan", "Q. Dai" ],
      "venue" : "Computer Vision–ECCV 2012, pages 340–353. Springer,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, pages 1097–1105,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Backpropagation applied to handwritten zip code recognition",
      "author" : [ "Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel" ],
      "venue" : "Neural computation, 1(4):541–551,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Features in concert: Discriminative feature selection meets unsupervised clustering",
      "author" : [ "M. Leordeanu", "A. Radu", "R. Sukthankar" ],
      "venue" : "arXiv preprint arXiv:1411.7714,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Network in network",
      "author" : [ "M. Lin", "Q. Chen", "S. Yan" ],
      "venue" : "CoRR, abs/1312.4400,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollár", "C.L. Zitnick" ],
      "venue" : "Computer Vision–ECCV 2014, pages 740–755. Springer,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "J. Long", "E. Shelhamer", "T. Darrell" ],
      "venue" : "arXiv preprint arXiv:1411.4038,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Distinctive image features from scaleinvariant keypoints",
      "author" : [ "D.G. Lowe" ],
      "venue" : "International journal of computer vision, 60(2):91–110,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The role of context for object  detection and semantic segmentation in the wild",
      "author" : [ "R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 891–898. IEEE,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deepid-net: multi-stage and deformable deep convolutional neural networks for object detection",
      "author" : [ "W. Ouyang", "P. Luo", "X. Zeng", "S. Qiu", "Y. Tian", "H. Li", "S. Yang", "Z. Wang", "Y. Xiong", "C. Qian" ],
      "venue" : "arXiv preprint arXiv:1409.3505,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "You only look once: Unified, real-time object detection",
      "author" : [ "J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi" ],
      "venue" : "arXiv preprint arXiv:1506.02640,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "S. Ren", "K. He", "R. Girshick", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1506.01497,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Object detection networks on convolutional feature maps",
      "author" : [ "S. Ren", "K. He", "R. Girshick", "X. Zhang", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1504.06066,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1312.6229,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "arXiv preprint arXiv:1409.4842,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Scalable, high-quality object detection",
      "author" : [ "C. Szegedy", "S. Reed", "D. Erhan", "D. Anguelov" ],
      "venue" : "arXiv preprint arXiv:1412.1441,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Segmentation as selective search for object recognition",
      "author" : [ "K.E. Van de Sande", "J.R. Uijlings", "T. Gevers", "A.W. Smeulders" ],
      "venue" : "In Computer Vision (ICCV),",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2011
    }, {
      "title" : "Multiple kernels for object detection",
      "author" : [ "A. Vedaldi", "V. Gulshan", "M. Varma", "A. Zisserman" ],
      "venue" : "Computer Vision, 2009 IEEE 12th International Conference on, pages 606–613. IEEE,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Improving object detection with deep convolutional networks via bayesian optimization and structured prediction",
      "author" : [ "Z. Yuting", "S. Kihyuk", "V. Ruben", "P. Gang", "H. Lee" ],
      "venue" : "arXiv preprint arXiv:1504.03293,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "Computer Vision–ECCV 2014, pages 818–833. Springer,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "segdeepm: Exploiting segmentation and context in deep neural networks for object detection",
      "author" : [ "Y. Zhu", "R. Urtasun", "R. Salakhutdinov", "S. Fidler" ],
      "venue" : "arXiv preprint arXiv:1502.04275,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Edge boxes: Locating object proposals from edges",
      "author" : [ "C.L. Zitnick", "P. Dollár" ],
      "venue" : "Computer Vision–ECCV 2014, pages 391–405. Springer,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "The last two years, huge improvements have been observed on this task thanks to the recent advances of deep learning community [19, 1, 15].",
      "startOffset" : 127,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "The last two years, huge improvements have been observed on this task thanks to the recent advances of deep learning community [19, 1, 15].",
      "startOffset" : 127,
      "endOffset" : 138
    }, {
      "referenceID" : 14,
      "context" : "The last two years, huge improvements have been observed on this task thanks to the recent advances of deep learning community [19, 1, 15].",
      "startOffset" : 127,
      "endOffset" : 138
    }, {
      "referenceID" : 29,
      "context" : "[30] with the Overfeat framework and the work of Girshick et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[10] with the R-CNN framework.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "Overfeat [30] uses two CNN models that applies on a sliding window fashion on multiple scales of an image.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 9,
      "context" : "R-CNN [10] uses Alex Krizhevsky’s Net [18] to extract features from box proposals provided by selective search [34] and then classifies them with class specific linear SVMs.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 17,
      "context" : "R-CNN [10] uses Alex Krizhevsky’s Net [18] to extract features from box proposals provided by selective search [34] and then classifies them with class specific linear SVMs.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 33,
      "context" : "R-CNN [10] uses Alex Krizhevsky’s Net [18] to extract features from box proposals provided by selective search [34] and then classifies them with class specific linear SVMs.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "This simple pipeline surpasses by a large margin the detection performance of all the previously published systems, such as deformable parts models [8] or non-linear multi-kernel approaches [35].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 34,
      "context" : "This simple pipeline surpasses by a large margin the detection performance of all the previously published systems, such as deformable parts models [8] or non-linear multi-kernel approaches [35].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 2,
      "context" : "Their success comes from the fact that they replaced the hand-engineered features like HOG [3] or SIFT [24] with the high level object representations produced from the last layer of a CNN model.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 23,
      "context" : "Their success comes from the fact that they replaced the hand-engineered features like HOG [3] or SIFT [24] with the high level object representations produced from the last layer of a CNN model.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 30,
      "context" : "By employing an even deeper CNN model, such as the 16-layers VGG-Net [31], they boosted the performance another 7 points.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : "Besides object representation, our work is also motivated from the observation that, due to the tremendous classification capability of the recent CNN models [18, 37, 31, 17, 14, 32], the bottleneck for good detection performance is now the accurate object localization.",
      "startOffset" : 158,
      "endOffset" : 182
    }, {
      "referenceID" : 36,
      "context" : "Besides object representation, our work is also motivated from the observation that, due to the tremendous classification capability of the recent CNN models [18, 37, 31, 17, 14, 32], the bottleneck for good detection performance is now the accurate object localization.",
      "startOffset" : 158,
      "endOffset" : 182
    }, {
      "referenceID" : 30,
      "context" : "Besides object representation, our work is also motivated from the observation that, due to the tremendous classification capability of the recent CNN models [18, 37, 31, 17, 14, 32], the bottleneck for good detection performance is now the accurate object localization.",
      "startOffset" : 158,
      "endOffset" : 182
    }, {
      "referenceID" : 16,
      "context" : "Besides object representation, our work is also motivated from the observation that, due to the tremendous classification capability of the recent CNN models [18, 37, 31, 17, 14, 32], the bottleneck for good detection performance is now the accurate object localization.",
      "startOffset" : 158,
      "endOffset" : 182
    }, {
      "referenceID" : 13,
      "context" : "Besides object representation, our work is also motivated from the observation that, due to the tremendous classification capability of the recent CNN models [18, 37, 31, 17, 14, 32], the bottleneck for good detection performance is now the accurate object localization.",
      "startOffset" : 158,
      "endOffset" : 182
    }, {
      "referenceID" : 31,
      "context" : "Besides object representation, our work is also motivated from the observation that, due to the tremendous classification capability of the recent CNN models [18, 37, 31, 17, 14, 32], the bottleneck for good detection performance is now the accurate object localization.",
      "startOffset" : 158,
      "endOffset" : 182
    }, {
      "referenceID" : 9,
      "context" : "Indeed, it was noticed on R-CNN [10] that the most common type of false positives is the mis-localized detections.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "9% on VOC2007 [6] and VOC2012 [7] detection challenges respectively, thus surpassing the previous state-of-art by a very significant margin.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 6,
      "context" : "9% on VOC2007 [6] and VOC2012 [7] detection challenges respectively, thus surpassing the previous state-of-art by a very significant margin.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 29,
      "context" : "Apart from Overfeat [30] and R-CNN [10], several other recent papers are dealing with the object detection problem using deep neural networks.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 9,
      "context" : "Apart from Overfeat [30] and R-CNN [10], several other recent papers are dealing with the object detection problem using deep neural networks.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 37,
      "context" : "[38], which shares some conceptual similarities with ours.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "As features they use the outputs of HOG [3]+SVM classifiers trained on each region separately and the 1000-class predictions of a CNN pretrained on ImageNet.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 12,
      "context" : "An “adaptive max pooling” layer uses spatially adaptive pooling as in [13] (but with a one-level pyramid).",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 32,
      "context" : "On [33], they designed a deep CNN model for object proposals generation and they use contextual features extracted from the last hidden layer of a CNN model trained on ImageNet classification task after they have applied it on large crops of the image.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 25,
      "context" : "On [26], they introduce a deep CNN with a novel deformation constrained pooling layer, a new strategy for pre-training that uses the bounding box annotations provided from ImageNet localization task, and contextual features derived by applying a pre-trained on ImageNet CNN on the whole image and treating the 1000class probabilities for ImageNet objects as global contextual features.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 12,
      "context" : "On SPP-Net [13] detection framework, instead of applying their deep CNN on each candidate box separately as R-CNN does, they extract the convolutional feature maps from the whole image, project the candidate boxes on them, and then with an adaptive max-pooling layer, which consists of multiple pooling levels, they produce fixed length feature vectors that they pass through the fully connected layers of the CNN model.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 28,
      "context" : "Contemporary to our work are the approaches of [29, 9, 28] that are also based on the SPP-Net framework.",
      "startOffset" : 47,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "Contemporary to our work are the approaches of [29, 9, 28] that are also based on the SPP-Net framework.",
      "startOffset" : 47,
      "endOffset" : 58
    }, {
      "referenceID" : 27,
      "context" : "Contemporary to our work are the approaches of [29, 9, 28] that are also based on the SPP-Net framework.",
      "startOffset" : 47,
      "endOffset" : 58
    }, {
      "referenceID" : 28,
      "context" : "On [29], they improve the SPP framework by replacing the subnetwork component that is applied on the convolutional features extracted from the whole image with a deeper convolutional network.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 8,
      "context" : "On [9], they focus on simplifying the training phase of SPP-Net and R-CNN and speeding up both the testing and the training phases.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 27,
      "context" : "Finally, on [28] they extend [9] by adding a new sub-network component for predicting class-independent proposals and thus making the system both faster and independent of object proposal algorithms.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "Finally, on [28] they extend [9] by adding a new sub-network component for predicting class-independent proposals and thus making the system both faster and independent of object proposal algorithms.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "Given a region R on the image and the activation maps of the image, this module projects R on the activation maps, crops the activations that lay inside it, pools them with a spatially adaptive (max-)pooling layer [13], and then forwards them through a multi-layer network.",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 9,
      "context" : "Region a: it is the candidate box itself as being used on R-CNN [10].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "Original candidate box: this is the candidate detection box itself as being used on R-CNN [10] (figure 3a).",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "Both of them were trained on PASCAL VOC2007 [6] trainval set and tested on the test set of the same challenge.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : "The motivation for this comes from the close connection between segmentation and detection as well as from the fact that segmentation related cues are empirically known to often help object detection [5, 12, 25].",
      "startOffset" : 200,
      "endOffset" : 211
    }, {
      "referenceID" : 11,
      "context" : "The motivation for this comes from the close connection between segmentation and detection as well as from the fact that segmentation related cues are empirically known to often help object detection [5, 12, 25].",
      "startOffset" : 200,
      "endOffset" : 211
    }, {
      "referenceID" : 24,
      "context" : "The motivation for this comes from the close connection between segmentation and detection as well as from the fact that segmentation related cues are empirically known to often help object detection [5, 12, 25].",
      "startOffset" : 200,
      "endOffset" : 211
    }, {
      "referenceID" : 22,
      "context" : "In order to serve the purpose of exploiting semantic segmentation aware features, for this module we adopt a Fully Convolutional Network [23], abbreviated hereafter as FCN, trained to predict class specific foreground probabilities (we refer the interested reader to [23] for more details about FCN where it is being used for the task of semantic segmentation).",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 22,
      "context" : "In order to serve the purpose of exploiting semantic segmentation aware features, for this module we adopt a Fully Convolutional Network [23], abbreviated hereafter as FCN, trained to predict class specific foreground probabilities (we refer the interested reader to [23] for more details about FCN where it is being used for the task of semantic segmentation).",
      "startOffset" : 267,
      "endOffset" : 271
    }, {
      "referenceID" : 22,
      "context" : "The reason that we do not repeat the same regions as in the iniFigure 5: Illustration of the weakly supervised training of the FCN [23] used as activation maps module for the semantic segmentation aware CNN features.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "It is applied on top of the activation maps produced from the Multi-Region CNN model and, instead of a typical one-layer ridge regression model [10], consists of two hidden fully connected layers and one prediction layer that outputs 4 values (i.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 33,
      "context" : "Our localization scheme starts from the selective search proposals [34] and works by iteratively scoring them and refining their coordinates.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 33,
      "context" : "For the first iteration t = 1, the box proposals Bc are coming from selective search [34] and are common between all the classes.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "First, standard non-max suppression [10] is applied on Dc and produces the detections Yc = {(si,c, Bi,c)} using an IoU overlap threshold of 0.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 30,
      "context" : "model [31] pre-trained on ImageNet [4] for the task of image classification3.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 3,
      "context" : "model [31] pre-trained on ImageNet [4] for the task of image classification3.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "Regarding the regions that are rectangular rings, both the inner and outer box are projected on the activation maps and then the activations that lay inside the inner box are masked out by setting them to zero (similar to the Convolutional Feature Masking layer proposed on [2]).",
      "startOffset" : 274,
      "endOffset" : 277
    }, {
      "referenceID" : 9,
      "context" : "In order to train the region adaptation modules, we follow the guidelines of R-CNN [10].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 33,
      "context" : "The positive samples are defined as the selective search proposals [34] that overlap a groundtruth bounding box by at least 0.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : "Its architecture consists of the 16-layers VGG-Net without the last classification layer and transformed to a FCN [23] (by reshaping the fc6 and fc7 fully connected layers to convolutional ones with kernel size of 7× 7 and 1× 1 correspondingly).",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : "Its architecture consists of a spatially adaptive max-pooling layer [23] that outputs feature maps of 512 channels on a 9 × 9 grid, and a fully connected layer with 2096 channels.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "In order to train the SVMs we follow the same principles as in [10].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 33,
      "context" : "As positive samples are considered the ground truth bounding boxes and as negative samples are considered the selective search proposals [34] that overlap with the ground truth boxes by less than 0.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "We use hard negative mining the same way as in [10, 8].",
      "startOffset" : 47,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "We use hard negative mining the same way as in [10, 8].",
      "startOffset" : 47,
      "endOffset" : 54
    }, {
      "referenceID" : 33,
      "context" : "For training samples we use the box proposals [34] that overlap by at least 0.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 9,
      "context" : "The target values are defined the same way as in R-CNN [10].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "In our system we adopt a similar multi-scale implementation as in SPP-Net [13].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "We evaluate our detection system on PASCAL VOC2007 [6] and on PASCAL VOC2012 [7].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "We evaluate our detection system on PASCAL VOC2007 [6] and on PASCAL VOC2012 [7].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 30,
      "context" : "During the presentation of the results, we will use as baseline either the Original candidate box region alone (figure 3a) and/or the R-CNN framework with VGG-Net [31].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "We note that, when the Original candidate box region alone is used then the resulted model is a realization of the SPP-Net [13] object detection framework with the 16-layers VGG-Net [31].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 30,
      "context" : "We note that, when the Original candidate box region alone is used then the resulted model is a realization of the SPP-Net [13] object detection framework with the 16-layers VGG-Net [31].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 35,
      "context" : "[36], our MR-CNN & S-CNN model scores 1 point higher than their best performing method that includes generation of extra box proposals via Bayesian optimization and structured loss during the fine-tuning of the VGG-Net.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "This metric was proposed from [36] in order to reveal the localization capability of their method.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 35,
      "context" : "From the table we observe that each of our modules exhibit very good localization capability, which was our goal when designing them, and our overall system exceeds in that metric the approach of [36].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 15,
      "context" : "[16] to analyse the detection errors of our system.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] to analyse the detection errors of our system.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "In this experiment, we estimate the correlation between the IoU overlap of box proposals [34] (with the",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 35,
      "context" : "Best approach of [36] 0.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 35,
      "context" : "Best approach of [36] & bbox reg.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 35,
      "context" : "R-CNN with VGG-Net from [36] 0.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 35,
      "context" : "Best approach of [36] 0.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 35,
      "context" : "Best approach of [36] & bbox reg.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 33,
      "context" : "Table 4: Correlation between the IoU overlap of selective search box proposals [34] (with the closest ground truth bounding box) and the scores assigned to them.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "In table 6, we compare our detection system against other published work on the test set of PASCAL VOC2012 [7].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 9,
      "context" : "R-CNN [10] with VGG-Net & bbox reg.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 20,
      "context" : "Network In Network [21] VOC12 0.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 35,
      "context" : "Best approach of [36] & bbox reg.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 27,
      "context" : "Faster R-CNN [28] VOC07+12 0.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 28,
      "context" : "NoC [29] VOC07+12 0.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 8,
      "context" : "Fast R-CNN [9] VOC07+12 0.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 27,
      "context" : "Faster R-CNN [28] VOC07+12 0.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 26,
      "context" : "Fast R-CNN & YOLO [27] VOC07+12 0.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 10,
      "context" : "Deep Ensemble COCO [11] VOC07+12, COCO [22] 0.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 21,
      "context" : "Deep Ensemble COCO [11] VOC07+12, COCO [22] 0.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 28,
      "context" : "NoC [29] VOC07+12 0.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 8,
      "context" : "Fast R-CNN [9] VOC07+12 0.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 28,
      "context" : "Approaches contemporary to ours [29, 9, 28, 27], train their models with extra data in order to improve the accuracy of their systems.",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "Approaches contemporary to ours [29, 9, 28, 27], train their models with extra data in order to improve the accuracy of their systems.",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 27,
      "context" : "Approaches contemporary to ours [29, 9, 28, 27], train their models with extra data in order to improve the accuracy of their systems.",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 26,
      "context" : "Approaches contemporary to ours [29, 9, 28, 27], train their models with extra data in order to improve the accuracy of their systems.",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 33,
      "context" : "Specifically, we trained our models on VOC 2007 and 2012 train+val datasets using both selective search [34] and EdgeBox [39] proposals.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 38,
      "context" : "Specifically, we trained our models on VOC 2007 and 2012 train+val datasets using both selective search [34] and EdgeBox [39] proposals.",
      "startOffset" : 121,
      "endOffset" : 125
    } ],
    "year" : 2015,
    "abstractText" : "We propose an object detection system that relies on a multi-region deep convolutional neural network (CNN) that also encodes semantic segmentation-aware features. The resulting CNN-based representation aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. We exploit the above properties of our recognition module by integrating it on an iterative localization mechanism that alternates between scoring a box proposal and refining its location with a deep CNN regression model. Thanks to the efficient use of our modules, we detect objects with very high localization accuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we achieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published work by a significant margin.",
    "creator" : "LaTeX with hyperref package"
  }
}