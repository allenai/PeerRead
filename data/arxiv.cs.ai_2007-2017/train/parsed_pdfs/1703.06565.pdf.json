{
  "name" : "1703.06565.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Evidence Updating for Stream-Processing in Big-Data: Robust Conditioning in Soft and Hard Data Fusion Environments",
    "authors" : [ "Thanuka Wickramarathne" ],
    "emails" : [ "thanuka@uml.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nOverview. It’s a streaming world—from financial markets to transportation to health monitoring to e-commerce applications, most of today’s data are generated and received in real-time as streams [1]. With real-time processing bearing the promise of improved efficiency and creating new opportunities many application domains, stream processing [2]–[4] has become the latest trend in the big-data world. The ability to predict future system states from real-time data streams while automatically accounting for data distribution drifts via a ‘single-pass’ processing of data [5] is a critical step in developing robust streaming processing methods for reasoning upon rapidly changing information. In particular, it is often required to ‘refine’ existing knowledge (e.g., about a state of the system), commonly referred to as evidence updating (or belief revision) as new evidence is generated [6]. To preserve the integrity of data fusion [7], adequately accounting for numerous uncertainties [8] is paramount, especially in big-data environments where soft (i.e., human or human-based) sources are frequently utilized in addition to hard (i.e., physicsbased) sensors. Among many uncertainty modeling/handling methodologies, notions of probability are very much likely\nto play a major role in big-data approaches as (a) data are intrinsically probabilistic in nature and (b) ability of probabilistic approaches in reducing the size of the input data that is needed to be processed (by each machine) through randomization techniques. In this paper, we present a new evidence updating scheme that is derived as a natural extension of Bayes conditioning, the primary belief revision mechanism utilized in probability theory [9]–[14], to tackle the challenges associated with belief revision in such big-data environments.\nBackground. Conditioning [15]–[17] is the primary method for belief revision in a vast majority of data fusion [7], [18], [19] systems which employ probabilistic inferencing [20]– [28]. As new evidence becomes available, existing belief of the propositions of interest (e.g., state of a sensor or agent, knowledge-base (KB), etc.) are updated (or revised) to reflect the new evidence. In probability theory, the Bayes conditional accomplishes exactly this task. Let us consider the following example to illustrate the classical belief revision process.\nExample 1 (MVP Poll). Consider a sports news agency that maintains a KB of current voter preferences for the top five candidates Θ ≡ {c1, c2, c3, c4, c5} for the most valuable player (MVP) award of a basketball league. The agency maintains an up-to-date KB via a probability mass function (pmf) Pk( ), where k is the discrete time index for current time tk. The agency generates Pk( ) by aggregating predictions generated by various sports analysts. Suppose the players c1 and c2 get injured early in the season. Now, the agency concludes with 100% certainty that the two players will not play for the remainder of the season. Therefore, the KB is updated to reflect these changes by conditioning Pk( ) with respect to (w.r.t.) the conditioning event A = Θ \\ {c1, c2} = {c3, c4, c5} thus yielding\nPk+1(ci) = Pk(ci | A) = Pk(ci ∩A) Pk(A) , i = 1, . . . , 5, (1)\nwhere Pk+1( ) denotes the updated voter preference.\nEvidence updating in fusion engines that utilize probabilistic inferencing is often carried out in a similar fashion to Example 1, where existing beliefs are conditioned w.r.t. an event A that characterizes the changes to existing “conditions.” However, it is often difficult to characterize such changes via a\nar X\niv :1\n70 3.\n06 56\n5v 2\n[ cs\n.A I]\n1 0\nJu n\n20 17\nsingle event in complex sensing and fusion situations that are characteristic of big-data environments [29], especially when non-traditional sources of evidence (e.g., soft data in the forms of witness reports, expert opinions, blogs, etc.) are also being used for gathering evidence [30]. The evidence provided by such sources are often complex data structures that are difficult to represent via a single event with certainty or even via a probabilistic model. For instance, given the injuries to players c1 and c2 in Example 1, a sports analyst A∗ may very well conclude that “with a 75% confidence, c1 and c2 will not return and in that case c3 will most likely be the MVP” (see Example 2).\nChallenges. The difficulties associated with evidence updating in such complex environments is rooted in the types of data/source imperfections that we may encounter. Given the very nature of big-data (viz., subjective, qualitative, and unstructured), one may be reluctant to revise existing beliefs with complete certainty (w.r.t. a single conditioning event). Furthermore, one must also take into account the differences in data sources, such as the reliability of the evidence source, credibility of the evidence, and frame of discernment (FoD) (i.e., the scope of source expertise). Purely probabilistic methods may pose several challenges in terms of adequately representing complex data uncertainties (that are characteristic of such data) in order to generate inferences that are robust against such data/source imperfections. Given the fact that vast majority of existing hard data (i.e., data generated by conventional physics-based sensors) fusion and tracking systems are based on probabilistic methods, a complete transition to a new framework, which does not support probabilistic inferencing, may also not be a feasible option.\nContributions. By viewing the evidence updating process as a thought experiment, we devise an elegant strategy for robust evidence updating in the presence of extreme uncertainties. With a belief theoretic [31] core, our proposed method generalizes the belief theoretic Fagin-Halpern conditional notions [15], thus allowing one to account for various data/source imperfections that are characteristic to complex soft/hard data fusion environments. Furthermore, a novel data fusion rule is derived as a natural extension of these ideas. The presented\nextension differs fundamentally from the previously published work on Conditional Update Equation (CUE) as appeared in [32], [33], including the authors own work in [8], [34], [35]. However, it can be thought of as an extension of [36]. In this paper, we provide an overview for the development of generalized conditional approach via illustrative examples. Then, we derive several algebraic and fusion properties of the new approach and compare them to the properties of CUE. Moreover, we provide insights into where each fusion rule may apply and also provide insights for parameter selection under various fusion contexts."
    }, {
      "heading" : "II. PRELIMINARIES",
      "text" : "a) Basic Notions: Let Θ ≡ {θ1, . . . , θn} denote the total set of mutually exclusive and exhaustive propositions. In DS theory, Θ is referred to as Frame of Discernment (FoD), where a proposition θi referred to as a singleton represents the lowest level of discernible information. We use |Θ| and 2Θ to denote the cardinality and the power set of Θ, respectively. Elements in 2Θ form all the propositions of interest in DS theory. When the FoD is clear from the context, we use B to denote all singletons in Θ that are not included in B; otherwise, we indicate these singletons via Θ\\B. The ‘support’ for proposition B is provided via a basic probability assignment (BPA) or mass assignment:\nDefinition 1 (BPA or Mass Assignment). The mapping m : 2Θ 7→ [0, 1] is a BPA for the FoD Θ if m(∅) = 0 and∑ B⊆Θm(B) = 1.\nA proposition receiving a positive BPA is referred to as a focal element; the set of focal elements is the core F; the triple {Θ,F ,m} is the corresponding body of evidence (BoE). The mass assigned to a proposition is free to move into the individual singleton objects that constitute the composite proposition thus generating the notion of ignorance. Indeed, complete lack of evidence to discern among the propositions can be conveniently captured via the vacuous BoE where the FoD Θ itself is the only focal element (i.e., m(Θ) = 1).\nb) Belief and Plausibility: While m(B) measures the support assigned to proposition B only, the belief assigned to B takes into account the supports for all proper subsets of B as well. In other words, Bl(B) represents the total support that can move into B without any ambiguity; and Pl(B) represents the extent to which one finds B plausible. Definition 2 (Belief and Plausibility). For B ⊆ Θ in the BoE {Θ,F ,m}, Bl : 2Θ 7→ [0, 1] where Bl(B) = ∑C⊆Bm(C) is the belief of B; and Pl : 2Θ 7→ [0, 1] where Pl(B) = 1−Bl(B) is the plausibility of B.\nWhen each focal set contains only one element, i.e., m(B) = 0, ∀|B| 6= 1, belief functions become probability functions. In such a case, the BPA, belief and plausibility all reduce to probability. A probability distribution Pr(·) such that Bl(B) ≤ Pr(B) ≤ Pl(B), ∀B ⊆ Θ, is said to be compatible with the underlying BPA m(·). An example of\nsuch a probability distribution is the pignistic probability distribution BetP(·) [37]\nBetP(θi) = ∑\nθi∈B⊆Θ\nm(B)\n|B| . (2)\nc) Conditional Notions: Let F̂ denote the set of propositions with non-zero belief, i.e., F̂ = {B ⊆ Θ | Bl(B) > 0}. The Fagin-Halpern (FH) conditional notions in DS theory [15] are applicable whenever the conditioning proposition A belongs to F̂.\nTheorem 1 (Fagin-Halpern (FH) Conditionals). [15] For the conditioning event A ∈ F̂ and B ⊂ Θ in the BoE E = {Θ,F,m}, the conditional belief Bl(B|A) : 2Θ 7→ [0, 1] and the conditional plausibility Pl(B|A) : 2Θ 7→ [0, 1] of B given A are\nBl(B|A) = Bl(A ∩B) Bl(A ∩B) + Pl(A ∩B) ; Pl(B|A) = Pl(A ∩B) Pl(A ∩B) + Bl(A ∩B) ,\nrespectively.\nA proposition with positive mass after conditioning is referred to as a conditional focal element. The collection of conditional focal elements that are generated with respect to the conditioning event A is referred to as the conditional core and denoted by FΘ|A. Thus FΘ|A = {B ⊆ Θ | m(B|A) > 0}, where A ∈ F̂ and m(·|A) : 2Θ 7→ [0, 1] is the corresponding conditional BPA related to Bl(·|A) via the Möbius transformation [31]\nm(B|A) = ∑ C⊆B (−1)|B−C| Bl(C|A), ∀B ⊆ Θ. (3)\nd) Evidence Updating: This refers to the process of updating the evidence in a BoE Ek with evidence received from another BoE E∗k , to arrive at Ek+1. Here k denote the discrete update index. We denote this as Ek+1 ≡ Ek C E∗k . Definition 3 (Conditional Update Equation (CUE)). [33] The CUE that updates Ek with the evidence in E∗k is\nBlk+1(B) = αk Blk(B) + ∑ A∈F∗k βk(A)Bl ∗ k(B|A),\nwhere the parameters αk, βk( ) ∈ <+ satisfy αk +∑ A∈F∗k βk(A) = 1.\nThe CUE proposed in [33] provides several interesting properties applicable to the task at hand. However, the updating mechanism presented in this paper fundamentally differers from CUE and previous work of the author. In fact, we provide a comprehensive discussion on these differences and how it affects the applicability and choice of parameters."
    }, {
      "heading" : "III. CONDITIONING IN SOFT/HARD FUSION ENVIRONMENTS",
      "text" : "As now being frequently harnessed in many big-data environments, soft data (i.e., human or human-generated) data plays a crucial role in inferencing tasks primarily due to their ability to provide complementary (to hard sources) and both critical and time-sensitive information. However, given the imperfect nature (subjective, incomplete, unstructured, inconsistent, contradictory, etc.) of these data/sources, one may not wish to sacrifice the integrity of the inferencing task by simply conditioning an existing KB w.r.t. soft evidence. Here, via a thought experiment, we develop a generalized conditioning operation as a direct extension of Bayes conditioning operation to account for these challenges.\nA. The Case of One Conditioning Event\nLet us look at a case where the new evidence comes in the form of an occurrence of one event, but not necessarily with 100% certainty as in the traditional Bayesian conditioning.\nExample 2 (MVP Poll v.2). Suppose the sports agency in Example 1 now maintains voter preferences via DS BoE Ek ≡ {Θ,Fk,mk( )} and is interested in updating its KB using the incoming evidence E∗k from a regional “star” sports analyst A∗. When c1 and c2 gets injured, A∗ has now inferred that the two players will not return with 75% confidence. Now, what is the best strategy to update Ek w.r.t. to recent changes?\nConditioning scenario in Example 2 is clearly different and rather complicated from that of Example 1, where the existing KB was simply conditioned w.r.t. and event that characterizes the changes. Furthermore, the problem at hand is also not an evidence combination, since the agency is only interested in updating its voter preference KB using the newly acquired evidence from A∗. This is a typical scenario in a soft/hard fusion network, where a node maybe interested in simply updating its existing state (or beliefs) by “eavesdropping” to evidence that is being relayed through it.\na) The Probabilistic Case: For illustration purposed, let us assume that the current voter preferences are stored in KB as Pk( ) as in Example 1. In this case, when a conditioning event, such as A = Θ \\ {c1, c2} = {c3, c4, c5} (i.e., c1 and c2 are no longer in the running) is specified with 100% certainty, one can interpret this as the original FoD being deflated (i.e., the # of available candidates are being reduced) to (c3, c4, c5). Therefore, the conditioning operation, redistributes originally cast voter preferences, as given by\nPk+1(ci) = Pk(ci | A) = Pk(ci ∩A) Pk(A) , i = 1, . . . , 5. (4)\nHere, note that conditioning simply normalizes (such that they sum to 1) the originally cast voter preferences to propositions c3, c4 and c5, as Pk(c1 ∩A) = Pk(c2 ∩A) = 0.\nNow, if the event A = Θ \\ {c1, c2} = {c3, c4, c5} only has 75% confidence (or certainty) associated with it, how can one update the KB? While a philosophical discussion on generating the most precise interpretation of “... c1 and c2 will\nnot return with 75% confidence” is out of the scope of this paper, it is clear from the context that c1 and c2 will not return with 75% confidence does not necessarily mean that they they will return with 25%. Then, as proposed in [36] for updating in belief functions, one may employ a linear combination to generate the updated KB:\nPk+1(ci) = αkPk(ci) + βk(A)Pk(ci | A), i = 1, . . . , 5, (5) where αk+βk(A) = 1. Here, one may utilize βk(A), perhaps as βk(A) = 0.75 to quantify the confidence on new evidence assigned by the agent A∗. One may interpret accordingly and use the parameter α to account for the integrity of existing KB.\nb) Belief theoretic update: Similar to the probabilistic case, When a conditioning event, such as A = Θ \\ {c1, c2} = {c3, c4, c5} (i.e., c1 and c2 are no longer in the running) is specified with 100% certainty, one can interpret this as the original FoD being deflated (i.e., the # of available candidates are being reduced) to (c3, c4, c5). Therefore, we can generate updated KB via FH Conditional as\nBlk+1(B) = Blk(B | A), (6) where B ⊆ Θ. Similar to the probabilistic case, the conditioning operation, redistributes originally cast voter preferences to ONLY propositions that are subsets of A. Now, when A is specified with less than 100% confidence as in Example 2, one may utilize updating strategy similar to [36] to obtain,\nBlk+1(B) = αk Blk(B) + βk(A) Blk(B | A), (7) where the parameters αk and βk(A) are chosen to reflect the confidence levels. In particular, for Example 2, one may choose βk(A) = 0.75 and αk = 1 − βk(A) = 0.25. An interpretation of αk is that it represents the integrity of current KB (i.e., Blk( )) in front of not 100% evidence. In fact, depending on the maturity of the KB, one may choose an αk not fully committing to incoming evidence (see [36] for a detailed discussion). For instance, even if A∗ is 95% confident in his assessment on c1 and c2, if the knowledge in existing KB has much higher integrity, one may choose a higher αk, say 0.80, thus only allowing small changes (20% in a very loose sense) to the existing KB.\nB. The Case of Two Disjoint Conditioning Events In complex fusion environments, especially when evidence is pooled from open sources, such as in crowd-sensing applications, it is highly unlikely that incoming evidence constitutes of a single event along with a confidence value. Let us look at a simplified scenario, where the incoming evidence constitutes ONLY of two disjoint events.\nExample 3 (MVP Poll v.3). Suppose the sports analyst A∗ in Example 2 has now gathered more evidence regarding the condition of c1 and c2. Now, he’s 100% certain that either event A1:= c1 and c2 will not return with 90% chance, or otherwise event A2:= if they return, due to their utmost dedication, only c1 and c2 will have a chance at the MVP. Now, what is the best strategy to update Ek?\na) Probabilistic Case: Let us assume that current voter preferences are stored in KB as Pk( ), where k is the discrete time index for current time tk. Now, the evidence is provided via two disjoint events A1 = (c3, c4, c5) and A2 = (c1, c2). For a proposition B ⊆ Θ, one may compute\nPk(B) = Pk(B|A1)Pk(A1) + Pk(B|A2)Pk(A2) (8)\nPerhaps, one direct way to extend this notion of total probability to derive up update equation similar to equation (8), while taking into account the less than perfect confidence of incoming evidence and it’s impact on the integrity of the KB (i.e., Pk(B)). Therefore, one may derive the intuitive extension, similar to Eq. (7), as\nPk+1(B) = αkPk(B) + (1− α)[Pk(B|A1)P ∗k (A1) + Pk(B|A2)P ∗k (A2)\n= αkPk(B) + 2∑ i=1 βk(Ai)Pk(B|Ai), (9)\nwhere βk(Ai) = (1− αk)P ∗k (Ai) with αk + ∑ i βk(Ai) = 1. Here, note that the support for each conditioning event Ai as provided by E∗k is used. Perhaps, one may interpret this as a weighted linear combination of conditioned evidence, where the parameters βk(Ai) are directly proportional to the support provided by incoming evidence to the conditioning events Ai, i = 1, 2 as given by βk(Ai) = (1− αk)P ∗k (Ai).\nb) Belief theoretic update: Now, one may directly extend the same thought process in Eq. 9 as\nBlk+1(B) = αk Blk(B) + 2∑ i=1 βk(Ai) Blk(B | Ai), (10)\nwhere αk + ∑ i βk(Ai) = 1. Here, the parameters βk(Ai) to be chosen s.t. they are directly proportional to the support for event Ai as provided by E∗k .\nC. The Case of Arbitrary (Multiple) Conditioning Events\nThe most general case can be analyzed when the incoming evidence E∗k is expressed as a BoE E∗k ≡ {Θ,F∗k,m∗k( )}, where the focal set F∗k and basic probability assignment m ∗ k( ) containing the set of conditioning events and their support, respectively.\nFollowing the notion of generating updated belief as a weighted linear combination of conditioned evidence, where the parameters βk(Ai) are taken to be directly proportional to the support provided by incoming evidence, one can extend Eq. 11 for updating with arbitrary conditioning events.\nDefinition 4 (Generalized Conditional Update (GCU)). The GCU that updates Ek ≡ {Θ,Fk,mk( )} with the evidence in E∗k ≡ {Θ,F∗k,m∗k( )} is given by\nBlk+1(B) = αk Blk(B) + ∑ A∈F∗k βk(A) Blk(B | A), (11)\nwhere the αk, βk( ) ∈ <+ satisfy αk + ∑ A∈F∗k βk(A) = 1.\nRemarks: The updating equations given by GCU proposed in this paper and CUE in [33] have similar functional form. However, the updating schemes are fundamentally different. In particular,\n— GCU conditions existing evidence Ek w.r.t. conditioning events A ∈ F∗k provided by incoming evidence in E∗k , whereas\n— CUE conditions incoming evidence E∗k within itself w.r.t. A ∈ F∗k.\nThis fundamental difference generates interesting differences for evidence updating and provides different and distinct features that maybe relevant in certain application contexts. In fact, we show that GCU boils down to Bayes conditional under the limiting conditions (i.e., αk = 0,∀k and mk=0( ) is Bayesian), irrespective of the structure of incoming evidence Ek. These features are directly influenced by the properties of FH conditional operation."
    }, {
      "heading" : "IV. BEHAVIOR OF GCU",
      "text" : "Understanding how the conditioning affects the focal elements in the current BoE is crucial to a proper understanding of any updating process.\nA. Focal Elements Generated via Conditioning\nA theorem that explains the focal elements generated by conditioning referred to as Conditional Core Theorem [38], [39] can be utilized for this task.\nTheorem 2 (Conditional Core Theorem (CCT)). [38] Given A ∈ F̂ in the BoE E = {Θ,F,m}, m(B|A) > 0 iff B can be expressed as B = X ∪ Y , for some X ∈ in(A) and Y ∈ OUT(A)∪{∅}. Here, in(A) = {B ⊆ A | B ∈ F}, OUT(A) = {B ⊆ A | B = ⋃i⊆I Ci, Ci ∈ out(A)}, where out(A) = {B ⊆ A | B ∪ C ∈ F, ∅ 6= B, ∅ 6= C ⊆ A}.\nThe CCT implies that conditional focal elements can only be generated from the disjunction of focal elements completely contained in the conditioning proposition A and focal elements that intersect but not included in A. For a comprehensive discussion on CCT, we refer the interested reader to [39]. The following example [38] illustrates the application of the CCT.\nExample 4. [38] Consider the BoE, Ek ≡ {Θ,Fk,mk( )} with Θ = {a, b, c, d, e, f, g, h, i}, mk = {a, b, h, df, beg,Θ} and mk(B) = {0.1, 0.1, 0.1, 0.2, 0.2, 0.3}, for B ∈ Fk (in the same order given in Fk). Then, for A = (abcde),\nin(A) = {a, b}; out(A) = {d, be, abcde}; IN(A) = {a, b, ab}; OUT(A) = {d, be, bde, abcde}.\nNote that, B = {ad, bd, be, abe, bde, abde, abcde}, are the only propositions that can be expressed as B = X ∪ Y , for some X ∈ in(A) and Y ∈ OUT(A). So, according to the CCT, the nine elements of B and in(A) are the only propositions that will belong to the conditional core (w.r.t. A = (abcde)).\nB. Impact on the Updating Process\nThe conditioning operation has a direct impact on the updating process. For pedagogical ease, let us consider the update with αk = 0, i.e., Ek+1 will only contain focal elements that are generated via conditioning.\n(a) If proposition B was not contained in at least one the conditioning events A ∈ F∗k (i.e., B 6⊆ A, ∀A ∈ F∗k), then B cannot belong to Fk+1.\n(b) For a proposition B that was contained in at least one conditioning event A ∈ F∗k,\n(b.1) if B ∈ Fk (i.e., it is a focal element in Ek), then B belongs to Fk+1.\n(b.2) if B 6∈ Fk, then B belongs to Fk+1 iff it can be expressed as the union of (i) a focal element B̂ ∈ Fk that is contained in a conditioning event A ∈ F∗k; and (ii) the intersection of A with some arbitrary set of focal elements B̃ ∈ Fk each of which straddles A and its complement A (see Fig. 2).\nThese properties clearly identifies how new propositions will be generated or existing propositions will be removed via conditioning operations. Furthermore,\n(d) for each conditioning operation, if there are no focal elements that straddle any of the conditioning events A ∈ F∗k and its complement, then Fk+1 = Fk;\n(e) propositions with zero belief does not belong to Fk+1. Therefore, with the exception of newly created focal elements (from straddling propositions as explained about), when α = 0 (i.e., completely update existing BoE with incoming evidence as in Bayes conditioning), the FH conditioning in GCU eliminates all propositions that were in complete disagreement with ALL conditioning events (as in the Bayes Conditioning).\nNow, when αk > 0, the core Fk+1 of updated BoE Ek+1 also retains all the focal elements that were contained in Fk.\nSince Fk+1 retains all focal elements that were contained in at least one of the conditioning events A ∈ F∗k (as is the case in Bayes Conditioning), it is interesting to look at the asymptotic behavior of propositions that are not supported by incoming evidence. Furthermore, how one initiates the updating process, or in other words, the initial mass assignments or the priors, will also have a clear impact on the final updated BoEs. In particular, it is easy to see that the vacuous initial assignment (i.e., mk=0(Θ) = 1.0) as a representation of complete ambiguity would not generate any refined BoEs irrespective of the incoming evidence.\nC. Vacuous Updating\nLet EΘ denote the vacuous BoE, i.e., the complete ambiguity often represented by EΘ = {Θ,Θ,m(Θ) = 1.0}.\n(i) Case 1: Ek+1 := EΘ C E∗k , i.e., updating a vacuous BoE with an arbitrary BoE: GCU generates Ek+1 = EΘ, irrespective of parameter selection. In particular, if one initiates an updating process with Ek=0 = EΘ, perhaps due to lack of prior information, Ek+1 := Ek C E∗k = EΘ, for all k. In other words, the KB continues to remain vacuous irrespective of the parameter selection or the incoming evidence. Obviously, conditioning only generates a refinement of originally cast evidence, and therefore proper selection of initial basic probability assignment (or priors) is crucial with the use of GCU.\n(ii) Case 2: Ek+1 := Ek C EΘ, i.e., updating an arbitrary BoE with a vacuous BoE: GCU generates Ek+1 = Ek, again, irrespective of parameter selection. In particular, if one carries out n updates as Ei+1 := Ei C EΘ, for i = k, . . . , k + n − 1, then, irrespective of GCU parameters, Ek+n := Ek. Therefore, GCU updates are tolerant against complete sensor failures in the sense that it will not complete erode an existing KB with vacuous incoming evidence.\nD. Selection of GCU Parameters\nGCU provides flexibility in parameter selection in order to accommodate the integrity of the existing knowledgebase, reliability of sensors and other application specific requirements. How one goes about selecting the appropriate parameter configuration however is is highly dependent on the application and domain. The work in [33], [34], [36] details several parameter selection strategies for CUE-based evidence updating. These strategies remain applicable for GCU. We do not intend to repeat a detailed description of these strategies here; the interested reader may refer to [32], [34], [36].\na) Selection of αk: The work in [36] provides several strategies for selection of αk w.r.t. the “inertia of existing body of evidence.” In particular, (i) infinite inertia-based selection: αk = 1; (ii) zero inertia-based selection: αk = 0; and, (iii) proportional inertia-based selection: k/(k+1), where k current discrete time index.\nb) Selection of βk( ): These weights allow one to emphasize/de-emphasize the propositions within each conditioning set A. Two very interesting choices that are inspired by the work in [32], [34], [36] are the following:\n(i) The receptive strategy: βk(A) = Kkm∗k(A), ∀A ∈ F∗k, where Kk 6= 0 is a constant. This receptive strategy ‘weighs’ the incoming evidence from E∗k according to the support E∗k itself has for it. In other words, the BoE that is being updated is ‘receptive’ to the support (assigned to the conditioning events) from incoming evidence E∗k .\n(ii) The cautious strategy: βk(A) = Kkmk(A), ∀A ∈ Fk, where Kk 6= 0 is a constant. This strategy ‘weights’ the incoming evidence from E∗k according to the support Ek itself has for it. In other words, being KB is being ‘cautious’ as to what events are in fact used for conditioning (or refining).\nE. Selection of Priors or Initial Mass Assignment\nDS theoretic sensor fusion methods are perhaps popular for its ability to conveniently ambiguous data, especially ignorance. For instance, with lack of information, one may start a fusion process by initializing E0 with m0(Θ) = 1.0, whereas in Bayesian probability, one may choose an approach, such as uniform priors. Unlike combination operators, since conditioning generates a refinement by redistributing initial mass assignments within the conditioning event(s), proper selection of initial mass assignment is crucial with GCU.\na) Selecting a vacuous BoE for E0: As shown, the uddate Ek+1 := EΘCE∗k generates Ek+1 = EΘ. Therefore, irrespective of E∗k , vacuous initial mass assignment will result in Ek→∞ = EΘ.\nb) Selecting a uniform mass assignment for E0: As in probability theory, one may utilize a uniform assignment, as m0(θi) = 1/|Θ|, i = 1, . . . , |Θ|.\nc) Selecting a Dirichlet BoE for E0: Another, perhaps more ‘DS like’ assignment strategy is to utilize, a Dirichlet BoE. With an application/context related parameter 0 < γ < 1, one may initialize the updating process as, m0(θi) = (1 − γ)/|Θ|, i = 1, . . . , |Θ| and m0(Θ) = γ. Perhaps, the gamma maybe chosen to represent the ‘ignorance’ associated with initial assignment."
    }, {
      "heading" : "V. CONCLUDING REMARKS",
      "text" : "Robust belief revision methods are crucial in streaming data situations for updating existing knowledge or beliefs with new incoming evidence. By viewing the evidence updating process as a thought experiment, a novel evidence updating strategy referred to as the GCU (i.e., generalized conditional update) is derived, especially targeting efficient belief revision and uncertainty handling in big-data stream-processing applications. The GCU generalizes the belief theoretic notion of FaginHalpern conditional, thus allowing one to account for various data/source imperfections that are characteristic to complex big-data fusion environments. The presented extension differs fundamentally from the previously published work on the conditional approach referred to as CUE (i.e., Conditional Update Equation) as well as authors own extensions of it. The GCU, the proposed evidence updating strategy based on a belief theoretic conditional approach, possesses several intuitively appealing features which seem to indicate its suitability for scenarios where large amounts of data/source uncertainties are\npresent. In particular, the initial basic belief assignment (or the priors) is fundamentally different from CUE and it’s counterparts. As shown in the derivations, a vacuous assignment as prior or initial basic probability assignment (as often done in Dempster-Shafer settings) is shown to generate inaccurate results. Among issues that warrant further investigation, of particular importance is the computational complexity that hampers the use of DS theoretic methods when working with a high number of sources and/or source FoDs having high cardinality."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "The author would like to thank Prof. Kamal Premaratne (University of Miami, Coral Gables) for enlightening discussions on numerous related topics and reviewers for their invaluable feedback."
    } ],
    "references" : [ {
      "title" : "It’s a streaming world! reasoning upon rapidly changing information",
      "author" : [ "E.D. Valle", "S. Ceri", "F. v. Harmelen", "D. Fensel" ],
      "venue" : "IEEE Intelligent Systems, vol. 24, no. 6, pp. 83–89, Nov 2009.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A knowledgebased platform for big data analytics based on publish/subscribe services and stream processing",
      "author" : [ "C. Esposito", "M. Ficco", "F. Palmieri", "A. Castiglione" ],
      "venue" : "Knowledge-Based Systems, vol. 79, pp. 3 – 17, 2015.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An Uncertainty-Aware Approach to Optimal Configuration of Stream Processing Systems",
      "author" : [ "P. Jamshidi", "G. Casale" ],
      "venue" : "June 2016. [Online]. Available: https://doi.org/10.5281/zenodo.56238",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Performance modeling and predictive scheduling for distributed stream data processing",
      "author" : [ "T. Li", "J. Tang", "J. Xu" ],
      "venue" : "IEEE Transactions on Big Data, vol. 2, no. 4, pp. 353–364, Dec 2016.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Predictive complex event processing based on evolving bayesian networks",
      "author" : [ "Y. Wang", "H. Gao", "G. Chen" ],
      "venue" : "Pattern Recognition Letters, 2017.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Kalman filter versus IMM estimator: When do we need the latter?",
      "author" : [ "T. Kirubarajan", "Y. Bar-Shalom" ],
      "venue" : "IEEE Transactions on Aerospace and Electronic Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "Mathematical Techniques in Multisensor Data Fusion",
      "author" : [ "D.L. Hall" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1992
    }, {
      "title" : "Consensusbased credibility estimation of soft evidence for robust data fusion",
      "author" : [ "T.L. Wickramarathne", "K. Premaratne", "M.N. Murthi" ],
      "venue" : "Belief Functions, ser. Advances in Intelligent and Soft Computing, T. Denoeux and M.-H. Masson, Eds. Compiégne, France: Springer Berlin / Heidelberg, May 2012, vol. 164, pp. 301–309.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning Bayesian belief networks: An approach based on the MDL principle",
      "author" : [ "W. Lam", "F. Bacchus" ],
      "venue" : "Computational Intelligence, vol. 10, pp. 269–293, July 1994.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Learning Bayesian networks: The combination of knowledge and statistical data",
      "author" : [ "D. Heckerman", "D. Geiger", "D. Chickering" ],
      "venue" : "Machine Learning, vol. 20, no. 3, pp. 197–243, 1994.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Bayesian network classifiers",
      "author" : [ "N. Friedman", "D. Geiger", "M. Goldszmidt" ],
      "venue" : "Machine Learning, vol. 29, no. 2/3, pp. 131–163, 1997.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Bayesian Networks and Decision Graphs, 1st ed",
      "author" : [ "F.V. Jensen" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2001
    }, {
      "title" : "Bayesian classification for data from the same unknown class",
      "author" : [ "H.-J. Huang", "C.-N. Hsu" ],
      "venue" : "IEEE Transactions on Systems, Man and Cybernetics, Part B: Cybernetics, vol. 32, no. 2, pp. 137–145, Apr. 2002.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Building Bayesian network models in medicine: The MENTOR experience",
      "author" : [ "S. Mani", "M. Valtorta", "S. McDermott" ],
      "venue" : "Applied Intelligence, vol. 22, no. 2, pp. 93–108, 2005.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A new approach to updating beliefs",
      "author" : [ "R. Fagin", "J.Y. Halpern" ],
      "venue" : "Proc. Conference on Uncertainty in Artificial Intelligence (UAI), P. P. Bonissone, M. Henrion, L. N. Kanal, and J. F. Lemmer, Eds. New York, NY: Elsevier Science, 1991, pp. 347–374.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Computational aspects of the Möbius transformation of graphs",
      "author" : [ "R. Kennes" ],
      "venue" : "IEEE Transactions on Systems, Man and Cybernetics, vol. 22, no. 2, pp. 201–223, Mar. 1992.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Incremental conditioning of lower and upper probabilities",
      "author" : [ "L. Chrisman" ],
      "venue" : "International Journal of Approximate Reasoning, vol. 13, no. 1, pp. 1–25, July 1995.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Data Fusion in Robotics and Machine Intelligence",
      "author" : [ "M.A. Abidi", "R.C. Gonzalez" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1992
    }, {
      "title" : "Shafer-Demspter reasoning with applications to multisensor target identification systems",
      "author" : [ "P.L. Bogler" ],
      "venue" : "IEEE Transactions on Systems, Man and Cybernetics, vol. 17, no. 6, pp. 968–977, 1987.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1988
    }, {
      "title" : "Belief decision trees: theoretical foundations",
      "author" : [ "Z. Elouedi", "K. Mellouli", "P. Smets" ],
      "venue" : "International Journal of Approximate Reasoning, vol. 28, no. 2/3, pp. 91–124, Nov. 2001.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Target identification based on the transferable belief model interpretation of Dempster-Shafer model",
      "author" : [ "F. Delmotte", "P. Smets" ],
      "venue" : "IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans, vol. 34, no. 4, pp. 457–471, July 2004.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Demspter-Shafer theory for intrusion detection in ad hoc networks",
      "author" : [ "T.M. Chen", "V. Venkataramanan" ],
      "venue" : "IEEE Internet Computing, vol. 9, no. 6, pp. 35–41, Nov./Dec. 2005.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Target identification using belief functions and implication rules",
      "author" : [ "B. Ristic", "P. Smets" ],
      "venue" : "IEEE Transactions on Aerospace and Electronic Systems, vol. 41, no. 3, pp. 1097–1103, July 2005.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Classification using belief functions: the relationship between the case-based and model-based approaches",
      "author" : [ "T. Denoeux", "P. Smets" ],
      "venue" : "IEEE Transaction on Systems, Man and Cybernetics, Part B: Cybernetics, vol. 36, no. 6, pp. 1395–1406, 2006.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Human experts fusion for image classification",
      "author" : [ "A. Martin", "C. Osswald" ],
      "venue" : "Information and Security: An International Journal, Special Issue on Fusing Uncertain, Imprecise and Conflicting Information, vol. 20, pp. 122–141, May 2006.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Ensemble clustering in the belief functions framework",
      "author" : [ "M.H. Masson", "T. Denoeux" ],
      "venue" : "International Journal of Approximate Reasoning, vol. 52, no. 1, pp. 92–109, Jan. 2011.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Participatory sensing",
      "author" : [ "J. Burke", "D. Estrin", "M. Hansen", "A. Parker", "N. Ramanathan", "S. Reddy", "M.B. Srivastava" ],
      "venue" : "Proc. Workshop on World-Sensor-Web (WSW): Mobile Device Centric Sensor Networks and Applications, 2006, pp. 117–134.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Distributed Data Fusion for Network-Centric Operations",
      "author" : [ "D. Hall", "M.L. II", "C.-Y. Chong", "J. Linas" ],
      "venue" : "Boca Raton, FL: CRC Press,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2012
    }, {
      "title" : "A Mathematical Theory of Evidence",
      "author" : [ "G. Shafer" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1976
    }, {
      "title" : "Evidence combination in an environment with heterogeneous sources",
      "author" : [ "K. Premaratne", "D.A. Dewasurendra", "P.H. Bauer" ],
      "venue" : "IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans, vol. 37, no. 3, pp. 298–309, 2007.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A Dempster-Shafer theoretic conditional approach to evidence updating for fusion of hard and soft data",
      "author" : [ "K. Premaratne", "M.N. Murthi", "J. Zhang", "M. Scheutz", "P.H. Bauer" ],
      "venue" : "Proc. International Conference on Information Fusion (FUSION), Seattle, WA, July 2009, pp. 2122–2129.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A Dempster-Shafer theoretic evidence updating strategy for non-identical frames of discernment",
      "author" : [ "T.L. Wickramarathne", "K. Premaratne", "M.N. Murthi", "M. Scheutz" ],
      "venue" : "Proc. Workshop on the Theory of Belief Functions (BELIEF), Brest, France, Apr. 2010.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convergence analysis of consensus belief functions within asynchronous ad-hoc fusion networks",
      "author" : [ "T.L. Wickramarathne", "K. Premaratne", "M.N. Murthi" ],
      "venue" : "Proc. International Conference on Statistical Signal Processing (ICASSP), Vancouver, Canada, May 2013.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Conditioning and updating evidence",
      "author" : [ "E.C. Kulasekere", "K. Premaratne", "D.A. Dewasurendra", "M.-L. Shyu", "P.H. Bauer" ],
      "venue" : "International Journal of Approximate Reasoning, vol. 36, no. 1, pp. 75–108, Apr. 2004.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Practical uses of belief functions",
      "author" : [ "P. Smets" ],
      "venue" : "Proc. Conference on Uncertainty in Artificial Intelligence (UAI), K. B. Laskey and H. Prade, Eds. San Francisco, CA: Morgan Kaufmann, 1999, pp. 612–621.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Focal elements generated by the Dempster-Shafer theoretic conditionals: A complete characterization",
      "author" : [ "T.L. Wickramarathne", "K. Premaratne", "M.N. Murthi" ],
      "venue" : "Proc. International Conference on Information Fusion (FUSION), Scotland, UK, July 2010.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Toward efficient computation of the dempster-shafer belief theoretic conditionals",
      "author" : [ "——" ],
      "venue" : "IEEE Transactions on Cybernetics, vol. 43, no. 2, pp. 712–724, Apr. 2012.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "It’s a streaming world—from financial markets to transportation to health monitoring to e-commerce applications, most of today’s data are generated and received in real-time as streams [1].",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : "With real-time processing bearing the promise of improved efficiency and creating new opportunities many application domains, stream processing [2]–[4] has become the latest trend in the big-data world.",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "With real-time processing bearing the promise of improved efficiency and creating new opportunities many application domains, stream processing [2]–[4] has become the latest trend in the big-data world.",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 4,
      "context" : "to predict future system states from real-time data streams while automatically accounting for data distribution drifts via a ‘single-pass’ processing of data [5] is a critical step in developing robust streaming processing methods for reasoning upon rapidly changing information.",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 5,
      "context" : "the system), commonly referred to as evidence updating (or belief revision) as new evidence is generated [6].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "To preserve the integrity of data fusion [7], adequately accounting for numerous uncertainties [8] is paramount, especially in big-data environments where soft (i.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 7,
      "context" : "To preserve the integrity of data fusion [7], adequately accounting for numerous uncertainties [8] is paramount, especially in big-data environments where soft (i.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "In this paper, we present a new evidence updating scheme that is derived as a natural extension of Bayes conditioning, the primary belief revision mechanism utilized in probability theory [9]–[14], to tackle the challenges associated with belief revision in such big-data environments.",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 13,
      "context" : "In this paper, we present a new evidence updating scheme that is derived as a natural extension of Bayes conditioning, the primary belief revision mechanism utilized in probability theory [9]–[14], to tackle the challenges associated with belief revision in such big-data environments.",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 14,
      "context" : "Conditioning [15]–[17] is the primary method for belief revision in a vast majority of data fusion [7], [18], [19] systems which employ probabilistic inferencing [20]– [28].",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 16,
      "context" : "Conditioning [15]–[17] is the primary method for belief revision in a vast majority of data fusion [7], [18], [19] systems which employ probabilistic inferencing [20]– [28].",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "Conditioning [15]–[17] is the primary method for belief revision in a vast majority of data fusion [7], [18], [19] systems which employ probabilistic inferencing [20]– [28].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 17,
      "context" : "Conditioning [15]–[17] is the primary method for belief revision in a vast majority of data fusion [7], [18], [19] systems which employ probabilistic inferencing [20]– [28].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 18,
      "context" : "Conditioning [15]–[17] is the primary method for belief revision in a vast majority of data fusion [7], [18], [19] systems which employ probabilistic inferencing [20]– [28].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 26,
      "context" : "Conditioning [15]–[17] is the primary method for belief revision in a vast majority of data fusion [7], [18], [19] systems which employ probabilistic inferencing [20]– [28].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 27,
      "context" : "single event in complex sensing and fusion situations that are characteristic of big-data environments [29], especially when non-traditional sources of evidence (e.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 28,
      "context" : ") are also being used for gathering evidence [30].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 29,
      "context" : "With a belief theoretic [31] core, our proposed method generalizes the belief theoretic Fagin-Halpern conditional notions [15], thus allowing one to account for various data/source imperfections that are characteristic to complex soft/hard data",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "With a belief theoretic [31] core, our proposed method generalizes the belief theoretic Fagin-Halpern conditional notions [15], thus allowing one to account for various data/source imperfections that are characteristic to complex soft/hard data",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 30,
      "context" : "The presented extension differs fundamentally from the previously published work on Conditional Update Equation (CUE) as appeared in [32], [33], including the authors own work in [8], [34], [35].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 31,
      "context" : "The presented extension differs fundamentally from the previously published work on Conditional Update Equation (CUE) as appeared in [32], [33], including the authors own work in [8], [34], [35].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 7,
      "context" : "The presented extension differs fundamentally from the previously published work on Conditional Update Equation (CUE) as appeared in [32], [33], including the authors own work in [8], [34], [35].",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 32,
      "context" : "The presented extension differs fundamentally from the previously published work on Conditional Update Equation (CUE) as appeared in [32], [33], including the authors own work in [8], [34], [35].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 33,
      "context" : "The presented extension differs fundamentally from the previously published work on Conditional Update Equation (CUE) as appeared in [32], [33], including the authors own work in [8], [34], [35].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 34,
      "context" : "However, it can be thought of as an extension of [36].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "The mapping m : 2 7→ [0, 1] is a BPA for the FoD Θ if m(∅) = 0 and ∑ B⊆Θm(B) = 1.",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "For B ⊆ Θ in the BoE {Θ,F ,m}, Bl : 2 7→ [0, 1] where Bl(B) = ∑C⊆Bm(C) is the belief of B; and Pl : 2 7→ [0, 1] where Pl(B) = 1−Bl(B) is the plausibility of B.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "For B ⊆ Θ in the BoE {Θ,F ,m}, Bl : 2 7→ [0, 1] where Bl(B) = ∑C⊆Bm(C) is the belief of B; and Pl : 2 7→ [0, 1] where Pl(B) = 1−Bl(B) is the plausibility of B.",
      "startOffset" : 105,
      "endOffset" : 111
    }, {
      "referenceID" : 35,
      "context" : "such a probability distribution is the pignistic probability distribution BetP(·) [37]",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "The Fagin-Halpern (FH) conditional notions in DS theory [15] are applicable whenever the conditioning proposition A belongs to F̂.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : "[15] For the conditioning event A ∈ F̂ and B ⊂ Θ in the BoE E = {Θ,F,m}, the conditional belief Bl(B|A) : 2 7→ [0, 1] and the conditional plausibility Pl(B|A) : 2 7→ [0, 1] of B given A are",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[15] For the conditioning event A ∈ F̂ and B ⊂ Θ in the BoE E = {Θ,F,m}, the conditional belief Bl(B|A) : 2 7→ [0, 1] and the conditional plausibility Pl(B|A) : 2 7→ [0, 1] of B given A are",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "[15] For the conditioning event A ∈ F̂ and B ⊂ Θ in the BoE E = {Θ,F,m}, the conditional belief Bl(B|A) : 2 7→ [0, 1] and the conditional plausibility Pl(B|A) : 2 7→ [0, 1] of B given A are",
      "startOffset" : 166,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "Thus FΘ|A = {B ⊆ Θ | m(B|A) > 0}, where A ∈ F̂ and m(·|A) : 2 7→ [0, 1] is the corresponding conditional BPA related to Bl(·|A) via the Möbius transformation [31]",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 29,
      "context" : "Thus FΘ|A = {B ⊆ Θ | m(B|A) > 0}, where A ∈ F̂ and m(·|A) : 2 7→ [0, 1] is the corresponding conditional BPA related to Bl(·|A) via the Möbius transformation [31]",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 31,
      "context" : "[33] The",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "The CUE proposed in [33] provides several interesting properties applicable to the task at hand.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 34,
      "context" : "Then, as proposed in [36] for updating in belief functions, one may employ a linear combination to generate the updated KB:",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 34,
      "context" : "Now, when A is specified with less than 100% confidence as in Example 2, one may utilize updating strategy similar to [36] to obtain,",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 34,
      "context" : "In fact, depending on the maturity of the KB, one may choose an αk not fully committing to incoming evidence (see [36] for a detailed discussion).",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 31,
      "context" : "Remarks: The updating equations given by GCU proposed in this paper and CUE in [33] have similar functional form.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 36,
      "context" : "A theorem that explains the focal elements generated by conditioning referred to as Conditional Core Theorem [38], [39] can be utilized for this task.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 37,
      "context" : "A theorem that explains the focal elements generated by conditioning referred to as Conditional Core Theorem [38], [39] can be utilized for this task.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 36,
      "context" : "[38] Given A ∈ F̂ in the BoE E = {Θ,F,m}, m(B|A) > 0 iff B can be expressed as B = X ∪ Y , for some X ∈ in(A) and Y ∈ OUT(A)∪{∅}.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "discussion on CCT, we refer the interested reader to [39].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 36,
      "context" : "The following example [38] illustrates the application of the CCT.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 36,
      "context" : "[38] Consider the BoE, Ek ≡ {Θ,Fk,mk( )} with Θ = {a, b, c, d, e, f, g, h, i}, mk = {a, b, h, df, beg,Θ} and mk(B) = {0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "The work in [33], [34], [36] details several parameter selection strategies for CUE-based evidence updating.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 32,
      "context" : "The work in [33], [34], [36] details several parameter selection strategies for CUE-based evidence updating.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 34,
      "context" : "The work in [33], [34], [36] details several parameter selection strategies for CUE-based evidence updating.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 30,
      "context" : "We do not intend to repeat a detailed description of these strategies here; the interested reader may refer to [32], [34], [36].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 32,
      "context" : "We do not intend to repeat a detailed description of these strategies here; the interested reader may refer to [32], [34], [36].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 34,
      "context" : "We do not intend to repeat a detailed description of these strategies here; the interested reader may refer to [32], [34], [36].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 34,
      "context" : "a) Selection of αk: The work in [36] provides several strategies for selection of αk w.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 30,
      "context" : "Two very interesting choices that are inspired by the work in [32], [34], [36] are the following: (i) The receptive strategy: βk(A) = Kkmk(A), ∀A ∈ Fk, where Kk 6= 0 is a constant.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 32,
      "context" : "Two very interesting choices that are inspired by the work in [32], [34], [36] are the following: (i) The receptive strategy: βk(A) = Kkmk(A), ∀A ∈ Fk, where Kk 6= 0 is a constant.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 34,
      "context" : "Two very interesting choices that are inspired by the work in [32], [34], [36] are the following: (i) The receptive strategy: βk(A) = Kkmk(A), ∀A ∈ Fk, where Kk 6= 0 is a constant.",
      "startOffset" : 74,
      "endOffset" : 78
    } ],
    "year" : 2017,
    "abstractText" : "Robust belief revision methods are crucial in streaming data situations for updating existing knowledge (or beliefs) with new incoming evidence. Bayes conditioning is the primary mechanism in use for belief revision in data fusion systems that use probabilistic inference. However, traditional conditioning methods face several challenges due to inherent data/source imperfections in big-data environments that harness soft (i.e., human or human-based) sources in addition to hard (i.e., physicsbased) sensors. The objective of this paper is to investigate the most natural extension of Bayes conditioning that is suitable for evidence updating in the presence of such uncertainties. By viewing the evidence updating process as a thought experiment, an elegant strategy is derived for robust evidence updating in the presence of extreme uncertainties that are characteristic of big-data environments. In particular, utilizing the Fagin-Halpern conditional notions, a natural extension to Bayes conditioning is derived for evidence that takes the form of a general belief function. The presented work differs fundamentally from the Conditional Update Equation (CUE) and authors own extensions of it. An overview of this development is provided via illustrative examples. Furthermore, insights into parameter selection under various fusion contexts are also provided.",
    "creator" : "LaTeX with hyperref package"
  }
}