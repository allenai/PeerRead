{
  "name" : "1201.0838.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "TMBP: A Topic Modeling Toolbox Using Belief Propagation",
    "authors" : [ "Jia Zeng" ],
    "emails" : [ "j.zeng@ieee.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 1.\n08 38\nv1 [\ncs .L\nG ]\n4 J\nLatent Dirichlet allocation (LDA) is an important class of hierarchical Bayesian models for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms. This toolbox is implemented by MEX C++/MATLAB platform for either Windows or Linux. The current version includes various learning algorithms for latent Dirichlet allocation (LDA), author-topic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project and more and more BP-based learning algorithms for various LDA-based topic models will be added in the near future. Interested readers may also extend this toolbox for solving more complicated topic modeling problems. The source code is freely available under the GNU General Public Licence, Version 1.0 at http://code.google.com/p/tmbp-topicmodel-beliefpropagation/"
    }, {
      "heading" : "1 Introduction",
      "text" : "The past decade has seen rapid development of latent Dirichlet allocation (LDA) [2] for solving various topic modeling and computer vision problems because of its elegant three-layer graphical representation as well as two efficient approximate inference methods like Variational Bayes (VB) [2] and collapsed Gibbs Sampling (GS) [6]. Both VB and GS have been widely used to learn variants of LDA-based topic models until a recent work [1] reveals that there is yet another learning algorithm for LDA based on loopy belief propagation (BP). Extensive experiments show that BP is faster and more accurate than both VB and GS, and has the potential to become a generic learning scheme for variants LDA-based topic models. Similar ideas have also been proposed within the approximate mean-field framework [7] as the zero-order approximation of collapsed VB (CVB0) algorithm [8].\nWe provide a topic modeling toolbox called TMBP implemented by MEX C++/MATLAB based on VB, GS and BP algorithms. This manual introduces how to use these tools for basic topic modeling tasks. Interested readers may extend this toolbox for more complicated topic modeling problems."
    }, {
      "heading" : "1.1 License",
      "text" : "This program is granted free of charge for research and education purposes. However you must obtain a license from the authors to use it for commercial purposes. Scientific results produced using the software provided shall acknowledge the use of TMBP. When using this toolbox, please cite [1]. The software can be modified and distributed with prior permission of the author."
    }, {
      "heading" : "2 Belief Propagation for Topic Modeling",
      "text" : "Given the vocabulary 1 ≤ w ≤ W and documents 1 ≤ d ≤ D in the corpus, the topic modeling is to allocate topic labels z = {zkw,d}, z k w,d ∈ {0, 1}, ∑K k=1 z k w,d = 1, 1 ≤ k ≤ K to partition the observed document-word co-occurrence matrix x = {xw,d} (xw,d is the number of word counts at the index {w, d}) into K topics according to three topic modeling rules:\n1. Co-occurrence: different word indices w in the same document d tend to have the same topic label.\n2. Smoothness: the same word index w in the different documents d tend to have the same topic label.\n3. Clustering: all word indices w do not tend to be associated with the same topic label.\nBased on the above rules, all approximate inference methods focus on inferring the marginal distribution of topic label µw,d(k) = p(zkw,d = 1) called message, and estimate parameters using the iterative EM algorithm according to the maximum-likelihood criterion. The difference lies in the message update equation. VB updates messages by involving relatively complicated digamma functions, which introduces bias but can be corrected by proper settings of hyperparameters [8]. In addition, it is the digamma function that slows down VB in updating messages [1]. GS updates messages by topic labels randomly sampled from the message in the previous iteration. Obviously, the sampling process will lose some information encoded in the previous message. By contrast, BP directly uses the previous message to update the current message iteratively. Similar ideas have also been proposed within the approximate mean-field framework as the zero-order approximation of collapsed VB (CVB0) algorithm [8].\nMore specifically, Table 1 compares the message update equations among VB, GS and BP. Compared with BP, VB uses the digamma function Ψ in message update, and GS uses the discrete count of sampled topic labels n−iw,d based on word tokens rather than word index in message update. The Dirichlet hyperparameters α and β can be viewed as the pseudo-messages. The notations −w and −d denote all word indices except w and all document indices except d, and −i denotes other word tokens except the current word token i."
    }, {
      "heading" : "3 Using TMBP",
      "text" : "TMBP contains mainly source codes for learning LDA based on VB, GS, and BP. It also includes GS and BP for author-topic models (ATM) [3], and BP for relational topic models (RTM) [4] and labeled LDA [5]. For LDA, there are six different BP implementations according to either the synchronous or the asynchronous update schedule.\nThis section presents an example of how to set up an experiment for five-fold cross-validation using the synchronous BP algorithm. Details on other examples can be found in the “Readme.pdf” file. We need to compile all MEX files in TMBP using “make.m” script. After compilation, we load the data set cora_wd in the MATLAB workspace:\n>> load cora_wd\nThe variable cora_wd is a W × D sparse matrix, where the non-zero elements are word counts in the corpus. Then, we perform the five-fold cross-validation for different topics K = 10, 20, 30, 40, 50:\nALPHA = 1e-2; BETA = 1e-2; N = 1000; SEED = 1; OUTPUT = 1;\nCV = cvpartition(2410, 'kfold', 5);\nfor i = 1:CV.NumTestSets\ntrIdx = CV.training(i); teIdx = CV.test(i); for K = 10:10:50\n[phi,theta] = sBPtrain(cora_wd(:,trIdx), K, N, ALPHA, BETA, SEED, OUTPUT); theta = sBPpredict(cora_wd(:,teIdx), phi, N, ALPHA, BETA, SEED, OUTPUT);\nend\nend\nALPHA and BETA are fixed symmetric Dirichlet hyperparameters provided by users. phi is a K ×W matrix for unnormalized topic-specific multinomial parameters over vocabulary. theta is a K ×D matrix for unnormalized document-specific topic proportions. N is the number of learning iterations. SEED is for random number generation. OUTPUT controls whether the program prints the perplexity on the screen. When OUTPUT = 1, the program prints on the screen:\nIteration 10 of 1000: 1041.620873 Iteration 20 of 1000: 883.708766 Iteration 30 of 1000: 833.396623 Iteration 40 of 1000: 808.610444 Iteration 50 of 1000: 792.929776\nThe difference between training sBPtrain and prediction sBPpredict is that the latter uses phi as fixed input from the former and only estimates the document-specific topic proportions theta on the unseen test set. We can show the top five words of each of K = 10 topic by the script “topicshow.m”:\n>> load cora_voc >> topicshow(phi, cora_voc, 5) network model neural networks data learning algorithm problem model algorithms paper learning control planning state learning paper system neural networks bayesian algorithm show decision paper learning problem algorithm search results model models data bayesian algorithms learning algorithm algorithms results decision neural network networks algorithm algorithms design problem system genetic research"
    }, {
      "heading" : "4 Data Structure",
      "text" : "In this toolbox, we include four publicly available document data sets [1]: 1) BLOG, 2) CORA, 3) MEDLINE and 4) NIPS. Without loss of generality, we will use CORA as an example to introduce the data structure. For the CORA data set, there are five files in the standard MATLAB *.mat format. We load these variables into MATLAB environment:\n>> load cora_wd\n>> load cora_voc >> load cora_ad >> load cora_author >> load cora_dd"
    }, {
      "heading" : "4.1 Word Count Matrix",
      "text" : "The variable cora_wd is a W ×D sparse matrix, where W is the vocabulary size, D is the total number of documents in the corpus, and each element is the word count xw,d 6= 0. We can visualize this sparse matrix by spy command:\n>> spy(cora_wd);\nWe show parts of the content in the first document:\n>> cora_wd(1:10,1)\nans =\n(1,1) 1 (2,1) 4 (3,1) 2 (4,1) 2 (5,1) 1 (6,1) 2 (7,1) 1 (8,1) 2 (9,1) 1 (10,1) 1\nWe may transform this sparse matrix to full matrix and vice versa,\n>> cora_wd = full(cora_wd); >> cora_wd = sparse(cora_wd);\nNote that all tools can handle only sparse matrix."
    }, {
      "heading" : "4.2 Vocabulary",
      "text" : "The variable cora_voc is a W -length cell array for the vocabulary. We show the first ten words in the vocabulary\n>> cora_voc(1:10)\nans =\n'computer' 'algorithms' 'discovering' 'patterns' 'groups' 'protein' 'sequences' 'based' 'fitting' 'parameters'"
    }, {
      "heading" : "4.3 Co-author Matrix",
      "text" : "The variable cora_ad is an A ×D sparse matrix, where A is the total number of unique authors, D is the total number of documents in the corpus. The element is 1 if and only if the author is associated with the document. We show the co-authors of the first document:\n>> cora_ad(:,1)\nans =\n(1481,1) 1 (2225,1) 1\nThere are two authors with indices 1481 and 2225 associated with the document 1."
    }, {
      "heading" : "4.4 Author Names",
      "text" : "The variable cora_author is an A-length cell array for the author names. We show two author names with indices 1481 and 2225 associated with the document 1:\n>> cora_author([1481,2225])\nans =\n'M Gribskov' 'T Bailey'"
    }, {
      "heading" : "4.5 Citation Links",
      "text" : "The variable cora_dd is a D×D sparse matrix, where the element is 1 if and only if two documents have a citation link. We show that the document 1 has two citations with the documents 389 and 484.\n>> cora_dd(:,1)\nans =\n(389,1) 1 (484,1) 1\nNote that we consider citations as undirected links, where citing and cited documents are not distinguished in topic modeling."
    }, {
      "heading" : "5 Compilation",
      "text" : "In MATLAB environment, we need to set up the compiler for MEX files:\n>> mex -setup Please choose your compiler building external interface (MEX) files:\nWould you like mex to locate installed compilers [y]/n? y\nSelect a compiler: [1] Microsoft Visual C++ 2010 Express\n[0] None\nCompiler: 1\nPlease verify your choices:\nCompiler: Microsoft Visual C++ 2010 Express Location: C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\nAre these correct [y]/n? y\nThen, we compile the MEX C++ files using the following commands\n>> mex -largeArrayDims sBPtrain.cpp\nwhere sBPtrain.cpp is the source file. Note that if we are not using 64-bit MATLAB, we may remove -largeArrayDims options.\nFor convenience, we provide make.m script that automatically compiles all MEX files on either Windows or Linux system:\n>> make\nAfter we compile all these files, we can use these tools as standard MATLAB commands.\nThe script test.m tests whether the compiled functions can work properly.\n>> test"
    }, {
      "heading" : "6 Parameters",
      "text" : "In the script test.m, we provide the following parameters:\nALPHA = 1e-2; BETA = 1e-2; OMEGA = 0.05; N = 20; M = 1; SEED = 1; OUTPUT = 1; J = 10;\nWe explain these parameters as follows:\n1. ALPHA and BETA are Dirichlet hyperparamters. In real-world applications, the asymmetric prior ALPHA may have substantial advantages over the symmetric prior, while the asymmetric prior BETA may not. Generally, the hyperparameters determine the sparseness of multinomial parameters THETA and PHI that influence the topic modeling performance. However, for simplicity, we assume that the hyperparameters are symmetric and are provided by users as prior knowledge in this toolbox. In many cases, we often use the smoothed LDA with fixed symmetric Dirichlet hyperparameters ALPHA = 0.01 and BETA = 0.01.\n2. OMEGA is a balancing weight OMEGA ∈ [0, 1] for relational topic models (RTM) [1], which balances messages from document content and document links. When OMEGA = 0, RTM reduces to the standard LDA.\n3. N is the number of learning iterations.\n4. M is the number of inner iterations in the VB algorithm.\n5. SEED is the seed for random number generation.\n6. OUTPUT = 0 denotes that no output is printed on the screen. OUTPUT = 1 prints the number of iterations and perplexity on the screen.\n7. J is the total number of topics pre-defined by the user.\nFor all tools, there are three common output parameters:\n1. phi is a J×W matrix for the unnormalized topic multinomial parameters over vocabulary.\n2. theta is a J ×D matrix for the unnormalized document-specific topic proportions.\n3. mu is a J × NZ matrix for the topic distribution over word index, where NZ is the total number of non-zero elements in the word count matrix cora_wd. Another similar output is z, which is a 1×NZ vector for the discrete assignment of topic label over word indices."
    }, {
      "heading" : "7 Latent Dirichlet Allocation (LDA)",
      "text" : "This toolbox contains mainly various source codes for learning LDA including Variational Bayes (VB) [2], collapsed Gibbs Sampling (GS) [6], and Belief Propagation (BP) [1]. Interested readers may refer to these papers for theoretical details of these algorithms."
    }, {
      "heading" : "7.1 Variational Bayes (VB)",
      "text" : "We use Blei’s implementation of digamma functions.1\n[phi, theta, mu] = VBtrain(cora_wd, J, N, M, ALPHA, BETA, SEED, OUTPUT); [phi, theta, mu] = VBtrain(cora_wd, J, N, M, ALPHA, BETA, SEED, OUTPUT, muin); [theta, mu] = VBpredict(cora_wd, phi, N, M, ALPHA, BETA, SEED, OUTPUT); [theta, mu] = VBpredict(cora_wd, phi, N, M, ALPHA, BETA, SEED, OUTPUT, muin);\nThe difference between VBtrain and VBpredict is that VBpredict estimates theta for unseen test set while holding the parameter phi fixed. If we do not provide the initialization messages muin to the functions, they will randomly initialize the message for propagation. The inner number of iterations M can be defined by users. Because VB often converges rapidly, M is often less than 20 in practice.\nBelow shows the unnormalized multinomial parameters phi and theta for the first vocabulary word and the first document, respectively.\n>> phi(:,1)\nans =\n52.8449\n3.1601 7.6788 17.7290\n9.6497 7.9235 20.9194 19.0359\n0.0107 0.0481\n>> theta(:,1)\nans =\n0.0000 3.4453 4.5136 78.8480\n0.0000\n1 http://www.cs.princeton.edu/˜blei/lda-c/index.html\n5.1930 0.0000 0.0000 0.0000 0.0000\nWe also show the normalized message mu for the first word index,\n>> mu(:,1)\nans =\n0.0000 0.0055 0.0181 0.9507 0.0000 0.0257 0.0000 0.0000 0.0000 0.0000"
    }, {
      "heading" : "7.2 Collapsed Gibbs Sampling (GS)",
      "text" : "For the GS algorithm, we re-write the MATLAB codes in the Topic Modeling Toolbox2 for our data structure.\n[phi, theta, z] = GStrain(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, z] = GStrain(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT, zin); [theta, z] = GSpredict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT); [theta, z] = GSpredict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT, zin);\nThe difference between GStrain and GSpredict is that GSpredict estimates theta for unseen test set while holding the parameter phi fixed. If we do not provide the initialization messages zin to the functions, they will randomly initialize the message for propagation.\nBecause GS works on word tokens, both phi and theta are organized as sparse matrices as following examples.\n>> phi(:,1)\nans =\n(2,1) 21 (4,1) 26 (6,1) 8 (9,1) 41 (10,1) 43\n>> theta(:,1)\nans =\n(1,1) 16 (2,1) 13 (5,1) 9 (7,1) 38 (10,1) 16\n2 http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm\nNote that z is a vector for discrete topic label assignment over word tokens:\n>> z(1:10)\nans =\n10 2 10 5 7 7 7 7 1 10"
    }, {
      "heading" : "7.3 Belief Propagation (BP)",
      "text" : "There are three major BP implementations called CVB0 [8], BP and simplified BP [1] (siBP). CVB0 passes messages over word tokens, while BP and siBP pass messages over word indices. The slight difference between BP and siBP lies in the message update equation. For each implementation, there are two scheduling schemes called synchronous schedule and asynchronous schedule. For asynchronous schedule, we randomly determine the route for message passing."
    }, {
      "heading" : "7.3.1 CVB0",
      "text" : "Here is the synchronous CVB0 algorithm:\n[phi, theta, mu] = sCVB0train(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, mu] = sCVB0train(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT, muin); [theta, mu] = sCVB0predict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT); [theta, mu] = sCVB0predict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT, muin);\nBelow is the asynchronous CVB0 algorithm:\n[phi, theta, mu] = aCVB0train(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, mu] = aCVB0train(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT, muin); [theta, mu] = aCVB0predict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT); [theta, mu] = aCVB0predict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT, muin);\nAll parameters are introduced in previous sections."
    }, {
      "heading" : "7.3.2 BP",
      "text" : "Here is the synchronous BP algorithm:\n[phi, theta, mu] = sBPtrain(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, mu] = sBPtrain(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT, muin); [theta, mu] = sBPpredict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT); [theta, mu] = sBPpredict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT, muin);\nBelow is the asynchronous BP algorithm:\n[phi, theta, mu] = aBPtrain(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, mu] = aBPtrain(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT, muin); [theta, mu] = aBPpredict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT); [theta, mu] = aBPpredict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT, muin);\nAll parameters are introduced in previous sections."
    }, {
      "heading" : "7.3.3 siBP",
      "text" : "Here is the synchronous siBP algorithm:\n[phi, theta, mu] = ssiBPtrain(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT);\n[phi, theta, mu] = ssiBPtrain(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT, muin); [theta, mu] = ssiBPpredict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT); [theta, mu] = ssiBPpredict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT, muin);\nBelow is the asynchronous siBP algorithm:\n[phi, theta, mu] = asiBPtrain(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, mu] = asiBPtrain(cora_wd, J, N, ALPHA, BETA, SEED, OUTPUT, muin); [theta, mu] = asiBPpredict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT); [theta, mu] = asiBPpredict(cora_wd, phi, N, ALPHA, BETA, SEED, OUTPUT, muin);\nThe motivation of siBP is for the fast vectorized MATLAB codes in functions siBPtrain.m and siBPpredict.m. Interested readers may easily revise these two functions for more complicated topic modeling tasks.\n[phi, theta] = siBPtrain(cora_wd, J, N, ALPHA, BETA, OUTPUT); theta = siBPpredict(cora_wd, phi, N, ALPHA, BETA, OUTPUT);\nThese two functions depend on the normalization function normalise.m. Note that the output parameters phi and theta are normalized multinomial parameters as follows.\n>> phi(:,1)\nans =\n0.0003 0.0006 0.0025 0.0010 0.0003 0.0031 0.0019 0.0000 0.0005 0.0000\n>> theta(:,1)\nans =\n0.0884 0.0009 0.0018 0.4341 0.0089 0.1050 0.0007 0.2384 0.0002 0.1216\nAll other parameters are introduced in previous sections."
    }, {
      "heading" : "8 Author-Topic Models (ATM)",
      "text" : "Unlike LDA, ATM [3] treats each author rather than document as a mixture of topics. In this way, the multinomial parameter theta is a J ×A matrix, where A is the total number unique authors in the corpus. We include GS and synchronous BP algorithms for learning ATM in this toolbox."
    }, {
      "heading" : "8.1 GS",
      "text" : "The GS algorithm is re-written based on Topic Modeling Toolbox3 for our data structure.\n[phi, theta, z, x] = ATMGStrain(cora_wd, cora_ad, J, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, z, x] = ATMGStrain(cora_wd, cora_ad, J, N, ALPHA, BETA, SEED, OUTPUT, zin, xin); [theta, z, x] = ATMGSpredict(cora_wd, cora_ad, phi, N, ALPHA, BETA, SEED, OUTPUT); [theta, z, x] = ATMGSpredict(cora_wd, cora_ad, phi, N, ALPHA, BETA, SEED, OUTPUT, zin, xin);\nThe output x is a 1 × NZ vector for author assignment over word tokens. The input xin is the user-defined initialization of this assignment. Below shows the author index for the first five word tokens:\nx(1:5)\nans =\n2225 2225 2225 2225 2225\nIt means the first five word tokens are generated by the author index 2225.\n>> cora_author(2225)\nans =\n'T Bailey'\nAll other parameters are introduced in previous sections."
    }, {
      "heading" : "8.2 BP",
      "text" : "The BP algorithm can be viewed as a soft version of the GS algorithm.\n[phi, theta, mu, x] = ATMBPtrain(cora_wd, cora_ad, J, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, mu, x] = ATMBPtrain(cora_wd, cora_ad, J, N, ALPHA, BETA, SEED, OUTPUT, muin, xin); [theta, mu, x] = ATMBPpredict(cora_wd, cora_ad, phi, N, ALPHA, BETA, SEED, OUTPUT); [theta, mu, x] = ATMBPpredict(cora_wd, cora_ad, phi, N, ALPHA, BETA, SEED, OUTPUT, muin, xin);\nThe ouput x is a NMAX × NZ matrix, where NMAX is the maximum number of co-authors per document. In this way, we assume that all co-authors contribute to generate word index with different probabilities. Below shows the first two authors’ contribution for the first five word indices:\nx(1:2,1:5)\nans =\n0.5239 0.4868 0.4139 0.5140 0.4727 0.4761 0.5132 0.5861 0.4860 0.5273\nAll other parameters are introduced in previous sections.\n3 http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm"
    }, {
      "heading" : "9 Relational Topic Models (RTM)",
      "text" : "RTM [4] introduces citation link modeling based on LDA. The GS algorithm for RTM can be found in R package.4 In this toolbox, we provide only BP algorithm for RTM.\n[phi, theta, gamma, mu] = RTMBPtrain(cora_wd, cora_dd, J, N, OMEGA, ALPHA, BETA, SEED, OUTPUT); [phi, theta, gamma, mu] = RTMBPtrain(cora_wd, cora_dd, J, N, OMEGA, ALPHA, BETA, SEED, OUTPUT, muin); [theta, mu] = RTMBPpredict(cora_wd, cora_dd, phi, gamma, N, OMEGA, ALPHA, BETA, SEED, OUTPUT); [theta, mu] = RTMBPpredict(cora_wd, cora_dd, phi, gamma, N, OMEGA, ALPHA, BETA, SEED, OUTPUT, muin);\nThe output gamma is a J × J matrix for topic dependencies over links. Below shows dependencies of the first five topics:\n>> gamma(1:5,1:5)\nans =\n0.1077 0.0991 0.0995 0.0930 0.0961 0.1150 0.1160 0.1123 0.1055 0.1123 0.1128 0.1096 0.1115 0.1096 0.1080 0.0981 0.0958 0.1021 0.1121 0.1045 0.1015 0.1022 0.1006 0.1047 0.1058\nWe see that the document has a higher likelihood to cite other documents with the same topic.\nAll other parameters are introduced in previous sections."
    }, {
      "heading" : "10 Labeled LDA (LaLDA)",
      "text" : "LaLDA [5] is a supervised topic model for multi-label classification based on LDA. The GS learning algorithm for LaLDA can found in the Stanford Topic Modeling Toolbox. 5 We provide only the BP algorithm for learning LaLDA.\n[phi, theta, z] = LaLDABPtrain(cora_wd, cora_ad, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, z] = LaLDABPtrain(cora_wd, cora_ad, N, ALPHA, BETA, SEED, OUTPUT, zin);\nHere we use the author names cora_ad as class labels for each document. Generally, each document will have multiple class labels. For unseen test set, we do not know class labels for each document, so that all labels in training set should be considered. In this case, we can simply infer theta using sBPpredict while holding phi fixed from RTMtrain. The output z is the class labeling assignment over word indices:\n>> z(1:5)\nans =\n1481 2225 1481 1481 1481\nAll other parameters are introduced in previous sections.\n4http://cran.r-project.org/web/packages/lda/lda.pdf 5 http://nlp.stanford.edu/software/tmt/tmt-0.3/"
    }, {
      "heading" : "11 Other Tools",
      "text" : "We also provide two scripts topicshow.m and perplexity.m. The former shows top N words in each topic, and the latter calculates perplexity on the data set.\n>> topicshow(phi, cora_voc, 5) network model neural networks data learning algorithm problem model algorithms paper learning control planning state learning paper system neural networks bayesian algorithm show decision paper learning problem algorithm search results model models data bayesian algorithms learning algorithm algorithms results decision neural network networks algorithm algorithms design problem system genetic research\nThe first input is the multinomial parameter phi, the second input is the vocabulary cora_voc, and the third input is the top N = 5 words in each topic.\n>> perplexity(cora_wd, phi, theta, ALPHA, BETA)\nans =\n1.0020e+003\nThe input cora_wd is the input data set, phi and theta are multinomial parameters, and ALPHA and BETA are Dirichlet hyperparameters."
    } ],
    "references" : [ {
      "title" : "Learning topic models by belief propagation",
      "author" : [ "J. Zeng", "W.K. Cheung", "J. Liu" ],
      "venue" : "arXiv:1109.3437v3 [cs.LG], 2011.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Latent Dirichlet allocation",
      "author" : [ "D.M. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "J. Mach. Learn. Res., vol. 3, pp. 993–1022, 2003.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The author-topic model for authors and documents",
      "author" : [ "M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth" ],
      "venue" : "UAI, 2004, pp. 487–494.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Hierarchical relational models for document networks",
      "author" : [ "J. Chang", "D.M. Blei" ],
      "venue" : "Annals of Applied Statistics, vol. 4, no. 1, pp. 124–150, 2010.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora",
      "author" : [ "D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning" ],
      "venue" : "Empirical Methods in Natural Language Processing, 2009, pp. 248–256.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "T.L. Griffiths", "M. Steyvers" ],
      "venue" : "Proc. Natl. Acad. Sci., vol. 101, pp. 5228–5235, 2004.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Approximate Mean Field for Dirichlet-Based Models",
      "author" : [ "A.U. Asuncion" ],
      "venue" : "ICML Workshop on Topic Models, 2010.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On smoothing and inference for topic models",
      "author" : [ "A. Asuncion", "M. Welling", "P. Smyth", "Y.W. Teh" ],
      "venue" : "UAI, 2009, pp. 27–34. 13",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "1 Introduction The past decade has seen rapid development of latent Dirichlet allocation (LDA) [2] for solving various topic modeling and computer vision problems because of its elegant three-layer graphical representation as well as two efficient approximate inference methods like Variational Bayes (VB) [2] and collapsed Gibbs Sampling (GS) [6].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "1 Introduction The past decade has seen rapid development of latent Dirichlet allocation (LDA) [2] for solving various topic modeling and computer vision problems because of its elegant three-layer graphical representation as well as two efficient approximate inference methods like Variational Bayes (VB) [2] and collapsed Gibbs Sampling (GS) [6].",
      "startOffset" : 306,
      "endOffset" : 309
    }, {
      "referenceID" : 5,
      "context" : "1 Introduction The past decade has seen rapid development of latent Dirichlet allocation (LDA) [2] for solving various topic modeling and computer vision problems because of its elegant three-layer graphical representation as well as two efficient approximate inference methods like Variational Bayes (VB) [2] and collapsed Gibbs Sampling (GS) [6].",
      "startOffset" : 344,
      "endOffset" : 347
    }, {
      "referenceID" : 0,
      "context" : "Both VB and GS have been widely used to learn variants of LDA-based topic models until a recent work [1] reveals that there is yet another learning algorithm for LDA based on loopy belief propagation (BP).",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "Similar ideas have also been proposed within the approximate mean-field framework [7] as the zero-order approximation of collapsed VB (CVB0) algorithm [8].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 7,
      "context" : "Similar ideas have also been proposed within the approximate mean-field framework [7] as the zero-order approximation of collapsed VB (CVB0) algorithm [8].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 0,
      "context" : "When using this toolbox, please cite [1].",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "Table 1: Comparison of message update equations [1].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "VB updates messages by involving relatively complicated digamma functions, which introduces bias but can be corrected by proper settings of hyperparameters [8].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "In addition, it is the digamma function that slows down VB in updating messages [1].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "Similar ideas have also been proposed within the approximate mean-field framework as the zero-order approximation of collapsed VB (CVB0) algorithm [8].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "It also includes GS and BP for author-topic models (ATM) [3], and BP for relational topic models (RTM) [4] and labeled LDA [5].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "It also includes GS and BP for author-topic models (ATM) [3], and BP for relational topic models (RTM) [4] and labeled LDA [5].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "It also includes GS and BP for author-topic models (ATM) [3], and BP for relational topic models (RTM) [4] and labeled LDA [5].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "4 Data Structure In this toolbox, we include four publicly available document data sets [1]: 1) BLOG, 2) CORA, 3) MEDLINE and 4) NIPS.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : ">> mex -setup Please choose your compiler building external interface (MEX) files: Would you like mex to locate installed compilers [y]/n? y Select a compiler: [1] Microsoft Visual C++ 2010 Express [0] None Compiler: 1",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : "OMEGA is a balancing weight OMEGA ∈ [0, 1] for relational topic models (RTM) [1], which balances messages from document content and document links.",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "OMEGA is a balancing weight OMEGA ∈ [0, 1] for relational topic models (RTM) [1], which balances messages from document content and document links.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "7 Latent Dirichlet Allocation (LDA) This toolbox contains mainly various source codes for learning LDA including Variational Bayes (VB) [2], collapsed Gibbs Sampling (GS) [6], and Belief Propagation (BP) [1].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "7 Latent Dirichlet Allocation (LDA) This toolbox contains mainly various source codes for learning LDA including Variational Bayes (VB) [2], collapsed Gibbs Sampling (GS) [6], and Belief Propagation (BP) [1].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 0,
      "context" : "7 Latent Dirichlet Allocation (LDA) This toolbox contains mainly various source codes for learning LDA including Variational Bayes (VB) [2], collapsed Gibbs Sampling (GS) [6], and Belief Propagation (BP) [1].",
      "startOffset" : 204,
      "endOffset" : 207
    }, {
      "referenceID" : 7,
      "context" : "3 Belief Propagation (BP) There are three major BP implementations called CVB0 [8], BP and simplified BP [1] (siBP).",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "3 Belief Propagation (BP) There are three major BP implementations called CVB0 [8], BP and simplified BP [1] (siBP).",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "8 Author-Topic Models (ATM) Unlike LDA, ATM [3] treats each author rather than document as a mixture of topics.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "9 Relational Topic Models (RTM) RTM [4] introduces citation link modeling based on LDA.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "10 Labeled LDA (LaLDA) LaLDA [5] is a supervised topic model for multi-label classification based on LDA.",
      "startOffset" : 29,
      "endOffset" : 32
    } ],
    "year" : 2017,
    "abstractText" : "Latent Dirichlet allocation (LDA) is an important class of hierarchical Bayesian models for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms. This toolbox is implemented by MEX C++/MATLAB platform for either Windows or Linux. The current version includes various learning algorithms for latent Dirichlet allocation (LDA), author-topic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project and more and more BP-based learning algorithms for various LDA-based topic models will be added in the near future. Interested readers may also extend this toolbox for solving more complicated topic modeling problems. The source code is freely available under the GNU General Public Licence, Version 1.0 at http://code.google.com/p/tmbp-topicmodel-beliefpropagation/",
    "creator" : "LaTeX with hyperref package"
  }
}