{
  "name" : "457.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "INCREMENTAL NETWORK QUANTIZATION: TOWARDS LOSSLESS CNNS WITH LOW-PRECISION WEIGHTS",
    "authors" : [ "Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen" ],
    "emails" : [ "yurong.chen}@intel.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Deep convolutional neural networks (CNNs) have demonstrated record breaking results on a variety of computer vision tasks such as image classification (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015), face recognition (Taigman et al., 2014; Sun et al., 2014), semantic segmentation (Long et al., 2015; Chen et al., 2015a) and object detection (Girshick, 2015; Ren et al., 2015). Regardless of the availability of significantly improved training resources such as abundant annotated data, powerful computational platforms and diverse training frameworks, the promising results of deep CNNs are mainly attributed to the large number of learnable parameters, ranging from tens of millions to even hundreds of millions. Recent progress further shows clear evidence that CNNs could easily enjoy the accuracy gain from the increased network depth and width (He et al., 2016; Szegedy et al., 2015; 2016). However, this in turn lays heavy burdens on the memory and other\n∗This work was done when Aojun Zhou was an intern at Intel Labs China, supervised by Anbang Yao who proposed the original idea and is responsible for correspondence. The first three authors contributed equally to the writing of the paper.\n1This notation applies to our method throughout the paper.\ncomputational resources. For instance, ResNet-152, a specific instance of the latest residual network architecture wining ImageNet classification challenge in 2015, has a model size of about 230 MB and needs to perform about 11.3 billion FLOPs to classify a 224× 224 image crop. Therefore, it is very challenging to deploy deep CNNs on the devices with limited computation and power budgets.\nSubstantial efforts have been made to the speed-up and compression on CNNs during training, feedforward test or both of them. Among existing methods, the category of network quantization methods attracts great attention from researches and developers. Some network quantization works try to compress pre-trained full-precision CNN models directly. Gong et al. (2014) address the storage problem of AlexNet (Krizhevsky et al., 2012) with vector quantization techniques. By replacing the weights in each of the three fully connected layers with respective floating-point centroid values obtained from the clustering, they can get over 20× model compression at about 1% loss in top-5 recognition rate. HashedNet (Chen et al., 2015b) uses a hash function to randomly map pre-trained weights into hash buckets, and all the weights in the same hash bucket are constrained to share a single floating-point value. In HashedNet, only the fully connected layers of several shallow CNN models are considered. For better compression, Han et al. (2016) present deep compression method which combines the pruning (Han et al., 2015), vector quantization and Huffman coding, and reduce the model storage by 35× on AlexNet and 49× on VGG-16 (Simonyan & Zisserman, 2015). Vanhoucke et al. (2011) use an SSE 8-bit fixed-point implementation to improve the computation of neural networks on the modern Intel x86 CPUs in feed-forward test, yielding 3× speed-up over an optimized floating-point baseline. Training CNNs by substituting the 32-bit floating-point representation with the 16-bit fixed-point representation has also been explored in Gupta et al. (2015). Other seminal works attempt to restrict CNNs into low-precision versions during training phase. Soudry et al. (2014) propose expectation backpropagation (EBP) to estimate the posterior distribution of deterministic network weights. With EBP, the network weights can be constrained to +1 and -1 during feed-forward test in a probabilistic way. BinaryConnect (Courbariaux et al., 2015) further extends the idea behind EBP to binarize network weights during training phase directly. It has two versions of network weights: floating-point and binary. The floating-point version is used as the reference for weight binarization. BinaryConnect achieves state-of-the-art accuracy using shallow CNNs for small datasets such as MNIST (LeCun et al., 1998) and CIFAR-10. Later on, a series of efforts have been invested to train CNNs with low-precision weights, low-precision activations and even low-precision gradients, including but not limited to BinaryNet (Courbariaux et al., 2016), XNOR-Net (Rastegari et al., 2016), ternary weight network (TWN) (Li & Liu, 2016), DoReFa-Net (Zhou et al., 2016) and quantized neural network (QNN) (Hubara et al., 2016).\nDespite these tremendous advances, CNN quantization still remains an open problem due to two critical issues which have not been well resolved yet, especially under scenarios of using low-precision weights for quantization. The first issue is the non-negligible accuracy loss for CNN quantization methods, and the other issue is the increased number of training iterations for ensuring convergence. In this paper, we attempt to address these two issues by presenting a novel incremental network quantization (INQ) method.\nIn our INQ, there is no assumption on the CNN architecture, and its basic goal is to efficiently convert any pre-trained full-precision (i.e., 32-bit floating-point) CNN model into a low-precision version whose weights are constrained to be either powers of two or zero. The advantage of such kind of low-precision models is that the original floating-point multiplication operations can be replaced by cheaper binary bit shift operations on dedicated hardware like FPGA. We noticed that most existing network quantization methods adopt a global strategy in which all the weights are simultaneously converted to low-precision ones (that are usually in the floating-point types). That is, they have not considered the different importance of network weights, leaving the room to retain network accuracy limited. In sharp contrast to existing methods, our INQ makes a very careful handling for the model accuracy drop from network quantization. To be more specific, it incorporates three interdependent operations: weight partition, group-wise quantization and re-training. Weight partition uses a pruning-inspired measure (Han et al., 2015; Guo et al., 2016) to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in our INQ. The weights in the first group are quantized to be either powers of two or zero by a variable-length encoding method, forming a low-precision base for the original model. The weights in the other group are re-trained while keeping the quantized weights fixed, compensating for the accuracy loss resulted from the quantization. Furthermore, these three operations are repeated on the\nlatest re-trained weight group in an iterative manner until all the weights are quantized, acting as an incremental network quantization and accuracy enhancement procedure (as illustrated in Figure 1).\nThe main insight of our INQ is that a compact combination of the proposed weight partition, groupwise quantization and re-training operations has the potential to get a lossless low-precision CNN model from any full-precision reference. We conduct extensive experiments on the ImageNet large scale classification task using almost all known deep CNN architectures to validate the effectiveness of our method. We show that: (1) For AlexNet, VGG-16, GoogleNet and ResNets with 5-bit quantization, INQ achieves improved accuracy in comparison with their respective full-precision baselines. The absolute top-1 accuracy gain ranges from 0.13% to 2.28%, and the absolute top-5 accuracy gain is in the range of 0.23% to 1.65%. (2) INQ has the property of easy convergence in training. In general, re-training with less than 8 epochs could consistently generate a lossless model with 5-bit weights in the experiments. (3) Taking ResNet-18 as an example, our quantized models with 4-bit, 3-bit and 2-bit ternary weights also have improved or very similar accuracy compared with its 32-bit floating-point baseline. (4) Taking AlexNet as an example, the combination of our network pruning and INQ outperforms deep compression method (Han et al., 2016) with significant margins."
    }, {
      "heading" : "2 INCREMENTAL NETWORK QUANTIZATION",
      "text" : "In this section, we clarify the insight of our INQ, describe its key components, and detail its implementation."
    }, {
      "heading" : "2.1 WEIGHT QUANTIZATION WITH VARIABLE-LENGTH ENCODING",
      "text" : "Suppose a pre-trained full-precision (i.e., 32-bit floating-point) CNN model can be represented by {Wl : 1 ≤ l ≤ L}, where Wl denotes the weight set of the lth layer, and L denotes the number of learnable layers in the model. To simplify the explanation, we only consider convolutional layers and fully connected layers. For CNN models like AlexNet, VGG-16, GoogleNet and ResNets as tested in this paper, Wl can be a 4D tensor for the convolutional layer, or a 2D matrix for the fully connected layer. For simplicity, here the dimension difference is not considered in the expression. Given a pre-trained full-precision CNN model, the main goal of our INQ is to convert all 32-bit floating-point weights to be either powers of two or zero without loss of model accuracy. Besides, we also attempt to explore the limit of the expected bit-width under the premise of guaranteeing lossless network quantization. Here, we start with our basic network quantization method on how to\nconvert Wl to be a low-precision version Ŵl, and each of its entries is chosen from\nPl = {±2n1 , · · · ,±2n2 , 0}, (1)\nwhere n1 and n2 are two integer numbers, and they satisfy n2 ≤ n1. Mathematically, n1 and n2 help to bound Pl in the sense that its non-zero elements are constrained to be in the range of either [−2n1 ,−2n2 ] or [2n2 , 2n1 ]. That is, network weights with absolute values smaller than 2n2 will be pruned away (i.e., set to zero) in the final low-precision model. Obviously, the problem is how to determine n1 and n2. In our INQ, the expected bit-width b for storing the indices in Pl is set beforehand, thus the only hyper-parameter shall be determined is n1 because n2 can be naturally computed once b and n1 are available. Here, n1 is calculated by using a tricky yet practically effective formula as\nn1 = floor(log2(4s/3)), (2)\nwhere floor(·) indicates the round down operation and s is calculated by using\ns = max(abs(Wl)), (3)\nwhere abs(·) is an element-wise operation and max(·) outputs the largest element of its input. In fact, Equation (2) helps to match the rounding power of 2 for s, and it could be easily implemented in practical programming. After n1 is obtained, n2 can be naturally determined as n2 = n1 + 1 − 2(b−1)/2. For instance, if b = 3 and n1 = −1, it is easy to get n2 = −2. Once Pl is determined, we further use the ladder of powers to convert every entry of Wl into a low-precision one by using\nŴl(i, j) = { βsgn(Wl(i, j)) if (α+ β)/2 ≤ abs(Wl(i, j)) < 3β/2 0 otherwise,\n(4)\nwhere α and β are two adjacent elements in the sorted Pl, making the above equation as a numerical rounding to the quantum values. It should be emphasized that factor 4/3 in Equation (2) is set to make sure that all the elements in Pl correspond with the quantization rule defined in Equation (4). In other words, factor 4/3 in Equation (2) highly correlates with factor 3/2 in Equation (4).\nHere, an important thing we want to clarify is the definition of the expected bit-width b. Taking 5-bit quantization as an example, since zero value cannot be written as the power of two, we use 1 bit to represent zero value, and the remaining 4 bits to represent at most 16 different values for the powers of two. That is, the number of candidate quantum values is at most 2b−1 + 1, so our quantization method actually adopts a variable-length encoding scheme. It is clear that the quantization described above is performed in a linear scale. An alternative solution is to perform the quantization in the log scale. Although it may also be effective, it should be a little bit more difficult in implementation and may cause some extra computational overhead in comparison to our method."
    }, {
      "heading" : "2.2 INCREMENTAL QUANTIZATION STRATEGY",
      "text" : "We can naturally use the above described method to quantize any pre-trained full-precision CNN model. However, noticeable accuracy loss appeared in the experiments when using small bit-width values (e.g., 5-bit, 4-bit, 3-bit and 2-bit).\nIn the literature, there are many existing network quantization works such as HashedNet (Chen et al., 2015b), vector quantization (Gong et al., 2014), fixed-point representation (Vanhoucke et al., 2011; Gupta et al., 2015), BinaryConnect (Courbariaux et al., 2015), BinaryNet (Courbariaux et al., 2016), XNOR-Net (Rastegari et al., 2016), TWN (Li & Liu, 2016), DoReFa-Net (Zhou et al., 2016) and QNN (Hubara et al., 2016). Similar to our basic network quantization method, they also suffer from non-negligible accuracy loss on deep CNNs, especially when being applied on the ImageNet large scale classification dataset. For all these methods, a common fact is that they adopt a global strategy in which all the weights are simultaneously converted into low-precision ones, which in turn causes accuracy loss. Compared with the methods focusing on the pre-trained models, accuracy loss becomes worse for the methods such as XNOR-Net, TWN, DoReFa-Net and QNN which intend to train low-precision CNNs from scratch.\nRecall that our main goal is to achieve lossless low-precision quantization for any pre-trained fullprecision CNN model with no assumption on its architecture. To this end, our INQ makes a special\nhandling of the strategy for suppressing resulting quantization loss in model accuracy. We are partially inspired by the latest progress in network pruning (Han et al., 2015; Guo et al., 2016). In these methods, the accuracy loss from removing less important network weights of a pre-trained neural network model could be well compensated by following re-training steps. Therefore, we conjecture that the nature of changing network weight importance is critical to achieve lossless network quantization.\nBase on this assumption, we present INQ which incorporates three interdependent operations: weight partition, group-wise quantization and re-training. Weight partition is to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in our INQ. The weights in the first group are responsible for forming a low-precision base for the original model, thus they are quantized by using Equation (4). The weights in the second group adapt to compensate for the loss in model accuracy, thus they are the ones to be re-trained. Once the first run of the quantization and re-training operations is finished, all the three operations are further conducted on the second weight group in an iterative manner, until all the weights are converted to be either powers of two or zero, acting as an incremental network quantization and accuracy enhancement procedure. As a result, accuracy loss under low-precision CNN quantization can be well suppressed by our INQ. Illustrative results at iterative steps of our INQ are provided in Figure 2.\nFor the lth layer, weight partition can be defined as\nA (1) l ∪A (2) l = {Wl(i, j)}, and A (1) l ∩A (2) l = ∅, (5)\nwhere A(1)l denotes the first weight group that needs to be quantized, and A2 denotes the other weight group that needs to be re-trained. We leave the strategies for group partition to be chosen in the experiment section. Here, we define a binary matrix Tl to help distinguish above two categories of weights. That is, Tl(i, j) = 0 means Wl(i, j) ∈ A(1)l , and Tl(i, j) = 1 means Wl(i, j) ∈ A(2)l ."
    }, {
      "heading" : "2.3 INCREMENTAL NETWORK QUANTIZATION ALGORITHM",
      "text" : "Now, we come to the training method. Taking the lth layer as an example, the basic optimization problem of making its weights to be either powers of two or zero can be expressed as\nmin Wl E(Wl) = L(Wl) + λR(Wl)\ns.t. Wl(i, j) ∈ Pl, 1 ≤ l ≤ L, (6)\nwhere L(Wl) is the network loss, R(Wl) is the regularization term, λ is a positive coefficient, and the constraint term indicates each weight entry Wl(i, j) should be chosen from the set Pl consisting of a fixed number of the values of powers of two plus zero. Direct solving above optimization problem in training from scratch is challenging since it is very easy to undergo convergence problem.\nBy performing weight partition and group-wise quantization operations beforehand, the optimization problem defined in (6) can be reshaped into a easier version. That is, we only need to optimize the following objective function\nmin Wl E(Wl) = L(Wl) + λR(Wl)\ns.t. Wl(i, j) ∈ Pl, if Tl(i, j) = 0, 1 ≤ l ≤ L, (7)\nwhere Pl is determined at group-wise quantization operation, and the binary matrix Tl acts as a mask which is determined by weight partition operation. Since Pl and Tl are known, the optimization problem (7) can be solved using popular stochastic gradient decent (SGD) method. That is, in INQ, we can get the update scheme for the re-training as\nWl(i, j)←Wl(i, j)− γ ∂E\n∂(Wl(i, j)) Tl(i, j), (8)\nwhere γ is a positive learning rate. Note that the binary matrix Tl forces zero update to the weights that have been quantized. That is, only the weights still keep with floating-point values are updated, akin to the latest pruning methods (Han et al., 2015; Guo et al., 2016) in which only the weights that are not currently removed are re-trained to enhance network accuracy. The whole procedure of our INQ is summarized as Algorithm 1.\nWe would like to highlight that the merits of our INQ are in three aspects: (1) Weight partition introduces the importance-aware weight quantization. (2) Group-wise weight quantization introduces much less accuracy loss than simultaneously quantizing all the network weights, thus making retraining have larger room to recover model accuracy. (3) By integrating the operations of weight partition, group-wise quantization and re-training into a nested loop, our INQ has the potential to obtain lossless low-precision CNN model from the pre-trained full-precision reference.\nAlgorithm 1 Incremental network quantization for lossless CNNs with low-precision weights. Input: X: the training data, {Wl : 1 ≤ l ≤ L}: the pre-trained full-precision CNN model, {σ1, σ2, · · · , σN}: the accumulated portions of weights quantized at iterative steps Output: {Ŵl : 1 ≤ l ≤ L}: the final low-precision model with the weights constrained to be either powers of two or zero\n1: Initialize A(1)l ← ∅, A (2) l ← {Wl(i, j)}, Tl ← 1, for 1 ≤ l ≤ L 2: for n = 1, 2, . . . , N do 3: Reset the base learning rate and the learning policy 4: According to σn, perform layer-wise weight partition and update A (1) l , A (2) l and Tl 5: Based on A(1)l , determine Pl layer-wisely 6: Quantize the weights in A(1)l by Equation (4) layer-wisely 7: Calculate feed-forward loss, and update weights in {A(2)l : 1 ≤ l ≤ L} by Equation (8) 8: end for"
    }, {
      "heading" : "3 EXPERIMENTAL RESULTS",
      "text" : "To analyze the performance of our INQ, we perform extensive experiments on the ImageNet large scale classification task, which is known as the most challenging image classification benchmark so far. ImageNet dataset has about 1.2 million training images and 50 thousand validation images. Each image is annotated as one of 1000 object classes. We apply our INQ to AlexNet, VGG-16, GoogleNet, ResNet-18 and ResNet-50, covering almost all known deep CNN architectures. Using the center crops of validation images, we report the results with two standard measures: top-1 error rate and top-5 error rate. For fair comparison, all pre-trained full-precision (i.e., 32-bit floatingpoint) CNN models except ResNet-18 are taken from the Caffe model zoo2. Note that He et al. (2016) do not release their pre-trained ResNet-18 model to the public, so we use a publicly available re-implementation by Facebook3. Since our method is implemented with Caffe, we make use of an open source tool4 to convert the pre-trained ResNet-18 model from Torch to Caffe."
    }, {
      "heading" : "3.1 RESULTS ON IMAGENET",
      "text" : "Setting expected bit-width to 5, the first set of experiments is performed to testify the efficacy of our INQ on different CNN architectures. Regarding weight partition, there are several candidate strategies as we tried in our previous work for efficient network pruning (Guo et al., 2016). In Guo et al. (2016), we found random partition and pruning-inspired partition are the two best choices compared with the others. Thus in this paper, we directly compare these two strategies for weight partition. In random strategy, the weights in each layer of any pre-trained full-precision deep CNN model are randomly split into two disjoint groups. In pruning-inspired strategy, the weights are divided into two disjoint groups by comparing their absolute values with layer-wise thresholds which are automatically determined by a given splitting ratio. Here we directly use pruning-inspired strategy and the experimental results in Section 3.2 will show why. After the re-training with no more than 8 epochs over each pre-trained full-precision model, we obtain the results as shown in Table 1. It can be concluded that the 5-bit CNN models generated by our INQ show consistently improved top-1 and top-5 recognition rates compared with respective full-precision references. Parameter settings are described below.\nAlexNet: AlexNet has 5 convolutional layers and 3 fully-connected layers. We set the accumulated portions of quantized weights at iterative steps as {0.3, 0.6, 0.8, 1}, the batch size as 256, the weight decay as 0.0005, and the momentum as 0.9.\nVGG-16: Compared with AlexNet, VGG-16 has 13 convolutional layers and more parameters. We set the accumulated portions of quantized weights at iterative steps as {0.5, 0.75, 0.875, 1}, the batch size as 32, the weight decay as 0.0005, and the momentum as 0.9.\n2https://github.com/BVLC/caffe/wiki/Model-Zoo 3https://github.com/facebook/fb.resnet.torch/tree/master/pretrained 4https://github.com/zhanghang1989/fb-caffe-exts\nGoogleNet: Compared with AlexNet and VGG-16, GoogleNet is more difficult to quantize due to a smaller number of parameters and the increased network width. We set the accumulated portions of quantized weights at iterative steps as {0.2, 0.4, 0.6, 0.8, 1}, the batch size as 80, the weight decay as 0.0002, and the momentum as 0.9.\nResNet-18: Different from above three networks, ResNets have batch normalization layers and relief the vanishing gradient problem by using shortcut connections. We first test the 18-layer version for exploratory purpose and test the 50-layer version later on. The network architectures of ResNet18 and ResNet-34 are very similar. The only difference is the number of filters in every convolutional layer. We set the accumulated portions of quantized weights at iterative steps as {0.5, 0.75, 0.875, 1}, the batch size as 80, the weight decay as 0.0005, and the momentum as 0.9. ResNet-50: Besides significantly increased network depth, ResNet-50 has a more complex network architecture in comparison to ResNet-18. However, regarding network architecture, ResNet-50 is very similar to ResNet-101 and ResNet-152. The only difference is the number of filters in every convolutional layer. We set the accumulated portions of quantized weights at iterative steps as {0.5, 0.75, 0.875, 1}, the batch size as 32, the weight decay as 0.0005, and the momentum as 0.9."
    }, {
      "heading" : "3.2 ANALYSIS OF WEIGHT PARTITION STRATEGIES",
      "text" : "In our INQ, the first operation is weight partition whose result will directly affect the following group-wise quantization and re-training operations. Therefore, the second set of experiments is conducted to analyze two candidate strategies for weight partition. As mentioned in the previous section, we use pruning-inspired strategy for weight partition. Unlike random strategy in which all the weights have equal probability to fall into the two disjoint groups, pruning-inspired strategy considers that the weights with larger absolute values are more important than the smaller ones to form a low-precision base for the original CNN model. We use ResNet-18 as a test case to compare the performance of these two strategies. In the experiments, the parameter settings are completely the same as described in Section 3.1. We set 4 epochs for weight re-training. Table 2 summarizes the results of our INQ with 5-bit quantization. It can be seen that our INQ achieves top-1 error rate of 32.11% and top-5 error rate of 11.73% by using random partition. Comparatively, pruning-inspired partition brings 1.09% and 0.83% decrease in top-1 and top-5 error rates, respectively. Apparently, pruning-inspired partition is better than random partition, and this is the reason why we use it in this paper. For future works, weight partition based on quantization error could also be an option worth exploring."
    }, {
      "heading" : "3.3 THE TRADE-OFF BETWEEN EXPECTED BIT-WIDTH AND MODEL ACCURACY",
      "text" : "The third set of experiments is performed to explore the limit of the expected bit-width under which our INQ can still achieve lossless network quantization. Similar to the second set of experiments, we also use ResNet-18 as a test case, and the parameter settings for the batch size, the weight decay and the momentum are completely the same. Finally, lower-precision models with 4-bit, 3-bit and even 2-bit ternary weights are generated for comparisons. As the expected bit-width goes down, the number of candidate quantum values will be decreased significantly, thus we shall increase the number of iterative steps accordingly for enhancing the accuracy of final low-precision model. Specifically, we set the accumulated portions of quantized weights at iterative steps as {0.3, 0.5, 0.8, 0.9, 0.95, 1}, {0.2, 0.4, 0.6, 0.7, 0.8, 0.9, 0.95, 1} and {0.2, 0.4, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.975, 1} for 4-bit, 3-bit and 2-bit ternary models, respectively. The required number of epochs also increases when the expected bit-width goes down, and it reaches 30 when training our 2-bit ternary model. Although our 4-bit model shows slightly decreased accuracy when compared with the 5-bit model, its accuracy is still better than that of the pre-trained full-precision model. Comparatively, even when the expected bit-width goes down to 3, our low-precision model shows only 0.19% and\n0.33% losses in top-1 and top-5 recognition rates, respectively. As for our 2-bit ternary model, although it incurs 2.25% decrease in top-1 error rate and 1.56% decrease in top-5 error rate in comparison to the pre-trained full-precision reference, its accuracy is considerably better than stateof-the-art results reported for binary-weight network (BWN) (Rastegari et al., 2016) and ternary weight network (TWN) (Li & Liu, 2016). Detailed results are summarized in Table 3 and Table 4."
    }, {
      "heading" : "3.4 LOW-BIT DEEP COMPRESSION",
      "text" : "In the literature, recently proposed deep compression method (Han et al., 2016) reports so far best results on network compression without loss of model accuracy. Therefore, the last set of experiments is conducted to explore the potential of our INQ for much better deep compression. Note that Han et al. (2016) is a hybrid network compression solution combining three different techniques, namely network pruning (Han et al., 2015), vector quantization (Gong et al., 2014) and Huffman coding. Taking AlexNet as an example, network pruning gets 9× compression, however this result is mainly obtained from the fully connected layers. Actually its compression performance on the convolutional layers is less than 3× (as can be seen in the Table 4 of Han et al. (2016)). Besides, network pruning is realized by separately performing pruning and re-training in an iterative way, which is very time-consuming. It will cost at least several weeks for compressing AlexNet. We solved this problem by our dynamic network surgery (DNS) method (Guo et al., 2016) which achieves about 7× speed-up in training and improves the performance of network pruning from 9× to 17.7×. In Han et al. (2016), after network pruning, vector quantization further improves compression ratio from 9× to 27×, and Huffman coding finally boosts compression ratio up to 35×. For fair comparison, we combine our proposed INQ and DNS, and compare the resulting method with Han et al. (2016). Detailed results are summarized in Table 5. When combing our proposed INQ and DNS, we achieve much better compression results compared with Han et al. (2016). Specifically, with 5-bit quantization, we can achieve 53× compression with slightly larger gains both in top-5 and top-1 recognition rates, yielding 51.43%/96.30% absolute improvement in compression performance compared with full version/fair version (i.e., the combination of network pruning and vector quantization) of Han et al. (2016), respectively. Consistently better results have also obtained for our 4-bit and 3-bit models.\nBesides, we also perform a set of experiments on AlexNet to compare the performance of our INQ and vector quantization (Gong et al., 2014). For fair comparison, re-training is also used to enhance the performance of vector quantization, and we set the number of cluster centers for all of 5 convolutional layers and 3 fully connect layers to 32 (i.e., 5-bit quantization). In the experiment, vector quantization incurs over 3% loss in model accuracy. When we change the number of cluster centers for convolutional layers from 32 to 128, it gets an accuracy loss of 0.98%. This is consistent with the results reported in (Gong et al., 2014). Comparatively, vector quantization is mainly proposed\nto compress the parameters in the fully connected layers of a pre-trained full-precision CNN model, while our INQ addresses all network layers simultaneously and has no accuracy loss for 5-bit and 4-bit quantization. Therefore, it is evident that our INQ is much better than vector quantization. Last but not least, the final weights for vector quantization (Gong et al., 2014), network pruning (Han et al., 2015) and deep compression (Han et al., 2016) are still floating-point values, but the final weights for our INQ are in the form of either powers of two or zero. The direct advantage of our INQ is that the original floating-point multiplication operations can be replaced by cheaper binary bit shift operations on dedicated hardware like FPGA."
    }, {
      "heading" : "4 CONCLUSIONS",
      "text" : "In this paper, we present INQ, a new network quantization method, to address the problem of how to convert any pre-trained full-precision (i.e., 32-bit floating-point) CNN model into a lossless lowprecision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which usually quantize all the network weights simultaneously, INQ is a more compact quantization framework. It incorporates three interdependent operations: weight partition, groupwise quantization and re-training. Weight partition splits the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in INQ. The weights in the first group is directly quantized by a variable-length encoding method, forming a low-precision base for the original CNN model. The weights in the other group are re-trained while keeping all the quantized weights fixed, compensating for the accuracy loss from network quantization. More importantly, the operations of weight partition, group-wise quantization and re-training are repeated on the latest re-trained weight group in an iterative manner until all the weights are quantized, acting as an incremental network quantization and accuracy enhancement procedure. On the ImageNet large scale classification task, we conduct extensive experiments and show that our quantized CNN models with 5-bit, 4-bit, 3-bit and even 2-bit ternary weights have improved or at least comparable accuracy against their full-precision baselines, including AlexNet, VGG-16, GoogleNet and ResNets. As for future works, we plan to extend incremental idea behind INQ from low-precision weights to low-precision activations and low-precision gradients (we have actually already made some good progress on it, as shown in our supplementary materials). We will also investigate computation and power efficiency by implementing our low-precision CNN models on hardware platforms."
    }, {
      "heading" : "A APPENDIX 1: STATISTICAL ANALYSIS OF THE QUANTIZED WEIGHTS",
      "text" : "Taking our 5-bit AlexNet model as an example, we analyze the distribution of the quantized weights. Detailed statistical results are summarized in Table 6. We can find: (1) in the 1st and 2nd convolutional layers, the values of {−2−6, −2−5, −2−4, 2−6, 2−5, 2−4} and {−2−8, −2−7, −2−6, −2−5, 0, 2−8, 2−7, 2−6, 2−5} occupy over 60% and 94% of all quantized weights, respectively; (2) the distributions of the quantized weights in the 3rd, 4th and 5th convolutional layers are similar to that of the 2nd convolutional layer, and more weights are quantized into zero in the 2nd, 3rd, 4th and 5th convolutional layers compared with the 1st convolutional layer; (3) in the 1st fully connected layer, the values of {−2−10, −2−9, −2−8, −2−7, 0, 2−10, 2−9, 2−8, 2−7} occupy about 98% of all quantized weights, and similar results can be seen for the 2nd fully connected layer; (4) generally, the distributions of the quantized weights in the convolutional layers are usually more scattered compared with the fully connected layers. This may be partially the reason why it is much easier to get good compression performance on fully connected layers in comparison to convolutional layers, when using methods such as network hashing (Chen et al., 2015b) and vector quantization (Gong et al., 2014); (5) for 5-bit AlexNet model, the required bit-width for each layer is actually 4 but not 5."
    }, {
      "heading" : "B APPENDIX 2: LOSSLESS CNNS WITH LOW-PRECISION WEIGHTS AND LOW-PRECISION ACTIVATIONS",
      "text" : "Recently, we have made some good progress on developing our INQ for lossless CNNs with both low-precision weights and low-precision activations. According to the results summarized in Table 7, it can be seen that our VGG-16 model with 5-bit weights and 4-bit activations shows improved top-5 and top-1 recognition rates in comparison to the pre-trained reference with 32-bit floating-point weights and 32-bit floating-point activations. To the best of our knowledge, this should be the best results reported on VGG-16 architecture so far."
    } ],
    "references" : [ {
      "title" : "Semantic image segmentation with deep convolutional nets and fully connected crfs",
      "author" : [ "Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "L. Yuille Alan" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Compressing neural networks with the hashing trick",
      "author" : [ "Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Binaryconnect: Training deep neural networks with binary weights during propagations",
      "author" : [ "Matthieu Courbariaux", "Bengio Yoshua", "David Jean-Pierre" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Courbariaux et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Courbariaux et al\\.",
      "year" : 2015
    }, {
      "title" : "Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1",
      "author" : [ "Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1602.02830v3,",
      "citeRegEx" : "Courbariaux et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Courbariaux et al\\.",
      "year" : 2016
    }, {
      "title" : "Fast r-cnn",
      "author" : [ "Ross Girshick" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Girshick.,? \\Q2015\\E",
      "shortCiteRegEx" : "Girshick.",
      "year" : 2015
    }, {
      "title" : "Compressing deep concolutional networks using vector quantization",
      "author" : [ "Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev" ],
      "venue" : "arXiv preprint arXiv:1412.6115v1,",
      "citeRegEx" : "Gong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2014
    }, {
      "title" : "Dynamic network surgery for efficient dnns",
      "author" : [ "Yiwen Guo", "Anbang Yao", "Yurong Chen" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Guo et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep learning with limited numerical precision",
      "author" : [ "Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Gupta et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning both weights and connections for efficient neural networks",
      "author" : [ "Song Han", "Jeff Pool", "John Tran", "William J. Dally" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Han et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "author" : [ "Song Han", "Jeff Pool", "John Tran", "William J. Dally" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Han et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Zhang Xiangyu", "Ren Shaoqing", "Sun Jian" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Quantized neural networks: Training neural networks with low precision weights and activations",
      "author" : [ "Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1609.07061v1,",
      "citeRegEx" : "Hubara et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hubara et al\\.",
      "year" : 2016
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Sutskever Ilya", "E. Hinton Geoffrey" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to documentrecognition",
      "author" : [ "Yann LeCun", "Bottou Leon", "Yoshua Bengio", "Patrick Hadner" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Ternary weight networks",
      "author" : [ "Fengfu Li", "Bin Liu" ],
      "venue" : "arXiv preprint arXiv:1605.04711v1,",
      "citeRegEx" : "Li and Liu.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li and Liu.",
      "year" : 2016
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "Jonathan Long", "Evan Shelhamer", "Trevor Darrell" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Long et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2015
    }, {
      "title" : "Xnor-net: Imagenet classification using binary convolutional neural networks",
      "author" : [ "Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi" ],
      "venue" : "arXiv preprint arXiv:1603.05279v4,",
      "citeRegEx" : "Rastegari et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rastegari et al\\.",
      "year" : 2016
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ren et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Simonyan and Zisserman.,? \\Q2015\\E",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2015
    }, {
      "title" : "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights",
      "author" : [ "Daniel Soudry", "Itay Hubara", "Ron Meir" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Soudry et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Soudry et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep learning face representation from predicting 10,000 classes",
      "author" : [ "Yi Sun", "Xiaogang Wang", "Xiaoou Tang" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Sun et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "author" : [ "Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke" ],
      "venue" : "arXiv preprint arXiv:1602.07261v1,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Deepface: Closing the gap to human-level performance in face verification",
      "author" : [ "Yaniv Taigman", "Ming Yang", "Marc’ Aurelio Ranzato", "Lior Wolf" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Taigman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Taigman et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving the speed of neural networks on cpus",
      "author" : [ "Vincent Vanhoucke", "Andrew Senior", "Mark Z. Mao" ],
      "venue" : "In Deep Learning and Unsupervised Feature Learning Workshop,",
      "citeRegEx" : "Vanhoucke et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Vanhoucke et al\\.",
      "year" : 2011
    }, {
      "title" : "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients",
      "author" : [ "Shuchang Zhou", "Zekun Ni", "Xinyu Zhou", "He Wen", "Yuxin Wu", "Yuheng Zou" ],
      "venue" : "arXiv preprint arXiv:1605.04711v1,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Deep convolutional neural networks (CNNs) have demonstrated record breaking results on a variety of computer vision tasks such as image classification (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015), face recognition (Taigman et al.",
      "startOffset" : 151,
      "endOffset" : 204
    }, {
      "referenceID" : 23,
      "context" : ", 2012; Simonyan & Zisserman, 2015), face recognition (Taigman et al., 2014; Sun et al., 2014), semantic segmentation (Long et al.",
      "startOffset" : 54,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : ", 2012; Simonyan & Zisserman, 2015), face recognition (Taigman et al., 2014; Sun et al., 2014), semantic segmentation (Long et al.",
      "startOffset" : 54,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : ", 2014), semantic segmentation (Long et al., 2015; Chen et al., 2015a) and object detection (Girshick, 2015; Ren et al.",
      "startOffset" : 31,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : ", 2015a) and object detection (Girshick, 2015; Ren et al., 2015).",
      "startOffset" : 30,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : ", 2015a) and object detection (Girshick, 2015; Ren et al., 2015).",
      "startOffset" : 30,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "Recent progress further shows clear evidence that CNNs could easily enjoy the accuracy gain from the increased network depth and width (He et al., 2016; Szegedy et al., 2015; 2016).",
      "startOffset" : 135,
      "endOffset" : 180
    }, {
      "referenceID" : 21,
      "context" : "Recent progress further shows clear evidence that CNNs could easily enjoy the accuracy gain from the increased network depth and width (He et al., 2016; Szegedy et al., 2015; 2016).",
      "startOffset" : 135,
      "endOffset" : 180
    }, {
      "referenceID" : 12,
      "context" : "(2014) address the storage problem of AlexNet (Krizhevsky et al., 2012) with vector quantization techniques.",
      "startOffset" : 46,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "(2016) present deep compression method which combines the pruning (Han et al., 2015), vector quantization and Huffman coding, and reduce the model storage by 35× on AlexNet and 49× on VGG-16 (Simonyan & Zisserman, 2015).",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "BinaryConnect (Courbariaux et al., 2015) further extends the idea behind EBP to binarize network weights during training phase directly.",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "BinaryConnect achieves state-of-the-art accuracy using shallow CNNs for small datasets such as MNIST (LeCun et al., 1998) and CIFAR-10.",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "Later on, a series of efforts have been invested to train CNNs with low-precision weights, low-precision activations and even low-precision gradients, including but not limited to BinaryNet (Courbariaux et al., 2016), XNOR-Net (Rastegari et al.",
      "startOffset" : 190,
      "endOffset" : 216
    }, {
      "referenceID" : 16,
      "context" : ", 2016), XNOR-Net (Rastegari et al., 2016), ternary weight network (TWN) (Li & Liu, 2016), DoReFa-Net (Zhou et al.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 25,
      "context" : ", 2016), ternary weight network (TWN) (Li & Liu, 2016), DoReFa-Net (Zhou et al., 2016) and quantized neural network (QNN) (Hubara et al.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : ", 2016) and quantized neural network (QNN) (Hubara et al., 2016).",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "Weight partition uses a pruning-inspired measure (Han et al., 2015; Guo et al., 2016) to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in our INQ.",
      "startOffset" : 49,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "Weight partition uses a pruning-inspired measure (Han et al., 2015; Guo et al., 2016) to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play complementary roles in our INQ.",
      "startOffset" : 49,
      "endOffset" : 85
    }, {
      "referenceID" : 1,
      "context" : "Gong et al. (2014) address the storage problem of AlexNet (Krizhevsky et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "HashedNet (Chen et al., 2015b) uses a hash function to randomly map pre-trained weights into hash buckets, and all the weights in the same hash bucket are constrained to share a single floating-point value. In HashedNet, only the fully connected layers of several shallow CNN models are considered. For better compression, Han et al. (2016) present deep compression method which combines the pruning (Han et al.",
      "startOffset" : 11,
      "endOffset" : 341
    }, {
      "referenceID" : 0,
      "context" : "HashedNet (Chen et al., 2015b) uses a hash function to randomly map pre-trained weights into hash buckets, and all the weights in the same hash bucket are constrained to share a single floating-point value. In HashedNet, only the fully connected layers of several shallow CNN models are considered. For better compression, Han et al. (2016) present deep compression method which combines the pruning (Han et al., 2015), vector quantization and Huffman coding, and reduce the model storage by 35× on AlexNet and 49× on VGG-16 (Simonyan & Zisserman, 2015). Vanhoucke et al. (2011) use an SSE 8-bit fixed-point implementation to improve the computation of neural networks on the modern Intel x86 CPUs in feed-forward test, yielding 3× speed-up over an optimized floating-point baseline.",
      "startOffset" : 11,
      "endOffset" : 579
    }, {
      "referenceID" : 0,
      "context" : "HashedNet (Chen et al., 2015b) uses a hash function to randomly map pre-trained weights into hash buckets, and all the weights in the same hash bucket are constrained to share a single floating-point value. In HashedNet, only the fully connected layers of several shallow CNN models are considered. For better compression, Han et al. (2016) present deep compression method which combines the pruning (Han et al., 2015), vector quantization and Huffman coding, and reduce the model storage by 35× on AlexNet and 49× on VGG-16 (Simonyan & Zisserman, 2015). Vanhoucke et al. (2011) use an SSE 8-bit fixed-point implementation to improve the computation of neural networks on the modern Intel x86 CPUs in feed-forward test, yielding 3× speed-up over an optimized floating-point baseline. Training CNNs by substituting the 32-bit floating-point representation with the 16-bit fixed-point representation has also been explored in Gupta et al. (2015). Other seminal works attempt to restrict CNNs into low-precision versions during training phase.",
      "startOffset" : 11,
      "endOffset" : 944
    }, {
      "referenceID" : 0,
      "context" : "HashedNet (Chen et al., 2015b) uses a hash function to randomly map pre-trained weights into hash buckets, and all the weights in the same hash bucket are constrained to share a single floating-point value. In HashedNet, only the fully connected layers of several shallow CNN models are considered. For better compression, Han et al. (2016) present deep compression method which combines the pruning (Han et al., 2015), vector quantization and Huffman coding, and reduce the model storage by 35× on AlexNet and 49× on VGG-16 (Simonyan & Zisserman, 2015). Vanhoucke et al. (2011) use an SSE 8-bit fixed-point implementation to improve the computation of neural networks on the modern Intel x86 CPUs in feed-forward test, yielding 3× speed-up over an optimized floating-point baseline. Training CNNs by substituting the 32-bit floating-point representation with the 16-bit fixed-point representation has also been explored in Gupta et al. (2015). Other seminal works attempt to restrict CNNs into low-precision versions during training phase. Soudry et al. (2014) propose expectation backpropagation (EBP) to estimate the posterior distribution of deterministic network weights.",
      "startOffset" : 11,
      "endOffset" : 1062
    }, {
      "referenceID" : 9,
      "context" : "(4) Taking AlexNet as an example, the combination of our network pruning and INQ outperforms deep compression method (Han et al., 2016) with significant margins.",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : ", 2015b), vector quantization (Gong et al., 2014), fixed-point representation (Vanhoucke et al.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 24,
      "context" : ", 2014), fixed-point representation (Vanhoucke et al., 2011; Gupta et al., 2015), BinaryConnect (Courbariaux et al.",
      "startOffset" : 36,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : ", 2014), fixed-point representation (Vanhoucke et al., 2011; Gupta et al., 2015), BinaryConnect (Courbariaux et al.",
      "startOffset" : 36,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : ", 2015), BinaryConnect (Courbariaux et al., 2015), BinaryNet (Courbariaux et al.",
      "startOffset" : 23,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : ", 2015), BinaryNet (Courbariaux et al., 2016), XNOR-Net (Rastegari et al.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 16,
      "context" : ", 2016), XNOR-Net (Rastegari et al., 2016), TWN (Li & Liu, 2016), DoReFa-Net (Zhou et al.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 25,
      "context" : ", 2016), TWN (Li & Liu, 2016), DoReFa-Net (Zhou et al., 2016) and QNN (Hubara et al.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 11,
      "context" : ", 2016) and QNN (Hubara et al., 2016).",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "We are partially inspired by the latest progress in network pruning (Han et al., 2015; Guo et al., 2016).",
      "startOffset" : 68,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "We are partially inspired by the latest progress in network pruning (Han et al., 2015; Guo et al., 2016).",
      "startOffset" : 68,
      "endOffset" : 104
    }, {
      "referenceID" : 8,
      "context" : "That is, only the weights still keep with floating-point values are updated, akin to the latest pruning methods (Han et al., 2015; Guo et al., 2016) in which only the weights that are not currently removed are re-trained to enhance network accuracy.",
      "startOffset" : 112,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "That is, only the weights still keep with floating-point values are updated, akin to the latest pruning methods (Han et al., 2015; Guo et al., 2016) in which only the weights that are not currently removed are re-trained to enhance network accuracy.",
      "startOffset" : 112,
      "endOffset" : 148
    }, {
      "referenceID" : 10,
      "context" : "Note that He et al. (2016) do not release their pre-trained ResNet-18 model to the public, so we use a publicly available re-implementation by Facebook3.",
      "startOffset" : 10,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : "Regarding weight partition, there are several candidate strategies as we tried in our previous work for efficient network pruning (Guo et al., 2016).",
      "startOffset" : 130,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "Regarding weight partition, there are several candidate strategies as we tried in our previous work for efficient network pruning (Guo et al., 2016). In Guo et al. (2016), we found random partition and pruning-inspired partition are the two best choices compared with the others.",
      "startOffset" : 131,
      "endOffset" : 171
    }, {
      "referenceID" : 16,
      "context" : "56% decrease in top-5 error rate in comparison to the pre-trained full-precision reference, its accuracy is considerably better than stateof-the-art results reported for binary-weight network (BWN) (Rastegari et al., 2016) and ternary weight network (TWN) (Li & Liu, 2016).",
      "startOffset" : 198,
      "endOffset" : 222
    }, {
      "referenceID" : 16,
      "context" : "Method Bit-width Top-1 error Top-5 error BWN(Rastegari et al., 2016) 1 39.",
      "startOffset" : 44,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "In the literature, recently proposed deep compression method (Han et al., 2016) reports so far best results on network compression without loss of model accuracy.",
      "startOffset" : 61,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "(2016) is a hybrid network compression solution combining three different techniques, namely network pruning (Han et al., 2015), vector quantization (Gong et al.",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : ", 2015), vector quantization (Gong et al., 2014) and Huffman coding.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "We solved this problem by our dynamic network surgery (DNS) method (Guo et al., 2016) which achieves about 7× speed-up in training and improves the performance of network pruning from 9× to 17.",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "Besides, we also perform a set of experiments on AlexNet to compare the performance of our INQ and vector quantization (Gong et al., 2014).",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "This is consistent with the results reported in (Gong et al., 2014).",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "In the literature, recently proposed deep compression method (Han et al., 2016) reports so far best results on network compression without loss of model accuracy. Therefore, the last set of experiments is conducted to explore the potential of our INQ for much better deep compression. Note that Han et al. (2016) is a hybrid network compression solution combining three different techniques, namely network pruning (Han et al.",
      "startOffset" : 62,
      "endOffset" : 313
    }, {
      "referenceID" : 5,
      "context" : ", 2015), vector quantization (Gong et al., 2014) and Huffman coding. Taking AlexNet as an example, network pruning gets 9× compression, however this result is mainly obtained from the fully connected layers. Actually its compression performance on the convolutional layers is less than 3× (as can be seen in the Table 4 of Han et al. (2016)).",
      "startOffset" : 30,
      "endOffset" : 341
    }, {
      "referenceID" : 5,
      "context" : ", 2015), vector quantization (Gong et al., 2014) and Huffman coding. Taking AlexNet as an example, network pruning gets 9× compression, however this result is mainly obtained from the fully connected layers. Actually its compression performance on the convolutional layers is less than 3× (as can be seen in the Table 4 of Han et al. (2016)). Besides, network pruning is realized by separately performing pruning and re-training in an iterative way, which is very time-consuming. It will cost at least several weeks for compressing AlexNet. We solved this problem by our dynamic network surgery (DNS) method (Guo et al., 2016) which achieves about 7× speed-up in training and improves the performance of network pruning from 9× to 17.7×. In Han et al. (2016), after network pruning, vector quantization further improves compression ratio from 9× to 27×, and Huffman coding finally boosts compression ratio up to 35×.",
      "startOffset" : 30,
      "endOffset" : 759
    }, {
      "referenceID" : 5,
      "context" : ", 2015), vector quantization (Gong et al., 2014) and Huffman coding. Taking AlexNet as an example, network pruning gets 9× compression, however this result is mainly obtained from the fully connected layers. Actually its compression performance on the convolutional layers is less than 3× (as can be seen in the Table 4 of Han et al. (2016)). Besides, network pruning is realized by separately performing pruning and re-training in an iterative way, which is very time-consuming. It will cost at least several weeks for compressing AlexNet. We solved this problem by our dynamic network surgery (DNS) method (Guo et al., 2016) which achieves about 7× speed-up in training and improves the performance of network pruning from 9× to 17.7×. In Han et al. (2016), after network pruning, vector quantization further improves compression ratio from 9× to 27×, and Huffman coding finally boosts compression ratio up to 35×. For fair comparison, we combine our proposed INQ and DNS, and compare the resulting method with Han et al. (2016). Detailed results are summarized in Table 5.",
      "startOffset" : 30,
      "endOffset" : 1031
    }, {
      "referenceID" : 5,
      "context" : ", 2015), vector quantization (Gong et al., 2014) and Huffman coding. Taking AlexNet as an example, network pruning gets 9× compression, however this result is mainly obtained from the fully connected layers. Actually its compression performance on the convolutional layers is less than 3× (as can be seen in the Table 4 of Han et al. (2016)). Besides, network pruning is realized by separately performing pruning and re-training in an iterative way, which is very time-consuming. It will cost at least several weeks for compressing AlexNet. We solved this problem by our dynamic network surgery (DNS) method (Guo et al., 2016) which achieves about 7× speed-up in training and improves the performance of network pruning from 9× to 17.7×. In Han et al. (2016), after network pruning, vector quantization further improves compression ratio from 9× to 27×, and Huffman coding finally boosts compression ratio up to 35×. For fair comparison, we combine our proposed INQ and DNS, and compare the resulting method with Han et al. (2016). Detailed results are summarized in Table 5. When combing our proposed INQ and DNS, we achieve much better compression results compared with Han et al. (2016). Specifically, with 5-bit quantization, we can achieve 53× compression with slightly larger gains both in top-5 and top-1 recognition rates, yielding 51.",
      "startOffset" : 30,
      "endOffset" : 1190
    }, {
      "referenceID" : 5,
      "context" : ", 2015), vector quantization (Gong et al., 2014) and Huffman coding. Taking AlexNet as an example, network pruning gets 9× compression, however this result is mainly obtained from the fully connected layers. Actually its compression performance on the convolutional layers is less than 3× (as can be seen in the Table 4 of Han et al. (2016)). Besides, network pruning is realized by separately performing pruning and re-training in an iterative way, which is very time-consuming. It will cost at least several weeks for compressing AlexNet. We solved this problem by our dynamic network surgery (DNS) method (Guo et al., 2016) which achieves about 7× speed-up in training and improves the performance of network pruning from 9× to 17.7×. In Han et al. (2016), after network pruning, vector quantization further improves compression ratio from 9× to 27×, and Huffman coding finally boosts compression ratio up to 35×. For fair comparison, we combine our proposed INQ and DNS, and compare the resulting method with Han et al. (2016). Detailed results are summarized in Table 5. When combing our proposed INQ and DNS, we achieve much better compression results compared with Han et al. (2016). Specifically, with 5-bit quantization, we can achieve 53× compression with slightly larger gains both in top-5 and top-1 recognition rates, yielding 51.43%/96.30% absolute improvement in compression performance compared with full version/fair version (i.e., the combination of network pruning and vector quantization) of Han et al. (2016), respectively.",
      "startOffset" : 30,
      "endOffset" : 1530
    }, {
      "referenceID" : 8,
      "context" : "Method Bit-width(Conv/FC) Compression ratio Decrease in top-1/top5 error Han et al. (2016) (P+Q) 8/5 27× 0.",
      "startOffset" : 73,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "Method Bit-width(Conv/FC) Compression ratio Decrease in top-1/top5 error Han et al. (2016) (P+Q) 8/5 27× 0.00%/0.03% Han et al. (2016) (P+Q+H) 8/5 35× 0.",
      "startOffset" : 73,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : "Method Bit-width(Conv/FC) Compression ratio Decrease in top-1/top5 error Han et al. (2016) (P+Q) 8/5 27× 0.00%/0.03% Han et al. (2016) (P+Q+H) 8/5 35× 0.00%/0.03% Han et al. (2016) (P+Q+H) 8/4 -0.",
      "startOffset" : 73,
      "endOffset" : 181
    }, {
      "referenceID" : 8,
      "context" : "Method Bit-width(Conv/FC) Compression ratio Decrease in top-1/top5 error Han et al. (2016) (P+Q) 8/5 27× 0.00%/0.03% Han et al. (2016) (P+Q+H) 8/5 35× 0.00%/0.03% Han et al. (2016) (P+Q+H) 8/4 -0.01%/0.00% Our method (P+Q) 5/5 53× 0.08%/0.03% Han et al. (2016) (P+Q+H) 4/2 -1.",
      "startOffset" : 73,
      "endOffset" : 261
    }, {
      "referenceID" : 5,
      "context" : "Last but not least, the final weights for vector quantization (Gong et al., 2014), network pruning (Han et al.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : ", 2014), network pruning (Han et al., 2015) and deep compression (Han et al.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : ", 2015) and deep compression (Han et al., 2016) are still floating-point values, but the final weights for our INQ are in the form of either powers of two or zero.",
      "startOffset" : 29,
      "endOffset" : 47
    } ],
    "year" : 2017,
    "abstractText" : "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pretrained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variablelength encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two) 1, our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.",
    "creator" : "LaTeX with hyperref package"
  }
}