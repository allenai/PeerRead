{
  "name" : "1506.04364.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Localized Multiple Kernel Learning—A Convex Approach",
    "authors" : [ "Yunwen Lei", "Alexander Binder", "Ürün Dogan", "Marius Kloft" ],
    "emails" : [ "yunwelei@cityu.edu.hk", "alexander_binder@sutd.edu.sg", "udogan@microsoft.com", "kloft@hu-berlin.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Multiple kernel learning, Localized algorithms, Generalization analysis"
    }, {
      "heading" : "1 Introduction",
      "text" : "Kernel-based methods such as support vector machines have found diverse applications due to their distinct merits such as the descent computational complexity, high usability, and the solid mathematical foundation [e.g., 44]. The performance of such algorithms, however, crucially depends on the involved kernel function as it intrinsically specifies the feature space where the learning process is implemented, and thus provides a similarity measure on the input space. Yet in the standard setting of these methods the choice of the involved kernel is typically left to the user.\nA substantial step toward the complete automatization of kernel-based machine learning is achieved in Lanckriet et al. [32], who introduce the multiple kernel learning (MKL) framework [15]. MKL offers a principal way of encoding complementary information with distinct base kernels and automatically learning an optimal combination of those [45]. MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56]. While early sparsity-inducing approaches failed to live up to its expectations in terms of improvement ∗yunwelei@cityu.edu.hk †alexander_binder@sutd.edu.sg ‡udogan@microsoft.com §kloft@hu-berlin.de\nar X\niv :1\n50 6.\n04 36\n4v 2\n[ cs\n.L G\n] 1\n3 O\nover uniform combinations of kernels [cf. 9, and references therein], it was shown that improved predictive accuracy can be achieved by employing appropriate regularization [28, 31].\nCurrently, most of the existing algorithms fall into the global setting of MKL, in the sense that all input instances share the same kernel weights. However, this ignores the fact that instances may require sampleadaptive kernel weights.\nFor instance, consider the two images of a horses given to the right. Multiple kernels can be defined, capturing the shapes in the image and the color distribution over various channels. On the image to the left, the depicted horse and the image backgrounds exhibit distinctly different color distributions, while for the image to the right the contrary is the case. Hence, a color kernel is more significant to detect a horse in the image to the left than for the image the right. This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].\nExisting approaches to localized MKL (reviewed in Section 1.1) optimize non-convex objective functions. This puts their generalization ability into doubt. Indeed, besides the recent work by [34], the generalization performance of localized MKL algorithms (as measured through large-deviation bounds) is poorly understood, which potentially could make these algorithms prone to overfitting. Further potential disadvantages of non-convex localized MKL approaches include computationally difficulty in finding good local minima and the induced lack of reproducibility of results (due to varying local optima).\nThis paper presents a convex formulation of localized multiple kernel learning, which is formulated as a single convex optimization problem over a precomputed cluster structure, obtained through a potentially convex or non-convex clustering method. We derive an efficient optimization algorithm based on Fenchel duality. Using Rademacher complexity theory, we establish large-deviation inequalities for localized MKL, showing that the smoothness in the cluster membership assignments crucially controls the generalization error. Computational experiments on data from the domains of computational biology and computer vision show that the proposed convex approach can achieve higher prediction accuracies than its global and nonconvex local counterparts (up to +5% accuracy for splice site detection)."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Gönen and Alpaydin [14] initiate the work on localized MKL by introducing gating models\nf(x) = M∑ m=1 ηm(x; v)〈wm, φm(x)〉+ b, ηm(x; v) ∝ exp(〈vm, x〉+ vm0)\nto achieve local assignments of kernel weights, resulting in a non-convex MKL problem. To not overly respect individual samples, Yang et al. [55] give a group-sensitive formulation of localized MKL, where kernel weights vary at, instead of the example level, the group level. Mu and Zhou [42] also introduce a non-uniform MKL allowing the kernel weights to vary at the cluster-level and tune the kernel weights under the graph embedding framework. Han and Liu [19] built on Gönen and Alpaydin [14] by complementing the spatial-similarity-based kernels with probability confidence kernels reflecting the likelihood of examples belonging to the same class. Li et al. [36] propose a multiple kernel clustering method by maximizing local kernel alignments. Liu et al. [37] present sample-adaptive approaches to localized MKL, where kernels can be switched on/off at the example level by introducing a latent binary vector for each individual sample, which and the kernel weights are then jointly optimized via margin maximization principle. Moeller et al.\n[41] present a unified viewpoint of localized MKL by interpreting gating functions in terms of local reproducing kernel Hilbert spaces acting on the data. All the aforementioned approaches to localized MKL are formulated in terms of non-convex optimization problems, and deep theoretical foundations in the form of generalization error or excess risk bounds are unknown. Although Cortes et al. [11] present a convex approach to MKL based on controlling the local Rademacher complexity, the meaning of locality is different in Cortes et al. [11]: it refers to the localization of the hypothesis class, which can result in sharper excess risk bounds [25, 26], and is not related to localized multiple kernel learning. Liu et al. [38] extend the idea of sample-adaptive MKL to address the issue with missing kernel information on some examples. More recently, Lei et al. [34] propose a MKL method by decoupling the locality structure learning with a hard clustering strategy from optimizing the parameters in the spirit of multi-task learning. They also develop the first generalization error bounds for localized MKL."
    }, {
      "heading" : "2 Convex Localized Multiple Kernel Learning",
      "text" : ""
    }, {
      "heading" : "2.1 Problem setting and notation",
      "text" : "Suppose that we are given n training samples (x1, y1), . . . , (xn, yn) that are partitioned into l disjoint clusters S1, . . . , Sl in a probabilistic manner, meaning that, for each cluster Sj , we have a function cj : X → [0, 1] indicating the likelihood of x falling into cluster j, i.e., ∑ j∈Nl cj(x) = 1 for all x ∈ X . Here, for any d ∈ N, we introduce the notation Nd = {1, . . . , d}. Suppose that we are given M base kernels k1, . . . , kM with km(x, x̃) = 〈φm(x), φm(x̃)〉km , corresponding to linear models fj(x) = 〈wj , φ(x)〉+b =∑ m∈NM 〈w (m) j , φm(x)〉 + b, where wj = (w (1) j , . . . , w (M) j ) and φ = (φ1, . . . , φM ). Then we consider the following proposed model, which is a weighted combination of these l local models:\nf(x) = ∑ j∈Nl cj(x)fj(x) = ∑ j∈Nl cj(x) [ ∑ m∈NM 〈w(m)j , φm(x)〉 ] + b. (1)"
    }, {
      "heading" : "2.2 Proposed convex localized MKL method",
      "text" : "Using the above notation, the proposed convex localized MKL model can be formulated as follows.\nProblem 1 (CONVEX LOCALIZED MULTIPLE KERNEL LEARNING (CLMKL)—PRIMAL). Let C > 0 and p ≥ 1. Given a loss function `(t, y) : R×Y → R convex w.r.t. the first argument and cluster likelihood functions cj : X → [0, 1], j ∈ Nl, solve\ninf w,t,β,b ∑ j∈Nl ∑ m∈NM ‖w(m)j ‖22 2βjm + C ∑ i∈Nn `(ti, yi)\ns.t. βjm ≥ 0, ∑ m∈NM\nβpjm ≤ 1 ∀j ∈ Nl,m ∈ NM∑ j∈Nl cj(xi) [ ∑ m∈NM 〈w(m)j , φm(xi)〉 ] + b = ti, ∀i ∈ Nn.\n(P)\nThe core idea of the above problem is to use cluster likelihood functions for each example and separate `p-norm constraint on the kernel weights βj := (βj1, . . . , βjM ) for each cluster j [31] . Thus each instance can obtain separate kernel weights. The above problem is convex, since a quadratic over a linear function is convex [e.g., 6, p.g. 89]. Note that Slater’s condition can be directly checked, and thus strong duality holds."
    }, {
      "heading" : "2.3 Dualization",
      "text" : "In this section we derive a dual representation of Problem 1. We consider two levels of duality: a partially dualized problem, with fixed kernel weights βjm, and the entirely dualized problem with respect to all occurring primal variables. From the former we derive an efficient two-step optimization scheme (Section 3). The latter allows us to compute the duality gap and thus to obtain a sound stopping condition for the proposed algorithm. We focus on the entirely dualized problem here. The partial dualization is deferred to Supplemental Material C.\nDual CLMKL Optimization Problem Forwj = (w (1) j , . . . , w (M) j ), we define the `2,p-norm by ‖wj‖2,p := ‖(‖w(1)j ‖k1 , . . . , ‖w (M) j ‖kM )‖p = ( ∑ m∈NM ‖w (m) j ‖ p km ) 1 p . For a function h, we denote by h∗(x) = supµ[x\n>µ− h(µ)] its Fenchel-Legendre conjugate. This results in the following dual.\nProblem 2 (CLMKL—DUAL). The dual problem of (P) is given by\nsup∑ i∈Nn αi=0\n{ − C ∑ i∈Nn `∗(−αi C , yi)− 1 2 ∑ j∈Nl ∥∥∥( ∑ i∈Nn αicj(xi)φm(xi) )M m=1 ∥∥∥2 2, 2pp−1 } . (D)\nDualization. Using Lemma A.2 from Supplemental Material A.1 to express the optimal βjm in terms of w\n(m) j , the problem (P) is equivalent to\ninf w,t,b\n1\n2 ∑ j∈Nl ( ∑ m∈NM ‖w(m)j ‖ 2p p+1 2 ) p+1 p + C ∑ i∈Nn `(ti, yi)\ns.t. ∑ j∈Nl [ cj(xi) ∑ m∈NM 〈w(m)j , φm(xi)〉 ] + b = ti, ∀i ∈ Nn. (2)\nIntroducing Lagrangian multipliers αi, i ∈ Nn, the Lagrangian saddle problem of Eq. (2) is\nsup α inf w,t,b\n1\n2 ∑ j∈Nl ( ∑ m∈NM ‖w(m)j ‖ 2p p+1 2 ) p+1 p + C ∑ i∈Nn `(ti, yi)− ∑ i∈Nn αi (∑ j∈Nl cj(xi) ∑ m∈NM 〈w(m)j , φm(xi)〉+ b− ti )\n= sup α\n{ − C ∑ i∈Nn sup ti [−`(ti, yi)− 1 C αiti]− sup b ∑ i∈Nn αib−\nsup w [ ∑ j∈Nl ∑ m∈NM 〈 w (m) j , ∑ i∈Nn αicj(xi)φm(xi) 〉 − 1 2 ∑ j∈Nl ( ∑ m∈NM ‖w(m)j ‖ 2p p+1 2 ) p+1 p ]}\n(3)\ndef = sup∑\ni∈Nn αi=0\n{ − C ∑ i∈Nn `∗(−αi C , yi)− ∑ j∈Nl [1 2 ∥∥( ∑ i∈Nn αicj(xi)φm(xi) )M m=1 ∥∥2 2, 2pp+1 ]∗}\nThe result (2) now follows by recalling that for a norm ‖ · ‖, its dual norm ‖ · ‖∗ is defined by ‖x‖∗ = sup‖µ‖=1〈x, µ〉 and satisfies: ( 12‖ · ‖ 2)∗ = 12‖ · ‖ 2 ∗ [6]. Furthermore, it is straightforward to show that ‖ · ‖2, 2pp−1 is the dual norm of ‖ · ‖2, 2pp+1 ."
    }, {
      "heading" : "2.4 Representer Theorem",
      "text" : "We can use the above derivation to obtain a lower bound on the optimal value of the primal optimization problem (P), from which we can compute the duality gap using the theorem below. The proof is given in Supplemental Material A.2.\nTheorem 3 (REPRESENTER THEOREM). For any dual variable (αi)ni=1 in (D), the optimal primal variable {w(m)j (α)} l,M j,m=1 in the Lagrangian saddle problem (3) can be represented as\nw (m) j (α)= [ ∑ m̃∈NM ‖ ∑ i∈Nn αicj(xi)φm̃(xi)‖ 2p p−1 2 ]− 1p ∥∥∑ i∈Nn αicj(xi)φm(xi) ∥∥ 2p−1 2 [∑ i∈Nn αicj(xi)φm(xi) ] ."
    }, {
      "heading" : "2.5 Support-Vector Classification",
      "text" : "For the hinge loss, the Fenchel-Legendre conjugate becomes `∗(t, y) = ty (a function of t) if −1 ≤ t y ≤ 0 and ∞ elsewise. Hence, for each i, the term `∗(−αiC , yi) translates to − αi Cyi\n, provided that 0 ≤ αiyi ≤ C. With a variable substitution of the form αnewi = αi yi , the complete dual problem (D) reduces as follows.\nProblem 4 (CLMKL—SVM FORMULATION). For the hinge loss, the dual CLMKL problem (D) is given by:\nsup α:0≤α≤C, ∑ i∈Nn αiyi=0 − 1 2 ∑ j∈Nl ∥∥∥( ∑ i∈Nn αiyicj(xi)φm(xi) )M m=1 ∥∥∥2 2, 2pp−1 + ∑ i∈Nn αi, (4)\nA corresponding formulation for support-vector regression is given in Supplemental Material B."
    }, {
      "heading" : "3 Optimization Algorithms",
      "text" : "As pioneered in Sonnenburg et al. [45], we consider here a two-layer optimization procedure to solve the problem (P) where the variables are divided into two groups: the group of kernel weights {βjm}l,Mj,m=1 and the group of weight vectors {w(m)j } l,M j,m=1. In each iteration, we alternatingly optimize one group of variables while fixing the other group of variables. These iterations are repeated until some optimality conditions are satisfied. To this aim, we need to find efficient strategies to solve the two subproblems.\nIt is not difficult to show (cf. Supplemental Material C) that, given fixed kernel weights β = (βjm), the CLMKL dual problem is given by\nsup α: ∑ i∈Nn αi=0 −1 2 ∑ j∈Nl ∑ m∈NM βjm ∥∥∥ ∑ i∈Nn αicj(xi)φm(xi) ∥∥∥2 2 − C ∑ i∈Nn `∗(−αi C , yi), (5)\nwhich is a standard SVM problem using the kernel\nk̃(xi, xĩ) := ∑ m∈NM ∑ j∈Nl βjmcj(xi)cj(xĩ)km(xi, xĩ) (6)\nThis allows us to employ very efficient existing SVM solvers [8]. In the degenerate case with cj(x) ∈ {0, 1}, the kernel k̃ would be supported over those sample pairs belonging to the same cluster.\nNext, we show that, the subproblem of optimizing the kernel weights for fixed w(m)j and b has a closedform solution.\nProposition 5 (SOLUTION OF THE SUBPROBLEM W.R.T. THE KERNEL WEIGHTS). Given fixed w(m)j and b, the minimal βjm in optimization problem (P) is attained for\nβjm = ‖w(m)j ‖ 2 p+1 2 ( ∑ k∈NM ‖w(k)j ‖ 2p p+1 2 )− 1p . (7)\nWe defer the detailed proof to Supplemental Material A.3 due to lack of space. To apply Proposition 5 for updating βjm, we need to compute the norm of w (m) j , and this can be accomplished by the following representation of w(m)j given fixed βjm: (cf. Supplemental Material C)\nw (m) j = βjm ∑ i∈Nn αicj(xi)φm(xi). (8)\nThe prediction function is then derived by plugging the above representation into Eq. (1). The resulting optimization algorithm for CLMKL is shown in Algorithm 1. The algorithm alternates between solving an SVM subproblem for fixed kernel weights (Line 4) and updating the kernel weights in a closed-form manner (Line 6). To improve the efficiency, we start with a crude precision and gradually improve the precision of solving the SVM subproblem. The proposed optimization approach can potentially be extended to an interleaved algorithm where the optimization of the MKL step is directly integrated into the SVM solver. Such a strategy can increase the computational efficiency by up to 1-2 orders of magnitude (cf. [45] Figure 7 in Kloft et al. [31]). The requirement to compute the kernel k̃ at each iteration can be further relaxed by updating only some randomly selected kernel elements.\nAlgorithm 1: Training algorithm for convex localized multiple kernel learning (CLMKL). input: examples {(xi, yi)ni=1} ⊂ ( X × {−1, 1} )n together with the likelihood functions {cj(x)}lj=1, M base kernels k1, . . . , kM .\n1 initialize βjm = p √ 1/M,w (m) j = 0 for all j ∈ Nl,m ∈ NM 2 while Optimality conditions are not satisfied do 3 calculate the kernel matrix k̃ by Eq. (6) 4 compute α by solving canonical SVM with k̃ 5 compute ‖w(m)j ‖ 2 2 for all j,m with w (m) j given by Eq. (8) 6 update βjm for all j,m according to Eq. (7) 7 end\nAn alternative strategy would be to directly optimize (2) (without the need of a two-step wrapper approach). Such an approach has been presented in Sun et al. [49] in the context of `p-norm MKL."
    }, {
      "heading" : "3.1 Convergence Analysis of the Algorithm",
      "text" : "The theorem below, which is proved in Supplemental Material A.4, shows convergence of Algorithm 1. The core idea is to view Algorithm 1 as an example of the classical block coordinate descent (BCD) method, convergence of which is well understood.\nTheorem 6 (CONVERGENCE ANALYSIS OF ALGORITHM 1). Assume that\n(B1) the feature map φm(x) is of finite dimension, i.e, φm(x) ∈ Rem , em <∞,∀m ∈ NM (B2) the loss function ` is convex, continuous w.r.t. the first argument and `(0, y) <∞,∀y ∈ Y (B3) any iterate βjm traversed by Algorithm 1 has βjm > 0 (B4) the SVM computation in line 4 of Algorithm 1 is solved exactly in each iteration.\nThen, any limit point of the sequence traversed by Algorithm 1 minimizes the problem (P)."
    }, {
      "heading" : "3.2 Runtime Complexity Analysis",
      "text" : "At each iteration of the training stage, we need O(n2Ml) operations to calculate the kernel (6), O(n2ns) operations to solve a standard SVM problem, O(Mln2s) operations to calculate the norm according to the representation (8) and O(Ml) operations to update the kernel weights. Thus, the computational cost at each iteration is O(n2Ml). The time complexity at the test stage is O(ntnsMl). Here, ns and nt are the number of support vectors and test points, respectively."
    }, {
      "heading" : "4 Generalization Error Bounds",
      "text" : "In this section we present generalization error bounds for our approach. We give a purely data-dependent bound on the generalization error, which is obtained using Rademacher complexity theory [3]. To start with, our basic strategy is to plug the optimal βjm established in Eq. (7) into (P), so as to equivalently rewrite (P) as a block-norm regularized problem as follows:\nmin w,b\n1\n2 ∑ j∈Nl [ ∑ m∈NM ‖w(m)j ‖ 2p p+1 2 ] p+1 p + C ∑ i∈Nn ` (∑ j∈Nl cj(xi) [ ∑ m∈NM 〈w(m)j ,φm(xi)〉 ] + b, yi ) . (9)\nSolving (9) corresponds to empirical risk minimization in the following hypothesis space:\nHp,D := Hp,D,M = { fw : x → ∑ j∈Nl cj(xi) [ ∑ m∈NM 〈w(m)j , φm(xi)〉 ] : ∑ j∈Nl ‖wj‖22, 2pp+1 ≤ D } .\nThe following theorem establishes the Rademacher complexity bounds for the function classHp,D, from which we derive generalization error bounds for CLMKL in Theorem 9. The proofs of the Theorems 8, 9 are given in Supplemental Material A.5.\nDefinition 7. For a fixed sample S = (x1, . . . , xn), the empirical Rademacher complexity of a hypothesis space H is defined as\nR̂n(H) := Eσ sup f∈H\n1\nn ∑ i∈Nn σif(xi),\nwhere the expectation is taken w.r.t. σ = (σ1, . . . , σn)> with σi, i ∈ Nn, being a sequence of independent uniform {±1}-valued random variables.\nTheorem 8 (CLMKL RADEMACHER COMPLEXITY BOUNDS). The empirical Rademacher complexity of Hp,D can be controlled by\nR̂n(Hp,D) ≤ √ D\nn inf\n2≤t≤ 2pp−1 ( t ∑ j∈Nl ∥∥∥( ∑ i∈Nn c2j (xi)km(xi, xi) )M m=1 ∥∥∥ t 2 )1/2 . (10)\nIf, additionally, km(x, x) ≤ B for any x ∈ X and any m ∈ NM , then we have\nR̂n(Hp,D) ≤ √ DB\nn inf\n2≤t≤ 2pp−1\n( tM 2 t ∑ j∈Nl ∑ i∈Nn c2j (xi) )1/2 .\nTheorem 9 (CLMKL GENERALIZATION ERROR BOUNDS). Assume that km(x, x) ≤ B, ∀m ∈ NM , x ∈ X . Suppose the loss function ` is L-Lipschitz and bounded by B`. Then, the following inequality holds with probability larger than 1− δ over samples of size n for all classifiers h ∈ Hp,D:\nE`(h) ≤ E`,z(h) +B`\n√ log(2/δ)\n2n + 2\n√ DB\nn inf\n2≤t≤ 2pp−1\n( tM 2 t [ ∑ j∈Nl ∑ i∈Nn c2j (xi) ])1/2 ,\nwhere E`(h) := E[`(h(x), y)] and E`,z(h) := 1n ∑ i∈Nn `(h(xi), yi).\nThe above bound enjoys a mild dependence on the number of kernels. One can show (cf. Supplemental Material A.5) that the dependence is O(logM) for p ≤ (logM − 1)−1 logM and O(M p−1 2p ) otherwise. In particular, the dependence is logarithmically for p = 1 (sparsity-inducing CLMKL). These dependencies recover the best known results for global MKL algorithms in Cortes et al. [10], Kloft and Blanchard [25], Kloft et al. [31].\nThe bounds of Theorem 8 exhibit a strong dependence on the likelihood functions, which inspires us to derive a new algorithmic strategy as follows. Consider the special case where cj(x) takes values in {0, 1} (hard cluster membership assignment), and thus the term determining the bound has ∑ j∈Nl ∑ m∈NM c 2 j (xi) = n. On the other hand, if cj(x) ≡ 1l , j ∈ Nl (uniform cluster membership assignment), we have the favorable term ∑ j∈N ∑ i∈Nn c 2 j (xi) = n l . This motivates us to introduce a parameter τ controlling the complexity of the bound by considering likelihood functions of the form\ncj(x) ∝ exp(−τdist2(x, Sj)), (11)\nwhere dist(x, Sj) is the distance between the example x and the cluster Sj . By letting τ = 0 and τ = ∞, we recover uniform and hard cluster assignments, respectively. Intermediate values of τ correspond to more balanced cluster assignments. As illustrated by Theorem 8, by tuning τ we optimally adjust the resulting models’ complexities."
    }, {
      "heading" : "5 Empirical Analysis and Applications",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "We implement the proposed convex localized MKL (CLMKL) algorithm in MATLAB and solve the involved canonical SVM problem with LIBSVM [8]. The clusters {S1, . . . , Sl} are computed through kernel k-means [e.g., 12], but in principle other clustering methods (including convex ones such as Hocking et al. [21]) could be used. To further diminish k-means’ potential fluctuations (which are due to random initialization of the cluster means), we repeat kernel k-means t times, and choose the one with minimal clustering error (the summation of the squared distance between the examples and the associated nearest cluster) as the final partition {S1, . . . , Sl}. To tune the parameter τ in (11) in a uniform manner, we introduce the notation\nAE(τ) := 1\nnl ∑ i∈Nn ∑ j∈Nl exp(−τdist2(xi, Sj)) maxj̃∈Nl exp(−τdist 2(xi, Sj̃))\nto measure the average evenness (or average excess over hard partition) of the likelihood function. It can be checked that AE(τ) is a strictly decreasing function of τ , taking value 1 at the point τ = 0 and l−1 at the point τ = ∞. Instead of tuning the parameter τ directly, we propose to tune the average excess/evenness over a subset in [l−1, 1]. The associated parameter τ are then fixed by the standard binary search algorithm.\nWe compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case. We optimize `pnorm MKL and CLMKL until the relative duality gap drops below 0.001. The calculation of the gradients in LMKL [14] requires O(n2M2d) operations, which scales poorly, and the definition of the gating model\nrequires the information of primitive features, which is not available for the biological applications studied below, all of which involve string kernels. In Supplemental Material D, we therefore give a fast and general formulation of LMKL, which requires only O(n2M) operations per iteration. Our implementation of which is available from the following webpage, together with our CLMKL implementation and scripts to reproduce the experiments: https://www.dropbox.com/sh/hkkfa0ghxzuig03/AADRdtSSdUSm8hfVbsdjcRqva?dl=0.\nIn the following we report detailed results for various real-world experiments. Further details are shown in Supplemental Material E."
    }, {
      "heading" : "5.2 Splice Site Recognition",
      "text" : "Our first experiment aims at detecting splice sites in the organism Caenorhabditis elegans, which is an important task in computational gene finding as splice sites are located on the DNA strang right at the boundary of exons (which code for proteins) and introns (which do not). We experiment on the mkl-splice data set, which we download from http://mldata.org/repository/data/viewslug/mkl-splice/. It includes 1000 splice site instances and 20 weighted-degree kernels with degrees ranging from 1 to 20 [4]. The experimental setup for this experiment is as follows. We create random splits of this dataset into training set, validation set and test set, with size of training set traversing over the set {50, 100, 200, 300, . . . , 800}. We apply kernel-kmeans with uniform kernel to generate a partition with l = 3 clusters for both CLMKL and HLMKL, and use this kernel to define the gating model in LMKL. To be consistent with previous studies, we use the area under the ROC curve (AUC) as an evaluation criterion. We tune the SVM regularization parameter from 10{−1,−0.5,...,2}, and the average evenness over the interval [0.4, 0.8] with eight linearly equally spaced points, based on the AUCs on the validation set. All the base kernel matrices are multiplicatively normalized before training. We repeat the experiment 50 times, and report mean AUCs on the test set as well as standard deviation. Figure 1 (a) shows the results as a function of the training set size n.\nWe observe that CLMKL achieves, for all n, a significant gain over all baselines. This improvement is especially strong for small n. For n = 50, CLMKL attains 90.9% accuracy, while the best baseline only achieves 85.4%, improving by 5.5%. Detailed results with standard deviation are reported in Table 1. A hypothetical explanation of the improvement from CLMKL is that splice sites are characterized by nucleotide sequences—so-called motifs—the length of which may differ from site to site [47]. The 20\nemployed kernels count matching subsequences of length 1 to 20, respectively. For sites characterized by smaller motifs, low-degree WD-kernels are thus more effective than high-degree ones, and vice versa for sites containing longer motifs."
    }, {
      "heading" : "5.3 Transcription Start Site Detection",
      "text" : "Our next experiment aims at detecting transcription start sites (TSS) of RNA Polymerase II binding genes in genomic DNA sequences. We experiment on the TSS data set, which we downloaded from http:// mldata.org/repository/data/viewslug/tss/. This data set, which is included in the larger study of [46], comes with 5 kernels. The SVM based on the uniform combination of these 5 kernels was found to have the highest overall performance among 19 promoter prediction programs [1]. It therefore constitutes a strong baseline. To be consistent with previous studies [1, 24, 46], we use the area under the ROC curve (AUC) as an evaluation criterion. We consider the same experimental setup as in the splice detection experiment. The gating function and the partition are computed with the TSS kernel, which carries most of the discriminative information [46]. All kernel matrices were normalized with respect to their trace, prior to the experiment.\nFigure 1 (b) shows the AUCs on the test data sets as a function of the number of training examples.We observe that CLMKL attains a consistent improvement over other competing methods. Again, this improvement is most significant when n is small. Detailed results with standard deviation are reported in Table 2."
    }, {
      "heading" : "5.4 Protein Fold Prediction",
      "text" : "Protein fold prediction is a key step towards understanding the function of proteins, as the folding class of a protein is closely linked with its function; thus it is crucial for drug design. We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25]. This dataset consists of 27 fold classes with 311 proteins used for training and 383 proteins for testing. We use exactly the same 12 kernels as in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25] reflecting different features, such as van der Waals volume, polarity and hydrophobicity. We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], which is detailed in Supplementary Material E.1. We report the mean prediction accuracies, as well as standard deviations in Table 3.\nThe results show that CLMKL surpasses regular `p-norm MKL for all values of p, and achieves accuracies up to 0.6% higher than the one reported in Kloft [24], which is higher than the initially reported accuracies in Campbell and Ying [7]. LMKL works poorly in this dataset, possibly because LMKL based on precomputed custom kernels requires to optimize nM additional variables, which may overfit."
    }, {
      "heading" : "5.5 Visual Image Categorization—UIUC Sports",
      "text" : "We experiment on the UIUC Sports event dataset [35] consisting of 1574 images, belonging to 8 image classes of sports activities. We compute 9 χ2-kernels based on SIFT features and global color histograms, which is described in detail in Supplemental Material E.2, where we also give background on the experimental setup.\nFrom the results shown in Table 4, we observe that CLMKL achieves a performance improvement by 0.26% over the `p-norm MKL baseline while localized MKL as in Gönen and Alpaydin [14] underperforms the MKL baseline."
    }, {
      "heading" : "5.6 Execution Time Experiments",
      "text" : "To demonstrate the efficiency of the proposed implementation, we compare the training time for UNIF, LMKL, `p-norm MKL, HLMKL and CLMKL on the TSS dataset. We fix the regularization parameter"
    }, {
      "heading" : "6 Conclusions",
      "text" : "Localized approaches to multiple kernel learning allow for flexible distribution of kernel weights over the input space, which can be a great advantage when samples require varying kernel importance. As we show in this paper, this can be the case in image recognition and several computational biology applications. However, almost prevalent approaches to localized MKL require solving difficult non-convex optimization problems, which makes them potentially prone to overfitting as theoretical guarantees such as generalization error bounds are yet unknown.\nIn this paper, we propose a theoretically grounded approach to localized MKL, consisting of two subsequent steps: 1. clustering the training instances and 2. computation of the kernel weights for each cluster through a single convex optimization problem. For which we derive an efficient optimization algorithm based on Fenchel duality. Using Rademacher complexity theory, we establish large-deviation inequalities for localized MKL, showing that the smoothness in the cluster membership assignments crucially controls the generalization error. The proposed method is well suited for deployment in the domains of computer vision and computational biology. For splice site detection, CLMKL achieves up to 5% higher accuracy than its global and non-convex localized counterparts.\nFuture work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features."
    }, {
      "heading" : "Acknowledgments",
      "text" : "YL acknowledges support from the NSFC/RGC Joint Research Scheme [RGC Project No. N_CityU120/14 and NSFC Project No. 11461161006]. AB acknowledges support from Singapore University of Technology and Design Startup Grant SRIS15105. MK acknowledges support from the German Research Foundation (DFG) award KL 2698/2-1 and from the Federal Ministry of Science and Education (BMBF) awards 031L0023A and 031B0187B.\nSupplemental Material"
    }, {
      "heading" : "A Lemmata and Proofs",
      "text" : "A.1 Lemmata Used for Dualization in Section 2.3\nLetH1, . . . ,HM beM Hilbert spaces and p ≥ 1. Define the function gp(v1, . . . , vM ) : H1×· · ·×HM → R by\ngp(v1, . . . , vM ) = 1\n2 ‖(v1, . . . , vM )‖22,p, p ≥ 1.\nFor any p ≥ 1, denote by p∗ the conjugated exponent satisfying 1p + 1 p∗ = 1.\nLemma A.1. The gradient of gp is\n∂gp(v1, . . . , vM ) ∂vm = [ ∑ m̃∈NM ‖vm̃‖p2 ] 2 p−1‖vm‖p−22 vm.\nProof. By the chain rule, we have\n∂gp(v1, . . . , vM )\n∂vm =\n1\np [ ∑ m̃∈NM ‖vm̃‖p2 ] 2 p−1 ∂〈vm, vm〉 p 2 ∂vm\n= 1\n2 [ ∑ m̃∈NM ‖vm̃‖p2 ] 2 p−1 ∂〈vm, vm〉 ∂vm 〈vm, vm〉 p 2−1\n= [ ∑ m̃∈NM ‖vm̃‖p2 ] 2 p−1‖vm‖p−22 vm.\nLemma A.2 (Micchelli and Pontil 40). Let ai ≥ 0, i ∈ Nd and 1 ≤ r <∞. Then\nmin η:ηi≥0, ∑ i∈Nd ηri≤1 ∑ i∈Nd ai ηi = ( ∑ i∈Nd a r r+1 i )1+ 1r and the minimum is attained at ηi = a 1 r+1\ni (∑ k∈Nd a r r+1 k )− 1r .\nA.2 Proof of Representer Theorem (Theorem 3)\nProof of Theorem 3. In our derivation (3) of the dual problem, the variablewj(α) := ( w (1) j (α), . . . , w (M) j (α) ) should meet the optimality in the sense\nwj(α) = arg max v1∈H1,...,vM∈HM\n[〈 (vm) M m=1, ( n∑ i=1 αicj(xi)φm(xi) )M m=1 〉 − 1 2 ‖(vm)Mm=1‖22, 2pp+1 ] .\nSince (5f)−1 = 5f∗ for any convex function f and the Fenchel-conjugate of gp is gp∗ , we obtain\nwj(α) = 5g−12p p+1 ( ∑ i∈Nn αicj(xi)φ1(xi), . . . , ∑ i∈Nn αicj(xi)φM (xi) )\n= 5g 2p p−1 ( ∑ i∈Nn αicj(xi)φ1(xi), . . . , ∑ i∈Nn αicj(xi)φM (xi) ) Lem. A.1 =\n[ ∑ m̃∈NM ‖ ∑ i∈Nn αicj(xi)φm̃(xi)‖ 2p p−1 2 ]− 1p(∥∥ ∑ i∈Nn αicj(xi)φ1(xi) ∥∥ 2p−1 2 [ ∑ i∈Nn αicj(xi)φ1(xi) ] ,\n. . . , ∥∥ ∑ i∈Nn αicj(xi)φM (xi) ∥∥ 2p−1 2 [ ∑ i∈Nn αicj(xi)φM (xi) ]) .\nNote that the above derivation uses Lemma A.1 from Supplemental Material A.1.\nA.3 Proof of Proposition 5\nProof of Proposition 5. Fixing the variables w(m)j and b, the optimization problem (P) reduces to\nmin β ∑ j∈Nl ∑ m∈NM ‖w(m)j ‖22 2βjm s.t. ∑ m∈NM βpjm ≤ 1, βjm ≥ 0, ∀j ∈ Nl,m ∈ NM .\nThis problem can be decomposed into l independent subproblems, one at each locality. For example, the subproblem at the j-th locality is as follows\nmin β ∑ m∈NM ‖w(m)j ‖22 2βjm s.t. ∑ m∈NM βpjm ≤ 1, βjm ≥ 0, ∀m ∈ NM .\nApplying Lemma A.2 with αm = ‖w(m)j ‖22, ηm = βjm and r = p completes the proof.\nA.4 Proof of Theorem 6: Convergence of the CLMKL Optimization Algorithm\nThe following lemma is a direct consequence of Lemma 3.1 and Theorem 4.1 in Tseng [50].\nLemma A.3. Let f : Rd1+···+dR → R ∪ {∞} be a function. Put d = d1 + · · · + dR. Suppose that f can be decomposed into f(α1, . . . , αR) + ∑ r∈NR fr(αr) for some f0 : R\nd → R ∪ {∞} and fr : Rdr → R∪{∞}, r ∈ NR. Initialize the block coordinate descent method by α0 = (α01, . . . , α0R). Define the iterates αk = (αk1 , . . . , α k R) by\nαk+1r = arg min u∈Rdk f(αk+11 , . . . , α k+1 r−1 , u, α k r+1, . . . , α k R), ∀r ∈ NR, k ∈ N+. (A.1)\nAssume that\n(A1) f is convex and proper, i.e., f 6≡ ∞\n(A2) the sublevel set A0 := {α ∈ Rd : f(α) ≤ f(α0)} is compact and f is continuous on A0\n(A3) dom(f0) := {α ∈ Rd : f0(α) ≤ ∞} is open and f0 is continuously differentiable on dom(f0).\nThen, the minimizer in (A.1) exists and any limit point of the sequence (αk)k∈N+ minimizes f over A0.\nThe proof of Theorem 6 is a direct consequence of the above lemma.\nProof of Theorem 6. The primal problem (P) can be rewritten as follows:\ninf w,βj∈Θp,j∈Nl ∑ j∈Nl ∑ m∈NM ‖w(m)j ‖22 2βjm + C ∑ i∈Nn `( ∑ j∈Nl cj(xi) ∑ m∈NM 〈w(m)j , φm(xi)〉, yi), (A.2)\nwhere Θp = {(θ1, . . . , θM ) ∈ RM : θm ≥ 0, ‖(θm)Mm=1‖p ≤ 1}, and w (m) j ∈ Rem ,∀j ∈ Nl,m ∈ NM . Note that Eq. (A.2) can be written as the following unconstrained problem:\ninf w,β f(w, β), where f(w, β) = f0(w, β) + f1(w) + f2(β),\nwith\nf0(w, β) = ∑ j∈Nl ∑ m∈NM [‖w(m)j ‖22 2βjm + Iβjm≥0(βjm) ]\nf1(w) = C ∑ i∈Nn `( ∑ j∈Nl cj(xi) ∑ m∈NM 〈w(m)j , φm(xi)〉, yi) and f2(β) = ∑ j∈Nl I‖βj‖p≤1(βj).\nHere I is the indicator function, i.e., IS(s) = 0 if s ∈ S and∞ otherwise. Now, it remains to check the assumption (A1), (A2) and (A3) in Lemma A.3 for Algorithm 1.\nVALIDITY OF A1. It is known that a quadratic over a linear function is convex, so the term ∑ j∈Nl ∑ m∈NM ‖w(m)j ‖ 2 2 2βjm\nis convex. Also, since ` is convex w.r.t. the first argument and the term ∑ j∈Nl cj(xi) ∑ m∈NM 〈w (m) j , φm(xi)〉\nis a linear function ofw, we immediately know the term ∑ i∈Nn `( ∑ j∈Nl cj(xi) ∑ m∈NM 〈w (m) j , φm(xi)〉, yi) is convex. The convexity of f follows immediately. For the initial assignment w(0) = {w(m,0)j }j,m with w\n(m,0) j = 0 and β (0) = {β(0)jm}jm with β (0) jm = M −1/p, we know\nf(w(0), β(0)) = Cn sup y∈Y `(0, y) <∞\nand therefore f is proper. VALIDITY OF A2. Recall that our algorithm is initialized with w(0) = 0 and β(0) = M−1/p. For any (w, β) ∈ A0 := {(w̄, β̄) : f(w̄, β̄) ≤ Cn supy∈Y `(0, y)}, we have\n‖w(m)j ‖ 2 2 ≤ 2βjmCn sup y∈Y `(0, y) ≤ 2Cn sup y∈Y `(0, y),\nwhich, coupled with the constraint ‖(βjm)Mm=1‖p ≤ 1,∀j ∈ Nl, immediately shows that A0 is bounded. Furthermore, since f0, f1 and f2 are continuous on their respective domains, the function f is therefore continuous on A0 ⊂ dom(f0) ∩ dom(f1) ∩ dom(f2). It is also known that the preimage of a closed set is closed under a continuous function, from which we know the set A0 = f−1(−∞, f(w(0), β(0))] is closed. Any closed and bounded subset in Rd, d ∈ N is compact and thus A(0) is compact.\nVALIDITY OF A3. Clearly, dom(f0) = {(w, β) : β > 0} is open and f0 is continuously differentiable on dom(f0).\nA.5 Proof of Generalization Error Bounds (Theorem 8)\nIn this section we present the proof of the achieved generalization error bounds (Theorem 9 in the main text). Denote p̄ = 2pp+1 for any p ≥ 1 and observe that p̄ ≤ 2, which implies p̄\n∗ ≥ 2. To start with, we give a discussion on the interpretation and tightness of Rademacher complexity bounds in Theorem 8.\nInterpretation and Tightness of Rademacher complexity bounds It can be directly checked that the function x→ xM2/x is decreasing along the interval (0, 2 logM) and increasing along the interval (2 logM,∞). Therefore, under the assumption km(x, x) ≤ B the Rademacher complexity bounds thus satisfy the inequalities:\nR̂n(Hp,D) ≤ √ DB\nn ×  1 n √ 2eDB logM [ ∑ j∈Nl ∑ i∈Nn c 2 j (xi)], if p ≤ logM logM−1 ,\n1 n √ 2p p−1DBM p−1 p [ ∑ j∈Nl ∑ i∈Nn c 2 j (xi)], otherwise.\nIn particular, the former expression can be taken for p = 1, resulting in a mild logarithmic dependence on the number of kernels. Note that in the limiting case of just one cluster, i.e., l = 1, the Rademacher complexity bounds match the result by Cortes et al. [10], which was shown to be tight.\nThe proof of Theorem 8 is based on the following lemmata.\nLemma A.4 (Khintchine-Kahane inequality [23]). Let v1, . . . , vn ∈ H. Then, for any q ≥ 1, it holds\nEσ ∥∥ ∑ i∈Nn σivi ∥∥q 2 ≤ ( q ∑ i∈Nn ‖vi‖22 ) q 2 .\nLemma A.5 (Block-structured Hölder inequality [26]). Let x = (x(1), . . . , x(n)), y = (y(1), . . . , y(n)) ∈ H = H1 × · · · × Hn. Then, for any p ≥ 1, it holds\n〈x, y〉 ≤ ‖x‖2,p‖y‖2,p∗ .\nProof of Theorem 8. Firstly, for any 1 ≤ t̄ ≤ 2 we can apply a block-structured version of Hölder inequality to bound ∑ i∈Nn σifw(xi) by∑\ni∈Nn σifw(xi) = ∑ i∈Nn σi [ ∑ j∈Nl cj(xi) ∑ m∈NM 〈w(m)j , φm(xi)〉 ]\n= ∑ i∈Nn σi [ ∑ j∈Nl cj(xi)〈wj , φ(xi)〉 ]\n= ∑ j∈Nl 〈 wj , ∑ i∈Nn σicj(xi)φ(xi) 〉 Hölder ≤\n∑ j∈Nl [ ‖wj‖2,t̄ ∥∥∥ ∑ i∈Nn σicj(xi)φ(xi) ∥∥∥ 2,t̄∗ ] C. S ≤ [ ∑ j∈Nl ‖wj‖22,t̄ ] 1 2 [ ∑ j∈Nl ∥∥∥ ∑ i∈Nn σicj(xi)φ(xi) ∥∥∥2 2,t̄∗ ] 1 2 .\n(A.3)\nFor any j ∈ Nl, the Khintchine-Kahane (K.-K.) inequality and Jensen inequality (since t̄∗ ≥ 2) permit us to bound Eσ ∥∥∥∑i∈Nn σicj(xi)φ(xi)∥∥∥22,t̄∗ by Eσ ∥∥∥ ∑ i∈Nn σicj(xi)φ(xi) ∥∥∥2 2,t̄∗ def = Eσ [ ∑ m∈NM ∥∥∥ ∑ i∈Nn σicj(xi)φm(xi) ∥∥∥t̄∗ 2 ] 2 t̄∗\nJensen ≤ [ Eσ ∑ m∈NM ∥∥∥ ∑ i∈Nn σicj(xi)φm(xi) ∥∥∥t̄∗ 2 ] 2 t̄∗\nK.-K. ≤ [ ∑ m∈NM ( t̄∗ ∑ i∈Nn c2j (xi)‖φm(xi)‖22 ) t̄∗ 2 ] 2 t̄∗\n= t̄∗ [ ∑ m∈NM ( ∑ i∈Nn c2j (xi)km(xi, xi) ) t̄∗ 2 ] 2 t̄∗\n= t̄∗ ∥∥∥∥( ∑\ni∈Nn\nc2j (xi)km(xi, xi) )M m=1 ∥∥∥∥ t̄∗ 2 .\nPlugging the above inequalities into Eq. (A.3) and noticing the trivial inequality ‖wj‖2,t̄ ≤ ‖wj‖2,p̄,∀t ≥ p ≥ 1, we get the following bound:\nR̂n(Hp,D) ≤ inf t≥p\nR̂n(Ht,D) ≤ √ D\nn inf t≥p ( t̄∗ ∑ j∈Nl ∥∥∥∥( ∑ i∈Nn c2j (xi)km(xi, xi) )M m=1 ∥∥∥∥ t̄∗ 2 )1/2 .\nThe above inequality can be equivalently written as Eq. (10). Under the condition km(x, x) ≤ B, the term in the brace of Eq. (10) can be controlled by∑\nj∈Nl ∥∥∥∥( ∑ i∈Nn c2j (xi)km(xi, xi) )M m=1 ∥∥∥∥ t 2 = ∑ j∈Nl [ ∑ m∈NM ( ∑ i∈Nn c2j (xi)km(xi, xi)) t 2 ] 2 t\n≤ BM 2t ∑ j∈Nl ∑ i∈Nn c2j (xi).\nTherefore, the inequality (10) further translates to\nR̂n(Hp,D) ≤ √ DB\nn inf\n2≤t≤ 2pp−1\n( tM 2 t [ ∑ j∈Nl ∑ i∈Nn c2j (xi) ]) 12 .\nProof of Theorem 9. The proof now simply follows by plugging in the bound of Theorem 8 into Theorem 7 of Bartlett and Mendelson [3]."
    }, {
      "heading" : "B Support Vector Regression Formulation of CLMKL",
      "text" : "For the -insensitive loss `(t, y) = [|y − t| − ]+, denoting a+ = max(a, 0) for all a ∈ R, we have `∗(−αiC , yi) = − 1 Cαiyi + | αi C | if |αi| ≤ C and∞ elsewise [20]. Hence, the complete dual problem (D) reduces to\nsup α − 1 2 ∑ j∈Nl ∥∥∥( ∑ i∈Nn αicj(xi)φm(xi) )M m=1 ∥∥∥2 2, 2pp−1 + ∑ i∈Nn (αiyi − |αi|),\ns.t. ∑ i∈Nn αi = 0,\n|αi| ≤ C, ∀i ∈ Nn.\n(B.1)\nLet α+i , α − i ≥ 0 be the positive and negative parts of αi, that is, αi = α + i −α − i , |αi| = α + i +α − i . Then,\nthe optimization problem (B.1) translates as follows.\nProblem B.1 (CLMKL—Regression Problem). For the -insensitive loss, the dual CLMKL problem (D) is given by:\nsup α − 1 2 ∑ j∈Nl ∥∥∥( ∑ i∈Nn cj(xi)(α + i − α − i )φm(xi) )M m=1 ∥∥∥2 2, 2pp−1 + ∑ i∈Nn (α+i − α − i )yi − ∑ i∈Nn (α+i + α − i )\ns.t. ∑ i∈Nn (α+i − α − i ) = 0\n0 ≤ α+i , α − i ≤ C,α + i α − i = 0, ∀i ∈ Nn.\n(B.2)"
    }, {
      "heading" : "C Primal and Dual CLMKL Problem Given Fixed Kernel Weights",
      "text" : "Temporarily fixing the kernel weights β, the partial primal and dual optimization problems become as follows.\nProblem C.1 (PRIMAL CLMKL OPTIMIZATION PROBLEM). Given a loss function `(t, y) : R × Y → R convex in the first argument and kernel weights β = (βjm), solve\nmin w,t,b ∑ j∈Nl ∑ m∈NM ‖w(m)j ‖22 2βjm + C ∑ i∈Nn `(ti, yi)\ns.t. ∑ j∈Nl [ cj(xi) ∑ m∈NM 〈w(m)j , φm(xi)〉 ] + b = ti, ∀i ∈ Nn. (C.1)\nProblem C.2 (DUAL CLMKL PROBLEM—PARTIALLY DUALIZED). Given a loss function `(t, y) : R × Y → R convex in the first argument and kernel weights β = (βjm), solve\nsup α: ∑ i∈Nn αi=0 −1 2 ∑ j∈Nl ∑ m∈NM βjm ∥∥∥ ∑ i∈Nn αicj(xi)φm(xi) ∥∥∥2 2 − C ∑ i∈Nn `∗(−αi C , yi). (C.2)\nFor any feasible dual variables, the primal variable w(m)j (α) minimizing the associated Lagrangian saddle problem is\nw (m) j (α) = βjm ∑ i∈Nn αicj(xi)φm(xi). (C.3)\nDualization. With the Lagrangian multipliers αi, i ∈ Nn, the Lagrangian saddle problem of Eq. (C.1) is\nsup α inf w,t,b ∑ j∈Nl ∑ m∈NM ‖w(m)j ‖22 2βjm + C ∑ i∈Nn `(ti, yi)\n− ∑ i∈Nn αi (∑ j∈Nl [cj(xi) ∑ m∈NM 〈w(m)j , φm(xi)〉] + b− ti )\n= sup α\n{ − C ∑ i∈Nn sup ti [−`(ti, yi)− 1 C αiti]− sup b ∑ i∈Nn αib−\n∑ j∈Nl ∑ m∈NM sup w (m) j 1 βjm [ 〈w(m)j , ∑ i∈Nn βjmαicj(xi)φm(xi)− 1 2 ‖w(m)j ‖ 2 2 ]} def = sup∑\ni∈Nn αi=0\n{ − C ∑ i∈Nn `∗(−αi C , yi)− 1 2 ∑ j∈Nl ∑ m∈NM βjm ∥∥ ∑ i∈Nn αicj(xi)φm(xi) ∥∥2 2 } .\n(C.4)\nFrom the above deduction, the variable w(m)j (α) is a solution of the following problem\nw (m) j (α) = arg min\nv∈Hm [ 〈v, ∑ i∈Nn αiβjmcj(xi)φm(xi)〉 − 1 2 ‖v‖22 ] and it can be directly checked that this w(m)j (α) can be analytically represented by\nw (m) j (α) = βjm ∑ i∈Nn αicj(xi)φm(xi).\nPlugging the Fenchel conjugate function of the hinge loss and the -insensitive loss into Problem C.2, we have the following partial dual problems for the hinge loss and -insensitive loss. Here k̃ is the kernel defined in Eq. (6).\nProblem C.3 (DUAL CLMKL PROBLEM—PARTIALLY DUALIZED FOR HINGE LOSS).\nsup αi ∑ i∈Nn αi − 1 2 ∑ i,̃i∈Nn yiyĩαiαĩk̃(xi, xĩ)\ns.t. ∑ i∈Nn αiyi = 0\n0 ≤ αi ≤ C ∀i ∈ Nn.\nProblem C.4 (DUAL CLMKL PROBLEM—PARTIALLY DUALIZED FOR -INSENSITIVE LOSS).\nmax αi − 1 2 ∑ i,̃i∈Nn (α+i − α − i )(α + ĩ − α− ĩ )k̃(xi, xĩ) + ∑ i∈Nn (α+i − α − i )yi − ∑ i∈Nn (α+i + α − i )\ns.t. ∑ i∈Nn (α+i − α − i ) = 0\n0 ≤ α+i , α − i ≤ C,α + i α − i = 0, ∀i ∈ Nn."
    }, {
      "heading" : "D Details on Our Implementation of Localized MKL",
      "text" : "Gönen and Alpaydin [14] give the first formulation of localized MKL algorithm by using gating model ηm(x) ∝ exp(〈vm, x〉 + vm0) to realize locality, and optimize the parameters vm, vm0,m ∈ NM with a gradient descent method. However, the calculation of the gradients requires O(n2M2d) operations in Gönen and Alpaydin [14], which scales poorly w.r.t. the dimension d and the number of kernels. Also, the definition of gating model requires the information of primitive features, which is not accessible in some application areas. For example, data in bioinformatics may appear in a non-vectorial format such as trees and graphs for which the representation of the data with vectors is non-trivial but the calculation of kernel matrices is direct. Although Gönen and Alpaydın [16] propose to use the empirical feature map xG = [kG(x1, x), . . . , kG(xn, x)] to replace the primitive feature in this case (kG is a kernel), this turns out to not strictly obey the spirit of the gating function: the empirical feature does not reflect the location of the example in the feature space induced from the kernel. Furthermore, with this strategy the computation of gradient scales as O(n3M2), which is quite computationally expensive. In this paper, we give a natural definition of the gating model in a kernel-induced feature space, and provide a fast implementation of the resulting LMKL algorithm. Let k0 be the kernel used to define the gating model, and let φ0 be the associated feature map. Our basic idea is based on the discovery that the parameter vm can always be represented as a linear combination of φ0(x1), . . . , φ0(xn), so the calculation of the representation coefficients is sufficient to restore vm. We consider the gating model of the form\nηm(x) = exp\n( 〈vm, φ0(x) + vm0 )∑ m̃∈NM exp ( 〈vm̃, φ0(x) + vm̃0\n) . Gönen and Alpaydin [14] proposed to optimize the objective function\nJ(v) := ∑ i∈Nn αi − 1 2 ∑ i∈Nn ∑ ĩ∈Nn αiαĩyiyĩ [ ∑ m∈NM ηm(xi)km(xi, xĩ)ηm(xĩ) ]\nwith a gradient descent method. The gradient of J(v) can be expressed as:\n∂J(v)\n∂vm = − ∑ i∈Nn ∑ ĩ∈Nn ∑ m̃∈NM αiαĩyiyĩηm̃(xi)km̃(xi, xĩ)ηm̃(xĩ)φ0(xi)[δ m̃ m − ηm(xi)], (D.1)\nwhere δm̃m = 1 ifm = m̃ and 0 otherwise. Let v (t) = (v (t) 1 , . . . , v (t) M ) be the value of v = (v1, . . . , vM ) at the t-th iteration and let r(t)(i,m) be the representation of v(t)m in terms of φ0(xi), i.e., v (t) m = ∑ i∈Nn r\n(t)(i,m)φ0(xi). Analogously, let g(t)(i,m) be the representation coefficient of ∂J(v(t))/∂vm in terms of φ0(xi). Introduce two arrays for convenience:\nB(i,m) = ∑ ĩ∈Nn αĩyĩkm(xi, xĩ)ηm(xĩ), A(i) = ∑ m∈NM ηm(xi)B(i,m), i ∈ Nn,m ∈ NM .\nEq. (D.1) then implies that\ng(t)(i,m) = − ∑ ĩ∈Nn ∑ m̃∈NM αiαĩyiyĩηm̃(xi)km̃(xi, xĩ)ηm̃(xĩ)[δ m̃ m − ηm(xi)]\n= −αiyiηm(xi) [ ∑ ĩ∈Nn αĩyĩkm(xi, xĩ)ηm(xĩ)− ∑ ĩ∈Nn ∑ m̃∈NM αĩyĩηm̃(xi)km̃(xi, xĩ)ηm̃(xĩ) ] = −αiyiηm(xi)[B(i,m)−A(i)]. (D.2)\nWith the line search v(t+1)m = v (t) m + µ(t) ∂J(v(t)) ∂vm\n,m ∈ NM , the representation coefficient can be simply updated by taking\nr(t+1)(i,m) = r(t)(i,m)− µ(t)g(t)(i,m), ∀i ∈ Nn,m ∈ NM .\nAlso, in the calculation of gating model, we need to calculate 〈φ0(xi), v(t)m 〉, and this can be fulfilled by\n〈φ0(xi), v(t)m 〉 = ∑ ĩ∈Nn 〈φ0(xi), r(t)(̃i,m)φ0(xĩ)〉 = ∑ ĩ∈Nn k0(xi, xĩ)r (t)(̃i,m).\nAt each iteration, we can useO(n2M) operations to calculate the arraysA,B. Subsequently, the calculation of the gradients as illustrated by Eq. (D.2) can be fulfilled with O(nM) operations. The updating of the representation coefficients r(t)(i,m) requiresO(nM) operations, while calculating the gating model ηm(xi) requires further O(n2M) operations. Putting the above discussions together, our implementation of LMKL based on the kernel trick requiresO(n2M) operations at each iteration, which is much faster than the original implementation in Gönen and Alpaydin [14] with O(n2M2d) operations at each iteration. Here, d is the dimension of the primitive feature."
    }, {
      "heading" : "E Background on the Experimental Setup and Empirical Results",
      "text" : "E.1 Details on the Protein Fold Prediction Experiment\nWe precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes. We apply kernel k-means to the uniform kernel to generate a partition with 3 clusters for CLMKL and HLMKL, and, since we have no access to primitive features, use this kernel to define gating model in LMKL. All the base kernel matrices are multiplicatively normalized before training. We validate the regularization parameter C over 10{−1,−0.5,...,2}, and the average evenesses over the interval [0.4, 0.7] with eight linearly equally spaced points. Note that the model parameters are tuned separately for each training set and only based on the training set, not the test set. We repeat the experiment 15 times.\nE.2 Details on the Visual Image Categorization Experiment\nWe compute 9 bag-of-words features, each with a dictionary size of 512, resulting in 9 χ2-Kernels [57]. The first 6 bag-of-words features are computed over SIFT features [39] at three different scales and the two color channel sets RGB and opponent colors [51]. The remaining 3 bag-of-words features are computed over quantiles of color values at the same three scales. The quantiles are concatenated over RGB channels.\nFor each channel within a set of color channels, the quantiles are concatenated. Local features are extracted at a grid of step size 5 on images that were down-scaled to 600 pixels in the largest dimension. Assignment of local features to visual words is done using rank-mapping [5]. The kernel width of the kernels is set to the mean of the χ2-distances. All kernels are multiplicatively normalized.\nThe dataset is split into 11 parts for outer crossvalidation. The performance reported in Table 4 is the average over the 11 test splits of the outer cross validation. For each outer cross validation training split, a 10-fold inner crossvalidation is performed for determining optimal parameters. The parameters are selected using only the samples of the outer training split. This avoids to report a result merely on the most favorable train test split from the outer cross validation. For the proposed CLMKL we employ kernel k-means with 3 clusters on the outer training split of the dataset.\nWe compare CLMKL to regular `p-norm MKL [31] and to localized MKL as in [14]. For all methods, we employ a one-versus-all setup, running over `p-norms in {1.125, 1.333, 2} and regularization constants in {10k/2}k=5k=0 (optima attained inside the respective grids). CLMKL uses the same set of `p-norms, regularization constants from {10k/2}k=0,...,5, and average excesses in {0.5 + i/12}i=5i=0. Performance is measured through multi-class classification accuracy."
    } ],
    "references" : [ {
      "title" : "Toward a gold standard for promoter prediction evaluation",
      "author" : [ "T. Abeel", "Y. Van de Peer", "Y. Saeys" ],
      "venue" : "Bioinformatics, 25(12):i313–i320,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Multiple kernel learning, conic duality, and the smo algorithm",
      "author" : [ "F.R. Bach", "G.R. Lanckriet", "M.I. Jordan" ],
      "venue" : "ICML, page 6,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Rademacher and Gaussian complexities: Risk bounds and structural results",
      "author" : [ "P. Bartlett", "S. Mendelson" ],
      "venue" : "Journal of Machine Learning Research, 3:463–482,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Support vector machines and kernels for computational biology",
      "author" : [ "A. Ben-Hur", "C.S. Ong", "S. Sonnenburg", "B. Schölkopf", "G. Rätsch" ],
      "venue" : "PLoS Computational Biology, 4,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Enhanced representation and multi-task learning for image annotation",
      "author" : [ "A. Binder", "W. Samek", "K.-R. Müller", "M. Kawanabe" ],
      "venue" : "Computer Vision and Image Understanding,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convex optimization",
      "author" : [ "S.P. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge Univ. Press, New York,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning with support vector machines",
      "author" : [ "C. Campbell", "Y. Ying" ],
      "venue" : "Synthesis Lectures on Artificial Intelligence and Machine Learning, 5(1):1–95,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "LIBSVM: a library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Invited talk: Can learning kernels help performance",
      "author" : [ "C. Cortes" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Generalization bounds for learning kernels",
      "author" : [ "C. Cortes", "M. Mohri", "A. Rostamizadeh" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning, ICML’10,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning kernels using local rademacher complexity",
      "author" : [ "C. Cortes", "M. Kloft", "M. Mohri" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2760–2768,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Kernel k-means: spectral clustering and normalized cuts",
      "author" : [ "I.S. Dhillon", "Y. Guan", "B. Kulis" ],
      "venue" : "ACM SIGKDD international conference on Knowledge discovery and data mining, pages 551–556. ACM,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Multi-class protein fold recognition using support vector machines and neural networks",
      "author" : [ "C.H. Ding", "I. Dubchak" ],
      "venue" : "Bioinformatics, 17(4):349–358,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Localized multiple kernel learning",
      "author" : [ "M. Gönen", "E. Alpaydin" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning, pages 352–359. ACM,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Multiple kernel learning algorithms",
      "author" : [ "M. Gönen", "E. Alpaydin" ],
      "venue" : "J. Mach. Learn. Res., 12:2211–2268, July",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Localized algorithms for multiple kernel learning",
      "author" : [ "M. Gönen", "E. Alpaydın" ],
      "venue" : "Pattern Recognition, 46(3):795– 807,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Active and semi-supervised data domain description",
      "author" : [ "N. Görnitz", "M. Kloft", "U. Brefeld" ],
      "venue" : "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 407–422. Springer,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Toward supervised anomaly detection",
      "author" : [ "N. Görnitz", "M.M. Kloft", "K. Rieck", "U. Brefeld" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Probability-confidence-kernel-based localized multiple kernel learning with norm",
      "author" : [ "Y. Han", "G. Liu" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B, 42(3):827–837,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Fenchel duality-based algorithms for convex optimization problems with applications in machine learning and image restoration",
      "author" : [ "A. Heinrich" ],
      "venue" : "PhD thesis, Chemnitz University of Technology,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Clusterpath: an algorithm for clustering using convex fusion penalties",
      "author" : [ "T.D. Hocking", "A. Joulin", "F. Bach", "J.-P. Vert" ],
      "venue" : "In 28th international conference on machine learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Improved loss bounds for multiple kernel learning",
      "author" : [ "Z. Hussain", "J. Shawe-Taylor" ],
      "venue" : "AISTATS, pages 370–377,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Some random series of functions",
      "author" : [ "J.-P. Kahane" ],
      "venue" : "Cambridge University Press, Cambridge,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "`p-norm multiple kernel learning",
      "author" : [ "M. Kloft" ],
      "venue" : "PhD thesis, Berlin Institute of Technology,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The local Rademacher complexity of `p-norm multiple kernel learning",
      "author" : [ "M. Kloft", "G. Blanchard" ],
      "venue" : "Advances in Neural Information Processing Systems 24, pages 2438–2446. MIT Press,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On the convergence rate of lp-norm multiple kernel learning",
      "author" : [ "M. Kloft", "G. Blanchard" ],
      "venue" : "Journal of Machine Learning Research, 13(1):2465–2502,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Automatic feature selection for anomaly detection",
      "author" : [ "M. Kloft", "U. Brefeld", "P. Düessel", "C. Gehl", "P. Laskov" ],
      "venue" : "Proceedings of the 1st ACM workshop on Workshop on AISec, pages 71–76. ACM,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Non-sparse multiple kernel learning",
      "author" : [ "M. Kloft", "U. Brefeld", "P. Laskov", "S. Sonnenburg" ],
      "venue" : "NIPS Workshop on Kernel Learning: Automatic Selection of Optimal Kernels, volume 4,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Efficient and accurate lp-norm multiple kernel learning",
      "author" : [ "M. Kloft", "U. Brefeld", "P. Laskov", "K.-R. Müller", "A. Zien", "S. Sonnenburg" ],
      "venue" : "Advances in neural information processing systems, pages 997–1005,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A unifying view of multiple kernel learning",
      "author" : [ "M. Kloft", "U. Rückert", "P. Bartlett" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases, pages 66–81,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Lp-norm multiple kernel learning",
      "author" : [ "M. Kloft", "U. Brefeld", "S. Sonnenburg", "A. Zien" ],
      "venue" : "The Journal of Machine Learning Research, 12:953–997,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning the kernel matrix with semidefinite programming",
      "author" : [ "G.R. Lanckriet", "N. Cristianini", "P. Bartlett", "L.E. Ghaoui", "M.I. Jordan" ],
      "venue" : "The Journal of Machine Learning Research, 5:27–72,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Refined Rademacher chaos complexity bounds with applications to the multikernel learning problem",
      "author" : [ "Y. Lei", "L. Ding" ],
      "venue" : "Neural. Comput., 26(4):739–760,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Theory and algorithms for the localized setting of learning kernels",
      "author" : [ "Y. Lei", "A. Binder", "Ü. Dogan", "M. Kloft" ],
      "venue" : "Proceedings of The 1st International Workshop on “Feature Extraction: Modern Questions and Challenges”, NIPS, pages 173–195,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "What, where and who? classifying events by scene and object recognition",
      "author" : [ "L.-J. Li", "L. Fei-Fei" ],
      "venue" : "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1–8. IEEE,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Multiple kernel clustering with local kernel alignment maximization",
      "author" : [ "M. Li", "X. Liu", "L. Wang", "Y. Dou", "J. Yin", "E. Zhu" ],
      "venue" : "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI’16,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Sample-adaptive multiple kernel learning",
      "author" : [ "X. Liu", "L. Wang", "J. Zhang", "J. Yin" ],
      "venue" : "Proceedings of the Twenty- Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Québec City, Québec, Canada., pages 1975– 1981,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Absent multiple kernel learning",
      "author" : [ "X. Liu", "L. Wang", "J. Yin", "Y. Dou", "J. Zhang" ],
      "venue" : "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., pages 2807–2813,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Distinctive image features from scale-invariant keypoints",
      "author" : [ "D.G. Lowe" ],
      "venue" : "International Journal of Computer Vision, 60(2):91–110,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning the kernel function via regularization",
      "author" : [ "C.A. Micchelli", "M. Pontil" ],
      "venue" : "Journal of Machine Learning Research, pages 1099–1125,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A unified view of localized kernel learning",
      "author" : [ "J. Moeller", "S. Swaminathan", "S. Venkatasubramanian" ],
      "venue" : "arXiv preprint arXiv:1603.01374,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Non-uniform multiple kernel learning with cluster-based gating functions",
      "author" : [ "Y. Mu", "B. Zhou" ],
      "venue" : "Neurocomputing, 74(7):1095–1101,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning with Kernels",
      "author" : [ "B. Schölkopf", "A. Smola" ],
      "venue" : "MIT Press, Cambridge, MA,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Large scale multiple kernel learning",
      "author" : [ "S. Sonnenburg", "G. Rätsch", "C. Schäfer", "B. Schölkopf" ],
      "venue" : "The Journal of Machine Learning Research, 7:1531–1565,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Arts: accurate recognition of transcription starts in human",
      "author" : [ "S. Sonnenburg", "A. Zien", "G. Rätsch" ],
      "venue" : "Bioinformatics, 22(14):e472–e480,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Poims: positional oligomer importance matrices—understanding support vector machine-based signal detectors",
      "author" : [ "S. Sonnenburg", "A. Zien", "P. Philips", "G. Rätsch" ],
      "venue" : "Bioinformatics, 24(13):i6–i14,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning bounds for support vector machines with learned kernels",
      "author" : [ "N. Srebro", "S. Ben-David" ],
      "venue" : "COLT, pages 169–183. Springer-Verlag, Berlin,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Multiple kernel learning and the smo algorithm",
      "author" : [ "Z. Sun", "N. Ampornpunt", "M. Varma", "S. Vishwanathan" ],
      "venue" : "Advances in neural information processing systems, pages 2361–2369,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convergence of a block coordinate descent method for nondifferentiable minimization",
      "author" : [ "P. Tseng" ],
      "venue" : "Journal of optimization theory and applications, 109(3):475–494,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Evaluating color descriptors for object and scene recognition",
      "author" : [ "K.E.A. van de Sande", "T. Gevers", "C.G.M. Snoek" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2010
    }, {
      "title" : "Probabilistic clustering of time-evolving distance data",
      "author" : [ "J.E. Vogt", "M. Kloft", "S. Stark", "S.S. Raman", "S. Prabhakaran", "V. Roth", "G. Rätsch" ],
      "venue" : "Machine Learning, 100(2-3):635–654,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Simple and efficient multiple kernel learning by group lasso",
      "author" : [ "Z. Xu", "R. Jin", "H. Yang", "I. King", "M.R. Lyu" ],
      "venue" : "Proceedings of the 27th international conference on machine learning (ICML-10), pages 1175–1182,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Efficient sparse generalized multiple kernel learning",
      "author" : [ "H. Yang", "Z. Xu", "J. Ye", "I. King", "M.R. Lyu" ],
      "venue" : "IEEE Transactions on Neural Networks, 22(3):433–446,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Group-sensitive multiple kernel learning for object categorization",
      "author" : [ "J. Yang", "Y. Li", "Y. Tian", "L. Duan", "W. Gao" ],
      "venue" : "2009 IEEE 12th International Conference on Computer Vision, pages 436–443. IEEE,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Generalization bounds for learning the kernel",
      "author" : [ "Y. Ying", "C. Campbell" ],
      "venue" : "COLT,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Local features and kernels for classification of texture and object categories: A comprehensive study",
      "author" : [ "J. Zhang", "M. Marszalek", "S. Lazebnik", "C. Schmid" ],
      "venue" : "International Journal of Computer Vision, 73(2):213–238,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "[32], who introduce the multiple kernel learning (MKL) framework [15].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[32], who introduce the multiple kernel learning (MKL) framework [15].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 43,
      "context" : "MKL offers a principal way of encoding complementary information with distinct base kernels and automatically learning an optimal combination of those [45].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 1,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 141,
      "endOffset" : 168
    }, {
      "referenceID" : 26,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 141,
      "endOffset" : 168
    }, {
      "referenceID" : 28,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 141,
      "endOffset" : 168
    }, {
      "referenceID" : 43,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 141,
      "endOffset" : 168
    }, {
      "referenceID" : 51,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 141,
      "endOffset" : 168
    }, {
      "referenceID" : 52,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 141,
      "endOffset" : 168
    }, {
      "referenceID" : 9,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 257,
      "endOffset" : 293
    }, {
      "referenceID" : 10,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 257,
      "endOffset" : 293
    }, {
      "referenceID" : 21,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 257,
      "endOffset" : 293
    }, {
      "referenceID" : 24,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 257,
      "endOffset" : 293
    }, {
      "referenceID" : 25,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 257,
      "endOffset" : 293
    }, {
      "referenceID" : 29,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 257,
      "endOffset" : 293
    }, {
      "referenceID" : 32,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 257,
      "endOffset" : 293
    }, {
      "referenceID" : 46,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 257,
      "endOffset" : 293
    }, {
      "referenceID" : 54,
      "context" : "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].",
      "startOffset" : 257,
      "endOffset" : 293
    }, {
      "referenceID" : 27,
      "context" : "9, and references therein], it was shown that improved predictive accuracy can be achieved by employing appropriate regularization [28, 31].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 30,
      "context" : "9, and references therein], it was shown that improved predictive accuracy can be achieved by employing appropriate regularization [28, 31].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 13,
      "context" : "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 33,
      "context" : "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 35,
      "context" : "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 41,
      "context" : "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 53,
      "context" : "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 33,
      "context" : "Indeed, besides the recent work by [34], the generalization performance of localized MKL algorithms (as measured through large-deviation bounds) is poorly understood, which potentially could make these algorithms prone to overfitting.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "1 Related Work Gönen and Alpaydin [14] initiate the work on localized MKL by introducing gating models",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 53,
      "context" : "[55] give a group-sensitive formulation of localized MKL, where kernel weights vary at, instead of the example level, the group level.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 41,
      "context" : "Mu and Zhou [42] also introduce a non-uniform MKL allowing the kernel weights to vary at the cluster-level and tune the kernel weights under the graph embedding framework.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 18,
      "context" : "Han and Liu [19] built on Gönen and Alpaydin [14] by complementing the spatial-similarity-based kernels with probability confidence kernels reflecting the likelihood of examples belonging to the same class.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 13,
      "context" : "Han and Liu [19] built on Gönen and Alpaydin [14] by complementing the spatial-similarity-based kernels with probability confidence kernels reflecting the likelihood of examples belonging to the same class.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 35,
      "context" : "[36] propose a multiple kernel clustering method by maximizing local kernel alignments.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[37] present sample-adaptive approaches to localized MKL, where kernels can be switched on/off at the example level by introducing a latent binary vector for each individual sample, which and the kernel weights are then jointly optimized via margin maximization principle.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 40,
      "context" : "[41] present a unified viewpoint of localized MKL by interpreting gating functions in terms of local reproducing kernel Hilbert spaces acting on the data.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] present a convex approach to MKL based on controlling the local Rademacher complexity, the meaning of locality is different in Cortes et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11]: it refers to the localization of the hypothesis class, which can result in sharper excess risk bounds [25, 26], and is not related to localized multiple kernel learning.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[11]: it refers to the localization of the hypothesis class, which can result in sharper excess risk bounds [25, 26], and is not related to localized multiple kernel learning.",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 25,
      "context" : "[11]: it refers to the localization of the hypothesis class, which can result in sharper excess risk bounds [25, 26], and is not related to localized multiple kernel learning.",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 37,
      "context" : "[38] extend the idea of sample-adaptive MKL to address the issue with missing kernel information on some examples.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[34] propose a MKL method by decoupling the locality structure learning with a hard clustering strategy from optimizing the parameters in the spirit of multi-task learning.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : ", Sl in a probabilistic manner, meaning that, for each cluster Sj , we have a function cj : X → [0, 1] indicating the likelihood of x falling into cluster j, i.",
      "startOffset" : 96,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "the first argument and cluster likelihood functions cj : X → [0, 1], j ∈ Nl, solve",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 30,
      "context" : ", βjM ) for each cluster j [31] .",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : "The result (2) now follows by recalling that for a norm ‖ · ‖, its dual norm ‖ · ‖∗ is defined by ‖x‖∗ = sup‖μ‖=1〈x, μ〉 and satisfies: ( 12‖ · ‖ 2)∗ = 12‖ · ‖ 2 ∗ [6].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 43,
      "context" : "[45], we consider here a two-layer optimization procedure to solve the problem (P) where the variables are divided into two groups: the group of kernel weights {βjm} j,m=1 and the group of weight vectors {w j } l,M j,m=1.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "This allows us to employ very efficient existing SVM solvers [8].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 43,
      "context" : "[45] Figure 7 in Kloft et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[31]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 47,
      "context" : "[49] in the context of `p-norm MKL.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "We give a purely data-dependent bound on the generalization error, which is obtained using Rademacher complexity theory [3].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "[10], Kloft and Blanchard [25], Kloft et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[10], Kloft and Blanchard [25], Kloft et al.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 30,
      "context" : "[31].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "1 Experimental Setup We implement the proposed convex localized MKL (CLMKL) algorithm in MATLAB and solve the involved canonical SVM problem with LIBSVM [8].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 20,
      "context" : "[21]) could be used.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 33,
      "context" : "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 30,
      "context" : "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 31,
      "context" : "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 13,
      "context" : "The calculation of the gradients in LMKL [14] requires O(nMd) operations, which scales poorly, and the definition of the gating model 8",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "It includes 1000 splice site instances and 20 weighted-degree kernels with degrees ranging from 1 to 20 [4].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 45,
      "context" : "A hypothetical explanation of the improvement from CLMKL is that splice sites are characterized by nucleotide sequences—so-called motifs—the length of which may differ from site to site [47].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 44,
      "context" : "This data set, which is included in the larger study of [46], comes with 5 kernels.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "The SVM based on the uniform combination of these 5 kernels was found to have the highest overall performance among 19 promoter prediction programs [1].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "To be consistent with previous studies [1, 24, 46], we use the area under the ROC curve (AUC) as an evaluation criterion.",
      "startOffset" : 39,
      "endOffset" : 50
    }, {
      "referenceID" : 23,
      "context" : "To be consistent with previous studies [1, 24, 46], we use the area under the ROC curve (AUC) as an evaluation criterion.",
      "startOffset" : 39,
      "endOffset" : 50
    }, {
      "referenceID" : 44,
      "context" : "To be consistent with previous studies [1, 24, 46], we use the area under the ROC curve (AUC) as an evaluation criterion.",
      "startOffset" : 39,
      "endOffset" : 50
    }, {
      "referenceID" : 44,
      "context" : "The gating function and the partition are computed with the TSS kernel, which carries most of the discriminative information [46].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 12,
      "context" : "We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 23,
      "context" : "We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 24,
      "context" : "We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "We use exactly the same 12 kernels as in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25] reflecting different features, such as van der Waals volume, polarity and hydrophobicity.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 23,
      "context" : "We use exactly the same 12 kernels as in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25] reflecting different features, such as van der Waals volume, polarity and hydrophobicity.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 24,
      "context" : "We use exactly the same 12 kernels as in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25] reflecting different features, such as van der Waals volume, polarity and hydrophobicity.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], which is detailed in Supplementary Material E.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 23,
      "context" : "We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], which is detailed in Supplementary Material E.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 24,
      "context" : "We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], which is detailed in Supplementary Material E.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 23,
      "context" : "6% higher than the one reported in Kloft [24], which is higher than the initially reported accuracies in Campbell and Ying [7].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "6% higher than the one reported in Kloft [24], which is higher than the initially reported accuracies in Campbell and Ying [7].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 34,
      "context" : "5 Visual Image Categorization—UIUC Sports We experiment on the UIUC Sports event dataset [35] consisting of 1574 images, belonging to 8 image classes of sports activities.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "26% over the `p-norm MKL baseline while localized MKL as in Gönen and Alpaydin [14] underperforms the MKL baseline.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 16,
      "context" : "Future work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features.",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "Future work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features.",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 20,
      "context" : "Future work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features.",
      "startOffset" : 133,
      "endOffset" : 141
    }, {
      "referenceID" : 50,
      "context" : "Future work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features.",
      "startOffset" : 133,
      "endOffset" : 141
    }, {
      "referenceID" : 48,
      "context" : "1 in Tseng [50].",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 9,
      "context" : "[10], which was shown to be tight.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "4 (Khintchine-Kahane inequality [23]).",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : "5 (Block-structured Hölder inequality [26]).",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "The proof now simply follows by plugging in the bound of Theorem 8 into Theorem 7 of Bartlett and Mendelson [3].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 19,
      "context" : "B Support Vector Regression Formulation of CLMKL For the -insensitive loss `(t, y) = [|y − t| − ]+, denoting a+ = max(a, 0) for all a ∈ R, we have `∗(−αi C , yi) = − 1 Cαiyi + | αi C | if |αi| ≤ C and∞ elsewise [20].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 13,
      "context" : "Gönen and Alpaydin [14] give the first formulation of localized MKL algorithm by using gating model ηm(x) ∝ exp(〈vm, x〉 + vm0) to realize locality, and optimize the parameters vm, vm0,m ∈ NM with a gradient descent method.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "However, the calculation of the gradients requires O(nMd) operations in Gönen and Alpaydin [14], which scales poorly w.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "Although Gönen and Alpaydın [16] propose to use the empirical feature map xG = [kG(x1, x), .",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 13,
      "context" : "Gönen and Alpaydin [14] proposed to optimize the objective function",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "Putting the above discussions together, our implementation of LMKL based on the kernel trick requiresO(nM) operations at each iteration, which is much faster than the original implementation in Gönen and Alpaydin [14] with O(nMd) operations at each iteration.",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 6,
      "context" : "1 Details on the Protein Fold Prediction Experiment We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes.",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 23,
      "context" : "1 Details on the Protein Fold Prediction Experiment We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 24,
      "context" : "1 Details on the Protein Fold Prediction Experiment We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes.",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 6,
      "context" : "1 Details on the Protein Fold Prediction Experiment We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes.",
      "startOffset" : 247,
      "endOffset" : 250
    }, {
      "referenceID" : 55,
      "context" : "2 Details on the Visual Image Categorization Experiment We compute 9 bag-of-words features, each with a dictionary size of 512, resulting in 9 χ-Kernels [57].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 38,
      "context" : "The first 6 bag-of-words features are computed over SIFT features [39] at three different scales and the two color channel sets RGB and opponent colors [51].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 49,
      "context" : "The first 6 bag-of-words features are computed over SIFT features [39] at three different scales and the two color channel sets RGB and opponent colors [51].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 4,
      "context" : "Assignment of local features to visual words is done using rank-mapping [5].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 30,
      "context" : "We compare CLMKL to regular `p-norm MKL [31] and to localized MKL as in [14].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "We compare CLMKL to regular `p-norm MKL [31] and to localized MKL as in [14].",
      "startOffset" : 72,
      "endOffset" : 76
    } ],
    "year" : 2016,
    "abstractText" : "We propose a localized approach to multiple kernel learning that can be formulated as a convex optimization problem over a given cluster structure. For which we obtain generalization error guarantees and derive an optimization algorithm based on the Fenchel dual representation. Experiments on real-world datasets from the application domains of computational biology and computer vision show that convex localized multiple kernel learning can achieve higher prediction accuracies than its global and non-convex local counterparts.",
    "creator" : "LaTeX with hyperref package"
  }
}