{
  "name" : "1211.5829.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "ASIFT KEYPOINTS", "Reza Oji" ],
    "emails" : [ "oji.reza@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "DOI : 10.5121/sipij.2012.3503 29\nObject recognition is an important task in image processing and computer vision. This paper presents a perfect method for object recognition with full boundary detection by combining affine scale invariant feature transform (ASIFT) and a region merging algorithm. ASIFT is a fully affine invariant algorithm that means features are invariant to six affine parameters namely translation (2 parameters), zoom, rotation and two camera axis orientations. The features are very reliable and give us strong keypoints that can be used for matching between different images of an object. We trained an object in several images with different aspects for finding best keypoints of it. Then, a robust region merging algorithm is used to recognize and detect the object with full boundary in the other images based on ASIFT keypoints and a similarity measure for merging regions in the image. Experimental results show that the presented method is very efficient and powerful to recognize the object and detect it with high accuracy.\nKEYWORDS\nObject recognition, keypoint, affine invariant, region merging algorithm, ASIFT."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Extracting the points from an image that can give best define from an object in image namely keypoints is very important and valuable. These points have many applications in image processing like object detection, object and shape recognition, image registation and object tracking. By extracting the keypoints, we can use them for finding objects in the other images. Detect and recognize the object by using the keypoints and a segmentation algorithm is very accurate because if the keypoints are correctly identified, they achieve the best information from the image. ASIFT is a fully affine invariant method with respect to six parameters of affine transform [1,2], wherease the previous method SIFT was invariant with respect to four parameters namely translation, rotation and change in scale (zoom) [3]. ASIFT cover two other parameters namely longitude and latitude angle that are relevant to camera axis orientation. It means that ASIFT is more effective for our goal and can be more robust in the changes of images.\nMany image segmentation and object recognition algorithms have been presented, each having its own specifications [4,5]. Some of these algorithm are interactive image segmentation based on region merging [6,7]. One of them is a powerful algorithm [8] for detecting object and its boundary but it is not an automatic algorithm and has a problem. In this algorithm the users must indicate some of locations and regions of the background and object to run the algorithm.\nIn this paper, we combined ASIFT results and a region merging algorithm to recognize objects in images and detect them with full boundary. The presented algorithm does not have the stated problem in region merging algorithm. It means that, the algorithm does not need to indicate regions by user. We use the best keypoints of object that has been obtained from ASIFT results and apply them into the image. Therefore, the method will be an automatic algorithm and will not need marks by users and the achieved keypoints from ASIFT have been replaced with them. Lowe presented an object recognition algorithm [9] based on SIFT, but object recognition and detection algorithm with full boundary by using ASITF, has not yet been presented. Therefore, at this time we have an automatic algorithm for object recognition and full detection by using the obtained keypoints from ASIFT algorithm. These keypoints are very efficient and give us the best information of object to find it in the other images."
    }, {
      "heading" : "2. THE ASIFT ALGORITHM",
      "text" : "ASIFT is an improved algorithm from SIFT. SIFT has been presented by Lowe (1994) where SIFT is invariant to four of the six parameters of affine transform. ASIFT simulates all image views obtained by the longitude and the latatude angle (varying the two camera axis orientation parameters) and then applies the other parameters by using the SIFT algorithm. There is a new notion called transition tilt that is designed to quantify the amount of tilt between two such images. In continuation, major steps of the algorithm are described."
    }, {
      "heading" : "2.1. The Affine Camera Model",
      "text" : "Image distortions coming from viewpoint changes can be modeled by affine planar transforms.\nAny affine map A is defined:\n  \n\n  \n −\n  \n\n  \n\n  \n\n  \n − =Α\nϕϕ\nϕϕ\nψψ\nψψ λ\ncossin\nsincos\n10\n0\ncossin\nsincos t (1)\nWhich we note A = λ R1(ψ)Tt R2(φ), where λ > 0, λt is the deteminant of A, Ri are rotations, φ ϵ [0,180), and Tt is a tilt namely a diagonal matrix with first eigenvalue t > 1 and the second one equal to 1. Figure1, shows a camera motion interpretation of (1). θ and φ = arccos 1/ t are respectively the camera optical axis longitude and latitude, λ corresponds to the zoom and a third angle ψ parameterizes the camera spin.\nAccording to up preamble, each image should be transformed by simulating all possible affine distortions caused by the changes of camera optical axis orientations from a frontal position. These distortions depend upon two parameters, the longitude and the latitude. rotations and tilts are performed for a finite number of angles to decrease complexity and time computation. The sampling steps of these parameters ensuring that the simulated images keep close to any other possible view generated by other values of φ and θ.\nAfter this process, the SIFT algorithm will be applied in all simulated images. The other steps are relevant to SIFT."
    }, {
      "heading" : "2.2. Scale Space Extremum Detection",
      "text" : "The next step is extracting keypoints that are invariant to changes of scale. We need to search for stable features in all possible changes. In previous research it has been shown that for this task, the Gaussian function is the only possible scale-space kernel. A function, L(x,y,σ), is the scalespace of an image that is obtained from the convolution of an input image, I(x,y), with a variable scale-space Gaussian function, G(x,y,σ). For efficiently detect stable keypoint locations, Lowe proposed using the scale-space extremum in difference-of-Gaussian (DOG) function, D(x,y,σ). Extremum computed by the difference of two nearby scales separated by a constant factor K (2):\n, (2)\nThis process repeats in several octaves. In each octave, the initial image is repeatedly convolved with Gaussians to produce the set of scale space images. Adjacent Gaussian images are subtracted to produce the difference-of-Gaussian images. After each octave, the Gaussian images are downsampled by a factor of 2, and the process repeated. Figure 2, shows a visual representation from this process.\nEach sample point (pixel) is compared with its neighbors according to their intensities for finding out whether is smaller or larger than neighbors. For more accuracy, each pixel will be checked with the eight closest neighbors in image location and nine neighbors in the scale above and below (Figure 3). If the point is an extremum against all 26 neighbors, is selected as candidate keypoint. The cost of this comparison is reasonably low because most sample point will be eliminated at first few checks.\n( ) ( ) 2 2 22\n2\n1 , ,\n2\nx y G x y e σ σ\nπσ\n− + =\n( ) ( ) ( )( ) ( ) ( ) ( ) , , , , , , , , , , , D x y G x y k G x y I x y L x y k L x y σ σ σ σ σ = − ∗ = −"
    }, {
      "heading" : "2.3. Accurate Keypoint Localization",
      "text" : "For each candidate keypoint interpolation of nearby data is used to accurately determine its position. Then, many keypoints that are unstable and sensitive to noise, like the points with low contrast and the points on the edge, will be eliminated. (Figure 4)"
    }, {
      "heading" : "2.4. Assigning an Orientation",
      "text" : "The next step is assigning an orientation for each keypoint. By this step, the keypoint descriptor [10] can be represented relative to this orientation and therefore get invariance to image rotation. For each Gaussian smoothed image sample, points in regions around keypoint are selected and magnitude, m, and orientations, θ, of gradiant are calculated (3):\n(3)\nThen, created weighted (magnitude + gaussian) histogram of local gradiant directions computed at selected scale. Histogram is formed by quantizing the orientations into 36 bins to covering 360 degree range of orinetations. The highest peak in the histogram is detected where peaks in the orientation histogram correspond to dominant directions of local gradiant. (Figure 5)\n22 ))1,()1,(()),1(),1((),( −−++−−+= yxLyxLyxLyxLyxm\n))),1(),1(/())1,()1,(((tan),( 1 yxLyxLyxLyxLyx −−+−−+= −θ"
    }, {
      "heading" : "2.5. Keypoint Descriptor",
      "text" : "The previous operations have assigned location, scale, and orientation to each keypoint and provided invariance to these parameters. Remaining goals are to define a keypoint descriptor [11] for the local image regions and reduce the effects of illumination changes.\nThe descriptor is based on 16×16 samples that the keypoint is in the center of. Samples are divided into 4×4 subregions in 8 directions around keypoint. Magnitude of each point is weighted and gives less weight to gradients far from keypoint (Figure 6). Therefore, feature vector dimensional is 128 (4×4×8).\nFinally, vector normalization is applied. The vector is normalized to unit length. A change in image contrast in which each pixel value is multiplied by a constant will multiply gradients by the same constant. Contrast change will be canceled by vector normalization and brightness change in which a constant is added to each image pixel will not affect the gradient values, as they are computed from pixel differences.\nNow we can find keypoints from an image in the other images and match them together. One image is the training sample of what we are looking for and the other image is the world picture that might contain instances of the training sample. Both images have features associated with them across different octaves. Keypoints in all octaves in one image independently match with all keypoints in all octaves in other image. Features will be matched by using nearest neighbor algorithm. The nearest neighbor is defined as the keypoint with minimum Eculidean distance for the invariant descriptor vector as described upside. Also to solve the problem of features that have no correct match due to some reason like background noise or clutter, a threshold at 0.8 is chosen for ratio of closest nearest neighbor with second closest nearest neighbor that obtained\nexperimentally . If the distance is more than 0.8, then the algorithm does not match keypoints together."
    }, {
      "heading" : "3. PROPOSED OBJECT RECOGNITION AND DETECTION METHOD",
      "text" : "For each object, we train it by ASIFT algorithm in several images with different scales and viewpoints to find best keypoints to recognize desire object in the other images. Then, by using these keypoints and a region merging algorithm the object will be detected. A maximal similarity region mering segmentation algorithm was propoesd by Ning. This algorithm is based on a region merging method with using initial segmentation of mean shift [12] and user markers. User marks part of object and background to help the region merging process by maximal similarity. The non-marker background regions will be automatically merged while the non-marker object regions will be identified and avoided from being merged with background. It can extract the object from complex scenes but there is a weakness that the algorithm needs user marks, hence, the algorithm is not automatic. In our algorithm, we use this region merging but without user marks. We propose an efficient method for object recognition and detection by combining ASIFT keypoints (instead of user marks) with an automatic segmentation based on region merging which can detect object with full boundary."
    }, {
      "heading" : "3.1. Region Merging Based on Maximal Similarity",
      "text" : "An initial segmentation is required to partition image into regions for region merging in further steps. We use the mean shift segmentation software namely the EDISON System [13] for initial segmentation because it can preserve the boundries well and has high speed. Any other low level segmentation like super-pixel and watershed can be used too. Please refer to [14,15] for more information about mean shift segmentation.\nThere is many small region after initial segmentation. These regions represent using color histogram that is an effective descriptor because different regions from the same object often have high similarity in color whereas there have variation in other aspects like size and shape. The color histogram computed by RGB color space. We uniformly quantize each color chanel into 8 levels. Therefore, the feature space is 8×8×8=512 and the histogram of each region is computed in 512 bins. The normalized histogram of a region X denote by HistX. Now we want to merge the regions by their color histogram that can extract the desired object.\nWhen we applied ASIFT keypoints in the image, some regions in image that are relevant to object in image are compoesd of keypoints and the others are not. Here, an important issue is how to define the similarity between the regions inclusive keypoints with the regions without any point so that the similar regions can be merged. We use the Euclidean distance for this target because that is a well known goodness of fit statistical metric and simple.Therefore, ∆(X,Y) is a similarity measure between two regions X and Y based on the Euclidean distance (4):\n( ) ( )∑ =\n−=∆ 512\n1\n2\n, e\ne\nY\ne\nX HistHistYX (4)\nWhere HistX and HistY are the normalized histograms of X and Y. Also, The superscript m is mth member of the histograms. Lower Euclidean distance between X and Y means that the similarity between them is higher."
    }, {
      "heading" : "3.2. The Merging Process",
      "text" : "In the merging process we have three regions in the image with diferent lables: the object regions denote by RO that are identified by obtained keypoints from training ASIFT algorithm, the regions around the images denote by RB that usually are not inclusive objects, therefore, we cover all around the images as initial background regions to help and start the merging process and the third regions, the regions without any sign denote by N.\nThe merging rule is defined in continue (5). Let Y be an adjacent region of X and SY is the set of Y’s adjacent region that X is one of them. The similarity between Y and SY, i.e. ∆(Y,SY), is calculated. We will merge X and Y together if and only if the similarity between them is the maximum among all the similarities ∆(Y,SY).\nMerge X and Y if ∆(X,Y) = max ∆(Y,SY) (5)\nThe main strategy is to keep object regions from merging and merge background regions as many as possible. In the first stage, we try to merge background regions with their adjacent regions. Each region B ϵ RB will be merged into adjacent region if the merging rule is be satisfied. If the merging accured, the new region has the same label as region B. This stage will be repeated iteratively and in each iteration RB and N will be updated. Obviously, RB expands and N shrink due to merging process. The process in this stage stops when background regions RB can not find any region for new merging.\nAfter this stage, still, some background regions are, that can not be merged due to merging rule. In next stage we will focus on the remaining regions in N from the first stage that are combination of background (N) and object (RO). As before, the regions will be merged based on merging rule and the stage will be repeated iteratively and updated and stops when the entire region N can not find new region for merging. These stages will be repeated again until the merging is not occured. Finaly, all remaining regions in N will be labeled as object and merged into RO and we can easily extract the object from the image."
    }, {
      "heading" : "4. EXPERIMENTAL RESULTS",
      "text" : "Our results show that the proposed method is very powerfull and robust for object recognition and detection. We trained three objects (rack, bottle and calendar) from an online dataset [16] in several views with different scales and illuminations for checking our method. Using ASIFT keypoints in the region merging algorithm based on maximal similarity, helped us to obtain significant results. These keypoints are very exact and come from a strong algorithm (ASIFT) with statistical basis. Moreover, the keypoints give us the best information of objects which are very useful for merging process.\nFigure 7, shows ASIFT keypoints on the images with initial mean shift segmentation and detected objects with their boundaries. First row, (a), is relevant to the rack dataset, second row, (b), is the bottle and thirth row, (c), is the calendar.\nAccuracy rate (6) of full boundary detection for each object is computed and shown in Table 1. Results show that the accuracy rate of datasets is close together.\ndo\nto\nN\nN RateAccuracy ×= 100 (6)\nWhere Nto indicates the number of trained images of each dataset and Ndo means the number of full detected objects respective to the dataset.\nIn the previous paper [17], we used SIFT keypoints to recognize the objects and utilized City Block distance as similarity measure to compare the regions. But, in this paper, we have used ASIFT keypoints instead of SIFT keypoints. Also, the Euclidean distance has been replaced with the City Block distance. Comparison between detection rates of [17] and the proposed algorithm in this paper, shows that the proposed method is more accurate than [17] for object recognition and detection. For this Comparison, we have applied the proposed algorithm on 3 used dataset in [17]. The results of the comparison is shown in Table 2."
    }, {
      "heading" : "5. CONCLUSIONS",
      "text" : "This paper presented an object recognition and detection algorithm. These targets are achieved by combining ASIFT and a region merging segmentation algorithm based on a similarity measure. We train different objects seperately in several images with multiple aspects and camera viewpoints to find the best keypoints for recognizing them in the other images. These keypoints will be applied to the region merging algorithm. The merging process is started by using keypoints and presented similarity measure (Euclidean distance). The regions will be merged based on the merging role and Finaly, the object will be detected well, with its boundary. A final conclusion is that the more keypoints are obtained, and the more accurate they are, the results will be better and more acceptable. Currently, we are working to develop an approach for shape recognition of objects in images. We hope to present a robust algorithm for shape recognition by using the extracted boundaries of objects based on the proposed algorithm in this paper."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "Object recognition is an important task in image processing and computer vision. This paper presents a<lb>perfect method for object recognition with full boundary detection by combining affine scale invariant<lb>feature transform (ASIFT) and a region merging algorithm. ASIFT is a fully affine invariant algorithm that<lb>means features are invariant to six affine parameters namely translation (2 parameters), zoom, rotation<lb>and two camera axis orientations. The features are very reliable and give us strong keypoints that can be<lb>used for matching between different images of an object. We trained an object in several images with<lb>different aspects for finding best keypoints of it. Then, a robust region merging algorithm is used to<lb>recognize and detect the object with full boundary in the other images based on ASIFT keypoints and a<lb>similarity measure for merging regions in the image. Experimental results show that the presented method<lb>is very efficient and powerful to recognize the object and detect it with high accuracy.",
    "creator" : "PScript5.dll Version 5.2.2"
  }
}