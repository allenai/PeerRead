{
  "name" : "1410.1141.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Computational Efficiency of Training Neural Networks",
    "authors" : [ "Roi Livni", "Shai Shalev-Shwartz" ],
    "emails" : [ "roi.livni@mail.huji.ac.il", "shais@cs.huji.ac.il", "ohad.shamir@weizmann.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "One of the most significant recent developments in machine learning has been the resurgence of “deep learning”, usually in the form of artificial neural networks. A combination of algorithmic advancements, as well as increasing computational power and data size, has led to a breakthrough in the effectiveness of neural networks, and they have been used to obtain very impressive practical performance on a variety of domains (a few recent examples include [17, 16, 24, 10, 7]).\nA neural network can be described by a (directed acyclic) graph, where each vertex in the graph corresponds to a neuron and each edge is associated with a weight. Each neuron calculates a weighted sum of the outputs of neurons which are connected to it (and possibly adds a bias term). It then passes the resulting number through an activation function σ : R → R and outputs the resulting number. We focus on feed-forward neural networks, where the neurons are arranged in layers, in which the output of each layer forms the input of the next layer. Intuitively, the input goes through several transformations, with higher-level concepts derived from lower-level ones. The depth of the network is the number of layers and the size of the network is the total number of neurons.\nFrom the perspective of statistical learning theory, by specifying a neural network architecture (i.e. the underlying graph and the activation function) we obtain a hypothesis class, namely, the set of all prediction rules obtained by using the same network architecture while changing the weights of the network. Learning the class involves finding a specific set of weights, based on training examples, which yields a predictor that has good performance on future examples. When studying a hypothesis class we are usually concerned with three questions:\n1. Sample complexity: how many examples are required to learn the class. 2. Expressiveness: what type of functions can be expressed by predictors in the class. 3. Training time: how much computation time is required to learn the class.\nar X\niv :1\n41 0.\n11 41\nv1 [\ncs .L\nG ]\n5 O\nct 2\n01 4\nFor simplicity, let us first consider neural networks with a threshold activation function (i.e. σ(z) = 1 if z > 0 and 0 otherwise), over the boolean input space, {0, 1}d, and with a single output in {0, 1}. The sample complexity of such neural networks is well understood [3]. It is known that the VC dimension grows linearly with the number of edges (up to log factors). It is also easy to see that no matter what the activation function is, as long as we represent each weight of the network using a constant number of bits, the VC dimension is bounded by a constant times the number of edges. This implies that empirical risk minimization - or finding weights with small average loss over the training data - can be an effective learning strategy from a statistical point of view.\nAs to the expressiveness of such networks, it is easy to see that neural networks of depth 2 and sufficient size can express all functions from {0, 1}d to {0, 1}. However, it is also possible to show that for this to happen, the size of the network must be exponential in d (e.g. [19, Chapter 20]). Which functions can we express using a network of polynomial size? The theorem below shows that all boolean functions that can be calculated in time O(T (d)), can also be expressed by a network of depth O(T (d)) and size O(T (d)2).\nTheorem 1. Let T : N→ N and for every d, let Fd be the set of functions that can be implemented by a Turing machine using at most T (d) operations. Then there exist constants b, c ∈ R+ such that for every d, there is a network architecture of depth c T (d) + b, size of (c T (d) + b)2, and threshold activation function, such that the resulting hypotesis class contains Fd.\nThe proof of the theorem follows directly from the relation between the time complexity of programs and their circuit complexity (see, e.g., [22]), and the fact that we can simulate the standard boolean gates using a fixed number of neurons.\nWe see that from the statistical perspective, neural networks form an excellent hypothesis class; On one hand, for every runtime T (d), by using depth ofO(T (d)) we contain all predictors that can be run in time at most T (d). On the other hand, the sample complexity of the resulting class depends polynomially on T (d).\nThe main caveat of neural networks is the training time. Existing theoretical results are mostly negative, showing that successfully learning with these networks is computationally hard in the worst case. For example, neural networks of depth 2 contain the class of intersection of halfspaces (where the number of halfspaces is the number of neurons in the hidden layer). By reduction to k-coloring, it has been shown that finding the weights that best fit the training set is NP-hard ([9]). [6] has shown that even finding weights that result in close-to-minimal empirical error is computationally infeasible. These hardness results focus on proper learning, where the goal is to find a nearly-optimal predictor with a fixed network architecture A. However, if our goal is to find a good predictor, there is no reason to limit ourselves to predictors with one particular architecture. Instead, we can try, for example, to find a network with a different architecture A′, which is almost as good as the best network with architecture A. This is an example of the powerful concept of improper learning, which has often proved useful in circumventing computational hardness results. Unfortunately, there are hardness results showing that even with improper learning, and even if the data is generated exactly from a small, depth-2 neural network, there are no efficient algorithms which can find a predictor that performs well on test data. In particular, [15] and [12] have shown this in the case of learning intersections of halfspaces, using cryptographic and average case complexity assumptions. On a related note, [4] recently showed positive results on learning from data generated by a neural network of a certain architecture and randomly connected weights. However, the assumptions used are strong and unlikely to hold in practice.\nDespite this theoretical pessimism, in practice, modern-day neural networks are trained successfully in many learning problems. There are several tricks that enable successful training:\n• Changing the activation function: The threshold activation function, σ(a) = 1a>0, has zero derivative almost everywhere. Therefore, we cannot apply gradient-based methods with this activation function. To circumvent this problem, we can consider other activation functions. Most widely known is a sigmoidal activation, e.g. σ(a) = 11+ea , which forms a smooth approximation of the threshold function. Another recent popular activation function is the rectified linear unit (ReLU) function,\nσ(a) = max{0, a}. Note that subtracting a shifted ReLU from a ReLU yields an approximation of the threshold function, so by doubling the number of neurons we can approximate a network with threshold activation by a network with ReLU activation.\n• Over-specification: It was empirically observed that it is easier to train networks which are larger than needed. Indeed, we empirically demonstrate this phenomenon in Sec. 5.\n• Regularization: It was empirically observed that regularizing the weights of the network speeds up the convergence (e.g. [16]).\nThe goal of this paper is to revisit and re-raise the question of neural network’s computational efficiency, from a modern perspective. This is a challenging topic, and we do not pretend to give any definite answers. However, we provide several results, both positive and negative. Most of them are new, although a few appeared in the literature in other contexts. Our contributions are as follows:\n• We prove that for sufficiently over-specified networks, all local minima are also global. This can be seen as an indication that large neural networks can indeed be more amenable to gradient based methods used to train neural networks in practice.\n• Motivated by the idea of changing the activation function, we consider the quadratic activation function, σ(a) = a2. Networks with the quadratic activation compute polynomial functions of the input in Rd, hence we call them polynomial networks. Our main findings for such networks are as follows: – Networks with quadratic activation are as expressive as networks with threshold activation. – Constant depth networks with quadratic activation can be learned in polynomial time. – Sigmoidal networks of depth 2, and with `1 regularization, can be approximated by polynomial\nnetworks of depth O(log log(1/ )). It follows that sigmoidal networks with `1 regularization can be learned in polynomial time as well.\n– The aforementioned positive results are interesting theoretically, but lead to impractical algorithms. We provide a practical, provably correct, algorithm for training depth-2 polynomial networks. While such networks can also be learned using a linearization trick, our algorithm is more efficient and returns networks whose size does not depend on the data dimension. Our algorithm follows a forward greedy selection procedure, where each step of the greedy selection procedure builds a new neuron by solving an eigenvalue problem.\n– We generalize the above algorithm to depth-3, in which each forward greedy step involves an efficient approximate solution to a tensor approximation problem. The algorithm can learn a rich sub-class of depth-3 polynomial networks.\n– We describe some experimental evidence, showing that our practical algorithm is competitive with state-of-the-art neural network training methods for depth-2 networks."
    }, {
      "heading" : "2 Sufficiently Over-Specified Networks Have No Local Minima",
      "text" : "We begin by considering the idea of over-specification, and showing that for sufficiently over-specified networks, the optimization problem associated with training them has no local (non-global) minima. This can be seen as an indication that large neural networks are indeed more amenable to gradient-based methods used to train neural networks in practice. As an interesting contrast, note that for very small networks (such as a single neuron with a non-convex activation function) there can easily be exponentially many local (non-global) minima [5].\nTo present the formal result, let X ∈ Rd,m be a matrix of m training examples in Rd. We can think of the network as composed of two mappings. The first maps X into a matrix Z ∈ Rn,m, where n is the number of neurons whose outputs are connected to the output layer. The second mapping is a linear mapping Z 7→ WZ, where W ∈ Ro,n, that maps Z to the o neurons in the output layer. Finally, there is a loss function ` : Ro,m → R that assesses the quality of the prediction on the entire data (and will of course depend on the m labels). Let V denote all the weights that affect the mapping from X to Z,\nand denote by f(V ) the function that takes V and outputs Z. The optimization problem associated with learning the network is therefore minW,V `(W f(V )).\nThe function `(W f(V )) is generally non-convex, and may have local minima. However, the following theorem shows that if n is sufficiently large then all local minima are also global. Theorem 2. Assume that n ≥ m. If f(V ) is a continuous function, and the set {V : Rank (f(V )) = m} is dense in its domain, then every local minimum (W,V ) of `(W f(V )) is also a global minimum.\nThe proof is provided in the appendix. The assumption that f(V ) is continuous holds for any feedforward neural network, as long as the activation functions are continuous. The assumption that f(V ) has full-rank is reasonable considering the non-linear nature of the function computed by the neural network1.\nPreviously, we have argued, based on the results of [12], that even if data is generated exactly from a depth-2 neural network of small size, there is no efficient training algorithm which can find a predictor that performs well on test data. Seemingly, if we use sufficiently over-specified networks, gradient based method should be able to find a good solution. There is no contradiction, because our proof requires that the network will be very large, so it might overfit on the test data. Nevertheless, we believe that the idea of over-specification is interesting and may become useful under some distributional assumptions."
    }, {
      "heading" : "3 The Hardness of Learning Neural Networks",
      "text" : "We now review several known hardness results and apply them to our learning setting. For simplicity, throughout most of this section we focus on the PAC model in the binary classification case, over the Boolean cube, in the realizable case, and with a fixed target accuracy.2\nFix some , δ ∈ (0, 1). For every dimension d, let the input space be Xd = {0, 1}d and let H be a hypothesis class of functions from Xd to {±1}. We often omit the subscript d when it is clear from context. A learning algorithm A has access to an oracle that samples x according to an unknown distribution D over X and returns (x, f∗(x)), where f∗ is some unknown target hypothesis in H . The objective of the algorithm is to return a classifier f : X → {±1}, such that with probability of at least 1− δ,\nPx∼D [f(x) 6= f∗(x)] ≤ .\nWe say that A is efficient if it runs in time poly(d) and the function it returns can also be evaluated on a new instance in time poly(d). If there is such A, we say that H is efficiently learnable.\nIn the context of neural networks, every network architecture defines a hypothesis class, Nt,n,σ , that contains all target functions f that can be implemented using a neural network with t layers, n neurons (excluding input neurons), and an activation function σ. The immediate question is which Nt,n,σ are efficiently learnable. We will first address this question for the threshold activation function, σ0,1(z) = 1 if z > 0 and 0 otherwise.\nObserving that depth-2 networks with the threshold activation function can implement intersections of halfspaces, we will rely on the following hardness results, due to [15]. Theorem 3 (Theorem 1.2 in [15]). Let X = {±1}d, let\nHa = { x→ σ0,1 ( w>x− b− 1/2 ) : b ∈ N, w ∈ Nd, |b|+ ‖w‖1 ≤ poly(d) } ,\nand letHak = {x→ h1(x)∧h2(x)∧ . . .∧hk(x) : ∀i, hi ∈ Ha}, where k = dρ for some constant ρ > 0. Then under a certain cryptographic assumption, Hak is not efficiently learnable.\n1For example, consider the function computed by the first layer, X 7→ σ(VdX). Since the activation function σ is non-linear, then the columns of σ(VdX) will not be linearly dependent in general.\n2While we focus on the realizable case (i.e., there exists f∗ ∈ H that provides perfect predictions), with a fixed accuracy ( ) and confidence (δ), since we are dealing with hardness results, the results trivially apply to the agnostic case and to learning with arbitrarily small accuracy and confidence parameters.\nUnder a different complexity assumption, [12] showed a similar result even for k = ω(1).\nAs mentioned before, neural networks of depth ≥ 2 and with the σ0,1 activation function can express intersections of halfspaces: For example, the first layer consists of k neurons computing the k halfspaces, and the second layer computes their conjunction by the mapping x 7→ σ0,1 ( ∑ i xi − k + 1/2). Trivially, if some class H is not efficiently learnable, then any class containing it is also not efficiently learnable. We thus obtain the following corollary:\nCorollary 1. For every t ≥ 2, n = ω(1), the class Nt,n,σ0,1 is not efficiently learnable (under the complexity assumption given in [12]).\nWhat happens when we change the activation function? In particular, two widely used activation functions for neural networks are the sigmoidal activation function, σsig(z) = 1/(1 + exp(−z)), and the rectified linear unit (ReLU) activation function, σrelu(z) = max{z, 0}. As a first observation, note that for |z| 1 we have that σsig(z) ≈ σ0,1(z). Our data domain is the discrete Boolean cube, hence if we allow the weights of the network to be arbitrarily large, then Nt,n,σ0,1 ⊆ Nt,n,σsig . Similarly, the function σrelu(z)−σrelu(z− 1) equals σ0,1(z) for every |z| ≥ 1. As a result, without restricting the weights, we can simulate each threshold activated neuron by two ReLU activated neurons, which implies that Nt,n,σ0,1 ⊆ Nt,2n,σrelu . Hence, Corollary 1 applies to both sigmoidal networks and ReLU networks as well, as long as we do not regularize the weights of the network.\nWhat happens when we do regularize the weights? LetNt,n,σ,L be all target functions that can be implemented using a neural network of depth t, size n, activation function σ, and when we restrict the input weights of each neuron to be ‖w‖1 + |b| ≤ L. One may argue that in many real world distributions, the difference between the two classes,Nt,n,σ,L and Nt,n,σ0,1 is small. Roughly speaking, when the distribution density is low around the decision boundary of neurons (similarly to separation with margin assumptions), then sigmoidal neurons will be able to effectively simulate threshold activated neurons.\nIn practice, the sigmoid and ReLU activation functions are advantageous over the threshold activation function, since they can be trained using gradient based methods. Can these empirical successes be turned into formal guarantees? Unfortunately, a closer examination of Thm. 3 demonstrates that if L = Ω(d) then learning N2,n,σsig,L and N2,n,σrelu,L is still hard. Formally, to apply these networks to binary classification, we follow a standard definition of learning with a margin assumption: We assume that the learner receives examples of the form (x, sign(f∗(x))) where f∗ is a real-valued function that comes from the hypothesis class, and we further assume that |f∗(x)| ≥ 1. Even under this margin assumption, we have the following:\nCorollary 2. For every t ≥ 2, n = ω(1), L = Ω(d), the classes Nt,n,σsig,L and Nt,n,σrelu,L are not efficiently learnable (under the complexity assumption given in [12]).\nA proof is provided in the appendix.\nWhat happens when L is much smaller? Later on in the paper we will show positive results for L being a constant and the depth being fixed. These results will be obtained using polynomial networks, which we study in the next section."
    }, {
      "heading" : "4 Polynomial Networks",
      "text" : "In the previous section we have shown several strong negative results for learning neural networks with the threshold, sigmoidal, and ReLU activation functions. One way to circumvent these hardness results is by considering another activation function. Maybe the simplest non-linear function is the squared function, σ2(x) = x2. We call networks that use this activation function polynomial networks, since they compute polynomial functions of their inputs. As in the previous section, we denote by Nt,n,σ2,L the class of functions that can be implemented using a neural network of depth t, size n, squared activation\nfunction, and a bound L on the `1 norm of the input weights of each neuron. Whenever we do not specify L we refer to polynomial networks with unbounded weights.\nBelow we study the expressiveness and computational complexity of polynomial networks. We note that algorithms for efficiently learning (real-valued) sparse or low-degree polynomials has been studied in several previous works (e.g. [13, 14, 8, 2, 1]). However, these rely on strong distributional assumptions, such as the data instances having a uniform or log-concave distribution, while we are interested in a distribution-free setting."
    }, {
      "heading" : "4.1 Expressiveness",
      "text" : "We first show that, similarly to networks with threshold activation, polynomial networks of polynomial size can express all functions that can be implemented efficiently using a Turing machine. Theorem 4 (Polynomial networks can express Turing Machines). Let Fd and T be as in Thm. 1. Then there exist constants b, c ∈ R+ such that for every d, the class Nt,n,σ2,L, with t = c T (d) log(T (d)) + b, n = t2, and L = b, contains Fd.\nThe proof of the theorem relies on the result of [18] and is given in the appendix.\nAnother relevant expressiveness result, which we will use later, shows that polynomial networks can approximate networks with sigmoidal activation functions:\nTheorem 5. Fix 0 < < 1, L ≥ 3 and t ∈ N. There are Bt ∈ Õ(log(tL + L log 1 )) and Bn ∈ Õ(tL + L log 1 ) such that for every f ∈ Nt,n,σsig,L there is a function g ∈ NtBt,nBn,σ2 , such that sup‖x‖∞<1 ‖f(x)− g(x)‖∞ ≤ .\nThe proof relies on an approximation of the sigmoid function based on Chebyshev polynomials, as was done in [21], and is given in the appendix."
    }, {
      "heading" : "4.2 Training Time",
      "text" : "We now turn to the computational complexity of learning polynomial networks. We first show that it is hard to learn polynomial networks of depth Ω(log(d)). Indeed, by combining Thm. 5 and Corollary 2 we obtain the following: Corollary 3. The class Nt,n,σ2 , where t = Ω(log(d)) and n = Ω(d), is not efficiently learnable.\nOn the flip side, constant-depth polynomial networks can be learned in polynomial time, using a simple linearization trick. Specifically, the class of polynomial networks of constant depth t is contained in the class of multivariate polynomials of total degree at most s = 2t. This class can be represented as a dsdimensional linear space, where each vector is the coefficient vector of some such polynomial. Therefore, the class of polynomial networks of depth t can be learned in time poly(d2 t\n), by mapping each instance vector x ∈ Rd to all of its monomials, and learning a linear predictor on top of this representation (which can be done efficiently in the realizable case, or when a convex loss function is used). In particular, if t is a constant then so is 2t and therefore polynomial networks of constant depth are efficiently learnable. Another way to learn this class is using support vector machines with polynomial kernels.\nAn interesting application of this observation is that depth-2 sigmoidal networks are efficiently learnable with sufficient regularization, as formalized in the result below. This contrasts with corollary 2, which provides a hardness result without regularization. Theorem 6. The class N2,n,σsig,L can be learned, to accuracy , in time poly(T ) where T = (1/ ) · O(d4L ln(11L 2+1)).\nThe idea of the proof is as follows. Suppose that we obtain data from some f ∈ N2,n,σsig,L. Based on Thm. 5, there is g ∈ N2Bt,nBn,σ2 that approximates f to some fixed accuracy 0 = 0.5, where Bt\nand Bn are as defined in Thm. 5 for t = 2. Now we can learn N2Bt,nBn,σ2 by considering the class of all polynomials of total degree 22Bt , and applying the linearization technique discussed above. Since f is assumed to separate the data with margin 1 (i.e. y = sign(f∗(x)),|f∗(x)| ≥ 1|), then g separates the data with margin 0.5, which is enough for establishing accuracy in sample and time that depends polynomially on 1/ ."
    }, {
      "heading" : "4.3 Learning 2-layer and 3-layer Polynomial Networks",
      "text" : "While interesting theoretically, the above results are not very practical, since the time and sample complexity grow very fast with the depth of the network.3 In this section we describe practical, provably correct, algorithms for the special case of depth-2 and depth-3 polynomial networks, with some additional constraints. Although such networks can be learned in polynomial time via explicit linearization (as described in section 4.2), the runtime and resulting network size scales quadratically (for depth-2) or cubically (for depth-3) with the data dimension d. In contrast, our algorithms and guarantees have a much milder dependence on d.\nWe first consider 2 layer polynomial networks, of the following form:\nP2,k = { x 7→ b+ w>0 x + k∑ i=1 αi(w > i x) 2 : ∀i ≥ 1, |αi| ≤ 1, ‖wi‖2 = 1 } .\nThis networks corresponds to one hidden layer containing r neurons with the squared activation function, where we restrict the input weights of all neurons in the network to have bounded `2 norm, and where we also allow a direct linear dependency between the input layer and the output layer.\nWe’ll describe an efficient algorithm for learning this class, which is based on the GECO algorithm for convex optimization with low-rank constraints [20].\nThe goal of the algorithm is to find f that minimizes the objective\nR(f) = 1\nm m∑ i=1 `(f(xi), yi), (1)\nwhere ` : R× R→ R is a loss function. We’ll assume that ` is β-smooth and convex. The basic idea of the algorithm is to gradually add hidden neurons to the hidden layer, in a greedy manner, so as to decrease the loss function over the data. To do so, define V = {x 7→ (w>x)2 : ‖w‖2 = 1} the set of functions that can be implemented by hidden neurons. Then every f ∈ P2,r is an affine function plus a weighted sum of functions from V . The algorithm starts with f being the minimizer of R over all affine functions. Then at each greedy step, we search for g ∈ V that minimizes a first order approximation of R(f + ηg):\nR(f + ηg) ≈ R(f) + η 1 m m∑ i=1 `′(f(xi), yi)g(xi) , (2)\nwhere `′ is the derivative of ` w.r.t. its first argument. Observe that for every g ∈ V there is some w with ‖w‖2 = 1 for which g(x) = (w>x)2 = w>xx>w. Hence, the right-hand side of Eq. (2) can be rewritten as R(f) + η w> ( 1 m ∑m i=1 ` ′(f(xi), yi)xix > i ) w . The vector w that minimizes this expression\n(for positive η) is the leading eigenvector of the matrix (\n1 m ∑m i=1 ` ′(f(xi), yi)xix > i ) . We add this vector\n3If one uses SVM with polynomial kernels, the time and sample complexity may be small under margin assumptions in a feature space corresponding to a given kernel. Note, however, that large margin in that space is very different than the assumption we make here, namely, that there is a network with a small number of hidden neurons that works well on the data.\nas a hidden neuron to the network.4 Finally, we minimize R w.r.t. the weights from the hidden layer to the output layer (namely, w.r.t. the weights αi).\nThe following theorem, which follows directly from Theorem 1 of [20], provides convergence guarantee for GECO. Observe that the theorem gives guarantee for learning P2,k if we allow to output an overspecified network.\nTheorem 7. Fix some > 0. Assume that the loss function is convex and β-smooth. Then if the GECO Algorithm is run for r > 2βk 2\niterations, it outputs a network f ∈ N2,r,σ2 for which R(f) ≤ minf∗∈P2,k R(f∗) + .\nWe next consider a hypothesis class consisting of third degree polynomials, which is a subset of 3-layer polynomial networks (see Lemma 1 in the appendix) . The hidden neurons will be functions from the class:\nV = ∪3i=1Vi where Vi = x 7→ i∏\nj=1\n(w>j x) : ∀j, ‖wj‖2 = 1  . The hypothesis class we consider is P3,k = { x 7→ ∑k i=1 αigi(x) : ∀i, |αi| ≤ 1, gi ∈ V } .\nThe basic idea of the algorithm is the same as for 2-layer networks. However, while in the 2-layer case we could implement efficiently each greedy step by solving an eigenvalue problem, we now face the following tensor approximation problem at each greedy step:\nmax g∈V3\n1\nm m∑ i=1 `′(f(xi), yi)g(xi) = max ‖w‖=1,‖u‖=1,‖v‖=1 1 m m∑ i=1 `′(f(xi), yi)(w >xi)(u >xi)(v >xi) .\nWhile this is in general a hard optimization problem, we can approximate it – and luckily, an approximate greedy step suffices for success of the greedy procedure. This procedure is given in Figure 1, and is again based on an approximate eigenvector computation. A guarantee for the quality of approximation is given in the appendix, and this leads to the following theorem, whose proof is given in the appendix.\nTheorem 8. Fix some δ, > 0. Assume that the loss function is convex and β-smooth. Then if the GECO Algorithm is run for r > 4dβk 2\n(1−τ)2 iterations, where each iteration relies on the approximation procedure given in Figure 1, then with probability (1− δ)r, it outputs a network f ∈ N3,5r,σ2 for which R(f) ≤ minf∗∈P3,k R(f∗) + .\n4It is also possible to find an approximate solution to the eigenvalue problem and still retain the performance guarantees (see [20]). Since an approximate eigenvalue can be found in time O(d) using the power method, we obtain the runtime of GECO depends linearly on d."
    }, {
      "heading" : "5 Experiments",
      "text" : "To demonstrate the practicality of GECO to train neural networks for real world problems, we considered a pedestrian detection problem as follows. We collected 200k training examples of image patches of size 88x40 pixels containing either pedestrians (positive examples) or hard negative examples (containing images that were classified as pedestrians by applying a simple linear classifier in a sliding window manner). See a few examples of images above. We used half of the examples as a\ntraining set and the other half as a test set. We calculated HoG features ([11]) from the images 5.\nWe then trained, using GECO, a depth-2 polynomial network on the resulting features. We used 40 neurons in the hidden layer. For comparison we trained the same network architecture (i.e. 40 hidden neurons with a squared activation function) by SGD. We also trained a similar network (40 hidden neurons again) with the ReLU activation function. For the SGD implementation we tried the following tricks to speed up the convergence: heuristics for initialization of the weights, learning rate rules, mini-batches, Nesterov’s momentum (as explained in [23]), and dropout. The test errors of SGD as a function of the number of iterations are depicted on the top plot of the Figure on the side. We also mark the performance of GECO as a straight line (since it doesn’t involve SGD iterations). As can be seen, the error of GECO is slightly better than SGD. It should be also noted that we had to perform a very large number of SGD iterations to obtain a good solution, while the runtime of GECO was much faster. This indicates that GECO may be a valid alternative approach to SGD for training depth-2 networks. It is also apparent that the squared activation function is slightly better than the ReLU function for this task.\nThe second plot of the side figure demonstrates the benefit of overspecification for SGD. We generated random examples in R150 and passed them through a random depth-2 network that contains 60 hidden neurons with the ReLU activation function. We then tried to fit a new network to this data with over-specification factors of 1, 2, 4, 8 (e.g., over-specification factor of 4 means that we\nused 60 · 4 = 240 hidden neurons). As can be clearly seen, SGD converges much faster when we overspecify the network.\nAcknowledgements: This research is supported by Intel (ICRI-CI). OS was also supported by an ISF grant (No. 425/13), and a Marie-Curie Career Integration Grant."
    }, {
      "heading" : "A Proofs",
      "text" : ""
    }, {
      "heading" : "A.1 Proof of Thm. 2",
      "text" : "Suppose by contradiction that (W,V ) is not a global minimum. Then for any sufficiently small > 0, there exists some (W ∗, V ∗) such that `(W ∗ f(V ∗)) ≤ `(W f(V ))− . Since {V : Rank (f(V )) = m} is dense and f(V ) is continuous in V , then we can find V̂ such that Rank(f(V̂ )) = m, ρ(V, V̂ ) ≤ , and\n`(W f(V̂ )) ≥ `(W f(V ))− 2 ≥ `(W ∗ f(V ∗)) + 2 . (3)\nConsider the o ×m matrix W ∗ f(V ∗). Since f(V̂ ) has rank m, the rows of f(V̂ ) span Rm. Therefore, there exists some W̄ ∈ Ro×n such that W̄f(V̂ ) = W ∗ f(V ∗). Hence, by Eq. (3),\n`(W f(V̂ )) ≥ `(W ∗ f(V ∗)) + 2 = `(W̄f(V̂ )) + 2 . (4)\nWe now show that this inequality contradicts (W ;V ) being a local minimum. Indeed, consider the matrix Ŵ = (1− δ )W + δ W̄ , where δ is positive and sufficiently small so that ‖Ŵ −W‖ ≤ . By convexity of ` and Eq. (4), we have\n`(Ŵ f(V̂ )) = ` ( (1− δ )W f(V̂ ) + δ W̄ f(V̂ )\n≤ (1− δ )`(W f(V̂ )) + δ `(W̄ f(V̂ )) ≤ (1− δ )`(W f(V̂ )) + δ ( `(W f(V̂ ))−\n2\n) = `(W f(V ))− δ\n2 .\nOverall, we found a pair (Ŵ , V̂ ) such that ‖Ŵ −W‖ ≤ , ρ(V, V̂ ) ≤ , and `(Ŵ f(V̂ )) < `(W f(V̂ )). Since is arbitrarily small, this contradicts the assumption that (W,V ) is a local minimum."
    }, {
      "heading" : "A.2 Proof of Corollary 2",
      "text" : "A.2.1 Hardness result for the class N2,n,σsig,L:\nConsider Ha as defined in Thm. 3. Note that for every h ∈ Ha there are integral w and b such that h(x) = w>x− b− 12 and we have that |h(x)| ≥ 1/2. Given k hyperplanes {hi} k i=1 consider the neurons\ngi(x) = 1/ (1 + exp (−Chi(x))) , where C ∈ ω(1) is to be chosen later. Let\ng(x) = Cd\n2k + 13 ( k∑ i=1 gi(x)− k + 1 3 ) .\nIf ‖w‖1+ |b+ 12 | ≤ dwe have that g(x) ∈ N2,n,σsig,L, whenever L ≥ Cd. ChooseC ∈ O(k) sufficiently large so that\n1/ ( 1 + exp ( −C\n2\n)) − 1 > − 1\n3k .\nand\n1/ ( 1 + exp ( C\n2\n)) < 2\n3 .\nSince |hi(x)| ≥ 12 for all i, if the output of all neurons {gi} is positive we have\n2k + 1/3\nCd g(x) ≥ k\n( 1\n1 + exp(−C2 ) − 1\n) + 1\n3 > 0.\nOn the other hand, if hi(x) < − 12 for some i we have that\n2k + 1/3\nCd g(x) ≤ k−1∑ i=1 gi(x) + 1 1 + exp(C2 ) − k + 1 3 ≤ k − 1− k + 1 1 + exp C2 + 1 3 < 0.\nWe’ve demonstrated that the target function sign(g(x)) implements h1 ∧ h2 ∧ . . . ∧ hk thus Hak ⊆ N2,k+1,σsig,L.\nA.2.2 Hardness result for the class N2,n,σrelu,L:\nAgain, given k hyperplanes {hi}ki=1, for every k consider the two neurons:\ng+i (x) = max{0, 2hi(x)}, g − i = (x) max{0, 2hi(x)− 1}.\nAnd let\ng(x) = 1\n2k ( k∑ i=1 ( g+i (x)− g − i (x) ) − k ) .\nAs before g(x) ∈ N2,2k+1,σrelu,L, whenever L ≥ 2d. One can also verify that g(x) implements h1∧h2∧ . . . ∧ hk."
    }, {
      "heading" : "A.3 Proof of Thm. 4",
      "text" : "We start by showing that we can implement AND,OR,NEG, Id gates using polynomial networks of fixed depth and size. As a corollary, we can implement circuits with fixed number of fan-ins. NEG can be implemented with x 7→ 1 − x and Id can be implemented with x 7→ 14 ( (x+ 1)2 − (x− 1)2 ) . Next note that\nAND(x1,x2) = x1 · x2, and OR(x1,x2) = x1 + x2 −AND(x1,x2). and that x1 · x2 = 14 ( (x2 + x1) 2 − (x2 − x1)2 ) . Thus we can implement with two layers a conjunction and disjunction of 2 neurons. By adding a fixed number of layers, we can also implement the conjunction and disjunction of any fixed number of neurons. Therefore, if B is a circuit with fixed number of fan-ins, of size T, we can implement it using a polynomial network with O(T ) layers and O(T 2) neurons, where layer t simulates the calculation of all gates at depth t.\nNow, by [18], any Turing machine with runtime T can be simulated by an oblivious Turing machine with O(T log T )-steps. An oblivious Turing machine is a machine such that the position of the machine head at time t does not depend on the input of the machine (and therefore is known ahead of time). We can now easily simulate the machine by a network of depth O(T log T ), where the nodes at each layer contain the state of the turing machine (the content of the tape and the position at the state machine), and the transition from layer to layer depends on a constant size circuit, and hence can be implemented by a constant depth polynomial network."
    }, {
      "heading" : "A.4 Proof of Thm. 5",
      "text" : "The idea of proof of Thm. 5 is as follows: First we show that we can express any T -degree polynomial using O(log T ) layers and O(T ) neurons. This is done in Lemma 1 part 4. As a second step, we show in Lemma 2 that a sigmoidal function can be approximated in a ball of radius L by a O(log L )-degree polynomial. The result follows by replacing each sigmoid activation unit with added layers that approximate the sigmoidal function on the output of the previous layer. We will first prove the two Lemmas. The proof of Thm. 5 is then given at the end of the section.\nLemma 1. The following statements hold:\n1. If g ∈ Nt,n,σ2,L for some L ≥ 2 then g ∈ Nt′,n+2(t′−t),σ2,L for every t′ ≥ t.\n2. If G ∈ Nt,n,σ2,L for some L ≥ 2 and G is a network with two output neurons g1 and g2 then g1 · g2 ∈ Nt+1,n+1,σ2 .\n3. If g ∈ Nt,n,σ2,L for some L ≥ 2 then (g)T ∈ Nt′,n′,σ2,L. where t′ = t+ log T + log log T and n′ = n+ 2 log T + log T (log log T ).\n4. If g ∈ Nt,n,σ2,L then ∑T i=1 ai(g(x))\nk is in Nt′,n′,σ2,L′ where t′ = t+ log T + log log T, n′ = n+ 2‖a‖0(2 log T + log T (log log T ),\nwhere ‖a‖0 = |{k : ak 6= 0}|. And L′ = max{‖a‖1, L, 2}.\nProof. 1. Proof of 1]: Note that 14 ((x + 1) 2 − (x − 1)2) = x. Next we prove the statement by\ninduction. For t′ = t, the satement is trivial. Next assume that g ∈ Nt,n+2(t′−t),σ2,L. Let\nh1(x) =\n( 1\n2 g(x) +\n1\n2\n)2 , h2(x) = ( 1\n2 g(x)− 1 2\n)2 .\nLet h(x) = h1(x) − h2(x) then h(x) = g(x). By taking the network that implements g, removing the output neuron, adding an additional hidden layer that consists of h1 and h2 and finally adding an additional output neuron we have that h(x) ∈ Nt′+1,n+2(t′−t)+2,σ2,L.\n2. Proof of 2: Like before, note that x1 · x2 = 14 (x1 + x2) 2 − 14 (x1 − x2) 2. Let\nh1(x) = ( 1\n2 g1(x) +\n1 2 g2(x)) 2, h2(x) = ( 1 2 g1(x) + 1 2 g2(x)) 2.\nAs before we remove from the network that implements G the two nodes at the output layer, add an additional hidden layer that implements h1 and h2 and finally add an output neuron h(x) = h1(x)− h2(x).\n3. Proof of 3:Write T = ∑log T i=1 i2\ni where i = {0, 1}. We will first show that we can construct a polynomial network that contains in layer t + log T neurons h1, . . . , hlog T such that hk(x) = (g(x))2 k\n. It is easy to see that we can implement a neuron h′(x)k at layer t + k such that h′k(x) = (g(x))\n2k . Next, using 1 we add 2(log T − k) neurons and implement h′k in layer t+ log T .\nFinally, we implement ∏ {i: i 6=0} hi(x) using log log T layers and log T log log T additional neurons, this can be done by applying 2 where at each layer we pair the neurons at previous layer and do their product (e.g. if for every i i 6= 0 then at the next layer we implement (h1 ·h2, h3 ·h4, . . . ht−1ht) then at the next layer we implement (h1 ·h2 ·h3 ·h4, . . . , ht−4 · · ·ht) etc..)\n4. Proof of 4: Follows from 1 and 3.\nLemma 2 (Sigmoidals are approximable via polynomial networks). The following holds for any ≥ 0 and (for simplicity) L ≥ 3: Set\nT = log ( 2L4 + exp ( 7L ln ( 4L + 3 ))) + 2 log 8 .\nThere is a polynomial p(x) = ∑T j=1 ajx j , such that:\nsup |x|<4L\n|p(x)− σsig(x)| < .\nProof of Lemma 2. Set\nt′ = log ( 2L4 + exp(7L ln ( 4L + 3 )) .\nAccording to [21] Lemma 2, there is an analytic function q such that\nsup |x|≤1\n|q(x)− σsig(4Lx)| ≤\n2 ,\nand\nq(x) = ∞∑ j=0 βjx j\nwhere ∞∑ j=0 β2j 2 j ≤ 2t ′ .\nNote that for every j we have |βj | ≤ 2 t′−j 2 . Thus\nsup |x|<1 | ∑ j>T βjx j | ≤ ∑ j>T |βj | ≤ ∑ j>T 2 t′−j 2 = 2 t′−T 2 ∞∑ j=1 (√ 2 )−j < 4 · 2 t′−T 2 .\nRecalling that T = t′ + 2 log 8 and letting p0(x) = ∑T j=0 βjx j , we have by triangular inequality that\nsup |x|≤1\n|p0(x)− σsig(4Lx)| ≤ .\nFinally, take p(x) = p0( x4L )."
    }, {
      "heading" : "A.4.1 Back to proof of Thm. 5",
      "text" : "Set\nT = log ( 2L4 + exp(7L ln ( (4L)t + 3 )) + 2 log 8(4L)t−1 .\nand have\nBt = 1 + log T + log log T ∈ Õ ( logL log Lt ) , (5)\nBn = 1 + 2T (2 log T + log T log log T ) ∈ Õ(L log Lt ). (6)\nWe prove the statement by induction on t, our induction hypothesis will hold for networks with not necessarily a single output neuron. For t = 1, since N1,n,σ2 = N1,n,σsig , the statement is trivial. Next let F ∈ Nt,n,σsig,L, assume F : Rd → Rs (i.e. the output layer has s nodes). There is a target function F (t−1) ∈ Nt−1,n−s,σsig,L such that for every i = 1 . . . s we have\nFi = w > i σsig(F (t−1)(x)).\nwhere σsig(F (t−1)(x)) denotes pointwise activation of σsig on the coordinates of F (t−1).\nBy induction, there is some P (t−1) ∈ N(t−1)Bt,(n−s)Bn,σ2 such that\nsup ‖x‖∞≤1\n‖P (t−1)(x)− F (t−1)(x)‖∞ ≤ 4L ≤ 4\nBy Lemma 1 part 4 and Lemma 2 we can add Bt layers and Bn neurons and implement a new target function Pi such that\nPi(x) = w > i p(P (t−1)(x)),\nwhere p is taken from Lemma 2 and satisfies\nsup |x|≤4L\n|p(x)− σsig(x)| < (4L)t−1 ≤ 2L ,\nTaken together we can add sBn nodes to implement a target function P = P1, . . . , Ps. Next,\n‖P (x)−F (x)‖∞ ≤ sup i ‖Pi(x)−w>i σsig(P (t−1)(x))‖+‖w>i σsig(P (t−1)(x))−w>i σsig(F (t−1)(x))‖.\nRecall that the `1-norm of each weight vector of each neuron is bounded by L and that the output of each neuron is bounded by supx σsig(x) = 1, hence: ‖F (t−1)(x)‖∞ ≤ L. By induction we also have that ‖F (t−1)(x)− P (t−1)(x)‖∞ ≤ 1 hence ‖P t−1(x)‖∞ ≤ 2L and we have:\nsup i ‖w>i p(P (t−1)(x))−w>i σsig(P (t−1)(x))‖+ sup i ‖w>i σsig(P (t−1)(x))−w>i σsig(F (t−1)(x))‖ ≤ .\n‖wi‖1 2L + ‖wi‖1‖P (t−1)(x)− F (t−1)(x)‖∞ ≤ 2 + 4 ≤ .\nWhere we used the fact that σsig is 1-Lipschitz."
    }, {
      "heading" : "A.5 Proof of Thm. 8",
      "text" : "That f ∈ N3,5r,σ2 can be shown using Lemma 1 and the output’s structure.\nLet us denote by Approx( (1−τ)√ 2d ,∇R(f)), a procedure that returns g ∈ V such that\nm∑ i=1 `′(f(xi), yi)g(xi) ≥ (1− τ)√ 2d max g∗∈V 1 m m∑ i=1 `′(f(xi), yi)g ∗(xi)\nThe GECO algorithm is presented in Fig. 3 with an Approx procedure that is implemented with respect to V = ∪Vi. The guarantees in Thm. 8 are proven in exactly the same manner as in [20].\nThe remained challenge is to demonstrate that the Approx procedure can be implemented efficiently, relying on the algorithm presented in Fig. 1 . To this end, note that the only difficulty is when g∗ ∈ V3 (since if g∗ ∈ V2 or g∗ ∈ V1 we are back to the 2-layer scenario). The proof follows directly from the following lemma: Lemma 3. Let w∗,u∗,v∗ be the output of the Algorithm in Fig. 1 with parameters δ, τ, {xi}mi=1 and αi = ` ′(f(xi), yi). With probability at least 1− δ:\nF (w∗,u∗,v∗) ≥ 1− τ√ 2d max ‖w‖,‖u‖,‖v‖≤1 F (w,u,v),\nwhere\nF (w,u,v) = 1\nm m∑ i=1 `′(f(xi), yi)(w >xi) · (u>xi) · (v>xi).\nProof. Let us denote by w∗,u∗,v∗ the maximizers of F (w,u,v), over all ‖w‖, ‖u‖, ‖v‖ = 1. For each u,v let f(u,v) be the vector\nf(u,v) = m∑ i=1 αi(u >xi)(v >xi)xi.\nFirst, we claim that f(u∗,v∗) ∝ w∗ and that F (w∗,u∗,v∗) = ‖f(u∗,v∗)‖. Indeed for every ‖w‖ ≤ 1, by the Cauchy-Schwartz inequality:\nF (w,u∗,v∗) = f(u∗,v∗)>w ≤ ‖f(u∗,v∗)‖‖w‖ ≤ ‖f(u∗,v∗)‖. Again by Cauchy-Schwartz, equality is attained if and only if w ∝ f(u∗,v∗). Next, let us consider a single random variable ŵ such that w ∼ N(0, Id) and ŵ = w‖w‖ . Note that for any unit vector u1 we have E((ŵ>u1)2) = 1d . Indeed, extend u1 to an orthonormal basis u1, . . . ,ud. we have that\n1 = E(‖ŵ‖2) = E( d∑ i=1 (ŵ>ui) 2).\nBy symmetry we have that:\n1 = E( d∑ i=1 (ŵ>ui) 2) = d∑ i=1 E((ŵ>ui)2) = dE((ŵ>u1)2).\nIn particular we have E((ŵ>w∗)2) = 1d . In conclusion (ŵ >w∗)2 is a random variable that takes values in [0, 1] and has expected value 1d . Applying the inverse Markov inequality (i.e. applying Markov to the random variable 1− (ŵ>w∗)2), we have that\nP ((ŵ>w∗)2 > 1\n2d ) ≥\n1 d − 1 2d 1− 12d =\n1 2d− 1 ∈ O( 1 2d )\nLetting s ≥ − log 1 δ\nlog(1− 12d ) ≈ 2d log 1δ we have that with probability at least (1 − δ) for some wi we have\n|w>i w∗| ≥ 1√2d , say w1.\nFinally note that by definition of ui,vi:\nmax i≤s F (wi,ui,vi) ≥ F (w1,u1,v1) ≥ (1− τ) max u,v F (w1,u,v) ≥ (1− τ)F (w1,u∗,v∗) =\n= (1− τ)‖f(u∗,v∗)‖w∗>w1 ≥ 1− τ√\n2d f(u∗,v∗) = 1− τ√ 2d F (w∗,u∗,v∗)"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "It is well-known that neural networks are computationally hard to train. On the<lb>other hand, in practice, modern day neural networks are trained efficiently using SGD<lb>and a variety of tricks that include different activation functions (e.g. ReLU), over-<lb>specification (i.e., train networks which are larger than needed), and regularization. In<lb>this paper we revisit the computational complexity of training neural networks from a<lb>modern perspective. We provide both positive and negative results, some of them yield<lb>new provably efficient and practical algorithms for training certain types of neural net-<lb>works.",
    "creator" : "LaTeX with hyperref package"
  }
}