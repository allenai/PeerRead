{
  "name" : "1705.06709.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Spatiotemporal Features for Infrared Action Recognition with 3D Convolutional Neural Networks",
    "authors" : [ "Zhuolin Jiang", "Viktor Rozgic", "Sancar Adali" ],
    "emails" : [ "sadali}@bbn.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this work, we propose a novel two-stream 3D convolutional neural network (CNN) architecture by introducing the discriminative code layer and the corresponding discriminative code loss function. The proposed network processes IR image and the IR-based optical flow field sequences. We pretrain the 3D CNN model on the visible spectrum Sports-1M action dataset and finetune it on the Infrared Action Recognition (InfAR) dataset. To our best knowledge, this is the first application of the 3D CNN to action recognition in the IR domain. We conduct an elaborate analysis of different fusion schemes (weighted average, single and double-layer neural nets) applied to different 3D CNN outputs. Experimental results demonstrate that our approach can achieve state-of-the-art average precision (AP) performances on the InfAR dataset: (1) the proposed two-stream 3D CNN achieves the best reported 77.5% AP, and (2) our 3D CNN model applied to the optical flow fields achieves the best reported single stream 75.42% AP."
    }, {
      "heading" : "1. Introduction",
      "text" : "Deep convolutional neural networks (CNN) have shown remarkable success for various computer vision tasks in static images, such as object detection [6], recognition [10] and segmentation [16]. Encouraged by this success, researchers have proposed some CNN-based algorithms for action recognition in visible spectrum videos [29, 11, 3, 22, 27, 18, 4, 8]. One promising approach is using a two-stream CNN architecture developed by [22], which consists of a spatial stream network for learning salient appearance features from video frames, and a temporal stream network\nfor learning motion patterns. The prediction is computed by averaging the outputs of two networks. This architecture showed improved performance over traditional action recognition approaches such as improved dense trajectories features [24]. However, as pointed out by [23], 2D convolutions in a temporal network applied on the multiframe stacking of optical flow fields (treating them as different channels) generate 2D representations; and the temporal network loses temporal information important for action recognition after the first convolution layer. To address this, [23] introduced a 3D CNN which takes multiple RGB frames as inputs and performs 3D convolution and pooling, preserving temporal information. The 3D CNN models can process appearance and motion information simultaneously, hence it is able to learn spatiotemporal features for action recognition.\nFor action recognition in infrared videos, there is limited work that uses deep CNNs to combine spatial and temporal cues for spatiotemporal feature learning [5, 30]. [5] applied two-stream CNNs for infrared action recognition. However, the CNN stream that processes the infrared image sequence achieved worse performance than several hand-crafted lowlevel features such as spatio-temporal interest point [12] and dense SIFT [17]. There are two potential reasons for this: 1) the infrared InfAR dataset is not large enough to learn spatiotemporal features leading to severe overfitting; and 2) 2D CNN loses temporal information contained in an input video volume as pointed in [23], so it can not properly model the temporal action patterns.\nIn this paper, we propose a two-stream 3D CNN architecture to learn spatio-temporal features for infrared action recognition. The two-stream 3D CNN contains two separate recognition networks (IR and optical-flow nets) combined using late fusion. In order to reduce the chance of overfitting and learn discriminative spatio-temporal features, we incorporate a discriminative code loss introduced in [8], and combine it with softmax classification loss to form the objective function used for network training. For faster convergence during training, we pretrained 3D CNN model parameters on the large-scale Sports-1M action dataset [9]\n1\nar X\niv :1\n70 5.\n06 70\n9v 1\n[ cs\n.C V\n] 1\n8 M\nay 2\n01 7\nwith videos from the visible light spectrum and finetuned them on the infrared dataset. The results are surprisingly good.\nOur main contributions are the following:\n• We develop a two-stream 3D CNN architecture to learn spatiotemporal features from infrared videos. This two-stream model learns representations that capture spatial and temporal information simultaneously.\n• We add a discriminative code layer (DCL) on top of the last fully-connected layer and combine the discriminative code loss with softmax classification loss to train the 3D CNN. This discriminative code layer generates class-specific representations for infrared videos.\n• We achieve state-of-the-art performances on the InfAR dataset. We find that a single 3D CNN with DCL layer trained using the optical flow field images can achieve an excellent infrared action recognition performance."
    }, {
      "heading" : "1.1. Related work",
      "text" : "Many popular video feature extraction and classification approaches have been developed for action recognition in the visible spectrum, including low-level features (e.g., spatio-temporal interest point (STIP) [12], scale-invariance feature transform (SIFT) [17], optical flow fields [14], improved dense trajectory feature (IDT) [24]), and high-level semantic concepts (e.g., human action attributes [15, 20] and action parts [26]). In recent years, approaches based on convolutional neural networks have been proposed for action recognition [11, 3, 22, 27, 18, 4, 25]. The two-steam CNN architecture [22] achieved impressive recognition performances. [27] explored deeper two-stream network architectures and refined technical details yielding further performance improvements. [4] investigated an effective fusion strategy over space and time for two steam networks. However, all state-of-the-art approaches focus on the action recognition in the visible light spectrum.\nCompared with visible spectrum imaging, the infrared imaging has a nice property that it can work well even under poor light conditions, which is useful for nighttime surveillance [30, 5, 23]. Still, limited works address the action recognition in infrared spectrum. [30] trained a SVM classifier trained on visible light spectrum based bag-ofvisual-words video representation and adapted it to the infrared domain. Recently, [5] applied a two-stream 2D CNN architecture to infrared videos. This architecture employs a motion-history-image (MHI) stream network and an optical-flow stream network to extract image-level features. When the MHI stream is replaced with the raw infrared image stream, the action recognition performance becomes very poor, showing that the two-stream 2D CNN highly relies on the MHI stream network.\nDifferent to [5], we introduce a two-stream 3D CNNs, which can model the video appearance and motion information simultaneously from an infrared video. To our best knowledge, 3D CNN architectures have not been explored for action recognition in infrared videos. In addition, we integrated the discriminative code loss from [8] to the objective function of network training. This makes the learned representations more discriminative and good for classification tasks, specifically action recognition. These two sources of novelties enable an action recognition system that advances state-of-the art performance for action recognition in IR videos, as evidenced in our experiments."
    }, {
      "heading" : "2. Learning Spatiotemporal Features with 3D",
      "text" : "Convolutional Neural Networks\nOur pipeline for action recognition in IR videos is presented in Figure 1. The pipeline inputs are IR video clips obtained by splitting IR frame sequences into nonoverlapping segments of consecutive frames. Motivated by the success of the two-stream CNN architectures operating on visible spectrum videos and derived optical flow fields, we convert each consecutive pair of IR frames into optical flow field. We resize the IR and the optical flow images to the same height and width before feeding the IR and the optical flow clips to their corresponding networks. In the InfAR dataset experiments, we used a frame size of 128×171, creating the input IR and optical flow clips with the same dimension 3 × t × 128 × 171, where 3 is the number of channels of an IR or flow image 1 and t is the temporal clip\n1We compute the optical flow image according to [2]. We stack the x, y components of a flow field, and compute its magnitude as the third channel\nlength in number of frames. We process the corresponding IR and optical flow clips using a novel two-stream 3D CNN architecture. Both streams are processed using the same CNN model which extends the existing 3D-CNN architecture [23] by introduction of the additional discriminative code layer. The discriminative code loss associated with the discriminative code layer is combined with the softmax classification loss to train the 3D CNN.\nWe fuse the probabilistic outputs from the softmax (or the discriminative code) layers of the proposed two-stream network by using the weighted average, the single-layer neural network (NN) fusion or the two-layer NN fusion."
    }, {
      "heading" : "2.1. 3D Convolutional Neural Network with Discriminative Code Layer",
      "text" : "Compared to 2D CNN architecture, 3D CNN architecture is better suited for the action recognition task, because it models spatial and temporal information jointly using 3D convolution and 3D pooling operations. While 2D convolution architectures process multiple input frames as different input channels and transform them into 2D representations; 3D convolution transforms input volume into 3D representation preserving temporal information. To our best knowledge, 3D CNNs have not yet been applied to the task of action recognition in IR videos.\nOur two-stream 3D CNN architecture is based on the 3D CNN model with the discriminative code layer presented in Figure 2. The 3D CNN follows the architecture proposed in [23]. The network has eight 3D convolution layers, combined using five 3D max-pooling layers. The last pooling layer is followed by two fully connected layers (fc6 and fc7). The details on sizes of convolutional kernels, numbers of filters in different convolutional layers, sizes of the maxpooling and fully connected layers are provided in Section 3.1.\nThe proposed 3D CNN architecture has two output layers, a softmax layer that generates m-dimensional one-hot encoding of m activity categories and a discriminative code layer that generates discriminative codes for inputs signals. Assuming the discriminative code layer with N neurons, the training goal is to make it generateN dimensional p-hot encoding 2 of activity categories. The p-hot encoding represents a sample from kth action category (k = 1, ...,m) as a binary vector with coordinates [p(k − 1) + 1 : pk] equal to one and rest of coordinates equal to zero. Intuitively, the group of neurons activates only when a sample from the corresponding category is presented. In order to achieve this,\nof the flow image. The flow values of x, y components are centered around 128 and scaled such that flow values fall between 0 and 255.\n2here we assume that N can be exactly divided by m, i.e., N = pm. If N = pm + k(k < m) we allocate the remaining k neurons to classes with high intra-class appearance variations.\nwe introduce the discriminative code loss associated with the discriminative code layer activations. This loss encourages groups of output neurons to activate simultaneously encoding the category label.\nLet’s assume that the 3D CNN architecture has n+1 layers, n levels including all convolution layers, pooling layers and fully connected layers, and the layer n + 1 including the softmax layer and discriminative code outputs. The output of the ith layer is denoted as x(i), where x(0) represents input. Therefore, the network architecture can be concisely expressed as:\nx(i) = F (W(i)x(i−1)), i = 1, 2, ..., n (1)\nx (n+1) d = Ax (n) (2) x(n+1)c = softmax(Wx (n)) (3)\nwhere W(i) represents the network parameters3 of the ith layer for convolution and fully-connected layers, W(i)x(i−1) is a linear operation (e.g. convolution in a convolution layer, or a linear transformation in fully-connected layer), F (· ) is a non-linear activation function (e.g. ReLU). A contains parameters of a linear transformation implemented by the discriminative code layer and W contains the softmax layer parameters. x(n+1)d is the predicted code while x(n+1)c is the predicted class score vector.\nThe overall loss function used in network training is a linear combination of the softmax classification loss (multinomial logistic loss) Lc, and discriminative code loss Ld with a cost-balancing hyper parameter α.\nL = Lc + αLd (4)\nLc = Lc(x (n+1) c , y) (5)\nLd = Ld(x (n+1) d , y) (6)\nThe Ld cost component can be defined as:\nLd = ‖q(n) −Ax(n)‖22, (7)\nwhere the binary vector q(n) = [q\n(n) 1 , . . . , q (n) j , . . . , q (n) N ] T ∈ {0, 1}N denotes the phot label encoding (or target discriminative code), which indicates the ideal activations of neurons (j denotes the index of neuron. Each neuron is associated with a certain class label and, ideally, only activates to samples from that class. Therefore, when a sample is from the kth action category, q(n)j = 1 if and only if the j\nth neuron is assigned to class k, and neurons associated to other classes should not be activated so that the corresponding entries in q(n) are zero. Note that A is the only parameter3 to be learned in this cost component.\n3For simplicity, we ignore the bias term for convolution layers, fully connected layers, softmax layer and discriminative code layer."
    }, {
      "heading" : "2.2. Network Training",
      "text" : "The network parameters (W(i))i=1,2,...,n),A,W) are trained via back-propagation using the mini-batch stochastic gradient descent method. Compared to the parameter update equations for a multi-layer CNN [13] without the discriminative cost loss, the gradient term, i.e. ∂L\n∂x(n) changes\nand two gradient terms ∂L∂A and ∂L\n∂x (n+1) d\nare introduced,\nsince x(n), A and x(n+1)d are related to the discriminative code loss Ld.\nFrom Equations (5) and (6), we can calculate ∂L ∂x\n(n+1) d\nand ∂L ∂x (n+1) c . Then we can obtain ∂L∂A , ∂L ∂W and ∂L ∂x(n) by applying the chain rule:\n∂L\n∂x (n+1) d\n= α ∂Ld\n∂x (n+1) d\n, ∂L\n∂x (n+1) c\n= ∂Lc\n∂x (n+1) c\n(8)\n∂L ∂A = 2α(Ax(n) − q(n))x(n)T, ∂L ∂W = ∂Lc ∂W (9)\n∂L\n∂x(n) =\n∂L\n∂x (n+1) c\n∂x (n+1) c\n∂x(n) + 2α(Ax(n) − q(n))TA\n(10)\nOnce the partial derivative of L with respect to x(n) is known, the partial derivative of L with respect to W(i) and x(i−1) can be computed using the backward recurrence:\n∂L\n∂W(i) =\n∂L ∂x(i) ∂x(i) ∂W(i) ,\n∂L\n∂x(i−1) =\n∂L ∂x(i) ∂x(i) ∂x(i−1) , ∀i ∈ {1, ..., n} (11)\nwhere ∂x (i)\n∂W(i) and ∂x\n(i)\n∂x(i−1) can be computed from Equation\n(1)."
    }, {
      "heading" : "2.3. Fusion",
      "text" : "As a part of the two-stream pipeline, we perform fusion of the probabilistic outputs from the IR and the optic flow\nnets. While the softmax layer directly provides a probabilistic output, we propose a method to convert the discriminative code layer outputs to a multinomial distribution over action classes. Given the predicted code of a test sample, we find its k nearest neighbors from each class in the training set and calculate the average distances from the sample to its k neighbor training samples from each class. Then, we convert a set of average distances to sample-to-class similarity weights using a Gaussian kernel. Finally, we obtain a probability vector over action categories by `1 normalization of the similarity weights.\nWe fuse the probability outputs of the IR and the optical flow nets using a simple weighted average approach. In addition, we apply two neural network based methods to fuse the predicted codes from IR and optical flow nets: (1) we concatenate predicted codes from from the IR and flow nets and use the obtained vector as an input to a single softmax layer neural network, which outputs probability estimates for action classes (single-layer NN fusion); and (2) we use the concatenated predicted codes as inputs to the two-layer neural network consisting of one 1 × 1 convolution layer and the softmax output layer (two-layer NN fusion)."
    }, {
      "heading" : "3. Experiments",
      "text" : "We evaluate our approach on the recently released InfAR video dataset [5], which is collected using infrared cameras. This dataset contains videos of 12 different action classes 4 with 50 videos in each class. Figure 3 shows video examples from the dataset. First, using this dataset, we evaluate the performance of three widely used low-level descriptor features for the action class prediction task : dense SIFT (D-SIFT) [1], opponent SIFT (O-SIFT) [21], and motion features - improved dense trajectories features (IDT) [24]. Then, we evaluate the prediction performance of semantic feature vector produced by 2, 784 concept detectors, each of\n4The action classes include: ‘0-fight’, ‘1-handclap’, ‘2-handshake’, ‘3-hug’, ‘4-jog’, ‘5-jump’, ‘6-punch’, ‘7-push’, ‘8-skip’, ‘9-walk’, ‘10- wave1’ and ‘11-wave2’.\nwhich outputs a concept score, given the low-level feature vector (e.g. D-SIFT) . Finally, we evaluate the two-stream 3D CNNs with the discriminative code layer using different output fusion strategies."
    }, {
      "heading" : "3.1. Experimental Settings",
      "text" : "To extract low-level image-based features such as dense SIFT and opponent SIFT, we uniformly sample 50 frames per video. We use Fisher vector (FV) encoding [19] to obtain video-level representations from these local lowlevel descriptors. The video-level features are computed by spatio-temporal pooling of the frame-based FV features. In addition to these low-level video representations, we extract features whose dimensions correspond to measures of evidence of high-level concepts in videos. For extracting semantic concept features, we trained 2, 784 concept detectors utilizing the VideoStory Dataset [7]. The detectors are trained to predict high-level concept features from different FV-encoded features (D-SIFT, O-SIFT and IDT). Finally, we trained multiple linear multi-class SVM classifiers to predict actions from different low-level features and concept features.\nThe architectures of IR net and Flow net are identical and follow [23]. Assume C(k, n, s) is a convolutional layer with kernel size k × k × k, n filters and stride s × s × s, P (k1, k2, s1, s2) is a max-pooling layer with kernel temporal size k1, kernel spatial size k2 × k2, temporal stride s1 and spatial stride s2. FC(n) is a fully connected layer with n filters. SM(n) is a softmax layer with n filters. DC(n) is a discriminative code layer with n filters. The main architecture follows: C(3, 64, 1)– P (1, 2, 1, 2)–C(3, 128, 1)–P (2, 2, 2, 2)–C(3, 256, 1)– C(3, 256, 1)–P (2, 2, 2, 2)–C(3, 512, 1)–C(3, 512, 1)– P (2, 2, 2, 2)–C(3, 512, 1)–C(3, 512, 1)–P (2, 2, 2, 2)– FC(4096)–FC(4096)–SM(12)(DC(4096)).\nThe temporal length t of each IR and flow clip is 16 frames. The parameter α in Eq 4 is set to be 0.02 in our experiments. Both IR stream and Flow stream nets are pretrained on the large-scale Sports-1M dataset [9] and finetuned on the InfAR dataset. The learning rate, training\nbatch size, weight decay coefficient and maximum iterations are set as 0.0001, 30, 0.0005 and 10, 000, respectively. To extract video-level CNN features, we split a video into 16 frame long clips without overlapping between two consecutive clips. To get the video-level representations, we simply averaged representations extracted from each clip belonging to the video. To classify actions, we use two ways: (1) employ the softmax output layer to produce the confidence scores for all action classes for each video clip and use their average to predict the video-level class label (softmax); (2) employ the k-NN classification based on the video-level representation, which is computed as the average of predicted codes of all video clips belonging to the video (k-NN).\nWe follow the standard setting in [5], we randomly select 30 samples from each category as training, and the rest for testing. We repeat the experiments five times and report their performance average as the final performance in this paper. For the evaluation metrics, we used average precision (AP) as in [5], which is the average of recognition precisions of all actions."
    }, {
      "heading" : "3.2. Comparisons with Other Approaches",
      "text" : ""
    }, {
      "heading" : "3.2.1 Low-level and high-level semantic features",
      "text" : "We evaluate two static appearance features (D-SIFT and O-SIFT), one motion feature (IDT) and the corresponding high-level semantic concept features, all extracted from infrared videos. We perform early SVM fusion by concatenating concept feature vectors obtained using concept detectors on different low-level features. Finally, we perform late fusion by averaging the posterior scores of SVMs trained on different features.\nTable 1 summarizes the recognition performances with these approaches. The high-level concept features achieved similar or better performance compared with the corresponding low-level features. In addition, the early fusion of all concept features provided similar results as the late fusion approach that combined the prediction scores from six SVM classifiers."
    }, {
      "heading" : "3.2.2 Discriminative features from single 3D CNN",
      "text" : "Our two stream 3D-CNN is based on the C3D architecture in [23]. It consists of an IR net taking 16-frame clips as inputs and a flow net taking 16-frame sequences of the optical flow fields. We trained a 3D CNN in two ways: (1) using softmax classification loss only; (2) using both softmax classification loss and the discriminative code loss (DCL).\nWe first trained IR and Flow nets using softmax classification only. We called these two networks as ‘IR net without DCL’ and ‘Flow net without DCL’, respectively. Then we maintain the softmax layer in both networks but add the discriminative code layer to the fc7 layer to train two stream networks, which are referred to as ‘IR net’ and ‘Flow net’. For IR and flow nets, we can employ softmax or k-NN classification methods. The performances of different training and classification methods are presented in Table 2.\nAs shown in Table 1 and Table 2, the ‘IR net without DCL’ can still obtain marginally better results than the late fusion of all low-level features and their concept features. The ‘Flow net without DCL’ can obtain around 20% improvement on average precision compared to its partner stream, which demonstrate that the motion information is very important for action recognition. The ‘k-NN’ classification method achieved better performance than the ‘softmax’ method, due to the increase of inter-class distances in\nthe generated discriminative code space. We compare our results with two-stream CNN results reported in [5]. ‘Two-stream CNN-1’ uses a two-stream 2D convolutional architecture [22] consisting of an IR stream and a flow stream as ours. ‘Two-stream CNN-2’ is similar to ‘Two-stream CNN-1’, but the IR stream is replaced with a stream taking optical flow motion history images as inputs. As pointed out in [23], 3D CNN can model appearance and motion simultaneously, hence our IR net outperforms the two-stream-CNN-1, which is trained with IR images and optical flow fields. The two-stream-CNN-2 achieves good results due to the use of motion history images. However, the result of our flow net, which is based on optical flow field only, is still comparable to two-stream-CNN-2."
    }, {
      "heading" : "3.2.3 Fusion of two 3D CNNs",
      "text" : "We investigate different fusion strategies for the outputs from IR and flow nets:\nLate fusion1. We fuse the softmax probability outputs of IR and Flow nets using simple weighted average rule, where the weight is 2 for flow net and 1 for IR net.\nLate fusion2. Given a predicted code vector from the discriminative code layer, we compute the average distances to each class i based on its k neighbor training samples from each class, and convert them to a similarity vector using a Gaussian kernel function : exp(−γd2i ), where di is the average distance to class i and γ is a normalization factor. Finally, we compute the probability vector by `1 normalization on the similarity vector. k is set to 5 in our experiment. The parameter γ is set to be 0.05. We combine the probability vectors from different streams using a simple weighted average rule.\nSingle-layer NN fusion. We first concatenate discriminative codes from IR and Flow nets. Then, we construct a single softmax layer ‘shallow’ neural network with concatenated discriminative codes as inputs and action classes as outputs.\nTwo-layer NN fusion. We train a two-layer neural network consisting of one 1 × 1 convolution layer and a softmax output layer, with concatenated discriminative codes from IR and Flow nets as inputs. The convolution layer can play a role in selecting good features from the input concatenated features. We used 50 filters for the convolutional layer.\nTable 3 presents the results of these fusion strategies. Simple weighted rule for ‘Late fusion1’ and ‘Late fusion2’ can lead to performance improvement over single stream CNN. The more complex single-layer NN and two-layer NN fusion methods do not outperform the simple weighted average fusion rule, likely due to a limited size of the InfAR dataset, since there is insufficient training samples to learn the parameters of convolution and softmax layers in these\ntwo fusion networks. If we use a simple average rule to combine the confidence scores for all actions for each video from the method ‘Late fusion2’ with the scores generated from the method ‘Early fusion of all concepts’ in section 3.2.1, we can achieve 79.2% AP. This means that high-level concept features can provide complementary information to 3D CNN features for action recognition."
    }, {
      "heading" : "3.3. Discussion",
      "text" : "Figure 4 visualizes the learned discriminative codes of testing videos using both IR and Flow nets. The discriminative predicted code matrix of all testing videos should be block-diagonal. Y axis indicates the dimension of predicted code, and each position in X axis correspond to one test\nvideo. As can be seen from the figure, the videos from the same class have similar representations, while videos from different classes have different representations.\nFigure 5 shows confusion matrices obtained using IR net, flow net and late fusion of two nets. The misclassifications are mainly related to ‘push’ and ‘punch’ categories\nwhich are visually similar. ‘walk’ is misclassified as ’fight’, which is possibly caused by presence of moving people in the background. The fusion of two nets helps correcting some typical misclassifications, such as the confusion between ‘handclap’ and ‘wave1’ classes.\nFor 5 action categories, we achieved precisions higher than 90% when using 30 positive video samples per category. Figure 7 shows some samples from three of these classes.\nIn Figure 6(a), we plot the performance curves for a range of parameter α in flow net. We observe that our approach is not sensitive to the selection of α. In addition, the simple k-NN classification scheme consistently outperforms the ‘softmax’ classification on the full α range, this is because the generated predicted codes using our approach are discriminative. In Figure 6(b), we show the performances using different k (recall k is the number of nearest neighbors for a k-NN classifier) for the IR and flow nets. Our approach is not sensitive to k due to the increase of inter-class distances in the discriminative code space."
    }, {
      "heading" : "4. Conclusion",
      "text" : "We introduce a two-stream 3D convolutional network for action recognition in infrared videos. Each recognition\nstream (IR and flow nets), was trained with softmax classification loss and discriminative code loss making the extracted representations of infrared videos become more discriminative. Both nets are initialized by pretraining on visible spectrum videos, and finetuned on the infrared videos. Our experiments show that using even a single flow stream CNN can achieve state-of-the-art performance on the InfAR dataset. The goals of our future work are to extend the current approach to the cross-spectral feature learning and explore the domain adaptation techniques that can more effectively exploit the high resource spectrum for the action recognition in the low-resource spectrum."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center contract number D11PC20071. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government."
    } ],
    "references" : [ {
      "title" : "Learning mid-level features for recognition",
      "author" : [ "Y.-L. Boureau", "F. Bach", "Y. LeCun", "J. Ponce" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "High accuracy optical flow estimation based on a theory for warping",
      "author" : [ "T. Brox", "A. Bruhn", "N. Papenberg", "J. Weickert" ],
      "venue" : "In European conference on computer vision,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2004
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Convolutional two-stream network fusion for video action recognition",
      "author" : [ "C. Feichtenhofer", "A. Pinz", "A. Zisserman" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Infar dataset: Infrared action recognition at different",
      "author" : [ "C. Gao", "Y. Du", "J. Liu", "J. Lv", "L. Yang", "D. Meng", "A. Hauptmann" ],
      "venue" : "times. Neurocomputing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R.B. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Videostory: A new multimedia embedding for few-example recognition and translation of events",
      "author" : [ "A. Habibian", "T. Mensink", "C. Snoek" ],
      "venue" : "In ACM MM,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Learning discriminative features via label consistent neural network",
      "author" : [ "Z. Jiang", "Y. Wang", "L. Davis", "W. Andrews", "V. Rozgic" ],
      "venue" : "In WACV, 2017",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2017
    }, {
      "title" : "Large-scale video classification with convolutional neural networks",
      "author" : [ "A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Beyond gaussian pyramid: Multi-skip feature stacking for action recognition",
      "author" : [ "Z. Lan", "M. Lin", "X.L.A.G. Hauptmann", "B. Raj" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "On space-time interest points",
      "author" : [ "I. Laptev" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    }, {
      "title" : "Mller. Efficient backpro",
      "author" : [ "Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R" ],
      "venue" : "Neural Networks: Tricks of the Trade,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Recognizing actions by shape-motion prototype trees",
      "author" : [ "Z. Lin", "Z. Jiang", "L.S. Davis" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Recognizing human actions by attributes",
      "author" : [ "J. Liu", "B. Kuipers", "S. Savarese" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "J. Long", "E. Shelhamer", "T. Darrel" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Distinctive image features from scale-invariant keypoints",
      "author" : [ "D.G. Lowe" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "Beyond short snippets: Deep networks for video classification",
      "author" : [ "J.Y. Ng", "M.J. Hausknecht", "S. Vijayanarasimhan", "O. Vinyals", "R. Monga", "G. Toderici" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Improving the fisher kernel for large-scale image classifiction",
      "author" : [ "F. Perronnin", "J. Sanchez", "T. Mensink" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2010
    }, {
      "title" : "Sparse dictionary-based representation and recognition of action attributes",
      "author" : [ "Q. Qiu", "Z. Jiang", "R. Chellappa" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Evaluating color descriptors for object and scene recognition",
      "author" : [ "K. Sande", "T. Gevers", "C. Snoek" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "Two-stream convolutional networks for action recognition in videos",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Learning spatiotemporal features with 3d convolutional networks",
      "author" : [ "D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Action recognition with improved trajectories",
      "author" : [ "H. Wang", "C. Schmid" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Temporal segment networks: Towards good practices for deep action recognition",
      "author" : [ "L. Wang", "Y. Xiong", "Z. Wang", "Y. Qiao", "D. Lin", "X. Tang", "L.V. Gool" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    }, {
      "title" : "Human action recognition by learning bases of action attributes and parts",
      "author" : [ "B. Yao", "X. Jiang", "A. Khosla", "A.L. Lin", "L. Guibas", "L. Fei- Fei" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "Evaluating two-stream CNN for video classification",
      "author" : [ "H. Ye", "Z. Wu", "R. Zhao", "X. Wang", "Y. Jiang", "X. Xue" ],
      "venue" : "In ICMR,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Understanding neural networks through deep visualization",
      "author" : [ "J. Yosinski", "J. Clune", "A. Nguyen", "T. Fuchs", "H. Lipson" ],
      "venue" : "In ICML Workshop,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Exploiting image-trained CNN architectures for unconstrained video classification",
      "author" : [ "S. Zha", "F. Luisier", "W. Andrews", "N. Srivastava", "R. Salakhutdinov" ],
      "venue" : "In BMVC,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "A study on visible to infrared action recognition",
      "author" : [ "Y. Zhu", "G. Guo" ],
      "venue" : "IEEE Signal Processing Letters,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Deep convolutional neural networks (CNN) have shown remarkable success for various computer vision tasks in static images, such as object detection [6], recognition [10] and segmentation [16].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "Deep convolutional neural networks (CNN) have shown remarkable success for various computer vision tasks in static images, such as object detection [6], recognition [10] and segmentation [16].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 15,
      "context" : "Deep convolutional neural networks (CNN) have shown remarkable success for various computer vision tasks in static images, such as object detection [6], recognition [10] and segmentation [16].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 28,
      "context" : "Encouraged by this success, researchers have proposed some CNN-based algorithms for action recognition in visible spectrum videos [29, 11, 3, 22, 27, 18, 4, 8].",
      "startOffset" : 130,
      "endOffset" : 159
    }, {
      "referenceID" : 10,
      "context" : "Encouraged by this success, researchers have proposed some CNN-based algorithms for action recognition in visible spectrum videos [29, 11, 3, 22, 27, 18, 4, 8].",
      "startOffset" : 130,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "Encouraged by this success, researchers have proposed some CNN-based algorithms for action recognition in visible spectrum videos [29, 11, 3, 22, 27, 18, 4, 8].",
      "startOffset" : 130,
      "endOffset" : 159
    }, {
      "referenceID" : 21,
      "context" : "Encouraged by this success, researchers have proposed some CNN-based algorithms for action recognition in visible spectrum videos [29, 11, 3, 22, 27, 18, 4, 8].",
      "startOffset" : 130,
      "endOffset" : 159
    }, {
      "referenceID" : 26,
      "context" : "Encouraged by this success, researchers have proposed some CNN-based algorithms for action recognition in visible spectrum videos [29, 11, 3, 22, 27, 18, 4, 8].",
      "startOffset" : 130,
      "endOffset" : 159
    }, {
      "referenceID" : 17,
      "context" : "Encouraged by this success, researchers have proposed some CNN-based algorithms for action recognition in visible spectrum videos [29, 11, 3, 22, 27, 18, 4, 8].",
      "startOffset" : 130,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "Encouraged by this success, researchers have proposed some CNN-based algorithms for action recognition in visible spectrum videos [29, 11, 3, 22, 27, 18, 4, 8].",
      "startOffset" : 130,
      "endOffset" : 159
    }, {
      "referenceID" : 7,
      "context" : "Encouraged by this success, researchers have proposed some CNN-based algorithms for action recognition in visible spectrum videos [29, 11, 3, 22, 27, 18, 4, 8].",
      "startOffset" : 130,
      "endOffset" : 159
    }, {
      "referenceID" : 21,
      "context" : "One promising approach is using a two-stream CNN architecture developed by [22], which consists of a spatial stream network for learning salient appearance features from video frames, and a temporal stream network for learning motion patterns.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 23,
      "context" : "This architecture showed improved performance over traditional action recognition approaches such as improved dense trajectories features [24].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 22,
      "context" : "However, as pointed out by [23], 2D convolutions in a temporal network applied on the multiframe stacking of optical flow fields (treating them as different channels) generate 2D representations; and the temporal network loses temporal information important for action recognition after the first convolution layer.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 22,
      "context" : "To address this, [23] introduced a 3D CNN which takes multiple RGB frames as inputs and performs 3D convolution and pooling, preserving temporal information.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "For action recognition in infrared videos, there is limited work that uses deep CNNs to combine spatial and temporal cues for spatiotemporal feature learning [5, 30].",
      "startOffset" : 158,
      "endOffset" : 165
    }, {
      "referenceID" : 29,
      "context" : "For action recognition in infrared videos, there is limited work that uses deep CNNs to combine spatial and temporal cues for spatiotemporal feature learning [5, 30].",
      "startOffset" : 158,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "[5] applied two-stream CNNs for infrared action recognition.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "However, the CNN stream that processes the infrared image sequence achieved worse performance than several hand-crafted lowlevel features such as spatio-temporal interest point [12] and dense SIFT [17].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 16,
      "context" : "However, the CNN stream that processes the infrared image sequence achieved worse performance than several hand-crafted lowlevel features such as spatio-temporal interest point [12] and dense SIFT [17].",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 22,
      "context" : "There are two potential reasons for this: 1) the infrared InfAR dataset is not large enough to learn spatiotemporal features leading to severe overfitting; and 2) 2D CNN loses temporal information contained in an input video volume as pointed in [23], so it can not properly model the temporal action patterns.",
      "startOffset" : 246,
      "endOffset" : 250
    }, {
      "referenceID" : 7,
      "context" : "In order to reduce the chance of overfitting and learn discriminative spatio-temporal features, we incorporate a discriminative code loss introduced in [8], and combine it with softmax classification loss to form the objective function used for network training.",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 8,
      "context" : "For faster convergence during training, we pretrained 3D CNN model parameters on the large-scale Sports-1M action dataset [9]",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : ", spatio-temporal interest point (STIP) [12], scale-invariance feature transform (SIFT) [17], optical flow fields [14], improved dense trajectory feature (IDT) [24]), and high-level semantic concepts (e.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : ", spatio-temporal interest point (STIP) [12], scale-invariance feature transform (SIFT) [17], optical flow fields [14], improved dense trajectory feature (IDT) [24]), and high-level semantic concepts (e.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : ", spatio-temporal interest point (STIP) [12], scale-invariance feature transform (SIFT) [17], optical flow fields [14], improved dense trajectory feature (IDT) [24]), and high-level semantic concepts (e.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 23,
      "context" : ", spatio-temporal interest point (STIP) [12], scale-invariance feature transform (SIFT) [17], optical flow fields [14], improved dense trajectory feature (IDT) [24]), and high-level semantic concepts (e.",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 14,
      "context" : ", human action attributes [15, 20] and action parts [26]).",
      "startOffset" : 26,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : ", human action attributes [15, 20] and action parts [26]).",
      "startOffset" : 26,
      "endOffset" : 34
    }, {
      "referenceID" : 25,
      "context" : ", human action attributes [15, 20] and action parts [26]).",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : "In recent years, approaches based on convolutional neural networks have been proposed for action recognition [11, 3, 22, 27, 18, 4, 25].",
      "startOffset" : 109,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : "In recent years, approaches based on convolutional neural networks have been proposed for action recognition [11, 3, 22, 27, 18, 4, 25].",
      "startOffset" : 109,
      "endOffset" : 135
    }, {
      "referenceID" : 21,
      "context" : "In recent years, approaches based on convolutional neural networks have been proposed for action recognition [11, 3, 22, 27, 18, 4, 25].",
      "startOffset" : 109,
      "endOffset" : 135
    }, {
      "referenceID" : 26,
      "context" : "In recent years, approaches based on convolutional neural networks have been proposed for action recognition [11, 3, 22, 27, 18, 4, 25].",
      "startOffset" : 109,
      "endOffset" : 135
    }, {
      "referenceID" : 17,
      "context" : "In recent years, approaches based on convolutional neural networks have been proposed for action recognition [11, 3, 22, 27, 18, 4, 25].",
      "startOffset" : 109,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "In recent years, approaches based on convolutional neural networks have been proposed for action recognition [11, 3, 22, 27, 18, 4, 25].",
      "startOffset" : 109,
      "endOffset" : 135
    }, {
      "referenceID" : 24,
      "context" : "In recent years, approaches based on convolutional neural networks have been proposed for action recognition [11, 3, 22, 27, 18, 4, 25].",
      "startOffset" : 109,
      "endOffset" : 135
    }, {
      "referenceID" : 21,
      "context" : "The two-steam CNN architecture [22] achieved impressive recognition performances.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 26,
      "context" : "[27] explored deeper two-stream network architectures and refined technical details yielding further performance improvements.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "[4] investigated an effective fusion strategy over space and time for two steam networks.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 29,
      "context" : "Compared with visible spectrum imaging, the infrared imaging has a nice property that it can work well even under poor light conditions, which is useful for nighttime surveillance [30, 5, 23].",
      "startOffset" : 180,
      "endOffset" : 191
    }, {
      "referenceID" : 4,
      "context" : "Compared with visible spectrum imaging, the infrared imaging has a nice property that it can work well even under poor light conditions, which is useful for nighttime surveillance [30, 5, 23].",
      "startOffset" : 180,
      "endOffset" : 191
    }, {
      "referenceID" : 22,
      "context" : "Compared with visible spectrum imaging, the infrared imaging has a nice property that it can work well even under poor light conditions, which is useful for nighttime surveillance [30, 5, 23].",
      "startOffset" : 180,
      "endOffset" : 191
    }, {
      "referenceID" : 29,
      "context" : "[30] trained a SVM classifier trained on visible light spectrum based bag-ofvisual-words video representation and adapted it to the infrared domain.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "Recently, [5] applied a two-stream 2D CNN architecture to infrared videos.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 4,
      "context" : "Different to [5], we introduce a two-stream 3D CNNs, which can model the video appearance and motion information simultaneously from an infrared video.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 7,
      "context" : "In addition, we integrated the discriminative code loss from [8] to the objective function of network training.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "1We compute the optical flow image according to [2].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 22,
      "context" : "Both streams are processed using the same CNN model which extends the existing 3D-CNN architecture [23] by introduction of the additional discriminative code layer.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 22,
      "context" : "The 3D CNN follows the architecture proposed in [23].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "Compared to the parameter update equations for a multi-layer CNN [13] without the discriminative cost loss, the gradient term, i.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "We evaluate our approach on the recently released InfAR video dataset [5], which is collected using infrared cameras.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "First, using this dataset, we evaluate the performance of three widely used low-level descriptor features for the action class prediction task : dense SIFT (D-SIFT) [1], opponent SIFT (O-SIFT) [21], and motion features - improved dense trajectories features (IDT) [24].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 20,
      "context" : "First, using this dataset, we evaluate the performance of three widely used low-level descriptor features for the action class prediction task : dense SIFT (D-SIFT) [1], opponent SIFT (O-SIFT) [21], and motion features - improved dense trajectories features (IDT) [24].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 23,
      "context" : "First, using this dataset, we evaluate the performance of three widely used low-level descriptor features for the action class prediction task : dense SIFT (D-SIFT) [1], opponent SIFT (O-SIFT) [21], and motion features - improved dense trajectories features (IDT) [24].",
      "startOffset" : 264,
      "endOffset" : 268
    }, {
      "referenceID" : 4,
      "context" : "Video samples for 12 action classes from the InfAR action dataset [5].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "We use Fisher vector (FV) encoding [19] to obtain video-level representations from these local lowlevel descriptors.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "For extracting semantic concept features, we trained 2, 784 concept detectors utilizing the VideoStory Dataset [7].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 22,
      "context" : "The architectures of IR net and Flow net are identical and follow [23].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : "Both IR stream and Flow stream nets are pretrained on the large-scale Sports-1M dataset [9] and finetuned on the InfAR dataset.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "We follow the standard setting in [5], we randomly select 30 samples from each category as training, and the rest for testing.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "For the evaluation metrics, we used average precision (AP) as in [5], which is the average of recognition precisions of all actions.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "D-SIFT [1] 46.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 20,
      "context" : "7 O-SIFT [21] 47.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 23,
      "context" : "1 IDT [24] 43.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 4,
      "context" : "Two-stream-CNN-1 [5] 32.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "08 Two-stream-CNN-2 [5] 76.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "The results of ‘Two-stream CNN-1’ and ‘Two-stream CNN2’ are copied from the original paper [5].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 22,
      "context" : "Our two stream 3D-CNN is based on the C3D architecture in [23].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : "We compare our results with two-stream CNN results reported in [5].",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 21,
      "context" : "‘Two-stream CNN-1’ uses a two-stream 2D convolutional architecture [22] consisting of an IR stream and a flow stream as ours.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : "As pointed out in [23], 3D CNN can model appearance and motion simultaneously, hence our IR net outperforms the two-stream-CNN-1, which is trained with IR images and optical flow fields.",
      "startOffset" : 18,
      "endOffset" : 22
    } ],
    "year" : 2017,
    "abstractText" : "Infrared (IR) imaging has the potential to enable more robust action recognition systems compared to visible spectrum cameras due to lower sensitivity to lighting conditions and appearance variability. While the action recognition task on videos collected from visible spectrum imaging has received much attention, action recognition in IR videos is significantly less explored. Our objective is to exploit imaging data in this modality for the action recognition task. In this work, we propose a novel two-stream 3D convolutional neural network (CNN) architecture by introducing the discriminative code layer and the corresponding discriminative code loss function. The proposed network processes IR image and the IR-based optical flow field sequences. We pretrain the 3D CNN model on the visible spectrum Sports-1M action dataset and finetune it on the Infrared Action Recognition (InfAR) dataset. To our best knowledge, this is the first application of the 3D CNN to action recognition in the IR domain. We conduct an elaborate analysis of different fusion schemes (weighted average, single and double-layer neural nets) applied to different 3D CNN outputs. Experimental results demonstrate that our approach can achieve state-of-the-art average precision (AP) performances on the InfAR dataset: (1) the proposed two-stream 3D CNN achieves the best reported 77.5% AP, and (2) our 3D CNN model applied to the optical flow fields achieves the best reported single stream 75.42% AP.",
    "creator" : "LaTeX with hyperref package"
  }
}