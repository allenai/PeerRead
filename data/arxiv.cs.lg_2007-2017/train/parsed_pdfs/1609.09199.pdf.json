{
  "name" : "1609.09199.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 9.\n09 19\n9v 1\n[ cs\n.L G\n] 2\n9 Se\np 20\nIndex Terms— supervised dictionary learning, sparse coding, graph Laplacian, classification"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Dictionary learning aims to learn a set of atoms such that a given signal can be well approximated by a sparse linear combination of these atoms. The standard dictionary learning problem is formulated as\nargmin D,X\n‖Y −DX‖2F s.t. ‖xi‖0 ≤ T ∀i (1)\nwhere Y ∈ Rn×N is the data matrix, X ∈ RK×N contains the sparse representations and D ∈ Rn×K is an overcomplete dictionary with normalized columns (atoms).\nDespite the popularity of standard dictionary learning methods in many domains, their performance in classification tasks is sub-optimal, since an accurate reconstruction is not as important for classification as the discrimination capability of the dictionary. This motivates the emergence of supervised\nThe research leading to these results has received funding from the European Research Council under European Union’s Seventh Framework Program, ERC Grant agreement no. 320649, and from the Israel Science Foundation (ISF) grant number 1770/14.\ndictionary learning techniques, exploiting both the existing label information and the underlying structure of the data.\nSome of these methods attempt to learn a separate subdictionary for each class (e.g. [1, 2, 3, 4]). Consequently, the sparse codes over the learned dictionary are used as features on which a classifier is trained. More sophisticated approaches (e.g. [5, 6, 7, 8]) learn a discriminative dictionary by introducing a classification-error term into the objective function, and enforcing some discriminative criteria on the optimized sparse coefficients. By doing so, these methods form a unified learning problem and learn the dictionary and classifier jointly.\nOf the latter category, we focus on the Label Consistent K-SVD (LC-KSVD) method [8] for joint learning of an overcomplete dictionary D and an optimal linear classifier W :\narg min D,W,A,X\n‖Y −DX‖2F + α‖Q−AX‖2F + β‖H −WX‖2F\ns.t. ‖xi‖0 ≤ T ∀i, (2) where H ∈ Rq×N is a binary matrix containing the labels of the training data (out of q possible classes), and Q ∈ RK×N associates label information with each dictionary atom, thus forcing signals from the same class to have similar sparse representations. The minimized objective hence balances between the reconstruction error ‖Y −DX‖2F , the label consistency ‖Q−AX‖2F and the classification error ‖H−WX‖2F . These terms can be fused together, leading to a standard formulation:\nargmin D̃,X\n∥ ∥ ∥Ỹ − D̃X ∥ ∥ ∥ 2\nF s.t. ‖xi‖0 ≤ T ∀i, (3)\nwhere Ỹ =\n\n Y√ αQ√ βH\n\n and D̃ =\n\n D√ αA√ βW\n\n.\nEquation (3) can be efficiently solved using the K-SVD algorithm [9], which iteratively alternates between a sparse coding step (optimization over X) and a dictionary update step (that updates each dictionary atom in D̃ along with its related coefficients from X). Having completed the training process, the individual components D and W can be recovered from D̃. Consequently, classification of a new signal\nis simply performed by sparse coding over the dictionary D and applying the learned classifier W on the resulting sparse coefficient vector, choosing the class that yields the highest score.\nIn [10], we have suggested an unsupervised dictionary learning algorithm for graph signals. The algorithm takes into account the underlying structure of the data in both the feature and the manifold domains using graph smoothness constraints. Furthermore, the underlying structure, encapsulated by a graph Laplacian matrix, can be learned within the dictionary learning process to promote the desired smoothness.\nIn this paper, we propose an extension of our dual graph regularized dictionary learning algorithm to a supervised setting by applying the same ideas to the LC-KSVD approach [8]. The novelty of the LC-KSVD algorithm lies in the requirement that objects from the same class have similar sparse codes over some dictionary. While this method was shown to yield satisfactory classification results, we argue that optimizing a separate sub-dictionary for each class or directly relating the dictionary atoms with specific classes is overly restrictive and highly sensitive to the initialization of the algorithm. We therefore replace the label consistency constraint with a graph-based smoothness regularization that leverages the label information and promotes the discriminative nature of the sparse codes. Additionally, we propose to simultaneously take into account the underlying structure of the training data in the feature domain such that the feature dependencies are preserved in the learned dictionary atoms.\nIn the sequel, we describe the proposed algorithm and demonstrate its efficiency in simulations."
    }, {
      "heading" : "2. GRAPH-CONSTRAINED SUPERVISED DICTIONARY LEARNING",
      "text" : ""
    }, {
      "heading" : "2.1. Introducing the Data Manifold Structure",
      "text" : "Our proposed algorithm is based on the LC-KSVD approach [8]. In order to take into account the local geometrical structure of the data manifold, we shall model the relationships between different data samples using a graph and require smoothness of the sparse codes over the graph topology.\nGiven a set of training samples {y1, ..., ym} ∈ Rn, let us construct a weighted graph M with m vertices, where each node represents a training data point. The weight wij assigned to the edge connecting the i-th and j-th nodes is designed to be inversely proportional to the distance between them. A common choice uses a Gaussian kernel function\nwij = exp\n(\n−‖yi − yj‖ 2 2\nε\n)\n. (4)\nThe graph adjacency matrixWM consists of the edge weights wij . The graph Laplacian LM is then defined as LM =\nDM −WM, where the degree matrix DM is a diagonal matrix whose entries are DMii = ∑\nj wij . Following ideas from manifold learning and spectral graph theory, the graph Laplacian LM can be used as a smoothness operator to preserve the local manifold structure. Similarly to the methods proposed in [11, 12], we incorporate LM into the objective function as a regularizer of the form Tr(XLMX\nT ). Denoting the i-th column of X by xi, we observe that\nTr(XLMX T ) =\n1\n2\n∑\ni,j\nwij‖xi − xj‖22. (5)\nThis term therefore encourages similar signals, having a large proximity measure wij , to have similar sparse codes, thus satisfying the commonly known manifold assumption [13]. In other words, it promotes smoothness of the obtained sparse representations X along the geodesics of the underlying data manifold, described by the graph Laplacian LM.\nReplacing the original label consistency term with the graph regularization, the new dictionary learning problem formulation reads\narg min D,W,X\n‖Y −DX‖2F + β‖H −WX‖2F + γT r(XLMXT )\ns.t. ‖xi‖0 ≤ T ∀i. (6) Fusing the first two components together similarly to the form presented in Equation (3), we obtain the graph-regularized supervised dictionary learning problem\nargmin D̃,X\n∥ ∥ ∥Ỹ − D̃X ∥ ∥ ∥ 2\nF + γT r(XLMX\nT )\ns.t. ‖xi‖0 ≤ T ∀i (7)\nwhere Ỹ =\n(\nY√ βH\n)\nand D̃ =\n(\nD√ βW\n)\n."
    }, {
      "heading" : "2.2. Introducing Feature Interdependencies",
      "text" : "In order to take into account the interdependencies in the feature domain as well, we shall construct a second graph to model the relationships between different features, and require smoothness of the dictionary atoms over this new graph. That is, if two features behave similarly across the training signals, this behavior should be reflected in the structure of the learned atoms. Explicitly, given the set of training samples {y1, ..., ym} ∈ Rn, let us construct a weighted graph G with n vertices, where each node represents a feature (corresponding to a row in the data matrix). The weight assigned to the edge connecting the i-th and j-th nodes is again designed to be inversely proportional to the distance between them. The graph adjacency matrix WG consists of the edge weights, DG is the corresponding degree matrix, and the graph Laplacian is defined as LG = DG −WG .\nIn a symmetric view to the manifold graph regularization, the feature graph should be integrated through a regularization term of the form Tr(DTLGD) applied to the dictionary matrix D. Since in the current formulation (7) the dictionary D only occupies a subset of the matrix D̃, we shall zero-pad LG to the dimension of D̃:\nL̃G =\n[\nLG 0 0 0\n]\n(8)\nAs the extended matrix L̃G satisfies D̃T L̃GD̃ = DTLGD, the regularization may equivalently be imposed on D̃.\nThe result is the dual graph constrained dictionary learning problem:\nargmin D̃,X\n∥ ∥ ∥Ỹ − D̃X ∥ ∥ ∥ 2\nF + γT r(XLMX\nT ) + ρT r(D̃T L̃GD̃)\ns.t. ‖xi‖0 ≤ T ∀i (9) Solving this problem requires significant modifications of the K-SVD algorithm. This can be done using the graph2DL algorithm we proposed in [10] that reflects the added restrictions. Upon completion of the learning process, D and W can be recovered from D̃, and classification is again performed by sparse coding the test signals using D and applying the classifier W on the resulting coefficients."
    }, {
      "heading" : "2.3. Learning Dependencies",
      "text" : "The feature dependencies were so far inferred from the patterns detected in the training set. For this purpose, an initial weight matrix WG for the features graph can be constructed by computing the pairwise distances between rows in the data matrix Y , which correspond to the different features:\nwij = exp\n(\n−‖Y (i, :)− Y (j, :)‖ 2 2\nε\n)\n(10)\nTo better handle partial correlations, these dependencies can be learned and adapted along with the dictionary learning process, as previously suggested in [10]. Having obtained an intermediate dictionary D, the graph can be re-estimated based on the dictionary instead of the original input data Y , according to the following optimization problem:\narg min LG∈ΩnL\nρT r(DTLGD) + µ‖LG‖2F , (11)\nwhere\nΩNL = {L ∈ RN×N | L = LT , Lij ≤ 0 (i 6= j), L1 = 0, T r(L) = N}\n(12)\ndefines the set of trace-normalized valid graph Laplacian matrices of size N×N . The term ‖LG‖2F was added to the objective function to control the sparsity of the resulting Laplacian\nmatrix. In a similar manner, the manifold graph Laplacian LM may also be adapted by solving\narg min LM∈ΩmL\nβTr(XLMX T ) + η‖LM‖2F . (13)\nThe complete algorithm, as summarized in Algorithm 1, is assembled by alternating between the graph constrained supervised dictionary learning problem posed in Equation (9), and adapting the Laplacian matrices LG and LM.\nAlgorithm 1 Graph Constrained Supervised Dictionary Learning\n- Initialize D̃(0) - Iterate for k = 1, 2, ... • Run a few iterations of graph2DL (see [10], Al-\ngorithm 3), initialized with D̃(k−1), to solve Equation (9) and obtain D̃(k), X(k).\n• Update the graph Laplacians (Equations (11),(13)). - Return D̃(k), X(k)"
    }, {
      "heading" : "3. EXPERIMENTAL RESULTS",
      "text" : ""
    }, {
      "heading" : "3.1. Single-Label Classification",
      "text" : "First, we evaluate the performance of the proposed algorithm for single-label image classification on the AR Face database [14] and on the Extended YaleB database [15].\nThe AR Face database consists of over 4000 images: 26 images per person for 126 different people. The images feature frontal view faces with different facial expressions, illumination conditions, and occlusions (sun-glasses and scarf). Following the standard evaluation procedure, we use a subset of the database consisting of 2600 images from 50 male subjects and 50 female subjects. Randomly selected 20 images per person constitute the training set, and the rest are used for testing. Each image is represented by a 540-dimensional feature vector using the procedure described in [8].\nThe Extended YaleB database contains 2414 frontal-face images of 38 individuals, captured under various illumination conditions. We randomly selected half of the images (about 32 images per person) as the training set and the rest are used for testing. Each image is represented by a 504-dimensional feature vector.\nWe compare the proposed algorithm to the two variants proposed in [8]: LC-KSVD1, which refers to (2) for β = 0, and LC-KSVD2, which refers to (2) for α, β 6= 0. Two variants of our method are evaluated: SupGraphDL, standing for Algorithm 1 without learning the graph Laplacians LG and LM, and SupGraphDL-L, in which the two Laplacians are adapted throughout the process. The obtained results are summarized in Table 1. Note that we used a random partition of the data into training and testing sets, therefore the results obtained for LC-KSVD are different from the best case\nresults presented in [8]. Nevertheless, our algorithm is able to achieve a higher classification accuracy for both evaluated datasets."
    }, {
      "heading" : "3.2. Multi-Label Classification",
      "text" : "Next, we evaluate the proposed algorithm for the more challenging task of multi-label classification. In this setting, each instance may be associated with multiple classes simultaneously. Thus, exploiting the interdependency between labels can significantly affect the success of the classification algorithm.\nAs an initial step, we extended our method to support multi-label classification, by altering the binary label matrix H to allow multiple non-zeros per column. The classification procedure was also extended to support multiple labels: instead of choosing the class yielding the maximal score, the relevant labels were selected as those reaching a result above a threshold, i.e. Ωi = {ℓ : [Wxi](ℓ) ≥ 0.5}, where xi is the sparse representation of a test sample yi over the dictionary D, and W is the learned classifier.\nThe algorithm was evaluated for two datasets: natural scene images and yeast gene functionality.\nThe natural scene dataset consists of 2000 natural scene images, each belonging to one or more out of 5 possible semantic classes: desert, mountains, sea, sunset and trees. Half of the images were used for training and the rest constitute the test set. Each image is represented by a 294-dimensional feature vector using the procedure described in [16]. The extracted features are spatial color moments in the LUV space, which are commonly used in the scene classification literature.\nThe yeast dataset [17] is formed by micro-array expression data and phylogenetic profiles, and includes 2417 genes, 1500 of which are used for training and the rest constitute the test set. Each gene is represented by a 103-dimensional feature vector, and associated with a set of functional groups out of 14 possible classes (such as metabolism, transcription and protein synthesis).\nSimilarly to the single-label classification experiment, we compare two variants of our method (with and without adapting the graphs) to LC-KSVD1 and LC-KSVD2 proposed in [8].\nTo assess the accuracy of the algorithms in the multi-label experiments, we use the average precision measure as defined in [16]. The obtained classification results are summarized in Table 2, indicating that our algorithm clearly outperforms the other methods.\nIn the multi-modal scenario, the potential overlap between different combinations of classes implies that a more complex underlying structure could be learned and exploited. As expected, the impact of the structural constraints in the multimodal scenario is very significant, and more pronounced compared with the single-label examples. For the yeast dataset, which is known to be difficult, the achieved improvement in classification accuracy is almost 7%. Moreover, though increasing the computational complexity, it can be observed that adapting the Laplacian matrices further improves the classification results for both datasets."
    }, {
      "heading" : "4. CONCLUSIONS",
      "text" : "In this paper, we suggested an extension of our previously proposed dual graph regularized dictionary learning algorithm to a supervised setting. In the new algorithm, the dictionary atoms are encouraged to preserve the feature similarities as detected in the training data and encapsulated by the graph Laplacian LG , thus leading to a better representative and more robust dictionary. By adhering to the intrinsic geometrical structure of the data manifold, as captured by the graph Laplacian LM, the resulting sparse codes are more discriminative and can significantly enhance classification performance.\nExperiments performed on different datasets demonstrate that the proposed algorithm yields very good classification results, outperforming other supervised dictionary learning algorithms for both single-label and multi-label classification tasks."
    }, {
      "heading" : "5. REFERENCES",
      "text" : "[1] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Yi Ma, “Robust face recognition via sparse representation,” TPAMI, vol. 31, no. 2, pp. 210–227, Feb 2009.\n[2] M. Yang, L. Zhang, J. Yang, and D. Zhang, “Metaface\nlearning for sparse representation based face recognition,” in ICIP, 2010, pp. 1601–1604.\n[3] M. Yang, L. Zhang, X. Feng, and D. Zhang, “Fisher discrimination dictionary learning for sparse representation,” in ICCV, 2011, pp. 543–550.\n[4] I. Ramirez, P. Sprechmann, and G. Sapiro, “Classification and clustering via dictionary learning with structured incoherence and shared features,” in CVPR, 2010, pp. 3501–3508.\n[5] J. Mairal, F. R. Bach, J. Ponce, G. Sapiro, and A. Zisserman, “Supervised dictionary learning,” in NIPS, pp. 1033–1040. 2009.\n[6] D.-S. Pham and S. Venkatesh, “Joint learning and dictionary construction for pattern recognition,” in CVPR, June 2008, pp. 1–8.\n[7] Q. Zhang and B. Li, “Discriminative K-SVD for Dictionary Learning in Face Recognition,” in CVPR, June 2010, pp. 2691–2698.\n[8] Z. Jiang, Z. Lin, and L. S. Davis, “Learning a Discriminative Dictionary for Sparse Coding via Label Consistent K-SVD,” in CVPR, 2011, pp. 1697–1704.\n[9] M. Aharon, M. Elad, and A. Bruckstein, “K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation,” IEEE Trans. Sig. Proc., vol. 54, no. 11, pp. 4311–4322, Nov. 2006.\n[10] Y. Yankelevsky and M. Elad, “Dual Graph Regularized Dictionary Learning,” IEEE Transactions on Signal and Information Processing over Networks, 2016, doi:10.1109/TSIPN.2016.2605763.\n[11] M. Zheng, J. Bu, C. Chen, C. Wang, L. Zhang, G. Qiu, and D. Cai, “Graph Regularized Sparse Coding for Image Representation,” IEEE Trans. Img. Proc., vol. 20, no. 5, pp. 1327–1336, May 2011.\n[12] K. N. Ramamurthy, J. J. Thiagarajan, P. Sattigeri, and A. Spanias, “Learning Dictionaries with Graph Embedding Constraints,” in ASILOMAR, Nov 2012, pp. 1974– 1978.\n[13] M. Belkin and P. Niyogi, “Laplacian eigenmaps for dimensionality reduction and data representation,” Neural Comput., vol. 15, no. 6, pp. 1373–1396, June 2003.\n[14] A. Martinez and R. Benavente, “The AR Face Database,” Tech. Rep. 24, CVC, June 1998.\n[15] A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman, “From Few to Many: Illumination Cone Models for Face Recognition Under Variable Lighting and Pose,” TPAMI, vol. 23, no. 6, pp. 643–660, June 2001.\n[16] M. R. Boutell, J. Luo, X. Shen, and C. M. Brown, “Learning multi-label scene classification,” Pattern Recognition, vol. 37, no. 9, pp. 1757–1771, 2004.\n[17] A. Elisseeff and J. Weston, “A Kernel Method for MultiLabelled Classification,” in NIPS, 2001, pp. 681–687."
    } ],
    "references" : [ {
      "title" : "Robust face recognition via sparse representation",
      "author" : [ "J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Yi Ma" ],
      "venue" : "TPAMI, vol. 31, no. 2, pp. 210–227, Feb 2009.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Metaface  learning for sparse representation based face recognition",
      "author" : [ "M. Yang", "L. Zhang", "J. Yang", "D. Zhang" ],
      "venue" : "ICIP, 2010, pp. 1601–1604.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Fisher discrimination dictionary learning for sparse representation",
      "author" : [ "M. Yang", "L. Zhang", "X. Feng", "D. Zhang" ],
      "venue" : "ICCV, 2011, pp. 543–550.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Classification and clustering via dictionary learning with structured incoherence and shared features",
      "author" : [ "I. Ramirez", "P. Sprechmann", "G. Sapiro" ],
      "venue" : "CVPR, 2010, pp. 3501–3508.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Supervised dictionary learning",
      "author" : [ "J. Mairal", "F.R. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman" ],
      "venue" : "NIPS, pp. 1033–1040. 2009.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Joint learning and dictionary construction for pattern recognition",
      "author" : [ "D.-S. Pham", "S. Venkatesh" ],
      "venue" : "CVPR, June 2008, pp. 1–8.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Discriminative K-SVD for Dictionary Learning in Face Recognition",
      "author" : [ "Q. Zhang", "B. Li" ],
      "venue" : "CVPR, June 2010, pp. 2691–2698.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning a Discriminative Dictionary for Sparse Coding via Label Consistent K-SVD",
      "author" : [ "Z. Jiang", "Z. Lin", "L.S. Davis" ],
      "venue" : "CVPR, 2011, pp. 1697–1704.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation",
      "author" : [ "M. Aharon", "M. Elad", "A. Bruckstein" ],
      "venue" : "IEEE Trans. Sig. Proc., vol. 54, no. 11, pp. 4311–4322, Nov. 2006.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Dual Graph Regularized Dictionary Learning",
      "author" : [ "Y. Yankelevsky", "M. Elad" ],
      "venue" : "IEEE Transactions on Signal and Information Processing over Networks, 2016, doi:10.1109/TSIPN.2016.2605763.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Graph Regularized Sparse Coding for Image Representation",
      "author" : [ "M. Zheng", "J. Bu", "C. Chen", "C. Wang", "L. Zhang", "G. Qiu", "D. Cai" ],
      "venue" : "IEEE Trans. Img. Proc., vol. 20, no. 5, pp. 1327–1336, May 2011.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning Dictionaries with Graph Embedding Constraints",
      "author" : [ "K.N. Ramamurthy", "J.J. Thiagarajan", "P. Sattigeri", "A. Spanias" ],
      "venue" : "ASILOMAR, Nov 2012, pp. 1974– 1978.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Laplacian eigenmaps for dimensionality reduction and data representation",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "Neural Comput., vol. 15, no. 6, pp. 1373–1396, June 2003.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The AR Face Database",
      "author" : [ "A. Martinez", "R. Benavente" ],
      "venue" : "Tech. Rep. 24, CVC, June 1998.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "From Few to Many: Illumination Cone Models for Face Recognition Under Variable Lighting and Pose",
      "author" : [ "A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman" ],
      "venue" : "TPAMI, vol. 23, no. 6, pp. 643–660, June 2001.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Learning multi-label scene classification",
      "author" : [ "M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown" ],
      "venue" : "Pattern Recognition, vol. 37, no. 9, pp. 1757–1771, 2004.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A Kernel Method for Multi- Labelled Classification",
      "author" : [ "A. Elisseeff", "J. Weston" ],
      "venue" : "NIPS, 2001, pp. 681–687.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1, 2, 3, 4]).",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 1,
      "context" : "[1, 2, 3, 4]).",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 2,
      "context" : "[1, 2, 3, 4]).",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 3,
      "context" : "[1, 2, 3, 4]).",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 4,
      "context" : "[5, 6, 7, 8]) learn a discriminative dictionary by introducing a classification-error term into the objective function, and enforcing some discriminative criteria on the optimized sparse coefficients.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 5,
      "context" : "[5, 6, 7, 8]) learn a discriminative dictionary by introducing a classification-error term into the objective function, and enforcing some discriminative criteria on the optimized sparse coefficients.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 6,
      "context" : "[5, 6, 7, 8]) learn a discriminative dictionary by introducing a classification-error term into the objective function, and enforcing some discriminative criteria on the optimized sparse coefficients.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 7,
      "context" : "[5, 6, 7, 8]) learn a discriminative dictionary by introducing a classification-error term into the objective function, and enforcing some discriminative criteria on the optimized sparse coefficients.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 7,
      "context" : "Of the latter category, we focus on the Label Consistent K-SVD (LC-KSVD) method [8] for joint learning of an overcomplete dictionary D and an optimal linear classifier W :",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "Equation (3) can be efficiently solved using the K-SVD algorithm [9], which iteratively alternates between a sparse coding step (optimization over X) and a dictionary update step (that updates each dictionary atom in D̃ along with its related coefficients from X).",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "In [10], we have suggested an unsupervised dictionary learning algorithm for graph signals.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "In this paper, we propose an extension of our dual graph regularized dictionary learning algorithm to a supervised setting by applying the same ideas to the LC-KSVD approach [8].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 7,
      "context" : "Our proposed algorithm is based on the LC-KSVD approach [8].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "Similarly to the methods proposed in [11, 12], we incorporate LM into the objective function as a regularizer of the form Tr(XLMX T ).",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "Similarly to the methods proposed in [11, 12], we incorporate LM into the objective function as a regularizer of the form Tr(XLMX T ).",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "This term therefore encourages similar signals, having a large proximity measure wij , to have similar sparse codes, thus satisfying the commonly known manifold assumption [13].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 9,
      "context" : "This can be done using the graphDL algorithm we proposed in [10] that reflects the added restrictions.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "To better handle partial correlations, these dependencies can be learned and adapted along with the dictionary learning process, as previously suggested in [10].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 9,
      "context" : "• Run a few iterations of graphDL (see [10], Algorithm 3), initialized with D̃, to solve Equation (9) and obtain D̃, X.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "Single-Label Classification First, we evaluate the performance of the proposed algorithm for single-label image classification on the AR Face database [14] and on the Extended YaleB database [15].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 14,
      "context" : "Single-Label Classification First, we evaluate the performance of the proposed algorithm for single-label image classification on the AR Face database [14] and on the Extended YaleB database [15].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 7,
      "context" : "Each image is represented by a 540-dimensional feature vector using the procedure described in [8].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "We compare the proposed algorithm to the two variants proposed in [8]: LC-KSVD1, which refers to (2) for β = 0, and LC-KSVD2, which refers to (2) for α, β 6= 0.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "results presented in [8].",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : "Each image is represented by a 294-dimensional feature vector using the procedure described in [16].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : "The yeast dataset [17] is formed by micro-array expression data and phylogenetic profiles, and includes 2417 genes, 1500 of which are used for training and the rest constitute the test set.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "Similarly to the single-label classification experiment, we compare two variants of our method (with and without adapting the graphs) to LC-KSVD1 and LC-KSVD2 proposed in [8].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 15,
      "context" : "To assess the accuracy of the algorithms in the multi-label experiments, we use the average precision measure as defined in [16].",
      "startOffset" : 124,
      "endOffset" : 128
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we propose a supervised dictionary learning algorithm that aims to preserve the local geometry in both dimensions of the data. A graph-based regularization explicitly takes into account the local manifold structure of the data points. A second graph regularization gives similar treatment to the feature domain and helps in learning a more robust dictionary. Both graphs can be constructed from the training data or learned and adapted along the dictionary learning process. The combination of these two terms promotes the discriminative power of the learned sparse representations and leads to improved classification accuracy. The proposed method was evaluated on several different datasets, representing both single-label and multi-label classification problems, and demonstrated better performance compared with other dictionary based approaches.",
    "creator" : "LaTeX with hyperref package"
  }
}