{
  "name" : "1310.4210.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Demystifying Information-Theoretic Clustering",
    "authors" : [ "Greg Ver Steeg", "Aram Galstyan", "Fei Sha", "Simon DeDeo" ],
    "emails" : [ "GREGV@ISI.EDU", "GALSTYAN@ISI.EDU", "FEISHA@USC.EDU", "SIMON.DEDEO@GMAIL.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Clustering is a fundamental problem in machine learning (Jain, 2010). As the objective of clustering is typically exploratory in nature, we desire clustering algorithms that make as few assumptions about the data as possible. We would like those algorithms to be flexible enough to reveal complex data patterns that are nonlinear, multi-modal and invariant with respect to changes in data representation. Ideally, we would like to achieve those goals without explicitly defining some notion of similarity between data points (Slonim et al., 2005) or defining “prototypical” clusters (Böhm et al., 2006).\nLatent variable models are a common approach to clustering. This is exemplified by the wide adoption of Gaussian mixture models and its simplified version k-means. While\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nmaximizing likelihood of the data under some probabilistic model is a clear and operationally meaningful criteria, such models are not invariant to changes of data representation and require a fully specified generative model.\nOstensibly, information-theoretic criteria satisfy all our desiderata. There has been a growing interest in information-theoretic clustering where we assign cluster labels to data points such that the mutual information between data and labels is maximized (Faivishevsky & Goldberger, 2010; Wang & Sha, 2011; Müller et al., 2012; Sugiyama et al., 2011). Mutual information captures higher-order statistics in the data and is suitable for complex data patterns where linear approaches or Gaussian assumptions are inadequate. Moreover, mutual information is invariant with respect to smooth, invertible transformations of data (Cover & Thomas, 2006). It can also be estimated non-parametrically from the data samples without defining a parametric space of probability distributions (Kraskov et al., 2004). This is especially attractive for clustering high-dimensional data.\nThe main contribution of this paper is to demonstrate that information-theoretic clustering based on mutual information is fundamentally flawed and to derive a novel alternative grounded in information-theoretic principles. We derive a non-parametric estimator for our clustering objective and a tractable heuristic to optimize it. We demonstrate simple scenarios that cause mutual information clustering to fail dramatically while our method succeeds.\nMutual information is naturally interpreted as a measure of compressibility of data. However, compression alone does not capture natural cluster structure in the data. As we illustrate with an exemplar problem, with more information about the underlying probability distribution, mutual information clustering will prefer to split the probability mass into arbitrary but equal-sized masses of probability, completely ignoring the data’s intrinsic structure!\nar X\niv :1\n31 0.\n42 10\nv2 [\ncs .L\nG ]\n5 F\neb 2\nHow do we reconcile this observation with the many previously reported empirical successes of information-theoretic clustering? We argue that the good clustering performance demonstrated by those methods is not due to the objective but rather reflect transient effects of the estimators used. Thus, counterintuitively, all those methods eventually fail when given more data. In the large data limit, the estimators converge to their true values yielding equal-sized, but meaningless, clusters.\nWe show that a fix cannot be constructed by simply tweaking the information-theoretic objective. Instead, we construct an objective from first principles that preserves information theory axioms even when applied to finite samples from an unknown distribution. We motivate our approach by appeal to the axiom of consistency under coarsegraining that forms the definition of entropy (Shannon, 1948). While the consistency axiom is preserved exactly in the limit of infinite data, empirical estimation will generically lead to its violation. We shall show how larger violations signal a non-robust partition. A lower violation of consistency under coarse-graining is an important property preserved by natural cluster structures, but not by the spurious equal-sized partitions implied by mutual information. We thus propose data be clustered such that consistency is violated minimally. We construct a non-parametric estimator for this quantity and demonstrate an alternate interpretation of consistency violation as cluster label uncertainty.\nWe validate the proposed approach on synthetic data and commonly used datasets for clustering and contrast to existing approaches for information-theoretic clustering. For synthetic data, we show that our measure overcomes the shortcomings of previous information-theoretic estimators, recovering natural clusters even in the limit of large data. We also introduce a heuristic to optimize this objective and we show that it recovers non-convex clusters and achieves competitive clustering results on standard datasets.\nThe rest of the paper is organized as follows. In Sec. 2, we describe the basic idea of information-theoretic clustering and point out the pitfalls of the status quo which equates compression with clustering. In Sec. 3, we describe the proposed approach, starting by developing the idea of coarse-graining. In Sec. 4, we report empirical studies on applying our approach to synthetic and real-world datasets. Related work, conclusions, and future research directions are discussed in Sec. 5."
    }, {
      "heading" : "2. Information-theoretic clustering and its pitfalls",
      "text" : "Given samples drawn i.i.d. from a known distribution, the Shannon entropy of the distribution can be interpreted as the minimum number of bits needed to encode each sam-\nple (on average). For clustering, we are given samples of an unknown distribution, and we would like to label (encode) each sample to reflect some natural structure. Even if we knew the Shannon entropy of the distribution, a code that achieves this optimal compression does not necessarily reflect the natural structure of the underlying distribution."
    }, {
      "heading" : "2.1. Basic concepts and entropy estimation",
      "text" : "We begin with the generic clustering problem in which we are given some samples x(i) ∈ Rd for i = 1, . . . , N , drawn from some unknown distribution, p(x). The goal is to associate a discrete label, y(i) ∈ {0, 1} (for simplicity we use only binary labels), that somehow reflects a natural clustering of the data. Of course, the main difficulty is in defining what qualifies as a natural clustering. In what follows, we consider various information-theoretic criteria.\nEntropy is defined in the usual way as H(X) = E[− log p(x)]. We use base-two logarithms so that information is measured in bits. Using standard notation, capital X denotes a random variable whose instances are denoted in lowercase, and the fact that entropy is a functional of the probability distribution is only explicitly written when clarity demands it. The expectation value may be a sum for discrete variables or an integral in the continuous case. Higher-order entropies can be constructed in various ways from this standard definition. For reference, we provide a few alternate forms of the mutual information, I(X;Y ).\nI(X;Y ) = H(X) +H(Y )−H(X,Y ) = H(X)−H(X|Y ) = H(Y )−H(Y |X)\nA function that estimates entropy from some i.i.d. samples drawn from p(x) we denote with Ĥ(X). An intuitive way to estimate entropy directly from samples in a nonparametric way follows.\nH(X) = E(log 1 p(x) ) ≈ 1 N N∑ i=1 log 1 p(x(i)) (1)\n≈ 1 N N∑ i=1 log di,k k/N\nĤ(X) ≡ log(N/k) + d N N∑ i=1 log i,k + ck,N (2)\nIn the first line, we take the sample mean instead of the expectation value, but we still have the (unknown) density, p(x(i)), in the expression. On the next line, we locally approximate this density by making the smallest box that contains k neighboring points. Then the density is approximated by k/N , the fraction of points in the box,\nover the volume, di,k, where i,k denotes the distance to the k-th nearest neighbor of point x(i) according to the max-norm. In our definition in the second line, we add a small constant factor ck,N = ψ(N) − ψ(k) + log(2k/N) to match with the non-parametric, asymptotically unbiased Kozachenko-Leonenko entropy estimator (Kozachenko & Leonenko, 1987) (or k-nearest neighbor, or kNN, estimator), which requires a more involved derivation as provided by Kraskov et. al. (Kraskov et al., 2004). We write it in this alternate format to increase intuition and to ease later derivations. Note that this estimator depends only on distances between neighboring data points. The estimator has also been empirically shown to have good performance for small amounts of data, with k = 3 representing a good choice (Kraskov et al., 2004; Khan et al., 2007). For discrete variables, we use the standard plug-in estimator. DeDeo et. al. discuss more nuanced alternatives in the discrete case (DeDeo et al., 2013)."
    }, {
      "heading" : "2.2. Pitfalls",
      "text" : "Compression 6= clustering A frequently invoked and plausible sounding intuition is that we should maximize mutual information between data and cluster labels. Consider first the simple case of clustering discretely distributed data, exemplified in Fig. 1(a). Is there a purely information-theoretic criteria that clusters the bins of this discrete probability distribution into the two natural groups separated by the gap? There cannot be, because from a purely information-theoretic perspective the bin labels are arbitrary. That is, the bins can be re-ordered arbitrarily so that, e.g., there is no gap, and this re-ordering does not affect any information-theoretic quantity because they depend only on the values pi. While no one would attempt to cluster these categorical variables without defining some relationship between the variables, the same issue arises in a more subtle form for continuous distributions.\nIn the simplest picture of clustering for continuous variables, we have a mixture of two uniform probability distributions in one dimension, shown in Fig. 1(b). If we split the two pieces according to the intuitive clustering, a simple analytic calculation gives I(X;Y ) = H0(α/(α + β)), where H0 represents the binary entropy function which is in the range [0, 1]. If we split the space into two equally sized masses of probability, we maximize the mutual information, I(X; Ȳ ) = 1. Clearly, maximizing mutual information does not have the intended effect. Even slightly unbalanced clusters will not be found. Essentially, the scenario in Fig. 1(b) is the limit of the case in Fig. 1(a) with an infinite number of bins that are infinitely narrow (Cover & Thomas, 2006). These infinitesimal bins can be re-ordered arbitrarily without affecting the value of any informationtheoretic (IT) measure.\nThe mystery about the empirical success Although this example makes it clear that a good clustering is not the one that maximizes mutual information, or any solely IT criteria, this leads to a mystery. How have so many papers achieved good clustering performance using this criteria (Wang & Sha, 2011; Faivishevsky & Goldberger, 2010; Müller et al., 2012)? To understand this result, it helps to write mutual information in the form I(X;Y ) = H(Y )−H(Y |X). The first term, H(Y ) is maximized for equally sized clusters. The second term, H(Y |X), should be 0 for any exact partitioning of the input space (in which case y = f(x)).\nHowever, it is easy to see that if we cluster a finite sample of data points, as in Fig. 2(b), that our estimate, Ĥ(Y |X), will not be zero (using, e.g., the non-parametric estimator introduced in Sec. 2.1). In particular, this will be the case near the boundary between the two clusters. The natural clustering will have a smaller value for Ĥ(Y |X) (due to the gap of width L separating the clusters). On the other hand, Ĥ(Y ) will be larger for clusters of equal size. These two terms compete. For small amounts of data, we can see in Fig. 2(a) that the natural clustering is preferred, i.e. has higher estimated mutual information. However, as the amount of data increases, the uncertainty decreases and eventually equal-sized clusters will be preferred. Ironically, more data leads to a less desirable result. This behavior does not depend on the estimator used: any estimator\nthat is asymptotically unbiased will converge to the same value in the large N limit. The behavior also persists for arbitrary distributions because the contribution of Ĥ(Y |X) comes from points near the boundary. For any clusters, the percentage of points near the boundary will decrease as N increases. Tests with previous information-theoretic clustering objectives focused on small, nearly balanced datasets (like many of the UCI datasets considered in Sec. 4), so that these shortcomings went unnoticed.\nPrecision is both the problem and the solution. Increasing N reduces the uncertainty in cluster labels near the cluster boundaries, so that any clustering has approximately the same value of Ĥ(Y |X). On the other hand, given a sample of N points, we can always resample to include fewer points. Then we can test whether a clustering is more natural in the sense that it reduces the uncertainty in the cluster labels, even for small amounts of data. We make this intuition precise in the next section."
    }, {
      "heading" : "3. Proposed approach",
      "text" : "We saw that previous information-theoretic clustering schemes only work accidentally insofar as they mis-\nestimate the true uncertainty of cluster labels near the cluster boundaries because of finite data. This behavior is closely linked to the axioms of information theory. We briefly make this connection before deriving a simple expression that explicitly makes use of the uncertainty near cluster boundaries due to limited data."
    }, {
      "heading" : "3.1. Shannon’s Axiom of Consistency under Coarse-Graining",
      "text" : "Shannon’s original derivation of entropy (Shannon, 1948) begins with properties that a measure of uncertainty should be expected to obey and concludes that (Shannon) entropy is the unique measure that satisfies these properties. Consider a discrete probability distribution where outcome j occurs with probability pj ≡ p(y = j). The first two desired properties are continuity (a small change in pj does not cause a large change in uncertainty) and that uncertainty should grow with k if there are k equally likely events. The final property we refer to as consistency under coarse-graining.\nh(p1, p2, p3) = h(p1, p2 + p3)+\n(p2 + p3) h(p2/(p2 + p3), p3/(p2 + p3)).\nIntuitively, the measure of uncertainty should not change if we lump together (coarse-grain) bins 2 and 3. After combining the uncertainty of the coarse-grained bins with the (weighted) uncertainty within each coarse-grained bin we should recover the original measure of uncertainty. DeDeo et. al. make the further point that any estimator of entropy should at least approximately satisfy this property or it risks losing its meaning as an information-theoretic measure altogether (DeDeo et al., 2013).\nWhat is the natural analogue of the coarse-graining criteria for continuous distributions? Suppose we split our space into two disjoint regions R0,R1 so that R1 = Rd \\ R0. For some continuous distribution p(x), we can define a joint probability p(x, y), where y = j ↔ x ∈ Rj , or equivalently, y = f(x), f : Rd → {0, 1}. Then the probability of drawing a point in region j is just ∫ Rj dx p(x) =∫\ndx p(x, y = j) = p(y = j). Consistency under coarsegraining would demand,\nH(p(x)) = H(p(y))+ (3) p(y = 0)H(p(x|y = 0)) + p(y = 1)H(p(x|y = 1)),\nwhich is easily shown to be satisfied for differential entropy. This can be written in the more succinct, standard notation as H(X) = H(Y ) +H(X|Y )1.\nFollowing the logic for discrete entropy estimators, we would like to check if differential entropy estimators based\n1Note that H(X|Y ) ≡ ∑\ni p(y = i)H(p(x|y = i)), and that this condition should only hold if y = f(x)\non finite data obey some notion of consistency under coarse-graining. It is easily shown that no estimator can satisfy this condition for arbitrary coarse-grainings and remain asymptotically unbiased.2 Since we cannot construct an asymptotically unbiased entropy estimator that is consistent under arbitrary coarse-graining, instead we start with an unbiased estimator and search for coarse-grainings that lead to consistent entropy estimates. We refer to these coarse-grainings as natural. In principle, this argument can be applied to any entropy estimator, but we choose to focus on non-parametric estimators in keeping with our goal to minimize assumptions about the data.\nGiven samples, x(i), from a continuous distribution, p(x), a coarse-graining (clustering) can be defined by associating a discrete label y(i) with each sample point. We can then quantify the consistency violation (CV) with respect to Eq. 3, where CV = 0 if the equality is exactly satisfied.\nCV = Ĥ(Y ) + Ĥ(X|Y )− Ĥ(X) (4)\nHere Ĥ(X), Ĥ(X|Y ) are just defined by a differential entropy estimator like the one in Eq. 1. Fig. 3 gives an example of two different ways of coarse-graining the same data. The natural coarse-graining which separates well-defined clusters (solid line) produces a small consistency violation, CV, while the alternate coarse-graining (dashed line) leads to large violations. CV can be viewed as a measure of how well we can estimate the global entropy given the entropy of clusters of data points (i.e. the coarse-grained data). For well-separated clusters, the global entropy is just a weighted average of the cluster entropies. This observation provides some extra theoretical motivation for the clustering objective we propose next.\nThe right-hand side of Eq. 4 looks familiar as an alternate form for writing H(Y |X). We can simply define an estimator for the uncertainty of the cluster label given a sample point, Ĥ(Y |X), in terms of our previously defined estima-\n2Imagine we have N samples from an unknown distribution, p(x). We can randomly partition the N samples into two equal size sets and define regions R0,R1 accordingly to contain both sets. Call the entropy estimates for all N points and for a random subset of N/2 points ĤN (X), ĤN/2(X), respectively. In the large N limit, we expect both of these quantities to converge to the true entropy for an asymptotically unbiased estimator. On the other hand, consistency (Eq. 3) would require ĤN (X) = 1 + ĤN/2(X), a clear conflict.\ntors: Ĥ(Y |X) ≡ Ĥ(Y )+Ĥ(X|Y )−Ĥ(X). Our estimate of the uncertainty of cluster labels is exactly the same as the violation of the coarse-graining axiom due to limited data. This gives us two interpretations of our clustering objective. We want our coarse-graining to be natural in the sense that we do not violate information-theoretic axioms, even for small amounts of data. Equivalently, we want our estimated uncertainty about cluster labels to be as low as possible, even for small amounts of data."
    }, {
      "heading" : "3.2. Using conditional entropy for clustering",
      "text" : "Nonparametric estimation of conditional entropy We first derive a more compelling form for Eq. 4 in Sec. A of the supplementary material. For each sample point, x(i), we associate a discrete cluster label, y(i), then,\nĤk(Y |X) = d\nN N∑ i=1 log ̄i,k i,k , (5)\nwhere i,k represents the distance to the k-th nearest neighbor while ̄i,k is the distance to the k-nn restricted to points in the same cluster as sample i.3 As long as each point’s k nearest neighbors lie within the same cluster, the consistency violation, or estimated uncertainty about the cluster label, will be zero, as in Fig. 3 where we used k = 3.\nTotal cluster label uncertainty For any fixed partitioning of the space, CV approaches zero for large N . We want our cluster label uncertainty to be small even under arbitrary resampling with limited data, so we now imagine that each point is kept with random, independent probability 1 − α. We can estimate the expected value of Eq. 5 for any α, Ĥα,k(Y |X) ≡ Eα[Ĥk(Y |X)], where the expectation is over all possible resamplings. Toward this end, we note that a point’s k-th nearest neighbor in the resampled dataset corresponds to its m-th nearest neighbor (m ≥ k) in the original dataset with probability q(m|k) = Ck−1m−1(1− α)kαm−k, yielding,\nĤα,k(Y |X) = (6)\nd\nN N∑ i=1 N−1∑ m=k Ck−1m−1(1− α)kαm−k log ̄i,m i,m .\nIf we want Ĥα,k(Y |X) to be small for all α, we can just consider its value integrated over all α. We refer to this quantity as the total cluster label uncertainty or total consistency violation. After performing an elementary integra-\n3An ambiguity in this definition arises if there are k or fewer points in a cluster. Let ny represent the number of points with the cluster label y. Then if k > ny(i) + 1 we define ̄i,k = i,N−1. This definition reflects the fact that our uncertainty is maximal if we are not given sufficient data. See Appendix A for more details.\ntion and changing the summation variable we obtain\nĤT,k(Y |X) = ∫ 1 0 dα Ĥα,k(Y |X) = (7)\nd\nN N∑ i=1 N−k∑ l=1\nk\n(l + k)(l + k − 1) log ̄i,l+k−1 i,l+k−1 .\nFor entropy estimators, k = 1 leads to the lowest bias but higher values of k are often chosen to reduce variance (Khan et al., 2007). Because averaging over resamplings already reduces variance, we choose k = 1, simplifying the expression further, and we will refer to this quantity succinctly as ĤT (Y |X).4\nAlthough this expression looks simple, i,l is a function of all the x(j) and ̄i,l is a function of all the x(j), y(j). In principle, this quantity should vary between 0 (a perfect clustering) and H(Y ) (a completely random clustering). We can search for partitions that minimize the ratio of ĤT (Y |X)/Ĥ(Y ) so that we can objectively compare clusterings on a scale of zero to one. We will see in the examples that natural partitions have low CV while most partitions have a CV orders of magnitude larger. Unlike mutual information estimators, this quantity distinguishes natural cluster structure even in the large N limit.\nNumerical procedure While the focus of this work is on deriving a principled approach to information-theoretic clustering, we should briefly mention some practical concerns. Optimizing Eq. 7 over all possible partitions is a difficult problem. We consider a heuristic approach which involves solving a tractable semidefinite program in Appendix B of the supplementary material. We note that other information-theoretic approaches also require heuristic approaches to optimize (Wang & Sha, 2011). We leave a detailed exploration of the best heuristics for this optimization, along with extensive comparisons to other clustering objectives, for future work. In the next section, we consider simple clustering scenarios where we can compare all partitions to develop intuition about the meaning of this objective. We briefly compare results from our heuristic solver to other clustering methods for some standard datasets."
    }, {
      "heading" : "4. Results",
      "text" : ""
    }, {
      "heading" : "4.1. Synthetic datasets",
      "text" : "Our goal is to find clusterings, determined by y(i), that optimize the objective in Eq. 7. Recall the simple example in Sec. 2 which mutual information failed to cluster correctly for large N . For data taken from the simple one-\n4For k = 1, the log-distance of the l-th nearest neighbor is weighted by a factor of 1/(l(l + 1)). For comparison, the NIC objective (Faivishevsky & Goldberger, 2010) weights all the logdistances equally. See Sec. 5 for a more detailed comparison.\ndimensional distribution in Fig. 2, we can just measure the ratio ĤT (Y |X)/Ĥ(Y ) (CVR for short from here on) for all possible ways of splitting the x axis, shown in Fig. 4. We discard the trivial coarse-graining where all points are in the same group which has an undefined CVR of 0/0, and then the best partition exactly corresponds to our intuition of the most natural clustering. We also see from Fig. 4 that more data leads to a better separation between good and bad clusterings, as desired.\nFig. 7 shows that these results are robust to parameter changes, though NIC, unlike MI, converges to correct solutions if the cluster size imbalance is small. CVR is the only objective to prefer the correct partition over a wide range of parameter values. While in this simple example, we only considered partitions defined by Yr, we show in Sec. B that our heuristic optimizer is able to search over all possible partitions to recover the correct, non-convex clusters."
    }, {
      "heading" : "4.2. Real-world datasets",
      "text" : "Editor activity on Wikipedia In Fig. 8, we consider the behavior of users who frequently edit the Wikipedia page for George W. Bush (DeDeo, 2012). On Wikipedia, users may contest any article’s point of view by directly editing the text. When a change is made by some user, any other user can choose to reject the change by “reverting” to a prior version. Certain types of users are likely to get into turf wars or, conversely, to engage in pro-social vandalism repair, so that a large fraction of their activity consists of “reverts”. We look at the fraction of a user’s activities that does not consist of reverting to previous versions, fnr. For the 50 most active users on the George W. Bush Wikipedia page, we plot fnr in Fig. 8. Both CVR and Bayesian latent variable models like k-means discover the same natural cluster structure when used to determine a binary partition.\nUCI datasets We also consider three standard clustering datasets from the UCI Machine Learning database (Bache & Lichman, 2013): glass, iris, and wine. Although we cannot optimize the objective in Eq. 7 exactly, we describe details of a heuristic approach in Appendix B. The results are summarized in Table 1. We achieve competitive results to\nk-means and the best of IT methods that have been previously compared (Wang & Sha, 2011). Note that the cluster sizes for iris and glass are exactly and approximately balanced respectively which benefits mutual information since it is biased towards balanced solutions.\nWe also compare the CVR of the ground truth clustering to the best clustering found according to our heuristic. The CVR of the clusters found for iris and wine is comparable to that of the ground truth and much smaller than the maximal possible CVR of 1. On the other hand, the CVR for the ground truth clustering for glass is actually larger than 1 (possible due to noisy estimators). This implies cluster labels of neighboring sample points are nearly random. Note that the Rand index (Rand, 1971) of the clustering in which each point is its own cluster achieves a score of 0.74, comparable to all the results listed in Table 1."
    }, {
      "heading" : "5. Discussion",
      "text" : "Related Work Many clustering approaches explicitly combine information-theoretic methods with a similarity measure (Slonim et al., 2005; Böhm et al., 2006; Gokcay & Principe, 2002). That is they define some function f(x(i), x(j)) that characterizes the similarity of two points and then maximize intra-cluster similarity while simultaneously optimizing some information-theoretic quantity. While these methods side-step our critique of purely information-theoretic clustering methods, they also lose generality with their insistence on defining some ad hoc notions of similarity. A line of recent work has attempted to maximize the mutual information between data and cluster labels using non-parametric entropy estimators. Faivishevsky and Goldberger (Faivishevsky & Goldberger, 2010) attempt to use a modified version of a nonparametric entropy estimator (Kraskov et al., 2004) to create a more tractable objective for optimization.5 Wang and Sha demonstrate a semidefinite optimization based on this criteria (Wang & Sha, 2011). Other attempts to define tractable optimization problems invoke a squared loss variant of mutual information (Sugiyama et al., 2011) and estimators based on constructing minimum spanning trees (Müller et al., 2012). While these methods are general and have shown some success, we showed in Sec. 2 and Sec. 4 that these approaches are ultimately flawed and will fail in the large data limit.\nWe showed that our approach, unlike MI-based methods, is not biased towards balanced clusters. This is in keeping with our desire to minimize the number of assumptions. On the other hand, there may be scenarios in which a bias\n5It should be noted that the entropy estimator in (Faivishevsky & Goldberger, 2008) is not asymptotically correct. They argue “The MeanNN estimator exploits the fact that the kNN estimation is valid for every k and therefore averaging estimators for all possible values of k leads itself to a new estimator of the differential entropy”. The k-NN estimator is guaranteed to be consistent only when k grows sufficiently slowly (sublinearly) with N , the number of samples; see (Wang et al., 2009) and references therein. Averaging all values of k up to N violates this condition. A simple example where their estimator can be seen to fail is the distribution in Fig. 1(b). Regardless of N , their estimator is proportional to logL, while the true entropy is independent of L.\ntowards balanced clusters is desirable. One line of work attempts to define intuitive properties of clustering methods (like sensitivity to cluster balance) and then categorize them with respect to these properties; see (Ackerman et al., 2012) and references therein.\nConclusion We have demonstrated why existing information-theoretic clustering methods are conceptually flawed and presented a principled solution without introducing any model-based assumptions6. Consistency under coarse-graining is essential to the definition of entropy. Formally incorporating this notion in a finite-data setting provides a basic and foundational motivation for defining clusters. Alternately, our objective can be viewed as a way of minimizing the estimated uncertainty of cluster labels. Preliminary results indicate the feasibility of optimizing our criteria and also demonstrate competitive accuracy on standard benchmarks.\nInformation-theoretic learning methods are attractive because they make no assumptions about the underlying data while maintaining a clear, operational meaning. Realworld learning problems consist of finite samples of data from unknown distributions. To construct an informationtheoretic foundation for unsupervised learning, we need to carefully refine our measures so that they make sense in finite-data regimes. We hope that further development of these ideas will contribute to that goal."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank the reviewers for helpful comments. A.G. and G.V. were supported in part by the US AFOSR MURI grant FA9550-10-1-0569 and DTRA grant HDTRA1-10-1-0086. F.S. is supported by an Alfred P. Sloan Foundation Research Fellowship. S.D. acknowledges the support of National Science Foundation grant EF-1137929.\n6While we achieved our goal of avoiding reliance on a global similarity measure, non-parametric entropy estimators require a notion of adjacency to find data points that are nearest neighbors according to some metric. A major advantage of taking the information-theoretic approach is thatH(Y |X) is invariant under smooth, invertible transformations of the data (X). Therefore, any asymptotically unbiased estimator should converge to the same value regardless of the metric used for finding nearest neighbors."
    }, {
      "heading" : "A. Derivation of Eq. 5",
      "text" : "We have samples (x(i), y(i)), with x ∈ Rd and y ∈ {1, 2, . . . , l}, and we would like to evaluate Eq. 4 or Ĥ(Y |X). We define this estimator in terms of existing entropy estimators, Ĥ(Y |X) ≡ Ĥ(Y ) + Ĥ(X|Y )− Ĥ(X). The definition of the differential entropy estimators is given by Eq. 1.\nTo write down the standard plug-in estimator for discrete entropy, we first define nj ≡ ∑N i=1 δy(i),j .\nĤ(Y ) = − l∑\nj=1\nnj/N log(nj/N)\nNext we should unpack the second term, using the definition that ̄i,k denotes the distance to the k-th nearest neighbor to point x(i) that is in the same cluster.\nĤ(X|Y ) = l∑\nj=1\np(y = j)Ĥ(X|y = j)\n= l∑ j=1 nj/N ( log(nj/k) + ck,nj + d nj N∑ i=1 δy(i),j log ̄i,k )\nWe have used Eq. 1 in the second line. Next we expand and perform the sum over j for the last term only, eliminating the delta function.\n= l∑ j=1 nj/N ( log(nj/N) + logN/k + ck,nj ) + d N N∑ i=1 log ̄i,k = −Ĥ(Y ) + logN/k + l∑\nj=1\nck,njnj/N + d\nN N∑ i=1 log ̄i,k\nPutting this all together and using the nearest neighbor for estimation, or k = 1.\nĤ(Y |X) = Ĥ(Y ) + Ĥ(X|Y )− Ĥ(X)\n= logN + l∑ j=1 c1,njnj/N + d N N∑ i=1 log ̄i,1 − logN − c1,N − d N N∑ i=1 log i,1\n=  l∑ j=1 c1,njnj/N − c1,N + d N N∑ i=1 log ̄i,1 i,1\n≈ d N N∑ i=1 log ̄i,1 i,1\nThe constant quickly goes to zero for reasonable values of N and we neglect it. Recalling the definition, ck,N = ψ(N)− ψ(k) + log(2k/N). The constant term has the form ∑l j=1(ψ(nj) − log(nj))nj/N − (ψ(N) − log(N)). Using a wellknown expansion for the digamma function, ψ(n)− log(n) = −1/(2n)+O(1/(2n)2). Applying this first order expansion to our expression gives a single term (1− l)/N .\nThere is an ambiguity in applying our entropy estimator in Eq. 1 to some subset of points defined by a cluster if there are only k or fewer points in a cluster. Then we define, ̄i,k = i,N−1 if k > ny(i) + 1. This definition reflects the fact that our uncertainty is maximal if we are not given sufficient data. For instance, when k = 1, that means we have a cluster that contains a single sample point. In that case, we estimate the entropy of this single point as maximal with respect to the full dataset. In principle, the maximum length scale could be set by other prior information, but we strive to make data-driven choices whenever possible. In practice, this choice penalizes very small clusters, and the details of the penalty makes little difference in the outcome."
    }, {
      "heading" : "B. Heuristic Optimization",
      "text" : "The goal is to search for a natural coarse-graining, as defined in Eq. 7. The number of ways of partitioning N points into groups is very large and evaluating CV for any partitioning requires calculating all pairwise distances. Even calculating the change in CV from altering the cluster membership of a single point may require O(N) operations. Finally, the figures in Sec. 4 suggest that our optimization landscape is very rugged, so gradient-based methods are unlikely to succeed. Because of these difficulties, we suggest a heuristic optimization below.\nOur optimization proceeds first by generating a small number of candidate partitions that are likely to have small CV using a tractable semidefinite program. Then we rank the candidate partitions according to CVR.\nĤT (Y |X) = ∫ 1 0 dα Ĥα(Y |X) = d N N∑ i=1 N−1∑ k=1\n1\nk(k + 1) log ̄i,k i,k ,\nClearly, the contribution from terms coming from k-th nearest neighbors quickly decreases with k. If we ignore k ≥ kmax, then the objective is clearly minimized as long as the k-th nearest neighbors for point x(i) are all in the same cluster. We begin by relaxing our discrete cluster variable y(i) to be a continuous variable lying on a hypersphere, i.e., y(i) ∈ Rd′ and y(i) · y(i) = 1. If two points are close together in the x space, we want them to be close together in y space as well. We define a weighted adjacency matrix, Ai,j = 1/(k(k + 1)) if j is the k-th nearest neighbor to i.\nmin |y(i)|=1 ∑ i,j Ai,j(y (i) − y(j))2 = max |y(i)|=1 ∑ i,j Ai,jy (i) · y(j)\nThe optimal value of this optimization is for all y(i) to be equal. We need to add a term that forces all the y(i) as far apart as possible.\nmax |y(i)|=1 ∑ i,j (Ai,j − β)y(i) · y(j)\nWe represent the Gram matrix with components Mij = y(i) · y(j), and define Āi,j = Ai,j − β. Then our optimization takes the form:\nmax M is p.s.d.\nTrĀ •M, (8)\nwhere p.s.d. denotes that M is positive semidefinite, a necessary and sufficient condition for it to be a Gram matrix. This optimization is a semidefinite program and can be efficiently solved using convex optimization techniques. We used kmax = 10 and β = 1/(kmax(kmax + 1)) and made no effort to optimize these parameters.\nOnce the optimal M has been found, a discrete clustering can be found using the rounding method of Goemans and Williamson (Goemans & Williamson, 1995). First, taking the Cholesky decomposition of M recovers the vectors y(i). Now to recover a discrete partition (into two groups) from these vectors we pick a random unit vector u, and partition the data according to y(i) ≡ sign(u · y(i)). We generated 200 candidate partitions this way.\nNext, we calculate the CVR for each partition and pick the best (lowest) one. In the event we want multiple clusters, we combine the partition with the lowest CVR with each of the top 25 remaining candidate partitions and then we chose the one with the smallest overlap with the original partition (according to the Rand index). We continue this procedure until we have the desired number of clusters."
    } ],
    "references" : [ {
      "title" : "Weighted clustering",
      "author" : [ "Ackerman", "Margareta", "Ben-David", "Shai", "Branzei", "Simina", "Loker", "David" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Ackerman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ackerman et al\\.",
      "year" : 2012
    }, {
      "title" : "Robust information-theoretic clustering",
      "author" : [ "Böhm", "Christian", "Faloutsos", "Christos", "Pan", "Jia-Yu", "Plant", "Claudia" ],
      "venue" : "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Böhm et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Böhm et al\\.",
      "year" : 2006
    }, {
      "title" : "Collective phenomena and nonfinite state computation in a human social system",
      "author" : [ "DeDeo", "Simon" ],
      "venue" : "doi: 10.1371/journal.pone. 0075818",
      "citeRegEx" : "DeDeo and Simon.,? \\Q2012\\E",
      "shortCiteRegEx" : "DeDeo and Simon.",
      "year" : 2012
    }, {
      "title" : "Bootstrap methods for the empirical study of decision-making and information flows in social systems",
      "author" : [ "DeDeo", "Simon", "Hawkins", "Robert X. D", "Klingenstein", "Sara", "Hitchcock", "Tim" ],
      "venue" : "Entropy, 15(6):2246–2276,",
      "citeRegEx" : "DeDeo et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "DeDeo et al\\.",
      "year" : 2013
    }, {
      "title" : "Information theoretic clustering of sparse cooccurrence data",
      "author" : [ "Dhillon", "Inderjit S", "Guan", "Yuqiang" ],
      "venue" : "In Data Mining,",
      "citeRegEx" : "Dhillon et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Dhillon et al\\.",
      "year" : 2003
    }, {
      "title" : "Ica based on a smooth estimation of the differential entropy",
      "author" : [ "Faivishevsky", "Lev", "Goldberger", "Jacob" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Faivishevsky et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Faivishevsky et al\\.",
      "year" : 2008
    }, {
      "title" : "A nonparametric information theoretic clustering algorithm",
      "author" : [ "Faivishevsky", "Lev", "Goldberger", "Jacob" ],
      "venue" : "In Proceedings of ICML,",
      "citeRegEx" : "Faivishevsky et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Faivishevsky et al\\.",
      "year" : 2010
    }, {
      "title" : "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming",
      "author" : [ "M.X. Goemans", "D.P. Williamson" ],
      "venue" : "J. Assoc. Comput. Mach.,",
      "citeRegEx" : "Goemans and Williamson,? \\Q1995\\E",
      "shortCiteRegEx" : "Goemans and Williamson",
      "year" : 1995
    }, {
      "title" : "Information theoretic clustering",
      "author" : [ "Gokcay", "Erhan", "Principe", "Jose C" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Gokcay et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Gokcay et al\\.",
      "year" : 2002
    }, {
      "title" : "Data clustering: 50 years beyond k-means",
      "author" : [ "Jain", "Anil K" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "Jain and K.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jain and K.",
      "year" : 2010
    }, {
      "title" : "Sample estimate of the entropy of a random vector",
      "author" : [ "L.F. Kozachenko", "N.N. Leonenko" ],
      "venue" : "Probl. Peredachi Inf.,",
      "citeRegEx" : "Kozachenko and Leonenko,? \\Q1987\\E",
      "shortCiteRegEx" : "Kozachenko and Leonenko",
      "year" : 1987
    }, {
      "title" : "Estimating mutual information",
      "author" : [ "Kraskov", "Alexander", "Stögbauer", "Harald", "Grassberger", "Peter" ],
      "venue" : "Phys. Rev. E,",
      "citeRegEx" : "Kraskov et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Kraskov et al\\.",
      "year" : 2004
    }, {
      "title" : "Information theoretic clustering using minimum spanning trees",
      "author" : [ "Müller", "Andreas", "Nowozin", "Sebastian", "Lampert", "Christoph" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "Müller et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Müller et al\\.",
      "year" : 2012
    }, {
      "title" : "Objective criteria for the evaluation of clustering methods",
      "author" : [ "Rand", "William M" ],
      "venue" : "Journal of the American Statistical association,",
      "citeRegEx" : "Rand and M.,? \\Q1971\\E",
      "shortCiteRegEx" : "Rand and M.",
      "year" : 1971
    }, {
      "title" : "A mathematical theory of communication",
      "author" : [ "C.E. Shannon" ],
      "venue" : "The Bell System Technical Journal,",
      "citeRegEx" : "Shannon,? \\Q1948\\E",
      "shortCiteRegEx" : "Shannon",
      "year" : 1948
    }, {
      "title" : "Information-based clustering",
      "author" : [ "Slonim", "Noam", "Atwal", "Gurinder Singh", "Tkačik", "Gašper", "Bialek", "William" ],
      "venue" : "Proceedings of the National Academy of Sciences of the United States of America,",
      "citeRegEx" : "Slonim et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Slonim et al\\.",
      "year" : 2005
    }, {
      "title" : "Information-maximization clustering based on squared-loss mutual information",
      "author" : [ "Sugiyama", "Masashi", "Yamada", "Makoto", "Kimura", "Manabu", "Hachiya", "Hirotaka" ],
      "venue" : "arXiv preprint arXiv:1112.0611,",
      "citeRegEx" : "Sugiyama et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sugiyama et al\\.",
      "year" : 2011
    }, {
      "title" : "Information theoretical clustering via semidefinite programming",
      "author" : [ "Wang", "Meihong", "Sha", "Fei" ],
      "venue" : "AISTATS,",
      "citeRegEx" : "Wang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2011
    }, {
      "title" : "Universal estimation of information measures for analog sources",
      "author" : [ "Wang", "Qing", "Kulkarni", "Sanjeev R", "Verdú", "Sergio" ],
      "venue" : "Found. Trends Commun. Inf. Theory,",
      "citeRegEx" : "Wang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Ideally, we would like to achieve those goals without explicitly defining some notion of similarity between data points (Slonim et al., 2005) or defining “prototypical” clusters (Böhm et al.",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 1,
      "context" : ", 2005) or defining “prototypical” clusters (Böhm et al., 2006).",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "There has been a growing interest in information-theoretic clustering where we assign cluster labels to data points such that the mutual information between data and labels is maximized (Faivishevsky & Goldberger, 2010; Wang & Sha, 2011; Müller et al., 2012; Sugiyama et al., 2011).",
      "startOffset" : 186,
      "endOffset" : 281
    }, {
      "referenceID" : 16,
      "context" : "There has been a growing interest in information-theoretic clustering where we assign cluster labels to data points such that the mutual information between data and labels is maximized (Faivishevsky & Goldberger, 2010; Wang & Sha, 2011; Müller et al., 2012; Sugiyama et al., 2011).",
      "startOffset" : 186,
      "endOffset" : 281
    }, {
      "referenceID" : 11,
      "context" : "It can also be estimated non-parametrically from the data samples without defining a parametric space of probability distributions (Kraskov et al., 2004).",
      "startOffset" : 131,
      "endOffset" : 153
    }, {
      "referenceID" : 14,
      "context" : "We motivate our approach by appeal to the axiom of consistency under coarsegraining that forms the definition of entropy (Shannon, 1948).",
      "startOffset" : 121,
      "endOffset" : 136
    }, {
      "referenceID" : 11,
      "context" : "(Kraskov et al., 2004).",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 11,
      "context" : "The estimator has also been empirically shown to have good performance for small amounts of data, with k = 3 representing a good choice (Kraskov et al., 2004; Khan et al., 2007).",
      "startOffset" : 136,
      "endOffset" : 177
    }, {
      "referenceID" : 3,
      "context" : "discuss more nuanced alternatives in the discrete case (DeDeo et al., 2013).",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "How have so many papers achieved good clustering performance using this criteria (Wang & Sha, 2011; Faivishevsky & Goldberger, 2010; Müller et al., 2012)? To understand this result, it helps to write mutual information in the form I(X;Y ) = H(Y )−H(Y |X).",
      "startOffset" : 81,
      "endOffset" : 153
    }, {
      "referenceID" : 14,
      "context" : "Shannon’s Axiom of Consistency under Coarse-Graining Shannon’s original derivation of entropy (Shannon, 1948) begins with properties that a measure of uncertainty should be expected to obey and concludes that (Shannon) entropy is the unique measure that satisfies these properties.",
      "startOffset" : 94,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "make the further point that any estimator of entropy should at least approximately satisfy this property or it risks losing its meaning as an information-theoretic measure altogether (DeDeo et al., 2013).",
      "startOffset" : 183,
      "endOffset" : 203
    }, {
      "referenceID" : 11,
      "context" : "Note that the mutual information estimator we use is asymptotically unbiased (Kraskov et al., 2004), and so we expect a similar result for any mutual information estimator.",
      "startOffset" : 77,
      "endOffset" : 99
    } ],
    "year" : 2014,
    "abstractText" : "Greg Ver Steeg GREGV@ISI.EDU Aram Galstyan GALSTYAN@ISI.EDU Fei Sha FEISHA@USC.EDU Simon DeDeo SIMON.DEDEO@GMAIL.COM 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292, USA 2 University of Southern California, Los Angeles, CA 90089, USA 3 Santa Fe Institute, 1399 Hyde Park Rd., Santa Fe, NM 87501, USA 4 School of Informatics and Computing, Indiana University, 901 E 10th St., Bloomington, IN 47408, USA",
    "creator" : "LaTeX with hyperref package"
  }
}