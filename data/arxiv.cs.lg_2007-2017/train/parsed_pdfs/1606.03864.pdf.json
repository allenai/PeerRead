{
  "name" : "1606.03864.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Neural Associative Memory for Dual-Sequence Modeling",
    "authors" : [ "Dirk Weissenborn" ],
    "emails" : [ "dirk.weissenborn@dfki.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Dual-sequence modeling and sequence-tosequence modeling are important paradigms that are used in many applications involving natural language, including machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), recognizing textual entailment (Cheng et al., 2016; Rocktäschel et al., 2016; Wang and Jiang, 2016), auto-encoding (Li et al., 2015), syntactial parsing (Vinyals et al., 2015) or document-level question answering (Hermann et al., 2015). We might even argue that most, if not all, NLP problems can (at least partially) be modeled by this paradigm\n(Li and Hovy, 2015). These models operate on two distinct sequences, the source and the target sequence. Some tasks require the generation of the target based on the source (sequence-to-sequence modeling), e.g., machine translation, whereas other tasks involve making predictions about a given source and target sequence (dual-sequence modeling), e.g., recognizing textual entailment. Existing state-of-the-art, end-to-end differentiable models for both tasks exploit the same architectural ideas.\nThe ability of such models to carry information over long distances is a key enabling factor for their performance. Typically this can be achieved by employing recurrent neural networks (RNN) that convey information over time through an internal memory state. Most famous is the LSTM (Hochreiter and Schmidhuber, 1997) that accumulates information at every time step additively into its memory state, which avoids the problem of vanishing gradients that hindered previous RNN architectures from learning long range dependencies. For example, Sutskever et al. (2014) connected two LSTMs conditionally for machine translation where the memory state after processing the source was used as initialization for the memory state of the target LSTM. This very simple architecture achieved competitive results compared to existing, very elaborate and feature-rich models. However, learning the inherent long range dependencies between source and target requires extensive training on large datasets. Bahdanau et al. (2015) proposed an architecture that resolved this issue by allowing the model to attend over all positions in the source sentence when predicting the target sentence, which enabled the model to automatically learn alignments of words and phrases of the source with the target sentence. The important difference is that previous long range dependencies could be bridged directly via attention. However, this archiar X iv :1 60 6. 03 86 4v 1 [ cs .N E\n] 1\n3 Ju\nn 20\n16\ntecture requires a larger number of operations that scales with the product of the lengths of the sourceand target sequence and a memory that scales with the length of the source sequence.\nIn this work we introduce a novel architecture for dual-sequence modeling that is based on associative memories (AM). AMs are fixed sized memory arrays used to read and write content via an associated keys. Holographic Reduced Representations (HRR) (Plate, 1995)) enable the robust and efficient retrieval of previously written content from redundant memory arrays. Our approach is inspired by the works of Danihelka et al. (2016) who recently demonstrated the benefits of exchanging the memory cell of an LSTM with an associative memory on various sequence modeling tasks. In contrast to their architecture which directly adapts the LSTM architecture we propose an augmentation to generic RNNs (AM-RNNs, §3.2). Similar in spirit to Neural Turing Machines (Graves et al., 2014) we decouple the AM from the RNN and restrict the interaction with the AM to read and write operations which we believe to be important. Based on this architecture we derive the Dual AM-RNN (§4) that operates on two associative memories simultaneously for dual-sequence modeling. We conduct experiments on the task of recognizing textual entailment (§5). Our results and qualitative analysis demonstrate that AMs can be used to bridge long range dependencies similar to the attention mechanism while preserving the computational benefits of conveying information through a single, fixed-size memory state. Finally, an initial inspection into sequenceto-sequence modeling with Dual AM-RNNs shows that there are open problems that need to be resolved to make this approach applicable to these kinds of tasks.\nA TensorFlow (Abadi et al., 2015) implementation of (Dual)-AM RNNs can be found at https://github.com/ dirkweissenborn/dual_am_rnn."
    }, {
      "heading" : "2 Related Work",
      "text" : "Augmenting RNNs by the use of memory is not novel. Graves et al. (2014) introduced Neural Turing Machines which augment RNNs with external memory that can be written to and read from. It contains a predefined number of slots to write content to. This form of memory is addressable via content or position shifts. Neural Turing Machines inspired subsequent work on using different kinds\nof external memory, like queues or stacks (Grefenstette et al., 2015). Operations on these memories are calculated via a recurrent controller which is decoupled from the memory whereas AM-RNNs apply the RNN cell-function directly upon the content of the associative memory.\nDanihelka et al. (2016) introduced Associative LSTMs which extends standard LSTMs directly by reading and writing operations on an associative memory. This architecture is closely related to ours. However, there are crucial differences that are due to the fact that we decouple the associative array from the original cell-function. Danihelka et al. (2016) directly include operations on the AM in the definition of their Associative LSTM. This might cause problems, since some operations, e.g., forget, are directly applied to the entire memory array although this can affect all elements stored in the memory. We believe that only reading and writing operations with respect to a calculated key should be performed on the associative memory. Further operations should therefore only be applied on the stored elements.\nNeural attention is another important mechanism that realizes a form of content addressable memory. Most famously it has been applied to machine translation (MT) where attention models automatically learn soft word alignments between source and translation (Bahdanau et al., 2015). Attention requires memory that stores states of its individual entries, separately, e.g., states for every word in the source sentence of MT or textual entailment (Rocktäschel et al., 2016), or entire sentence states as in Sukhbaatar et al. (2015) which is an end-to-end memory network (Weston et al., 2015) for question answering. Attention weights are computed based on a provided input and the stored elements. The thereby weighted memory states are summed and the result is retrieved to be used as input to a down-stream neural network. Architectures based on attention require a larger amount of memory and a larger number of operations which scales with the usually dynamically growing memory. In contrast to attention Dual AM-RNNs utilize fixed size memories and a constant number of operations.\nAM-RNNs also have an interesting connection to LSTM-Networks (Cheng et al., 2016) which recently demonstrated impressive results on various text modeling tasks. LSTM-Networks (LSTMN) select a previous hidden state via attention on a memory tape of past states (intra-attention) op-\nposed to using the hidden state of the previous time step. The same idea is implicitly present in our architecture by retrieving a previous state via a computed key from the associative memory (Equation (6)). The main difference lies in the used memory architecture. We use a fixed size memory array in contrast to a dynamically growing memory tape which requires growing computational and memory resources. The drawback of our approach, however, is the potential loss of explicit memories due to retrieval noise or overwriting."
    }, {
      "heading" : "3 Associative Memory RNN",
      "text" : ""
    }, {
      "heading" : "3.1 Redundant Associative Memory",
      "text" : "In the following, we use the terminology of Danihelka et al. (2016) to introduce Redundant Associative Memories and Holographic Reduced Representations (HRR) (Plate, 1995). HRRs provide a mechanism to encode an item x with a key r that can be written to a fixed size memory arraym and that can be retrieved fromm via r.\nIn HRR, keys r and values x refer to complex vectors that consist of a real and imaginary part: r = rre + i · rim, x = xre + i · xim, where i is the imaginary unit. We represent these complex vectors as concatenations of their respective real and imaginary parts, e.g., r = [rre; rim]. The encoding- and retrieval-operation proposed by Plate (1995) and utilized by Danihelka et al. (2016) is the complex multiplication (Equation (1)) of a key r with its value x (encoding), and the complex conjugate of the key r = rre − i · rim with the memory (retrieval), respectively. Note, that this requires the modulus of the key to be equal to one, i.e., √ rre rre + rim rim = 1, such that r = r−1. Consider a single memory arraym containing N elements xk with respective keys rk (Equation (2)).\nr ~ x = [ rre xre − rim xim rre xim + rim xre ] (1)\nm = N∑ k=1 rk ~ xk (2)\nWe retrieve an element xk by multiplying rk\nwithm (Equation (3)).\nx̃k = rk ~m = N∑ k′=1 rk ~ rk′ ~ xk′\n= xk + N∑\nk′=16=k rk ~ rk′ ~ xk′\n= xk + noise (3)\nTo reduce noise Danihelka et al. (2016) introduce permuted, redundant copiesms ofm (Equation (4)). This results in decorrelated retrieval noises which effectively reduces the overall retrieval noise when computing their mean. Consider Nc permutations represented by permutation matrices Ps. The retrieval equation becomes the following.\nms = N∑ k=1 (Psrk)~ xk (4)\nx̃k = 1\nNc Nc∑ s=1 N∑ k′=1 (Psrk)~ms\n= xk + N∑ k′=16=k xk′ ~ 1 Nc Nc∑ s=1 Ps(rk ~ rk′)\n= xk + noise\nThe resulting retrieval noise becomes smaller because the mean of the permuted, complex key products tends towards zero with increasing Nc if the key dimensions are uncorrelated (see Danihelka et al. (2016) for more information)."
    }, {
      "heading" : "3.2 Augmenting RNNs with Associative Memory",
      "text" : "A recurrent neural network (RNN) can be defined by a parametrized cell-function fθ : RN × RM → RM × RH that is recurrently applied to an input sequence X = (x1, ...,xT ). At each time step t it emits an output ht and a state st, that is used as additional input in the following time step (Equation (5)).\nfθ(xt, st−1) = (st,ht)\nx ∈ RN , s ∈ RM , h ∈ RH (5)\nIn this work we augment RNNs, or more specifically their cell-function fθ, with associative memory to form Associative Memory RNNs (AM-RNN) f̃θ as follows. Let st = [ct;nt] be the concatenation of a memory state ct and, optionally, some\nremainder nt that might additionally be used in f , e.g., the output of an LSTM. For brevity, we neglect nt in the following, and thus st = ct. At first, we compute a key given the previous output and the current input, which is in turn used to read from the associative memory arraym to retrieve a memory state s for the specified key (Equation (6)).\nrt = bound ( Wr [ xt ht−1 ]) st−1 = rt ~mt−1 (6)\nThe bound-operation (Danihelka et al., 2016) (Equation (7)) guarantees that the modulus of rt is not greater than 1. This is an important necessity as mentioned in § 3.1.\nbound(r′) = [ r′re d r′im d ] (7)\nd = max ( 1, √ r′re r′re + r′im r′im )\nNext, we apply the original cell-function fθ to the retrieved memory state (Equation (8)) and the concatention of the current input and last output which serves as input to the internal RNN. We update the associative memory array with the updated state using the conjugate key of the retrieval key (Equation (9)).\nst,ht = fθ ([ xt ht−1 ] , st−1 ) (8)\nmt =mt−1 + rt ~ (st − st−1) f̃θ(xt,mt−1) = (mt,ht) (9)\nThe entire computation workflow is illustrated in Figure 1a."
    }, {
      "heading" : "4 Associative Memory RNNs for Dual Sequence Modeling",
      "text" : "Important NLP tasks such as machine translation (MT) or detecting textual entailment (TE) involve two distinct sequences as input, a source- and a target sequence. In MT a system predicts the target sequence based on the source whereas in TE source and target are given and an entailment-class should be predicted. Recently, both tasks were successfully modelled using an attention mechanism that can attend over positions in the source sentence at any time step in the target sentence (Bahdanau et al., 2015; Rocktäschel et al., 2016; Cheng et al.,\n2016). These models are able to learn important task specific correlations between words or phrases of the two sentences, like word/phrase translations, or word-/phrase-level entailments or contradictions. The success of these models is mainly due to the fact that long range dependencies can be bridged directly via attention, instead of keeping information over long distances in a memory state that can get overwritten.\nThe same can be achieved through associative memory. Given the correct key a state that was written at any time step in the source sentence can be retrieved from an AM with minor noise that can efficiently be reduced by redundancy. Therefore, AMs can bridge long range dependencies and can therefore be used as an alternative to attention. The trade-off for using an AM is that memorized states cannot be used for their retrieval. However, the retrieval operation is constant in time and memory whereas the computational and memory complexity of attention based architectures grow linearly with the length of the source sequence.\nWe propose two different architectures for solving dual sequence problems. Both approaches use at least one AM-RNN for processing the source and another for the target sequence. The first approach reads the source sequenceX = (x1, ...,xTx) and uses the final associative memory array mx(:= mxTx) to initialize the memory arraym y 0 =m\nx of the AM-RNN that processes the target sequence Y = (y1, ...,yTy). Note that this is basically the the conditional encoding architecture of Rocktäschel et al. (2016).\nThe second approach uses the final AM array of the source sequencemx in addition to an independent target AM arraymyt . At each time step t the Dual AM-RNN computes another key r′t that is used to read from mx and feeds the retrieved value as additional input to yt to the inner RNN of the target AM-RNN. These changes are reflected in the Equation (10) (compared to Equation (8)) and illustrated in Figure 1b.\nr′t = bound ( Wr′ [ yt hyt−1 ]) φt = r ′ t ~m x\nst,h y t = fθ  ythyt−1 φt  , st−1  (10)"
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Setup",
      "text" : "Dataset We conducted experiments on the Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) that consists of roughly 500k sentence pairs (premise-hypothesis). They are annotated with textual entailment labels. The task is to predict whether a premise entails, contradicts or is neutral to a given hypothesis.\nTraining We perform mini-batch (B = 50) stochastic gradient descent using ADAM (Kingma and Ba, 2015) with β1 = 0, β2 = 0.999 and an initial learning rate of 10−3 for small models (H ≈ 100) and 10−4 (H = 500) for our large model. The learning rate was halved whenever accuracy dropped over the period of one epoch. Performance on the development set was checked every 1000 mini-batches and the best model is used for testing. We employ dropout with a probability of 0.1 or 0.2 for the small and large models, respectively. Following Cheng et al. (2016), word embeddings are initialized with Glove (Pennington et al., 2014) or randomly for unknown words. Glove initialized embeddings are tuned only after an initial epoch through the training set.\nModel In this experiment we compare the traditional GRU with the (Dual) AM-GRU using conditional encoding (Rocktäschel et al., 2016) using\nshared parameters between source and target RNNs. Associative memory is implemented with 8 redundant memory copies. For the Dual AM-GRU we define r′t = rt (see § 4), i.e., we use the same key for interacting with the premise and hypothesis associative memory array while processing the hypothesis. The rationale behind this is that we want to retrieve text passages from the premise that are similar to text passages of the target sequence.\nAll of our models consist of 2 layers with a GRU as top-layer which is intended to summarize outputs of the bottom layer. The bottom layer corresponds to our different architectures. We concatenate the final output of the premise and hypothesis together with their absolute difference to form the final representation that is used as input to a two-layer perceptron with rectifier-activations for classification."
    }, {
      "heading" : "5.2 Results",
      "text" : "The results are presented in Table 1. They long range that the H=100-dimensional Dual AM-GRU and conditional AM-GRU outperform our baseline GRU system significantly. Especially the Dual AMGRU does very well on this task achieving 84.4% accuracy, which shows that it is important to utilize the associative memory of the premise separately for reading only. Most notably is that it achieves even better results than a comparable LSTM architecture with two-way attention between all premise\nand hypothesis words (LSTM-Attention). This indicates that our Dual AM-GRU architecture is at least able to perform similar or even better than an attention-based model in this setup.\nWe investigated this finding qualitatively from sampled examples by plotting heatmaps of cosine similarities between the content that has been written to memory at every time step in the premise and what has been retrieved from it while the Dual AMGRU processes the hypothesis. Random examples are shown in Figure 2, where we can see that the Dual AM-GRU is indeed able to retrieve the content from the premise memory that is most related with the respective hypothesis words, thus allowing to bridge important long-range dependencies for solving this task similar to attention. We observe that content for related words and phrases is retrieved from the premise memory when processing the hypothesis, e.g., “play\" and “video game\" or “artist\" and “sculptor\".\nIncreasing the size of the hidden dimension to 500 improves accuracy by another percentage point. The recently proposed LSTM Network achieves slightly better results. However, its number of operations scales with the square of the summed source and target sequence, which is even larger than traditional attention."
    }, {
      "heading" : "5.3 Sequence-to-Sequence Modeling",
      "text" : "End-to-end differentiable sequence-to-sequence models consist of an encoder that encodes the source sequence and a decoder which produces the target sequence based on the encoded source. In a preliminary experiment we applied the Dual AM-GRU without shared parameters to the task of auto-encoding where source- and target sequence\nare the same. Intuitively we would like the AMGRU to write phrase-level information with different keys to the associative memory. However, we found that the encoder AM-GRU learned very quickly to write everything with the same key to memory, which makes it work very similar to a standard RNN based encoder-decoder architecture where the encoder state is simply used to initialize the decoder state.\nThis finding is illustrated in Figure 3. The presented heatmap shows similarities between content that has been retrieved while predicting the target sequence and what has been written by the encoder to memory. We observe that the similarities between retrieved content and written content are horizontally slightly increasing, i.e., towards the end of the encoded source sentence. This indicates that the encoder overwrites the the associative memory while processing the source with the same key."
    }, {
      "heading" : "5.4 Discussion",
      "text" : "Our experiments on entailment show that the idea of using associative memory to bridge long term dependencies for dual-sequence modeling can work very well. However, this architecture is not naively transferable to the task of sequence-to-sequence modeling. We believe that the main difficulty lies in the computation of an appropriate key at every time step in the target sequence to retrieve related content. Furthermore, the encoder should be enforced to not always use the same key. For example, keys could be based on syntactical and semantical cues, which might ultimately result in capturing some form of Frame Semantics (Fillmore and Baker, 2001). This could facilitate decoding significantly. We believe that this might be achieved via\nregularization or by curriculum learning (Bengio et al., 2009)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We introduced the Dual AM-RNN, a recurrent neural architecture that operates on associative memories. The AM-RNN augments traditional RNNs generically with associative memory. The Dual AM-RNN extends AM-RNNs with a second readonly memory. Its ability to capture long range dependencies enables effective learning of dualsequence modeling tasks such as recognizing textual entailment. Our models achieve very competitive results and outperform a comparable attentionbased model while preserving constant computational and memory resources. Applying the Dual AM-RNN to a sequence-to-sequence modeling task revealed that the benefits of bridging long range dependencies cannot yet be achieved for this kind of problem. However, quantitative as well as qualitative results on textual entailment are very promising and therefore we believe that the Dual AM-RNN can be an important building block for NLP tasks involving two sequences."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Sebastian Krause, Tim Rocktäschel and Leonard Hennig for comments on an early draft of this work. This research was supported by the German Federal Ministry of Education and Research (BMBF) through the projects ALL SIDES (01IW14002), BBDC (01IS14013E), and Software Campus (01IS12050, sub-project GeNIE)."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng." ],
      "venue" : "Software available from",
      "citeRegEx" : "Vanhoucke et al\\.,? 2015",
      "shortCiteRegEx" : "Vanhoucke et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In The International Conference on Learning Representations (ICLR)",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Bengio et al.2009] Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston" ],
      "venue" : "In Proceedings of the 26th annual international conference on machine learning,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Gabor Angeli", "Christopher Potts", "Christopher D. Manning" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Bowman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memorynetworks for machine reading",
      "author" : [ "Cheng et al.2016] Jianpeng Cheng", "Li Dong", "Mirella Lapata" ],
      "venue" : "arXiv preprint arXiv:1601.06733",
      "citeRegEx" : "Cheng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2016
    }, {
      "title" : "Frame semantics for text understanding",
      "author" : [ "Fillmore", "Baker2001] Charles J Fillmore", "Collin F Baker" ],
      "venue" : "In Proceedings of WordNet and Other Lexical Resources Workshop,",
      "citeRegEx" : "Fillmore et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Fillmore et al\\.",
      "year" : 2001
    }, {
      "title" : "Learning to transduce with unbounded memory",
      "author" : [ "Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1819–1827.",
      "citeRegEx" : "Grefenstette et al\\.,? 2015",
      "shortCiteRegEx" : "Grefenstette et al\\.",
      "year" : 2015
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Tomáš Kočiský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Hermann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba" ],
      "venue" : "In The International Conference on Learning Representations (ICLR)",
      "citeRegEx" : "Kingma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2015
    }, {
      "title" : "The NLP engine: A universal turing machine for nlp",
      "author" : [ "Li", "Hovy2015] Jiwei Li", "Eduard Hovy" ],
      "venue" : "arXiv preprint arXiv:1503.00168",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "A hierarchical neural autoencoder for paragraphs and documents. In 53nd Annual Meeting of the Association for Computational Linguistics (ACL)",
      "author" : [ "Li et al.2015] Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Richard Socher", "Christopher D Manning" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Reasoning about entailment with neural attention",
      "author" : [ "Edward Grefenstette", "Karl Moritz Hermann", "Tomáš Kočiskỳ", "Phil Blunsom" ],
      "venue" : "The International Conference on Learning Representations (ICLR)",
      "citeRegEx" : "Rocktäschel et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rocktäschel et al\\.",
      "year" : 2016
    }, {
      "title" : "End-to-end memory networks",
      "author" : [ "Jason Weston", "Rob Fergus" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104–3112",
      "author" : [ "Oriol Vinyals", "Quoc V Le" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Łukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning natural language inference with lstm",
      "author" : [ "Wang", "Jiang2016] Shuohang Wang", "Jing Jiang" ],
      "venue" : "In Proceedings of the 2016 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Dual-sequence modeling and sequence-tosequence modeling are important paradigms that are used in many applications involving natural language, including machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), recognizing textual entailment (Cheng et al.",
      "startOffset" : 173,
      "endOffset" : 220
    }, {
      "referenceID" : 15,
      "context" : "Dual-sequence modeling and sequence-tosequence modeling are important paradigms that are used in many applications involving natural language, including machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), recognizing textual entailment (Cheng et al.",
      "startOffset" : 173,
      "endOffset" : 220
    }, {
      "referenceID" : 4,
      "context" : ", 2014), recognizing textual entailment (Cheng et al., 2016; Rocktäschel et al., 2016; Wang and Jiang, 2016), auto-encoding (Li et al.",
      "startOffset" : 40,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : ", 2014), recognizing textual entailment (Cheng et al., 2016; Rocktäschel et al., 2016; Wang and Jiang, 2016), auto-encoding (Li et al.",
      "startOffset" : 40,
      "endOffset" : 108
    }, {
      "referenceID" : 10,
      "context" : ", 2016; Wang and Jiang, 2016), auto-encoding (Li et al., 2015), syntactial parsing (Vinyals et al.",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : ", 2015), syntactial parsing (Vinyals et al., 2015) or document-level question answering (Hermann et al.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 7,
      "context" : ", 2015) or document-level question answering (Hermann et al., 2015).",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "For example, Sutskever et al. (2014) connected two LSTMs conditionally for machine translation where the memory state after processing the source was used as initialization for the memory state of the target LSTM.",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "Bahdanau et al. (2015) proposed an architecture that resolved this issue by allowing the model to attend over all positions in the source sentence when predicting the target sentence, which enabled the model to automatically learn alignments of words and phrases of the source with the target sentence.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "Neural Turing Machines inspired subsequent work on using different kinds of external memory, like queues or stacks (Grefenstette et al., 2015).",
      "startOffset" : 115,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "Most famously it has been applied to machine translation (MT) where attention models automatically learn soft word alignments between source and translation (Bahdanau et al., 2015).",
      "startOffset" : 157,
      "endOffset" : 180
    }, {
      "referenceID" : 13,
      "context" : ", states for every word in the source sentence of MT or textual entailment (Rocktäschel et al., 2016), or entire sentence states as",
      "startOffset" : 75,
      "endOffset" : 101
    }, {
      "referenceID" : 14,
      "context" : "in Sukhbaatar et al. (2015) which is an end-to-end memory network (Weston et al.",
      "startOffset" : 3,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "AM-RNNs also have an interesting connection to LSTM-Networks (Cheng et al., 2016) which recently demonstrated impressive results on various text modeling tasks.",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "Recently, both tasks were successfully modelled using an attention mechanism that can attend over positions in the source sentence at any time step in the target sentence (Bahdanau et al., 2015; Rocktäschel et al., 2016; Cheng et al., 2016).",
      "startOffset" : 171,
      "endOffset" : 240
    }, {
      "referenceID" : 13,
      "context" : "Recently, both tasks were successfully modelled using an attention mechanism that can attend over positions in the source sentence at any time step in the target sentence (Bahdanau et al., 2015; Rocktäschel et al., 2016; Cheng et al., 2016).",
      "startOffset" : 171,
      "endOffset" : 240
    }, {
      "referenceID" : 4,
      "context" : "Recently, both tasks were successfully modelled using an attention mechanism that can attend over positions in the source sentence at any time step in the target sentence (Bahdanau et al., 2015; Rocktäschel et al., 2016; Cheng et al., 2016).",
      "startOffset" : 171,
      "endOffset" : 240
    }, {
      "referenceID" : 13,
      "context" : "Note that this is basically the the conditional encoding architecture of Rocktäschel et al. (2016).",
      "startOffset" : 73,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "Dataset We conducted experiments on the Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) that consists of roughly 500k sentence pairs (premise-hypothesis).",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "(2016), word embeddings are initialized with Glove (Pennington et al., 2014) or randomly for unknown words.",
      "startOffset" : 51,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "Following Cheng et al. (2016), word embeddings are initialized with Glove (Pennington et al.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : "Model In this experiment we compare the traditional GRU with the (Dual) AM-GRU using conditional encoding (Rocktäschel et al., 2016) using shared parameters between source and target RNNs.",
      "startOffset" : 106,
      "endOffset" : 132
    }, {
      "referenceID" : 13,
      "context" : "LSTM (Rocktäschel et al., 2016) 116/252k 80.",
      "startOffset" : 5,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "9 LSTM shared (Rocktäschel et al., 2016) 159/252k 81.",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "4 LSTM-Attention (Rocktäschel et al., 2016) 100/252k 83.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "4 LSTM Network (Cheng et al., 2016) 450/3.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "regularization or by curriculum learning (Bengio et al., 2009).",
      "startOffset" : 41,
      "endOffset" : 62
    } ],
    "year" : 2017,
    "abstractText" : "Many important NLP problems can be posed as dual-sequence or sequence-tosequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling that is based on associative memory. We derive AM-RNNs, a recurrent associative memory (AM) which augments generic recurrent neural networks (RNN). This architecture is extended to the Dual AM-RNN which operates on two AMs at once. Our models achieve very competitive results on textual entailment. A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs. However, an initial experiment on autoencoding reveals that these benefits are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed.",
    "creator" : "LaTeX with hyperref package"
  }
}