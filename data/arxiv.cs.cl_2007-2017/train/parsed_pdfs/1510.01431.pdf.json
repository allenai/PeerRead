{
  "name" : "1510.01431.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SentiCap: Generating Image Descriptions with Sentiments",
    "authors" : [ "Alexander Mathews", "Lexing Xie", "Xuming He" ],
    "emails" : [ "alex.mathews@anu.edu.au,", "lexing.xie@anu.edu.au,", "xuming.he@nicta.com.au" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Automatically describing an image by generating a coherent sentence unifies two core challenges in artificial intelligence – vision and language. Despite being a difficult problem, the research community has recently made headway into this area, thanks to large labeled datasets, and progresses in learning expressive neural network models. In addition to composing a factual description about the objects, scene, and their interactions in an image, there are richer variations in language, often referred to as styles (Crystal and Davy 1969). Take emotion, for example, it is such a common phenomena in our day-to-day communications that over half of text accompanying online pictures contains an emoji (a graphical alphabet for emotions) (Instagram 2015). How well emotions are expressed and understood influences decision-making (Lerner et al. 2015) – from the mundane (e.g., making a restaurant menu appealing) to major (e.g., choosing a political leader in elections). Recognizing sentiment and opinions from written communications has been an active research topic for the past decade (Pang and Lee 2008; Socher et al. 2013), the synthesis of text with sentiment that is relevant to a given image is still an open prob-\nlem. In Figure 1, each image is described with a factual caption, and with positive or negative emotion, respectively. One may argue that the descriptions with sentiments are more likely to pique interest about the subject being pictured (the dog and the motocycle), or about their background settings (interaction with the dog at home, or how the motocycle came about).\nIn this paper, we describe a method, called SentiCap, to generate image captions with sentiments. We build upon the CNN+RNN (Convolution Neural Network + Recurrent Neural Network) recipe that has seen many recent successes (Donahue et al. 2015; Karpathy and Fei-Fei 2015; Mao et al. 2015; Vinyals et al. 2015; Xu et al. 2015a). In particular, we propose a switching Recurrent Neural Network (RNN) model to represent sentiments. This model consists of two parallel RNNs – one represents a general background language model; another specialises in descriptions with sentiments. We design a novel word-level regularizer, so as to emphasize the sentiment words during training and to optimally combine the two RNN streams (Section 3). We have gathered a new dataset of several thousand captions with positive and negative sentiments by re-writing factual descriptions (Section 4). Trained on 2000+ sentimental captions and 413K neutral captions, our switching RNN outperforms a range of heuristic and learned baselines in the number of emotional captions generated, and in a variety of subjective and human evaluation metrics. In particular SentiCap has the highest number of success in placing at least one sentiment word into the caption, 88% positive (or 72% negative) captions are perceived by crowd workers as more ar X iv :1 51 0.\n01 43\n1v 2\n[ cs\n.C V\n] 1\n3 D\nec 2\n01 5\npositive (or negative) than the factual caption, with a similar descriptiveness rating."
    }, {
      "heading" : "2 Related Work",
      "text" : "Recent advances in visual recognition have made “an image is a thousand words” much closer to reality, largely due to the advances in Convolutional Neural Networks (CNN) (Simonyan and Zisserman 2015; Szegedy et al. 2015). A related topic also advancing rapidly is image captioning, where most early systems were based on similarity retrieval using objects and attributes (Farhadi et al. 2010; Kulkarni et al. 2011; Hodosh, Young, and Hockenmaier 2013; Gupta, Verma, and Jawahar 2012), and assembling sentence fragments such as object-action-scene (Farhadi et al. 2010), subject-verb-object (Rohrbach et al. 2013), object-attributeprepositions (Kulkarni et al. 2011) or global image properties such as scene and lighting (Nwogu, Zhou, and Brown 2011). Recent systems model richer language structure, such as formulating a integer linear program to map visual elements to the parse tree of a sentence (Kuznetsova et al. 2014), or embedding (Xu et al. 2015b) video and compositional semantics into a joint space.\nWord-level language models such as RNNs (Mikolov et al. 2011; Sutskever, Martens, and Hinton 2011) and maximum-entropy (max-ent) language models (Mikolov et al. 2011) have improved with the aid of significantly larger datasets and more computing power. Several research teams independently proposed image captioning systems that combine CNN-based image representation and such language models. Fang et al. (2015) used a cascade of word detectors from images and a max-ent model. The Show and Tell (Vinyals et al. 2015) system used an RNN as the language model, seeded by CNN image features. Xu et al. (2015a) estimated spatial attention as a latent variable, to make the Show and Tell system aware of local image information. Karpathy and Li (2015) used an RNN to generate a sentence from the alignment between objects and words. Other work has employed multi-layer RNNs (Chen and Zitnick 2015; Donahue et al. 2015) for image captioning. Most RNN-based multimodal language models use the Long Short Term Memory (LSTM) unit that preserves longterm information and prevents overfitting (Hochreiter and Schmidhuber 1997). We adopt one of the competitive systems (Vinyals et al. 2015) – CNN+RNN with LSTM units as our basic multimodal sentence generation engine, due to its simplicity and computational efficiency.\nResearchers have modeled how an image is presented, and what kind of response it is likely to elicit from viewers, such as analyzing the aesthetics and emotion in images (Murray, Marchesotti, and Perronnin 2012; Joshi et al. 2011). More recently, the Visual SentiBank (Borth et al. 2013) system constructed a catalogue of Adjective-NounPairs (ANPs) that are frequently used to describe online images. We build upon Visual SentiBank to construct sentiment vocabulary, but to the best of our knowledge, no existing work tries to compose image descriptions with desired sentiments. Identifying sentiment in text is an active area of research (Pang and Lee 2008; Socher et al. 2013). Several teams (Nakagawa, Inui, and Kurohashi 2010;\nTäckström and McDonald 2011) designed sentence models with latent variables representing the sentiment. Our work focuses on generating sentences and not explicitly modelling sentiment using hidden variables."
    }, {
      "heading" : "3 Describing an Image with Sentiments",
      "text" : "Given an image I and its Dx-dimensional visual feature x ∈ RDx , our goal is to generate a sequence of words (i.e. a caption) Y = {y1, · · · ,yT } to describe the image with a specific style, such as expressing sentiment. Here yt ∈ {0, 1}V is 1-of-V encoded indicator vector for the tth word; V is the size of the vocabulary; and T is the length of the caption.\nWe assume that sentence generation involves two underlying mechanisms, one of which focuses on the factual description of the image while the other describes the image content with sentiments. We formulate such caption generation process using a switching multi-modal language model, which sequentially generates words in a sentence. Formally, we introduce a binary sentiment variable st ∈ {0, 1} for every word yt to indicate which mechanism is used. At each time step t, our model produces the probability of yt and the current sentiment variable st given the image feature x and the previous words y1:t−1, denoted by p(yt, st|x,y1:t−1). We generate the word probability by marginalizing over the sentiment variable st:\np(yt|x,y1:t−1) = ∑ st p(yt|st,x,y1:t−1)p(st|x,y1:t−1) (1)\nHere p(yt|st, ·) is the caption model conditioned on the sentiment variable and p(st|·) is the probability of the word sentiment. The rest of this section will introduce these components and model learning in detail."
    }, {
      "heading" : "3.1 Switching RNNs for Sentiment Captions",
      "text" : "We adopt a joint CNN+RNN architecture (Vinyals et al. 2015) in the conditional caption model. Our full model combines two CNN+RNNs running in parallel: one capturing the factual word generation (referred to as the background language model), the other specializing in words with sentiment. The full model is a switching RNN, in which the variable st functions as a switching gate. This model design aims to learn sentiments well, despite data sparsity – using only a small dataset of image description with sentiments (Section 4), with the help from millions of image-sentence pairs that factually describe pictures (Chen et al. 2015).\nEach RNN stream consists of a series of LSTM units. Formally, we denote the D-dimensional hidden state of an LSTM as ht ∈ RD, its memory cell as ct ∈ RD, the input, output, forget gates as it, ot, ft ∈ RD, respectively. Let k indicate which RNN stream it is, the LSTM can be implemented as:\nikt fkt okt gkt\n =  σσσ\ntanh\nTk (Ekyt−1 hkt−1 ) (2)\nckt = f k t ckt−1 + ikt gkt , hkt = okt ckt .\nHere σ(χ) is the sigmoid function 1/(1 + e−χ); tanh is the hyperbolic tangent function; Tk ∈ R4D×2D is a set of learned weights; gkt ∈ RD is the input to the memory cell; Ek ∈ RD×V is a learned embedding matrix in model k, and Ekyt is the embedding vector of the word yt.\nTo incorporate image information, we use an image representation x̂ = Wxx as the word embedding Ey0 when t = 1, where x is a high-dimensional image feature extracted from a convolutional neural network (Simonyan and Zisserman 2015), and Wx is a learned embedding matrix. Note that the LSTM hidden state hkt summarizes y1:t−1 and x. The conditional probability of the output caption words depends on the hidden state of the corresponding LSTM,\np(yt|st = k,x,y1:t−1) ∝ exp(Wkyhkt ) (3)\nwhere Wky ∈ RD×V is a set of learned output weights. The sentiment switching model generates the probability of switching between the two RNN streams at each time t, with a single layer network taking the hidden states of both RNNs as input:\np(st = 1|x,y1:t−1) = σ(Ws[h0t ;h1t ]) (4)\nwhere Ws is the weight matrix for the hidden states. An illustration of this sentiment switching model is in Figure 2. In summary, the parameter set for each RNN (k = {0, 1}) is Θk = {Tk,Wky ,Ek,Wkx}, and that of the switching RNN is Θ = Θ0 ∪ Θ1 ∪Ws. We have tried including x for learning p(st|x,y1:t−1) but found no benefit."
    }, {
      "heading" : "3.2 Learning the Switching RNN Model",
      "text" : "One of the key challenges is to design a learning scheme for p(st|x,y1:t−1) and two CNN+RNN components. We take a two-stage learning approach to estimate the parameters Θ in our switching RNN model based on a large dataset with factual captions and a small set with sentiment captions. Learning a background multi-modal RNN. We first train a CNN+RNN with a large dataset of image and caption pairs, denoted as D0 = {(xi0,yi0)}Ni=1. Θ0 are learned by minimizing the negative log-likelihood of the caption words given images,\nL0(Θ0,D0) = − ∑ i ∑ t log p(yi0,t|st = 0,xi0,yi0,1:t−1). (5)\nLearning from captions with sentiments. Based on the pre-trained CNN+RNN in Eq (5), we then learn the switching RNN using a small image caption dataset with a specific sentiment polarity, denoted as D = {(xi,yi, ηi)}Mi=1, M N . Here ηit ∈ [0, 1] is the sentiment strength of the tth word in the i-th training sentence, being either positive or negative as specified in the training data.\nWe design a new training objective function to use wordlevel sentiment information for learning Θ1 and the switching weights Ws, while keeping the pre-learned Θ0 fixed. For clarity, we denote the sentiment probability as:\nγ0t = p(st = 0|x,y1:t−1), γ1t = 1− γ0t ; (6)\nand the log likelihood of generating a new word yt given image and word histories (x,y1:t−1) as Lt(Θ,x,y), which can be written as (cf. Eq (1)),\nLt(Θ,x,y) = log p(yt|x,y1:t−1) = (7) log[γ0t p(yt|st = 0,x,y−t) + γ1t p(yt|st = 1,x,y−t)].\nThe overall learning objective function for incorporating word sentiment is a combination of a weighted log likelihood and the cross-entropy between γt and ηt,\nL(Θ,D) = − ∑ i ∑ t (1 + ληη i t)[Lt(Θ,x i,yi) (8)\n+ λγ(η i t log γ 1,i t + (1− ηit) log γ 0,i t )] +R(Θ),\nR(Θ) = λθ 2 ‖Θ1 −Θ0‖2 (9)\nwhere λη and λγ are weight parameters, and R(Θ) is the regularization term with weight parameter λθ. Intuitively, when ηt > 0, i.e. the training sentence encounters a sentiment word, the likelihood weighting factor ληηit increases the importance of Lt in the overall likelihood; at the same time, the cross-entropy term λγ(ηit log γ 1,i t + (1− ηit) log γ 0,i t ) encourage switching variable γ 1 t to be > 0, emphasizing the new model. The regularized training finds a trade-off between the data likelihood and L2 difference between the current and base RNN, and is one of the most competitive approaches in domain transfer (Schweikert et al. 2008). Settings for model learning. We use stochastic gradient descent with backpropagation on mini-batches to optimize the RNNs. We apply dropout to the input of each step, which is either the image embedding x̂ for t = 1 or the word embedding Ekyt−1 and the hidden output hkt−1 from time t − 1, for both the background and sentiment streams k = 0, 1.\nWe learn models for positive and negative sentiments separately, due to the observation that either sentiment could be valid for the majority of images (Section 4). We initialize Θ1 as Θ0 and use the following gradient of to minimize L(Θ,D) with respect to Θ1 and Ws, holding Θ0 fixed. ∂L ∂Θ =− ∑ i ∑ t (1 + ληη i t)[ ∂Lt ∂Θ\n+ λγ( ηit\nγ1,it\n∂γ1,it ∂Θ + 1− ηit γ0,it ∂γ0,it ∂Θ )] + ∂R(Θ) ∂Θ (10)\nHere ∂Lt∂Θ , ∂γ0,it ∂Θ , and ∂γ1,it ∂Θ are computed through differentiating across Equations (1)–(6). During training, we set ηt = 1 when word yt is part of an ANP with the target sentiment polarity, otherwise ηt = 0. We also include a default L2-norm regularization for neural network tuning |Θ|2 with a small weight (10−8). We automatically search for the hyperparameters λθ, λη and λγ on a validation set using Whetlab (Snoek, Larochelle, and Adams 2012)."
    }, {
      "heading" : "4 An Image Caption Dataset with Sentiments",
      "text" : "In order to learn the association between images and captions with sentiments, we build a novel dataset of imagecaption pairs where the caption both describes an image, and also convey the desired sentiment. We summarize the new dataset, and the crowd-sourcing task to collect imagesentiment caption data. More details of the data collection process are included in the suplementary1.\nThere are many ways a photo could evoke emotions. In this work, we focus on creating a collection and learning sentiments from an objective viewer who does not know the back story outside of the photo – a setting also used by recent collections of objectively descriptive image captions (Chen et al. 2015; Hodosh, Young, and Hockenmaier 2013). Dataset construction. We design a crowd-sourcing task to collect such objectively described emotional image captions. This is done in a caption re-writing task based upon objective captions from MSCOCO (Chen et al. 2015) by asking Amazon Mechanical Turk (AMT) workers to choose among ANPs of the desired sentiment, and incorporate one or more of them into any one of the five existing captions. Detailed design of the AMT task is in the appendix1.\nThe set of candidate ANPs required for this task is collected from the captions for a large sets of online images. We expand the Visual SentiBank (Borth et al. 2013) vocabulary with a set of ANPs from the YFCC100M image captions (Thomee et al. 2015) as the overlap between the original SentiBank ANPs and the MSCOCO images is insuffcient. We keep ANPs with non-trival frequency and a clear positive or negative sentiment, when rated in the same way as SentiBank. This gives us 1,027 ANPs with a positive emotion, 436 with negative emotions. We collect at least 3 positive and 3 negative captions per image. Figure 3(a) contains one example image and its respective positive and negative caption written by AMT workers. We release the list of ANPs and the captions in the online appendix1. Quality validation. We validate the quality of the resulting captions with another two-question AMT task as detailed in the suppliment1. This validation is done on 124 images with 3 neutral captions from MSCOCO, and images with 3 positive and 3 negative captions from our dataset. We first ask AMT workers to rate the descriptiveness of a caption for a given image on a four-point scale (Hodosh, Young, and Hockenmaier 2013; Vinyals et al. 2015). The descriptiveness column in Figure 3(b), shows that the measure for objective descriptiveness tend to decrease when the caption contains additional sentiment. Ratings for the positive captions (POS) have a small decrease (by 0.08, or one-tenth of\n1http://users.cecs.anu.edu.au/∼u4534172/senticap.html\nthe standard deviation), while those for the negative captions (NEG) have a significant decrease (by 0.73), likely because the notion of negativity is diverse.\nWe also ask whether the sentiment of the sentence matches the image. Each rating task is completed by 3 different AMT workers. In the correct sentiment column of Figure 3(b), we record the number of votes each caption received for bearing a sentiment that matches the image. We can see that the vast majority of the captions are unanimously considered emotionally appropriate (94%, or 315/335 for POS; 82%, or 250/305 for NEG). Among the captions with less than unanimous votes received, most of them (20 for POS and 49 for NEG) still have majority agreement for having the correct sentiment, which is on par with the level of noise (16 for COCO captions)."
    }, {
      "heading" : "5 Experiments",
      "text" : "Implementation details. We implement RNNs with LSTM units using the Theano package (Bastien et al. 2012). Our implementation of CNN+RNN reproduces caption generation performance in recent work (Karpathy and Fei-Fei 2015). The visual input to the switching RNN is 4096- dimensional feature vector from the second last layer of the Oxford VGG CNN (Simonyan and Zisserman 2015). These features are linearly embedded into a D = 512 dimensional space. Our word embeddings Ey are 512 dimensions and the hidden state h and memory cell c of the LSTM module also have 512 dimensions. The size of our vocabulary for generating sentences is 8,787, and becomes 8,811 after including additional sentiment words.\nWe train the model using Stochastic Gradient Descent (SGD) with mini-batching and the momentum update rule. Mini-batches of size 128 are used with a fixed momentum of 0.99 and a fixed learning rate of 0.001. Gradients are clipped to the range [−5, 5] for all weights during back-propagation. We use perplexity as our stopping criteria. The entire system has about 48 million parameters, and learning them on the sentiment dataset with our implementation takes about 20 minutes at 113 image-sentence pairs per second, while the original model on the MSCOCO dataset takes around 24 hours at 352 image-sentence pairs per second. Given a new image, we predict the best caption by doing a beam-search with beam-size 5 for the best words at each position. We implementd the system on a multicore workstation with an Nvidia K40 GPU. Dataset setup. The background RNN is learned on the MSCOCO training set (Chen et al. 2015) of 413K+ sentences on 82K+ images. We construct an additional set of caption with sentiments as described in Section 4 using images from the MSCOCO validation partition. The POS subset contains 2,873 positive sentences and 998 images for training, and another 2,019 sentences over 673 images for testing. The NEG subset contains 2,468 negative sentences and 997 images for training, and another 1,509 sentences over 503 images for testing. Each of the test images has three positive and/or three negative captions. Systems for comparison. The starting point of our model is the RNN with LSTM units and CNN input (Vinyals et al. 2015) learned on the MS COCO training set only, denoted as\nCNN+RNN. Two simple baselines ANP-Replace and ANPScoring use sentences generated by CNN+RNN and then add an adjective with strong sentiment to a random noun. ANPReplace adds the most common adjective, in the sentiment captions for the chosen noun. ANP-Scoring uses multi-class logistic regression to select the most likely adjective for the chosen noun, given the Oxford VGG features. The next model, denoted as RNN-Transfer, learns a fine-tuned RNN on the sentiment dataset with additional regularization from CNN+RNN (Schweikert et al. 2008), as inR(Θ) (cf. Eq (9)). We name the full switching RNN system as SentiCap, which jointly learns the RNN and the switching probability with word-level sentiments from Equation (8).\nEvaluation metrics. We evaluate our system both with automatic metrics and with crowd-sourced judgements through Amazon Mechanical Turk. Automatic evaluation uses the BLEU, ROUGEL, METEOR, CIDEr metrics from the Microsoft COCO evaluation software (Chen et al. 2015).\nIn our crowd-sourced evaluation task AMT workers are given an image and two automatically generated sentences displayed in a random order (example provided in supplement1). One sentence is from the CNN+RNN model without sentiment, while the other sentence is from SentiCap or one of the systems being compared. AMT workers are asked to rate the descriptiveness of each image from 1-4 and select the more positive or more negative image caption. A\nprocess for filtering out noisy ratings is described in the supplement1. Each pair of sentences is rated by three different AMT workers; at least two must agree that a sentence is more positive/negative for it to be counted as such. The descriptiveness score uses mean aggregation.\nResults. Table 1 summarizes the automatic and crowdsourced evaluations. We can see that CNN+RNN presents almost no sentiment ANPs as it is trained only on MSCOCO. SentiCap contains significantly more sentences with sentiment words than any of the three baseline methods, which is expected when the word-level regularization has taken effect. That SentiCap has more sentiment words than the two insertion baselines ANP-Replace and ANP-Scoring shows that SentiCap actively drives the flow of the sentence towards using sentimental ANPs. Sentences from SentiCap are, on average, judged by crowd sourced workers to have stronger sentiment than any of the three baselines. For positive SentiCap, 88.4% are judged to have a more positive sentiment than the CNN+RNN baseline. These gains are made with only a small reduction in the descriptiveness – yet this decrease is due to a minority of failure cases, since 84.6% of captions ranked favorably in the pair-wise descriptiveness comparison. SentiCap negative sentences are judged to have more negative sentiment 72.5% of the time. On the automatic metrics SentiCap generating negative captions outperforms all three baselines by a margin. This improvement is\nBackground color indicate the probability of the switching variable γ1t = p(st|·): da rk if γ1t ≥ 0.75; med ium if γ1t ≥ 0.5; lig ht if γ1t ≥ 0.25. Row 1 and 2 contain generally successful examples. Row 3 contains examples with various amounts of error in either semantics or sentiment, at times with amusing effects. See Section 5 for discussions.\nlikely due to negative SentiCap being able to learn more reliable statistics for the new words that only appear in negative ANPs.\nSentiCap sentences with positive sentiment were judged by AMT workers as more interesting than those without sentiment in 66.4% of cases, which shows that our method improves the expressiveness of the image captions. On the other hand, negative sentences were judged to be less interesting than those without sentiment in 63.2% of cases. This is mostly due to that negativity in the sentence naturally contradicts with being interesting, a positive sentiment.\nIt has been noted by (Vinyals et al. 2015) that RNN captioning methods tend to exactly reproduce sentences from the training set. Our SENTICAP method produces a larger fraction of novel sentences than an RNN trained on a single caption domain. A sentence is novel if there is no match in the MSCOCO training set or the sentiment caption dataset. Overall, SENTICAP produces 95.7% novel captions; while CNN+RNN, which was trained only on MSCOCO, produces 38.2% novel captions – higher than the 20% observed in (Vinyals et al. 2015).\nFigure 4 contains a number of examples with generated sentiment captions – the left half are positive, the right half negative. We can see that the switch variable captures almost all sentiment phrases, and some of the surrounding words (e.g. train station, plate). Examples in the first two rows are generally descriptive and accurate such as delicious piece of\ncake (2a), ugly car and abandoned buildings (1c). Results for the other examples contain more or less inappropriateness in either the content description or sentiment, or both. (3b) captures the happy spirit correctly, but the semantic of a child in playground is mistaken with that of a man on a skateboard due to very high visual resemblance. (3d) interestingly juxtaposed the positive ANP clever trick and negative ANP dead man, creating an impossible yet amusing caption."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We proposed SentiCap, a switching RNN model for generating image captions with sentiments. One novel feature of this model is a specialized word-level supervision scheme to effectively make use of a small amount of training data with sentiments. We also designed a crowd-sourced caption re-writing task to generate sentimental yet descriptive captions. We demonstrate the effectiveness of the proposed model using both automatic and crowd-sourced evaluations, with the SentiCap model able to generate an emotional caption for over 90% of the images, and the vast majority of the generated captions are rated as having the appropriate sentiment by crowd workers. Future work can include unified model for positive and negative sentiment; models for linguistic styles (including sentiments) beyond the word level, and designing generative models for a richer set of emotions such as pride, shame, anger.\nAcknowledgments NICTA is funded by the Australian Government as represented by the Dept. of Communications and the ARC through the ICT Centre of Excellence program. This work is also supported in part by the Australian Research Council via the Discovery Project program. The Tesla K40 used for this research was donated by the NVIDIA Corporation."
    }, {
      "heading" : "7 Appendix",
      "text" : "This appendix primarily provides extra details on the model and data collection process. This is included to enusre our results are easily reproducable and to clarify exactly how the data was collected.\nWe first provide additional details on the LSTM units used by our approach in Section 7.1. Section 7.2 discusses the differences between 1st, 2nd, and 3rd person sentiment. See Section 7.3 for a discussion of how the ANPs with sentiment where chosen. For details on rewriting sentences to incorporate ANPs see Section 7.4. Details on validating the rewritten sentences are in Section 7.5. The crowd sourced evaluation of generated sentences is described in Section 7.6."
    }, {
      "heading" : "7.1 The LSTM unit",
      "text" : "The LSTM units we have used are functionally the same as the units used by Vinyals et al. (2015). This differs from the LSTM unit used by Xu et al. (2015a) because we do not concatenate contextual information to the units input. A graphical representation of our LSTM units is shown in Figure 5; for a more complete definition see Equation 2 in the companion paper. In Figure 5, note that only the LSTM unit is shown, without the fully connected output layers or word embedding layers."
    }, {
      "heading" : "7.2 Sentimental descriptions in the first, second, and third person",
      "text" : "There are many ways a photo could evoke emotions, they can be referred to as sentiments from the first, second, and third person.\nA first person sentiment is for a photo to elicit the emotions of its owner / author / uploader, who then records such sentiment for personal organization or communication to others (Ames and Naaman 2007). Such as the Flickr photo titled “This is the best day ever”1, see Figure 6. The title\n1 https://www.flickr.com/photos/pixelmama/7612700314/\nand the caption describes a story but not the contents of the photo.\nA second person sentiment is expressed by someone whom the photo is communicated to,such as the comments “awesome” and “so sweet” for the photo above.\nThe third person sentiment is one expressed by an objective viewer, who has information about its visual content but does not know the backstory, such as describing the photo above as “Dreamy sunset by the sea”.\nIt will be difficult to learn the correct sentiments for the first or second person, since the computer lacks knowledge of the personal and communication context – to the extent that a change in context and assumptions could completely flip the polarity of the sentiment (See Figure 3). In this work, we focus on learning possible sentiments from the third person. We collect descriptions with sentiment by people who are asked to describe them – this setting is close to that of recent collections of subjectively descriptive image captions (Chen et al. 2015; Hodosh, Young, and Hockenmaier 2013)."
    }, {
      "heading" : "7.3 Customizing Visual Sentibank for captions",
      "text" : "Visual SentiBank (Borth et al. 2013) is a database of Adjective-Noun Pairs (ANP) that are frequently used to describe online images. We adopt its methodology to build the\nsentiment vocabulary. We take the title and the first sentence of the description from the YFCC100M dataset (Thomee et al. 2015), keep entries that are in English, tokenize, and obtain all ANPs that appear in at least 100 images. We score these ANPs using the average of SentiWordNet (Esuli and Sebastiani 2006) and SentiStrength (Thelwall et al. 2010), with the former being able to recognize common lexical variations and the latter designed to score short informal text. We keep ANPs that contain clear positive or negative sentiment, i.e., having an absolute score of 0.1 and above. We then take a union with the Visual SentiBank ANPs. This gives us 1,027 ANPs with a positive emotion, 436 with negative emotions. A full set of these ANPs are released online, along with sentences containing these ANPs written by AMT workers."
    }, {
      "heading" : "7.4 AMT interface for collecting image captions with sentiment",
      "text" : "We went through three design iterations for collecting relevant and succinct captions with the intended sentiment.\nOur first attempt was to invite workers from Amazon Mechanical Turk (AMT) to compose captions with either a positive or negative sentiment for an image – which resulted in overly long, imaginative captions. A typical example is: “A crappy picture embodies the total cliche of the photographer ’catching himself in the mirror,’ while it also includes a toobright bathroom, with blazing white walls, dark, unattractive, wood cabinets, lurking beneath a boring sink, holding an amber-colored bowl, that seems completely pointless, below the mirror, with its awkward teenage-composition of a door, showing inside a framed mirror (cheesy, forced perspective,) and a goofy-looking man with a camera.”\nWe then asked turkers to place ANPs into an existing caption, which resulted in rigid or linguistically awkward captions. Typical examples include: ”a bear that is inside of the great water” and ”a bear inside the beautiful water”.\nThese prompts us to design the following re-writing task: we take the available MSCOCO captions, perform tokenization and part-of-speech tagging, and identify nouns and their corresponding candidate ANPs. We provide ten candidate ANPs with the same sentiment polarity and asked AMT worker to rewrite any one of the original captions about the picture using at least one of the ANPs. The form that the AMT workers are shown is presented in Figure 7. We obtained three positive and three negative descriptions for each image, authored by different Turkers. As anecdotal evidence, several turkers emailed to say that this task is very interesting.\nThe instructions given to workers are shown in Figure 7. We based these instructions on those used by Chen et al. (2015) to construct the MSCOCO dataset. They were modified for brevity and to provide instruction on generating a sentence using the provided ANPs. We found that these instructions were clear to the majority of workers."
    }, {
      "heading" : "7.5 AMT interface validating image captions with sentiment",
      "text" : "The AMT validation interface, in Figure 8 was designed to determine what effect adding sentiment into the ground\ntruth captions effects their descriptiveness. Additionally we wanted to understand the fraction of images that could reasonably be described using either positive or negative sentiment. Each task presents the user with three MSCOCO captions and three positive or negative sentences, and asks users to rate them. Our four point descriptiveness scale is based on schemes used by other authors (Hodosh, Young, and Hockenmaier 2013; Vinyals et al. 2015)."
    }, {
      "heading" : "7.6 AMT interface for rating captions with a sentiment",
      "text" : "The AMT rating interface shown in Figure 9 was used to evaluate the performance of the four different methods. Each task consists of three different types of rating: most positive, most interesting and descriptiveness. The most positive and most interesting ratings are done pair-wise, comparing a sentence generated from one of the four meth-\nods to a sentence generated by CNN+RNN. The descriptiveness rating uses the same four point scale as the validation interface from Section 7.5. There are 5 images to rate per task; this is essential because of the way AMT calculates prices.\nWe found that asking Turkers to rate sentences using this method initially produced very poor results, with many Turkers selecting random options without reading the sentences. We suspect that in a number of cases bots were used to complete the tasks. Our first solution was to use more skilled Turkers, called masters workers, although this lead to cleaner results the smaller number of workers meant that a large batch of tasks took far too long to complete. Instead we used workers with a 95% or greater approval rating. To combat the quality issues we randomly interspersed the manual sentiment captions from our dataset, and then rejected all tasks from worker who failed to achieve 60% accuracy for\nthe most positive rating. This was found to be an effective way of filtering out the results. We note that there were very few cases where workers were close to the 60% accuracy cut-off, they were typically much higher or much lower than the threshold, this validates the idea that some workers were not completing the task correctly."
    } ],
    "references" : [ {
      "title" : "and Naaman",
      "author" : [ "M. Ames" ],
      "venue" : "M.",
      "citeRegEx" : "Ames and Naaman 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "I",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "Goodfellow" ],
      "venue" : "J.; Bergeron, A.; Bouchard, N.; and Bengio, Y.",
      "citeRegEx" : "Bastien et al. 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Large-scale visual sentiment ontology and detectors using adjective noun pairs. ACMMM",
      "author" : [ "Borth" ],
      "venue" : null,
      "citeRegEx" : "Borth,? \\Q2013\\E",
      "shortCiteRegEx" : "Borth",
      "year" : 2013
    }, {
      "title" : "C",
      "author" : [ "X. Chen", "Zitnick" ],
      "venue" : "L.",
      "citeRegEx" : "Chen and Zitnick 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "C",
      "author" : [ "X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Dollar", "Zitnick" ],
      "venue" : "L.",
      "citeRegEx" : "Chen et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and Davy",
      "author" : [ "D. Crystal" ],
      "venue" : "D.",
      "citeRegEx" : "Crystal and Davy 1969",
      "shortCiteRegEx" : null,
      "year" : 1969
    }, {
      "title" : "L",
      "author" : [ "Donahue, J.", "Hendricks" ],
      "venue" : "A.; Guadarrama, S.; Rohrbach, M.; Venugopalan, S.; Saenko, K.; and Darrell, T.",
      "citeRegEx" : "Donahue et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and Sebastiani",
      "author" : [ "A. Esuli" ],
      "venue" : "F.",
      "citeRegEx" : "Esuli and Sebastiani 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "J",
      "author" : [ "H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Dollar", "J. Gao", "X. He", "M. Mitchell", "Platt" ],
      "venue" : "C.; Lawrence Zitnick, C.; and Zweig, G.",
      "citeRegEx" : "Fang et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "M",
      "author" : [ "A. Farhadi", "M. Hejrati", "Sadeghi" ],
      "venue" : "A.; Young, P.; Rashtchian, C.; Hockenmaier, J.; and Forsyth, D.",
      "citeRegEx" : "Farhadi et al. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "C",
      "author" : [ "A. Gupta", "Y. Verma", "Jawahar" ],
      "venue" : "V.",
      "citeRegEx" : "Gupta. Verma. and Jawahar 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "and Schmidhuber",
      "author" : [ "S. Hochreiter" ],
      "venue" : "J.",
      "citeRegEx" : "Hochreiter and Schmidhuber 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Framing image description as a ranking task: Data, models and evaluation metrics. JAIR",
      "author" : [ "Young Hodosh", "M. Hockenmaier 2013] Hodosh", "P. Young", "J. Hockenmaier" ],
      "venue" : null,
      "citeRegEx" : "Hodosh et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hodosh et al\\.",
      "year" : 2013
    }, {
      "title" : "J",
      "author" : [ "D. Joshi", "R. Datta", "E. Fedorovskaya", "Q.-T. Luong", "Wang" ],
      "venue" : "Z.; Li, J.; and Luo, J.",
      "citeRegEx" : "Joshi et al. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "and Fei-Fei",
      "author" : [ "A. Karpathy" ],
      "venue" : "L.",
      "citeRegEx" : "Karpathy and Fei.Fei 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Baby talk: Understanding and generating simple image descriptions. CVPR’11",
      "author" : [ "Kulkarni" ],
      "venue" : null,
      "citeRegEx" : "Kulkarni,? \\Q2011\\E",
      "shortCiteRegEx" : "Kulkarni",
      "year" : 2011
    }, {
      "title" : "T",
      "author" : [ "P. Kuznetsova", "V. Ordonez", "Berg" ],
      "venue" : "L.; and Choi, Y.",
      "citeRegEx" : "Kuznetsova et al. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "K",
      "author" : [ "J.S. Lerner", "Y. Li", "P. Valdesolo", "Kassam" ],
      "venue" : "S.",
      "citeRegEx" : "Lerner et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN). ICLR’15",
      "author" : [ "Mao" ],
      "venue" : null,
      "citeRegEx" : "Mao,? \\Q2015\\E",
      "shortCiteRegEx" : "Mao",
      "year" : 2015
    }, {
      "title" : "Strategies for training large scale neural network language models. ASRU’11",
      "author" : [ "Mikolov" ],
      "venue" : null,
      "citeRegEx" : "Mikolov,? \\Q2011\\E",
      "shortCiteRegEx" : "Mikolov",
      "year" : 2011
    }, {
      "title" : "AVA: A large-scale database for aesthetic visual analysis. CVPR’12",
      "author" : [ "Marchesotti Murray", "N. Perronnin 2012] Murray", "L. Marchesotti", "F. Perronnin" ],
      "venue" : null,
      "citeRegEx" : "Murray et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Murray et al\\.",
      "year" : 2012
    }, {
      "title" : "Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables",
      "author" : [ "Inui Nakagawa", "T. Kurohashi 2010] Nakagawa", "K. Inui", "S. Kurohashi" ],
      "venue" : null,
      "citeRegEx" : "Nakagawa et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nakagawa et al\\.",
      "year" : 2010
    }, {
      "title" : "DISCO: Describing Images Using Scene Contexts and Objects. AAAI’11",
      "author" : [ "Zhou Nwogu", "I. Brown 2011] Nwogu", "Y. Zhou", "C. Brown" ],
      "venue" : null,
      "citeRegEx" : "Nwogu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Nwogu et al\\.",
      "year" : 2011
    }, {
      "title" : "and Lee",
      "author" : [ "B. Pang" ],
      "venue" : "L.",
      "citeRegEx" : "Pang and Lee 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Translating video content to natural language descriptions. ICCV’13",
      "author" : [ "Rohrbach" ],
      "venue" : null,
      "citeRegEx" : "Rohrbach,? \\Q2013\\E",
      "shortCiteRegEx" : "Rohrbach",
      "year" : 2013
    }, {
      "title" : "An empirical analysis of domain adaptation algorithms for genomic sequence analysis. NIPS’08",
      "author" : [ "Schweikert" ],
      "venue" : null,
      "citeRegEx" : "Schweikert,? \\Q2008\\E",
      "shortCiteRegEx" : "Schweikert",
      "year" : 2008
    }, {
      "title" : "and Zisserman",
      "author" : [ "K. Simonyan" ],
      "venue" : "A.",
      "citeRegEx" : "Simonyan and Zisserman 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "R",
      "author" : [ "J. Snoek", "H. Larochelle", "Adams" ],
      "venue" : "P.",
      "citeRegEx" : "Snoek. Larochelle. and Adams 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A",
      "author" : [ "R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "Ng" ],
      "venue" : "Y.; and Potts, C.",
      "citeRegEx" : "Socher et al. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "G",
      "author" : [ "I. Sutskever", "J. Martens", "Hinton" ],
      "venue" : "E.",
      "citeRegEx" : "Sutskever. Martens. and Hinton 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "and McDonald",
      "author" : [ "O. Täckström" ],
      "venue" : "R.",
      "citeRegEx" : "Täckström and McDonald 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Sentiment strength detection in short informal text. JASIST",
      "author" : [ "Thelwall" ],
      "venue" : null,
      "citeRegEx" : "Thelwall,? \\Q2010\\E",
      "shortCiteRegEx" : "Thelwall",
      "year" : 2010
    }, {
      "title" : "D",
      "author" : [ "Thomee, B.", "Shamma" ],
      "venue" : "A.; Friedland, G.; Elizalde, B.; Ni, K.; Poland, D.; Borth, D.; and Li, L.-J.",
      "citeRegEx" : "Thomee et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator. CVPR’15",
      "author" : [ "Vinyals" ],
      "venue" : null,
      "citeRegEx" : "Vinyals,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals",
      "year" : 2015
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Xu" ],
      "venue" : null,
      "citeRegEx" : "Xu,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu",
      "year" : 2015
    }, {
      "title" : "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework. AAAI’15",
      "author" : [ "Xu" ],
      "venue" : null,
      "citeRegEx" : "Xu,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu",
      "year" : 2015
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "The recent progress on image recognition and language modeling is making automatic description of image content a reality. However, stylized, non-factual aspects of the written description are missing from the current systems. One such style is descriptions with emotions, which is commonplace in everyday communication, and influences decision-making and interpersonal relationships. We design a system to describe an image with emotions, and present a model that automatically generates captions with positive or negative sentiments. We propose a novel switching recurrent neural network with word-level regularization, which is able to produce emotional image captions using only 2000+ training sentences containing sentiments. We evaluate the captions with different automatic and crowd-sourcing metrics. Our model compares favourably in common quality metrics for image captioning. In 84.6% of cases the generated positive captions were judged as being at least as descriptive as the factual captions. Of these positive captions 88% were confirmed by the crowd-sourced workers as having the appropriate sentiment.",
    "creator" : "LaTeX with hyperref package"
  }
}