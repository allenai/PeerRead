{
  "name" : "1702.07664.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Dipan K. Pal", "Marios Savvides" ],
    "emails" : [ "dipanp@cmu.edu", "msavvid@cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we theoretically address three fundamental problems involving deep convolutional networks regarding invariance, depth and hierarchy. We introduce the paradigm of Transformation Networks (TN) which are a direct generalization of Convolutional Networks (ConvNets). Theoretically, we show that TNs (and thereby ConvNets) are can be invariant to non-linear transformations of the input despite pooling over mere local translations. Our analysis provides clear insights into the increase in invariance with depth in these networks. Deeper networks are able to model much richer classes of transformations. We also find that a hierarchical architecture allows the network to generate invariance much more efficiently than a non-hierarchical network. Our results provide useful insight into these three fundamental problems in deep learning using ConvNets."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "It is a well known fact that deep Convolutional Networks (or ConvNets) LeCun et al. (1998) generate invariance to local translations due to convolutions followed by a form of pooling. In practice, however, studies such as Krizhevsky et al. (2012) have applied these models very successfully to domains such as vision, which typically involve data undergoing highly non-linear transformations. It is therefore clear, that these models can model invariance towards these global non-linear transformations despite solely employing pooling over local translations. Further, ? observed that a deeper ConvNet usually performs better (and thus is more invariant) on large scale tasks. This raises some fundamental questions.\nProblem 1: How does a ConvNet generate invariance to global non-linear transformations through pooling over mere local translations?\nProblem 2: How does invariance increase with depth in ConvNets?\nProblem 3: How does a hierarchical architecture help?\nThese have been long standing problems in vision since the inception of these networks. Intuitions and empirical observations abound, the problems still are not completely addressed from a theoretical standpoint.\nMain results: In this paper, we take a significant step towards answering these questions.\nAddressing Problem 1: We show that these non-linear invariances arise from the architecture of the network itself rather than the exact features learnt. More specifically, the entire pipeline of convolution followed by pooling and then a non-linearity itself contributes towards learning such powerful invariances. Although optimizing the features is important to capture the most amount of “information” and provide descriptive features, invariance strictly speaking, is not generated due to the features themselves. Instead, it is a by-product of the architecture. Our main result shows that a L layered ConvNet (and also a generalization of such architectures introduced as Transformation Networks or TNs), generates invariance to transformations h(x) of the input x of the form\nar X\niv :1\n70 2.\n07 66\n4v 1\n[ cs\n.C V\n] 2\n4 Fe\nb 20\n17\nh(x) = g1 ◦ η ◦ g2...η ◦ gL(x)1 where gi is a unitary transformation and η is a point-wise applied non-linearity satisfying certain conditions of unitarity and stability. A very good approximation of such a non-linearity is the hard-ReLU which is prevalent in practice Nair & Hinton (2010), thereby providing a theoretical justification of the same. The form of h(x) transformation highly non-linear. Even though unitary transforms include commonly known and “elementary” transforms such as translation and in-plane rotation, their composition with L − 1 non-linearities make the overall transformation very rich and powerful.\nAddressing Problem 2: Further, it immediately shows why depth is an important parameter in ConvNet architecture design. Increasing L in our model allows us to be invariant to a more expressive transformation form. Loosely speaking, each layer of the ConvNet can be said to generate invariance to one pair of gi and η. The precise form of h(·) depends on the exact hierarchy employed by the architecture and is discussed in more detail in a later section. The architecture of a ConvNet itself is a form of incorporating a prior on the kind of nuisance transformations expected to be observed in the data. This is complimentary to the regularization implications of weight sharing.\nAddressing Problem 3: We also show that the hierarchical nature of a ConvNet also helps in significantly improving efficiency in generating invariance. A L layered ConvNet reduces the number of required observations of transformed inputs for training from O(|G|L) to O(|G|), a reduction of the order L.\nIntuitive Proof Sketch: We first prove that each node at the first layer of a ConvNet (also Transformation Networks) generates invariances towards or factors out local translations (and more general unitary transforms for TNs). Then we put two conditions (unitarity and stability) on the point-wise non-linearity used in these networks such that transformations that were not factored out in the first layer are propagated to the second layer. We find that a the implicit mapping of a fractional degree polynomial kernel exactly satisfies unitarity and very closely approximates stability for a well chosen range of degrees. This function is also a very close approximation of the hard-ReLU non-linearity. The non-linearity helps preserve the group structure of the transformed inputs in the feature space. We finally show that every second layer node then is able to generate invariance to the left over transformations (not captured in the first layer) even if they had acted on the input after a non-linearity. This way the second layer node is overall invariant to a non-linear transformation of the input. As we go up passing through more layers, they add in abilities to be invariant to more non-linearities and complexities.\nPrior Art: Deep learning despite its great success in learning useful representations, has yet to have a very concrete theoretical foundation. Nonetheless, there have been many attempts at a deeper understanding of its mechanics. For instance, Kawaguchi (2016) proved important results for deep neural networks. Whereas Cohen et al. (2015); Haeffele & Vidal (2015) approached deep learning from the perspective of general tensor decompositions. All of these studies however, have focused on the supervised version of deep learning. Under supervision, theoretical results can be broadly described to be concerned with the optimality of a solution or properties of the optimization landscape. Given the success of supervised models, such an approach is definitely beneficial in advancing overall understanding. It however, considers architectures more general in nature since supervised results for specialized architectures are more difficult to obtain.\nUnsupervised deep learning however, promises to play an important role in the future not to mention kindling interests from a neoroscientific perspective. The analysis of our models is therefore aimed at the unsupervised setting and focuses more on the invariance properties of such networks. This reveals new insights into properties of the architecture itself and provides an explanation as to why increasing depth is useful on many fronts. Even though there have been theoretical efforts Delalleau & Bengio (2011); Martens & Medabalimi (2014) to provide results related to the “depth” of a network, the models studied do not immediately resemble the most successful architecture class in practice, ConvNets and its variants. We present results on a generalization of ConvNets called Transformation Networks (TN) which are directly applicable to ConvNets. In fact, TNs are very closely related to ConvNets and become identical under a very simple constraint.\nThere have been a few important efforts towards providing results from a unsupervised standpoint Anselmi et al. (2013); Mallat (2012). Mallat (2012) shows that local translation invariance leads to contractions in space. However, it is not clear whether those contractions are due to non-linear\n1 g ◦ η(x) = g(η(x))\ninvariances. Anselmi et al. (2013) approach the problem in a fashion more similar to ours with the use of unitary groups to “transfer” invariance. They show that for a hierarchical feed-forward network with unitary group structure, the features at top layers would be exactly invariant to groups of transformations acting over a larger receptive field. Our main result, on the other hand is more precise. We show that the top layer features is in fact invariant to non-linear transformations despite only pooling over linear transforms. Further, these non-linear transformations need not form a group overall. They are only required to form a group locally at every layer. The architecture we consider is very closely related to practical architectures used for ConvNets, whereas Anselmi et al. (2013) model the architecture utilizing simple and complex cell constructions from a more biologically motivated approach. Further, they hypothesize that the non-linearity serves as a way measuring bins of the CDF of an invariant distribution. On the other hand, we consider the non-linearity to be an integral part of the process to preserve unitary group structure in the feature space. This also leads to it being a part of the class or range of transformations to be invariant towards. In turn this observation leads to the critical result that the overall architecture is invariant to non-linear transformations despite pooling over linear transforms.\nFinally, Bruna et al. (2013); Paul & Venkatasubramanian (2014) also applied group theory to a certain extent to the problem of representation learning. These works provide useful insights into stabilization with groups. Here, stabilization is meant along the lines of resulting in a contraction or non-expansion of the space. Nonetheless, they do not explore exact invariance to explicitly nonlinear transforms as our study."
    }, {
      "heading" : "2 TRANSFORMATION NETWORKS",
      "text" : "We introduce the paradigm of Transformation Networks (TN), a more general way of looking at feed forward architectures such as ConvNets and present results on these and then apply them directly to ConvNets. We first briefly review the notion of unitary groups and group invariant functions.\nPremise and Notations: We denote images and general vectors by x ∈ Rd. Given such a x, we define a support set Λ which defines a subset of pixels or dimensions over x, i.e. xΛ defines the subset of pixels contained in the set of indices Λ arranged in a column. Given an image x, we consider it divided it into small non-overlapping regions covering the entire image. Each support set is denoted by Λli i.e. the ith support set at layer l as shown in Fig. 1. Λli is a union of certain Λ(l−1)j as defined by a hierarchy (say in a ConvNet). For instance in Fig. 1, Λ2 (shaded light blue) is the union of the supports Λ11,Λ12,Λ13,Λ14 in the image plane. This union of support is similar to the hierarchical structure observed in ConvNets and is defined by the specific architecture.\nUnitary-Group: A group G is a set of elements g ∈ G along with the properties of closure, associativity, invertibility and identity 2. A unitary group is any group whose elements are unitary in nature, i.e. the dot-product is preserved under the unitary transformation. More precisely, 〈g(x), g(y)〉 = 〈x, y〉 ∀x, y. g(x) denotes the action of the group element (or transformation) g on x. The action of a group can also be constrained by Λ. For instance, gΛ is a unitary transform acting only on the support set Λ. We express the action of a transform on a restricted support by gΛ(x).\nThe Unitary Non-linear Image Transformation Model: Unitary groups are very useful in modelling linear transformations in domains such as images. Indeed, translation and in-plane rotation can be modelled as unitary and expressed as g(x). However, coupled with a non-linearity η(·) and restricted support on the image Λ, unitary transforms can model a far richer class of images. Let X = {g(x) | g ∈ G} be the set of all transformations of x generated G. Now, for a given non-linearity η(·), consider a non-linear transformation as gΛ11gΛ12gΛ13gΛ14(η(gΛ2(x))). Here gΛ11gΛ12 apply the individual transforms over the specified support. Notice that gΛ11 ...gΛ14 are jointly unitary. This is because each gΛ1i is a unitary transformation over the support Λ1i, and ⋂ i Λ1i = 0, i.e. the supports are non-overlapping. Lastly, Λ2 is a union of Λ11..Λ14 and gΛ2 is a unitary transformation over a larger support. This expression of a non-linear transformation of x is more powerful than the simply linear g(x) primarily due to the non-linearity η, thereby allowing the modelling of much richer variation in data.\nTransformation Networks (TN): Transformation Networks (TN) are essentially feed forward networks that operate primarily on the principle of generating invariance towards a group or set of\n2We will mostly deal with continuous groups however, our results also hold for discrete groups.\ntransformations through pooling modelled as group integration. The architecture of of these networks are hierarchical in nature and they explicitly invoke invariances only locally and can potentially have multiple layers. In doing so, they implicitly can model global invariances. Consider a TN with L layers. Each layer has a number of TN nodes each with a receptive field size of (wl, hl), i.e. each cell or node in the layer can only look at patches of size wl×hl of the output from the previous layer. Every node at layer l can take in a number of input channels ol−1 from the previous layer, and output a number of channels ol to the next layer. Further, each node has a set of filters or templates Tli = {g(ti) | g ∈ Gli},∀i = 1, .., ol of size wl×hl. Here Gli is any unitary group specific to the ith output of the lth layer. We call Tli as a template set (henceforth to be assumed under some specified Gli). The template set simply a set of templates transformed under the action of Gli. Thus, there are ol such transformation blocks in layer l. Every node contains a pooling operation which performs group integration over the template set (essentially mean pooling). Further, there is a point-wise non-linearity applied to the pooled feature.\nTN Node: A TN node Υli (the ith node at layer l) provides a single dimensional feature given a patch x of size (wl, hl). The node output, for a given non-linearity η and input x, is given by\nΥli(x) = η( ∫ Gli 〈x, g(ti)〉dg) (1)\nu η( 1 |Gli| ∑ Gli 〈x, g(ti)〉) (2)\nHere, recall that Gli is a unitary group and ti is the template for that particular node. Note that Equation 2 models an average pooled ConvNet exactly for η being the hard-ReLU function and Gli being the translation group. However, the results for the TN node also hold for max pooling. Equation 2 is the version in which the group is a discrete finite group. All results also hold for the discrete case. Fig. 2 illustrates a single channeled TN node observing two support sets.\nLearnable Components in a Transformation Network: The only learnable parameters in a Transformation Network (after the architecture is finalized) are the sets of filters Tli ∀i in each l. However, each set has two components to be learnt. 1) The first is the template tli, the template for the ith node at layer l (analogous to a feature). 2) The second is the group Gli with which the template tli transforms. Note that once a single template tli is specified along with the corresponding group Gli, all transformed templates in the template set Tli are specified. Thus, contrary to convolutional\narchitectures which only learn the filters, Transformation Networks are required to learn both the transformations and the filters. Though main focus of this paper are the invariance properties of these networks, we briefly investigate how one could learn a Transformation Network.\nUnsupervised Learning of a Transformation Network: In the unsupervised setting, TNs can be trained in a greedy layer-by-layer fashion. The training data is passed through layer 1 of the TN to learn the templates tli and the corresponding transformation groups Gli at the same time. One simple way is to sample the transforming input sequence. Doing so specifies both the templates and the corresponding groups simultaneously. Unsupervised feature learning techniques such as ICA can also be applied. Once layer 1 is trained, layer 1 features can be extracted from the training data before being passed to layer 2 for training the second layer. This process can be repeated until all layers are trained.\nSupervised Learning of a Transformation Network: Under the supervised setting, one can assume that gradients are available. It is harder to train under this setting since the gradients need to update each template set or transformation block while keeping its group structure intact. One way of addressing this issue is to assume a particular group structure throughout the TN. This is the exact assumption that ConvNets make. ConvNets model all transformation groups in the network as the translation group which is parametric. The parametric nature allows one to compute the transformed template set on the fly. Thereby the only learnable parameters are the initial templates or filters tli. This brings us to the realization that a TN modelling general groups might model invariances better than a ConvNet, an observation we explore more in the following section. Nonetheless, our main result shows that ConvNets (and TNs in general) can in fact model non-linear invariances."
    }, {
      "heading" : "3 INVARIANCES IN A TRANSFORMATION NETWORK",
      "text" : ""
    }, {
      "heading" : "3.1 LINEAR UNITARY GROUP INVARIANCE IN SINGLE LAYER TRANSFORMATION NETWORKS",
      "text" : "We will show that a single layered TN, more specifically a single TN node, Υ(x) can be invariant to any unitary group G in the following sense. Definition 3.1 (G-Invariant Function). For any group G, we define a function f : X → Rn to be G-invariant if f(x) = f(g(x)) ∀x ∈ X ∀g ∈ G.\nAn invariant to any group G can be generated through the following (previously) known property utilizing group integration. This is a basic property of groups and arises due to the invariance of the Haar measure dg 3. Lemma 3.1. (Invariance Property) Given a vector x ∈ Rd, and any group G, for any fixed g′ ∈ G and a normalized Haar measure dg, the following is true g′ (∫ G g(x) ) dg = ∫ G g(x) dg\nOne layer TN is invariant to unitary transformation groups in the input space: Consider a TN with just a single layer of TN nodes. Each of these nodes looks at a patch of the same size. Each output feature of the network is given by Eq. 2, although to study the properties of such a construction, we will utilize Eq. 1. Utilizing Lemma 3.1 along with the definition of a TN node, we have the following. Lemma 3.2. (TN node linear Invariance) Under a unitary group G, under the action of which the filters or templates T of a TN node are transformed, the node output is invariant to the action of G on the input x, i.e. Υ(x) = Υ(g′(x)) ∀g′ ∈ G,∀x.\nThe proof is provided in the supplementary. This result shows that the TN node is invariant to local linear transformations (locality depending on the size of the receptive field). There are two main properties of the unitary group which allow for such invariance of the input. First, the group structure itself allows for invariant to be computed through group integration. Secondly, the unitary property of each element allows for the transformation to be “transferred” from the template t to the input x i.e. 〈x, g(t)〉 = 〈g−1(x), t〉. Thus, integrating over g(·) is equivalent whether we compute this over input or the template. Transformation Networks compute this integration over the pre-transformed templates, thereby computing an invariant feature of x even though it has never observed any other transformation of x. The unitarity of the transformations allows us to be invariant to the transformed versions of the input even though we might have never observed them in training.\nIn the following section, we show that under certain conditions that are very closely approximated in practice, exact invariance can be achieved to non-linear transformations of the input as well. This is a fundamental problem in generalized (supervised and unsupervised) deep learning. Specifically, how does a deep feed-forward network generate invariance to the highly non-linear transformations in data? Much of the attention for the answer to this question has gone to learnable features. We find that the inherent structure of the network itself (such as in ConvNets) is ideal to invoke invariance. In our group theoretic framework, these ”features” or filter weights would be the point from which the transformed filters are generated i.e. x in g(x)."
    }, {
      "heading" : "3.2 NON-LINEAR ACTIVATION IN TRANSFORMATION NETWORKS",
      "text" : "In the case of a TN with 2 or more layers, the non-linear activation function (under certain conditions) can help in generating invariance to non-linear transformations in the input space. In order to show this, we first show that under the unitary condition, such a non-linear activation can preserve the unitary group structure in the range space of the function i.e. the unitary transformation in the input domain of the non-linear activation function is also a corresponding albeit different unitary transformation in the range of the function. This unitary group structure is observed by TN nodes downstream (higher up the layers), which then through group integration to be able to generate invariance to the same utilizing Lemma 3.2.\nConditions on the non-linear activation function: We now state the conditions on the non-linear activation function η(·).\n1. Condition 1: (Unitarity) We define a function η : X → H to be a unitary function if, for a unitary group G, it satisfies 〈η(g(x)), η(g(y))〉 = 〈η(x), η(y)〉 ∀g ∈ G,∀x, y ∈ X .\n2. Condition 2: (Stability) We define a function η : X → H to be stable if η ◦ η(x) = η(η(x)) = η(x) ∀x ∈ X .\nMany functions prevalent in machine learning are unitary in the sense of Condition 1. One example is the class of polynomial kernels k(x, y) = (xT y + c)d. Since the kernel employs an actual dot-product, it is clear that the function is unitary. The activation function of interest η(·) in this\n3Proof in the supplementary.\ncase would be the non-linear implicit map that the kernel defines from the input space to the Reproducing Kernel Hilbert Space (RKHS) (i.e. k(x, y) = 〈η(x), η(y)〉). For an example of a function that is stable in the sense of Condition 2, we consider Rectified Linear Units or the hard ReLU activation function (max(0, x)) which is prevalent in deep learning Nair & Hinton (2010). Note that both conditions of unitarity (Condition 1) and stability (Condition 2) need to be satisfied by the activation function η(·). We find such a class of non-linear functions in the implicit kernel map of the polynomial kernel k(x, y) = 〈η(x), η(y)〉 = 〈x, y〉d with d strictly less than but close to 1 i.e. d → 1, d 6= 1. Although not prevalent, such kernels are valid Rossius et al. (1998). These functions exactly unitary and are approximately stable (d being arbitrarily close to 1 but not equal) for the range of values typical in activation functions. For the 1-D case with d = 0.9, η(x) = x0.9. Restricting the function to produce only real values, it rejects all negative values in its domain. This behavior is a very close approximation of the hard rectified linear unit i.e. max(0, x) as illustrated in Fig. 3.\nGroup structure is preserved in the range of η(·): One of our central results is that group invariance can be invoked through group integration in the non-linear feature space as well. This is the crux of the invariance generation capabilities of ConvNets and Transformation Networks in general.\nWe define an operator gη : η(x)→ η(g(x)) for any g ∈ G where G is unitary. gη is thus a mapping withinH (the range of η(·)). Under unitary G, we then have the following result. Theorem 3.3. (Covariance in the range of η(·)) If η(·) is a unitary function in the sense of Definition 1, then gη is unitary, and the set Gη = {gη | gη : η(x) → η(g(x)) ∀g ∈ G} is a unitary-group inH. This implies η(g(x)) = gη(η(x)) ∀x with gη being unitary.\nTheorem 3.3 shows that the unitary group transformation in the input space of a TN node can be expressed as a unitary group transformation in the range or feature space of the node 4. Since the group structure is preserved in the non-linear space, unitary group integration allows for transformation invariance of the input."
    }, {
      "heading" : "3.3 NON-LINEAR GROUP INVARIANCE IN MULTI-LAYER TRANSFORMATION NETWORKS",
      "text" : "Analysis of a two-layered TN: Consider a simple 2 layered TN with four TN nodes in layer 1 {Υ11,Υ12,Υ13,Υ14} each looking at non-overlapping patches of the raw input image. Let one TN node Υ2 at layer 2 be receiving the spatially concatenated input from layer 1 as shown in Fig. 1.\n4 Proof in the supplementary.\nLet all nodes be single channel nodes with the templates and their corresponding unitary groups as {(t11,G11), (t12,G12), (t13,G13), (t14,G14), (t2,G2)} 5. The transformed templates were learnt according to the unsupervised learning protocol or according to a supervised learning protocol which preserves the group structure of each template set. Since layer 1 TN nodes are already invariant to unitary groups {G11,G12,G13,G14}, the only transformation that layer 2 nodes might observe are the ones that are not captured by these groups. G2 is a transformation group that is unitary over the support Λ2 which has a receptive field that is a union of {Λ11,Λ12,Λ13,Λ14}. G2 is not necessarily unitary over the individual supports (receptive field defined by Λ1j) of layer 1 nodes. The output of the layer 1 nodes for a single channel with templates {t1i} are of the form Υ1i(xΛ1i) = η( ∫ G1i〈xΛ1i , g(t1i)〉dg) = η(I(xΛ1i)), where we replaced the group integral over the dot-product with I(·) to emphasize an invariant feature. The output for the layer 2 node Υ2 for a single channel with template t2 is as shown below.\nΥ2(x) = η (∫ G2 〈x, η(g(t2))〉dg ) Note that since the templates are learnt in an unsupervised fashion by passing in input through the previous layers, the transformed template t2 is of the form η(g(t2)). This non-linearity appears due to the non-linear activation function of the previous layer. Even in the case of learning by back-propagation, weights pass through multiple non-linearities resulting in similar forms for the templates. For a two layered TN network, the second layer TN node expression is of the following form.\nΥ2(o1) = η ∫ G2 〈  η(I(xΛ11))η(I(xΛ12))η(I(xΛ13)) η(I(xΛ14))  , η(g(t2))〉dg \nwhere o1 = [Υ11(xΛ11),Υ12(xΛ12),Υ13(xΛ13),Υ14(xΛ14)] T . Replacing the concatenated invariant feature vector [I(xΛ11), I(xΛ12), I(xΛ13), I(xΛ14)] T = x′Λ2 and applying the point-wise nonlinearity over the entire vector, the layer 2 feature output becomes,\nΥ2(η(x ′ Λ2)) = η (∫ G2 〈η(x′Λ2), η(g(t2))〉dg ) (3)\nHere the receptive field of x′Λ2 is the union of the receptive fields of the four Υ1i. Since layer 1 features are invariant to the respective groups, variation in x′Λ2 occurs only due to transformations that do not fall into the groups modelled by layer 1 nodes. However, in order for group integration to be applied at layer 2, the transformation needs to be propagated or be covariant in the layer 1 feature space. We express this formally through a property which was previously shown to be true for hierarchical architectures employing group integrals Anselmi et al. (2013).\nProperty 3.4. (Covariance in the TN node feature space) Given a unitary gΛ2 over Λ2 that is not modelled by the unitary groups in layer 1 i.e. {G11,G12,G13,G14}, ∃g′Λ2 s.t. I(gΛ2|Λ11(xΛ11))I(gΛ2|Λ12(xΛ12))I(gΛ2|Λ13(xΛ13))\nI(gΛ2|Λ14(xΛ14))  = g′Λ2(x′Λ2) where gΛa|Λb is the transformation gΛa restricted to the support Λb and [I(xΛ11), I(xΛ12), I(xΛ13), I(xΛ14)] T = x′Λ2\nThis property allows for a unitary transformation acting on the support Λ2 in the input space to have a corresponding action or effect in the feature space. For instance, if one considers an in-plane rotation over a 16 × 16 image, then a 2 × 2 pooling (which is essentially feature extraction with the identity template) of pixels still preserves the rotation to a large degree. Feature extraction with general templates will also preserve the transformation due to the linearity of the dot-product.\n5 Weight sharing would imply the same templates and groups. Our results are not affected by that constraint.\nApplying Property 3.4 to the layer 2 features, we have for any transformed g′Λ2(x ′ Λ2 ) having support over Λ2,\nΥ2(η ◦ g′Λ2(x ′ Λ2)) (4)\n= η (∫ G2 〈η ◦ g′Λ2(x ′ Λ2), η ◦ g(t2)〉dg ) (5)\n= η (∫ Gη2 〈g′ηΛ2 ◦ η(x ′ Λ2), gη ◦ η(t2)〉dgη ) (6)\n= η (∫ Gη2 〈η(x′Λ2), (g ′ ηΛ2) −1 ◦ gη ◦ η(t2)〉dgη ) (7)\n= η (∫ Gη2 〈η(x′Λ2), gη ◦ η(t2)〉dgη ) (8)\n= Υ2(η(x ′ Λ2)) (9)\nEquation 6 utilizes Theorem 3.3 and Equation 8 utilizes the fact that since the templates are considered to be modelled using the same transformation model as the raw input images due to training (thereby layer 2 always observes the same group structure (g′ηΛ2)\n−1 ∈ Gη2). This implies that (g′ηΛ2)\n−1gη forms a bijective mapping to some g′η ∈ Gη2 and thus the transformation results in a rotation of the group elements or a reordering of the group6. The group essentially is invariant and hence the group integral does not change. We therefore arrive at the layer 2 linear invariance expression, which is\nΥ2(η(gΛ2x ′ Λ2)) = Υ2(η(x ′ Λ2)) (10)\nHere x′Λ2 is simply the activation response or output of the layer 1 TN nodes. Intuitively, layer 2 TN node is invariant to linear transformations over the larger support Λ2 in the input space. The invariance expression of Υ2 however, is coupled with the non-linearity η from the previous layer (i.e. layer 1). This coupling is what allows the node to model more general non-linear invariances as we will see shortly. In order to highlight the invariance specifically, we rewrite the invariance expression for a general x and unitary group element g and replacing Υ2(η(·)) = Γ2(·) where with 2 denotes layer 2 node. Therefore, we have\nΓ2(g(x)) = Γ2(x)\nFurther, it is interesting to note that if x itself is a non-linear transformation of some x′Λ2 , i.e. x = η ◦ g′Λ2(x ′ Λ2\n), we then arrive at our main result. Theorem 3.5. (Two-layer TN node Non-linear Invariance) Under a unitary group GΛ2 acting on the support Λ2, the output of the second layer node Υ2(η(·)) = Γ2(·) covering the support Λ2, is invariant to the action or transformations of η ◦ GΛ2 on any input x′Λ2 , i.e.\nΓ2(x ′ Λ2) = Γ2(η ◦ g ′ Λ2(x ′ Λ2)) ∀g ′ Λ2 ∈ GΛ2 ,∀x ′ Λ2\nfor η(·) satisfying the conditions of unitarity and stability.\nProof. We have,\nΓ2(η ◦ g′Λ2(x ′ Λ2)) = Υ2(η ◦ η ◦ g ′ Λ2(x ′ Λ2)) (11)\n= Υ2(η ◦ g′Λ2(x ′ Λ2)) (12) = Υ2(η(x ′ Λ2)) = Γ2(x ′ Λ2) (13)\nThe second equality utilizes the stability property of η(·) whereas the third equality arises from the invariance of Υ(η(·)) as demonstrated in Equation 10.\nThis shows that a TN node at layer 2 is invariant to a non-linear transformation η ◦ g′Λ2 of any x ′ Λ2 over the support Λ2. Combining the invariance generated due to the first layer as well, the node\n6Here the identity element maps to (g′ηΛ2) −1 and g′ηΛ2 maps to the identity\noverall is invariant to the non-linear transformation gΛ11 ◦ gΛ12 ◦ gΛ13 ◦ gΛ14 ◦ η ◦ gΛ2(·). More specifically, the four layer 1 TN nodes are invariant to the first 4 group elements in the sequence and the second node is invariant to the last element combined with the non-linearity. This result can be extended to multiple layers directly.\nRich non-linear invariance in the case of a multi-layered TN: Our result can be naturally extended to multi-layered TNs. Consider a TN with L layers with kl non-overlapping receptive fields at layer l. The ith node at layer l observes a receptive field Λli. Assuming the last layer L’s receptive field covers the entire image, the Lth layer node is invariant to the following class of transformation GL = gΛ11 ◦ ... ◦ gΛ1k1 ◦ η ◦ gΛl1 ◦ ... ◦ gΛlkl ◦ η ◦ gΛL ◦ (·). One can rewrite the form as GL = gΛ1η ◦ ...gΛl ... ◦ η ◦ gΛL ◦ (·) where we collapse all unitary transforms in a layer into one variable. This class of transformations contains L − 1 non-linearities and is extremely rich. The structure of a TN itself along with unitary group modelling and a special class of non-linearities allow for generating invariance to such a large class of transformations of the input.\nHierarchy helps in efficient invariance generation: Consider the class of transformations h(x) = g1 ◦ η ◦ g2...η ◦ gL(x) that a L layered ConvNet is invariant to. Using a naive single layered approach to be invariant to h(x), one would need to generate all transforms modelled by h and integrate over them. If gi ∈ Gi for a finite group Gi with the cardinality |Gi|, then the size of all possible h(x) is of the form ∏L j=1 |Gj |. If all individual groups have the same cardinality |G|, then the number of transformations is of the order |G|L. However, with a hierarchical architecture that generates invariance to the individual groups at every layer, the machine only needs to integrate over |Gi| transforms at layer i. The total number of transforms needed to be integrated over becomes∑L j=1 |Gj |. Under the assumption that all groups are the same size, the total number becomes L|G|. This is a significant reduction from O(|G|L) to O(|G|), by an order of L. Even though deeper networks require more data to train well, they can generate invariance to more complicatedly transformed data more efficiently. Further, lower layers having a smaller receptive field helps since cardinality of the transformation groups acting on smaller sized input is lower than those for a larger sized input. This helps the network in factorizing transformations with smaller less complicated transformations before deadling with larger more complicated non-linear ones."
    }, {
      "heading" : "3.4 THE NEED FOR MULTIPLE TEMPLATES OR CHANNELS",
      "text" : "Up until now in our analysis, we have assumed that the TN nodes have a single channel or a single template. The feature at layer 2 and above was multi-dimensional merely due to the distinct support sets Λ over the image. Our results however extend naturally to multiple channels with multiple templates since we make no assumption regarding the relation between the templates. Anselmi et al. (2013) suggest the need for multiple templates as a way of better measuring the invariant probability distribution (over pixels) of a group of transformed images. Indeed, the quantity 〈g(x), t〉 ∀g ∈ G is a 1-D projection along t of the distribution of the set {g(x)} ∀g ∈ G. More the number of templates or channels, better the estimate of the probability distribution due to the Cramer-Wold’s theorem. This result also holds true for our framework since the dot-products in a TN are 1-D projections of the transformed data onto a TN node template. The reason that this probability distribution is important is because Anselmi et al. (2013) show that it itself is an invariant to the action of the group G. Therefore, moments of the distribution are also invariant including the first (leading to mean pooling) and infinite moment (leading to max pooling). Group integrals can be seen as measuring the first moment. Thus our results can be integrated with theirs almost seamlessly."
    }, {
      "heading" : "4 TOWARDS CONVOLUTIONAL ARCHITECTURES",
      "text" : "Our framework for Transformation Networks models the transforming templates in each TN node as unitary groups. In order to apply supervised learning or back-propagation to these architectures, one must address the crucial issue of maintaining group structure in the template sets while optimizing the templates themselves. If back-propagation is applied naively to all templates in a template set, assuming they start with a group structure intact, they will converge to the same template throughout the set and the group structure will be lost. One way of addressing this issue is to assume all groups in the TN to be identical and parametric. A parametric transformation that can be efficiently applied would allow us to explicitly generate the template set on the fly for pooling or group integration. In\ndoing so, back-propagation needs to only update one of the templates in each template set. This is because the group structure is explicitly maintained by applying the parametric transformations to that template to generate rest of the template set. ConvNets adopt this exact approach with the transformation of choice being translation since translations can be efficiently implemented during run-time as convolutions.\nOur results apply directly to ConvNets since they are simply TNs instantiated with the unitary groups being discrete translations. We therefore find that the architecture of a ConvNet itself allows it to be able to model non-linear transformations. The weight sharing property of ConvNets (leading to convolutions), originally meant for regularization or merely local translation invariance, therefore has a very powerful by-product of generating invariance to much more complicated non-linear transforms overall. Goodfellow et al. (2009) has studied the problem of visualizing and measuring these invariances generated by a ConvNet and provided empirical justifications for increasing depth. Recall that given a ConvNet with L layers, it is invariant to a class of transformations with at least L− 1 non-linearities. By increasing the depth of a ConvNet, we are essentially adding in a layer of non-lnearity in the class of transformations that the ConvNet can be invariant towards. This provides a theoretical justification for the well known fact that depth can improve performance of a ConvNet."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "We have shown that TNs (and thereby ConvNets) are can be invariant to non-linear transformations of the input despite pooling over mere local unitary transformations. We also showed that deeper networks are able to model much richer classes of transformations. Further, we find that a hierarchical architecture allows the network to generate invariance much more efficiently than a non-hierarchical network."
    }, {
      "heading" : "6 PROOFS OF THEORETICAL RESULTS",
      "text" : "All group theoretic resuts hold true for finite groups as well."
    }, {
      "heading" : "6.1 PROOF OF LEMMA 3.1",
      "text" : "Proof. We have, g′ (∫ G g(x) dg ) = ∫ G g′ ◦ g(x) dg = ∫ G g′′(x) dg′′ = ∫ G g(x) dg\nSince the normalized Haar measure is invariant, i.e. dg = dg′. Intuitively, g′ simply rearranges the group integral owing to elementary group properties."
    }, {
      "heading" : "6.2 PROOF OF LEMMA 3.2",
      "text" : "Proof. We have,\nΥ(g′(x)) = η (∫ G 〈g′(x), g(t)〉dg ) (14)\n= η (∫ G 〈x, g′−1(g(t))〉dg ) (15)\n= η (∫ G 〈x, g′′(t)〉dg′′ ) (16)\n= Υ(x) (17)\nEq. 15 uses the fact that g′ ∈ G is unitary. Eq. 16 showcases a change of variable. Since g′, g ∈ G, therefore g′′ ∈ G. Further dg = dg′′ since the Haar measure is unitary."
    }, {
      "heading" : "6.3 PROOF OF THEOREM 3.3",
      "text" : "Proof. We have 〈η(gx), η(gy)〉 = 〈η(x), η(y)〉 = 〈gηη(x), gηη(y)〉, since the function η is unitary. We define gηη(x) as the action or transformation of gη on η(x). This is one of the requirements of a unitary operator, however gη needs to be linear. Linearity of gη can be derived from the linearity of the inner product and its preservation under gη in η. For an arbitrary vector p and a scalar α, we have\n||αgη(p)− gη(αp)||2 (18) = 〈αgηp− gη(αp), αgηp− gη(αp)〉 (19) = ||αgη(p)||2 + ||gη(αp)||2 − 2〈αgη(p), gη(αp)〉 (20) = |α|||p||2 + ||αp||2 − 2α2〈p, p〉 = 0 (21)\nSimilarly for vectors p, q, we have ||gη(p+ q)− (gη(p) + gη(q))||2 = 0\nWe now prove that the set Gη is a group. We start with proving the closure property. We have for any fixed gη, g′η ∈ Gη\ngη(g ′ η(η(x))) = gη(η(g ′(x))) = η(g(g′(x))) = η(g′′(x)) = g′′η (η(x))\nSince g′′ ∈ G therefore g′′η ∈ Gη by definition. Also, gηg′η = g′′η and thus closure is established. Associativity, identity and inverse properties can be proved similarly. The set Gη = {gη | gη : η(x)→ η(gx) ∀g ∈ G} is therefore a unitary-group in η."
    } ],
    "references" : [ {
      "title" : "Unsupervised learning of invariant representations in hierarchical architectures",
      "author" : [ "Fabio Anselmi", "Joel Z Leibo", "Lorenzo Rosasco", "Jim Mutch", "Andrea Tacchetti", "Tomaso Poggio" ],
      "venue" : "arXiv preprint arXiv:1311.4158,",
      "citeRegEx" : "Anselmi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Anselmi et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning stable group invariant representations with convolutional networks",
      "author" : [ "Joan Bruna", "Arthur Szlam", "Yann LeCun" ],
      "venue" : "arXiv preprint arXiv:1301.3537,",
      "citeRegEx" : "Bruna et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bruna et al\\.",
      "year" : 2013
    }, {
      "title" : "On the expressive power of deep learning: A tensor analysis",
      "author" : [ "Nadav Cohen", "Or Sharir", "Amnon Shashua" ],
      "venue" : "CoRR, abs/1509.05009,",
      "citeRegEx" : "Cohen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2015
    }, {
      "title" : "Shallow vs. deep sum-product networks",
      "author" : [ "Olivier Delalleau", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Delalleau and Bengio.,? \\Q2011\\E",
      "shortCiteRegEx" : "Delalleau and Bengio.",
      "year" : 2011
    }, {
      "title" : "Measuring invariances in deep networks. In Advances in neural information processing",
      "author" : [ "Ian Goodfellow", "Honglak Lee", "Quoc V Le", "Andrew Saxe", "Andrew Y Ng" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2009
    }, {
      "title" : "Global optimality in tensor factorization, deep learning, and beyond",
      "author" : [ "Benjamin D. Haeffele", "René Vidal" ],
      "venue" : "CoRR, abs/1506.07540,",
      "citeRegEx" : "Haeffele and Vidal.,? \\Q2015\\E",
      "shortCiteRegEx" : "Haeffele and Vidal.",
      "year" : 2015
    }, {
      "title" : "Deep learning without poor local minima",
      "author" : [ "Kenji Kawaguchi" ],
      "venue" : "In Advances In Neural Information Processing Systems,",
      "citeRegEx" : "Kawaguchi.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kawaguchi.",
      "year" : 2016
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Group invariant scattering",
      "author" : [ "Stéphane Mallat" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "Mallat.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mallat.",
      "year" : 2012
    }, {
      "title" : "On the expressive efficiency of sum product networks",
      "author" : [ "James Martens", "Venkatesh Medabalimi" ],
      "venue" : "arXiv preprint arXiv:1411.7717,",
      "citeRegEx" : "Martens and Medabalimi.,? \\Q2014\\E",
      "shortCiteRegEx" : "Martens and Medabalimi.",
      "year" : 2014
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the 27th international conference on machine learning",
      "citeRegEx" : "Nair and Hinton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nair and Hinton.",
      "year" : 2010
    }, {
      "title" : "Why does deep learning work?-a perspective from group theory",
      "author" : [ "Arnab Paul", "Suresh Venkatasubramanian" ],
      "venue" : "arXiv preprint arXiv:1412.6621,",
      "citeRegEx" : "Paul and Venkatasubramanian.,? \\Q2014\\E",
      "shortCiteRegEx" : "Paul and Venkatasubramanian.",
      "year" : 2014
    }, {
      "title" : "A short note about the application of polynomial kernels with fractional degree in support vector learning",
      "author" : [ "Rolf Rossius", "Gérard Zenker", "Andreas Ittner", "Werner Dilger" ],
      "venue" : "In European Conference on Machine Learning,",
      "citeRegEx" : "Rossius et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Rossius et al\\.",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "It is a well known fact that deep Convolutional Networks (or ConvNets) LeCun et al. (1998) generate invariance to local translations due to convolutions followed by a form of pooling.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "In practice, however, studies such as Krizhevsky et al. (2012) have applied these models very successfully to domains such as vision, which typically involve data undergoing highly non-linear transformations.",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "For instance, Kawaguchi (2016) proved important results for deep neural networks.",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "Whereas Cohen et al. (2015); Haeffele & Vidal (2015) approached deep learning from the perspective of general tensor decompositions.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "Whereas Cohen et al. (2015); Haeffele & Vidal (2015) approached deep learning from the perspective of general tensor decompositions.",
      "startOffset" : 8,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "Whereas Cohen et al. (2015); Haeffele & Vidal (2015) approached deep learning from the perspective of general tensor decompositions. All of these studies however, have focused on the supervised version of deep learning. Under supervision, theoretical results can be broadly described to be concerned with the optimality of a solution or properties of the optimization landscape. Given the success of supervised models, such an approach is definitely beneficial in advancing overall understanding. It however, considers architectures more general in nature since supervised results for specialized architectures are more difficult to obtain. Unsupervised deep learning however, promises to play an important role in the future not to mention kindling interests from a neoroscientific perspective. The analysis of our models is therefore aimed at the unsupervised setting and focuses more on the invariance properties of such networks. This reveals new insights into properties of the architecture itself and provides an explanation as to why increasing depth is useful on many fronts. Even though there have been theoretical efforts Delalleau & Bengio (2011); Martens & Medabalimi (2014) to provide results related to the “depth” of a network, the models studied do not immediately resemble the most successful architecture class in practice, ConvNets and its variants.",
      "startOffset" : 8,
      "endOffset" : 1158
    }, {
      "referenceID" : 1,
      "context" : "Whereas Cohen et al. (2015); Haeffele & Vidal (2015) approached deep learning from the perspective of general tensor decompositions. All of these studies however, have focused on the supervised version of deep learning. Under supervision, theoretical results can be broadly described to be concerned with the optimality of a solution or properties of the optimization landscape. Given the success of supervised models, such an approach is definitely beneficial in advancing overall understanding. It however, considers architectures more general in nature since supervised results for specialized architectures are more difficult to obtain. Unsupervised deep learning however, promises to play an important role in the future not to mention kindling interests from a neoroscientific perspective. The analysis of our models is therefore aimed at the unsupervised setting and focuses more on the invariance properties of such networks. This reveals new insights into properties of the architecture itself and provides an explanation as to why increasing depth is useful on many fronts. Even though there have been theoretical efforts Delalleau & Bengio (2011); Martens & Medabalimi (2014) to provide results related to the “depth” of a network, the models studied do not immediately resemble the most successful architecture class in practice, ConvNets and its variants.",
      "startOffset" : 8,
      "endOffset" : 1187
    }, {
      "referenceID" : 0,
      "context" : "There have been a few important efforts towards providing results from a unsupervised standpoint Anselmi et al. (2013); Mallat (2012).",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "There have been a few important efforts towards providing results from a unsupervised standpoint Anselmi et al. (2013); Mallat (2012). Mallat (2012) shows that local translation invariance leads to contractions in space.",
      "startOffset" : 97,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "There have been a few important efforts towards providing results from a unsupervised standpoint Anselmi et al. (2013); Mallat (2012). Mallat (2012) shows that local translation invariance leads to contractions in space.",
      "startOffset" : 97,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) approach the problem in a fashion more similar to ours with the use of unitary groups to “transfer” invariance.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) approach the problem in a fashion more similar to ours with the use of unitary groups to “transfer” invariance. They show that for a hierarchical feed-forward network with unitary group structure, the features at top layers would be exactly invariant to groups of transformations acting over a larger receptive field. Our main result, on the other hand is more precise. We show that the top layer features is in fact invariant to non-linear transformations despite only pooling over linear transforms. Further, these non-linear transformations need not form a group overall. They are only required to form a group locally at every layer. The architecture we consider is very closely related to practical architectures used for ConvNets, whereas Anselmi et al. (2013) model the architecture utilizing simple and complex cell constructions from a more biologically motivated approach.",
      "startOffset" : 0,
      "endOffset" : 789
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) approach the problem in a fashion more similar to ours with the use of unitary groups to “transfer” invariance. They show that for a hierarchical feed-forward network with unitary group structure, the features at top layers would be exactly invariant to groups of transformations acting over a larger receptive field. Our main result, on the other hand is more precise. We show that the top layer features is in fact invariant to non-linear transformations despite only pooling over linear transforms. Further, these non-linear transformations need not form a group overall. They are only required to form a group locally at every layer. The architecture we consider is very closely related to practical architectures used for ConvNets, whereas Anselmi et al. (2013) model the architecture utilizing simple and complex cell constructions from a more biologically motivated approach. Further, they hypothesize that the non-linearity serves as a way measuring bins of the CDF of an invariant distribution. On the other hand, we consider the non-linearity to be an integral part of the process to preserve unitary group structure in the feature space. This also leads to it being a part of the class or range of transformations to be invariant towards. In turn this observation leads to the critical result that the overall architecture is invariant to non-linear transformations despite pooling over linear transforms. Finally, Bruna et al. (2013); Paul & Venkatasubramanian (2014) also applied group theory to a certain extent to the problem of representation learning.",
      "startOffset" : 0,
      "endOffset" : 1468
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) approach the problem in a fashion more similar to ours with the use of unitary groups to “transfer” invariance. They show that for a hierarchical feed-forward network with unitary group structure, the features at top layers would be exactly invariant to groups of transformations acting over a larger receptive field. Our main result, on the other hand is more precise. We show that the top layer features is in fact invariant to non-linear transformations despite only pooling over linear transforms. Further, these non-linear transformations need not form a group overall. They are only required to form a group locally at every layer. The architecture we consider is very closely related to practical architectures used for ConvNets, whereas Anselmi et al. (2013) model the architecture utilizing simple and complex cell constructions from a more biologically motivated approach. Further, they hypothesize that the non-linearity serves as a way measuring bins of the CDF of an invariant distribution. On the other hand, we consider the non-linearity to be an integral part of the process to preserve unitary group structure in the feature space. This also leads to it being a part of the class or range of transformations to be invariant towards. In turn this observation leads to the critical result that the overall architecture is invariant to non-linear transformations despite pooling over linear transforms. Finally, Bruna et al. (2013); Paul & Venkatasubramanian (2014) also applied group theory to a certain extent to the problem of representation learning.",
      "startOffset" : 0,
      "endOffset" : 1502
    }, {
      "referenceID" : 13,
      "context" : "Although not prevalent, such kernels are valid Rossius et al. (1998). These functions exactly unitary and are approximately stable (d being arbitrarily close to 1 but not equal) for the range of values typical in activation functions.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "We express this formally through a property which was previously shown to be true for hierarchical architectures employing group integrals Anselmi et al. (2013). Property 3.",
      "startOffset" : 139,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) suggest the need for multiple templates as a way of better measuring the invariant probability distribution (over pixels) of a group of transformed images.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Anselmi et al. (2013) suggest the need for multiple templates as a way of better measuring the invariant probability distribution (over pixels) of a group of transformed images. Indeed, the quantity 〈g(x), t〉 ∀g ∈ G is a 1-D projection along t of the distribution of the set {g(x)} ∀g ∈ G. More the number of templates or channels, better the estimate of the probability distribution due to the Cramer-Wold’s theorem. This result also holds true for our framework since the dot-products in a TN are 1-D projections of the transformed data onto a TN node template. The reason that this probability distribution is important is because Anselmi et al. (2013) show that it itself is an invariant to the action of the group G.",
      "startOffset" : 0,
      "endOffset" : 656
    }, {
      "referenceID" : 4,
      "context" : "Goodfellow et al. (2009) has studied the problem of visualizing and measuring these invariances generated by a ConvNet and provided empirical justifications for increasing depth.",
      "startOffset" : 0,
      "endOffset" : 25
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we theoretically address three fundamental problems involving deep convolutional networks regarding invariance, depth and hierarchy. We introduce the paradigm of Transformation Networks (TN) which are a direct generalization of Convolutional Networks (ConvNets). Theoretically, we show that TNs (and thereby ConvNets) are can be invariant to non-linear transformations of the input despite pooling over mere local translations. Our analysis provides clear insights into the increase in invariance with depth in these networks. Deeper networks are able to model much richer classes of transformations. We also find that a hierarchical architecture allows the network to generate invariance much more efficiently than a non-hierarchical network. Our results provide useful insight into these three fundamental problems in deep learning using ConvNets.",
    "creator" : "LaTeX with hyperref package"
  }
}