{
  "name" : "1707.07719.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Global Normalization of Convolutional Neural Networks for Joint Entity and Relation Classification",
    "authors" : [ "Heike Adel" ],
    "emails" : [ "heike@cis.lmu.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Named entity classification (EC) and relation extraction (RE) are important topics in natural language processing. They are relevant, e.g., for populating knowledge bases or answering questions from text, such as “Where does X live?”\nMost approaches consider the two tasks independent from each other or treat them as a sequential pipeline by first applying a named entity recognition tool and then classifying relations between entity pairs. However, named entity types and relations are often mutually dependent. If the types of entities are known, the search space of possible relations between them can be reduced and vice versa. This can help, for example, to resolve ambiguities, such as in the case of “Mercedes”, which can be a person, organization and location. However, knowing that in the given context, it is the second argument for the relation “live in” helps concluding that it is a location. Therefore, we propose a single neural network (NN) for both tasks. In contrast to joint training and multitask learning, which calculate taskwise costs, we propose to learn a joint classification layer which is globally normalized on the outputs of both tasks. In particular, we train the NN parameters based on the loss of a linear-chain\nconditional random field (CRF) (Lafferty et al., 2001). CRF layers for NNs have been introduced for token-labeling tasks like named entity recognition (NER) or part-of-speech tagging (Collobert et al., 2011; Lample et al., 2016; Andor et al., 2016). Instead of labeling each input token as in previous work, we model the joint entity and relation classification problem as a sequence of length three for the CRF layer. In particular, we identify the types of two candidate entities (words or short phrases) given a sentence (we call this entity classification to distinguish it from the token-labeling task NER) as well as the relation between them. To the best of our knowledge, this architecture for combining entity and relation classification in a single neural network is novel. Figure 1 shows an example of how we model the task: For each sentence, candidate entities are identified. Every possible combination of candidate entities (query entity pair) then forms the input to our model which predicts the classes for the two query entities as well as for the relation between them. ar X iv :1 70 7. 07 71\n9v 1\n[ cs\n.C L\n] 2\n4 Ju\nl 2 01\n7\nTo sum up, our contributions are as follows: We introduce globally normalized convolutional neural networks for a sentence classification task. In particular, we present an architecture which allows us to model joint entity and relation classification with a single neural network and classify entities and relations at the same time, normalizing their scores globally. Our experiments confirm that a CNN with a CRF output layer outperforms a CNN with locally normalized softmax layers. Our source code is available at http://cistern.cis.lmu.de."
    }, {
      "heading" : "2 Related Work",
      "text" : "Some work on joint entity and relation classification uses distant supervision for building their own datasets, e.g., (Yao et al., 2010; Yaghoobzadeh et al., 2016). Other studies, which are described in more detail in the following, use the “entity and relation recognition” (ERR) dataset from (Roth and Yih, 2004, 2007) as we do in this paper. Roth and Yih (2004) develop constraints and use linear programming to globally normalize entity types and relations. Giuliano et al. (2007) use entity type information for relation extraction but do not train both tasks jointly. Kate and Mooney (2010) train task-specific support vector machines and develop a card-pyramid parsing algorithm to jointly model both tasks. Miwa and Sasaki (2014) use the same dataset but model the tasks as a table filling problem (see Section 4.2). Their model uses both a local and a global scoring function. Recently, Gupta et al. (2016) apply recurrent neural networks to fill the table. They train them in a multitask fashion. Previous work also uses a variety of linguistic features, such as part-of-speech tags. In contrast, we use convolutional neural networks and only word embeddings as input. Furthermore, we are the first to adopt global normalization of neural networks for this task.\nSeveral studies propose different variants of non-neural CRF models for information extraction tasks but model them as token-labeling problems (Sutton and McCallum, 2006; Sarawagi et al., 2004; Culotta et al., 2006; Zhu et al., 2005; Peng and McCallum, 2006). In contrast, we propose a simpler linear-chain CRF model which directly connects entity and relation classes instead of assigning a label to each token of the input sequence. This is more similar to the factor graph by Yao et al. (2010) but computationally simpler. Xu and\nSarikaya (2013) also apply a CRF layer on top of continuous representations obtained by a CNN. However, they use it for a token labeling task (semantic slot filling) while we apply the model to a sentence classification task, motivated by the fact that a CNN creates single representations for whole phrases or sentences."
    }, {
      "heading" : "3 Model",
      "text" : ""
    }, {
      "heading" : "3.1 Modeling Context and Entities",
      "text" : "Figure 2 illustrates our model.\nInput. Given an input sentence and two query entities, our model identifies the types of the entities and the relation between them; see Figure 1. The input tokens are represented by word embeddings trained on Wikipedia with word2vec (Mikolov et al., 2013). For identifying the class of an entity ek, the model uses the context to its left, the words constituting ek and the context to its right. For classifying the relation between two entities ei and ej , the sentence is split into six parts: left of ei, ei, right of ei, left of ej , ej , right of ej .1 For the example sentence in Figure 1 and the entity pair (“Anderson”, “chief”), the context split is: [] [Anderson] [, 41 , was the chief Middle ...] [Anderson , 41 , was the] [chief] [Middle East correspondent for ...]\nSentence Representation. For representing the different parts of the input sentence, we use convolutional neural networks (CNNs). CNNs are suitable for RE since a relation is usually expressed by the semantics of a whole phrase or sentence. Moreover, they have proven effective for RE in previous work (Vu et al., 2016). We train one CNN layer for convolving the entities and one for the contexts. Using two CNN layers instead of one gives our model more flexibility. Since entities are usually shorter than contexts, the filter width for entities can be smaller than for contexts. Furthermore, this architecture simplifies changing the entity representation from words to characters in future work.\nAfter convolution, we apply k-max pooling for both the entities and the contexts and concatenate the results. The concatenated vector cz ∈ RCz , z ∈ {EC,RE} is forwarded to a task-specific hidden layer of size Hz which learns patterns across the different input parts:\nhz = tanh(V T z cz + bz) (1)\n1The ERR dataset we use provides boundaries for entities to concentrate on the classification task (Roth and Yih, 2004).\nwith weights Vz ∈ RCz×Hz and bias bz ∈ RHz ."
    }, {
      "heading" : "3.2 Global Normalization Layer",
      "text" : "For global normalization, we adopt the linearchain CRF layer by Lample et al. (2016).2 It expects scores for the different classes as input. Therefore, we apply a linear layer first which maps the representations hz ∈ RHz to a vector vz of the size of the output classes N = NEC +NRE :\nvz =W T z hz (2)\nwith Wz ∈ RHz×N . For a sentence classification task, the input sequence for the CRF layer is not inherentely clear. Therefore, we propose to model the joint entity and relation classification problem with the following sequence of scores (cf., Figure 2):\nd = [vEC(e1), vRE(r12), vEC(e2)] (3)\nwith rij being the relation between ei und ej . Thus, we approximate the joint probability of entity types Te1 , Te2 and relations Re1e2 as follows:\nP (Te1Re1e2Te2)\n≈P (Te1) · P (Re1e2 |Te1) · P (Te2 |Re1e2) (4)\nOur intuition is that the dependence between relation and entities is stronger than the dependence between the two entities.\nThe CRF layer pads its input of length n = 3 with begin and end tags and computes the following score for a sequence of predictions y:\ns(y) = n∑\ni=0\nQyiyi+1 + n∑\ni=1\ndi,yi (5)\n2https://github.com/glample/tagger\nwithQk,l being the transition score from class k to class l and dp,q being the score of class q at position p in the sequence. The scores are summed because all the variables of the CRF layer live in the log space. The matrix of transition scores Q ∈ R(n+2)×(n+2) is learned during training.3 For training, the forward algorithm computes the scores for all possible label sequences Y to get the log-probability of the correct label sequence ŷ:\nlog(p(ŷ)) = es(ŷ)∑ ỹ∈Y e s(ỹ) (6)\nFor testing, Viterbi is applied to obtain the label sequence y∗ with the maximum score:\ny∗ = argmax ỹ∈Y s(ỹ) (7)"
    }, {
      "heading" : "4 Experiments and Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 Data and Evaluation Measure",
      "text" : "We use the “entity and relation recognition” (ERR) dataset from (Roth and Yih, 2004)4 with the train-test split by Gupta et al. (2016). We tune the parameters on a held-out part of train. The data is labeled with entity types and relations (see Table 1). For entity pairs without a relation, we use the label N. Dataset statistics and model parameters are provided in the appendix.\nFollowing previous work, we compute F1 of the individual classes for EC and RE, as well as a taskwise macro F1 score. We also report the average of scores across tasks (Avg EC+RE).\n32 is added because of the padded begin and end tag 4http://cogcomp.cs.illinois.edu/page/resource view/43"
    }, {
      "heading" : "4.2 Experimental Setups",
      "text" : "Setup 1: Entity Pair Relations. Roth and Yih (2004, 2007); Kate and Mooney (2010) train separate models for EC and RE on the ERR dataset. For RE, they only identify relations between named entity pairs. In this setup, the query entities for our model are only named entity pairs. Note that this facilitates EC in our experiments.\nSetup 2: Table Filling. Following Miwa and Sasaki (2014); Gupta et al. (2016), we also model the joint task of EC and RE as a table filling task. For a sentence with length m, we create a quadratic table. Cell (i, j) contains the relation between word i and word j (or N for no relation). A diagonal cell (k, k) contains the entity type of word k. Following previous work, we only predict classes for half of the table, i.e. for m(m + 1)/2 cells. Figure 3 shows the table for the example sentence from Figure 1. In this setup, each cell (i, j) with i 6= j is a separate input query to our model. Our model outputs a prediction for cell (i, j) (the relation between i and j) and predictions for cells (i, i) and (j, j) (the types of i and j). To fill the diagonal with entity classes, we aggregate all predictions for the particular entity by using majority vote. Section 4.4 shows that the individual predictions agree with the majority vote in almost all cases.\nSetup 3: Table Filling Without Entity Boundaries. The table from setup 2 includes one row/column per multi-token entity, utilizing the given entity boundaries of the ERR dataset. In order to investigate the impact of the entity boundaries on the classification results, we also consider another table filling setup where we ignore the boundaries and assign one row/column per token. Note that this setup is also used by prior work on table filling (Miwa and Sasaki, 2014; Gupta et al., 2016). For evaluation, we follow Gupta et al. (2016) and score a multi-token entity as correct if at least one of its comprising cells has been classified correctly.\nComparison. The most important difference between setup 1 and setup 2 is the number of entity pairs with no relation (test set: ≈3k for setup 1, ≈121k for setup 2). This makes setup 2 more challenging. The same holds for setup 3 which considers the same number of entity pairs with no relation as setup 2. To cope with this, we randomly subsample negative instances in the train set of setup 2 and 3. Setup 3 considers the most query\nentity pairs in total since multi-token entities are split into their comprising tokens. However, setup 3 represents a more realistic scenario than setup 1 or setup 2 because in most cases, entity boundaries are not given. In order to apply setup 1 or 2 to another dataset without entity boundaries, a preprocessing step, such as entity boundary recognition or chunking would be required."
    }, {
      "heading" : "4.3 Experimental Results",
      "text" : "Table 1 shows the results of our globally normalized model in comparison to the same model with locally normalized softmax output layers (one for EC and one for RE). For setup 1, the CRF layer performs comparable or better than the softmax layer. For setup 2 and 3, the improvements are more apparent. We assume that the model can benefit more from global normalization in the case of table filling because it is the more challenging setup. The comparison between setup 2 and setup 3 shows that the entity classification suffers from not given entity boundaries (in setup 3). A reason could be that the model cannot convolve the token embeddings of the multi-token entities anymore when computing the entity representation (context B and D in Figure 2). Nevertheless, the relation classification performance is comparable in setup 2 and setup 3. This shows that the model can internally account for potentially wrong entity classification results due to missing entity boundaries.\nThe overall results (Avg EC+RE) of the CRF are better than the results of the softmax layer for all three setups. To sum up, the improvements of the linear-chain CRF show that (i) joint EC and RE benefits from global normalization and (ii) our way of creating the input sequence for the CRF for joint EC and RE is effective.\nComparison to State of the Art. Table 2 shows our results in the context of state-of-the-art results: (Roth and Yih, 2007), (Kate and Mooney, 2010),\n(Miwa and Sasaki, 2014), (Gupta et al., 2016).5 Note that the results are not comparable because of the different setups and different train-test splits.6\nOur results are best comparable with (Gupta et al., 2016) since we use the same setup and traintest splits. However, their model is more complicated with a lot of hand-crafted features and various iterations of modeling dependencies among entity and relation classes. In contrast, we only use pre-trained word embeddings and train our model end-to-end with only one iteration per entity pair. When we compare with their model without additional features (G et al. 2016 (2)), our model performs worse for EC but better for RE and comparable for Avg EC+RE."
    }, {
      "heading" : "4.4 Analysis of Entity Type Aggregation",
      "text" : "As described in Section 4.2, we aggregate the EC results by majority vote. Now, we analyze their disagreement. For our best model, there are only 9 entities (0.12%) with disagreement in the test data. For those, the max, min and median disagreement with the majority label is 36%, 2%, and 8%, resp. Thus, the disagreement is negligibly small.\n5We only show results of single models, no ensembles. Following previous studies, we omit the entity class “Other” when computing the EC score.\n6Our results on EC in setup 1 are also not comparable"
    }, {
      "heading" : "4.5 Analysis of CRF Transition Matrix",
      "text" : "To analyze the CRF layer, we extract which transitions have scores above 0.5. Figure 4 shows that the layer has learned correct correlations between entity types and relations."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we presented the first study on global normalization of neural networks for a sentence classification task without transforming it into a token-labeling problem. We trained a convolutional neural network with a linear-chain conditional random field output layer on joint entity and relation classification and showed that it outperformed using a locally normalized softmax layer.\nAn interesting future direction is the extension of the linear-chain CRF to jointly normalize all predictions for table filling in a single model pass. Furthermore, we plan to verify our results on other datasets in future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Heike Adel is a recipient of the Google European Doctoral Fellowship in Natural Language Processing and this research is supported by this fellowship. This work was also supported by DFG (SCHU 2246/4-2).\nsince we only input named entities into our model."
    }, {
      "heading" : "A Dataset Statistics",
      "text" : "Table 3 provides statistics of the data composition in our different setups which are described in the paper. The N class of setup 2 and setup 3 has been subsampled in the training and development set as described in the paper.\nNote that the sum of numbers of relation labels is slightly different to the numbers reported in (Roth and Yih, 2004). According to their website https://cogcomp.cs.illinois.edu/page/resource view/43, they have updated the corpus."
    }, {
      "heading" : "B Hyperparameters",
      "text" : "Table 4 provides the hyperparameters we optimized on dev (nkC : number of convolutional filters for the CNN convolving the contexts, nkE : number of convolutional filters for the CNN convolving the entities; hC : number of hidden units for creating the final context representation, hE : number of hidden units for creating the final entity representation).\nFor all models, we use a filter width of 3 for the context CNN and a filter width of 2 for the entity\nCNN (tuned in prior experiments and fixed for the optimization of the parameters in Table 4).\nFor training, we apply gradient descent with a batch size of 10 and an initial learning rate of 0.1. When the performance on dev decreases, we halve the learning rate. The model is trained with early stopping on dev, with a maximum number of 20 epochs. We apply L2 regularization with λ = 10−3."
    } ],
    "references" : [ {
      "title" : "Globally normalized transition-based neural networks",
      "author" : [ "Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Associa-",
      "citeRegEx" : "Andor et al\\.,? 2016",
      "shortCiteRegEx" : "Andor et al\\.",
      "year" : 2016
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of Machine Learning Research, 12:2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Integrating probabilistic extraction models and data mining to discover relations and patterns in text",
      "author" : [ "Aron Culotta", "Andrew McCallum", "Jonathan Betz." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,",
      "citeRegEx" : "Culotta et al\\.,? 2006",
      "shortCiteRegEx" : "Culotta et al\\.",
      "year" : 2006
    }, {
      "title" : "Relation extraction and the influence of automatic named-entity recognition",
      "author" : [ "Claudio Giuliano", "Alberto Lavelli", "Lorenza Romano." ],
      "venue" : "ACM Trans. Speech Lang. Process., 5(1):2:1–2:26.",
      "citeRegEx" : "Giuliano et al\\.,? 2007",
      "shortCiteRegEx" : "Giuliano et al\\.",
      "year" : 2007
    }, {
      "title" : "Table filling multi-task recurrent neural network for joint entity and relation extraction",
      "author" : [ "Pankaj Gupta", "Hinrich Schütze", "Bernt Andrassy." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Techni-",
      "citeRegEx" : "Gupta et al\\.,? 2016",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2016
    }, {
      "title" : "Joint entity and relation extraction using card-pyramid parsing",
      "author" : [ "Rohit J. Kate", "Raymond Mooney." ],
      "venue" : "Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 203–212, Uppsala, Sweden. Association for Com-",
      "citeRegEx" : "Kate and Mooney.,? 2010",
      "shortCiteRegEx" : "Kate and Mooney.",
      "year" : 2010
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning, ICML",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of Workshop",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Modeling joint entity and relation extraction with table representation",
      "author" : [ "Makoto Miwa", "Yutaka Sasaki." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1858–1869, Doha, Qatar. Associa-",
      "citeRegEx" : "Miwa and Sasaki.,? 2014",
      "shortCiteRegEx" : "Miwa and Sasaki.",
      "year" : 2014
    }, {
      "title" : "Information extraction from research papers using conditional random fields",
      "author" : [ "Fuchun Peng", "Andrew McCallum." ],
      "venue" : "Information processing & management, 42(4):963–979.",
      "citeRegEx" : "Peng and McCallum.,? 2006",
      "shortCiteRegEx" : "Peng and McCallum.",
      "year" : 2006
    }, {
      "title" : "Global inference for entity and relation identification via a linear programming formulation",
      "author" : [ "D. Roth", "W. Yih." ],
      "venue" : "Introduction to Statistical Relational Learning. MIT Press.",
      "citeRegEx" : "Roth and Yih.,? 2007",
      "shortCiteRegEx" : "Roth and Yih.",
      "year" : 2007
    }, {
      "title" : "A linear programming formulation for global inference in natural language tasks",
      "author" : [ "Dan Roth", "Wen-tau Yih." ],
      "venue" : "HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004), pages 1–8,",
      "citeRegEx" : "Roth and Yih.,? 2004",
      "shortCiteRegEx" : "Roth and Yih.",
      "year" : 2004
    }, {
      "title" : "Semimarkov conditional random fields for information extraction",
      "author" : [ "Sunita Sarawagi", "William W Cohen" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sarawagi and Cohen,? \\Q2004\\E",
      "shortCiteRegEx" : "Sarawagi and Cohen",
      "year" : 2004
    }, {
      "title" : "An introduction to conditional random fields for relational learning",
      "author" : [ "Charles Sutton", "Andrew McCallum." ],
      "venue" : "Introduction to statistical relational learning, pages 93–128.",
      "citeRegEx" : "Sutton and McCallum.,? 2006",
      "shortCiteRegEx" : "Sutton and McCallum.",
      "year" : 2006
    }, {
      "title" : "Combining recurrent and convolutional neural networks for relation classification",
      "author" : [ "Ngoc Thang Vu", "Heike Adel", "Pankaj Gupta", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Vu et al\\.,? 2016",
      "shortCiteRegEx" : "Vu et al\\.",
      "year" : 2016
    }, {
      "title" : "Convolutional neural network based triangular crf for joint intent detection and slot filling",
      "author" : [ "Puyang Xu", "Ruhi Sarikaya." ],
      "venue" : "2013 IEEE Workshop on Automatic Speech Recognition and Understanding, pages 78–83, Olomouc, Czech Republic. IEEE.",
      "citeRegEx" : "Xu and Sarikaya.,? 2013",
      "shortCiteRegEx" : "Xu and Sarikaya.",
      "year" : 2013
    }, {
      "title" : "Noise mitigation for neural entity typing and relation extraction",
      "author" : [ "Yadollah Yaghoobzadeh", "Heike Adel", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Valencia,",
      "citeRegEx" : "Yaghoobzadeh et al\\.,? 2016",
      "shortCiteRegEx" : "Yaghoobzadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "Collective cross-document relation extraction without labelled data",
      "author" : [ "Limin Yao", "Sebastian Riedel", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1013–1023, Cambridge,",
      "citeRegEx" : "Yao et al\\.,? 2010",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2010
    }, {
      "title" : "2d conditional random fields for web information extraction",
      "author" : [ "Jun Zhu", "Zaiqing Nie", "Ji-Rong Wen", "Bo Zhang", "Wei-Ying Ma." ],
      "venue" : "Proceedings of the 22nd international conference on Machine learning, pages 1044–1051. ACM.",
      "citeRegEx" : "Zhu et al\\.,? 2005",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "conditional random field (CRF) (Lafferty et al., 2001).",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : ", (Yao et al., 2010; Yaghoobzadeh et al., 2016).",
      "startOffset" : 2,
      "endOffset" : 47
    }, {
      "referenceID" : 17,
      "context" : ", (Yao et al., 2010; Yaghoobzadeh et al., 2016).",
      "startOffset" : 2,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "Roth and Yih (2004) develop constraints and use linear programming to globally normalize entity types and relations.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "Giuliano et al. (2007) use entity type information for relation extraction but do not train",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "Kate and Mooney (2010) train task-specific support vector machines and develop a card-pyramid parsing algorithm to jointly model both tasks.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "Kate and Mooney (2010) train task-specific support vector machines and develop a card-pyramid parsing algorithm to jointly model both tasks. Miwa and Sasaki (2014) use the same dataset but model the tasks as a table filling problem (see Section 4.",
      "startOffset" : 0,
      "endOffset" : 164
    }, {
      "referenceID" : 4,
      "context" : "Recently, Gupta et al. (2016) apply recurrent neural networks to fill the table.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 14,
      "context" : "Several studies propose different variants of non-neural CRF models for information extraction tasks but model them as token-labeling problems (Sutton and McCallum, 2006; Sarawagi et al., 2004; Culotta et al., 2006; Zhu et al., 2005; Peng and McCallum, 2006).",
      "startOffset" : 143,
      "endOffset" : 258
    }, {
      "referenceID" : 2,
      "context" : "Several studies propose different variants of non-neural CRF models for information extraction tasks but model them as token-labeling problems (Sutton and McCallum, 2006; Sarawagi et al., 2004; Culotta et al., 2006; Zhu et al., 2005; Peng and McCallum, 2006).",
      "startOffset" : 143,
      "endOffset" : 258
    }, {
      "referenceID" : 19,
      "context" : "Several studies propose different variants of non-neural CRF models for information extraction tasks but model them as token-labeling problems (Sutton and McCallum, 2006; Sarawagi et al., 2004; Culotta et al., 2006; Zhu et al., 2005; Peng and McCallum, 2006).",
      "startOffset" : 143,
      "endOffset" : 258
    }, {
      "referenceID" : 10,
      "context" : "Several studies propose different variants of non-neural CRF models for information extraction tasks but model them as token-labeling problems (Sutton and McCallum, 2006; Sarawagi et al., 2004; Culotta et al., 2006; Zhu et al., 2005; Peng and McCallum, 2006).",
      "startOffset" : 143,
      "endOffset" : 258
    }, {
      "referenceID" : 2,
      "context" : ", 2004; Culotta et al., 2006; Zhu et al., 2005; Peng and McCallum, 2006). In contrast, we propose a simpler linear-chain CRF model which directly connects entity and relation classes instead of assigning a label to each token of the input sequence. This is more similar to the factor graph by Yao et al. (2010) but computationally simpler.",
      "startOffset" : 8,
      "endOffset" : 311
    }, {
      "referenceID" : 2,
      "context" : ", 2004; Culotta et al., 2006; Zhu et al., 2005; Peng and McCallum, 2006). In contrast, we propose a simpler linear-chain CRF model which directly connects entity and relation classes instead of assigning a label to each token of the input sequence. This is more similar to the factor graph by Yao et al. (2010) but computationally simpler. Xu and Sarikaya (2013) also apply a CRF layer on top of continuous representations obtained by a CNN.",
      "startOffset" : 8,
      "endOffset" : 363
    }, {
      "referenceID" : 8,
      "context" : "(Mikolov et al., 2013).",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : "Moreover, they have proven effective for RE in previous work (Vu et al., 2016).",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : "The ERR dataset we use provides boundaries for entities to concentrate on the classification task (Roth and Yih, 2004).",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "For global normalization, we adopt the linearchain CRF layer by Lample et al. (2016).2 It expects scores for the different classes as input.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : "We use the “entity and relation recognition” (ERR) dataset from (Roth and Yih, 2004)4 with the train-test split by Gupta et al.",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "We use the “entity and relation recognition” (ERR) dataset from (Roth and Yih, 2004)4 with the train-test split by Gupta et al. (2016). We tune",
      "startOffset" : 115,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : "Roth and Yih (2004, 2007); Kate and Mooney (2010) train separate models for EC and RE on the ERR dataset.",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "Following Miwa and Sasaki (2014); Gupta et al.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : "Following Miwa and Sasaki (2014); Gupta et al. (2016), we also model the joint task of EC and RE as a table filling task.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : "Note that this setup is also used by prior work on table filling (Miwa and Sasaki, 2014; Gupta et al., 2016).",
      "startOffset" : 65,
      "endOffset" : 108
    }, {
      "referenceID" : 4,
      "context" : "Note that this setup is also used by prior work on table filling (Miwa and Sasaki, 2014; Gupta et al., 2016).",
      "startOffset" : 65,
      "endOffset" : 108
    }, {
      "referenceID" : 4,
      "context" : "Note that this setup is also used by prior work on table filling (Miwa and Sasaki, 2014; Gupta et al., 2016). For evaluation, we follow Gupta et al. (2016) and score a multi-token entity as correct if at least one of its comprising cells has been classified correctly.",
      "startOffset" : 89,
      "endOffset" : 156
    }, {
      "referenceID" : 11,
      "context" : "Table 2 shows our results in the context of state-of-the-art results: (Roth and Yih, 2007), (Kate and Mooney, 2010),",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : "Table 2 shows our results in the context of state-of-the-art results: (Roth and Yih, 2007), (Kate and Mooney, 2010),",
      "startOffset" : 92,
      "endOffset" : 115
    }, {
      "referenceID" : 9,
      "context" : "(Miwa and Sasaki, 2014), (Gupta et al.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "(Miwa and Sasaki, 2014), (Gupta et al., 2016).",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "Our results are best comparable with (Gupta et al., 2016) since we use the same setup and traintest splits.",
      "startOffset" : 37,
      "endOffset" : 57
    } ],
    "year" : 2017,
    "abstractText" : "We introduce globally normalized convolutional neural networks for joint entity classification and relation extraction. In particular, we propose a way to utilize a linear-chain conditional random field output layer for predicting entity types and relations between entities at the same time. Our experiments show that global normalization outperforms a locally normalized softmax layer on a benchmark dataset.",
    "creator" : "TeX"
  }
}