{
  "name" : "1604.02264.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Probabilistic classifiers with low rank indefinite kernels",
    "authors" : [ "Frank-Michael Schleif", "Andrej Gisbrecht", "Peter Tino" ],
    "emails" : [ "schleify@cs.bham.ac.uk", "andrej.gisbrecht@aalto.fi", "pxt@cs.bham.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: indefinite kernel, kernel fisher discriminant, minimum enclosing ball, Nyström approximation, low rank approximation, classification"
    }, {
      "heading" : "1. Introduction",
      "text" : "Domain specific proximity measures, like alignment scores in bioinformatics Smith et al. (1981), the modified Hausdorff-distance for structural pattern recog-\n∗Corresponding author Email addresses: schleify@cs.bham.ac.uk (Frank-Michael Schleif1),\nandrej.gisbrecht@aalto.fi (Andrej Gisbrecht2), pxt@cs.bham.ac.uk (Peter Tino1)\nar X\niv :1\n60 4.\n02 26\n4v 1\n[ cs\n.L G\n] 8\nA pr\nnition Dubuisson and Jain (1994), shape retrieval measures like the inner distance Ling and Jacobs (2007) and many other ones generate non-metric or indefinite similarities or dissimilarities. Classical learning algorithms like kernel machines assume Euclidean metric properties in the underlying data space and may not be applicable for this type of data.\nOnly few machine learning methods have been proposed for non-metric proximity data, like the indefinite kernel Fisher discriminant (iKFD) Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009), the probabilistic classification vector machine (PCVM) Chen et al. (2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015).\nWhile being very efficient these methods do not scale to larger datasets with in general cubic complexity. In Schleif et al. (2015a); Gisbrecht and Schleif (2015) the authors proposed a few Nyström based (see e.g. Williams and Seeger (2000)) approximation techniques to improve the scalability of the PCVM for low rank matrices. The suggested techniques use the Nyström approximation in a nontrivial way to provide exact eigenvalue estimations also for indefinite kernel matrices. This approach is very generic and can be applied in different algorithms. In this contribution we further extend our previous work and not only derive a low rank approximation of the indefinite kernel Fisher discriminant, but also address the landmark selection from a novel view point. The obtained Ny-iKFD approach is linear in runtime and memory consumption for low rank matrices. The formulation is exact if the rank of the matrix equals the number of independent landmarks points. The selection of the landmarks of the Nyström approximation is a critical point addressed in previous work (see e.g. Zhang and Kwok (2010); Si et al. (2014); Brabanter et al. (2010)). In general these strategies use the full psd kernel matrix or expect that the kernel is of some standard class like an RBF kernel. In each case the approaches presented so far are costly in runtime and memory consumption as can be seen in the subsequent experiments.\nAdditionally, former approaches for landmark selection aim on generic matrix reconstructions of positive semi definite (psd) kernels. We propose a restricted reconstruction of the psd or non-psd kernel matrix with respect to a supervised learning scenario only. We do not any longer expect to obtain an accurate kernel\nreconstruction from the approximated matrix (e.g. by using the Frobenius norm) but are pleased if the approximated matrix preserves the class boundaries in the data space.\nIn Gisbrecht and Schleif (2015) the authors derived methods to approximate large proximity matrices by means of the Nyström approximation and conversion rules between similarities and dissimilarities. These techniques have been applied in Schleif et al. (2015a) and Schleif, F.-M., Gisbrecht, A., Tino, P., (2015b) in a proof of concept setting, to obtain approximate models for the Probabilistic Classification Vector Machine and the Indefinite Fisher Kernel Discriminant analysis using a random landmark selection scheme. This work is substantially extended and detailed in this article with a specific focus on indefinite kernels, only. A novel landmark selection scheme is proposed. Based on this new landmark selection scheme we provide detailed new experimental results and compare to alternative landmark selection approaches.\nStructure of the paper: First we give some basic notations necessary in the subsequent derivations. Then we review iKFD and PCVM as well as some approximation concepts proposed by the authors in Schleif et al. (2015a) which are based on the well known Nyström approximation. Subsequently, we consider the landmark selection problem in more detail and show empirically results motivating a supervised selection strategy. Finally we detail the reformulation of iKFD and PCVM based on the introduced concepts and show the efficiency in comparison to Ny-PCVM and Ny-iKFD for various indefinite proximity benchmark data sets."
    }, {
      "heading" : "2. Methods",
      "text" : ""
    }, {
      "heading" : "2.1. Notation and basic concepts",
      "text" : "Let X be a collection of N objects xi, i = 1, 2, ..., N , in some input space. If the similarity function or inner product used to compare two objects xi, xj is metric, proper mercer kernels can be obtained as discussed subsequently. A classical similarity function in this context, is the Euclidean inner product with the respective Euclidean distance which is a frequent core component of various metric kernel functions, like the famous radial basis function (rbf) kernel.\nNow, let φ : X 7→ H be a mapping of patterns from X to a high-dimensional or infinite dimensional Hilbert space H equipped with the inner product 〈·, ·〉H. The transformation φ is in general a non-linear mapping to a high-dimensional space H and may in general not be given in an explicit form. Instead a kernel function k : X × X 7→ R is given which encodes the inner product in H. The\nkernel k is a positive (semi) definite function such that k(x, x′) = φ(x)>φ(x′) for any x, x′ ∈ X . The matrix K := Φ>Φ is an N × N kernel matrix derived from the training data, where Φ : [φ(x1), . . . , φ(xN)] is a matrix of images (column vectors) of the training data in H. The motivation for such an embedding comes with the hope that the non-linear transformation of input data into higher dimensionalH allows for using linear techniques inH. Kernelized methods process the embedded data points in a feature space utilizing only the inner products 〈·, ·〉H (kernel trick) (Shawe-Taylor and Cristianini, 2004), without the need to explicitly calculate φ. The specific kernel function can be very generic. Most prominent are the linear kernel with k(x,x′) = 〈φ(x), φ(x′)〉 where 〈φ(x), φ(x′)〉 is the Euclidean inner product or the rbf kernel k(x,x′) = exp ( − ||x−x\n′||2 2σ2\n) , with σ as a\nfree parameter. Thereby it is assumed that the kernel function k(x,x′) is positive semi definite (psd). This assumption is however not always fulfilled, and the underlying similarity measure may not be metric and hence not lead to a mercer kernel. Examples can be easily found in domain specific similarity measures as mentioned before and detailed later on. These measures imply indefinite kernels. In what follows we will review some basic concepts and approaches related to such non-metric situations."
    }, {
      "heading" : "2.2. Krein and Pseudo-Euclidean spaces",
      "text" : "A Krein space is an indefinite inner product space endowed with a Hilbertian topology.\nDefinition 1 (Inner products and inner product space). Let K be a real vector\nspace. An inner product space with an indefinite inner product 〈·, ·〉K on K is a\nbi-linear form where all f, g, h ∈ K and α ∈ R obey the following conditions.\n• Symmetry: 〈f, g〉K = 〈g, f〉K\n• linearity: 〈αf + g, h〉K = α〈f, h〉K + 〈g, h〉K;\n• 〈f, g〉K = 0 ∀g ∈ K implies f = 0\nAn inner product is positive definite if ∀f ∈ K, 〈f, f〉K ≥ 0, negative definite if ∀f ∈ K, 〈f, f〉K ≤ 0, otherwise it is indefinite. A vector space K with inner product 〈·, ·〉K is called an inner product space.\nDefinition 2 (Krein space and pseudo Euclidean space). An inner product space\n(K, 〈·, ·〉K) is a Krein space if we have two Hilbert spaces H+ and H− spanning\nK such that ∀f ∈ K we have f = f+ + f− with f+ ∈ H+ and f− ∈ H− and\n∀f, g ∈ K, 〈f, g〉K = 〈f+, g+〉H+ −〈f−, g−〉H− . A finite-dimensional Krein-space\nis a so called pseudo Euclidean space (pE).\nIndefinite kernels are typically observed by means of domain specific nonmetric similarity functions (such as alignment functions used in biology (Smith et al., 1981)), by specific kernel functions - e.g. the Manhattan kernel k(x,x′) = −||x−x′||1, tangent distance kernel (Haasdonk and Keysers, 2002) or divergence measures plugged into standard kernel functions (Cichocki and Amari, 2010). Another source of non-psd kernels are noise artifacts on standard kernel functions (Haasdonk, 2005).\nFor such spaces vectors can have negative squared ”norm”, negative squared ”distances” and the concept of orthogonality is different from the usual Euclidean case. In the subsequent experiments our input data are in general given by a symmetric indefinite kernel matrix K.\nGiven a symmetric dissimilarity matrix with zero diagonal 1, an embedding of the data in a pseudo-Euclidean vector space determined by the eigenvector decomposition of the associated similarity matrix S is always possible (Goldfarb, 1984) 2\nGiven the eigendecomposition of S, S = UΛU>, we can compute the corresponding vectorial representation V in the pseudo-Euclidean space by\nV = Up+q+z |Λp+q+z|1/2 (1)\nwhere Λp+q+z consists of p positive, q negative non-zero eigenvalues and z zero eigenvalues. Up+q+z consists of the corresponding eigenvectors. The triplet (p, q, z) is also referred to as the signature of the Pseudo-Euclidean space. This operation is however very costly and should be avoided for larger data sets. A detailed presentation of similarity and dissimilarity measures, and mathematical aspects of metric and non-metric spaces is provided in (Pekalska and Duin, 2005).\n1A similarity matrix can be easily converted into squared dissimilarities using d2(x, y) = k(x, x) + k(y, y)− 2 · k(x, y).\n2 The associated similarity matrix can be obtained by double centering (Pekalska and Duin, 2005) of the (squared) dissimilarity matrix. S = −JDJ/2 with J = (I − 11>/N) and identity matrix I and vector of ones 1."
    }, {
      "heading" : "2.3. Indefinite Fisher and kernel quadratic discriminant",
      "text" : "In Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009) the indefinite kernel Fisher discriminant analysis (iKFD) and indefinite kernel quadratic discriminant analysis (iKQD) was proposed focusing on binary classification problems, recently extended by a weighting scheme in Yang and Fan (2013)3.\nThe initial idea is to embed the training data into a Krein space (see Def. 2) and to apply a modified kernel Fisher discriminant analysis or kernel quadratic discriminant analysis for indefinite kernels. Consider binary classification and a data set of input-target training pairs D = {xi, yi}Ni=1, where yi ∈ {−1,+1}. Given the indefinite kernel matrixK and the embedded data in a pseudo-Euclidean space (pE), the linear Fisher Discriminant function f(x) = 〈w,Φ(x)〉pE + b is based on a weight vector w such that the between-class scatter is maximized while the within-class scatter is minimized along w. Φ(x) is a vector of basis function evaluations for data item x and b is a bias term. This direction is obtained by maximizing the Fisher criterion in the pseudo Euclidean space\nJ(w) = 〈w,ΣbpEw〉pE 〈w,ΣwpEw〉pE\nwhere ΣbpE = ΣbJ is the scatter matrix in the pseudo Euclidean space, with J = diag(1p,−1q) where 1p ∈ Rp denotes the p-dimensional vector of all ones. The number of positive eigenvalues is denoted by p and for the negative eigenvalues by q. The within-scatter-matrix in the pseudo-Euclidean space is given as ΣwpE = ΣwJ . The Euclidean between- and within-scatter-matrices can be expressed as:\nΣb = (µ+ − µ−)(µ+ − µ−)> (2) Σw = ∑ i∈I+ (φ(xi)− µ+)(φ(xi)− µ+)> + ∑ i∈I− (φ(xi)− µ−)(φ(xi)− µ−)>(3)\nWhere the set of indices of each class are I+ := {i : yi = +1} and I− := {i : yi = −1}. In Haasdonk and Pekalska (2008) it is shown that the Fisher Discriminant in the pE space ∈ R(p,q) is identical to the Fisher Discriminant in the associated Euclidean space Rp+q. To avoid the explicit embedding into the pE space a kernelization is considered such that the weight vector w ∈ R(p,q) is expressed as a linear combination of the training data φ(xi, hence w = ∑N i=1 αiφ(xi. Transferred to the Fisher criterion this allows to use the kernel trick. A similar strategy\n3For multiclass problems a classical 1 vs rest wrapper is used within this paper\ncan be used for KQD as well as the indefinite kernel PCA Pekalska and Haasdonk (2009)."
    }, {
      "heading" : "2.4. Probabilistic Classification Vector Learning",
      "text" : "PCVM uses a kernel regression model ∑N i=1wiφi(x) + b with a link function, with wi being again the weights of the basis functions φi(x) and b as a bias term. The Expectation Maximization (EM) implementation of PCVM Chen et al. (2014) uses the probit link function, i.e. Ψ(x) = ∫ x −∞N (t|0, 1)dt,where Ψ(x) is the\ncumulative distribution of the normal distribution N (0, 1). We get: l(x; w, b) = Ψ (∑N i=1wiφi(x) + b ) = Ψ (Φ(x)w + b)\nIn the PCVM formulation Chen et al. (2009a), a truncated Gaussian prior Nt with support on [0,∞) and mode at 0 is introduced for each weight wi and a zero-mean Gaussian prior is adopted for the bias b. The priors are assumed to be\nmutually independent. p(w|α) = N∏ i=1 p(wi|αi) = N∏ i=1 Nt(wi|0, α−1i ), p(b|β) = N (b|0, β−1), δ(x) = 1, x > 0.\np(wi|αi) = {\n2N (wi|0, α−1i ) if yiwi > 0 0 otherwise = 2N (wi|0, α −1 i ) · δ(yiwi).\nWe follow the standard probabilistic formulation and assume that z(x) = Φ(x)w+ b is corrupted by an additive random noise , where ∼ N (0, 1). According to the probit link model, we have:\nh(x) = Φ(x)w + b+ ≥ 0, y = 1, h(x) = Φ(x)w + b+ < 0, y = −1 (4) and obtain: p(y = 1|x,w, b) = p(Φ(x)w + b+ ≥ 0) = Ψ(Φ(x)w + b). h(x) is a latent variable because is an unobservable variable. We collect evaluations of h(x) at training points in a vector H(x) = (h(x1), . . . , h(xN))>. In the expectation step the expected value H̄ of H with respect to the posterior distribution over the latent variables is calculated (given old values wold, bold). In the maximization step the parameters are updated through\nwnew = M(MΦ>(x)Φ(x)M + IN) −1 M(Φ>(x)H̄− bΦ>(x)I) (5)\nbnew = t(1 + tNt)−1t(I>H̄− I>Φ(x)w) (6) where IN is a N-dimensional identity matrix and I a all-ones vector, the diagonal elements in the diagonal matrix M are:\nmi = (ᾱi) −1/2 =\n{√ 2wi if yiwi ≥ 0\n0 else (7)\nand the scalar t = √\n2|b|. For further details can be found in Chen et al. (2009a). Even though kernel machines and their derivatives have shown great promise in practical application, their scope is somehow limited by the fact that the computational complexity grows rapidly with the size of the kernel matrix (number of data items). Among methods suggested to deal with this issue in the literature, the Nyström method has been popular and widely used."
    }, {
      "heading" : "3. Nyström approximated matrix processing",
      "text" : "The Nyström approximation technique has been proposed in the context of kernel methods in (Williams and Seeger, 2000). Here, we give a short review of this technique before it is employed in PCVM and iKFD. One well known way to approximate a N ×N Gram matrix, is to use a low-rank approximation. This can be done by computing the eigendecomposition of the kernel matrix K = UΛUT , where U is a matrix, whose columns are orthonormal eigenvectors, and Λ is a diagonal matrix consisting of eigenvalues Λ11 ≥ Λ22 ≥ ... ≥ 0, and keeping only the m eigenspaces which correspond to the m largest eigenvalues of the matrix. The approximation is K̃ ≈ UN,mΛm,mUm,N , where the indices refer to the size of the corresponding submatrix restricted to the larges m eigenvalues. The Nyström method approximates a kernel in a similar way, without computing the eigendecomposition of the whole matrix, which is an O(N3) operation.\nBy the Mercer theorem kernels k(x,x′) can be expanded by orthonormal eigenfunctions ϕi and non negative eigenvalues λi in the form\nk(x,x′) = ∞∑ i=1 λiϕi(x)ϕi(x ′).\nThe eigenfunctions and eigenvalues of a kernel are defined as the solution of the integral equation ∫\nk(x′,x)ϕi(x)p(x)dx = λiϕi(x ′),\nwhere p(x) is the probability density of x. This integral can be approximated based on the Nyström technique by an i.i.d. sample {xk}mk=1 from p(x):\n1\nm m∑ k=1 k(x′,xk)ϕi(x k) ≈ λiϕi(x′).\nUsing this approximation we denote with K(m) the corresponding m ×m Gram sub-matrix and get the corresponding matrix eigenproblem equation as:\nK(m)U (m) = U (m)Λ(m)\nwith U (m) ∈ Rm×m is column orthonormal and Λ(m) is a diagonal matrix. Now we can derive the approximations for the eigenfunctions and eigenvalues of the kernel k\nλi ≈ λ (m) i ·N m , ϕi(x ′) ≈\n√ m/N\nλ (m) i\nk′,>x u (m) i , (8)\nwhere u(m)i is the ith column of U (m). Thus, we can approximate ϕi at an arbitrary point x′ as long as we know the vector k′x = (k(x 1,x′), ..., k(xm,x′)). For a given N × N Gram matrix K one may randomly choose m rows and respective columns. The corresponding indices are called landmarks, and should be chosen such that the, data distribution is sufficiently covered. Strategies how to chose the landmarks have recently been addressed in Zhang and Kwok (2010); Zhang et al. (2008) and Gittens and Mahoney (2013); Brabanter et al. (2010). We denote these rows by Km,N . Using the formulas Eq. (8) we obtain K̃ =∑m\ni=1 1/λ (m) i ·KTm,N(u (m) i ) T (u (m) i )Km,N , where λ (m) i and u (m) i correspond to the m×m eigenproblem. Thus we get, K−1m,m denoting the Moore-Penrose pseudoinverse,\nK̃ = KN,mK −1 m,mKm,N . (9)\nas an approximation of K. This approximation is exact, if Km,m has the same rank as K."
    }, {
      "heading" : "3.1. Pseudo Inverse and Singular Value Decomposition of a Nyström approximated matrix",
      "text" : "In the Ny-PCVM approach discussed in Section 5 we need the pseudo inverse of a Nyström approximated matrix while for the Ny-iKFD a Nyström approximated eigenvalue decomposition (EVD) is needed.\nA Nyström approximated pseudo inverse can be calculated by a modified singular value decomposition (SVD) with a rank limited by r∗ = min{r,m} where r is the rank of the pseudo inverse and m the number of landmark points. The output is given by the rank reduced left and right singular vectors and the reciprocal of the singular values. The singular value decomposition based on a Nyström approximated similarity matrix K̃ = KNmK−1m,mK > Nm with m landmarks, calculates\nthe left singular vectors of K̃ as the eigenvectors of K̃K̃> and the right singular vectors of K̃ as the eigenvectors of K̃>K̃4. The non-zero singular values of K̃ are then found as the square roots of the non-zero eigenvalues of both K̃>K̃ or K̃K̃>. Accordingly one only has to calculate a new Nyström approximation of the matrix K̃K̃> using e.g. the same landmark points as for the input matrix K̃. Subsequently an eigenvalue decomposition (EVD) is calculated on the approximated matrix ζ = K̃K̃>. For a matrix approximated by Eq. (9) it is possible to compute its exact eigenvalue estimators in linear time."
    }, {
      "heading" : "3.2. Eigenvalue decomposition of a Nyström approximated matrix",
      "text" : "To compute the eigenvectors and eigenvalues of an indefinite matrix we first compute the squared form of the Nyström approximated kernel matrix. Let K be a psd similarity matrix, for which we can write its decomposition as\nK̃ = KN,mK −1 m,mKm,N = KN,mUΛ −1U>K>N,m = BB >,\nwhere we defined B = KN,mUΛ−1/2 with U and Λ being the eigenvectors and eigenvalues of Km,m, respectively. Further it follows for the squared K̃:\nK̃2 = BB>BB> = BV AV >B>,\nwhere V and A are the eigenvectors and eigenvalues of B>B, respectively. The square operation does not change the eigenvectors of K but only the eigenvalues. The corresponding eigenequation can be written as B>Bv = av. Multiplying with B from left we get: BB>︸ ︷︷ ︸\nK̃ (Bv)︸︷︷︸ u = a (Bv)︸︷︷︸ u . It is clear that A must be the ma-\ntrix with the eigenvalues of K̃. The matrix Bv is the matrix of the corresponding eigenvectors, which are orthogonal but not necessary orthonormal. The normalization can be computed from the decomposition:\nK̃ = B V V >︸ ︷︷ ︸ diag(1) B> = BV A−1/2AA−1/2V >B> = CAC>,\nwhere we defined C = BV A−1/2 as the matrix of orthonormal eigenvectors of K. The eigenvalues of K̃ can be obtained using A = C>K̃C. Using this derivation\n4For symmetric matrices we have K̃K̃> = K̃>K̃\nwe can obtain exact eigenvalues and eigenvectors of an indefinite low rank kernel matrix K, given rank(K) = m and the landmarks points are independent5\nThe former approximation scheme is focused on preserving the full low rank eigen structure of the underlying data space. The accuracy of this approximation is typically measured by the Frobenius norm. A low value of the Frobenius norm of the approximated versus the original kernel matrix ensures that the approximated kernel matrix K̃ can be used similar asK for any kernel based data analysis method like kernel-PCA, kernel-k-means, SVM, laplacian eigenmaps, preserving also small between point distances. In the context of classification this requirement is very strong and unnecessary. We suggest to restrict the approximation K̃ to a low rank kernel which preserves only the between class distances focusing on class separation. To achieve this objective we suggest to use a supervised landmark selection scheme. This is introduced in the following section and compared with a number of baseline methods."
    }, {
      "heading" : "4. Supervised landmark selection using minimum enclosing balls",
      "text" : "The Nyström approximation is based on m characteristic landmark points taken from the dataset. The number of landmarks should be sufficiently large and the landmarks should be diverse enough to get accurate approximations of the dominating singular vectors of the similarity matrix. In Zhang and Kwok (2010) multiple strategies for landmark selection have been studied and a clustering based approach was suggested to find the specific landmarks. Thereby the number of landmarks is a user defined parameter and a classical kmeans algorithm is applied on the kernel matrix to identify characteristic landmark points in the empirical feature space. This approach is quite effective (see Zhang and Kwok (2010)), with some small improvements using an advanced clustering scheme as shown in Si et al. (2014). We will use it as a baseline for an advanced landmark section approach. Further we will also consider a pure random selection strategy. It should be noted that the formulation given in Zhang and Kwok (2010) takes the full kernel matrix as an input into the k-means clustering. This is obviously also very costly and may become inapplicable for larger kernel matrices 6 It is however not yet\n5An implementation of this linear time eigen-decomposition for low rank indefinite matrices is available at: http://www.techfak.uni-bielefeld.de/˜fschleif/ eigenvalue_corrections_demos.tgz.\n6 It may however be possible to circumvent this full complexity approach e.g. by subsampling concepts or by more advanced concepts of k-means, but this is not the focus of this paper.\nclear how the number of landmarks can be appropriately chosen. If the number of landmarks is large we can expect the data space to be sufficiently covered after the clustering but the model complexity can become prohibitive. On the other hand if the number of landmarks is to small the clustering may lead to inappropriate results by merging disjunct parts of the data space. We propose to consider the Nyström approximation in a restricted form with respect to a supervised learning problem only. This relieves us from the need of a perfect reconstruction of the kernel matrix. It is in fact sufficient to reconstruct the kernel such that it is close to the ideal kernel (see e.g. Kwok and Tsang (2003)). We will however not learn an idealized kernel as proposed in Kwok and Tsang (2003), which by itself is very costly for large scale matrices, but provide a landmark selection strategy leading into a similar direction.\nTypically the approximation quality of a Nyström approximated similarity matrix is evaluated using the Frobenius norm.For real valued data the Frobenius norm of two squared matrices, is simply the sum of the squared difference between the individual kernel entries. The Frobenius norm is very sensitive to small perturbations of the kernel matrix, addressing also small local geometric violations. Instead we propose a margin based similarity measure between matrices taking labels into account.\nWe define a supervised similarity measure between two matrices to estimate improved prediction performance if a linear classifier is used 7\nDefinition 3 (Supervised matrix similarity score (SMSS)). Assume we have a\ntwo class problem with y ∈ {−1, 1}8. Given a similarity functionK : X×X → R\nfor a learning problem P with underlying points (x, y) ∼ P . We define a score\ns(K̂,K) = f(K̂) f(K) with f(S) = ∑\ny∈{−1,1} ∣∣∣E(xi,x′j)|(yi=y){S(xi, x′j)|y = y′j} −E(xi,x′j)|yi=y{S(xi, x′j)|y 6= y′j}∣∣∣ The function f(S) provides a margin estimate of the scores between pairwise similarities of a class y and pairwise similarities between the entries of class y with respect to entries of the other class(es). Assuming that f(K̂) gives a margin estimate of the improved matrix and f(K) for the original input matrix, the score\n7Note that for non-linear separable data the kernel trick can be used and we can still use linear decision functions as shown in iKFD and PCVM.\n8The extension to more than two classes is straight forward\ns(K̂,K) > 1, if the margin has increased and s(K̂,K) ≤ 1 otherwise. Similar as in Balcan et al. (2008) we assume that a good linear classifier can be obtained from a given similarity function (or kernel) if the similarities between classes are much lower than those within the same class. However the score s(K̂,K) is only a rough estimate. It will likely work well for datasets which can easily be modeled by conceptually related classifiers focusing e.g. on exemplar based representations. The median classifier proposed in Nebel et al. (2015) is such a simple classifier. In its simplest form it identifies for each class a single basis function or prototype, showing maximum margin with respect to the other prototypes with different class labels. In Nebel et al. (2015) it was shown that such a classifier can be very efficient also for a variety of classification problems.\nIn our study the modified similarity matrix K̂ is a Nyström approximated matrix, where the landmarks should be chosen to keep good prediction accuracy instead of a good data reconstruction, as typically aimed for. Note that due to the approximation it may in fact happen that the SMSS values is below 1, indicating a decreased discrimination power with respect to the full original matrix. But in the considered setting the approximation is a mandatory step and we try to achieve a large SMSS value for the approximated similarity matrix.\nApparently the (supervised) representation accuracy of the Nyström approximation of K depends on the number and type of the selected landmarks. We propose to calculate minimum enclosing ball solutions (MEB) on the individual classwise kernels, to address both problems:\n1. finding a sufficient number of landmarks 2. find landmarks explaining the data characteristics and preserve a good class\nseparation for K̂\nAs an additional constraint we are looking for an approach where also indefinite proximity matrices can be processed without costly preprocessing steps."
    }, {
      "heading" : "4.1. MEB for psd input kernels",
      "text" : "We denote the set of indices or points of a sub kernel matrix referring to class j by Rj . Assuming approximately spherical clusters, we can approximate this problem by the minimum enclosing ball :\nminR2,wj R 2 such that ‖wj − Φ(ξi)‖2 ≤ R2 ∀ξi ∈ Rj\nwhereR is the radius of the sphere and wj is a center point which can be indirectly represented in the kernel space) as a weighted linear combination of the points in\nRj . The assumption of a sphere is in fact no substantial restriction if the provided kernel is sufficiently expressive. This is also the reason why core-vector data description (CVDD) can be used as a linear time replacement for support vector data description Tsang, I. W., Kwok, J. T., Cheung, P., (2005).\nIt has been shown e.g. in Badoiu and Clarkson (2008) that the minimum enclosing ball can be approximated with quality in (worst case) linear time using an algorithm which requires only a constant subset of the receptive field Rj , the core set. Given fixed quality , the following algorithm converges in O(1/ 2) steps:\nMEB: S := {ξi, ξk} for a pair of largest distance ‖Φ(ξi)−Φ(ξk)‖2 inRj and ξi chosen randomly repeat\nsolve MEB(S)→ w̃j, R if exists ξl ∈ Rj where ‖Φ(ξl)− w̃j‖2 > R2(1 + )2 then S := S ∪ {ξl}\nend if until all ξl are covered by the R(1 + ) ball in the feature space return w̃j In each step, the MEB problem is solved for a small subset of constant size\nonly. This is possible by referring to the dual problem which has the form minαi≥0 ∑ ij αiαjkij − ∑ i αik 2 ii\nwhere ∑\ni αi = 1\nwith data points occurring only as dot products, i.e. kernelization is possible. The same holds for all distance computations of the approximate MEB problem. Note that the dual MEB problem provides a solution in terms of the dual variables αi. The identified finite number of core points (those with non-vanishing αi) will be used as landmarks for this class and considered to be sufficient to represent the enclosing sphere of the data. Each class is represented by at least two core points. Combining all core sets of the various classes provides us with the full set of landmarks used to get a Nystöm approximation of K.\nThe MEB solution typically consists of a very small number of points (independent of N ), sufficient to describe the hyper-ball enclosing the respective data. If the kernel is psd we can use the MEB approach directly in the kernel space."
    }, {
      "heading" : "4.2. MEB for non-psd input kernels",
      "text" : "If the given kernel is non-psd we either can apply various eigenvalue correction approaches see Schleif and Tino (2015) or we use K̂ = K ·K>, which can\nalso be easily done for Nyström approximated matrices without calculating a full matrix (see first part of Eq. (15)). This procedure does not change the eigenvectors of K but takes the square of the eigenvalues such that K̂ becomes psd. It should be noted that if we use K̂ as an input of a kernel k-means algorithm this is equivalent as using K as the input of the classical k-means with Euclidean distance as suggested in Zhang and Kwok (2010).\nThe proposed supervised landmark selection using MEB does not only identify a good estimate for the number of landmarks but also ensures that the landmarks are sufficient to explain the data space. The solutions of the MEB consist of non-redundant points at the perimeter of the sphere, which can considered to be unrelated, although not necessarily orthogonal in the similarity space (with potentially squared negative eigenvalues). Especially only those points are included in the MEB solution which are needed to explain the sphere such that redundancy within this set is avoided Badoiu and Clarkson (2008). Therefore for each class the MEB solutions provides a local span of the underlying eigen-space. The combination of the different subspaces can lead to redundancy but we can expect that the full data space is sufficiently covered. We will show the effectiveness of this approach in some short experiments."
    }, {
      "heading" : "4.3. Small scale experiments - landmark selection scheme",
      "text" : "We use the ball dataset as proposed in Duin and Pekalska (2010). It is an artificial dataset based on the surface distances of randomly positioned balls of two classes having a slightly different radius. The dataset is non-Euclidean with substantial information encoded in the negative part of the eigenspectrum. We generated the data with 100 samples per class leading to an N × N dissimilarity matrix D, with N = 200.\nWe also use the protein data (213 pts, 4 classes) set represented by an indefinite similarity matrix, with a high intrinsic dimension Schleif and Tino (2015). Further we analyzed two simulated metric datasets which are not linear separable using the Euclidean norm: (1) the checker board data, generated as a two dimensional dataset with datapoints organized on a 3×3 checkerboard, with alternating labels. This dataset has multi-modal classes. (2) a simple gaussian cloud dataset with two gaussian with substantial overlap. The simulated data have been represented by an extreme learning machine (elm) kernel. Checker is linear separable in the elm-kernel space, whereas Gaussian is not separable by construction.\nIt should be noted that the elm kernel, used for the vectorial data, typically increases the number of non-vanishing eigenvalues such that the original two dimensional data are finally indeed higher dimensional and not representable by\nonly two basis functions. Two dimensional visualizations of the unapproximated K · K> similarity matrices obtained by using laplacian eigenmaps Belkin and Niyogi (2003). are shown in Figure 1.For the checker board data we also show two-dimensional plots of the obtained iKFD decision boundaries and different landmark selection schemes in Figure 2.\nNow the obtained (indefinite) kernel matrix has been used in the iKFD in six different ways using different landmark selection schemes:\na) we used the original kernel matrix (SIM1),\nb) the matrix is Nystöm approximated using the MEB approach (SIM2),\nc) the matrix is Nyström approximated using the approach of Zhang and Kwok (2010) where the number of landmarks is taken from the MEB solution (SIM3),\nd) using the approach of Zhang and Kwok (2010) but with C landmarks where C is the number of classes (SIM4)\ne) using a random sample of C landmarks (SIM5). SIM5 can be considered as a very basic baseline approach.\nf) using an entropy based selection as proposed in Brabanter et al. (2010) (SIM6) 9 where the number of landmarks is again taken from the MEB solution\nOne may also simply use a very large number of randomly selected landmarks, but this can become prohibitive if N is large such that the calculation of N ×m similarities can be costly in memory and runtime. Further it can be very unattractive to have a larger m for the out of sample extension to new points. If for example costly alignment scores are used one is interested on having a very small m to avoid large costs in the test phase of the model.\nThe results of a 10-fold crossvalidation are shown in the Table 1 with runtimes given in Table 2. Here and in the following experiments the landmark selection was part of the crossvalidation scheme and the landmarks are selected on the training set only and the test data have been mapped to the approximated kernel space by the Nytröm kernel expansion (see e.g. Williams and Seeger (2000) ).\nFor the ball data set the data contain substantial information in the negative fraction of the eigenspectrum, accordingly one may expect that these eigenvalues should not be removed. This is also reflected in the results. In SIM4 and SIM 5 only the two dominating eigenvectors are kept such that the negative eigenvalues are removed, degenerating the prediction accuracy. The SIM3 encoding is a bit better, but the landmark optimization via k-means is not very effective for this dataset. Also the entropy approach in SIM6 was not very efficient. The SIM2 encoding has a substantial drop in the accuracy with respect to the unapproximated kernel but the intrinsic dimension of the dataset is very high and the m = 8 landmarks are enough to preserve the dominating positive and negative eigenvalues. The unapproximated kernel leads to perfect separation, clearly showing that the negative eigenspectrum contains discriminative information. The respective eigenvalue plots are provided in Figure 3.\nThe results show that the proposed MEB approach is capable in preserving the geometric information also for the negative (squared) eigendimensions while being quite simple. We believe that controlling the approximation accuracy of the kernel by in the MEB is much easier than selecting the number of clusters (per class) in k-means clustering. In fact it will almost always be sufficient to keep ≈ 0.01 to get reliable landmark sets whereas the number of clusters is very dataset dependent and not easy to choose. However, in contrast to the results\n9We use the implementation as provided by the authors in the LSSVM toolbox http:// www.esat.kuleuven.be/sista/lssvmlab/\nshown in Table 1 the approach by Zhang and Kwok (2010) is typically effective for a large variety of datasets also with indefinite kernels, given the number of landmarks is reasonable large and discriminating information is sufficiently provided in the dominating eigenvectors of the cluster solutions. For the protein data we observe similar results and the proposed approach, the k-means strategy and the entropy approach are effective. SIM4 and SIM5 is again substantially worse because four landmarks are in general not sufficient to represent these data from a discriminative point of view.\nFor the checker board and Gaussian data SIM2 and SIM3 are again close and SIM4 and SIM5 are substantially worse using only two landmark points. The entropy approach was efficient only for the Gaussian data, but failed for Checker which may be attributed to the strong multi-modality of the data.\nThe runtimes shown in Table 2 show already for the small data examples that the MEB approach is much faster then k-means or the entropy approach if the number of points gets larger which was already expected from the theoretical runtime complexity of these algorithms.\nIn another small experiment we analyzed the effect of the k-means based landmark selection Zhang and Kwok (2010) in more detail. We consider three Gaus-\nsians where one Gaussian has 500 points spread in two dimensions and two other Gaussians each with 20 points spread in another dimensions. All Gaussians are perfectly separated to each other located in a three dimensional space. To make the task more challenging we further add 7 dimensions with small noise contributions to the large Gaussian. The final data are given in a 10 dimensional space, whereby the small Gaussians are intrinsically low dimensional and the large Gaussian is 10 dimensional. with major contributions only in two dimensions. The points from the large Gaussian are labeled 0 and the other 1. Using the MEB approach we obtain 10 landmarks and the approximated kernel is sufficient to give a perfect prediction of 100% in a 10-fold crossvalidation with iKFD. Using the k-means or entropy based approach (with the same number of landmarks) the prediction accuracy drops down to≈ 84% and for random sampling we get a prediction accuracy in the same range of 83% - again with 10 landmarks . This can be explained by the behavior of k-means to assign the prototypes or landmarks to dense regions. It is hence more likely that after the k-means clustering (almost) all prototypes are used to represent the large Gaussian and no prototypes are left for the other classes. Due to the fact that the other classes are located in different dimensions with respect to the large Gaussian these dimensions are not any longer well represented and hence the respective classes are often missing in the approximated kernel (see Figure 4). This density related behavior is also known as magnification Villmann, T., Claussen, J. C., (2006) in the context of different vector quantization approaches. Hence using the unsupervised k-means landmark selection it\nAlgorithm 1 Proposed handling of indefinite kernels by the MEB approach 1. let k(x, y) be a symmetric (indefinite) similarity function (e.g. a sequence\nalignment) 2. for all labels c let Dc = {(xi, yi) : yi = c} 3. calculate the (indefinite) kernel matrix Kc using Dc and k(x, y) 4. if the kernel matrix is indefinite, apply a square operation on the small ma-\ntrix Kc by using Kc ·K>c 5. calculate ∀x∆(x) = k(x, x) (respectively for all Kc) 6. apply the MEB algorithm for each of the kernel matrices Kc with = 0.01\nand the respective subset of ∆ 7. combine all landmark indices obtained from the former step and calculate\nthe Nyström approximation using Eq. (9) 8. apply Ny-PCVM or Ny-iKFD using the approximated kernel matrix\ncan easily happen, that the majority of the data space is well presented but small classes are ignored - which is obviously a problem for a supervised data analysis.\nFrom these initial experiments we see that the proposed landmark selection scheme is sufficient to approximate the original kernel function for a supervised analysis as indicated by the prediction accuracy of the iKFD model and the SMSS value. We also see that the Nyström approximation can introduce substantial error if the data are not low rank (for checker) due to a more complicated kernel mapping aka similarity function. We would like to highlight again that without an advocated guess of the number of landmarks neither the k-means strategy nor the entropy approach are very efficient.\nIn the experiment in section 7 we will restrict our analysis to the proposed landmark selection using the MEB approach, the k-means strategy and the entropy based technique."
    }, {
      "heading" : "5. Large scale indefinite learning with PCVM and iKFD",
      "text" : "We now integrate the aforementioned Nyström approximation approaches and the supervised landmark selection into PCVM and iKFD. The modifications ensure that all matrices are processed with linear memory complexity and that the underlying algorithms have a linear runtime complexity. For both algorithms the initial input is the Nyström approximated kernel matrix with landmarks selected by using one of the formerly provided landmark selection schemes."
    }, {
      "heading" : "5.1. PCVM for large scale proximity data",
      "text" : "The PCVM parameters are optimized using the EM algorithm to prune the weight vector w during learning and hence the considered basis functions representing the model. We will now show multiple modifications of PCVM to integrate the Nyström approximation and to ensure that the memory and runtime complexity remains linear at all time. We refer to our method as Ny-PCVM. Initially the Ny-PCVM algorithm makes use of the matrices K1 = KN,m and K2 = K −1 m,m·K>1 obtained from the original kernel matrix using the Nyström landmark technique described above. Given a matrix X , we denote by X̂ the matrix formed from X containing elements at indices that have not yet been pruned out of the weight vector w. As an example, the matrices K̂1 = K w 6=0,· 1 , K̂2 = K ·,w 6=0 2 hold only those columns/rows of K1 or K2 not yet pruned out from the weight vector. We will use the same notation also for other variables. We denote the set of indices of m randomly selected landmarks by [m]. Finally, in contrast to the original PCVM formulation Chen et al. (2009a), in our notation we explicitly use the data labels - for example, instead of vector Φθ(x) we write Ξθ(x) ◦ y, where Ξθ(x) is the kernel vector of x without any label information, y is the label vector and ◦ is the element-wise multiplication.\nWe now adapt multiple equations of the original PCVM to integrate the Nyström approximated matrix. Beginning with the elements of vector (for a single training vector i) zθ:\nzi,θ = Ξθ(xi)(y ◦w) + b, (10)\nwe rewrite Eq.(10) in matrix notation for all training points:\nẑ = (((ŷ ◦ ŵ)>K̂1) ·K2)> + b (11)\nand further obtain column vectors H̄θ and the reduced form ¯̂Hθ, by using only the non-vanishing basis functions and the Nyström approximated matrices in Eq. (4). In the maximization step of the original PCVM the w are updated as (see Eq. (5)):\nwnew = M(MΦθ(x) >Φθ(x)M + IN)︸ ︷︷ ︸\nΥ\n−1 M(Φθ(x) >H̄θ − bΦθ(x)>I) (12)\nTo account for the now excluded labels we reformulate Equation (5) as:\nwnew = M(M(Ξθ(x) >Ξθ(x)ŷ >ŷ)M + IN )︸ ︷︷ ︸ Υ −1 M(ŷ>(Ξθ(x) >H̄θ)− bŷ>(Ξθ(x)>I))\nThe update equations of the weight vector include the calculation of a matrix inverse of Υ which was originally calculated using the Cholesky decomposition. To keep our objective of small matrices we will instead calculate the pseudoinverse of this matrix using a Nyström approximation of Υ. It should be noted at this point that the matrix Υ is psd by construction. We approximate Υ by selecting another set of m∗ landmarks from the indices of the not yet pruned weights and calculate the matrix Υ̃ = CNm∗W−1m∗,m∗C>Nm∗ in analogy to Eq (9) with submatrices: 10\nCNm∗ = EN [m] + ((K̂1 · (K2 · (K1 · K̂2·,[m∗]))(ŷ>ŷ[m∗])) ◦ √ 2ŵ) ◦ √\n2ŵ>[m∗]\nWm∗,m∗ = C −1 m∗,·\nWhere ◦ indicates (in analogy to its previous meaning) that each row of the left matrix is elementwise multiplied by the right vector and EN [m] is the matrix consisting of the m landmark columns of the N ×N identity matrix. The terms √ 2ŵ and √\n2ŵ>[m∗] are the entries of the diagonal matrix M as defined in Eq. (7) but now given in vector form.\nThese two matrices serve as the input of a Nyström approximation based pseudo-inverse (as discussed in sub section 3.1) and we obtain matrices V ∈ RN×r, U ∈ Rr×N and S ∈ Rr×r, where r ≤ m∗ is the rank of the pseudo inverse. Further we define two vectors v1 = ¯̂Hθ > ·K1 and v2 = I> ·K1. We obtain\nthe approximated weight update wnew = V · (S · U> · ( √\n2ŵ(ŷ(v1 · K̂2)> − b · ŷ(v2 · K̂2)>))) √ 2ŵ. The update of the bias is originally done as\nb = t(1 + tNt)−1t(I>H̄θ − I>Φθ(ŷŵ)) (13)\nwhich is replaced to: b = t(1 + tNt)−1t(I> ¯̂Hθ − I>((((ŷŵ)>K̂1) ·K2)>)) Subsequently the entries in ŵ which are close to zero are pruned out and the matrices K̂1 and K̂2 are modified accordingly."
    }, {
      "heading" : "5.2. Nyström based Indefinite Kernel Fisher Discriminant",
      "text" : "Given a Nyström approximated kernel matrix a few adaptations have to be made to obtain a valid iKFD formulation solely based on the Nyström approximated kernel, without any full matrix operations.\n10The number of landmarks m∗ is fixed to be 1% of |w| but not more then 500 landmarks. If the length of w drops below 100 points we use the original PCVM formulations.\nFirst we need to calculate the classwise means µ+ and µ− based on the row/column sums of the approximated input kernel matrix. This can be done by rather simple matrix operations on the two low rank matrices of the Nyström approximation of K. For better notation let us define the matrices KNm as Ψ and Kmm as Γ then for each row k of the matrix K we get the row/column sum as:\n∑ i [K̃]k,i = m∑ l=1 ( N∑ j=1 Ψj,·Γ −1 ) Ψ>l,k (14)\nThis can obviously also be done in a single matrix operation for all rows in a batch, with linear complexity only. Based on these mean estimates we can calculate Eq. (2). In a next step we need to calculate a squared approximated kernel matrix for the positive and the negative class with removed means µ+ or µ− respectively. For the positive class with n+ entries, we can define a new Nyström approximated (squared) matrix with subtracted mean as :\nK̂+N,m = KN,m ·K −1 m,m · (K>I+,m ·KI+,m) ·K −1 m,m ·K>m,m − µ+ · µ>+ · n+ (15)\nAn equivalent term can be derived for the negative class providing K̂−N,m. It should be noted that no obtained matrix in Eq (15) has more than N ×m entries. Finally K̂+N,m and K̂ − N,m are combined to approximate the within class matrix as shown in Eq. (3). From the derivation in Haasdonk and Pekalska (2008) we know, that only the eigenvector of the Nyström approximated kernel matrix based on K̂N,m = K̂+N,m+K̂ − N,m are needed. Using a Nyström based eigen-decomposition (explained before) on K̂N,m we obtain:\nα = C · A−1 · (C ′ · (µ+ − µ−))\nwhere C contains the eigenvectors andA the eigenvalues of K̂N,m. Instead ofA−1 one can use the pseudo-inverse. The bias term b is obtained as b = −α>(µ+ + µ−)/2."
    }, {
      "heading" : "6. Complexity analysis",
      "text" : "The original iKFD update rules have costs of O(N3) and memory storage O(N2), where N is the number of points. The Ny-iKFD may involve the extra Nyström approximation of the kernel matrix to obtain KN,m and K−1m,m, if not already given. If we have m landmarks, m N , this gives costs ofO(mN) for the\nfirst matrix and O(m3) for the second, due to the matrix inversion. Further both matrices are multiplied within the optimization so we getO(m2N). Similarly, the matrix inversion of the original iKFD withO(N3) is reduced toO(m2N)+O(m3) due to the Nyström approximation of the pseudo-inverse. If we assume m N the overall runtime and memory complexity of Ny-iKFD is linear in N . For the Ny-PCVM we obtain a similar analysis as shown in Schleif et al. (2015a) but with extra costs to calculate the Nyström approximated SVD. Additionally, Ny-PCVM uses an iterative optimization scheme to optimize and sparsify w with constant costs CI , as the number of iterations. Accordingly Ny-iKFD and Ny-PCVM have both linear memory and runtime complexity O(N), but Ny-PCVM maybe slower than Ny-iKFD due to extra overhead costs."
    }, {
      "heading" : "7. Experiments",
      "text" : "We compare iKFD, Ny-iKFD, Ny-PCVM and PCVM on various larger indefinite proximity data. In contrast to many standard kernel approaches, for iKFD and PCVM, the indefinite kernel matrices need not to be corrected by costly eigenvalue correction Chen et al. (2009c); Schleif and Gisbrecht (2013) 11\nFurther the iKFD and PCVM provides direct access to probabilistic classification decisions. First we show a small simulated experiment for two Gaussians which exist in an intrinsically two dimensional pseudo-Euclidean space R(1,1). The plot in Figure 5 shows a typical result for the obtained decision planes using the iKFD or Ny-iKFD. The Gaussians are slightly overlapping and both approaches achieve a good separation with 93.50% and 88.50% prediction accuracy, respectively.\nSubsequently we consider a few public available datasets for some real life experiments. The data are Zongker (2000pts, 10 classes) and Proteom (2604pts, 53 classes (restricted to classes with at least 10 entries)) from Duin (2012); Chromo (4200pt, 21 classes) from Neuhaus and Bunke (2006) and the SwissProt database Swiss (10988 pts, 30 classes) from Boeckmann et al. (2003), (version 10/2010, reduced to prosite labeled classes with at least 100 entries ). Further we used the Sonatas data (1068pts, 5 classes) taken from Mokbel et al. (2009). All data are processed as indefinite kernels and the landmarks are selected using the respective landmark selection schemes. The mean number of Nyström landmarks as obtained by the MEB approach is given in brackets after the dataset label. For all\n11In Schleif and Tino (2015) various correction methods have been studied on the same data indicating that eigenvalue corrections may be helpful.\nexperiments we report mean and standard errors as obtained by a 10 fold crossvalidation. For PCVM we fixed the upper number of optimization cycles to 500. The probabilistic outputs can be directly used to allow for a reject region but can also be used to provide alternative classification decisions e.g. in a ranking framework\nIn Table 4, 3 and Table 6 we show the results for different non-metric proximity datasets using Ny-PCVM, PCVM and iKFD or Ny-iKFD. The overall best results for a dataset are underlined and the best approximations are highlighted in bold. Considering Table 4 and Table 3 we see that iKFD and PCVM are similarly effective with slightly better results for iKFD. The Nyström approximation of the kernel matrix only, often leads to a in general small decrease of the accuracy, but the additional approximation step, in the algorithm itself, does not substantially decrease the prediction accuracy further12.\nConsidering the overall results in Table 4 and Table 3 the approximations used in the algorithms Ny-iKFD and Ny-PCVM appear to be effective. The runtime analysis in Table 6 clearly shows that the classical iKFD is very complex. As ex-\n12Also the runtime and model complexity are similar and therefore not reported in the following.\npected, the integration of the Nyström approximation leads to substantial speedups. Larger datasets like the Swiss data with ≈ 10.000 entries could not be analyzed by iKFD or PCVM before. We also see that the landmark selection scheme using MEB is slightly more effective than by using k-means but without the need to tune the number of clusters (landmarks). The entropy approach is similar efficient than the k-means strategy but more costly due to the iterative optimization of the landmark set and the respective eigen-decompositions (see Brabanter et al. (2010)).\nThe PCVM is focusing on a sparse parameter vectorw in contrast to the iKFD. For the iKFD most training points are also used in the model (≥ 94%) whereas for Ny-PCVM often less than 5% are kept in general as shown in Table 7. In practice it is often costly to calculate the non-metric proximity measures like sequence alignments and also a large number of kernel expansions should be avoided. Accordingly sparse models are very desirable. Considering the runtime again NyPCVM and Ny-iKFD are in general faster than the original algorithms, typically by at least a magnitude. the PCVM and Ny-PCVM are also very fast in the test case or out-of sample extension due to the inherent model sparsity."
    }, {
      "heading" : "8. Conclusions",
      "text" : "We presented an alternative formulation of the iKFD and PCVM employing the Nyström approximation. We also provided an alternative way to identify the landmark points of the Nyström approximation in cases where the objective is\na supervised problem. Our results indicate that in general the MEB approach is similar efficient compared to the k-means clustering or the entropy strategy but with less effort and almost parameter free. We found that Ny-iKFD is competitive in the prediction accuracy with the original iKFD and alternative approaches, while taking substantially less memory and runtime but being less sparse then NyPCVM. The Ny-iKFD and Ny-PCVM provides now an effective way to obtain a probabilistic classification model for medium to large psd and non-psd datasets, in batch mode with linear runtime and memory complexity. If sparsity is not an issue one may prefer Ny-iKFD which is slightly better in the prediction accuracy then Ny-PCVM. Using the presented approach we believe that iKFD is now applicable for realistic problems and may get a larger impact then before. In future work it could be interesting to incorporate sparsity concepts into iKFD and NyiKFD similar as shown for classical KFD in Diethe et al. (2009).\nImplementation: The Nyström approximation for iKFD is provided at http:// www.techfak.uni-bielefeld.de/˜fschleif/source/ny_ikfd.tgz and the PCVM/Ny-PCVM code can be found at https://mloss.org/software/ view/610/.\nAcknowledgment: A Marie Curie Intra-European Fellowship (IEF): FP7-PEOPLE-\n2012-IEF (FP7-327791-ProMoS) and support from the Cluster of Excellence 277 Cognitive Interaction Technology funded by the German Excellence Initiative is gratefully acknowledged. PT was supported by the EPSRC grant EP/L000296/1, ”Personalized Health Care through Learning in the Model Space”. We would like to thank R. Duin, Delft University for various support with distools and prtools and Huanhuan Chen,University of Science and Technology of China, for providing support with the Probabilistic Classification Vector Machine."
    } ],
    "references" : [ {
      "title" : "Support vector machines with indefinite kernels",
      "author" : [ "I.M. Alabdulmohsin", "X. Gao", "X. Zhang" ],
      "venue" : "Proceedings of the Sixth Asian",
      "citeRegEx" : "Alabdulmohsin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Alabdulmohsin et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimal core-sets for balls",
      "author" : [ "M. Badoiu", "K.L. Clarkson" ],
      "venue" : "Comput. Geom",
      "citeRegEx" : "Badoiu and Clarkson,? \\Q2008\\E",
      "shortCiteRegEx" : "Badoiu and Clarkson",
      "year" : 2008
    }, {
      "title" : "A theory of learning with similarity functions",
      "author" : [ "Balcan", "M.-F", "A. Blum", "N. Srebro" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "Balcan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2008
    }, {
      "title" : "Laplacian eigenmaps for dimensionality reduction and data representation",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "Belkin and Niyogi,? \\Q2003\\E",
      "shortCiteRegEx" : "Belkin and Niyogi",
      "year" : 2003
    }, {
      "title" : "The SWISS-PROT protein knowledgebase and its supplement TrEMBL in 2003",
      "author" : [ "B. Boeckmann", "A. Bairoch", "R. Apweiler", "Blatter", "M.-C", "A. Estreicher", "E. Gasteiger", "M. Martin", "K. Michoud", "C. O’Donovan", "I. Phan", "S. Pilbout", "M. Schneider" ],
      "venue" : "Nucleic Acids Research",
      "citeRegEx" : "Boeckmann et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Boeckmann et al\\.",
      "year" : 2003
    }, {
      "title" : "Optimized fixed-size kernel models for large data sets",
      "author" : [ "K.D. Brabanter", "J.D. Brabanter", "J. Suykens", "B.D. Moor" ],
      "venue" : "Computational Statistics & Data Analysis",
      "citeRegEx" : "Brabanter et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Brabanter et al\\.",
      "year" : 2010
    }, {
      "title" : "Probabilistic classification vector machines",
      "author" : [ "H. Chen", "P. Tino", "X. Yao" ],
      "venue" : "IEEE Transactions on Neural Networks",
      "citeRegEx" : "Chen et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient probabilistic classification vector machine with incremental basis function selection",
      "author" : [ "H. Chen", "P. Tino", "X. Yao" ],
      "venue" : "IEEE TNN-LS",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Families of alpha- beta- and gamma- divergences: Flexible and robust measures of similarities",
      "author" : [ "A. Cichocki", "Amari", "S.-I" ],
      "venue" : "Entropy",
      "citeRegEx" : "Cichocki et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cichocki et al\\.",
      "year" : 2010
    }, {
      "title" : "Matching pursuit kernel fisher discriminant analysis",
      "author" : [ "T. Diethe", "Z. Hussain", "D.R. Hardoon", "J. Shawe-Taylor" ],
      "venue" : "Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Diethe et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Diethe et al\\.",
      "year" : 2009
    }, {
      "title" : "A modified hausdorff distance for object matching",
      "author" : [ "M. Dubuisson", "A. Jain", "Oct" ],
      "venue" : "In: Pattern Recognition, 1994. Vol. 1 - Conference A: Computer Vision amp; Image Processing., Proceedings of the 12th IAPR International Conference on. Vol. 1. pp. 566–568 vol.1.",
      "citeRegEx" : "Dubuisson et al\\.,? 1994",
      "shortCiteRegEx" : "Dubuisson et al\\.",
      "year" : 1994
    }, {
      "title" : "Non-euclidean dissimilarities: Causes and informativeness",
      "author" : [ "R.P.W. Duin", "E. Pekalska" ],
      "venue" : "Joint IAPR International Workshop,",
      "citeRegEx" : "Duin and Pekalska,? \\Q2010\\E",
      "shortCiteRegEx" : "Duin and Pekalska",
      "year" : 2010
    }, {
      "title" : "Metric and non-metric proximity transformations at linear costs",
      "author" : [ "A. Gisbrecht", "Schleif", "F.-M" ],
      "venue" : "Neurocomputing to appear",
      "citeRegEx" : "Gisbrecht et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gisbrecht et al\\.",
      "year" : 2015
    }, {
      "title" : "Revisiting the nyström method for improved large-scale machine learning",
      "author" : [ "A. Gittens", "M.W. Mahoney" ],
      "venue" : "CoRR abs/1303.1849",
      "citeRegEx" : "Gittens and Mahoney,? \\Q2013\\E",
      "shortCiteRegEx" : "Gittens and Mahoney",
      "year" : 2013
    }, {
      "title" : "A unified approach to pattern recognition",
      "author" : [ "L. Goldfarb" ],
      "venue" : "Pattern Recognition",
      "citeRegEx" : "Goldfarb,? \\Q1984\\E",
      "shortCiteRegEx" : "Goldfarb",
      "year" : 1984
    }, {
      "title" : "Feature space interpretation of svms with indefinite kernels",
      "author" : [ "B. Haasdonk" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citeRegEx" : "Haasdonk,? \\Q2005\\E",
      "shortCiteRegEx" : "Haasdonk",
      "year" : 2005
    }, {
      "title" : "Tangent distance kernels for support vector machines",
      "author" : [ "B. Haasdonk", "D. Keysers" ],
      "venue" : "ICPR",
      "citeRegEx" : "Haasdonk and Keysers,? \\Q2002\\E",
      "shortCiteRegEx" : "Haasdonk and Keysers",
      "year" : 2002
    }, {
      "title" : "Indefinite kernel fisher discriminant",
      "author" : [ "B. Haasdonk", "E. Pekalska" ],
      "venue" : "International Conference on Pattern Recognition (ICPR",
      "citeRegEx" : "Haasdonk and Pekalska,? \\Q2008\\E",
      "shortCiteRegEx" : "Haasdonk and Pekalska",
      "year" : 2008
    }, {
      "title" : "Learning with idealized kernels",
      "author" : [ "J.T. Kwok", "I.W. Tsang" ],
      "venue" : "Machine Learning, Proceedings of the Twentieth International Conference (ICML",
      "citeRegEx" : "Kwok and Tsang,? \\Q2003\\E",
      "shortCiteRegEx" : "Kwok and Tsang",
      "year" : 2003
    }, {
      "title" : "Shape classification using the inner-distance",
      "author" : [ "H. Ling", "D.W. Jacobs" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell",
      "citeRegEx" : "Ling and Jacobs,? \\Q2007\\E",
      "shortCiteRegEx" : "Ling and Jacobs",
      "year" : 2007
    }, {
      "title" : "Learning svm in krein spaces",
      "author" : [ "G. Loosli", "S. Canu", "C. Ong" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on PP",
      "citeRegEx" : "Loosli et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Loosli et al\\.",
      "year" : 2015
    }, {
      "title" : "Graph-based representation of symbolic musical data",
      "author" : [ "B. Mokbel", "A. Hasenfuss", "B. Hammer" ],
      "venue" : "GbRPR",
      "citeRegEx" : "Mokbel et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mokbel et al\\.",
      "year" : 2009
    }, {
      "title" : "Median variants of learning vector quantization for learning of dissimilarity data",
      "author" : [ "D. Nebel", "B. Hammer", "K. Frohberg", "T. Villmann" ],
      "venue" : "NeurocomputingCited By",
      "citeRegEx" : "Nebel et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nebel et al\\.",
      "year" : 2015
    }, {
      "title" : "Edit distance based kernel functions for structural pattern classification",
      "author" : [ "M. Neuhaus", "H. Bunke" ],
      "venue" : "Pattern Recognition",
      "citeRegEx" : "Neuhaus and Bunke,? \\Q2006\\E",
      "shortCiteRegEx" : "Neuhaus and Bunke",
      "year" : 2006
    }, {
      "title" : "The dissimilarity representation for pattern recognition",
      "author" : [ "E. Pekalska", "R. Duin" ],
      "venue" : "World Scientific",
      "citeRegEx" : "Pekalska and Duin,? \\Q2005\\E",
      "shortCiteRegEx" : "Pekalska and Duin",
      "year" : 2005
    }, {
      "title" : "Kernel discriminant analysis for positive definite and indefinite kernels",
      "author" : [ "E. Pekalska", "B. Haasdonk" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citeRegEx" : "Pekalska and Haasdonk,? \\Q2009\\E",
      "shortCiteRegEx" : "Pekalska and Haasdonk",
      "year" : 2009
    }, {
      "title" : "Probabilistic classification vector machine at large scale",
      "author" : [ "Schleif", "F.-M", "A.Gisbrecht", "P. Tino" ],
      "venue" : "Proceedings of ESANN",
      "citeRegEx" : "Schleif et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schleif et al\\.",
      "year" : 2015
    }, {
      "title" : "Data analysis of (non-)metric proximities at linear costs",
      "author" : [ "Schleif", "F.-M", "A. Gisbrecht" ],
      "venue" : "Proceedings of SIMBAD",
      "citeRegEx" : "Schleif et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Schleif et al\\.",
      "year" : 2013
    }, {
      "title" : "2015b. Large scale indefinite kernel fisher discriminant",
      "author" : [ "Schleif", "F.-M", "A. Gisbrecht", "P. Tino" ],
      "venue" : "Proceedings of Simbad",
      "citeRegEx" : "Schleif et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schleif et al\\.",
      "year" : 2015
    }, {
      "title" : "Indefinite proximity learning - a review",
      "author" : [ "Schleif", "F.-M", "P. Tino" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "Schleif et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schleif et al\\.",
      "year" : 2015
    }, {
      "title" : "Kernel Methods for Pattern Analysis and Discovery",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : null,
      "citeRegEx" : "Shawe.Taylor and Cristianini,? \\Q2004\\E",
      "shortCiteRegEx" : "Shawe.Taylor and Cristianini",
      "year" : 2004
    }, {
      "title" : "Memory efficient kernel approximation",
      "author" : [ "S. Si", "C. Hsieh", "I.S. Dhillon" ],
      "venue" : "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing,",
      "citeRegEx" : "Si et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Si et al\\.",
      "year" : 2014
    }, {
      "title" : "Identification of common molecular subsequences",
      "author" : [ "T.F. Smith", "M.S.", "Waterman", "Mar." ],
      "venue" : "Journal of molecular biology 147 (1), 195–197.",
      "citeRegEx" : "Smith et al\\.,? 1981",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 1981
    }, {
      "title" : "Core vector machines: Fast SVM training on very large data sets",
      "author" : [ "I.W. Tsang", "J.T. Kwok", "P. Cheung" ],
      "venue" : "Journal of Machine Learning Research 6,",
      "citeRegEx" : "Tsang et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Tsang et al\\.",
      "year" : 2005
    }, {
      "title" : "Magnification control in self-organizing maps and neural gas",
      "author" : [ "T. Villmann", "J.C. Claussen" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "Villmann and Claussen,? \\Q2006\\E",
      "shortCiteRegEx" : "Villmann and Claussen",
      "year" : 2006
    }, {
      "title" : "Using the nyström method to speed up kernel machines",
      "author" : [ "C.K.I. Williams", "M. Seeger" ],
      "venue" : "NIPS",
      "citeRegEx" : "Williams and Seeger,? \\Q2000\\E",
      "shortCiteRegEx" : "Williams and Seeger",
      "year" : 2000
    }, {
      "title" : "A novel indefinite kernel dimensionality reduction algorithm: Weighted generalized indefinite kernel discriminant analysis",
      "author" : [ "J. Yang", "L. Fan" ],
      "venue" : "Neural Processing Letters,",
      "citeRegEx" : "Yang and Fan,? \\Q2013\\E",
      "shortCiteRegEx" : "Yang and Fan",
      "year" : 2013
    }, {
      "title" : "Clustered nyström method for large scale manifold learning and dimension reduction",
      "author" : [ "K. Zhang", "J.T. Kwok" ],
      "venue" : "IEEE Transactions on Neural Networks",
      "citeRegEx" : "Zhang and Kwok,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang and Kwok",
      "year" : 2010
    }, {
      "title" : "Improved Nystrom low-rank approximation and error analysis",
      "author" : [ "K. Zhang", "I.W. Tsang", "J.T. Kwok" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning. ICML ’08. ACM,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 32,
      "context" : "Domain specific proximity measures, like alignment scores in bioinformatics Smith et al. (1981), the modified Hausdorff-distance for structural pattern recog-",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "nition Dubuisson and Jain (1994), shape retrieval measures like the inner distance Ling and Jacobs (2007) and many other ones generate non-metric or indefinite similarities or dissimilarities.",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 11,
      "context" : "Only few machine learning methods have been proposed for non-metric proximity data, like the indefinite kernel Fisher discriminant (iKFD) Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009), the probabilistic classification vector machine (PCVM) Chen et al.",
      "startOffset" : 138,
      "endOffset" : 167
    }, {
      "referenceID" : 11,
      "context" : "Only few machine learning methods have been proposed for non-metric proximity data, like the indefinite kernel Fisher discriminant (iKFD) Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009), the probabilistic classification vector machine (PCVM) Chen et al.",
      "startOffset" : 138,
      "endOffset" : 197
    }, {
      "referenceID" : 4,
      "context" : "Only few machine learning methods have been proposed for non-metric proximity data, like the indefinite kernel Fisher discriminant (iKFD) Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009), the probabilistic classification vector machine (PCVM) Chen et al. (2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al.",
      "startOffset" : 253,
      "endOffset" : 273
    }, {
      "referenceID" : 4,
      "context" : "Only few machine learning methods have been proposed for non-metric proximity data, like the indefinite kernel Fisher discriminant (iKFD) Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009), the probabilistic classification vector machine (PCVM) Chen et al. (2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al.",
      "startOffset" : 253,
      "endOffset" : 363
    }, {
      "referenceID" : 0,
      "context" : "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G.",
      "startOffset" : 99,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied.",
      "startOffset" : 99,
      "endOffset" : 166
    }, {
      "referenceID" : 0,
      "context" : "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity.",
      "startOffset" : 99,
      "endOffset" : 587
    }, {
      "referenceID" : 0,
      "context" : "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity. In Schleif et al. (2015a); Gisbrecht and Schleif (2015) the authors proposed a few Nyström based (see e.",
      "startOffset" : 99,
      "endOffset" : 721
    }, {
      "referenceID" : 0,
      "context" : "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity. In Schleif et al. (2015a); Gisbrecht and Schleif (2015) the authors proposed a few Nyström based (see e.",
      "startOffset" : 99,
      "endOffset" : 751
    }, {
      "referenceID" : 0,
      "context" : "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity. In Schleif et al. (2015a); Gisbrecht and Schleif (2015) the authors proposed a few Nyström based (see e.g. Williams and Seeger (2000)) approximation techniques to improve the scalability of the PCVM for low rank matrices.",
      "startOffset" : 99,
      "endOffset" : 829
    }, {
      "referenceID" : 0,
      "context" : "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity. In Schleif et al. (2015a); Gisbrecht and Schleif (2015) the authors proposed a few Nyström based (see e.g. Williams and Seeger (2000)) approximation techniques to improve the scalability of the PCVM for low rank matrices. The suggested techniques use the Nyström approximation in a nontrivial way to provide exact eigenvalue estimations also for indefinite kernel matrices. This approach is very generic and can be applied in different algorithms. In this contribution we further extend our previous work and not only derive a low rank approximation of the indefinite kernel Fisher discriminant, but also address the landmark selection from a novel view point. The obtained Ny-iKFD approach is linear in runtime and memory consumption for low rank matrices. The formulation is exact if the rank of the matrix equals the number of independent landmarks points. The selection of the landmarks of the Nyström approximation is a critical point addressed in previous work (see e.g. Zhang and Kwok (2010); Si et al.",
      "startOffset" : 99,
      "endOffset" : 1694
    }, {
      "referenceID" : 0,
      "context" : "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity. In Schleif et al. (2015a); Gisbrecht and Schleif (2015) the authors proposed a few Nyström based (see e.g. Williams and Seeger (2000)) approximation techniques to improve the scalability of the PCVM for low rank matrices. The suggested techniques use the Nyström approximation in a nontrivial way to provide exact eigenvalue estimations also for indefinite kernel matrices. This approach is very generic and can be applied in different algorithms. In this contribution we further extend our previous work and not only derive a low rank approximation of the indefinite kernel Fisher discriminant, but also address the landmark selection from a novel view point. The obtained Ny-iKFD approach is linear in runtime and memory consumption for low rank matrices. The formulation is exact if the rank of the matrix equals the number of independent landmarks points. The selection of the landmarks of the Nyström approximation is a critical point addressed in previous work (see e.g. Zhang and Kwok (2010); Si et al. (2014); Brabanter et al.",
      "startOffset" : 99,
      "endOffset" : 1712
    }, {
      "referenceID" : 0,
      "context" : "(2009a) or the indefinite Support Vector Machine (iSVM) in different formulations Haasdonk (2005); Alabdulmohsin et al. (2014); Loosli, G., Canu, S., Ong, C., (2015). For the PCVM the provided kernel evaluations are considered only as basis functions and no mercer conditions are implied. In contrast to the iKFD the PCVM is a sparse probabilistic kernel classifier pruning unused basis functions during training, applicable to arbitrary positive definite and indefinite kernel matrices. A recent review about learning with indefinite proximities can be found in Schleif and Tino (2015). While being very efficient these methods do not scale to larger datasets with in general cubic complexity. In Schleif et al. (2015a); Gisbrecht and Schleif (2015) the authors proposed a few Nyström based (see e.g. Williams and Seeger (2000)) approximation techniques to improve the scalability of the PCVM for low rank matrices. The suggested techniques use the Nyström approximation in a nontrivial way to provide exact eigenvalue estimations also for indefinite kernel matrices. This approach is very generic and can be applied in different algorithms. In this contribution we further extend our previous work and not only derive a low rank approximation of the indefinite kernel Fisher discriminant, but also address the landmark selection from a novel view point. The obtained Ny-iKFD approach is linear in runtime and memory consumption for low rank matrices. The formulation is exact if the rank of the matrix equals the number of independent landmarks points. The selection of the landmarks of the Nyström approximation is a critical point addressed in previous work (see e.g. Zhang and Kwok (2010); Si et al. (2014); Brabanter et al. (2010)).",
      "startOffset" : 99,
      "endOffset" : 1737
    }, {
      "referenceID" : 26,
      "context" : "These techniques have been applied in Schleif et al. (2015a) and Schleif, F.",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 26,
      "context" : "These techniques have been applied in Schleif et al. (2015a) and Schleif, F.-M., Gisbrecht, A., Tino, P., (2015b) in a proof of concept setting, to obtain approximate models for the Probabilistic Classification Vector Machine and the Indefinite Fisher Kernel Discriminant analysis using a random landmark selection scheme.",
      "startOffset" : 38,
      "endOffset" : 114
    }, {
      "referenceID" : 26,
      "context" : "These techniques have been applied in Schleif et al. (2015a) and Schleif, F.-M., Gisbrecht, A., Tino, P., (2015b) in a proof of concept setting, to obtain approximate models for the Probabilistic Classification Vector Machine and the Indefinite Fisher Kernel Discriminant analysis using a random landmark selection scheme. This work is substantially extended and detailed in this article with a specific focus on indefinite kernels, only. A novel landmark selection scheme is proposed. Based on this new landmark selection scheme we provide detailed new experimental results and compare to alternative landmark selection approaches. Structure of the paper: First we give some basic notations necessary in the subsequent derivations. Then we review iKFD and PCVM as well as some approximation concepts proposed by the authors in Schleif et al. (2015a) which are based on the well known Nyström approximation.",
      "startOffset" : 38,
      "endOffset" : 851
    }, {
      "referenceID" : 30,
      "context" : "Kernelized methods process the embedded data points in a feature space utilizing only the inner products 〈·, ·〉H (kernel trick) (Shawe-Taylor and Cristianini, 2004), without the need to explicitly calculate φ.",
      "startOffset" : 128,
      "endOffset" : 164
    }, {
      "referenceID" : 32,
      "context" : "Indefinite kernels are typically observed by means of domain specific nonmetric similarity functions (such as alignment functions used in biology (Smith et al., 1981)), by specific kernel functions - e.",
      "startOffset" : 146,
      "endOffset" : 166
    }, {
      "referenceID" : 16,
      "context" : "the Manhattan kernel k(x,x′) = −||x−x||1, tangent distance kernel (Haasdonk and Keysers, 2002) or divergence measures plugged into standard kernel functions (Cichocki and Amari, 2010).",
      "startOffset" : 66,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "Another source of non-psd kernels are noise artifacts on standard kernel functions (Haasdonk, 2005).",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 14,
      "context" : "Given a symmetric dissimilarity matrix with zero diagonal 1, an embedding of the data in a pseudo-Euclidean vector space determined by the eigenvector decomposition of the associated similarity matrix S is always possible (Goldfarb, 1984) 2 Given the eigendecomposition of S, S = UΛU>, we can compute the corresponding vectorial representation V in the pseudo-Euclidean space by V = Up+q+z |Λp+q+z| (1)",
      "startOffset" : 222,
      "endOffset" : 238
    }, {
      "referenceID" : 24,
      "context" : "A detailed presentation of similarity and dissimilarity measures, and mathematical aspects of metric and non-metric spaces is provided in (Pekalska and Duin, 2005).",
      "startOffset" : 138,
      "endOffset" : 163
    }, {
      "referenceID" : 24,
      "context" : "2 The associated similarity matrix can be obtained by double centering (Pekalska and Duin, 2005) of the (squared) dissimilarity matrix.",
      "startOffset" : 71,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : "Indefinite Fisher and kernel quadratic discriminant In Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009) the indefinite kernel Fisher discriminant analysis (iKFD) and indefinite kernel quadratic discriminant analysis (iKQD) was proposed focusing on binary classification problems, recently extended by a weighting scheme in Yang and Fan (2013)3.",
      "startOffset" : 55,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "Indefinite Fisher and kernel quadratic discriminant In Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009) the indefinite kernel Fisher discriminant analysis (iKFD) and indefinite kernel quadratic discriminant analysis (iKQD) was proposed focusing on binary classification problems, recently extended by a weighting scheme in Yang and Fan (2013)3.",
      "startOffset" : 55,
      "endOffset" : 114
    }, {
      "referenceID" : 15,
      "context" : "Indefinite Fisher and kernel quadratic discriminant In Haasdonk and Pekalska (2008); Pekalska and Haasdonk (2009) the indefinite kernel Fisher discriminant analysis (iKFD) and indefinite kernel quadratic discriminant analysis (iKQD) was proposed focusing on binary classification problems, recently extended by a weighting scheme in Yang and Fan (2013)3.",
      "startOffset" : 55,
      "endOffset" : 353
    }, {
      "referenceID" : 15,
      "context" : "In Haasdonk and Pekalska (2008) it is shown that the Fisher Discriminant in the pE space ∈ R is identical to the Fisher Discriminant in the associated Euclidean space R.",
      "startOffset" : 3,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "can be used for KQD as well as the indefinite kernel PCA Pekalska and Haasdonk (2009).",
      "startOffset" : 70,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "The Expectation Maximization (EM) implementation of PCVM Chen et al. (2014) uses the probit link function, i.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "The Expectation Maximization (EM) implementation of PCVM Chen et al. (2014) uses the probit link function, i.e. Ψ(x) = ∫ x −∞N (t|0, 1)dt,where Ψ(x) is the cumulative distribution of the normal distribution N (0, 1). We get: l(x; w, b) = Ψ (∑N i=1wiφi(x) + b ) = Ψ (Φ(x)w + b) In the PCVM formulation Chen et al. (2009a), a truncated Gaussian prior Nt with support on [0,∞) and mode at 0 is introduced for each weight wi and a zero-mean Gaussian prior is adopted for the bias b.",
      "startOffset" : 57,
      "endOffset" : 321
    }, {
      "referenceID" : 6,
      "context" : "For further details can be found in Chen et al. (2009a). Even though kernel machines and their derivatives have shown great promise in practical application, their scope is somehow limited by the fact that the computational complexity grows rapidly with the size of the kernel matrix (number of data items).",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 35,
      "context" : "The Nyström approximation technique has been proposed in the context of kernel methods in (Williams and Seeger, 2000).",
      "startOffset" : 90,
      "endOffset" : 117
    }, {
      "referenceID" : 35,
      "context" : "Strategies how to chose the landmarks have recently been addressed in Zhang and Kwok (2010); Zhang et al.",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 35,
      "context" : "Strategies how to chose the landmarks have recently been addressed in Zhang and Kwok (2010); Zhang et al. (2008) and Gittens and Mahoney (2013); Brabanter et al.",
      "startOffset" : 70,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "(2008) and Gittens and Mahoney (2013); Brabanter et al.",
      "startOffset" : 11,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "(2008) and Gittens and Mahoney (2013); Brabanter et al. (2010). We denote these rows by Km,N .",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 36,
      "context" : "In Zhang and Kwok (2010) multiple strategies for landmark selection have been studied and a clustering based approach was suggested to find the specific landmarks.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 36,
      "context" : "In Zhang and Kwok (2010) multiple strategies for landmark selection have been studied and a clustering based approach was suggested to find the specific landmarks. Thereby the number of landmarks is a user defined parameter and a classical kmeans algorithm is applied on the kernel matrix to identify characteristic landmark points in the empirical feature space. This approach is quite effective (see Zhang and Kwok (2010)), with some small improvements using an advanced clustering scheme as shown in Si et al.",
      "startOffset" : 3,
      "endOffset" : 424
    }, {
      "referenceID" : 31,
      "context" : "This approach is quite effective (see Zhang and Kwok (2010)), with some small improvements using an advanced clustering scheme as shown in Si et al. (2014). We will use it as a baseline for an advanced landmark section approach.",
      "startOffset" : 139,
      "endOffset" : 156
    }, {
      "referenceID" : 31,
      "context" : "This approach is quite effective (see Zhang and Kwok (2010)), with some small improvements using an advanced clustering scheme as shown in Si et al. (2014). We will use it as a baseline for an advanced landmark section approach. Further we will also consider a pure random selection strategy. It should be noted that the formulation given in Zhang and Kwok (2010) takes the full kernel matrix as an input into the k-means clustering.",
      "startOffset" : 139,
      "endOffset" : 364
    }, {
      "referenceID" : 18,
      "context" : "Kwok and Tsang (2003)).",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 18,
      "context" : "Kwok and Tsang (2003)). We will however not learn an idealized kernel as proposed in Kwok and Tsang (2003), which by itself is very costly for large scale matrices, but provide a landmark selection strategy leading into a similar direction.",
      "startOffset" : 0,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "Similar as in Balcan et al. (2008) we assume that a good linear classifier can be obtained from a given similarity function (or kernel) if the similarities between classes are much lower than those within the same class.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "Similar as in Balcan et al. (2008) we assume that a good linear classifier can be obtained from a given similarity function (or kernel) if the similarities between classes are much lower than those within the same class. However the score s(K̂,K) is only a rough estimate. It will likely work well for datasets which can easily be modeled by conceptually related classifiers focusing e.g. on exemplar based representations. The median classifier proposed in Nebel et al. (2015) is such a simple classifier.",
      "startOffset" : 14,
      "endOffset" : 478
    }, {
      "referenceID" : 2,
      "context" : "Similar as in Balcan et al. (2008) we assume that a good linear classifier can be obtained from a given similarity function (or kernel) if the similarities between classes are much lower than those within the same class. However the score s(K̂,K) is only a rough estimate. It will likely work well for datasets which can easily be modeled by conceptually related classifiers focusing e.g. on exemplar based representations. The median classifier proposed in Nebel et al. (2015) is such a simple classifier. In its simplest form it identifies for each class a single basis function or prototype, showing maximum margin with respect to the other prototypes with different class labels. In Nebel et al. (2015) it was shown that such a classifier can be very efficient also for a variety of classification problems.",
      "startOffset" : 14,
      "endOffset" : 707
    }, {
      "referenceID" : 1,
      "context" : "in Badoiu and Clarkson (2008) that the minimum enclosing ball can be approximated with quality in (worst case) linear time using an algorithm which requires only a constant subset of the receptive field Rj , the core set.",
      "startOffset" : 3,
      "endOffset" : 30
    }, {
      "referenceID" : 36,
      "context" : "It should be noted that if we use K̂ as an input of a kernel k-means algorithm this is equivalent as using K as the input of the classical k-means with Euclidean distance as suggested in Zhang and Kwok (2010). The proposed supervised landmark selection using MEB does not only identify a good estimate for the number of landmarks but also ensures that the landmarks are sufficient to explain the data space.",
      "startOffset" : 187,
      "endOffset" : 209
    }, {
      "referenceID" : 1,
      "context" : "Especially only those points are included in the MEB solution which are needed to explain the sphere such that redundancy within this set is avoided Badoiu and Clarkson (2008). Therefore for each class the MEB solutions provides a local span of the underlying eigen-space.",
      "startOffset" : 149,
      "endOffset" : 176
    }, {
      "referenceID" : 11,
      "context" : "Small scale experiments - landmark selection scheme We use the ball dataset as proposed in Duin and Pekalska (2010). It is an artificial dataset based on the surface distances of randomly positioned balls of two classes having a slightly different radius.",
      "startOffset" : 91,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "Small scale experiments - landmark selection scheme We use the ball dataset as proposed in Duin and Pekalska (2010). It is an artificial dataset based on the surface distances of randomly positioned balls of two classes having a slightly different radius. The dataset is non-Euclidean with substantial information encoded in the negative part of the eigenspectrum. We generated the data with 100 samples per class leading to an N × N dissimilarity matrix D, with N = 200. We also use the protein data (213 pts, 4 classes) set represented by an indefinite similarity matrix, with a high intrinsic dimension Schleif and Tino (2015). Further we analyzed two simulated metric datasets which are not linear separable using the Euclidean norm: (1) the checker board data, generated as a two dimensional dataset with datapoints organized on a 3×3 checkerboard, with alternating labels.",
      "startOffset" : 91,
      "endOffset" : 630
    }, {
      "referenceID" : 3,
      "context" : "Two dimensional visualizations of the unapproximated K · K> similarity matrices obtained by using laplacian eigenmaps Belkin and Niyogi (2003). are shown in Figure 1.",
      "startOffset" : 118,
      "endOffset" : 143
    }, {
      "referenceID" : 37,
      "context" : "c) the matrix is Nyström approximated using the approach of Zhang and Kwok (2010) where the number of landmarks is taken from the MEB solution (SIM3),",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 37,
      "context" : "d) using the approach of Zhang and Kwok (2010) but with C landmarks where C is the number of classes (SIM4)",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 5,
      "context" : "f) using an entropy based selection as proposed in Brabanter et al. (2010) (SIM6) 9 where the number of landmarks is again taken from the MEB solution",
      "startOffset" : 51,
      "endOffset" : 75
    }, {
      "referenceID" : 35,
      "context" : "Williams and Seeger (2000) ).",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 37,
      "context" : "shown in Table 1 the approach by Zhang and Kwok (2010) is typically effective for a large variety of datasets also with indefinite kernels, given the number of landmarks is reasonable large and discriminating information is sufficiently provided in the dominating eigenvectors of the cluster solutions.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 37,
      "context" : "shown in Table 1 the approach by Zhang and Kwok (2010) is typically effective for a large variety of datasets also with indefinite kernels, given the number of landmarks is reasonable large and discriminating information is sufficiently provided in the dominating eigenvectors of the cluster solutions. For the protein data we observe similar results and the proposed approach, the k-means strategy and the entropy approach are effective. SIM4 and SIM5 is again substantially worse because four landmarks are in general not sufficient to represent these data from a discriminative point of view. For the checker board and Gaussian data SIM2 and SIM3 are again close and SIM4 and SIM5 are substantially worse using only two landmark points. The entropy approach was efficient only for the Gaussian data, but failed for Checker which may be attributed to the strong multi-modality of the data. The runtimes shown in Table 2 show already for the small data examples that the MEB approach is much faster then k-means or the entropy approach if the number of points gets larger which was already expected from the theoretical runtime complexity of these algorithms. In another small experiment we analyzed the effect of the k-means based landmark selection Zhang and Kwok (2010) in more detail.",
      "startOffset" : 33,
      "endOffset" : 1274
    }, {
      "referenceID" : 6,
      "context" : "Finally, in contrast to the original PCVM formulation Chen et al. (2009a), in our notation we explicitly use the data labels - for example, instead of vector Φθ(x) we write Ξθ(x) ◦ y, where Ξθ(x) is the kernel vector of x without any label information, y is the label vector and ◦ is the element-wise multiplication.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "From the derivation in Haasdonk and Pekalska (2008) we know, that only the eigenvector of the Nyström approximated kernel matrix based on K̂N,m = K̂ N,m+K̂ − N,m are needed.",
      "startOffset" : 23,
      "endOffset" : 52
    }, {
      "referenceID" : 26,
      "context" : "For the Ny-PCVM we obtain a similar analysis as shown in Schleif et al. (2015a) but with extra costs to calculate the Nyström approximated SVD.",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "In contrast to many standard kernel approaches, for iKFD and PCVM, the indefinite kernel matrices need not to be corrected by costly eigenvalue correction Chen et al. (2009c); Schleif and Gisbrecht (2013) 11 Further the iKFD and PCVM provides direct access to probabilistic classification decisions.",
      "startOffset" : 155,
      "endOffset" : 175
    }, {
      "referenceID" : 5,
      "context" : "In contrast to many standard kernel approaches, for iKFD and PCVM, the indefinite kernel matrices need not to be corrected by costly eigenvalue correction Chen et al. (2009c); Schleif and Gisbrecht (2013) 11 Further the iKFD and PCVM provides direct access to probabilistic classification decisions.",
      "startOffset" : 155,
      "endOffset" : 205
    }, {
      "referenceID" : 5,
      "context" : "In contrast to many standard kernel approaches, for iKFD and PCVM, the indefinite kernel matrices need not to be corrected by costly eigenvalue correction Chen et al. (2009c); Schleif and Gisbrecht (2013) 11 Further the iKFD and PCVM provides direct access to probabilistic classification decisions. First we show a small simulated experiment for two Gaussians which exist in an intrinsically two dimensional pseudo-Euclidean space R. The plot in Figure 5 shows a typical result for the obtained decision planes using the iKFD or Ny-iKFD. The Gaussians are slightly overlapping and both approaches achieve a good separation with 93.50% and 88.50% prediction accuracy, respectively. Subsequently we consider a few public available datasets for some real life experiments. The data are Zongker (2000pts, 10 classes) and Proteom (2604pts, 53 classes (restricted to classes with at least 10 entries)) from Duin (2012); Chromo (4200pt, 21 classes) from Neuhaus and Bunke (2006) and the SwissProt database Swiss (10988 pts, 30 classes) from Boeckmann et al.",
      "startOffset" : 155,
      "endOffset" : 914
    }, {
      "referenceID" : 5,
      "context" : "In contrast to many standard kernel approaches, for iKFD and PCVM, the indefinite kernel matrices need not to be corrected by costly eigenvalue correction Chen et al. (2009c); Schleif and Gisbrecht (2013) 11 Further the iKFD and PCVM provides direct access to probabilistic classification decisions. First we show a small simulated experiment for two Gaussians which exist in an intrinsically two dimensional pseudo-Euclidean space R. The plot in Figure 5 shows a typical result for the obtained decision planes using the iKFD or Ny-iKFD. The Gaussians are slightly overlapping and both approaches achieve a good separation with 93.50% and 88.50% prediction accuracy, respectively. Subsequently we consider a few public available datasets for some real life experiments. The data are Zongker (2000pts, 10 classes) and Proteom (2604pts, 53 classes (restricted to classes with at least 10 entries)) from Duin (2012); Chromo (4200pt, 21 classes) from Neuhaus and Bunke (2006) and the SwissProt database Swiss (10988 pts, 30 classes) from Boeckmann et al.",
      "startOffset" : 155,
      "endOffset" : 973
    }, {
      "referenceID" : 4,
      "context" : "The data are Zongker (2000pts, 10 classes) and Proteom (2604pts, 53 classes (restricted to classes with at least 10 entries)) from Duin (2012); Chromo (4200pt, 21 classes) from Neuhaus and Bunke (2006) and the SwissProt database Swiss (10988 pts, 30 classes) from Boeckmann et al. (2003), (version 10/2010, reduced to prosite labeled classes with at least 100 entries ).",
      "startOffset" : 264,
      "endOffset" : 288
    }, {
      "referenceID" : 4,
      "context" : "The data are Zongker (2000pts, 10 classes) and Proteom (2604pts, 53 classes (restricted to classes with at least 10 entries)) from Duin (2012); Chromo (4200pt, 21 classes) from Neuhaus and Bunke (2006) and the SwissProt database Swiss (10988 pts, 30 classes) from Boeckmann et al. (2003), (version 10/2010, reduced to prosite labeled classes with at least 100 entries ). Further we used the Sonatas data (1068pts, 5 classes) taken from Mokbel et al. (2009). All data are processed as indefinite kernels and the landmarks are selected using the respective landmark selection schemes.",
      "startOffset" : 264,
      "endOffset" : 457
    }, {
      "referenceID" : 5,
      "context" : "The entropy approach is similar efficient than the k-means strategy but more costly due to the iterative optimization of the landmark set and the respective eigen-decompositions (see Brabanter et al. (2010)).",
      "startOffset" : 183,
      "endOffset" : 207
    }, {
      "referenceID" : 9,
      "context" : "In future work it could be interesting to incorporate sparsity concepts into iKFD and NyiKFD similar as shown for classical KFD in Diethe et al. (2009).",
      "startOffset" : 131,
      "endOffset" : 152
    } ],
    "year" : 2016,
    "abstractText" : "Indefinite similarity measures can be frequently found in bio-informatics by means of alignment scores, but are also common in other fields like shape measures in image retrieval. Lacking an underlying vector space, the data are given as pairwise similarities only. The few algorithms available for such data do not scale to larger datasets. Focusing on probabilistic batch classifiers, the Indefinite Kernel Fisher Discriminant (iKFD) and the Probabilistic Classification Vector Machine (PCVM) are both effective algorithms for this type of data but, with cubic complexity. Here we propose an extension of iKFD and PCVM such that linear runtime and memory complexity is achieved for low rank indefinite kernels. Employing the Nyström approximation for indefinite kernels, we also propose a new almost parameter free approach to identify the landmarks, restricted to a supervised learning problem. Evaluations at several larger similarity data from various domains show that the proposed methods provides similar generalization capabilities while being easier to parametrize and substantially faster for large scale data.",
    "creator" : "LaTeX with hyperref package"
  }
}