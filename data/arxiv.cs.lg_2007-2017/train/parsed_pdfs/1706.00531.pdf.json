{
  "name" : "1706.00531.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "PixelGAN Autoencoders", "Alireza Makhzani", "Brendan Frey" ],
    "emails" : [ "makhzani@psi.toronto.edu", "frey@psi.toronto.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In recent years, generative models that can be trained via direct back-propagation have enabled remarkable progress in modeling natural images. One of the most successful models is the generative adversarial network (GAN) [1], which employs a two player min-max game. The generative model, G, samples the prior p(z) and generates the sample G(z). The discriminator, D(x), is trained to identify whether a point x is a sample from the data distribution or a sample from the generative model. The generator is trained to maximally confuse the discriminator into believing that generated samples come from the data distribution. The cost function of GAN is\nmin G max D Ex∼pdata [logD(x)] + Ez∼p(z)[log(1−D(G(z))].\nGANs can be considered within the wider framework of implicit generative models [2–4]. Implicit distributions can be sampled through their generative path, but their likelihood function is not tractable. Recently, several papers have proposed another application of GAN-style algorithms for approximate inference [2–9]. These algorithms use implicit distributions to learn posterior approximations that are more expressive than the distributions with tractable densities that are often used in variational inference. For example, adversarial autoencoders (AAE) [6] use a universal approximator posterior as the implicit posterior distribution and use adversarial training to match the aggregated posterior of the latent code to the prior distribution. Adversarial variational Bayes [3, 7] uses a more general amortized GAN inference framework within a maximum-likelihood learning setting. Another type of GAN inference technique is used in the ALI [8] and BiGAN [9] models, which have been shown to approximate maximum likelihood learning [3]. In these models, both the recognition and generative models are implicit and are jointly learnt by an adversarial training process.\nVariational autoencoders (VAE) [10, 11] are another state-of-the-art image modeling technique that use neural networks to parametrize the posterior distribution and pair it with a top-down generative network. Both networks are jointly trained to maximize a variational lower bound on the data loglikelihood. A different framework for learning density models is autoregressive neural networks such as NADE [12], MADE [13], PixelRNN [14] and PixelCNN [15]. Unlike variational autoencoders,\nar X\niv :1\n70 6.\n00 53\n1v 1\n[ cs\n.L G\n] 2\nJ un\n2 01\n7\nwhich capture the statistics of the data in hierarchical latent codes, the autoregressive models learn the image densities directly at the pixel level without learning a hierarchical latent representation.\nIn this paper, we present the PixelGAN autoencoder as a generative autoencoder that combines the benefits of latent variable models with autoregressive architectures. The PixelGAN autoencoder is a generative autoencoder in which the generative path is a PixelCNN that is conditioned on a latent variable. The latent variable is inferred by matching the aggregated posterior distribution to the prior distribution by an adversarial training technique similar to that of the adversarial autoencoder [6]. However, whereas in adversarial autoencoders the statistics of the data distribution are captured by the latent code, in the PixelGAN autoencoder they are captured jointly by the latent code and the autoregressive decoder. We show that imposing different distributions as the prior results in different factorizations of information between the latent code and the autoregressive decoder. For example, in Section 2.1, we show that by imposing a Gaussian distribution on the latent code, we can achieve a global vs. local decomposition of information. In this case, the global latent code no longer has to model all the irrelevant and fine details of the image, and can use its capacity to capture more relevant and global statistics of the image. Another type of decomposition of information that can be learnt by PixelGAN autoencoders is a discrete vs. continuous decomposition. In Section 2.2, we show that we can achieve this decomposition by imposing a categorical prior on the latent code using adversarial training. In this case, the categorical latent code captures the discrete underlying factors of variation in the data, such as class label information, and the autoregressive decoder captures the remaining continuous structure, such as style information, in an unsupervised fashion. We then show how PixelGAN autoencoders with categorical priors can be directly used in clustering and semi-supervised scenarios and achieve very competitive classification results on several datasets in Section 3. Finally, we present one of the main potential applications of PixelGAN autoencoders in learning cross-domain relations between two different domains in Section 4."
    }, {
      "heading" : "2 PixelGAN Autoencoders",
      "text" : "Let x be a datapoint that comes from the distribution pdata(x) and z be the hidden code. The recognition path of the PixelGAN autoencoder (Figure 1) defines an implicit posterior distribution q(z|x) by using a deterministic neural function z = f(x,n) that takes the input x along with random noise n with a fixed distribution p(n) and outputs z. The aggregated posterior q(z) of this model is defined as follows:\nq(z) = ∫ x q(z|x)pdata(x)dx.\nThis parametrization of the implicit posterior distribution was originally proposed in the adversarial autoencoder work [6] as the universal approximator posterior. We can sample from this implicit distribution q(z|x), by evaluating f(x,n) at different samples of n, but the density function of this posterior distribution is intractable. Appendix A.1 discusses the importance of the input noise in training PixelGAN autoencoders. The generative path p(x|z) is a conditional PixelCNN [15] that conditions on the latent vector z using an adaptive bias in PixelCNN layers. The inference is done by an amortized GAN inference technique that was originally proposed in the adversarial autoencoder\nwork [6]. In this method, an adversarial network is attached on top of the hidden code vector of the autoencoder and matches the aggregated posterior distribution, q(z), to an arbitrary prior, p(z). Samples from q(z) and p(z) are provided to the adversarial network as the negative and positive examples respectively, and the generator of the adversarial network, which is also the encoder of the autoencoder, tries to match q(z) to p(z) by the gradient that comes through the discriminative adversarial network.\nThe adversarial network, the PixelCNN decoder and the encoder are trained jointly in two phases – the reconstruction phase and the adversarial phase – executed on each mini-batch. In the reconstruction phase, the ground truth input x along with the hidden code z inferred by the encoder are provided to the PixelCNN decoder. The PixelCNN decoder weights are updated to maximize the log-likelihood of the input x. The encoder weights are also updated at this stage by the gradient that comes through the conditioning vector of the PixelCNN. In the adversarial phase, the adversarial network updates both its discriminative network and its generative network (the encoder) to match q(z) to p(z). Once the training is done, we can sample from the model by first sampling z from the prior distribution p(z), and then sampling from the conditional likelihood p(x|z) parametrized by the PixelCNN decoder. We now establish a connection between the PixelGAN autoencoder cost and maximum likelihood learning using a decomposition of the aggregated evidence lower bound (ELBO) proposed in [16]:\nEx∼pdata(x)[log p(x)] > −Ex∼pdata(x) [ Eq(z|x)[− log(p(x|z)] ] − Ex∼pdata(x) [ KL(q(z|x)‖p(z)) ] (1)\n= −Ex∼pdata(x) [ Eq(z|x)[− log(p(x|z)] ] ︸ ︷︷ ︸\nreconstruction term\n−KL(q(z)‖p(z))︸ ︷︷ ︸ marginal KL − I(z;x)︸ ︷︷ ︸ mutual info.\n(2)\nThe first term in Equation 2 is the reconstruction term and the second term is the marginal KL divergence between the aggregated posterior and the prior distribution. The third term is the mutual information between the latent code z and the input x. This is a regularization term that encourages z and x to be decoupled by removing the information of the data distribution from the hidden code. If the training set has N examples, I(z;x) is bounded as follows (see [16]).\n0 < I(z;x) < logN (3)\nIn order to maximize the ELBO, we need to minimize all the three terms of Equation 2. We consider two cases for the decoder p(x|z): Deterministic Decoder. If the decoder p(x|z) is deterministic or has very limited stochasticity such as the simple factorized decoder of the VAE, the mutual information term acts in the complete opposite direction of the reconstruction term. This is because the only way to minimize the reconstruction error of x is to learn a hidden code z that is relevant to x, which results in maximizing I(z;x). Indeed, it can be shown that minimizing the reconstruction term maximizes a variational lower bound on I(z;x) [17, 18]. For example, in the case of the VAE trained on MNIST, since the reconstruction is precise, the mutual information term is dominated and is close to its maximum value I(z;x) ≈ logN ≈ 11.00 nats [16]. Stochastic Decoder. If we use a powerful decoder such as the PixelCNN, the reconstruction term and the mutual information term will not compete with each other anymore and the network can minimize both independently. In this case, the optimal solution for maximizing the ELBO would be to model pdata(x) solely by p(x|z) and thereby minimizing the reconstruction term, and at the same time, minimizing the mutual information term by ignoring the latent code. As a result, even though the model achieves a high likelihood, the latent code does not learn any useful representation, which is undesirable. This problem has been observed in several previous works [19, 20] and different techniques such as annealing the weight of the KL term [19] or weakening the decoder [20] have been proposed to make z and x more dependent.\nAs suggested in [21, 20], we think that the maximum likelihood objective by itself is not a useful objective for representation learning especially when a powerful decoder is used. In PixelGAN autoencoders, in order to encourage learning more useful representations, we modify the ELBO (Equation 2) by removing the mutual information term from it, since this term is explicitly encouraging z to become independent of x. So our cost function only includes the reconstruction term and the marginal KL term. The reconstruction term is optimized by the reconstruction phase of training and\nthe marginal KL term is approximately optimized by the adversarial phase1. Note that since the mutual information term is upper bounded by a constant (logN ), we are still maximizing a lower bound on the log-likelihood of data. However, this bound is weaker than the ELBO, which is the price that is paid for learning more useful latent representations by balancing the decomposition of information between the latent code and the autoregressive decoder.\nFor implementing the conditioning adaptive bias in the PixelCNN decoder, we explore two different architectures [15]. In the location-invariant bias, for each PixelCNN layer, we use the latent code to construct a vector that is broadcasted within each feature map of the layer and then added as an adaptive bias to that layer. In the location-dependent bias, we use the latent code to construct a spatial feature map that is broadcasted across different feature maps and then added only to the first layer of the decoder as an adaptive bias. We will discuss the effect of these architectures on the learnt representation in Figure 3 of Section 2.1 and their implementation details in Appendix A.2."
    }, {
      "heading" : "2.1 PixelGAN Autoencoders with Gaussian Priors",
      "text" : "Here, we show that PixelGAN autoencoders with Gaussian priors can decompose the global and local statistics of the images between the latent code and the autoregressive decoder. Figure 2a shows the samples of a PixelGAN autoencoder model with the location-dependent bias trained on the MNIST dataset. For the purpose of better illustrating the decomposition of information, we have chosen a 2-D Gaussian latent code and a limited the receptive field of size 9 for the PixelGAN autoencoder. Figure 2b shows the samples of a PixelCNN model with the same limited receptive field size of 9 and Figure 2c shows the samples of an adversarial autoencoder with the 2-D Gaussian latent code. The PixelCNN can successfully capture the local statistics, but fails to capture the global statistics due to the limited receptive field size. In contrast, the adversarial autoencoder, whose sample quality is very similar to that of the VAE, can successfully capture the global statistics, but fails to generate the details of the images. However, the PixelGAN autoencoder, with the same receptive field and code size, can combine the best of both and generates sharp images with global statistics.\nIn PixelGAN autoencoders, both the PixelCNN depth and the conditioning architecture affect the decomposition of information between the latent code and the autoregressive decoder. We investigate these effects in Figure 3 by training a PixelGAN autoencoder on MNIST where the code size is chosen to be 2 for the visualization purpose. As shown in Figure 3a,b, when a shallow decoder is used, most of the information will be encoded in the hidden code and there is a clean separation between the digit clusters. As we make the PixelCNN more powerful (Figure 3c,d), we can see that the hidden code is still used to capture some relevant information of the input, but the separation of digit clusters is not as sharp when the limited code size of 2 is used. In the next section, we will show that by using a larger code size (e.g., 30), we can get a much better separation of digit clusters even when a powerful PixelCNN is used.\n1The original GAN formulation optimizes the Jensen-Shannon divergence [1], but there are other formulations that optimize the KL divergence, e.g. [3].\nThe conditioning architecture also affects the decomposition of information. In the case of the location-invariant bias, the hidden code is encouraged to learn the global information that is locationinvariant (the what information and not the where information) such as the class label information. For example, we can see in Figure 3a,c that the network has learnt to use one of the axes of the 2D Gaussian code to explicitly encode the digit label even though a continuous prior is imposed. In this case, we can potentially get a much better separation if we impose a discrete prior. This makes this architecture suitable for the discrete vs. continuous decomposition and we use it for our clustering and semi-supervised learning experiments. In the case of the location-dependent bias (Figure 3b,d), the hidden code is encouraged to learn the global information that has location dependent information such as low-frequency content of the image, similar to what the hidden code of an adversarial or variational autoencoder would learn (Figure 2c). This makes this architecture suitable for the global vs. local decomposition experiments such as Figure 2a.\nFrom Figure 3, we can see that the class label information is mostly captured by p(z) while the style information of the images is captured by both p(z) and p(x|z). This decomposition of information has also been studied in other works that combine the latent variable models with autoregressive decoders such as PixelVAE [22] and variational lossy autoencoders (VLAE) [20]. For example, the VLAE model [20] proposes to use the depth of the PixelCNN decoder to control the decomposition of information. In their model, the PixelCNN decoder is designed to have a shallow depth (small local receptive field) so that the latent code z is forced to capture more global information. This approach is very similar to our example of the PixelGAN autoencoder in Figure 2. However, the question that has remained unanswered is whether it is possible to achieve a complete decomposition of content and style in an unsupervised fashion, where the class label or discrete structure information is encoded in the latent code z, and the remaining continuous structure such as style is captured by a powerful and deep PixelCNN decoder. This kind of decomposition is particularly interesting as it can be directly used for clustering and semi-supervised classification. In the next section, we show that we can learn this decomposition of content and style by imposing a categorical distribution on the latent representation z using adversarial training. Note that this discrete vs. continuous decomposition is very different from the global vs. local decomposition, because a continuous factor of variation such as style can have both global and local effect on the image. Indeed, in order to achieve the discrete vs. continuous decomposition, we have to use very deep and powerful PixelCNN decoders (up to 20 residual blocks) to capture both the global and local statistics of the style by the PixelCNN while the discrete content of the image is captured by the categorical latent variable."
    }, {
      "heading" : "2.2 PixelGAN Autoencoders with Categorical Priors",
      "text" : "In this section, we present an architecture of the PixelGAN autoencoder that can separate the discrete information (e.g., class label) from the continuous information (e.g., style information) in the images. We then show how our architecture can be naturally adopted for the semi-supervised settings.\nThe architecture that we use is similar to Figure 1, with the difference that we impose a categorical distribution as the prior rather the Gaussian distribution (Figure 4) and also use the location-independent bias architecture. Another difference is that we use a convolutional network as the inference network q(z|x) to encourage the encoder to preserve the content and lose the style information of the image. The inference network has a softmax output and predicts a one-hot vector whose dimension is the\nnumber of discrete labels or categories that we wish the data to be clustered into. The adversarial network is trained directly on the continuous probability outputs of the softmax layer of the encoder.\nImposing a categorical distribution at the output of the encoder imposes two constraints. The first constraint is that the encoder has to make confident decisions about the class labels of the inputs. The adversarial training pushes the output of the encoder to the corners of the softmax simplex, by which it ensures that the autoencoder cannot use the latent vector z to carry any continuous style information. The second constraint imposed by adversarial training is that the aggregated posterior distribution of z should match the categorical prior distribution with uniform outcome probabilities. This constraint enforces the encoder to evenly distribute the class labels across the corners of the softmax simplex. Because of these constraints, the latent variable will only capture the discrete content of the image and all the continuous style information will be captured by the autoregressive decoder.\nIn order to better understand and visualize the effect of the adversarial training on shaping the hidden code distribution, we train a PixelGAN autoencoder on the first three digits of MNIST (18000 training and 3000 test points) and choose the number of clusters to be 3. Suppose z = [z1, z2, z3] is the hidden code which in this case is the output probabilities of the softmax layer of the inference network. In Figure 5a, we project the 3D softmax simplex of z1 + z2 + z3 = 1 onto a 2D triangle and plot the hidden codes of the training examples when no distribution is imposed on the hidden code. We can see from this figure that the network has learnt to use the surface of the softmax simplex to encode style information of the digits and thus the three corners of the simplex do not have any meaningful interpretation. Figure 5b corresponds to the code space of the same network when a categorical distribution is imposed using the adversarial training. In this case, we can see the network has successfully learnt to encode the label information of the three digits in the three corners of the simplex, and all the style information has been separately captured by the autoregressive decoder.\nThis network achieves an almost perfect test error-rate of 0.3% on the first three digits of MNIST, even though it is trained in a purely unsupervised fashion.\nOnce the PixelGAN autoencoder is trained, its encoder can be used for clustering new points and its decoder can be used to generate samples from each cluster. Figure 6 illustrates the samples of the PixelGAN autoencoder trained on the full MNIST dataset. The number of clusters is set to be 30 and each row corresponds to the conditional samples of one of the clusters (only 16 are shown). We can see that the discrete latent code of the network has learnt discrete factors of variation such as class label information and some discrete style information. For example digit 1s are put in different clusters based on how much tilted they are. The network is also assigning different clusters to digit 2s (based on whether they have a loop) and digit 7s (based on whether they have a dash in the middle). In Section 3.1, we will show that by using the encoder of this network, we can obtain about 5% error rate in classifying digits in an unsupervised fashion, just by matching each cluster to a digit type.\nSemi-Supervised PixelGAN Autoencoders. The PixelGAN autoencoder can be used in a semisupervised setting. In order to incorporate the label information, we add a semi-supervised training phase. Specifically, we set the number of clusters to be the same as the number of class labels and after executing the reconstruction and the adversarial phases on an unlabeled mini-batch, the semi-supervised phase is executed on a labeled mini-batch, by updating the weights of the encoder q(z|x) to minimize the cross-entropy cost. The semi-supervised cost also reduces the mode-missing behavior of the GAN training by enforcing the encoder to learn all the modes of the categorical distribution. In Section 3.2, we will evaluate the performance of the PixelGAN autoencoders on the semi-supervised classification tasks."
    }, {
      "heading" : "3 Experiments",
      "text" : "In this paper, we presented the PixelGAN autoencoder as a generative model, but the currently available metrics for evaluating the likelihood of GAN-based generative models such as Parzen window estimate are fundamentally flawed [23]. So in this section, we only present the performance of the PixelGAN autoencoder on downstream tasks such as unsupervised clustering and semi-supervised classification. The details of all the experiments can be found in Appendix B."
    }, {
      "heading" : "3.1 Unsupervised Clustering",
      "text" : "We trained a PixelGAN autoencoder in an unsupervised fashion on the MNIST dataset (Figure 6). We chose the number of clusters to be 30 and used the following evaluation protocol: once the training is done, for each cluster i, we found the validation example xn that maximizes q(zi|xn), and assigned the label of xn to all the points in the cluster i. We then computed the test error based on the assigned class labels to each cluster. As shown in the first column of Table 1, the performance of PixelGAN autoencoders is on par with other GAN-based clustering algorithms such as CatGAN [24], InfoGAN [18] and adversarial autoencoders [6]."
    }, {
      "heading" : "3.2 Semi-supervised Classification",
      "text" : "Table 1 and Figure 7 report the results of semi-supervised classification experiments on the MNIST, SVHN and NORB datasets. On the MNIST dataset with 20, 50 and 100 labels, our classification results are highly competitive. Note that the classification rate of unsupervised clustering of MNIST is better than semi-supervised MNIST with 20 labels. This is because in the unsupervised case, the number of clusters is 30, but in the semi-supervised case, there are only 10 class labels which makes it more likely to confuse two digits. On the SVHN dataset with 500 and 1000 labels, the PixelGAN autoencoder outperforms all the other methods except the recently proposed temporal ensembling work [30] which is not a generative model. On the NORB dataset with 1000 labels, the PixelGAN autoencoder outperforms all the other reported results.\nFigure 8 shows the conditional samples of the semi-supervised PixelGAN autoencoder on the MNIST, SVHN and NORB datasets. Each column of this figure presents sampled images conditioned on a fixed one-hot latent code. We can see from this figure that the PixelGAN autoencoder can achieve a rather clean separation of style and content on these datasets with very few labeled data."
    }, {
      "heading" : "4 Learning Cross-Domain Relations with PixelGAN Autoencoders",
      "text" : "In this section, we discuss how the PixelGAN autoencoder can be viewed in the context of learning cross-domain relations between two different domains. We also describe how the problem of clustering or semi-supervised learning can be cast as the problem of finding a smooth cross-domain mapping from the data distribution to the categorical distribution.\nRecently several GAN-based methods have been developed to learn a cross-domain mapping between two different domains [31–33, 6, 34]. In [33], an unsupervised cost function called the output distribution matching (ODM) is proposed to find a cross-domain mapping F between two domains D1 and D2 by imposing the following unsupervised constraint on the uncorrelated samples from x ∼ D1 and y ∼ D2:\nDistr[F (x)] = Distr[y] (4)\nwhere Distr[z] denotes the distribution of the random variable z. The adversarial training is proposed as one of the methods for matching these distributions. If we have access to a few labeled pairs (x,y), then F can be further trained on them in a supervised fashion to satisfy F (x) = y. For example, in speech recognition, we want to find a cross-domain mapping from a sequence of phonemes to a sequence of characters. By optimizing the ODM cost function in Equation 4, we can find a smooth function F that takes phonemes at its input and outputs a sequence of characters that respects the language model. However, the main problem with this method is that the network can learn to ignore part of the input distribution and still satisfy the ODM cost function by its output distribution. This problem has also been observed in other works such as [31]. One way to avoid this problem is to add a reconstruction term to the ODM cost function by introducing a reverse mapping from the output of the encoder to the input domain. The is essentially the idea of the adversarial autoencoder [6] which learns a generative model by finding a cross-domain mapping between a Gaussian distribution and the data distribution. Using the ODM cost function along with a reconstruction term to learn cross-domain relations have been explored in several previous works. For example, InfoGAN [18] adds a mutual information term to the ODM cost function and optimizes a variational lower bound on this term. It can be shown that maximizing this variational bound is indeed minimizing the reconstruction cost of an autoencoder [17]. Similarly, in [34, 35], an adversarial autoencoder is used to learn the cross-domain relations of the vector representations of words from two different languages. The architecture of the recent works of DiscoGAN [31] and CycleGAN [32] are also similar to an adversarial autoencoder in which the latent representation is enforced to have the distribution of the other domain. Here we describe how our proposed PixelGAN autoencoder can be potentially used in all these application areas to learn better cross-domain relations. Suppose we want to learn a mapping from domain D1 to D2. In the architecture of Figure 1, we can use independent samples of x ∼ D1 at the input and instead of imposing a Gaussian distribution on the latent code, we can impose the distribution of the second domain using its independent samples y ∼ D2. Unlike adversarial autoencoders, the encoder of PixelGAN autoencoders does not have to retain all the input information in order to have a lossless reconstruction. So the encoder can use all its capacity to learn the most relevant mapping from D1 to D2 and at the same time, the PixelCNN decoder can capture the remaining information that has been lost by the encoder.\nWe can adopt the ODM idea for semi-supervised learning by assuming D1 is the image domain and D2 is the label domain (Figure 9a). Independent samples of D1 and D2 correspond to samples from the data distribution pdata(x) and the categorical distribution. The function F = q(y|x) can be parametrized by a neural network that is trained to satisfy the ODM cost function by matching the aggregated distribution q(y) = ∫ q(y|x)pdata(x)dx to the categorical distribution using adversarial training. The few labeled examples are used to further train F to satisfy F (x) = y. However, as explained above, the problem with this method is that the network can learn to generate the categorical distribution by ignoring some part of the input distribution. The adversarial autoencoder (Figure 9b) solves this problem by adding an inverse mapping from the categorical distribution to the data distribution. However, the main drawback of the adversarial autoencoder architecture is that due\nto the reconstruction term, the latent representation now has to model all the underlying factors of variation in the image. For example, in the architecture of Figure 9b, while we are only interested in the one-hot label representation to do semi-supervised learning, we also need to infer the style of the image so that we can have a lossless reconstruction of the image. The PixelGAN autoencoder solves this problem by enabling the encoder to only infer the factor of variation that we are interested in (i.e., label information), while the remaining structure of the input (i.e., style information) is automatically captured by the autoregressive decoder."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we proposed the PixelGAN autoencoder, which is a generative autoencoder that combines a generative PixelCNN with a GAN inference network that can impose arbitrary priors on the latent code. We showed that imposing different distributions as the prior enables us to learn a latent representation that captures the type of statistics that we care about, while the remaining structure of the image is captured by the PixelCNN decoder. Specifically, by imposing a Gaussian prior, we were able to disentangle the low-frequency and high-frequency statistics of the images, and by imposing a categorical prior we were able to disentangle the style and content of images and learn representations that are specifically useful for clustering and semi-supervised learning tasks. While the main focus of this paper was to demonstrate the application of PixelGAN autoencoders in downstream tasks such as semi-supervised learning, we discussed how these architectures have many other potentials such as learning cross-domain relations between two different domains."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Nathan Killoran for helpful discussions. We also thank NVIDIA for GPU donations."
    }, {
      "heading" : "Appendix A Implementation Details",
      "text" : "In this section, we describe two important architecture design choices for training PixelGAN autoencoders.\nA.1 Input noise\nIn all the semi-supervised experiments, we found it crucial to use the universal approximator posterior discussed in Section 2, as opposed to a deterministic posterior. Specifically, the input noise that we use is an additive Gaussian noise, which results in a posterior distribution q(z|x) that is more expressive than that of a model without the input corruption. This is similar to the denoising criterion idea proposed in [36]. We believe this additive noise is also playing an important role in preventing the mode-missing behavior of the GAN when imposing a degenerate distribution such as the categorical distribution. Similar related ideas have been used to stabilize GAN training such as instance noise [37] or one-sided label noise [29]."
    }, {
      "heading" : "A.2 Conditioning of PixelCNN",
      "text" : "There are three methods to implement how the PixelCNN conditions on the latent vector.\nLocation-Invariant Bias. This is the method that was proposed in the conditional PixelCNN model [15]. Suppose the size of the convolutional layer of the decoder is (batch, width, height, channels). Then the PixelCNN can use a linear mapping to convert the conditioning tensor of size (batch, condition_size) to generate a tensor of size (batch, channels) that is then broadcasted and added to the feature maps of all the layers of the PixelCNN decoder as an adaptive bias. In this method, the hidden code is encouraged to learn the global information that is locationinvariant (the what information and not the where information) such as the class label information. We use this method in all the clustering and semi-supervised learning experiments.\nLocation-Dependent Bias. Suppose the size of the convolutional layer of the PixelCNN decoder is (batch, width, height, channels). Then the PixelCNN can use a one layer neural network to convert the conditioning tensor of size (batch, condition_size) to generate a spatial tensor of size (batch, width, height, k) followed by a 1× 1 convolutional layer to construct a tensor of size (batch, width, height, channels) that is then added only to the feature maps of the first layer of the decoder as an adaptive bias (similar to the VPN model [38]). When k = 1, we can simply broadcast the tensor of size (batch, width, height, k=1) to get a tensor of size (batch, width, height, channels) instead of using the 1 × 1 convolution. In this method, the latent vector has spatial and location-dependent information within the feature map. This is the method that we used in experiments of Figure 2a.\nInput Channel. Another method for conditioning is proposed in the PixelVAE [22] and the variational lossy autoencoder (VLAE) [20]. In this method, first a tensor of size (batch, width, height, k) is constructed using the conditioning tensor similar to the location-dependent bias. This tensor is then concatenated to the input of the PixelCNN. The performance and computational complexity of this method is very similar to that of the location-dependent bias method."
    }, {
      "heading" : "Appendix B Experiment Details",
      "text" : "We used TensorFlow [39] in all of our experiments. As suggested in [1], in order to improve the stability of GAN training, the generator of the GAN in all our experiments is trained to maximize logD(G(z)) rather than minimizing log(1−D(G(z)))."
    }, {
      "heading" : "B.1 MNIST Dataset",
      "text" : "The MNIST dataset has 50K training points, 10K validation points and 10K test points. We perform experiments on both the binary MNIST and the real-valued MNIST. In the real valued MNIST experiments, we subtract 127.5 from the data points and then divide them by 127.5 and use the discretized logistic mixture likelihood [40] as the cost function for the PixelCNN. In the case of binary MNIST, the data points are binarized by setting pixel values larger than 0.5 to 1, and values smaller than 0.5 to 0."
    }, {
      "heading" : "B.1.1 PixelGAN Autoencoders with Gaussian Prior on MNIST",
      "text" : "Here we describe the model architecture used for training the PixelGAN autoencoder with a Gaussian prior on the binary MNIST dataset in Figure 2a. The PixelCNN decoder uses both the vertical and horizontal stacks similar to [15]. The cost function of the PixelCNN is the cross-entropy cost function. The PixelCNN uses the location-dependent bias as described in Appendix A.2. Specifically, a tensor of size (batch, width, height, 1) is constructed from the conditioning vector by using a one-layer neural network with 1000 hidden units, ReLU activation and linear output. This tensor is then broadcasted and added only to the feature maps of the first layer of the PixelCNN decoder. The PixelCNN is designed to have a local receptive field by having 3 residual blocks (filter size of 3x5, 32 feature maps, ReLU non-linearity as in [15]). The adversarial discriminator has two layers of 2000 hidden units with ReLU activation function. The encoder architecture has two fully-connected layers of size 2000 with ReLU non-linearity. The last layer of the encoder q(z|x) has a linear activation function. On the latent representation of size 2, we impose a Gaussian distribution with standard deviation of 5. We used the gradient descent with momentum algorithm for optimizing all the cost functions of the network. For the PixelCNN reconstruction cost, we used the learning rate of 0.001 and the momentum value of 0.9. After 25 epochs we reduce the learning rate to 0.0001. For both of the generator and the discriminator costs, the learning rates and the momentum values were set to 0.1."
    }, {
      "heading" : "B.1.2 Unsupervised Clustering of MNIST",
      "text" : "Here we describe the model architecture used for clustering the binary MNIST dataset in Figure 6 and Section 3.1. The PixelCNN decoder uses both the vertical and horizontal stacks similar to [15]. The cost function of the PixelCNN is the cross-entropy cost function. The PixelCNN uses the location-invariant bias as described in Appendix A.2 and has 15 residual blocks (filter size of 3x5, 32 feature maps, ReLU non-linearity as in [15]). The adversarial discriminator has two layers of 3000 hidden units with ReLU activation function. The encoder architecture has a convolutional layer (filter size of 7, 32 feature maps, ReLU activation) and a max-pooling layer (pooling size 2), followed by another convolutional layer (filter size of 7, 32 feature maps, ReLU activation) and a max-pooling layer (pooling size 2) with no fully-connected layer. The last layer of the encoder q(z|x) has the softmax activation function. We found it important to use batch-normalization [41] for all the layers of the encoder including the softmax layer. The number of clusters is chosen to be 30. The clusters are represented by a discrete one-hot variable of size 30. On the continuous probability output of the softmax, we impose a categorical distribution with uniform probabilities. We use Adam [42] optimizer with learning rate of 0.001 for optimizing the PixelCNN reconstruction cost function, but we found it important to use the gradient descent with momentum algorithm for optimizing the generator and the discriminator costs of the adversarial network. For both of the generator and the discriminator costs, the momentum values were set to 0.1 and the learning rates were set to 0.01. We use an input dropout noise with the keep probability of 0.8 at the input layer and only at the training time.\nThe model architecture used for Figure 5 is the same as this architecture except that the number of clusters is chosen to be 3."
    }, {
      "heading" : "B.1.3 Semi-Supervised MNIST",
      "text" : "We performed semi-supervised learning experiments on both binary and real-valued MNIST dataset. We found that the semi-supervised error-rate of the real-valued MNIST is roughly the same as the binary MNIST (about 1.10% with 100 labels), but it takes longer to train due to the logistic mixture likelihood cost function [40]. So in Table 1, we only report the performance with the binary MNIST, but in Figure 8b we are showing the samples of the real-valued MNIST with 100 labels.\nBinary MNIST. Here we describe the model architecture used for the semi-supervised learning experiments on the binary MNIST in Section 3.2 and Table 1. The PixelCNN decoder uses both the vertical and horizontal stacks similar to [15] and uses the cross-entropy cost function. The PixelCNN uses the location-invariant bias as described in Appendix A.2. The PixelCNN has 6 residual blocks (filter size of 3x5, 32 feature maps, ReLU non-linearity as in [15]). The adversarial discriminator has two layers of 1000 hidden units with ReLU activation function. The encoder architecture has three convolutional layers (filter size of 5, 32 feature maps, ReLU activation) and a max-pooling layer (pooling size 2), followed by another three convolutional layers (filter size of 5, 32 feature maps, ReLU activation) and a max-pooling layer (pooling size 2) with no fully-connected layer. The\nlast layer of the encoder q(z|x) has the softmax activation function. All the convolutional layers of the encoder except the softmax layer use batch-normalization [41]. On the latent representation, we impose a categorical distribution with uniform probabilities. The semi-supervised cost is the cross-entropy cost function at the output of q(z|x). We use Adam [42] optimizer with learning rate of 0.001 for optimizing the PixelCNN cost and the cross-entropy cost, but we found it important to use the gradient descent with momentum algorithm for optimizing the generator and the discriminator costs of the adversarial network. For both of the generator and the discriminator costs, the momentum values were set to 0.1 and the learning rates were set to 0.1. We add a Gaussian noise with standard deviation of 0.3 to the input layer as described in Appendix A.1. The labeled examples were chosen at random but evenly distributed across the classes.\nReal-valued MNIST. Here we describe the model architecture used for the semi-supervised learning experiments on the real-valued MNIST in Figure 8b. The PixelCNN decoder uses both the vertical and horizontal stacks similar to [15] and uses a discretized logistic mixture likelihood cost function with 10 logistic distribution as proposed in [40]. The PixelCNN uses the location-invariant bias as described in Appendix A.2. The PixelCNN has 20 residual blocks (filter size of 2x3, 64 feature maps, gated sigmoid-tanh non-linearity as in [15]). The adversarial discriminator has two layers of 1000 hidden units with ReLU activation function. The encoder architecture has three convolutional layers (filter size of 5, 32 feature maps, ReLU activation) and a max-pooling layer (pooling size 2), followed by another three convolutional layers (filter size of 5, 32 feature maps, ReLU activation) and a max-pooling layer (pooling size 2) with no fully-connected layer. The last layer of the encoder q(z|x) has the softmax activation function. All the convolutional layers of the encoder except the softmax layer use batch-normalization [41]. On the latent representation, we impose a categorical distribution with uniform probabilities. The semi-supervised cost is the cross-entropy cost function at the output of q(z|x). We use Adam [42] optimizer with learning rate of 0.001 for optimizing the PixelCNN cost and the cross-entropy cost, but we found it important to use the gradient descent with momentum algorithm for optimizing the generator and the discriminator costs of the adversarial network. For both of the generator and the discriminator costs, the momentum values were set to 0.1 and the learning rates were set to 0.1. After 150 epochs, we divide all the learning rates by 10. We add a Gaussian noise with standard deviation of 0.3 to the input layer as described in Appendix A.1. The labeled examples were chosen at random but evenly distributed across the classes."
    }, {
      "heading" : "B.2 SVHN Dataset",
      "text" : "The SVHN dataset has about 530K training points and 26K test points. We use 10K points for the validation set. Similar to [26], we downsample the images from 32× 32× 3 to 16× 16× 3 and then subtracte 127.5 from the data points and then divide them by 127.5."
    }, {
      "heading" : "B.2.1 Semi-Supervised SVHN",
      "text" : "Here we describe the model architecture used for the semi-supervised learning experiments on the SVHN dataset in Section 3.2. The PixelCNN decoder uses both the vertical and horizontal stacks similar to [15]. The cost function of the PixelCNN is a discretized logistic mixture likelihood cost function with 10 logistic distribution as proposed in [40]. The PixelCNN uses the location-invariant bias as described in Appendix A.2 and has 20 residual blocks (filter size of 3x5, 32 feature maps, gated sigmoid-tanh non-linearity as in [15]). The adversarial discriminator has two layers of 1000 hidden units with ReLU activation function. The encoder architecture has two convolutional layers (filter size of 5, 32 feature maps, ReLU activation) and a max-pooling layer (pooling size 2), followed by another two convolutional layers (filter size of 5, 32 feature maps, ReLU activation) and a max-pooling layer (pooling size 2) with no fully-connected layer. The last layer of the encoder q(z|x) has the softmax activation function. All the convolutional layers of the encoder except the softmax layer use batch-normalization [41]. On the latent representation, we impose a categorical distribution with uniform probabilities. The semi-supervised cost is the cross-entropy cost function at the output of q(z|x). We use Adam [42] optimizer for optimizing all the cost function. For the PixelCNN cost and the cross-entropy cost we use the learning rate of 0.001 and for the generator and the discriminator costs of the adversarial network we use the learning rate of 0.0001. We add a Gaussian noise with standard deviation of 0.2 to the input layer as described in Appendix A.1."
    }, {
      "heading" : "B.3 NORB Dataset",
      "text" : "The NORB dataset has about 24K training points and 24K test points. We use 4K points for the validation set. This dataset has 5 object categories: animals, human figures, airplanes, trucks and cars. We downsample the images to have the size of 32× 32× 1, subtract 127.5 from the data points and then divide them by 127.5."
    }, {
      "heading" : "B.3.1 Semi-Supervised NORB",
      "text" : "The PixelCNN decoder uses both the vertical and horizontal stacks similar to [15]. The cost function of the PixelCNN is a discretized logistic mixture likelihood cost function with 10 logistic distribution as proposed in [40]. The PixelCNN uses the location-invariant bias as described in Appendix A.2 and has 15 residual blocks (filter size of 3x5, 32 feature maps, gated sigmoid-tanh non-linearity as in [15]). The adversarial discriminator has two layers of 1000 hidden units with ReLU activation function. The encoder architecture has a convolutional layer (filter size of 7, 32 feature maps, ReLU activation) and a max-pooling layer (pooling size 2), followed by another convolutional layer (filter size of 7, 32 feature maps, ReLU activation) and a max-pooling layer (pooling size 2), followed by another convolutional layer (filter size of 7, 32 feature maps, ReLU activation) and a max-pooling layer (pooling size 2) with no fully-connected layer. The last layer of the encoder q(z|x) has the softmax activation function. All the convolutional layers of the encoder except the softmax layer use batch-normalization [41]. On the latent representation, we impose a categorical distribution with uniform probabilities. The semi-supervised cost is the cross-entropy cost function at the output of q(z|x). We use Adam [42] optimizer for optimizing all the cost function. For the PixelCNN cost and the cross-entropy cost we use the learning rate of 0.001 and for the generator and the discriminator costs of the adversarial network we use the learning rate of 0.0001. We add a Gaussian noise with standard deviation of 0.3 to the input layer as described in Appendix A.1. The labeled examples were chosen at random but evenly distributed across the classes. In the case of NORB with 1000 labels, the test error after 10 epochs is 12.97%, after 100 epochs is 11.63% and after 500 epochs is 8.17%."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In this paper, we describe the “PixelGAN autoencoder”, a generative autoencoder<lb>in which the generative path is a convolutional autoregressive neural network on<lb>pixels (PixelCNN) that is conditioned on a latent code, and the recognition path<lb>uses a generative adversarial network (GAN) to impose a prior distribution on the<lb>latent code. We show that different priors result in different decompositions of<lb>information between the latent code and the autoregressive decoder. For example,<lb>by imposing a Gaussian distribution as the prior, we can achieve a global vs. local<lb>decomposition, or by imposing a categorical distribution as the prior, we can<lb>disentangle the style and content information of images in an unsupervised fashion.<lb>We further show how the PixelGAN autoencoder with a categorical prior can be<lb>directly used in semi-supervised settings and achieve competitive semi-supervised<lb>classification results on the MNIST, SVHN and NORB datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}