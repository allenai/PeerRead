{
  "name" : "1306.2295.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 6.\n22 95\nv1 [\ncs .A\nI] 1\n0 Ju\nn 20\nMarkov random fields provide a compact representation of joint probability distributions by representing its independence properties in an undirected graph. The well-known Hammersley-Clifford theorem uses these conditional independences to factorize a Gibbs distribution into a set of factors. However, an important issue of using a graph to represent independences is that it cannot encode some types of independence relations, such as the context-specific independences (CSIs). They are a particular case of conditional independences that is true only for a certain assignment of its conditioning set; in contrast to conditional independences that must hold for all its assignments. This work presents a method for factorizing a Markov random field according to CSIs present in a distribution, and formally guarantees that this factorization is correct. This is presented in our main contribution, the context-specific Hammersley-Clifford theorem, a generalization to CSIs of the Hammersley-Clifford theorem that applies for conditional independences."
    }, {
      "heading" : "1 Introduction",
      "text" : "Markov random fields (MRFs), also known as undirected graphical models, or Markov networks, belong to the family of probabilistic graphical models (Koller and Friedman, 2009), a well-known computational framework for compact representation of joint probability distributions. These models are composed of an independence structure, and a set of numerical parameters. The independence structure is an undirected graph that encodes compactly the conditional independences among the variables in the do-\nmain. Given the structure, the numerical parameters quantify the relationships in the structure. Probability distributions present in practice important complexity deficiencies, with exponential space complexity of their representation, time complexity of inference, and sample complexity when learning them from data. Based on the structure of independences, it is possible to represent efficiently the joint probability distribution by factorizing it into smaller functions (or factors), each over a subset of the domain variables, resulting some times in exponential reductions in these complexities. This factorization can be done by using the well-known Hammersley-Clifford theorem (Hammersley and Clifford, 1971).\nAn important issue of using a graph to represent independences is that it cannot encode some types of independence relations, such as the context-specific independences (CSIs) (Boutilier et al., 1996). These independences are similiar to conditional independences except that are only true for certain assignments of its conditioning set. The CSIs have been applied in a wide range of scenarios achieving significant improvements in time, space and sample complexities, in comparison with other approaches that only uses conditional independences encoded by the graph. (Chickering et al., 1997; Fierens, 2010; Poole and Zhang, 2003; Wexler and Meek, 2008; Lowd and Davis, 2010; Ravikumar et al., 2010). In these contributions, the CSIs are encoded in alternative data structures (e.g., using a decision tree instead of a graph). This is carried out by assuming that the factors of the distribution are conditional probability distributions. In this sense, the CSIs are not used to factorize the distribution, but they are used for representing efficiently the factors.\nThe main contribution of our work is the contextspecific Hammersley-Clifford theorem. The importance of this theoretical result lies in that it allows to factorize a distribution using CSIs, to obtain a more sparse representation than that obtained\nwith conditional independences, providing theoretical guarantees. For this, a log-linear model is used as a more fine-grained representation of the MRFs (Koller and Friedman, 2009). By using such models it is possible to extend the advantages of the Hammersley-Clifford theorem, that is, improvements in time, space and sample complexities.\nThe remainder of this work is organized as follows. The next Section provides a summary of the related work in the literature. Section 3 presents an overview of how to factorize a distribution by exploiting its independences. Section 4 formally describes the contextspecific Hammersley-Clifford theorem that factorizes a log-linear model according to a set of CSIs. The paper concludes with a summary in Section 5."
    }, {
      "heading" : "2 Related work",
      "text" : "There are several works in the literature (Della Pietra et al., 1997; Lee et al., 2006; Lowd and Davis, 2010; Van Haaren and Davis, 2012) that learn log-linear models directly by presenting different procedures for selecting features from data. Neither of these works discuss CSIs, nor present any guarantee on how the log-linear model generated is related to the underlying distribution.\nCSIs were first introduced by (Boutilier et al., 1996) by coding them locally within conditional probability tables (factors) of Bayesian networks as decision trees. Their approach is hybrid, encoding conditional independencies in the directed graph and CSIs as decision trees over the variables of a conditional probability table. Also, their work presents theoretical results for a sound graphical representation. This work instead proposes a unified representation for CSIs and conditional independencies into a log-linear model. As such, it requires first theoretical guarantees on how a distribution factorizes according to this model (not needed for the work of Boutlier as the factorization into conditional probability tables is not affected by the CSIs). It remains for future investigation to find an efficient graphical representation (and theoretical guarantees thereon).\nThe work of (Gogate et al., 2010) is the closests to our work, presenting an algorithm for factorizing a loglinear model according to CSIs. For that it introduces a statistical independence test for eliciting this independencies from data. The work assumes the underlying distribution to be a thin junction tree. Although some theoretical results are presented that guarantee an efficient computational performance, no results are presented that guarantee the factorization proposed is sound."
    }, {
      "heading" : "3 Preliminaries",
      "text" : "This section provides some background on MRFs, explaining how to factorize a distribution by exploiting its independences. Let us start by introducing some necessary notation. We use capital letters for sets of indexes, reserving the X letter for the domain of a distribution, and V for the nodes of a graph. Let X = (Xa, Xb, . . . , Xn) represent a vector of n = |X | random variables. The Val(Xa) function returns all the values of the domain of Xa, and Val(XU ) returns all the possible values of the set of variables XU = (Xi, i ∈ U). Let x = (xa, xb, . . . , xn) be a complete assignment of X . The values of Xa are denoted by xja ∈ Val(Xa), where j = 1, . . . , |Val(Xa)|. Finally, we denote by x〈W 〉 the value taken by variables XW in the complete assignment x.\nConditional independences are regularities of distributions that has been extensively studied in the field of statistics, demonstrating how they can be effectively and soundly used for reducing the dimensionality of the distribution (Pearl, 1988; Spirtes et al., 2000; Koller and Friedman, 2009). Formally, a conditional independence is defined as follows:\nDefinition 1. Conditional independence. Let Xa, Xb ∈ X be two random variables, and XU ⊆ X \\ {Xa, Xb} be a set of variables. We say that Xa and Xb are conditionally independent given XU , denoted as I(Xa, Xb | XU ), if and only if for all values xa ∈ Val(Xa), xb ∈ Val(Xb), and xU ∈ Val(XU ):\np(Xa|Xb, XU ) = p(Xa|XU ), (1)\nwhenever p(Xb, XU ) > 0.\nThrough the notion of conditional independence it is possible to construct a dependency model I, defined formally as follows:\nDefinition 2. Dependency model.\nA dependency model I is a discrete function that returns a truth value, given an input triplet 〈Xa, Xb | XU 〉, for all Xa, Xb ∈ X, XU ⊆ X \\ {Xa, Xb}.\nRemark. An alternative viewpoint of the above definition can be obtained by considering that every triplet 〈Xa, Xb | XU 〉 over a domain X are implicitly conditioned by a constant assignment to some external variable of the domain E = e. In this sense, all the triplets of the dependency model become to be conditioned by the assignment E = e.\nIn that sense, any probability distribution is a dependency model, because for any conditional independence assertion it is possible to test its truth value using Equation (1). In this work, we are particularly interested in the set of dependency models that\nare graph-isomorph, that is when all its independences and dependences can be represented in an undirected graph. Formally, an undirected graph G = (V,E) is defined by a set of nodes V = (a, b, . . . , n), and a set of edges E ⊂ V × V . Each node a ∈ V is associated with a random variable Xa ∈ X , and each edge (a, b) ∈ E represents a direct probabilistic influence between Xa and Xb. A necessary and sufficient condition for dependency models to be graph-isomorph is that all its independence assertions satisfy the following independence axioms, commonly called the Pearl axioms (Pearl and Paz, 1985):\nSymmetry I(XA, XB | XU ) ⇔ I(XB , XA | XU ) (2)\nDecomposition\nI(XA, XB ∪ XW | XU ) ⇒ I(XA, XB | XU ) & I(XA, XW | XU ) (3)\nIntersection\nI(XA, XB | XU ∪ XW ) & I(XA, XW | XU ∪ XB) ⇒\nI(XA, XB ∪ XW | XU ) (4)\nStrong union\nI(XA, XB | XW ) ⇒ I(XA, XB | XW ∪ XU ) (5)\nTransitivity\nI(XA, XB | XW ) ⇒ I(XA, Xc | XW ) or I(Xc, XB | XW ) (6)\nOther important property that we will need later to reconstruct graphs from dependency models is the pairwise Markov property, that asserts that an undirected graph can be built from a dependency model which is graph-isomorph, as follows:\nDefinition 3 (Pairwise Markov property (Koller and Friedman, 2009)). Let G be a graph over X. Two nodes a and b are non-adjacent if and only if the random variables Xa and Xb are conditionally independent given all other variables X \\ {Xa, Xb}, i.e.,\nI(Xa, Xb | X \\ {Xa, Xb}) iff (a, b) /∈ E. (7)\nIf every independence assertion contained in a dependency model I holds for p(X), I is said to be an I-map of p(X). In a similar fashion, we say that G is also an I-map of p(X). The pairwise property is necessary for those cases for which the graph can only encode a subset of the independences present in the distribution.\nA distribution can present additional type of independences. In this work we focus in a finer-grained type of independences: the context-specific independences (CSI) (Boutilier et al., 1996; Geiger and Heckerman, 1996; Chickering et al., 1997; Koller and Friedman, 2009). These independences are similar to conditional\nindependences, but hold for a specific assignment of the conditioning set, called the context of the independence. We define CSIs formally as follows:\nDefinition 4 (Context-specific independence (Boutilier et al., 1996)). Let Xa, Xb ∈ X be two random variables, XU , XW ⊆ X \\ {Xa, Xb} be pairwise disjoint sets of variables that does not contain Xa, Xb; and xW some assignment of XW . We say that variables Xa and Xb are contextually independent given XU and a context XW = xW , denoted I(Xa, Xb | XU , xW ), if and only if\np(Xa|Xb, XU , xW ) = p(Xa|XU , xW ), (8)\nwhenever p(Xb, XU , xW ) > 0.\nInterestingly, a conditional independence assertion can be seen as a conjunction of CSIs, that is, the CSIs for all the contexts of the conditioning set of the conditional independence. Since each CSIs is defined for a specific context, they cannot be represented all together in a single undirected graph (Koller and Friedman, 2009). Instead, they can be captured by a dependency model I, extended for CSIs by using Equation (8) to test the validity of every assertion I(Xa, Xb | XU , xW ). We call this model a context-specific dependency model Ic. If every independence assertion contained in Ic holds for p(X), Ic is said to be an CSI-map of p(X) (Boutilier et al., 1996). We define formally the Context-specific dependency model as follows:\nDefinition 5. Context-specific dependency model. A dependency model Ic is a discrete function that returns a truth value given an input triplet 〈Xa, Xb | XU , xW 〉, for all Xa, Xb ∈ X, XU ⊆ X \\ {Xa, Xb}, and xW a context over the subset XW ⊆ X."
    }, {
      "heading" : "3.1 Undirected graphs factorization",
      "text" : "AMRF uses an undirected graphG and a set of numerical parameters θ ∈ R to represent a distribution. The completely connected sub-graphs of G (a.k.a., cliques) can be used to factorize the distribution into a set of potential functions {φC(XC) : C ∈ cliques(G))} of lower dimension than p(X), parameterized by θ. The following theorem shows how to factorize the distribution:\nTheorem 1 (Hammersley-Clifford (Hammersley and Clifford, 1971)). Let p(X) be a positive distribution over the domain of variables X, and let G be an undirected graph over X. If G is an I-map of p(X), then p(X) can be factorized as:\np(X) = exp{ ∑\nC∈cliques(G)\nφC(XC)− ln(Z)}, (9)\nwhere Z is a normalizing constant.\nA distribution factorized by the above theorem is called a Gibbs distribution. The most näive form contains potentials φC(·) represented by tables, where each entry corresponds to an assignment xC ∈ Val(XC) that has associated a numerical parameter.\nDespite the clear benefit of the factorization described by the Hammersley Clifford theorem, the representation of a factor as a potential does not allow to encode CSIs. These patterns are more easily encoded in a more convenient representation called log-linear. The log-linear model represents a Gibbs distribution by using a set of features F to represent the potentials. A feature is an assignment to a subset of variables of domain. We denote a features as f jC , to make more clear the distinction between the features of a log-linear and its input assignment x. Thus, a potential in a loglinear is represented as a linear combination of features as follows:\nφC(XC = x〈C〉) =\n|Val(XC)|∑\nj\nθjδ(x〈C〉, f j C),\nwhere δ(x〈C〉, f jC) is the Kronecker delta function, that is, it equals to 1 when x〈C〉 = f jC , and 0 otherwise. By joining the linear combinations of all the potentials and merging its indexes into a unique index α ∈ {1, . . . , |F|}, we can represent Equation (9) by using the following log-linear model:\np(X = x) = exp{ ∑\nα\nθαδ(x〈Cα〉, f α Cα )− ln(Z)}. (10)\nIn the next section we present the context-specific Hammersley-Clifford theorem, a generalization of the Hammersley Clifford theorem that shows how to factorize a distribution (represented by a log-linear) using a context-specific dependency model Ic that captures the CSIs."
    }, {
      "heading" : "4 Context-specific",
      "text" : "Hammersley-Clifford\nThis section presents the main contribution of this work: a generalization of the Hammersley-Clifford theorem for factorizing a distribution represented by a log-linear based on a context-specific dependency model Ic CSI-map of p(X). For this, we begin by defining the following Corollary of the HammersleyClifford theorem:\nCorollary 1 (Independence-based Hammersley-Clifford). Let p(X) be a positive distribution, and let I be a graph-isomorph dependency model over X. If I is\nan I-map of p(X), then p(X) can be factorized into a set of potential functions {φCi(XCi)}i, such that for any I(Xa, Xb | XW ) that is true in I, there is no factor φi(XCi) that contains both variables Xa and Xb in XCi .\nProof. From the assmuptions, I is graph-isomorph and is an I-map of p(x). By definition, the former implies there exists an undirected graph G(V,E) that exactly encodes I, and it therefore must also be Imap of p(x). The assumptions of the HammersleyClifford Theorem 1 hold, so p(X) can be factorized into a set of potential functions over the cliques of G. Also, since I is graph-isomorph, its conditional independences satisfy the Pearl axiom, in particular the strong union axiom. Therefore if conditional independence I(Xa, Xb | XW ) is in I, the conditional independence I(Xa, Xb | X \\Xa, Xb) is also in I. Using this fact in the pairwise Markov property we can imply the no-edge (a, b) /∈ E; in other words, a and b cannot belong to the same clique. Since HammersleyClifford holds, this last fact implies no factor φi(XCi) can contain both variables Xa and Xb in XCi .\nThis corollary shows how to use a dependency model I (instead of a graph) to factorize the distribution p(X). In what follows, we present theoretical results that show how a context-specific dependency model Ic can be used to factorize P (X). The general rationale is to decompose Ic into subsets of CSIs contextualized on certain context xW that are themselves dependency models over sub-domains, and use those to decompose the conditional distributions of p(X) using Hammersley-Clifford.\nDefinition 6 (Reduced dependency model). Let p(X) be a distribution over X, xW a context over subset XW ⊆ X, and Ic a context-specific dependency model over X. We define the reduced dependency model IxW of Ic over domain X \\ XW as the rule that for each Xa, Xb ∈ X, each pair XU , XW of disjoint subsets of X \\ {Xa, Xb}, and each assignments xW of XW , assigns a truth value to a triplet 〈Xa, Xb | XU , xW 〉 from independence assertions in Ic as follows:\nIxW (〈Xa, Xb | XU , xW 〉) = (11)∧\nXU∈V al(XU )\nIc(〈Xa, Xb | xU , xW 〉)\nThe following proposition relates the CSI-mapness of a context-specific dependency model and the I-mapness of its reduced dependency models.\nProposition 1. Let p(X) be a distribution over X, xW be a context over subset of X, and Ic be a context-\nspecific dependency model over X. If Ic is a CSI-map of p(X), then IxW is an I-map of p(X \\XW | xW ).\nProof. We start arguing that IxW is a CSI-map of p(X), and then extend the proof to show that it is an I-map of the conditional p(X \\ XW | xW ). That IxW is a CSI-map of p(X) follows from the fact that Ic is a CSI-map of p(X), that implies that not only its CSIs holds in p(X), but any CSI obtained by conjoining those CSIs over all values of any of its variables, in particular the conjunction of Equation (11). That IxW is an I-map of p(X \\XW | xW ) follows from the fact that any CSI I(Xa, Xb | XU , xW ) in p(X) is equivalent to a conditional independence I(Xa, Xb | XU ) in the conditional p(X \\XW | xW ).\nIn the next auxiliary lemma it is shown how to factorize a distribution p(X) using a dependency model IxW :\nAuxiliary Lemma 1. Let p(X) be a positive distribution over X, Ic be a dependency model over X, and IxW be a graph-isomorph dependency model over X \\ XW . If IxW is an I-map of the conditional p(X \\XW | xW ), then this conditional can be factorized into a set of potential functions {φi(XCi)}i over X \\XW , such that for any I(Xa, Xb | XU , xW ) that is true in IxW , there is no factor φi(XCi) that contains both a and b in Ci.\nProof. The proof consists on using Corollary 1 for the conditional p(X \\ XW | xW ) as the distribution, and IxW as the dependency model. For that, we show they satisfy the requirements of the Corollary, that is, p(X \\ XW | xW ) is positive, and IxW is a graphisomorph dependency model over domain X XW that is an I-map of the conditional. The IxW is an I-map of the conditional and graph-isomorph follows from the assumptions. It remains to prove then the positivity of the conditional. For that, the conditional is expanded as follows:\np(X \\ {XW } | xW ) = p(X \\ {XW }, xW )\np(xW )\n= p(X \\ {XW }, xW )∑\nxX\\XW ∈Val(X\\{XW }) p(xX\\W , xW )\n,\nwhere the sum expansion of the denominator follows from the law of total probability. The conditional has been expressed then as an operation over joints, and being all positive, it follows that both the numerator and denominator, and therefore the whole quotient is positive.\nWith Lemma 1, we can present our main theoretical result, a theorem that generalizes Theorem 1 to factorize the features F in a log-linear of p(X) according to some given context-specific dependency model Ic. For this, we need to define precisely what we mean by factorization of a set of features F . We do this in two steps, one that defines a factorization according to dependency models, and then the contextualized case for context-specific dependency models.\nDefinition 7 (Feature factorization). Let F be a set of features over some domain X, and IxW some reduced dependency model over X XW . We say features F factorize according to IxW if for each I(Xa, Xb | XU , xW ) that is true in IxW , and each feature fC ∈ F such that fC〈W 〉 = xW , it holds that either a /∈ C or b /∈ C.\nDefinition 8 (Context-specific feature factorization). Let F be a set of features over some domain X, and Ic be a context-specific dependency model. The features F are said to factorize according to Ic if they factorize according to each reduced dependency model Ixw of Ic (as defined by Definition 7), with XW ⊆ X, and xW ∈ Val(XW ).\nWe present now our main theorem, and then discuss practical issues regarding its requirements.\nTheorem 2 (Context-Specific Hammersley-Clifford). Let p(X) be a positive distribution over X, F be a set of features from a log-linear of p(X), and Ic be a context-specific dependency model over X, such that each of its reduced dependency models (over all possible contexts) is graph-isomorph. If Ic is CSI-map of p(X) then F factorizes according to Ic.\nProof. From the definition of context-specific feature factorization, the conclusion of the theorem holds if F factorizes according to each reduced dependency model of Ic. So let IxW be some arbitrary reduced dependency model for context xW , and prove F factorizes according to IxW , which by Definition 7 requires that (a) for each I(Xa, Xb | Xu, xW ) that is true in IxW , and (b) for each fC ∈ F s.t. fC〈W 〉 = xW , it holds that (c) either a /∈ C or b /∈ C.\nTo proceed then, we first apply the Auxiliary Lemma 1 for p(X), the context xW , and the reduced dependency model IxW . These requirements are satisfied, that is, p(X) is positive and IxW is both graph-isomorph and I-map of the conditional p(X \\XW | xW ) (by Proposition 1). From this we conclude the consequent of the Lemma, i.e., that the conditional p(X \\ XW | xW ) can be factorized into a set of potencial functions {φi(XCi)}i s.t. (i) for each I(Xa, Xb | XU , xW ) that is true in IxW , (ii) for each factor φi(XCi) ∈ {φi(XCi)}i, it holds that (iii) either a /∈ Ci or b /∈ Ci.\nTo conclude then, we argue that conclusions (i), (ii)\nand (iii) of the Auxiliary Lemma are equivalent to the requirements (a), (b), and (c) of the factorization. Clearly, conclusions (i) and (iii) matches requirement (a) and (c) of the factorization. We now show the equivalence of (ii) with (b). A factor φi(XCi) of the conditional p(X \\XW | xW ) is equivalent to a factor φi(XCi , xW ) over the joint p(X), which is composed of features fCi∪W whose values over XW matches xW , i.e., fCi∪W 〈W 〉 = xW .\nThe theorem requires that each possible reduced dependency model of Ic be graph-isomorph. What is the implication of this requirement? By definition of graph-isomorphism, this implies that for each possible context xW , the reduced dependency model IxW can be encoded as an undirected graph over the subdomain X \\ XW . This provides us a mean to construct Ic graphically, i.e., constructing an undirected graph for each possible sub-domain and assignment of its complement. In practice, this may be done by experts that provide a list of CSIs that hold in the domain, or running a structure learning algorithm over each context. This may sound overly complex, as there are cleary an exponential number of such contexts. No doubt future works can explore this aspect, finding alternatives for simplifying this complexity on different special cases."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We have presented a theoretical method for factorizing a Markov random field according to the CSIs present in a distribution, that is formally guaranteed to be correct. This is presented by the context-specific Hammersley-Clifford theorem, as a generalization to CSIs of the Hammersley-Clifford theorem that applies for conditional independences. According with our theoretical result, we believe that it is worth guiding our future work in implementing algorithms for learning from data the structure of MRFs for each possible context, and then factorizing the distribution by using the learned structures. Intuitively, it seems likely to achieve improvements in time, space and sample complexities, in comparison with other approaches that only uses conditional independences encoded by the graph."
    } ],
    "references" : [ {
      "title" : "Context-specific independence in Bayesian networks",
      "author" : [ "C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller" ],
      "venue" : "Proceedings of the Twelfth international conference on Uncertainty in artificial intelligence, pages 115–123. Morgan Kaufmann",
      "citeRegEx" : "Boutilier et al\\.,? 1996",
      "shortCiteRegEx" : "Boutilier et al\\.",
      "year" : 1996
    }, {
      "title" : "A Bayesian Approach to Learning Bayesian Networks with Local Structure",
      "author" : [ "D.M. Chickering", "D. Heckerman", "C. Meek" ],
      "venue" : "Uncertainty in Artificial Intelligence, pages 80–89. Morgan Kaufmann Publishers Inc.",
      "citeRegEx" : "Chickering et al\\.,? 1997",
      "shortCiteRegEx" : "Chickering et al\\.",
      "year" : 1997
    }, {
      "title" : "Inducing Features of Random Fields",
      "author" : [ "S. Della Pietra", "V.J. Della Pietra", "J.D. Lafferty" ],
      "venue" : "IEEE Trans. PAMI., 19(4):380–393.",
      "citeRegEx" : "Pietra et al\\.,? 1997",
      "shortCiteRegEx" : "Pietra et al\\.",
      "year" : 1997
    }, {
      "title" : "Context-specific independence in directed relational probabilistic models and its influence on the efficiency of Gibbs sampling",
      "author" : [ "D. Fierens" ],
      "venue" : "European Conference on Artificial Intelligence, pages 243–248.",
      "citeRegEx" : "Fierens,? 2010",
      "shortCiteRegEx" : "Fierens",
      "year" : 2010
    }, {
      "title" : "Knowledge representation and inference in similarity networks and Bayesian multinets",
      "author" : [ "D. Geiger", "D. Heckerman" ],
      "venue" : "Artificial Intelligence, 82:45–74.",
      "citeRegEx" : "Geiger and Heckerman,? 1996",
      "shortCiteRegEx" : "Geiger and Heckerman",
      "year" : 1996
    }, {
      "title" : "Learning efficient markov networks",
      "author" : [ "V. Gogate", "W. Austin", "W.P. Domingos" ],
      "venue" : null,
      "citeRegEx" : "Gogate et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gogate et al\\.",
      "year" : 2010
    }, {
      "title" : "Markov fields on finite graphs and lattices",
      "author" : [ "J.M. Hammersley", "P. Clifford" ],
      "venue" : null,
      "citeRegEx" : "Hammersley and Clifford,? \\Q1971\\E",
      "shortCiteRegEx" : "Hammersley and Clifford",
      "year" : 1971
    }, {
      "title" : "Learning Bayesian networks: The combination of knowledge and statistical data",
      "author" : [ "D. Heckerman", "D. Geiger", "D.M. Chickering" ],
      "venue" : "Machine Learning.",
      "citeRegEx" : "Heckerman et al\\.,? 1995",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 1995
    }, {
      "title" : "Probabilistic Graphical Models: Principles and Techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : "MIT Press, Cambridge.",
      "citeRegEx" : "Koller and Friedman,? 2009",
      "shortCiteRegEx" : "Koller and Friedman",
      "year" : 2009
    }, {
      "title" : "Learning Bayesian belief networks: an approach based on the MDL principle",
      "author" : [ "W. Lam", "F. Bacchus" ],
      "venue" : "Computational Intelligence, 10:269–293.",
      "citeRegEx" : "Lam and Bacchus,? 1994",
      "shortCiteRegEx" : "Lam and Bacchus",
      "year" : 1994
    }, {
      "title" : "Efficient structure learning of Markov networks using L1-regularization",
      "author" : [ "S. Lee", "V. Ganapathi", "D. Koller" ],
      "venue" : "Neural Information Processing Systems. Citeseer.",
      "citeRegEx" : "Lee et al\\.,? 2006",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2006
    }, {
      "title" : "Learning markov network structure with decision trees",
      "author" : [ "D. Lowd", "J. Davis" ],
      "venue" : "Data Mining (ICDM), 2010 IEEE 10th International Conference on, pages 334–343. IEEE.",
      "citeRegEx" : "Lowd and Davis,? 2010",
      "shortCiteRegEx" : "Lowd and Davis",
      "year" : 2010
    }, {
      "title" : "Efficiently inducing features of conditional random fields",
      "author" : [ "A. McCallum" ],
      "venue" : "Proceedings of Uncertainty in Artificial Intelligence (UAI).",
      "citeRegEx" : "McCallum,? 2003",
      "shortCiteRegEx" : "McCallum",
      "year" : 2003
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : "Morgan Kaufmann Publishers, Inc., 1re edition.",
      "citeRegEx" : "Pearl,? 1988",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "GRAPHOIDS : A graph based logic for reasonning ab relevance relations",
      "author" : [ "J. Pearl", "A. Paz" ],
      "venue" : "Technical Report 850038 (R-53-L), Cognitive Systems Laboratory, University of California, Los Angeles.",
      "citeRegEx" : "Pearl and Paz,? 1985",
      "shortCiteRegEx" : "Pearl and Paz",
      "year" : 1985
    }, {
      "title" : "Exploiting contextual independence in probabilistic inference",
      "author" : [ "D. Poole", "N.L. Zhang" ],
      "venue" : "J. Artif. Intell. Res. (JAIR), 18:263–313.",
      "citeRegEx" : "Poole and Zhang,? 2003",
      "shortCiteRegEx" : "Poole and Zhang",
      "year" : 2003
    }, {
      "title" : "High-dimensional Ising model selection using L1-regularized logistic regression",
      "author" : [ "P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty" ],
      "venue" : "Annals of Statistics, 38:1287–1319.",
      "citeRegEx" : "Ravikumar et al\\.,? 2010",
      "shortCiteRegEx" : "Ravikumar et al\\.",
      "year" : 2010
    }, {
      "title" : "Causation, Prediction, and Search",
      "author" : [ "P. Spirtes", "C. Glymour", "R. Scheines" ],
      "venue" : "Adaptive Computation and Machine Learning Series. MIT Press.",
      "citeRegEx" : "Spirtes et al\\.,? 2000",
      "shortCiteRegEx" : "Spirtes et al\\.",
      "year" : 2000
    }, {
      "title" : "Inference for multiplicative models",
      "author" : [ "Y. Wexler", "C. Meek" ],
      "venue" : "Uncertainty in Artificial Intelligence, pages 595–602.",
      "citeRegEx" : "Wexler and Meek,? 2008",
      "shortCiteRegEx" : "Wexler and Meek",
      "year" : 2008
    }, {
      "title" : "Markov network structure learning: A randomized feature generation approach",
      "author" : [ "J. Van Haaren", "J. Davis" ],
      "venue" : "Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (2012).",
      "citeRegEx" : "Haaren and Davis,? 2012",
      "shortCiteRegEx" : "Haaren and Davis",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Markov random fields (MRFs), also known as undirected graphical models, or Markov networks, belong to the family of probabilistic graphical models (Koller and Friedman, 2009), a well-known computational framework for compact representation of joint probability distributions.",
      "startOffset" : 147,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "This factorization can be done by using the well-known Hammersley-Clifford theorem (Hammersley and Clifford, 1971).",
      "startOffset" : 83,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "An important issue of using a graph to represent independences is that it cannot encode some types of independence relations, such as the context-specific independences (CSIs) (Boutilier et al., 1996).",
      "startOffset" : 176,
      "endOffset" : 200
    }, {
      "referenceID" : 1,
      "context" : "(Chickering et al., 1997; Fierens, 2010; Poole and Zhang, 2003; Wexler and Meek, 2008; Lowd and Davis, 2010; Ravikumar et al., 2010).",
      "startOffset" : 0,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "(Chickering et al., 1997; Fierens, 2010; Poole and Zhang, 2003; Wexler and Meek, 2008; Lowd and Davis, 2010; Ravikumar et al., 2010).",
      "startOffset" : 0,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "(Chickering et al., 1997; Fierens, 2010; Poole and Zhang, 2003; Wexler and Meek, 2008; Lowd and Davis, 2010; Ravikumar et al., 2010).",
      "startOffset" : 0,
      "endOffset" : 132
    }, {
      "referenceID" : 18,
      "context" : "(Chickering et al., 1997; Fierens, 2010; Poole and Zhang, 2003; Wexler and Meek, 2008; Lowd and Davis, 2010; Ravikumar et al., 2010).",
      "startOffset" : 0,
      "endOffset" : 132
    }, {
      "referenceID" : 11,
      "context" : "(Chickering et al., 1997; Fierens, 2010; Poole and Zhang, 2003; Wexler and Meek, 2008; Lowd and Davis, 2010; Ravikumar et al., 2010).",
      "startOffset" : 0,
      "endOffset" : 132
    }, {
      "referenceID" : 16,
      "context" : "(Chickering et al., 1997; Fierens, 2010; Poole and Zhang, 2003; Wexler and Meek, 2008; Lowd and Davis, 2010; Ravikumar et al., 2010).",
      "startOffset" : 0,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "For this, a log-linear model is used as a more fine-grained representation of the MRFs (Koller and Friedman, 2009).",
      "startOffset" : 87,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "There are several works in the literature (Della Pietra et al., 1997; Lee et al., 2006; Lowd and Davis, 2010; Van Haaren and Davis, 2012) that learn log-linear models directly by presenting different procedures for selecting features from data.",
      "startOffset" : 42,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "There are several works in the literature (Della Pietra et al., 1997; Lee et al., 2006; Lowd and Davis, 2010; Van Haaren and Davis, 2012) that learn log-linear models directly by presenting different procedures for selecting features from data.",
      "startOffset" : 42,
      "endOffset" : 137
    }, {
      "referenceID" : 0,
      "context" : "CSIs were first introduced by (Boutilier et al., 1996) by coding them locally within conditional probability tables (factors) of Bayesian networks as decision trees.",
      "startOffset" : 30,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "The work of (Gogate et al., 2010) is the closests to our work, presenting an algorithm for factorizing a loglinear model according to CSIs.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 13,
      "context" : "Conditional independences are regularities of distributions that has been extensively studied in the field of statistics, demonstrating how they can be effectively and soundly used for reducing the dimensionality of the distribution (Pearl, 1988; Spirtes et al., 2000; Koller and Friedman, 2009).",
      "startOffset" : 233,
      "endOffset" : 295
    }, {
      "referenceID" : 17,
      "context" : "Conditional independences are regularities of distributions that has been extensively studied in the field of statistics, demonstrating how they can be effectively and soundly used for reducing the dimensionality of the distribution (Pearl, 1988; Spirtes et al., 2000; Koller and Friedman, 2009).",
      "startOffset" : 233,
      "endOffset" : 295
    }, {
      "referenceID" : 8,
      "context" : "Conditional independences are regularities of distributions that has been extensively studied in the field of statistics, demonstrating how they can be effectively and soundly used for reducing the dimensionality of the distribution (Pearl, 1988; Spirtes et al., 2000; Koller and Friedman, 2009).",
      "startOffset" : 233,
      "endOffset" : 295
    }, {
      "referenceID" : 14,
      "context" : "A necessary and sufficient condition for dependency models to be graph-isomorph is that all its independence assertions satisfy the following independence axioms, commonly called the Pearl axioms (Pearl and Paz, 1985):",
      "startOffset" : 196,
      "endOffset" : 217
    }, {
      "referenceID" : 8,
      "context" : "Definition 3 (Pairwise Markov property (Koller and Friedman, 2009)).",
      "startOffset" : 39,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "In this work we focus in a finer-grained type of independences: the context-specific independences (CSI) (Boutilier et al., 1996; Geiger and Heckerman, 1996; Chickering et al., 1997; Koller and Friedman, 2009).",
      "startOffset" : 105,
      "endOffset" : 209
    }, {
      "referenceID" : 4,
      "context" : "In this work we focus in a finer-grained type of independences: the context-specific independences (CSI) (Boutilier et al., 1996; Geiger and Heckerman, 1996; Chickering et al., 1997; Koller and Friedman, 2009).",
      "startOffset" : 105,
      "endOffset" : 209
    }, {
      "referenceID" : 1,
      "context" : "In this work we focus in a finer-grained type of independences: the context-specific independences (CSI) (Boutilier et al., 1996; Geiger and Heckerman, 1996; Chickering et al., 1997; Koller and Friedman, 2009).",
      "startOffset" : 105,
      "endOffset" : 209
    }, {
      "referenceID" : 8,
      "context" : "In this work we focus in a finer-grained type of independences: the context-specific independences (CSI) (Boutilier et al., 1996; Geiger and Heckerman, 1996; Chickering et al., 1997; Koller and Friedman, 2009).",
      "startOffset" : 105,
      "endOffset" : 209
    }, {
      "referenceID" : 0,
      "context" : "Definition 4 (Context-specific independence (Boutilier et al., 1996)).",
      "startOffset" : 44,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "Since each CSIs is defined for a specific context, they cannot be represented all together in a single undirected graph (Koller and Friedman, 2009).",
      "startOffset" : 120,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "If every independence assertion contained in Ic holds for p(X), Ic is said to be an CSI-map of p(X) (Boutilier et al., 1996).",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "Theorem 1 (Hammersley-Clifford (Hammersley and Clifford, 1971)).",
      "startOffset" : 31,
      "endOffset" : 62
    } ],
    "year" : 2013,
    "abstractText" : "Markov random fields provide a compact representation of joint probability distributions by representing its independence properties in an undirected graph. The well-known Hammersley-Clifford theorem uses these conditional independences to factorize a Gibbs distribution into a set of factors. However, an important issue of using a graph to represent independences is that it cannot encode some types of independence relations, such as the context-specific independences (CSIs). They are a particular case of conditional independences that is true only for a certain assignment of its conditioning set; in contrast to conditional independences that must hold for all its assignments. This work presents a method for factorizing a Markov random field according to CSIs present in a distribution, and formally guarantees that this factorization is correct. This is presented in our main contribution, the context-specific Hammersley-Clifford theorem, a generalization to CSIs of the Hammersley-Clifford theorem that applies for conditional independences.",
    "creator" : "LaTeX with hyperref package"
  }
}