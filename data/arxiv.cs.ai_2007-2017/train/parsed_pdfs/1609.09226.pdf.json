{
  "name" : "1609.09226.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ICE: Information Credibility Evaluation on Social Media via Representation Learning",
    "authors" : [ "Qiang Liu", "Shu Wu", "Feng Yu", "Liang Wang" ],
    "emails" : [ "tnt}@nlpr.ia.ac.cn." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—information credibility evaluation, rumor detection, social media, representation learning.\nI. INTRODUCTION\nW ITH the rapid growth of social media, such as Face-book, Twitter, and Sina Weibo, people are able to share information and express their attitudes publicly. Social media brings great convenience to users, and information can be spread more rapidly and widely nowadays. At the same time, rumors can also be spread on the Internet more easily and viewed by more people. A rumor is an unverified and instrumentally relevant statement of information spreading among people [5]. Rumors bring significant harm to daily life, social harmony, or even public security. With the growth of the Internet and social media, such harm will also grow greater. For instance, as the loss of MH370 has drawn worldwide attention, a great amount of rumors has spread on social media, e.g., MH370 has landed in China,1 the loss of MH370 is\nThe authors are with the University of Chinese Academy of Sciences (UCAS) and the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, 100190, China. E-mail: {qiang.liu, shu.wu, feng.yu, wangliang, tnt}@nlpr.ia.ac.cn.\n1http://www.fireandreamitchell.com/2014/03/07/rumor-malaysia-airlinesmh370-landed-china/\ncaused by terrorists,2 and Russian jets are related to the loss of MH370.3 These rumors about MH370 mislead public attitudes to a wrong direction and delay the search of MH370. Up to October 10, 2015, on the biggest Chinese microblog website Sina Weibo,4 28,454 rumors have been reported and collected in its misinformation management center.5 Accordingly, it is crucial to evaluate information credibility and to detect rumors on social media.\nNowadays, to automatically evaluate information credibility on social media, some methods have been proposed. Existing methods are mainly based on feature engineering, i.e., methods with handcrafted features. Most of the methods are based on content information and source credibility at the microblog level [3][27][10] or event (containing a group of microblogs) level [16][42][22]. Some research also studies the aggregation of credibility from the microblog level to the event level [14]. On the contrary, considering dynamic information, some work designs temporal features based on prorogation properties over time [16] or trains a model with features generated from different time periods [22]. Some works also take usage of users’ feedbacks (comments and attitudes) to evaluate credibility [8][29]. The Enquiry Post (EP) model [42] takes out signal tweets, which indicates users’ suspicious attitudes for detecting rumors and achieves satisfactory performance.\nAlthough these methods have been widely used, they have several drawbacks. First, these methods based on feature engineering require great labor for designing features [3]. Moreover, a rough fusion resting on the statistical summation of these feature values is not competent to model elaborate interactions among different features on social media for information credibility evaluation. For instance, there are two combinations: (1) “a user with high credibility posted a microblog” and “a user with low credibility reposted a microblog” and (2) “a user with low credibility posted a microblog” and “a user with high credibility reposted a microblog”. The values of the corresponding features, such as user credibility (high or low) and behavior type (post or repost) from the above two combinations, are equal by statistical summation. Therefore, a rough fusion rested on statistical summation cannot tell these two combinations apart. Intuitively, the former combination is more like the style of non-rumor and the later combination is more like the\n2http://www.csmonitor.com/World/Asia-Pacific/2014/0310/MalaysiaAirlines-flight-MH370-China-plays-down-terrorism-theories-video\n3http://www.inquisitr.com/1689765/malaysia-airlines-flight-mh370-russianjets-in-baltic-may-hold-clue-to-how-flight-370-vanished/\n4http://weibo.com 5http://service.account.weibo.com/?type=5&status=4\nar X\niv :1\n60 9.\n09 22\n6v 1\n[ cs\n.S I]\n2 9\nSe p\n20 16\nstyle of rumor. On the contrary, methods based on feature engineering cannot model some real-world scenarios from a joint perspective, i.e., who did what at when and how others reacted. Those methods treat factors (who, what, when, how) as separate features and can extract simple compound features, such as a user with low credibility tended to post a rumor. There are complicated compound features, such as a user with low credibility posted a rumor at early stage of spreading receiving suspicion comments and a user with high credibility reposted a rumor at medium term of spreading receiving identification comments. The analysis of those complicated compound features from statistical summation requires enumerating all possible complicated compound features, which results in the explosion of time complexity and problem of data sparsity. What’s more, a complicated compound feature may include some simple compound features, so the statistical summation of those enumerated compound features cannot truly reflect the distribution of semantic information, because a simple compound feature combined with another simple compound may generate a reverse meaning, such as “a user with high credibility” may be a non-rumor style and “a user with high credibility reposted a rumor at medium term of spreading” may be a rumor style. As a consequence, it is better to model elaborate interactions among different features to obtain an overall and joint understanding of complicated behaviors on social media.\nIn this paper, we evaluate the credibility of information about events on social media. Usually, each event contains several microblogs posted and reposted by users. To identify whether an event is a rumor or not, we first investigate microblogs of events on social media. Figure 1 shows some examples of rumors on Sina Weibo with extracted source\ninformation, content information, temporal information, and comment information. According to this information on social media, we conclude four key factors for evaluating information credibility on social media:\n(1) “Who” means source credibility or user credibility. Generally speaking, a source usually means a user. Normally, the higher the credibility of a user, the higher the credibility of information it creates [3]. However, some studies [13] point out that a great amount of users with high credibility on social media would repost and share misinformation unintentionally. As shown in the example in Figure 1, even a regular media (usually with high credibility), Global Times, would post unverified news about MH370. Therefore, it is not reliable to model user credibility information alone for information credibility evaluation.\n(2) “What” denotes behavior types. Usually, there are two types of behaviors for users, i.e., posting and reposting. Compared to reposts, an original post indicates that the microblog is more original and relatively more important for evaluating credibility. For non-rumors, original microblogs are usually posted by users with high credibility. For rumors, original microblogs are posted by users with low credibility, whereas users with high credibility may repost the microblogs.\n(3) “When” refers to temporal properties that describe the spread process of a microblog post. As shown in Figure 2(a), temporal properties are usually different between rumors and non-rumors. Compared to rumors, most non-rumor microblogs tended to be posted or reposted at the beginning and vanish very fast. Maybe those plain truths will become less and less attractive as time goes on. However, rumors usually draw comparatively sustained attention. Moreover, the spreading curve of rumors may have multiple peaks. There may be some rumormongers promoting the spreading of rumors. In addition, for non-rumors, original microblogs at the beginning event are usually posted by users with high credibility, whereas, for rumors, original microblogs are usually posted by users with low credibility and then reposted by other users including those with high credibility.\n(4) “How” denotes comments and attitudes towards corresponding microblogs. Users on social media can express their attitudes and collective intelligence can be gathered to help us evaluate the credibility of information [8]. Comments reveal the users suspicion or identification attitudes towards microblogs. As shown in Figure 2(b), rumors usually receive more suspicion comments, which is extremely helpful for detecting rumors.\nThe aggregation of these key factors makes the joint perspective of a microblog and helps evaluate credibility. However, conventional feature engineering-based methods consider these factors as separate features and roughly summarize them, which cannot model the interactions among the key factors. Accordingly, we plan to learn representations to obtain an overall and joint understanding of complicated and dynamic behaviors in information spreading and evaluate the credibility, which are shaped by modeling elaborate interactions among different features. To be specific, we model semantic operations among different features and form an overall representation of each microblog post. Recently, representa-\ntion learning [1] is showing a promising performance in a variety of applications, such as word embedding [23][24][25], network embedding [2][9][26][32], and user representations [6][7][18][19].\nTo the end, we propose a novel ICE model that learns the joint representations of key factors of microblogs and further evaluates the credibility of events. In ICE, each user is represented as a vector according to his or her personal features, indicating the credibility information of the user (“who”). For the sake of modeling elaborate interactions among different features, other features such as “what”, “how”, and “when” are represented as operating matrices [19]. For instance, behavior types (post or repost) are modeled as latent operating matrices indicating the properties of different behaviors (“what”) and we incorporate matrix representations for time intervals since the beginning of spreading of an event’s information to capture the temporal properties of behaviors (“when”). Moreover, the attitudes of comments (suspicious or not) are also modeled as latent operating matrices indicating collective attitudes (“how”). These operating matrices model semantic operations of one feature on another. Consequently, based on the representations of users, the representations of dynamic and complicated behaviors can be obtained through multiplication with all operating matrices of varied features. Each representation of a dynamic behavior can also be viewed as a representation of a corresponding microblog about a specific event. After aggregating all the microblog representations during information spreading, we can generate the credibility representation of the event. Then, we apply a pairwise learning method to enlarge the credibility difference between rumors and non-rumors for a better and fast learning of parameters. We crawl a data set from Sina Weibo, and experiments show that our model achieves better performance compared to stateof-the-art methods.\nThe main contributions of this work are listed as follows:\n• We introduce a representation learning method for information credibility evaluation. The proposed method captures elaborate interactions among the key factors of microblogs during information spreading through learning operating matrices, which model abundant semantic\noperations among varied features. • ICE learns latent representations for user credibility,\nbehavior types, temporal properties and attitudes of comments. Based on these representations, ICE generates overall credibility representations of information and presents a novel perspective on information credibility evaluation. • Experiments conducted on a real-world data set show that ICE is effective and clearly outperforms the state-of-theart methods.\nThe rest of the paper is organized as follows. In Section 2, we review some related work on truth discovery, credibility evaluation, and representation learning. In Section 3, we introduce our data set and give some analysis. Section 4 details our ICE model. In Section 5, we report experimental results on the Weibo data set and compare them to several state-of-theart methods. In Section 6, we present a real-time information credibility evaluation system that we constructed based on our proposed model. Section 7 concludes our work and discusses future research."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "In this section, we review some related works, including credibility evaluation on social media, representation learning and truth discovery."
    }, {
      "heading" : "A. Credibility Evaluation on Social Media",
      "text" : "Recently, some works have been proposed to automatically evaluate the information credibility and detect rumors on social media. Most of the methods are based on artificial features. Some of them evaluate the credibility of a single microblog [3][27] or a single image [10]. Some of them evaluate information credibility at the event level to distinguish whether an event is a rumor or a non-rumor [11][31][16][42][22], where each event consists of several microblogs. News Credibility Propagation (NewsCP) [14] studies how to aggregate credibility from the microblog level to the event level and presents a graph optimization method, which has further incorporated conflict viewpoints in the model [15]. Some works detect\nrumors based on the dynamic properties. For instance, the Periodic External Shocks (PES) model [16] uses ordinary structural features and user features and designs temporal features according to the properties of information spreading over time.The Dynamic Series-Time Structure (DSTS) [22] generates content-, user-, and diffusion-based features in different time periods during information spreading and uses all these features to train a model. Some works also take usage of users feedbacks to evaluate credibility [8][29]. The EP model [42] extracts signal tweets that indicate users suspicious attitudes for detecting rumors and achieves satisfactory performance. The main drawback of these feature engineering-based models lies in that they require great labor for designing a great many features and cannot reveal underlying relations among these features. Moreover, these methods have difficulty in modeling elaborate interactions among different factors during information spreading."
    }, {
      "heading" : "B. Representation Learning",
      "text" : "Nowadays, representation learning [1] has been extensively studied in different areas. In natural language processing, learning embeddings [25] is a hot topic, where recurrent neural networks [23][24] are widely applied. In web mining, learning network embedding has drawn great attention for studying node classification [12] or information diffusion [2]. Recently, network embedding models have incorporated random walk [26][9] and second-order connection in the representation learning methods [32]. Meanwhile, representation models are also playing a role for modeling user behaviors. Contextual Operating Tensor (COT) [19][36] and CARS2 [30] study context-aware user representations for recommendation. Hierarchical Interaction Representation (HIR) [18] studies joint representations of entities, e.g., users, items and contexts, to model their interaction. Some works [6][37][41] utilize deep neural networks for better user modeling. Convolutional Click Prediction Model (CCPM) [21] applies convolutional neural networks in predicting clicking behaviors of users. Hierarchical Representation Model (HRM) [34] and Dynamic Recurrent Basket Model (DREAM)[40] learn the representation of behaviors of a user in a short period for better recommendation. These methods achieve the state-of-the-art performance in different areas, and give us inspiration for learning representations of dynamic behaviors to evaluate information credibility.\nNowadays, representation learning [1] has been extensively studied in different areas. In natural language processing, learning embedding [25] is a hot topic, where recurrent neural networks [23][24] are widely applied. In web mining, learning network embedding has drawn great attention for studying node classification [12] or information diffusion [2]. Recently, network embedding models have incorporated random walk\n[26][9] and second-order connection in the representation learning methods [32]. Meanwhile, representation models are also playing a role for modeling user behaviors. The contextual operating tensor (COT) [19][36] and CARS2 [30] study context-aware user representations for recommendation. Hierarchical Interaction Representation (HIR) [18] studies joint representations of entities, e.g., users, items, and contexts, to model their interaction. Some works [6][37][41] use deep neural networks for better user modeling. The convolutional click prediction model (CCPM) [21] applies convolutional neural networks in predicting clicking behaviors of users. The hierarchical representation model (HRM) [34] and the dynamic recurrent basket model (DREAM) [40] learn the representation of behaviors of a user in a short period for better recommendation. These methods achieve state-of-the-art performance in different areas and give inspiration for learning representations of dynamic behaviors to evaluate information credibility."
    }, {
      "heading" : "C. Truth Discovery",
      "text" : "Truth discovery refers to the problem of finding the truth with conflicting information, which has been first addressed in [38]. It can be viewed as some kind of information credibility evaluation. Mainly based on the source credibility information, truth discovery evaluates the credibility via aggregating from different sources. Truth discovery methods are usually based on Bayesian algorithms or graph learning algorithms on stock data or flight data [17][33]. And Semi-Supervised Truth Discovery (SSTF) [39] studies the problem with semi-supervised graph learning with a small set of ground truth data to help evaluating credibility. Truth discovery is an unsupervised or semi-supervised method to find the truth with conflicting information and make an evaluation of information credibility [17]. Truth discovery is mainly based on the evaluated credibility aggregated from different information sources, usually referring to users who release information. And it is not capable to make the most of various kinds of information, such as time properties and comment attitudes, which are abundant in complex online social media scenario. Therefore, truth discovery is suitable for ideal situations with constrained topics, such as price prediction and flight arrival prediction. They are hard to be applied in complex online social media."
    }, {
      "heading" : "III. DATA",
      "text" : "In this section, we introduce our data set. Considering that there is a lack of public rumor data sets, we collected a microblog data set containing rumors and non-rumors from Sina Weibo, which is the biggest social media in China.\nTo crawl rumors, we collected some rumor seeds, i.e., some microblogs containing rumors that have been reported,\nfrom misinformation management center of Sina Weibo. We extracted keywords from these rumor seeds and retrieved rumor microblogs with these keywords. Then, we identified the starting point of a rumor, i.e., the first microblog about the rumor, and collected all the following microblogs. For each microblog, we collected its reposting information, commenting information, and the corresponding users profile. To crawl non-rumors, we collected some hot topics on Sina Weibo and used the same strategy as for rumors to crawl corresponding information about the non-rumors.\nAs shown in Table I, we collected 936 events containing 500 rumors and 436 non-rumors. Each event consists of several microblogs (postings or repostings), and the average number is about 673. The total number of microblogs is 630, 363, including 98, 429 postings and 532, 236 repostings. Each microblog has its posting time. The posting time of the first microblog about an event is set as the beginning of the event. The data set contains 321, 246 users. The personal profile of a user includes gender, verified or not, number of followers, number of followees, and number of microblogs.\nMoreover, considering that it is necessary to mine suspicion and identification attitudes towards microblogs from comments, we need to annotate each microblog suspicious or not. For there is no proper corpus for training a classifier about suspicion, we used an unsupervised method to identify suspicious attitudes towards microblogs. We built up a list of suspicion words and distinguished a microblog according to whether those suspicion words appear in the microblog. We first found several typical suspicion words then train wrod2vec6 [25] on our data set and found dozens of words similar with the typical suspicion words according to their embedding distance. Finally, we built up a word list with about 100 suspicion words and annotated all the microblogs suspicious or not in our Weibo data set.\nBased on the data set, we also investigated the data distribution difference between rumors and non-rumors, which is shown in Figure 2, i.e., distribution of percentage of microblogs with time illustrated in Figure 2(a) and distribution of percentage of events with the percentage of suspicious microblogs in one event shown in Figure 2(b).\n6https://code.google.com/p/word2vec/"
    }, {
      "heading" : "IV. THE ICE MODEL",
      "text" : "In this section, we first formulate the problem. Then, we detail the proposed ICE model. Finally, we present the pairwise learning procedure for the ICE model."
    }, {
      "heading" : "A. Problem Formulation",
      "text" : "The problem we studied in this paper can be formulated in math as follows. Suppose a set of events are denoted as E = {e1, e2, ..., en} and sei is the credibility score of the corresponding event ei. sei = 0 means event ei is a rumor and sei = 1 means event ei is a non-rumor. The microblogs of the event ei can be denoted as Mei = { mei1 ,m ei 2 , ...,m ei nei } , where nei is the number of microblogs of this event. All microblog sets can be written as M = {Me1 ,Me2 , ...,Men}. Each microblog meij consists of four elements “who”, “what”, “how”, and “when”, which are denoted as ueij , b ei j , c ei j and t ei j . ueij is the corresponding user of the microblog, b ei j denotes the behavior type (posting or reposting), ceij describes the user’s comments and attitudes (suspicious or not), and teij denotes the time interval since the beginning of an event. In this work, our task is to evaluate the credibility of an event on social media."
    }, {
      "heading" : "B. Proposed Model",
      "text" : "Here, we detail the representation learning procedure of the ICE model. Based on handcrafted features that indicate global-wise statistics, conventional methods have difficulty in modeling the correlation among different key elements in information spreading. Thus, we need to model their joint representations and yield their joint characteristics. It is necessary for a model based on user credibility (who), behavior types (what), comment attitudes (how) and dynamic properties (when).\nWe first start with user information and behavior information. User information tells us the properties and credibility of a user. Behavior information tells us the behavior type, i.e., posting or reposing. Moreover, the combination of these two information shows “who did what”. Mathematically, for the j-th microblog meij of the event ei, the representation of this microblog with the user ueij and the behavior b ei j can be written as\nReij = B ei j U ei j , (1)\nwhere Ueij ∈ Rd is the vector representation of user u ei j , Beij ∈ Rd×d is the matrix representation of behavior b ei j , and d denotes the dimensionality of representations. Additionally, users may express their attitudes in the comments. These attitudes contain the knowledge and life experience of users and can be used to distinguish rumors from non-rumors. As shown in Figure 2(b), rumors often receive more suspicious comments than non-rumors. Incorporating the representation of comment attitude ceij of microblog m ei j , Equation 1 can be written as\nReij = C ei j B ei j U ei j , (2)\nwhere Ceij ∈ Rd×d is the matrix representation of the comment ceij . Now, this equation can reveal the joint representation of “who did what under how”.\nMoreover, Figure 2(a) illustrates the difference between dynamic properties of rumors and non-rumors. It shows that time interval information is a significant factor for evaluating information credibility and should be modeled jointly with user behaviors. For instance, time interval teij of microblog meij means that the microblog appears from the beginning of ei. Incorporating time interval teij in Equation 2, the representation of microblog meij can be written as\nReij = T ei j C ei j B ei j U ei j , (3)\nwhere Teij ∈ Rd×d is the matrix representation of time interval teij . Now, this equation can reveal the joint representation of “who did what under how at when”.\nRight now, we generate the representation Reij of the microblog meij , which can capture the joint properties of four key elements. Because each event consists of several microblogs, we need to aggregate representations of microblogs to generate the final credibility representation of the event. Using the average calculation, the representation of the event ei can be generated as\nRei = 1\nnei ∑ m\nei j ∈Mei\nReij\n= 1\nnei ∑ m\nei j ∈Mei\nTeij C ei j B ei j U ei j .\n(4)\nThen, we can predict whether an event ei is a rumor or not using\nyei = WTRei , (5)\nwhere W ∈ Rd is linear weights of the prediction function. A larger value of yei indicates higher credibility of ei."
    }, {
      "heading" : "C. User Representation Generation",
      "text" : "For learning user representations, i.e., “who”, in the ICE model, it would be desirable if we can learn a distinct latent vector for each user to capture his or her properties and credibility. However, according to Table I, each user (re)posts only two microblogs in average, which can not bring enough information to directly learn a latent representation for each user.\nInstead, we can learn embeddings of rich features for users. These features contained in the Weibo data set are gender, number of followers, number of followees, numbers of microblogs, and verified or not. Then, users can be shaped based on the above features. For user u, we have a feature vector Fu ∈ Rf which is constructed as\nFu = [F gender u ,F followers u ,F followees u ,F microblogs u ,F verified u ] T .\nBoth Fgenderu and F verified u have two bits. F gender u (1) = 1 denotes that the gender is male, and Fgenderu (2) = 1 denotes that the gender is female. Fverifiedu (1) = 1 means that the user is verified, and Fverifiedu (2) = 1 otherwise. For the numbers of followers, followees, and microblogs, it is hard to learn an\nembedding for each distinct value. Therefore, we partition the values into discrete bins according to a log10 distribution. If a user u has vu followers, the corresponding features can be constructed as\nFfollowersu (i) =  U(logvu10 )− log vu 10 , i = L(log vu 10 ) + 1 logvu10 − L(log vu 10 ), i = U(log vu 10 ) + 1\n0 , i = others\n,\nwhere U(logvu10 ) and L(log vu 10 ) denote the upper and lower bounds of logvu10 respectively. Meanwhile, F followees u and Fmicroblogsu can be constructed in the same way. Figure 4 illustrates an example of generating user representation. In the example, suppose vu = 32, then log3210 = 1.51, the corresponding upper and lower bounds are 2 and 1. Ffollowersu can be computed as\nFfollowersu (2) = 2− 1.51 = 0.49 ,\nFfollowersu (3) = 1.51− 1 = 0.51 ,\nand other bits will be set to be 0. Then, based on feature vector Fu, we can generate the user representation as Uu = SFu , (6)\nwhere S ∈ Rd×f is the feature embedding matrix."
    }, {
      "heading" : "D. Nonlinear Interpolation for Generating Time-Specific Matrices",
      "text" : "In ICE, we use time-specific matrices to capture the properties of users’ dynamic behaviors, i.e., “when”, in the information spreading. However, if we learn a distinct matrix for each possible continuous time interval, the ICE model will face the data sparsity problem. Therefore, as in [20], we use a similar strategy for generating time-specific matrices. We partition the time interval into discrete time bins. Considering the power law distribution of dynamic behaviors shown in Figure 2(a), it is not plausible if we partition the time interval equally. Instead, our partition confirms to a log2 distribution. Only the matrices of the upper and lower bounds of the corresponding bins are learned in our model. For time intervals in a time\nbin, their transition matrices can be calculated via a nonlinear interpolation.\nMathematically, the time-specific matrix Tt for time interval t can be calculated as Tt = (U(logt2)− logt2)T2L(logt2) + (log t 2 − L(logt2))T2U(logt2)\nU(logt2)− L(logt2) ,\nwhere U(logt2) and L(log t 2) denote the upper bound and lower bounds of logt2 respectively. An example is shown is Figure 5, where t = 1.6h, log1.6h2 = 0.67, the corresponding upper and lower bounds will be 1 and 0, respectively; then, T1.6h can be computed as\nT1.6h = (1− 0.67)T1h + (0.67− 0)T2h\n1− 0 = 0.33T1h + 0.67T2h\n.\nSuch an interpolation method can solve the problem of learning matrices for continuous values in the ICE model and provide a solution for modeling the dynamic behaviors of users."
    }, {
      "heading" : "E. Pair-wise Learning",
      "text" : "Here, we introduce the parameter estimation process of ICE with a pair-wise learning method and calculate the complexity of the algorithm.\nBecause rumors are often hard to collect for training credibility evaluation models, we apply a pair-wise learning method to enlarge the number of training instances. Similar to [28], our basic assumption is that the credibility of a non-rumor is larger than that of a rumor. In ICE, we can maximize the credibility difference between rumors and non-rumors. Accordingly, we should maximize the following probability:\np(en er) = g(yen − yer ) ,\nwhere en denotes a non-rumor, er denotes a rumor, and g(x) is a nonlinear function that is selected as:\ng(x) = 1\n1 + e−x .\nIncorporating the negative log likelihood, for the whole data set, we can minimize the following objective function equivalently:\nJ = ∑\n{en,er}∈E,len=1,ler=0\nln(1 + e−W T (Ren−Rer ))+\nλ 2 ‖Θ‖2 ,\nwhere Θ = {U,B,C,T,W} denotes all the parameters to be estimated and λ is a parameter to control the power of regularization. The derivations of J with respect to W, Ren and Rer can be calculated as ∂J\n∂W = ∑ en,er∈E,len=1,ler=0 (Rer −Ren)l(en, er) 1 + l(en, er) + λW ,\n∂J\n∂Ren = − ∑ er∈E,ler=0 Wl(en, er) 1 + l(en, er) ,\n∂J\n∂Rer = ∑ en∈E,len=1 Wl(en, er) 1 + l(en, er) .\nwhere l(en, er) = e −WT (Ren−Rer ) .\nAfter calculating the derivation ∂J/∂Rei of event representation Rei , the corresponding gradients all the parameters can be calculated as\n∂J\n∂Teij =\n1\nnei\n∂J\n∂Rei\n( Ceij B ei j U ei j )T ,\n∂J\n∂Ceij =\n1\nnei\n( Teij )T ∂J\n∂Rei\n( Beij U ei j )T ,\n∂J\n∂Beij =\n1\nnei\n( Teij C ei j )T ∂J ∂Rei ( Ueij )T ,\n∂J\n∂Ueij =\n1\nnei\n( Teij C ei j B ei j )T ∂J ∂Rei .\nThen, ∂J ∂Tt = ∑ ei∈E ∑ m\nei j ∈Mei ,t ei j =t\n∂J\n∂Teij + λTt ,\n∂J ∂Cc = ∑ ei∈E ∑ m\nei j ∈Mei ,c ei j =c\n∂J\n∂Ceij + λCc ,\n∂J ∂Bb = ∑ ei∈E ∑ m\nei j ∈Mei ,b ei j =b\n∂J\n∂Beij + λBb ,\n∂J ∂Uu = ∑ ei∈E ∑ m\nei j ∈Mei ,u ei j =u\n∂J\n∂Ueij + λUu .\nAfter all the gradients are calculated, we can employ gradient descent to estimate the model parameters. This process can be repeated iteratively until convergence.\nBased on the above calculation, now we analyze the corresponding time complexity and suppose we totally have n event with m microblogs. During the training procedure, in each iteration, the time complexities of updating T , C, B and U are O(d2 × m) respectively. And the time complexity of updating W is O(d × n). So, the total time complexity is O[4d2 ×m+ d× n]. Since m is usually much larger than n and d is a constant, the time complexity is approximately equal to O(m). During the testing procedure, the time complexity is O(d2×m+d×n). It is also approximately equal to O(m). This indicates that both training and testing time complexities grow linearly with size of the dataset, and ICE has potential to scale up to large-scale data.\nTABLE II PERFORMANCE COMPARISON EVALUATED WITH DIMENSIONALITY d = 8.\nMethods Accuracy Rumors Non-rumorsPrecision Recall F1-score Precision Recall F1-score\nNewsCP-Content 0.608 0.524 0.899 0.662 0.531 0.782 0.633 NewsCP-Social 0.618 0.617 0.929 0.742 0.556 0.793 0.654\nNewsCP 0.758 0.741 0.808 0.773 0.728 0.770 0.749 EP 0.812 0.795 0.899 0.844 0.802 0.793 0.798 EP+Content 0.823 0.809 0.899 0.852 0.868 0.759 0.810 ICE 0.860 0.830 0.939 0.882 0.919 0.782 0.845 ICE+Content 0.887 0.831 0.990 0.903 0.946 0.805 0.870"
    }, {
      "heading" : "V. EXPERIMENTS",
      "text" : "In this section, we conduct empirical experiments to demonstrate the effectiveness of the ICE model on the Sina Weibo data set. We first introduce settings of our experiments. Then, we compare the ICE model to the state-of-the-art baseline methods. We also study the performance of the ICE model with varying parameters and under different situations. Finally, we analyze the scalability of the ICE model."
    }, {
      "heading" : "A. Experimental Settings",
      "text" : "First, we split our data set into training set, testing set, and validation set. Randomly, we use 60% of the events (rumors or non-rumors) in the data set for training, 30% for testing, and the remaining 10% data as the validation set for tuning parameters, i.e., the dimensionality of latent representations and the regularization parameter.\nMoreover, we have several evaluation metrics for our experiments: Accuracy, Precision, Recall, and F1-score. Accuracy is a standard metric for classification tasks, which is evaluated by the percentage of correctly predicted rumors and nonrumors. Precision, Recall and F1-score are widely-used metrics for classification tasks, which are computed according to where correctly predicted rumors or non-rumors appear in the predicted list. The larger the values of above evaluation metrics, the better the performance.\nThree competitive methods and their extensions are compared in our experiments: • News Credibility Propagation (NewsCP) [14] studies\nhow to aggregate credibility from microblogs to events based on a graph optimization method. The classifier we use for each microblog is the widely-used Support Vector Machine (SVM), which is implemented via libSVM7 [4]. There are three different versions of NewsCP: NewsCP-Content, NewsCP-Social, and NewsCP. NewsCP-Content only uses the content of microblogs as features. Meanwhile, NewsCP-Social only uses social features of the corresponding microblogs. NewsCP takes usage of all the features. Social features include number of user followers, number of user followees, number of user microblogs, gender of user, user verified or not, number of repostings, number of comments and time of posting. • The Enquiry Post (EP) model [42] is proposed mainly based on signal tweets. We use our suspicion word\n7http://www.csie.ntu.edu.tw/ cjlin/libsvm/index.html\nlist to identify signal tweets and then apply libSVM [4] for information credibility evaluation. The features used here include percentage of signal tweets, content length, average number of repostings, average number of URLs, average number of hashtags, average number of usernames mentioned, and average time of posting. Considering that EP does not take content information of microblogs into consideration, we further make a fusion of EP and NewsCP-Content at the score level and achieve an extended version EP+Content. • Our proposed ICE model uses representation learning method to evaluate credibility. Similarly to the EP model, considering ICE only models user behaviors, we make a fusion of ICE and NewsCP-Content at the score level to incorporate content information and achieve an extended version ICE+Content.\nNote that, the score level fusion means the final predicted score is the sum of the predicted scores of the two methods. Mathematically, for fusing the scores of ICE and NewsCPContent to generate the score of ICE+Content, it can be calculated as:\nSICE+Content = µSICE + (1− µ)SNewsCP−Content ,\nwhere SICE+Content, SICE , and SNewsCP−Content denote predicted credibility scores of methods ICE+Content, ICE, and NewsCP-Content respectively, and µ is selected to be µ = 0.8 in our experiments. For generating the predicted credibility score of EP+Content, the process is the same."
    }, {
      "heading" : "B. Performance Comparison",
      "text" : "To investigate the performance of ICE and compared methods, we conduct experiments on the Weibo data set, and report the Accuracy, Precision, Precision, F1-score, and Precision-Recall curves of these methods.\nTable II illustrates the performance comparison with dimensionality d = 8 on the Sina Weibo data set evaluated by Accuracy, Precision, Precision and F1-score. Using part of the features, NewsCP-Content and NewsCP-Social have the lowest Accuracy and F1-score among all the methods. Meanwhile, we can see that the performance of NewsCP-Social is better than that of NewsCP-Content. This may indicate that social features are more important than content features for evaluating credibility. Involving both kinds of features, NewsCP achieves great improvement and has a satisfactory performance. Then, EP further improves the performance\n0 5 10 15 0.7\n0.75\n0.8\n0.85\nA cc u ra cy\nDimensionality d\nICE ICE+Content\n(a) Accuracy curves of ICE with varying dimensionality d with λ = 0.01.\n10 −4\n10 −2\n10 0\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\nA cc u ra cy\nRegularization parameter λ\nICE ICE+Content\n(b) Accuracy curves of ICE with varying regularization parameter λ with d = 8.\nFig. 7. Performance of ICE with varying parameters evaluated by Accuracy.\ncompared to NewsCP and achieves Accuracy more than 80%. Incorporating content information of NewsCP-Content, EP+Content achieves a little improvement and becomes the best one among all the compared methods. We can clearly observe that, our proposed ICE model outperforms the compared methods. Incorporating content features, ICE+Content achieves the best performance among all the methods. Compared to EP+Content, ICE and ICE+Content improve the Accuracy by 3.7% and 6.4%, respectively. When the target class is rumor, the F1-score improvements are 3.0% and 5.1%, respectively, and when the target class is non-rumor, the improvements are 3.5% and 6.0%, respectively. Moreover, among the results of all the methods, the F1-score of rumors is higher than the F1-score of non-rumors. This may mean that it is more difficult to distinguish non-rumors from rumors than to distinguish rumors from non-rumors.\nWe also illustrate the Precision-Recall curves of the different methods in Figure 6. The Precision-Recall curve for rumors in Figure 6(a) shows that ICE+Content outperforms the other methods. The Precision of ICE+Content stays 100% until Recall is more than 50%. ICE is better than the other compared methods in most cases, except at around recall = 45%. The Precision-Recall curve for\nSociety\nLife Policy\nPolitics\nEntertainment\n(a) Distribution of different topics.\n0-100\n100-300\n300-1000\n1000+\n(b) Distribution of different levels of popularity.\nFig. 8. Distribution of different situations in the Weibo datatset.\nnon-rumors in Figure 6(b) shows that the performance of ICE and ICE+Content is clearly better than that of other methods. The Precision of ICE stays 100% until its Recall is more than 40%. The Precision of ICE+Content stays 100% until its Recall is more than 70%. In our experiments, both experimental results in Table II and Precision-Recall curves in Figure 6 clearly show that our proposed ICE model achieves satisfactory performances and outperforms the other state-ofthe-art methods.\nTABLE III PERFORMANCE COMPARISON UNDER DIFFERENT EVENT TOPICS AND EVENT POPULARITY EVALUATED BY Accuracy.\nMethods Topics PopularitySociety Life Policy Politics Entertainment 0-100 100-300 300-1000 1000+\nNewsCP-Content 0.608 0.524 0.667 0.541 0.676 0.662 0.556 0.568 0.611 NewsCP-Social 0.649 0.571 0.667 0.568 0.595 0.647 0.556 0.595 0.639\nNewsCP 0.797 0.619 0.933 0.703 0.730 0.794 0.689 0.757 0.778 EP 0.811 0.857 0.800 0.811 0.811 0.824 0.822 0.811 0.778 EP+Content 0.811 0.857 0.867 0.838 0.838 0.853 0.844 0.811 0.778 ICE 0.838 0.905 0.933 0.865 0.838 0.868 0.867 0.865 0.861 ICE+Content 0.851 0.952 0.933 0.946 0.838 0.882 0.933 0.865 0.861\nSociety\nLife Policy\nPolitics\nEntertainment\n(a) Distribution of different topics.\n0-100\n100-300\n300-1000\n1000+\n(b) Distribution of different levels of popularity.\nFig. 9. Distribution of different situations in the Weibo datatset.\nC. Impact of Parameters\nTo investigate the impact of parameters on the performance of ICE, we illustrate the Accuracy performance of ICE with varying parameters in Figure 7. Based on the figure, we can select the best parameters for ICE.\nIn Figure 7(a), we illustrate the performance of ICE and ICE+Content with varying dimensionality d, where the regularization parameter is set to be λ = 0.01. The performance of ICE increases rapidly from d = 3 and then becomes stable since d = 6. ICE achieves the best performance at d = 8 and then decreases slightly with increasing dimensionality. Meanwhile, the performance of ICE+Content has the similar trend. However, the Accuracy curve of ICE+Content is more stable than that of ICE, because the performance of modeling content information is not influenced by dimensionality. From the observation of the curves, we select the best dimensionality of ICE as d = 8. Moreover, the curves show that ICE is not very sensitive to the dimensionality in a large range, and ICE still outperforms compared methods even not with the best dimensionality.\nThe Accuracy curves of ICE and ICE+Content with varying regularization parameter λ are shown in Figure 7(b). The performance of ICE grows slowly from λ = 0.0001 and achieves the highest Accuracy at λ = 0.01. It is obvious that the best parameter for ICE is λ = 0.01. We can also clearly observe that ICE stays stable in the range of λ from 0.0001 to 1. Moreover, recall results in Table II, even not the best one, the performances of ICE are still better than those of the compared methods."
    }, {
      "heading" : "D. Performance Under Different Situations",
      "text" : "We have shown that the proposed ICE model can outperform the state-of-the-art methods. Here, we are going to investigate if ICE can perform better than the compared methods in some specific kinds of situations. We partition our data set according to topics and popularity, and the results evaluated by Accuracy under different situations are shown in Table III. The distribution of different situations is shown in Figure 9.\nAccording to topics of the events, we first partition the data set into five categories: society, life, policy, politics and entertainment, as shown in Figure 9(a). Topic “society” talks about all kinds of stuff happening around us, topic “life” contains life skills such as health tips, topic “policy” denotes news about newly released policies, topic “politics” talks about politics, government and military, and topic “entertainment” means news about movies, music and sports. Table III illustrates the Accuracy of the different methods on the five topics. The results show that our proposed ICE model outperforms the compared methods on all the five topics, and ICE+Content achieves the best performance in all the situations. Meanwhile, NewsCP performs well on the topic of “policy”, and EP has a good performance on the topic of “entertainment”. Moreover, in average, these methods achieve slightly poor performances on topics of “society” and “entertainment” compared to the\nother topics. This may indicates that news about “society” and “entertainment” has more noise and such rumors are difficult to be identified.\nThen, we partition the data set according to the popularity of events. The popularity of an event is computed as the amount of its microblogs, including postings and repostings. As shown in Figure 9(b), the whole data set is partitioned into four categories: 0-100, 100-300, 300-1000, and 1000+. From Table III, we can clearly observe that with all kinds of popularity, ICE outperforms the compared methods, and ICE+Content achieves the best performance. Moreover, we can observe that the larger the popularity, the lower the accuracy in most cases. It may be because that the majority of microblogs contain noise and do not contribute to the evaluation very much. Among the massive microblogs on social media, several significant microblogs are easily hidden by a large amount of noise. Thus, in the future work, we need to find a method to select significant microblogs, instead of average calculation."
    }, {
      "heading" : "E. Scalability Analysis",
      "text" : "Besides the analysis of the effectiveness of ICE, we also investigate the scalability of the ICE model with varying portions of the Weibo data set. The model is implemented with Python8 and Theano9. The code is run on a computer with 4 Core 2.5 GHz CPU and 16 GB RAM, and the GPU model is NVIDIA Tesla K20Xm. On the Weibo data set, we measure the corresponding time cost of one iteration in both training and testing process. Figure 10 shows the time consumption with varying portions of the whole data set. We can observe that both training and testing time consumption of ICE are linear with respect to the size of data set. This shows the scalability of ICE. Our proposed model not only can achieve the state-\n8https://www.python.org/. 9http://deeplearning.net/software/theano/.\nof-the-art prediction performance but also can run effectively on large-scale data."
    }, {
      "heading" : "VI. SYSTEM",
      "text" : "As introduced and discussed above, we have achieved a information credibility evaluation model ICE with state-of-theart performances. Rather than only using the ICE model in academic data sets and research, it is vital to construct a realtime information credibility evaluation system on social media and make our proposed model applied in real applications. Thus, based on our model and the Sina Weibo data set, we built a Network Information Credibility Evaluation (NICE) system [35]. NICE is a webpage-based system that can automatically crawl online information from Sina Weibo and evaluate the credibility of online information that users enquire.\nFigure 11 illustrates the flow chart of the NICE system. Using the system, a user can input a query to retrieve the related information. If a user’s query matches rumors in the Weibo data set, users can identify the rumor immediately. Otherwise, NICE will crawl real-time information from social media, i.e., Sina Weibo. Then, the user can select one microblog to evaluate the corresponding credibility based on our model. Based on the selected microblog, the system will crawl all the related microblogs from Weibo and collect related information including content information, temporal information, comment information, and corresponding user profiles. Based on this information and the trained model, NICE can evaluate the credibility of the related information and provide a predicted score of the event. With our proposed ICE model, the NICE system can achieve great performances in information credibility evaluation. It can be applied effectively and stably in information credibility evaluation and online information management on social media."
    }, {
      "heading" : "VII. CONCLUSIONS AND FUTURE WORK",
      "text" : "In this work, to evaluate information credibility on social media, a novel method, i.e., ICE, has been proposed. ICE aims to learn dynamic representations for the microblogs that describe events spreading on social media. The learning is based on user credibility, behavior types, temporal properties, and comment attitudes. The aggregation of these key factors makes the dynamic and joint representations of microblogs, and the aggregation of representations of all the microblogs during information spreading can generate the credibility representation of events on social media. Experiments conducted on a real data set crawled from Sina Weibo show that ICE outperforms the state-of-the-art methods.\nIn the future, we can further investigate the following directions. First, in ICE, the content information has not been considered when learning credibility representations. We plan to analyze the event content and extract main elements in it to predict the happening probability of the event based on a large news database. Second, information about an event on other platforms, e.g., news websites and forums, can be incorporated in our model. Third, for the aggregation of microblogs of an event, we use average computation in ICE, which is clearly not the best solution. We need to find a method to select significant microblogs."
    } ],
    "references" : [ {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Y. Bengio", "A. Courville", "P. Vincent" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Learning social network embeddings for predicting information diffusion",
      "author" : [ "S. Bourigault", "C. Lagnier", "S. Lamprier", "L. Denoyer", "P. Gallinari" ],
      "venue" : "In Proceedings of the 7th ACM international conference on Web search and data mining,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Information credibility on twitter",
      "author" : [ "C. Castillo", "M. Mendoza", "B. Poblete" ],
      "venue" : "In Proceedings of the 20th international conference on World wide web,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Libsvm: a library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Rumor, gossip and urban legends",
      "author" : [ "N. DiFonzo", "P. Bordia" ],
      "venue" : "Diogenes,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2007
    }, {
      "title" : "A multi-view deep learning approach for cross domain user modeling in recommendation systems",
      "author" : [ "A.M. Elkahky", "Y. Song", "X. He" ],
      "venue" : "In Proceedings of the 24th International Conference on World Wide Web,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Personalized ranking metric embedding for next new poi recommendation",
      "author" : [ "S. Feng", "X. Li", "Y. Zeng", "G. Cong", "Y.M. Chee", "Q. Yuan" ],
      "venue" : "In Proceedings of the 24th International Conference on Artificial Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Crowdsourcing credibility: The impact of audience feedback on web page credibility",
      "author" : [ "K.D. Giudice" ],
      "venue" : "Proceedings of the American Society for Information Science and Technology,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "node2vec: Scalable feature learning for networks",
      "author" : [ "A. Grover", "J. Leskovec" ],
      "venue" : "In Proceedings of the 22nd ACM International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Faking sandy: characterizing and identifying fake images on twitter during hurricane sandy",
      "author" : [ "A. Gupta", "H. Lamba", "P. Kumaraguru", "A. Joshi" ],
      "venue" : "In Proceedings of the 22nd international conference on World Wide Web companion,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Evaluating event credibility on twitter",
      "author" : [ "M. Gupta", "P. Zhao", "J. Han" ],
      "venue" : "In Proceedings of the 12th SIAM International Conference on Data Mining,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Learning latent representations of nodes for classifying in heterogeneous social networks",
      "author" : [ "Y. Jacob", "L. Denoyer", "P. Gallinari" ],
      "venue" : "In Proceedings of the 7th ACM international conference on Web search and data mining,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Misinformation propagation in the age of twitter",
      "author" : [ "F. Jin", "W. Wang", "L. Zhao", "E. Dougherty", "Y. Cao", "C.-T. Lu", "N. Ramakrishnan" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "News credibility evaluation on microblog with a hierarchical propagation model",
      "author" : [ "Z. Jin", "J. Cao", "Y.-G. Jiang", "Y. Zhang" ],
      "venue" : "In Data Mining (ICDM),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "News verification by exploiting conflicting social viewpoints in microblogs",
      "author" : [ "Z. Jin", "J. Cao", "Y. Zhang", "J. Luo" ],
      "venue" : "In Proceedings of the 30th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Prominent features of rumor propagation in online social media",
      "author" : [ "S. Kwon", "M. Cha", "K. Jung", "W. Chen", "Y. Wang" ],
      "venue" : "In Data Mining (ICDM),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Truth finding on the deep web: Is the problem solved",
      "author" : [ "X. Li", "X.L. Dong", "K. Lyons", "W. Meng", "D. Srivastava" ],
      "venue" : "Proceedings of the VLDB Endowment,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Collaborative prediction for multientity interaction with hierarchical representation",
      "author" : [ "Q. Liu", "S. Wu", "L. Wang" ],
      "venue" : "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Cot: Contextual operating tensor for context-aware recommender systems",
      "author" : [ "Q. Liu", "S. Wu", "L. Wang" ],
      "venue" : "In Proceedings of the 29th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Predicting the next location: A recurrent model with spatial and temporal contexts",
      "author" : [ "Q. Liu", "S. Wu", "L. Wang", "T. Tan" ],
      "venue" : "In Proceedings of the 30th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "A convolutional click prediction model",
      "author" : [ "Q. Liu", "F. Yu", "S. Wu", "L. Wang" ],
      "venue" : "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Detect rumors using time series of social context information on microblogging websites",
      "author" : [ "J. Ma", "W. Gao", "Z. Wei", "Y. Lu", "K.-F. Wong" ],
      "venue" : "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "T. Mikolov", "M. Karafiát", "L. Burget", "J. Cernockỳ", "S. Khudanpur" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Extensions of recurrent neural network language model",
      "author" : [ "T. Mikolov", "S. Kombrink", "L. Burget", "J.H. Černockỳ", "S. Khudanpur" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Deepwalk: Online learning of social representations",
      "author" : [ "B. Perozzi", "R. Al-Rfou", "S. Skiena" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Rumor has it: Identifying misinformation in microblogs",
      "author" : [ "V. Qazvinian", "E. Rosengren", "D.R. Radev", "Q. Mei" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    }, {
      "title" : "Bpr: Bayesian personalized ranking from implicit feedback",
      "author" : [ "S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme" ],
      "venue" : "In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2009
    }, {
      "title" : "Audience-aware credibility: From understanding audience to establishing credible blogs",
      "author" : [ "S.Y. Rieh", "G.Y. Jeon", "J.Y. Yang", "C. Lampe" ],
      "venue" : "In Proceedings of the 8th International Conference on Weblogs and Social Media. AAAI Press,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2014
    }, {
      "title" : "Cars2: Learning context-aware representations for context-aware recommendations",
      "author" : [ "Y. Shi", "A. Karatzoglou", "L. Baltrunas", "M. Larson", "A. Hanjalic" ],
      "venue" : "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "Detecting event rumors on sina weibo automatically",
      "author" : [ "S. Sun", "H. Liu", "J. He", "X. Du" ],
      "venue" : "In Web Technologies and Applications,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2013
    }, {
      "title" : "Line: Large-scale information network embedding",
      "author" : [ "J. Tang", "M. Qu", "M. Wang", "M. Zhang", "J. Yan", "Q. Mei" ],
      "venue" : "In Proceedings of the 24th International Conference on World Wide Web,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2015
    }, {
      "title" : "Truth discovery algorithms: An experimental evaluation",
      "author" : [ "D.A. Waguih", "L. Berti-Equille" ],
      "venue" : "arXiv preprint arXiv:1409.6428,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2014
    }, {
      "title" : "Learning hierarchical representation model for next basket recommendation",
      "author" : [ "P. Wang", "J. Guo", "Y. Lan", "J. Xu", "S. Wan", "X. Cheng" ],
      "venue" : "In Proceedings of the 38th International ACM SIGIR conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2015
    }, {
      "title" : "Information credibility evaluation on social media",
      "author" : [ "S. Wu", "Q. Liu", "Y. Liu", "L. Wang", "T. Tan" ],
      "venue" : "In Proceedings of the 30th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2016
    }, {
      "title" : "Contextual operation for recommender systems",
      "author" : [ "S. Wu", "Q. Liu", "L. Wang", "T. Tan" ],
      "venue" : "Knowledge and Data Engineering, IEEE Transactions on,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2016
    }, {
      "title" : "Collaborative denoising auto-encoders for top-n recommender systems",
      "author" : [ "Y. Wu", "C. DuBois", "A.X. Zheng", "M. Ester" ],
      "venue" : "In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2016
    }, {
      "title" : "Truth discovery with multiple conflicting information providers on the web. Knowledge and Data Engineering",
      "author" : [ "X. Yin", "J. Han", "P.S. Yu" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2008
    }, {
      "title" : "Semi-supervised truth discovery",
      "author" : [ "X. Yin", "W. Tan" ],
      "venue" : "In Proceedings of the 20th international conference on World wide web,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2011
    }, {
      "title" : "A dynamic recurrent model for next basket recommendation",
      "author" : [ "F. Yu", "Q. Liu", "S. Wu", "L. Wang", "T. Tan" ],
      "venue" : "In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2016
    }, {
      "title" : "Deep learning over multi-field categorical data - - A case study on user response prediction",
      "author" : [ "W. Zhang", "T. Du", "J. Wang" ],
      "venue" : "In Proceedings of the 38th European Conference on Information Retrieval,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2016
    }, {
      "title" : "Enquiring minds: Early detection of rumors in social media from enquiry posts",
      "author" : [ "Z. Zhao", "P. Resnick", "Q. Mei" ],
      "venue" : "In Proceedings of the 24th International Conference on World Wide Web,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "A rumor is an unverified and instrumentally relevant statement of information spreading among people [5].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : "Most of the methods are based on content information and source credibility at the microblog level [3][27][10] or event (containing a group of microblogs) level [16][42][22].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 26,
      "context" : "Most of the methods are based on content information and source credibility at the microblog level [3][27][10] or event (containing a group of microblogs) level [16][42][22].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "Most of the methods are based on content information and source credibility at the microblog level [3][27][10] or event (containing a group of microblogs) level [16][42][22].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : "Most of the methods are based on content information and source credibility at the microblog level [3][27][10] or event (containing a group of microblogs) level [16][42][22].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 41,
      "context" : "Most of the methods are based on content information and source credibility at the microblog level [3][27][10] or event (containing a group of microblogs) level [16][42][22].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 21,
      "context" : "Most of the methods are based on content information and source credibility at the microblog level [3][27][10] or event (containing a group of microblogs) level [16][42][22].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 13,
      "context" : "Some research also studies the aggregation of credibility from the microblog level to the event level [14].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 15,
      "context" : "On the contrary, considering dynamic information, some work designs temporal features based on prorogation properties over time [16] or trains a model with features generated from different time periods [22].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "On the contrary, considering dynamic information, some work designs temporal features based on prorogation properties over time [16] or trains a model with features generated from different time periods [22].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 7,
      "context" : "Some works also take usage of users’ feedbacks (comments and attitudes) to evaluate credibility [8][29].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 28,
      "context" : "Some works also take usage of users’ feedbacks (comments and attitudes) to evaluate credibility [8][29].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 41,
      "context" : "The Enquiry Post (EP) model [42] takes out signal tweets, which indicates users’ suspicious attitudes for detecting rumors and achieves satisfactory performance.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 2,
      "context" : "First, these methods based on feature engineering require great labor for designing features [3].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "Normally, the higher the credibility of a user, the higher the credibility of information it creates [3].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "However, some studies [13] point out that a great amount of users with high credibility on social media would repost and share misinformation unintentionally.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "Users on social media can express their attitudes and collective intelligence can be gathered to help us evaluate the credibility of information [8].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "tion learning [1] is showing a promising performance in a variety of applications, such as word embedding [23][24][25], network embedding [2][9][26][32], and user representations [6][7][18][19].",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 22,
      "context" : "tion learning [1] is showing a promising performance in a variety of applications, such as word embedding [23][24][25], network embedding [2][9][26][32], and user representations [6][7][18][19].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 23,
      "context" : "tion learning [1] is showing a promising performance in a variety of applications, such as word embedding [23][24][25], network embedding [2][9][26][32], and user representations [6][7][18][19].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 24,
      "context" : "tion learning [1] is showing a promising performance in a variety of applications, such as word embedding [23][24][25], network embedding [2][9][26][32], and user representations [6][7][18][19].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "tion learning [1] is showing a promising performance in a variety of applications, such as word embedding [23][24][25], network embedding [2][9][26][32], and user representations [6][7][18][19].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "tion learning [1] is showing a promising performance in a variety of applications, such as word embedding [23][24][25], network embedding [2][9][26][32], and user representations [6][7][18][19].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 25,
      "context" : "tion learning [1] is showing a promising performance in a variety of applications, such as word embedding [23][24][25], network embedding [2][9][26][32], and user representations [6][7][18][19].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 31,
      "context" : "tion learning [1] is showing a promising performance in a variety of applications, such as word embedding [23][24][25], network embedding [2][9][26][32], and user representations [6][7][18][19].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "tion learning [1] is showing a promising performance in a variety of applications, such as word embedding [23][24][25], network embedding [2][9][26][32], and user representations [6][7][18][19].",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 6,
      "context" : "tion learning [1] is showing a promising performance in a variety of applications, such as word embedding [23][24][25], network embedding [2][9][26][32], and user representations [6][7][18][19].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 17,
      "context" : "tion learning [1] is showing a promising performance in a variety of applications, such as word embedding [23][24][25], network embedding [2][9][26][32], and user representations [6][7][18][19].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 18,
      "context" : "tion learning [1] is showing a promising performance in a variety of applications, such as word embedding [23][24][25], network embedding [2][9][26][32], and user representations [6][7][18][19].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 18,
      "context" : "For the sake of modeling elaborate interactions among different features, other features such as “what”, “how”, and “when” are represented as operating matrices [19].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 2,
      "context" : "Some of them evaluate the credibility of a single microblog [3][27] or a single image [10].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 26,
      "context" : "Some of them evaluate the credibility of a single microblog [3][27] or a single image [10].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "Some of them evaluate the credibility of a single microblog [3][27] or a single image [10].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "Some of them evaluate information credibility at the event level to distinguish whether an event is a rumor or a non-rumor [11][31][16][42][22], where each event consists of several microblogs.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 30,
      "context" : "Some of them evaluate information credibility at the event level to distinguish whether an event is a rumor or a non-rumor [11][31][16][42][22], where each event consists of several microblogs.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 15,
      "context" : "Some of them evaluate information credibility at the event level to distinguish whether an event is a rumor or a non-rumor [11][31][16][42][22], where each event consists of several microblogs.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 41,
      "context" : "Some of them evaluate information credibility at the event level to distinguish whether an event is a rumor or a non-rumor [11][31][16][42][22], where each event consists of several microblogs.",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 21,
      "context" : "Some of them evaluate information credibility at the event level to distinguish whether an event is a rumor or a non-rumor [11][31][16][42][22], where each event consists of several microblogs.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 13,
      "context" : "News Credibility Propagation (NewsCP) [14] studies how to aggregate credibility from the microblog level to the event level and presents a graph optimization method, which has further incorporated conflict viewpoints in the model [15].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 14,
      "context" : "News Credibility Propagation (NewsCP) [14] studies how to aggregate credibility from the microblog level to the event level and presents a graph optimization method, which has further incorporated conflict viewpoints in the model [15].",
      "startOffset" : 230,
      "endOffset" : 234
    }, {
      "referenceID" : 15,
      "context" : "For instance, the Periodic External Shocks (PES) model [16] uses ordinary structural features and user features and designs temporal features according to the properties of information spreading over time.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "The Dynamic Series-Time Structure (DSTS) [22] generates content-, user-, and diffusion-based features in different time periods during information spreading and uses all these features to train a model.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "Some works also take usage of users feedbacks to evaluate credibility [8][29].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 28,
      "context" : "Some works also take usage of users feedbacks to evaluate credibility [8][29].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 41,
      "context" : "The EP model [42] extracts signal tweets that indicate users suspicious attitudes for detecting rumors and achieves satisfactory performance.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "Nowadays, representation learning [1] has been extensively studied in different areas.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 24,
      "context" : "In natural language processing, learning embeddings [25] is a hot topic, where recurrent neural networks [23][24] are widely applied.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 22,
      "context" : "In natural language processing, learning embeddings [25] is a hot topic, where recurrent neural networks [23][24] are widely applied.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 23,
      "context" : "In natural language processing, learning embeddings [25] is a hot topic, where recurrent neural networks [23][24] are widely applied.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 11,
      "context" : "In web mining, learning network embedding has drawn great attention for studying node classification [12] or information diffusion [2].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "In web mining, learning network embedding has drawn great attention for studying node classification [12] or information diffusion [2].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 25,
      "context" : "Recently, network embedding models have incorporated random walk [26][9] and second-order connection in the representation learning methods [32].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "Recently, network embedding models have incorporated random walk [26][9] and second-order connection in the representation learning methods [32].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 31,
      "context" : "Recently, network embedding models have incorporated random walk [26][9] and second-order connection in the representation learning methods [32].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 18,
      "context" : "Contextual Operating Tensor (COT) [19][36] and CARS2 [30] study context-aware user representations for recommendation.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 35,
      "context" : "Contextual Operating Tensor (COT) [19][36] and CARS2 [30] study context-aware user representations for recommendation.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 29,
      "context" : "Contextual Operating Tensor (COT) [19][36] and CARS2 [30] study context-aware user representations for recommendation.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "Hierarchical Interaction Representation (HIR) [18] studies joint representations of entities, e.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "Some works [6][37][41] utilize deep neural networks for better user modeling.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 36,
      "context" : "Some works [6][37][41] utilize deep neural networks for better user modeling.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 40,
      "context" : "Some works [6][37][41] utilize deep neural networks for better user modeling.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 20,
      "context" : "Convolutional Click Prediction Model (CCPM) [21] applies convolutional neural networks in predicting clicking behaviors of users.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 33,
      "context" : "Hierarchical Representation Model (HRM) [34] and Dynamic Recurrent Basket Model (DREAM)[40] learn the representation of behaviors of a user in a short period for better recommendation.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 39,
      "context" : "Hierarchical Representation Model (HRM) [34] and Dynamic Recurrent Basket Model (DREAM)[40] learn the representation of behaviors of a user in a short period for better recommendation.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "Nowadays, representation learning [1] has been extensively studied in different areas.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 24,
      "context" : "In natural language processing, learning embedding [25] is a hot topic, where recurrent neural networks [23][24] are widely applied.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "In natural language processing, learning embedding [25] is a hot topic, where recurrent neural networks [23][24] are widely applied.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 23,
      "context" : "In natural language processing, learning embedding [25] is a hot topic, where recurrent neural networks [23][24] are widely applied.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 11,
      "context" : "In web mining, learning network embedding has drawn great attention for studying node classification [12] or information diffusion [2].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "In web mining, learning network embedding has drawn great attention for studying node classification [12] or information diffusion [2].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 25,
      "context" : "Recently, network embedding models have incorporated random walk [26][9] and second-order connection in the representation learning methods [32].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "Recently, network embedding models have incorporated random walk [26][9] and second-order connection in the representation learning methods [32].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 31,
      "context" : "Recently, network embedding models have incorporated random walk [26][9] and second-order connection in the representation learning methods [32].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 18,
      "context" : "The contextual operating tensor (COT) [19][36] and CARS2 [30] study context-aware user representations for recommendation.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 35,
      "context" : "The contextual operating tensor (COT) [19][36] and CARS2 [30] study context-aware user representations for recommendation.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 29,
      "context" : "The contextual operating tensor (COT) [19][36] and CARS2 [30] study context-aware user representations for recommendation.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "Hierarchical Interaction Representation (HIR) [18] studies joint representations of entities, e.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "Some works [6][37][41] use deep neural networks for better user modeling.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 36,
      "context" : "Some works [6][37][41] use deep neural networks for better user modeling.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 40,
      "context" : "Some works [6][37][41] use deep neural networks for better user modeling.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 20,
      "context" : "The convolutional click prediction model (CCPM) [21] applies convolutional neural networks in predicting clicking behaviors of users.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 33,
      "context" : "The hierarchical representation model (HRM) [34] and the dynamic recurrent basket model (DREAM) [40] learn the representation of behaviors of a user in a short period for better recommendation.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 39,
      "context" : "The hierarchical representation model (HRM) [34] and the dynamic recurrent basket model (DREAM) [40] learn the representation of behaviors of a user in a short period for better recommendation.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 37,
      "context" : "Truth discovery refers to the problem of finding the truth with conflicting information, which has been first addressed in [38].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : "Truth discovery methods are usually based on Bayesian algorithms or graph learning algorithms on stock data or flight data [17][33].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 32,
      "context" : "Truth discovery methods are usually based on Bayesian algorithms or graph learning algorithms on stock data or flight data [17][33].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 38,
      "context" : "And Semi-Supervised Truth Discovery (SSTF) [39] studies the problem with semi-supervised graph learning with a small set of ground truth data to help evaluating credibility.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "Truth discovery is an unsupervised or semi-supervised method to find the truth with conflicting information and make an evaluation of information credibility [17].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 24,
      "context" : "We first found several typical suspicion words then train wrod2vec6 [25] on our data set and found dozens of words similar with the typical suspicion words according to their embedding distance.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 19,
      "context" : "Therefore, as in [20], we use a similar strategy for generating time-specific matrices.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 27,
      "context" : "Similar to [28], our basic assumption is that the credibility of a non-rumor is larger than that of a rumor.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 13,
      "context" : "• News Credibility Propagation (NewsCP) [14] studies how to aggregate credibility from microblogs to events based on a graph optimization method.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "The classifier we use for each microblog is the widely-used Support Vector Machine (SVM), which is implemented via libSVM7 [4].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 41,
      "context" : "• The Enquiry Post (EP) model [42] is proposed mainly based on signal tweets.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "html list to identify signal tweets and then apply libSVM [4] for information credibility evaluation.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 34,
      "context" : "Thus, based on our model and the Sina Weibo data set, we built a Network Information Credibility Evaluation (NICE) system [35].",
      "startOffset" : 122,
      "endOffset" : 126
    } ],
    "year" : 2017,
    "abstractText" : "With the rapid growth of social media, rumors are also spreading widely on social media and bring harm to people’s daily life. Nowadays, information credibility evaluation has drawn attention from academic and industrial communities. Current methods mainly focus on feature engineering and achieve some success. However, feature engineering based methods require a lot of labor and cannot fully reveal the underlying relations among data. In our viewpoint, the key elements of user behaviors for evaluating credibility are concluded as who, what, when, and how. These existing methods cannot model the correlation among different key elements during the spreading of microblogs. In this paper, we propose a novel representation learning method, Information Credibility Evaluation (ICE), to learn representations of information credibility on social media. In ICE, latent representations are learnt for modeling user credibility, behavior types, temporal properties, and comment attitudes. The aggregation of these factors in the microblog spreading process yields the representation of a users behavior, and the aggregation of these dynamic representations generates the credibility representation of an event spreading on social media. Moreover, a pairwise learning method is applied to maximize the credibility difference between rumors and non-rumors. To evaluate the performance of ICE, we conduct experiments on a Sina Weibo data set, and the experimental results show that our ICE model outperforms the state-of-the-art methods.",
    "creator" : "LaTeX with hyperref package"
  }
}