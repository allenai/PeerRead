{
  "name" : "1604.00788.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models",
    "authors" : [ ],
    "emails" : [ "lmthang@stanford.edu", "manning@stanford.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 4.\n00 78\n8v 1\n[ cs\n.C L\n] 4\nA pr\n2 01"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural Machine Translation (NMT) is a simple new architecture for getting machines to translate. At its core, NMT is a single deep neural network that is trained end-to-end with several advantages\n1Code, data, and models will be available at http:// anonymized upon publication.\nsuch as simplicity and generalization. Despite being relatively new, NMT has already achieved state-of-the-art translation results for many language pairs such as English-French (Luong et al., 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015), and English-Czech (Jean et al., 2015b).\nWhile NMT offers many advantages over traditional phrase-based approaches, such as small memory footprint and simple decoder implementation, nearly all previous work in NMT has used quite restricted vocabularies, crudely treating all other words the same with an <unk> symbol. Sometimes, a post-processing step that patches\nin unknown words is introduced to alleviate this problem. Luong et al. (2015b) propose to annotate occurrences of target <unk> with positional information to track their alignments, after which simple word dictionary lookup or identity copy can be performed to replace <unk> in the translation. Jean et al. (2015a) approach the problem similarly but obtain the alignments for unknown words from the attention mechanism. We refer to these as the unk replacement technique.\nThough simple, these approaches ignore several important properties of languages. First, monolingually, words are morphologically related; however, they are currently treated as independent entities. This is problematic as pointed out by Luong et al. (2013): neural networks can learn good representations for frequent words such as “distinct”, but fail for rare-but-related words like “distinctiveness”. Second, crosslingually, languages have different alphabets, so one cannot naïvely memorize all possible surface word translations such as name transliteration between “Christopher” (English) and “Krys̆tof” (Czech). See more on this problem in (Sennrich et al., 2015).\nTo overcome these shortcomings, we propose a novel hybrid architecture for NMT that translates mostly at the word level and consults the character components for rare words when necessary. As illustrated in Figure 1, our hybrid model consists of a word-based NMT that performs most of the translation job, except for the two (hypothetically) rare words, “cute” and “joli”, that are handled separately. On the source side, representations for rare words, “cute”, are computed on-thefly using a deep recurrent neural network that operates at the character level. On the target side, we have a separate model that recovers the surface forms, “joli”, of <unk> tokens character-bycharacter. These components are learned jointly end-to-end, removing the need for a separate unk replacement step as in current NMT practice.\nOur hybrid NMT offers a twofold advantage: it is much faster and easier to train than characterbased models; at the same time, it never produces unknown words as in the case of wordbased ones. We demonstrate at scale that on the WMT’15 English to Czech translation task, such a hybrid approach provides a boost of up to +7.9 BLEU points over models that do not handle unknown words. Our best hybrid system has established a new state-of-the-art result with 19.9\nBLEU score. Our analysis demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highlyinflected language with a very complex vocabulary, but also build correct representations for English source words."
    }, {
      "heading" : "2 Related Work",
      "text" : "There has been a recent line of work on end-toend character-based neural models which achieve good results for part-of-speech tagging (Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), text classification (Zhang et al., 2015), speech recognition (Chan et al., 2015; Bahdanau et al., 2015b), and language modeling (Kim et al., 2016; Jozefowicz et al., 2016). However, success has not been shown for cross-lingual tasks such as machine translation.2 Sennrich et al. (2015) propose to segment words into smaller units and translate just like at the word level, which does not learn to understand relationships among words.\nOur work takes inspiration from (Luong et al., 2013) and (Li et al., 2015). Similar to the former, we build representations for rare words on-the-fly from subword units. However, we utilize recurrent neural networks with characters as the basic units; whereas Luong et al. (2013) use recursive neural networks with morphemes as units, which requires existence of a morphological analyzer. In comparison with (Li et al., 2015), our hybrid architecture is also a hierarchical sequence-to-sequence model, but operates at a different granularity level, word-character. In contrast, Li et al. (2015) build hierarchical models at the sentence-word level for paragraphs and documents."
    }, {
      "heading" : "3 Background & Our Models",
      "text" : "Neural machine translation aims to directly model the conditional probability p(y|x) of translating a source sentence, x1, . . . , xn, to a target sentence, y1, . . . , ym. It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). The encoder computes a representation s for each source sentence. Based on that source\n2There is a recent work by Ling et al. (2015b) that attempts at character-level NMT; however, the experimental evidence is weak. The authors demonstrate only small improvements over word-level baselines and acknowledge that there are no differences of significance. Furthermore, these results were tested on small datasets without comparable results from past NMT work.\nrepresentation, the decoder generates a translation, one target word at a time, and hence, decomposes the log conditional probability as:\nlog p(y|x) = ∑m\nt=1 log p (yt|y<t, s) (1)\nA natural model for sequential data is the recurrent neural network (RNN), used by most of the recent NMT work. Papers, however, differ in terms of: (a) architecture – from unidirectional, to bidirectional, and deep multi-layer RNNs; and (b) RNN type – which are long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and the gated recurrent unit (Cho et al., 2014). All our models utilize the deep multi-layer architecture with LSTM as the recurrent unit; detailed formulations are in (Zaremba et al., 2014).\nConsidering the top recurrent layer in a deep LSTM, with ht being the current target hidden state as in Figure 2, one can compute the probability of decoding each target word yt as:\np (yt|y<t, s) = softmax (ht) (2)\nFor a parallel corpus D, we train our model by minimizing the below cross-entropy loss:\nJ = ∑\n(x,y)∈D − log p(y|x) (3)\nAttention Mechanism – The early NMT approaches (Sutskever et al., 2014; Cho et al., 2014), which we have described above, use only the last encoder state to initialize the decoder, i.e., setting the input representation s in Eq. (1) to [h̄n]. Recently, Bahdanau et al. (2015a) propose an attention mechanism, a form of random access memory for NMT to cope with long input sequences. Luong et al. (2015a) further extend the attention mechanism to different scoring functions, used to compare source and target hidden states, as well as different strategies to place the attention. In all our models, we utilize the global attention mechanism and the bilinear form for the attention scoring function similar to (Luong et al., 2015a).\nSpecifically, we set s in Eq. (1) to the set of source hidden states at the top layer, [h̄1, . . . , h̄n]. As illustrated in Figure 2, the attention mechanism consists of two stages: (a) context vector – the current hidden state ht is compared with individual source hidden states in s to learn an alignment vector, which is then used to compute the context vector ct as a weighted average of s; and (b) attentional hidden state – the context vector ct is then\nused to derive a new attentional hidden state:\nh̃t = tanh(W[ct;ht]) (4)\nThe attentional vector h̃t then replaces ht in Eq. (2) in predicting the next words."
    }, {
      "heading" : "4 Hybrid Neural Machine Translation",
      "text" : "Our hybrid architecture, as illustrated in Figure 1, leverages the power of both words and characters to achieve the goal of open vocabulary NMT. At the core of the design lies a word-level NMT with an advantage of being fast and easy to train; whereas, purely character-based NMT has to deal with very long sequences. The character recurrent models empower the word-level system with the abilities to compute any source word representation on the fly from characters and recover character-by-character unknown target words originally produced as <unk>."
    }, {
      "heading" : "4.1 Word-based Translation as a Backbone",
      "text" : "The core of our hybrid NMT is a deep LSTM encoder-decoder that translates at the word level as described in Section 3. We maintain a vocabulary of W frequent words for each language. Other words not inside these lists are represented by a universal symbol <unk>, one per language. We\ntranslate just like a word-based NMT system with respect to these source and target vocabularies, except for cases that involve <unk> in the source input or the target output. These correspond to the character-level components illustrated in Figure 1.\nA nice property of our hybrid approach is that by varying the vocabulary sizes, one can control how much to blend the word- and character-based models; hence, taking the best of both worlds."
    }, {
      "heading" : "4.2 Source Character-based Representation",
      "text" : "In regular word-based NMT, for all rare words outside the source vocabulary, one will feed the universal embedding representing <unk> as input to the encoder. This is problematic because it discards valuable information about the word surface forms. To fix that, we learn a deep LSTM model over characters of source words. In our running example, for the word “cute”, we will run our deep character-based LSTM over ‘c’, ‘u’, ‘t’, ‘e’, and ‘_’ (the boundary symbol). The final hidden state at the top layer will be used as the on-the-fly representation for the current rare word as illustrated in the bottom-left part of Figure 1.\nIt is useful to note that the layers of the deep character-based LSTM are always initialized with zero hidden states and cell values. One might propose to connect hidden states of the word-based LSTM to the character-based model; however, we chose this design for various reasons. First, it simplifies the architecture. Second, it allows for efficiency through precomputation: before each minibatch, we can compute representations for rare source words all at once. All instances of the same word share the same embedding, so the computation is per type. Lastly, while Ling et al. (2015b) found that it is slow and difficult to train source character-level models and had to resort to pretraining, we demonstrate later in the experiment section that we can train our deep character-level LSTM perfectly fine in an end-to-end fashion.\nThis approach is inspired by the work of (Luong et al., 2013) which also computes on-the-fly representations for rare words. Their work, however, is different from ours in that the representations are derived from morphemes, with recursive neural networks, and for the language modeling task."
    }, {
      "heading" : "4.3 Target Character-level Generation",
      "text" : "General word-based NMT allows generation of <unk> in the target output. Afterwards, there will be another post-processing step that handles these\nunknown tokens by utilizing the alignment information derived from the attention mechanism and then performing simple word dictionary lookup or identity copy (Jean et al., 2015a; Luong et al., 2015a). While this approach works, it suffers from various problems such as alphabet mismatches between the source and target vocabularies and multi-word alignments Our goal is to address all these issues and create a coherent framework that handles an unlimited output vocabulary.\nOur solution is to have a separate deep LSTM that “translates” at the character level given the current word-level state. We train our system such that whenever the word-level NMT produces an <unk>, we can consult this character-level decoder to recover the correct surface form of the unknown target word. This is illustrated in Figure 1. The training objective in Eq. (3) now consists of two components:\nJ = Jw + αJc (5)\nHere, Jw refers to the usual loss of the wordlevel NMT; in our example, it is the sum of the negative log likelihood of generating {“un”, “<unk>”, “chat”, “_”}. The remaining component Jc corresponds to the loss incurred by the newly-introduced decoder when predicting characters of those rare words not in the target vocabulary. In our running example, the predicted characters are {‘j’, ‘o’, ‘l’, ‘i’, ‘_’}."
    }, {
      "heading" : "4.3.1 Hidden-state Initialization",
      "text" : "Unlike the source character-based representations, which can be generated independent of context, the target character-level generation requires the current word-level context to produce meaningful translation. This brings up an important question about what can best represent the current context to so as to initialize the character-level decoder. We answer this question in the context of the attention mechanism described in Section 3, which has now become the defacto standard in NMT.\nThe final vector h̃t, just before the softmax as shown in Figure 2, seems to be a good candidate to initialize the character-level decoder. The reason is that h̃t combines information from both the context vector ct and the top-level recurrent state ht. We refer to it later in our experiments as the same-path target generation approach.\nOn the other hand, the same-path approach worries us because all vectors h̃t used to seed the character-level decoder might have similar values,\nleading to the same character sequence being produced. The reason is because h̃t is directly used in the softmax, Eq. (2), to predict the same <unk>. That might pose some challenges for the model to learn useful representations that can be used to accomplish two tasks at the same time, that is to predict <unk> and to generate character sequences. To address that concern, we propose another approach called separate-path target generation.\nOur separate-path target generation approach works as follows. We mimic the process described in Eq. (4) to create a counterpart vector h̆t that will be used to seed the character-level decoder:\nh̆t = tanh(W̆ [ct;ht]) (6)\nHere, W̆ is a new parameter matrix to be learned with which we hope to release from W the pressure of not having to extract information relevant to both the word- and character-generation processes. It is useful to note that only the hidden state of the first layer is initialized as discussed above. The other components in the characterlevel decoder such as the LSTM cells of all layers and the hidden states of higher layers, all start with zero values.\nImplementation-wise, the computation in the character-level decoder is done per word token instead of per type as in the source character component (§4.2). This is because the initialized hidden states are different across time steps and across sentences even if the surface forms to be generated are the same. For speed efficiency, we run a forward pass over the word-level decoder first. Then, we invoke, in batch mode, a forward pass over the character-level decoder for the surface forms of all the <unk> tokens. For memory efficiency, the character-level backward pass can be executed right after the character-level forward pass and we can split these computation into mini-batches if the amount of <unk> is large."
    }, {
      "heading" : "4.3.2 Word-Character Generation Strategy",
      "text" : "With the character-level decoder, we can view the final hidden states as representations for the surface forms of unknown tokens and could have fed these to the next time step. However, we chose not to do so for the efficiency reason explained next; instead, <unk> is fed to the word-level decoder “as is” using its corresponding word embedding.\nDuring training, this design choice helps decoupling all executions of the character-level decoder over <unk> instances as soon the word-\nlevel NMT completes. As such, we can invoke forward and backward passes of the character-level decoder over rare words independently in batch mode. At test time, our strategy is to first run a beam search decoder at the word level to find the best translation given by the word-level NMT. Such translation contains <unk>, so we utilize our character-level decoder with beam search to generate the actual words. translation according to the combined scores."
    }, {
      "heading" : "5 Experiments",
      "text" : "We evaluate the effectiveness of our models on the publicly available WMT’15 translation task from English into Czech with newstest2013 (3000 sentences) as a development set and newstest2015 (2656 sentences) as a test set. Two metrics are used: case-sensitive NIST BLEU (Papineni et al., 2002) and chrF3 (Popović, 2015).3 The latter measures the amounts of overlapping character ngrams and has been shown to be a better metric for translation tasks out of English."
    }, {
      "heading" : "5.1 Data",
      "text" : "Among the available language pairs in WMT’15, all involving English, we choose Czech as a target language for several reasons. First and foremost, Czech is a Slavic language with not only rich and complex inflection, but also fusional morphology in which a single morpheme can encode multiple grammatical, syntactic, or semantic meanings. As a result, Czech possesses an enormously large vocabulary (about 1.5 to 2 times bigger than that of English according to statistics in Table 1) and is a challenging language to translate into. Furthermore, this language pair has a large amount of training data, so we can evaluate at scale. Lastly, though our techniques are language independent, it is easier for us to work with Czech since Czech uses the Latin alphabet with some diacritics.\nIn terms of preprocessing, we only apply the standard tokenization practice.4 We choose for each language a list of 200 characters found in the top frequent words, which, as shown in Table 1, can represent more than 98% of the vocabulary. In our current experiments, we only consider the top\n3For NIST BLEU, we first run detokenizer.pl and then use mteval-v13a to compute the scores as per WMT guideline. For chrF3, we utilize the implementation here https://github.com/rsennrich/subword-nmt.\n4Use tokenizer.perl in Moses with default settings.\n500K frequent words per language.5"
    }, {
      "heading" : "5.2 Training Details",
      "text" : "We train three types of systems, purely wordbased, purely character-baed, and hybrid. Common to these architectures is a word-based NMT since the character-based systems are essentially word-based ones with longer sequences and the core of hybrid models is also a word-based NMT.\nIn training word-based NMT, we follow Luong et al. (2015a) to use the global attention mechanism together with similar hyperparameters: (a) deep LSTM models, 4 layers, 1024 cells, and 1024-dimensional embeddings, (b) uniforml initialization of parameters in [−0.1, 0.1], (c) 6- epoch training with plain SGD and a simple learning rate schedule – start with a learning rate of 1.0; after 4 epochs, halve the learning rate every 0.5 epoch, (d) mini-batches are of size 128 and shuffled, (e) the gradient is rescaled whenever its norm exceeds 5, and (f) dropout is used with probability 0.2 according to (Pham et al., 2014). We now detail differences across the three architectures.\nWord-based NMT – We constrain our source and target sequences to have a maximum length of 50 each; words that go past the boundary are ignored. The vocabularies are limited to the top 50K most frequent words in both languages.\n5All remaining words are represented by a <rare> token. This is mostly historical when training our early models. Better models trained on full vocabularies are under the way and we will report results in the next version.\nWords not in these vocabularies are converted into <unk>. After translating, we will perform dictionary6 lookup or identity copy for <unk> using the alignment information from the attention models. Such procedure was proposed by Luong et al. (2015b) and referred as the unk replace technique.\nCharacter-based NMT – The source and target sequences at the character level are often about 5 times longer than their counterparts in the wordbased models as we can infer from the statistics in Table 1. Due to memory constraint in GPUs, we limit our source and target sequences to a maxium length of 150 each, i.e., we backpropagate through at most 300 timesteps from the decoder to the encoder. With smaller 512-dimensional models, we can afford to have longer sequences with up to 600-step backpropagation. The vocabularies are limited to the first 200 characters appearing in top frequent words of each language.\nHybrid NMT – The word-level component uses the same settings as the purely word-based NMT. For the character-level source and target components, we experiment with both shallow and deep 1024-dimensional models of 1 and 2 LSTM layers. Similar to the purely characterbased NMT, we use a vocabulary of 200 characters per language. We set the weight α in Eq. (5) for our character-level loss to 1.0."
    }, {
      "heading" : "5.3 Results",
      "text" : "We compare our models with various other strong systems. These include the winning entry in WMT’15, which was trained on a huge amount of data, 52.6M parallel and 393.0M monolingual sentences (Bojar and Tamchyna, 2015).7 In contrast, we merely use the provided parallel corpus of 15.8M sentences. For NMT, to the best of our knowledge, (Jean et al., 2015b) has the best published performance on English-Czech translation.\nAs shown in Table 2, for purely word-based models, we achieve progressive improvements when using attention and performing unk replacement. Our single NMT model (e) outperforms the best single model in (Jean et al., 2015b) by +1.4 points despite using a smaller vocabulary of only 50K words versus 200K words. Our ensemble sys-\n6Obtained from the alignment links produced by the Berkeley aligner (Liang et al., 2006) over the training corpus.\n7This entry combines two independent systems, a phrasebased Moses model and a deep-syntactic transfer-based model. Additionally, there is an automatic post-editing system with hand-crafted rules to correct errors in morphological agreement and semantic meanings, e.g., loss of negation.\ntem has advanced the state-of-the-art (SOTA) result among NMT systems with 18.4 BLEU.\nTo our surprise, the purely character-based models, though extremely slow to train and test, perform quite well. The 512-dimensional attention-based model (h) is best, surpassing even the single word-based model in (Jean et al., 2015b) despite having much fewer parameters. It even scores strongly on chrF3 with 44.3 points, outperforming all word-based NMT systems. This indicates that this model is able to translate words that closely but not exactly match the reference ones as evidenced in Section 6.2. We notice two interesting observations. First, attention is critical for character-based models to work as is obvious from the poor performance of the non-attentional model; this has been verified in speech recognition (Chan et al., 2015). Second, long time-step backpropagation is more important as reflected by the fact that the larger 1024-dimensional model with shorter backprogration is inferior to the model (h).\nLastly, results in Table 2 justify why our proposed hybrid architecture is needed. With only 1K words, the model (j) achieves a non-trivial performance of 12.9 BLEU thanks to an improvement of +7.9 points given by the character-level compo-\nnents when replacing <unk>.8 In terms of vocabulary sizes, hybrid models with larger vocabulary sizes are clearly better. This suggests that an extreme hybrid system with W =0, a close approximation to those of (Ling et al., 2015b), will not work well. As such, among similar hierarchical models, our word-character approach is preferred.\nFurthermore, we demonstrate that our separatepath strategy for the character-level target generation (§4.3.1) is effective. At 10K words, the separate-path model (l) outperforms the model (k) with the same-path approach by +1.5 BLEU points. A deeper character-level architecture of 2 LSTM layers provides another significant boost of +1.9 BLEU points, leading to the system (m) with the lowest character-level perplexity of 1.59. The model (m) achieves a strong BLEU score of 17.5 points, surpassing the best word-level NMT. This proves that our hybrid model has successfully replaced the standard unk replacement process, offering a large improvement of +6.2 BLEU over the case when <unk> is not handled.\nLastly, when extending to 50K words, we further improve the translation quality. Our ensemble\n8A purely word-based model with 1K words (trained for 4 epochs with attention and unk replacement) gives <1 BLEU.\nsystem (o) has established a SOTA result of 19.9 BLEU in the WMT’15 English-Czech translation task, outperforming not just our own SOTA wordbased NMT system but also the best non-neural approach by +1.1 points. Our ensemble model is also best in terms of chrF3.9"
    }, {
      "heading" : "6 Analysis",
      "text" : "Apart from measuring translation quality, this section analyzes the effects of our source and target character-level components in more details."
    }, {
      "heading" : "6.1 Rare Word Embeddings",
      "text" : "We evaluate the source character-level model by building representations for rare words from characters and measuring how good these embeddings are. Quantitatively, we follow Luong et al. (2013) in using the word similarity task, specifically on the Rare Word dataset, to judge the learned representations for complex words. The evaluation metric is the Spearman’s correlation ρ between similarity scores assigned by a model and by human annotators. From the results in Table 3, we can see that source representations produced by\n9Note that, our other models combining both the separatepath approach and the 2-layer character-level components have not converged yet. We expect to obtain even better results in the next version of our paper.\nour hybrid10 models are significantly better than those of the word-based one. It is noteworthy that our deep recurrent character-level models can outperform the model of (Luong et al., 2013), which uses recursive neural networks and requires a complex morphological analyzer, by a large margin.\nQualitatively, we visualize embeddings produced by the hybrid model (n) for selected words in the Rare Word dataset. Figure 3 shows the two-dimensional representations of words computed by the Barnes-Hue-SNE algorithm (van der Maaten, 2013).11 It is extremely interesting to observe that words are clustered together not only\n10We look up the encoder embeddings for frequent words and build representations for rare word from characters.\n11We run Barnes-Hue-SNE algorithm over a set of 91 words, but filter out 27 words for displaying clarity.\nby the word structures but also by the meanings. For example, in the top-left box, the characterbased representations for “loveless”, “spiritless”, “heartlessly”, and “heartlessness” are nearby, but clearly separated into two groups. Similarly, in the center boxes, word-based embeddings of “acceptable”, “satisfactory”, “unacceptable”, and “unsatisfactory”, are close by but separated by meanings. Lastly, the remaining boxes demonstrate that our character-level models are able to build representations comparable to the word-based ones, e.g., “impossibilities” vs. “impossible” and “antagonize” vs. “antagonist”. All of these evidences strongly support that the source characterlevel models are useful and effective."
    }, {
      "heading" : "6.2 Sample Translations",
      "text" : "We show in Table 4 sample translations between various systems. In the first example, our hybrid model translates perfectly. The word-based model fails to translate “diagnosis” because the second <unk> was incorrectly aligned to the word “after”. The character-based model, on the other hand, makes a mistake in translating names.\nFor the second example, the hybrid model surprises us when it can capture the long-distance reordering of “said fifty years ago” and “pr̆ed\npadesáti lety” while the other two models do not. The word-based model translates “Jr.” inaccurately due to the incorrect alignment between the second <unk> and the word “said”. The character-based model literally translates the name “King” into “král” which means “king”.\nLastly, both the character-based and hybrid models impress us by their ability to translate compound words exactly, e.g., “11-year-old” and “jedenáctiletá”; whereas the identity copy strategy of the word-based model fails. Of course, our hybrid model does make mistakes, e.g., it fails to translate the name “Shani Bart”. Overall, these examples highlight how challenging translating into Czech is and that being able to translate at the character level helps improve the quality."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we have proposed a novel hybrid architecture that combines the strength of both word- and character-based models. Word-level models are fast to train and offer high-quality translation; whereas, character-level models help achieve the goal of open vocabulary NMT. We have demonstrated these two aspects through our experimental results and translation examples.\nOur best hybrid model have surpassed the per-\nformance of both the best word-based NMT system and the best non-neural model to establish a new state-of-the-art result for English-Czech translation in WMT’15 with 19.9 BLEU. Moreover, we have succeeded in replacing the standard unk replacement technique in NMT with our character-level components, yielding an improvement of up to +7.9 BLEU points. Our analysis have proven that our model has the ability to not only generate well-formed words for Czech, a highly inflected language with an enormous and complex vocabulary, but also build accurate representations for English source words.\nAdditionally, we have also demonstrated the potential of purely character-based models in producing good translation; they have outperformed past word-level NMT models. For future work, we hope to be able to improve the memory usage and speed of purely character-based models."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This work was partially supported by NSF Award IIS-1514268 and partially supported by a gift from Bloomberg L.P. We thank Dan Jurafsky, Andrew Ng, and Quoc Le for earlier feedback on the work, as well as Sam Bowman, Ziang Xie, and Jiwei Li for their valuable comments on the paper draft. Lastly, we acknowledge NVIDIA Corporation with the donation of Tesla K40 GPUs as well as the support of Andrew Ng and his group in letting us use their computing resources."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015a",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "End-to-end attention-based large vocabulary speech recognition",
      "author" : [ "Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1508.04395.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015b",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Improved transition-based parsing by modeling characters instead of words with lstms",
      "author" : [ "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Ballesteros et al\\.,? 2015",
      "shortCiteRegEx" : "Ballesteros et al\\.",
      "year" : 2015
    }, {
      "title" : "CUNI in WMT15: Chimera Strikes Again",
      "author" : [ "Ondřej Bojar", "Aleš Tamchyna." ],
      "venue" : "WMT.",
      "citeRegEx" : "Bojar and Tamchyna.,? 2015",
      "shortCiteRegEx" : "Bojar and Tamchyna.",
      "year" : 2015
    }, {
      "title" : "Listen, attend and spell",
      "author" : [ "William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1508.01211.",
      "citeRegEx" : "Chan et al\\.,? 2015",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "Bengio." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Bengio.,? 2014",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2014
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "On using very large target vocabulary for neural machine translation",
      "author" : [ "Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio." ],
      "venue" : "ACL.",
      "citeRegEx" : "Jean et al\\.,? 2015a",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Montreal neural machine translation systems for WMT’15",
      "author" : [ "Sébastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio." ],
      "venue" : "WMT.",
      "citeRegEx" : "Jean et al\\.,? 2015b",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploring the limits of language modeling",
      "author" : [ "R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu." ],
      "venue" : "arXiv preprint arXiv:1602.02410.",
      "citeRegEx" : "Jozefowicz et al\\.,? 2016",
      "shortCiteRegEx" : "Jozefowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical neural autoencoder for paragraphs and documents",
      "author" : [ "Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky." ],
      "venue" : "ACL.",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Alignment by agreement",
      "author" : [ "Percy Liang", "Ben Taskar", "Dan Klein." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Liang et al\\.,? 2006",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2006
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Wang Ling", "Chris Dyer", "Alan W. Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luís Marujo", "Tiago Luís." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Ling et al\\.,? 2015a",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Character-based neural machine translation",
      "author" : [ "Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan Black." ],
      "venue" : "arXiv preprint arXiv:1511.04586.",
      "citeRegEx" : "Ling et al\\.,? 2015b",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Stanford neural machine translation systems for spoken language domain",
      "author" : [ "Minh-Thang Luong", "Christopher D. Manning." ],
      "venue" : "IWSLT.",
      "citeRegEx" : "Luong and Manning.,? 2015",
      "shortCiteRegEx" : "Luong and Manning.",
      "year" : 2015
    }, {
      "title" : "Better word representations with recursive neural networks for morphology",
      "author" : [ "Minh-Thang Luong", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Luong et al\\.,? 2013",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2013
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Luong et al\\.,? 2015a",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Addressing the rare word problem in neural machine translation",
      "author" : [ "Minh-Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba." ],
      "venue" : "ACL.",
      "citeRegEx" : "Luong et al\\.,? 2015b",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei jing Zhu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Dropout improves recurrent neural networks for handwriting recognition",
      "author" : [ "Vu Pham", "Théodore Bluche", "Christopher Kermorvant", "Jérôme Louradour." ],
      "venue" : "Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on, pages 285–",
      "citeRegEx" : "Pham et al\\.,? 2014",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2014
    }, {
      "title" : "chrF: character n-gram F-score for automatic MT evaluation",
      "author" : [ "Maja Popović." ],
      "venue" : "WMT.",
      "citeRegEx" : "Popović.,? 2015",
      "shortCiteRegEx" : "Popović.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "arXiv preprint arXiv:1508.07909.",
      "citeRegEx" : "Sennrich et al\\.,? 2015",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Barnes-hut-sne",
      "author" : [ "Laurens van der Maaten." ],
      "venue" : "CoRR, abs/1301.3342.",
      "citeRegEx" : "Maaten.,? 2013",
      "shortCiteRegEx" : "Maaten.",
      "year" : 2013
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals." ],
      "venue" : "CoRR, abs/1409.2329.",
      "citeRegEx" : "Zaremba et al\\.,? 2014",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Despite being relatively new, NMT has already achieved state-of-the-art translation results for many language pairs such as English-French (Luong et al., 2015b), English-German (Jean et al.",
      "startOffset" : 139,
      "endOffset" : 160
    }, {
      "referenceID" : 7,
      "context" : ", 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015), and English-Czech (Jean et al.",
      "startOffset" : 25,
      "endOffset" : 91
    }, {
      "referenceID" : 18,
      "context" : ", 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015), and English-Czech (Jean et al.",
      "startOffset" : 25,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : ", 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015), and English-Czech (Jean et al.",
      "startOffset" : 25,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : ", 2015a; Luong and Manning, 2015), and English-Czech (Jean et al., 2015b).",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 15,
      "context" : "Luong et al. (2015b) propose to annotate occurrences of target <unk> with positional information to track their alignments, after which simple word dictionary lookup or identity copy can be performed to replace <unk> in the translation.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 7,
      "context" : "Jean et al. (2015a) approach the problem similarly but obtain the alignments for unknown words from the attention mechanism.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 23,
      "context" : "See more on this problem in (Sennrich et al., 2015).",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 17,
      "context" : "This is problematic as pointed out by Luong et al. (2013): neural networks can learn good representations for frequent words such as “distinct”, but fail for rare-but-related words like “distinctiveness”.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "There has been a recent line of work on end-toend character-based neural models which achieve good results for part-of-speech tagging (Ling et al., 2015a), dependency parsing (Ballesteros et al.",
      "startOffset" : 134,
      "endOffset" : 154
    }, {
      "referenceID" : 2,
      "context" : ", 2015a), dependency parsing (Ballesteros et al., 2015), text classification (Zhang et al.",
      "startOffset" : 29,
      "endOffset" : 55
    }, {
      "referenceID" : 27,
      "context" : ", 2015), text classification (Zhang et al., 2015), speech recognition (Chan et al.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : ", 2015), speech recognition (Chan et al., 2015; Bahdanau et al., 2015b), and language modeling (Kim et al.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : ", 2015), speech recognition (Chan et al., 2015; Bahdanau et al., 2015b), and language modeling (Kim et al.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : ", 2015b), and language modeling (Kim et al., 2016; Jozefowicz et al., 2016).",
      "startOffset" : 32,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : ", 2015b), and language modeling (Kim et al., 2016; Jozefowicz et al., 2016).",
      "startOffset" : 32,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : ", 2015; Bahdanau et al., 2015b), and language modeling (Kim et al., 2016; Jozefowicz et al., 2016). However, success has not been shown for cross-lingual tasks such as machine translation.2 Sennrich et al. (2015) propose to segment words into smaller units and translate just like at the word level, which does not learn to understand relationships among words.",
      "startOffset" : 8,
      "endOffset" : 213
    }, {
      "referenceID" : 17,
      "context" : "Our work takes inspiration from (Luong et al., 2013) and (Li et al.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : ", 2013) and (Li et al., 2015).",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : "In comparison with (Li et al., 2015), our hybrid architecture is also a hierarchical sequence-to-sequence model, but operates at a different granularity level, word-character.",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : ", 2013) and (Li et al., 2015). Similar to the former, we build representations for rare words on-the-fly from subword units. However, we utilize recurrent neural networks with characters as the basic units; whereas Luong et al. (2013) use recursive neural networks with morphemes as units, which requires existence of a morphological analyzer.",
      "startOffset" : 13,
      "endOffset" : 235
    }, {
      "referenceID" : 12,
      "context" : ", 2013) and (Li et al., 2015). Similar to the former, we build representations for rare words on-the-fly from subword units. However, we utilize recurrent neural networks with characters as the basic units; whereas Luong et al. (2013) use recursive neural networks with morphemes as units, which requires existence of a morphological analyzer. In comparison with (Li et al., 2015), our hybrid architecture is also a hierarchical sequence-to-sequence model, but operates at a different granularity level, word-character. In contrast, Li et al. (2015) build hierarchical models at the sentence-word level for paragraphs and documents.",
      "startOffset" : 13,
      "endOffset" : 550
    }, {
      "referenceID" : 10,
      "context" : "It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014).",
      "startOffset" : 63,
      "endOffset" : 137
    }, {
      "referenceID" : 24,
      "context" : "It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014).",
      "startOffset" : 63,
      "endOffset" : 137
    }, {
      "referenceID" : 14,
      "context" : "There is a recent work by Ling et al. (2015b) that attempts at character-level NMT; however, the experimental evidence is weak.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : "Papers, however, differ in terms of: (a) architecture – from unidirectional, to bidirectional, and deep multi-layer RNNs; and (b) RNN type – which are long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and the gated recurrent unit (Cho et al.",
      "startOffset" : 181,
      "endOffset" : 215
    }, {
      "referenceID" : 26,
      "context" : "All our models utilize the deep multi-layer architecture with LSTM as the recurrent unit; detailed formulations are in (Zaremba et al., 2014).",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "Attention Mechanism – The early NMT approaches (Sutskever et al., 2014; Cho et al., 2014), which we have described above, use only the last encoder state to initialize the decoder, i.",
      "startOffset" : 47,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : "In all our models, we utilize the global attention mechanism and the bilinear form for the attention scoring function similar to (Luong et al., 2015a).",
      "startOffset" : 129,
      "endOffset" : 150
    }, {
      "referenceID" : 0,
      "context" : "Recently, Bahdanau et al. (2015a) propose an attention mechanism, a form of random access memory for NMT to cope with long input sequences.",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "Recently, Bahdanau et al. (2015a) propose an attention mechanism, a form of random access memory for NMT to cope with long input sequences. Luong et al. (2015a) further extend the attention mechanism to different scoring functions, used to compare source and target hidden states, as well as different strategies to place the attention.",
      "startOffset" : 10,
      "endOffset" : 161
    }, {
      "referenceID" : 18,
      "context" : "Figure 2: Attention mechanism – shown are the two steps of the attention mechanism described in (Luong et al., 2015a): first, compute a context vector ct based on the current target hidden state ht and all the source hidden states [h̄1, .",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 17,
      "context" : "This approach is inspired by the work of (Luong et al., 2013) which also computes on-the-fly representations for rare words.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : "Lastly, while Ling et al. (2015b) found that it is slow and difficult to train source character-level models and had to resort to pretraining, we demonstrate later in the experiment section that we can train our deep character-level LSTM perfectly fine in an end-to-end fashion.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "Afterwards, there will be another post-processing step that handles these unknown tokens by utilizing the alignment information derived from the attention mechanism and then performing simple word dictionary lookup or identity copy (Jean et al., 2015a; Luong et al., 2015a).",
      "startOffset" : 232,
      "endOffset" : 273
    }, {
      "referenceID" : 18,
      "context" : "Afterwards, there will be another post-processing step that handles these unknown tokens by utilizing the alignment information derived from the attention mechanism and then performing simple word dictionary lookup or identity copy (Jean et al., 2015a; Luong et al., 2015a).",
      "startOffset" : 232,
      "endOffset" : 273
    }, {
      "referenceID" : 20,
      "context" : "Two metrics are used: case-sensitive NIST BLEU (Papineni et al., 2002) and chrF3 (Popović, 2015).",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 22,
      "context" : ", 2002) and chrF3 (Popović, 2015).",
      "startOffset" : 18,
      "endOffset" : 33
    }, {
      "referenceID" : 21,
      "context" : "2 according to (Pham et al., 2014).",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 17,
      "context" : "In training word-based NMT, we follow Luong et al. (2015a) to use the global attention mechanism together with similar hyperparameters: (a) deep LSTM models, 4 layers, 1024 cells, and 1024-dimensional embeddings, (b) uniforml initialization of parameters in [−0.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "Such procedure was proposed by Luong et al. (2015b) and referred as the unk replace technique.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "0M monolingual sentences (Bojar and Tamchyna, 2015).",
      "startOffset" : 25,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "For NMT, to the best of our knowledge, (Jean et al., 2015b) has the best published performance on English-Czech translation.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "Our single NMT model (e) outperforms the best single model in (Jean et al., 2015b) by +1.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : "Obtained from the alignment links produced by the Berkeley aligner (Liang et al., 2006) over the training corpus.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "(a) Best WMT’15, big data (Bojar and Tamchyna, 2015) - - - - 18.",
      "startOffset" : 26,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : "(b) RNNsearch + unk replace (Jean et al., 2015b) 200K - - - 15.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "7 (c) Ensemble 4 models + unk replace (Jean et al., 2015b) 200K - - - 18.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "The 512-dimensional attention-based model (h) is best, surpassing even the single word-based model in (Jean et al., 2015b) despite having much fewer parameters.",
      "startOffset" : 102,
      "endOffset" : 122
    }, {
      "referenceID" : 4,
      "context" : "First, attention is critical for character-based models to work as is obvious from the poor performance of the non-attentional model; this has been verified in speech recognition (Chan et al., 2015).",
      "startOffset" : 179,
      "endOffset" : 198
    }, {
      "referenceID" : 15,
      "context" : "This suggests that an extreme hybrid system with W =0, a close approximation to those of (Ling et al., 2015b), will not work well.",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 17,
      "context" : "Quantitatively, we follow Luong et al. (2013) in using the word similarity task, specifically on the Rare Word dataset, to judge the learned representations for complex words.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 17,
      "context" : "It is noteworthy that our deep recurrent character-level models can outperform the model of (Luong et al., 2013), which uses recursive neural networks and requires a complex morphological analyzer, by a large margin.",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 17,
      "context" : "(Luong et al., 2013) 138K 34.",
      "startOffset" : 0,
      "endOffset" : 20
    } ],
    "year" : 2016,
    "abstractText" : "Nearly all previous work in neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel wordcharacter solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT’15 English to Czech translation task, this hybrid approach offers a boost of up to +7.9 BLEU points over models that do not handle unknown words. Our best hybrid system has established a new stateof-the-art result with 19.9 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highlyinflected language with a very complex vocabulary, but also build correct representations for English source words.1",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}