{
  "name" : "1709.01423.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A heterogeneity based iterative clustering approach for obtaining samples with reduced bias",
    "authors" : [ "Chandrasekaran Anirudh Bhardwaj", "Megha Mishra", "Kalyani Desikan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "A heterogeneity based iterative clustering approach\nfor obtaining samples with reduced bias\nChandrasekaran Anirudh Bhardwaj1, Megha Mishra1 and Kalyani Desikan2\n1 B. Tech Computer Science, VIT University, Vandalur-Kelabmbakam Road, Chennai – 600127, India\n2 Faculty-School of Advanced Sciences, VIT University, Vandalur-Kelabmbakam Road, Chennai – 600127, India\nMedical and social sciences demand sampling techniques which are robust, reliable, replicable and give samples with the least bias. Majority of the applications of sampling use randomized sampling, albeit with stratification where applicable to lower the bias. The randomized technique is not consistent, and may provide different samples each time, and the different samples themselves may not be similar to each other. In this paper, we introduce a novel sampling technique called Wobbly Center Algorithm, which relies on iterative clustering based on maximizing heterogeneity to achieve samples which are consistent, and with low bias. The algorithm works on the principle of iteratively building clusters by finding the points with the maximal distance from the cluster center. The algorithm consistently gives a better result in lowering the bias by reducing the standard deviations in the means of each feature in a scaled data."
    }, {
      "heading" : "1. Introduction",
      "text" : "Sampling has never been more relevant, in part due to the ever increasing sizes of data been generated at each possible unit of time. It is necessary that samples truly represent the population from which they are taken, and they preserve the full range of variance in it. Such sampling is required in the fields of medical and social sciences, where there is a need for no-replacement sampling technique, which provides samples which may contain different data points, but must represent the original group (population) as it is, with no bias and full variance. The samples, should have high variance among themselves, but must be similar to each other. The problem can be constructed as an inverse clustering problem, wherein the intra-cluster distances are maximized and the intercluster distances are minimized, instead of minimizing intra-cluster distance and maximizing inter-cluster distances. In general, clustering approaches tend to maximize homogeneity within clusters, while the Wobbly Center Algorithm tends to maximize heterogeneity inside each cluster, thus creating clusters which truly represent the entire data.\nTill now, the focus of clustering techniques has been to split the population into different heterogeneous groups, by placing them into clusters which group them according to similarity. This is evident by the fact that most clustering algorithms including but not limited to K-means [1], K-medoid [2], CLARA [3], and CLARANS [4] use similarity based clustering approach.\nThe proposed Wobbly Center Algorithm uses a dissimilarity based clustering approach to fully capture the variances in the dataset, so that each cluster individually represents the full range of population. This is achieved by iteratively adding the most distant point from the mid-point of the cluster and then re-computing the mean of the cluster. This process is repeated until the required number of points are placed, or all the points in the dataset (population) are exhausted.\nThe Wobbly Center Algorithm, henceforth termed as WCA, performs consistently well in reducing bias. This is achieved by decreasing the inter-attribute variances in the mean of each attribute for each cluster. This algorithm is exceedingly effective on data which is scaled. WCA also gives the same result regardless of the number of times it is run, hence providing a better and more reliable solution compared to the randomized sampling techniques which are currently being employed extensively.\nThe K-means clustering algorithm gave rise to the similarity based clustering paradigm [5]. It involved assigning points closest to a cluster center to the same cluster, and then re-calculating the means of the clusters. The K-medoid clustering algorithm introduced an additional constraint mandating that the center of a cluster must be one of the data points [6]. Algorithms like Fuzzy c-means [7] introduced the concept of fuzziness coefficient and multiple membership to the clustering approach. It involved assigning a membership percentage to each data point for each cluster. Scalability in clustering was introduced by CLARA [8] and CLARANS [9] by employing different sampling techniques and combining different clustering approaches. Techniques like one pass sampling [10] for large datasets can also been deployed for reducing the numerosity of data points. Simultaneous Gaussian Model-Based clustering [11] for samples of multiple origin involves splitting the input data into multiple origin sources which they may have originated from. This is done by assigning cluster labels to the samples based on the source to which they belong. As such, a majority, if not all the clustering approaches, seek to maximize homogeneity in each cluster. WCA uses a contrarian dissimilarity based approach for clustering, to generate heterogeneous samples from a large dataset.\nMost of the applications of sampling rely on inherently approximate and unreliable randomized sampling techniques. The Sampling Importance Resampling commonly abbreviated as SIR [12] technique is objectively a better method of sampling as compared to simple randomized sampling [13]. It takes an initial sample from a large dataset (full population), and then resamples it to get a\nsmaller sample. This gives better results with lesser difference between the sample and the population as compared to simple random sampling.\nAs the random sampling error is inversely proportional to the square root of sample size [14], very small samples are inherently\nnot suitable for generalization. Secondly, since the obtained samples are different for each instance of randomized sampling, they are not reliable. Critical fields do not employ sampling to their fullest due to lapses in the current sampling techniques in terms of replicability. WCA produces results which are objectively better, and at the same time are replicable.\nThere are principally two types of sampling, Quantitative sampling and Qualitative Sampling [15]. Quantitative sampling method tries to deduce data into smaller packets [16]. The packets must be representative of the entire dataset. There are various methods in the Quantitative Sampling approach like Quota Sampling [17], Random samples [18], Probability sampling [19] etc. The WCA lies in the domain of Quantitative Sampling. Qualitative sampling tries to reduce the sample size from thousands of data points to a few tens of representative data points [14]. The reduced samples in qualitative sampling need not necessarily be generalizable, and they are required only to validate the hypothesis.\nThere are many applications of reliable and consistent sampling technique such as Medical Testing [20], Human Resources [21] etc. Testing medical drugs require that the people must be split into two groups with very similar characteristics. The groups must encompass the whole variance in the data, and should be similar to one another. WCA could be the best fit algorithm to perform such a sampling task. WCA could also be used by the Human Resource department to cluster neutral teams comprising of people from different nationalities, different physical and mental characteristics, and possessing different skillsets. This would help eliminate racial and skill bias. Likewise, WCA could be used for many other sampling tasks.\nThe key difference between the samples obtained from WCA and other sampling techniques is its ability to be invariant irrespective of distribution of data. Additionally, the different samples sampled from the original population are similar to each other in addition to being similar to the original population."
    }, {
      "heading" : "2. Wobbly Center Algorithm",
      "text" : "Wobbly Center Algorithm works on the principle of iteratively building clusters based on maximizing the dissimilarity inside each cluster as the selection criterion. The dataset is first scaled because the algorithm is scale variant. A vector comprising of the mean of each feature for entire dataset is created. The vector created is termed as the center of the dataset. A suitable distance metric is chosen, and the distances of each point from the center vector are calculated. The points nearest to the center are used as the seed points for creating the clusters. This ensures quicker convergence rates and better overall results. Trivially, it could also be inferred that the seed points are also the centers of each cluster for the first iteration. Next, the distances of each point from centers of cluster are calculated for each cluster. The most distant points from the centers of each cluster are placed sequentially, without repetition, into the cluster set. The mean of each feature for each cluster is computed again, and the process of adding the most distant point to cluster set is repeated. The process is repeated until all points have been placed in any one of the cluster set."
    }, {
      "heading" : "3. Test Conditions",
      "text" : "The data was first scaled using Z score normalization with the help of StandardScaler Function in Ski-kit Learn [22] Library. This transformation ensured that the data had zero normal mean and unit variance, as the Wobbly Center Algorithm is scale variant (Due to the nature of the distance metric used).\nOn the normalized dataset, the wobbly center algorithm was tested with respect to random sampling obtained from train_test_split function in the Ski-Kit Learn library. All the datasets were procured from the repository hosted by the University of California, Irvine [23].\n4. Algorithm\n4.1. Notations\n4.1.1 Let \uD835\uDC3A\uD835\uDC59\uD835\uDC5C\uD835\uDC4F\uD835\uDC4E\uD835\uDC59\uD835\uDC3F\uD835\uDC56\uD835\uDC60\uD835\uDC61 contain all the points in the dataset 4.1.2 Let \uD835\uDC37 represent the number of Clusters 4.1.3 Let \uD835\uDC36\uD835\uDC58 represent Cluster k 4.1.4 Let \uD835\uDC4C\uD835\uDC56 represent the vector for the point situated at ith position in the dataset 4.1.5 Let \uD835\uDC49 represent the vector comprising of mean values of all features of all the points 4.1.6 Let \uD835\uDC49\uD835\uDC58 represent the vector comprising of mean values of all features of all the points in Cluster 4.1.7 Let \uD835\uDC37\uD835\uDC56\uD835\uDC60\uD835\uDC61(\uD835\uDC4E, \uD835\uDC4F) represent the Euclidean Distance between point and point\n4.2 Pseudo code 4.2.1 Find \uD835\uDC49 4.2.2 Find \uD835\uDC37\uD835\uDC56\uD835\uDC60\uD835\uDC61(\uD835\uDC49, \uD835\uDC4C\uD835\uDC56) where \uD835\uDC56 ∈ \uD835\uDC3A\uD835\uDC59\uD835\uDC5C\uD835\uDC4F\uD835\uDC4E\uD835\uDC59\uD835\uDC3F\uD835\uDC56\uD835\uDC60\uD835\uDC61 4.2.3 Sort the distances in ascending order 4.2.4 Assign \uD835\uDC5B\uD835\uDC62\uD835\uDC5A = 1 4.2.5 Repeat while \uD835\uDC5B\uD835\uDC62\uD835\uDC5A ≤ \uD835\uDC37\n4.2.5.1 Choose least distant point in the \uD835\uDC3A\uD835\uDC59\uD835\uDC5C\uD835\uDC4F\uD835\uDC4E\uD835\uDC59\uD835\uDC3F\uD835\uDC56\uD835\uDC60\uD835\uDC61 4.2.5.2 Let the least distant point be \uD835\uDC4C\uD835\uDC59 4.2.5.3 Assign \uD835\uDC4C\uD835\uDC59 \uD835\uDF16 \uD835\uDC36\uD835\uDC5B\uD835\uDC62\uD835\uDC5A 4.2.5.4 Discard \uD835\uDC4C\uD835\uDC59 from \uD835\uDC3A\uD835\uDC59\uD835\uDC5C\uD835\uDC4F\uD835\uDC4E\uD835\uDC59\uD835\uDC3F\uD835\uDC56\uD835\uDC60\uD835\uDC61 4.2.5.5 Assign \uD835\uDC5B\uD835\uDC62\uD835\uDC5A = \uD835\uDC5B\uD835\uDC62\uD835\uDC5A + 1\n4.2.6 Repeat while \uD835\uDC3A\uD835\uDC59\uD835\uDC5C\uD835\uDC4F\uD835\uDC4E\uD835\uDC59\uD835\uDC3F\uD835\uDC56\uD835\uDC60\uD835\uDC61 is not \uD835\uDC38\uD835\uDC5A\uD835\uDC5D\uD835\uDC61\uD835\uDC66 4.2.6.1 Assign \uD835\uDC5B\uD835\uDC62\uD835\uDC5A = 1 4.2.6.2 Repeat while \uD835\uDC5B\uD835\uDC62\uD835\uDC5A ≤ \uD835\uDC37\n4.2.6.2.1 Find \uD835\uDC37\uD835\uDC56\uD835\uDC60\uD835\uDC61(\uD835\uDC49\uD835\uDC5B\uD835\uDC62\uD835\uDC5A, \uD835\uDC4C\uD835\uDC56) where \uD835\uDC56 ∈ \uD835\uDC3A\uD835\uDC59\uD835\uDC5C\uD835\uDC4F\uD835\uDC4E\uD835\uDC59\uD835\uDC3F\uD835\uDC56\uD835\uDC60\uD835\uDC61 4.2.6.2.2 Find the most distant point in \uD835\uDC3A\uD835\uDC59\uD835\uDC5C\uD835\uDC4F\uD835\uDC4E\uD835\uDC59\uD835\uDC3F\uD835\uDC56\uD835\uDC60\uD835\uDC61 4.2.6.2.3 Let the most distant point in \uD835\uDC3A\uD835\uDC59\uD835\uDC5C\uD835\uDC4F\uD835\uDC4E\uD835\uDC59\uD835\uDC3F\uD835\uDC56\uD835\uDC60\uD835\uDC61 be \uD835\uDC4Cℎ 4.2.6.2.4 Assign \uD835\uDC4Cℎ \uD835\uDF16 \uD835\uDC36\uD835\uDC5B\uD835\uDC62\uD835\uDC5A 4.2.6.2.5 Discard \uD835\uDC4Cℎ from \uD835\uDC3A\uD835\uDC59\uD835\uDC5C\uD835\uDC4F\uD835\uDC4E\uD835\uDC59\uD835\uDC3F\uD835\uDC56\uD835\uDC60\uD835\uDC61 4.2.6.2.6 Assign \uD835\uDC5B\uD835\uDC62\uD835\uDC5A = \uD835\uDC5B\uD835\uDC62\uD835\uDC5A + 1"
    }, {
      "heading" : "5 Experimental Analysis and Results",
      "text" : "Setting the null hypothesis as no statistical deviation for the two samples obtained via the Wobbly Center Algorithm as well as the Random Split Algorithm using Ski-Kit Learn, with confidence interval as 95% and subsequently the alpha as\n\uD835\uDEFC = 0.05\nwe can test whether the obtained splits fit the probability distribution of one other or not by comparing the obtained Anova\nvalue with the alpha. We use one-way Analysis of Variance [25], (ANOVA), to test the hypothesis.\nOn testing the hypothesis for splits obtained by the Wobbly Center Algorithm, we fail to reject the null hypothesis for each attribute in the dataset. Using ANOVA on the splits obtained by the Random Split Algorithm, we find that there exists at least one statistically deviant attribute for the dataset which is rejected by the null hypothesis ANOVA was performed using SCIPY [26] stats one_way_anova function.\nFigures 1.a and 1.b represent the convergence achieved by the distances between the mean of the clusters and the mean of the full data set. The convergence indicates that the clusters begin to resemble the full dataset as more iterations are performed, in concurrence with the Central Limit Theorem.\nTable 2.a and 2.b show the calculations performed over the results obtained from the data points allocated to each cluster. Mean, Standard Deviation for all features of both the clusters are calculated, and 1-way ANOVA test is performed over the two clusters to find the similarity in the fit of probability distributions."
    }, {
      "heading" : "5. Discussion",
      "text" : "By applying ANOVA, we find statistical differences between the clusters obtained by the random sampling algorithm, whereas we fail to find any statistical deviation between the clusters obtained by the Wobbly Center Algorithm. This means that Wobbly Center Algorithm gives a better split than a simple random sampling technique. Moreover, the Wobbly Center Algorithm consistently gives the same split over multiple runs. Hence, it is a more reliable method of finding heterogeneous splits.\nThe values obtained for other datasets, Wine Dataset [27] and Cloud Cover [28] Dataset, also indicate similar results as Abalone Dataset.\nThe centers of the clusters oscillate between the extremes and gradually dampen to converge to the original center of the full data from which the samples are derived. Hence, the distance of the center of the clusters from the mean also converges. This oscillation ensures that each cluster contains points which represent the full dataset (population). The graph tends to converge towards zero distance achieving stability at approximately 20th iteration. The initial variance between the values of the distances of the means of the two clusters signify that the clusters capture the outliers first, ensuring that a misbalance in the number of outlier points between the clusters does not occur. The tapering of the graph after some iterations indicate that the clusters try to fit the entire distribution of the population and the clusters themselves begin to represent the populations independently.\nThe clusters obtained can be used to test hypothesis in the domain of physical and medical sciences where there is a need for noreplacement selection of samples. The most basic need of research in the aforementioned fields is the presence of unbiased samples which represent each other as well as the original full sample distribution. Additionally, constraints like no-replacement criterion also apply. The Wobbly Center Algorithm excels in this aspect, and provides consistent samples which are statistically similar and can be reliably reproduced over multiple runs.\nMoreover, since the data points are Z-score normalized to fit zero mean and unit variance, the individual means of each feature of the dataset should also be closer to zero. This is achieved by the Wobbly Center Algorithm, which provides clusters which have the mean of each feature to be closer to zero than simple random sampling technique. The standard deviations obtained for all the features also show a similar trend, and Wobbly Center Algorithm consistently gives standard deviations near to the value of one as compared to the standard deviations obtained for each feature of the splits derived from random sampling.\nThe Wobbly Center Algorithm also takes care of the outliers by placing them in different clusters, due to the emphasis on maximizing the distance metric. This means that there is lesser possibility of a group of outliers to have an impact on a cluster’s generalizability.\nThe random sampling algorithm uses the Central Limit theorem, which states that, given a sufficiently large sample from the original population, the mean of the sample and the mean of the population would approximately be the same, and the distribution will approximately represent a normal distribution. The Wobbly Center Algorithm tries to overcome a flaw that the Central Limit Theorem [29], henceforth abbreviated as CLT does not cover. Specifically, CLT does not state anything about the mean and the distribution of two samples obtained from the population being similar to each other. This is represented by the fact that the samples obtained by random sampling algorithm do not pass the ANOVA Test for similar distributions between the obtained sample. Furthermore, Wobbly Center Algorithm is invariant to the distribution of the original population. But, the Wobbly Center Algorithm is variant with respect to the distance metric, and the variance in each feature.\nThe Wobbly Center Algorithm can be parallelized by enabling the calculations of the distances and sorting of the distance array be done independently for each cluster, and only the allocation of the points to the clusters be done sequentiallyi"
    } ],
    "references" : [ {
      "title" : "Algorithm AS 136: A k-means clustering algorithm",
      "author" : [ "J.A. Hartigan", "M.A. Wong" ],
      "venue" : "Journal of the Royal Statistical Society. Series C (Applied Statistics),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1979
    }, {
      "title" : "Clustering by means of medoids",
      "author" : [ "Kaufman", "L. Rousseeuw", "Peter" ],
      "venue" : "Statistical Data Analysis Based on the L1 Norm and Related Methods",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1987
    }, {
      "title" : "Clustering Large Applications (Program CLARA), in Finding Groups in Data: An Introduction to Cluster Analysis",
      "author" : [ "L. Kaufman", "P.J. Rousseeuw" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1990
    }, {
      "title" : "CLARANS: a method for clustering objects for spatial data mining",
      "author" : [ "R.T. Ng", "Jiawei Han" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, vol. 14, no. 5, pp. 1003-1016, Sep/Oct 2002.doi: 10.1109/TKDE.2002.1033770",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Data clustering: 50 years beyond K-means",
      "author" : [ "A.K. Jain" ],
      "venue" : "Pattern recognition letters,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "FCM: The fuzzy c-means clustering algorithm, Computers & Geosciences",
      "author" : [ "James C. Bezdek", "Robert Ehrlich", "William Full" ],
      "venue" : "Volume 10, Issue",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1984
    }, {
      "title" : "Analysis and implementation of CLARA algorithm on clustering",
      "author" : [ "G.F. ZHAO", "G.Q. QU" ],
      "venue" : "Journal of Shandong University of Technology (Science and Technology),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2006
    }, {
      "title" : "Fast algorithms for projected clustering",
      "author" : [ "C.C. Aggarwal", "J.L. Wolf", "P.S. Yu", "C. Procopiuc", "J.S. Park", "June" ],
      "venue" : "In ACM SIGMoD Record (Vol. 28,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "Random sampling techniques for space efficient online computation of order statistics of large datasets",
      "author" : [ "Gurmeet Singh Manku", "Sridhar Rajagopalan", "Bruce G. Lindsay" ],
      "venue" : "In Proceedings of the 1999 ACM SIGMOD international conference on Management of data (SIGMOD '99)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1999
    }, {
      "title" : "Simultaneous Gaussian Model-Based Clustering for Samples of Multiple Origins",
      "author" : [ "Alexandre Lourme", "Christophe Biernacki" ],
      "venue" : "Computational Statistics, Springer Verlag,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Improved Sampling‐Importance Resampling and Reduced Bias Importance Sampling",
      "author" : [ "Ø. Skare", "E. Bølviken", "L. Holden" ],
      "venue" : "Scandinavian Journal of Statistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2003
    }, {
      "title" : "Random sampling from databases (Doctoral dissertation, University of California at Berkeley)",
      "author" : [ "F. Olken" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1993
    }, {
      "title" : "Sampling for qualitative research",
      "author" : [ "M.N. Marshall" ],
      "venue" : "Family practice,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1996
    }, {
      "title" : "Focus on research methods combining qualitative and quantitative sampling, data collection, and analysis techniques",
      "author" : [ "M. Sandelowski" ],
      "venue" : "Research in nursing & health,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2000
    }, {
      "title" : "An Experimental Study of Quota Sampling.",
      "author" : [ "C.A. Moser", "A. Stuart" ],
      "venue" : "Journal of the Royal Statistical Society. Series A (General)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1953
    }, {
      "title" : "Introductory statistics (Vol. 19690)",
      "author" : [ "T.H. Wonnacott", "R.J. Wonnacott" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1972
    }, {
      "title" : "An evaluation of model-dependent and probability-sampling inferences in sample surveys",
      "author" : [ "M.H. Hansen", "W.G. Madow", "B.J. Tepping" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1983
    }, {
      "title" : "Design, data analysis and sampling techniques for clinical research",
      "author" : [ "K. Suresh", "S.V. Thomas", "G. Suresh" ],
      "venue" : "Annals of Indian Academy of Neurology,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "The management and control of quality",
      "author" : [ "J.R. Evans", "W.M. Lindsay" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1999
    }, {
      "title" : "UCI Machine Learning Repository [http://archive.ics.uci.edu/ml",
      "author" : [ "M. Lichman" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Abalone Dataset UCI Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/abalone]. Irvine, CA: University of California, School of Information and Computer Science [Dataset",
      "author" : [ "Sam Waugh" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1995
    }, {
      "title" : "Using multivariate statistics",
      "author" : [ "B.G. Tabachnick", "L.S. Fidell", "S.J. Osterlind" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2001
    }, {
      "title" : "Scipy: Open Source Scientific Tools for Python, 2001-, http://www.scipy.org/ [Online; accessed",
      "author" : [ "E Jones", "E Oliphant", "P Peterson" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2017
    }, {
      "title" : "Wine Dataset UCI Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/wine]. Irvine, CA: University of California, School of Information and Computer Science [Dataset",
      "author" : [ "Stefan Aeberhard" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1991
    }, {
      "title" : "Cloud Dataset UCI Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/cloud] Irvine, CA: University of California, School of Information and Computer Science [Dataset",
      "author" : [ "Philippe Collard" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1989
    }, {
      "title" : "Normal Approximations and the Central Limit Theorem. In: Fundamentals of Probability: A First Course. Springer Texts in Statistics. Springer, New York, NY, https://doi.org/10.1007/978-1-4419-5780-1_10 i The research is under consideration at Pattern Recognition Letters (Elsevier",
      "author" : [ "A. DasGupta" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This is evident by the fact that most clustering algorithms including but not limited to K-means [1], K-medoid [2], CLARA [3], and CLARANS [4] use similarity based clustering approach.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "This is evident by the fact that most clustering algorithms including but not limited to K-means [1], K-medoid [2], CLARA [3], and CLARANS [4] use similarity based clustering approach.",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "This is evident by the fact that most clustering algorithms including but not limited to K-means [1], K-medoid [2], CLARA [3], and CLARANS [4] use similarity based clustering approach.",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "This is evident by the fact that most clustering algorithms including but not limited to K-means [1], K-medoid [2], CLARA [3], and CLARANS [4] use similarity based clustering approach.",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : "The K-means clustering algorithm gave rise to the similarity based clustering paradigm [5].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : "Algorithms like Fuzzy c-means [7] introduced the concept of fuzziness coefficient and multiple membership to the clustering approach.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : "Scalability in clustering was introduced by CLARA [8] and CLARANS [9] by employing different sampling techniques and combining different clustering approaches.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "Scalability in clustering was introduced by CLARA [8] and CLARANS [9] by employing different sampling techniques and combining different clustering approaches.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "Techniques like one pass sampling [10] for large datasets can also been deployed for reducing the numerosity of data points.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "Simultaneous Gaussian Model-Based clustering [11] for samples of multiple origin involves splitting the input data into multiple origin sources which they may have originated from.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "The Sampling Importance Resampling commonly abbreviated as SIR [12] technique is objectively a better method of sampling as compared to simple randomized sampling [13].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "The Sampling Importance Resampling commonly abbreviated as SIR [12] technique is objectively a better method of sampling as compared to simple randomized sampling [13].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "As the random sampling error is inversely proportional to the square root of sample size [14], very small samples are inherently not suitable for generalization.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "There are principally two types of sampling, Quantitative sampling and Qualitative Sampling [15].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : "There are various methods in the Quantitative Sampling approach like Quota Sampling [17], Random samples [18], Probability sampling [19] etc.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 15,
      "context" : "There are various methods in the Quantitative Sampling approach like Quota Sampling [17], Random samples [18], Probability sampling [19] etc.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 16,
      "context" : "There are various methods in the Quantitative Sampling approach like Quota Sampling [17], Random samples [18], Probability sampling [19] etc.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "Qualitative sampling tries to reduce the sample size from thousands of data points to a few tens of representative data points [14].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 17,
      "context" : "There are many applications of reliable and consistent sampling technique such as Medical Testing [20], Human Resources [21] etc.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 18,
      "context" : "There are many applications of reliable and consistent sampling technique such as Medical Testing [20], Human Resources [21] etc.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 19,
      "context" : "All the datasets were procured from the repository hosted by the University of California, Irvine [23].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : "We use one-way Analysis of Variance [25], (ANOVA), to test the hypothesis.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : "Using ANOVA on the splits obtained by the Random Split Algorithm, we find that there exists at least one statistically deviant attribute for the dataset which is rejected by the null hypothesis ANOVA was performed using SCIPY [26] stats one_way_anova function.",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 20,
      "context" : "b represent the graph for distance of individual means of each cluster from the actual means of the data for Abalone [24] Dataset for the first 100 iterations of WCA",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 20,
      "context" : "a represents the results calculated for Abalone [24] Dataset for Wobbly Center Algorithm for 2 clusters Attribute Mean(Cluster 1) Mean(Cluster 2) STD(Cluster 1) STD(Cluster 2) 1-way Anova",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 20,
      "context" : "b represents the results calculated for Abalone [24] Dataset for Random Sampling Algorithm for 2 clusters Attribute Mean(Cluster 1) Mean(Cluster 2) STD(Cluster 1) STD(Cluster 2) 1-way Anova",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 23,
      "context" : "The values obtained for other datasets, Wine Dataset [27] and Cloud Cover [28] Dataset, also indicate similar results as Abalone Dataset.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 24,
      "context" : "The values obtained for other datasets, Wine Dataset [27] and Cloud Cover [28] Dataset, also indicate similar results as Abalone Dataset.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 25,
      "context" : "The Wobbly Center Algorithm tries to overcome a flaw that the Central Limit Theorem [29], henceforth abbreviated as CLT does not cover.",
      "startOffset" : 84,
      "endOffset" : 88
    } ],
    "year" : 2017,
    "abstractText" : "Medical and social sciences demand sampling techniques which are robust, reliable, replicable and give samples with the least bias. Majority of the applications of sampling use randomized sampling, albeit with stratification where applicable to lower the bias. The randomized technique is not consistent, and may provide different samples each time, and the different samples themselves may not be similar to each other. In this paper, we introduce a novel sampling technique called Wobbly Center Algorithm, which relies on iterative clustering based on maximizing heterogeneity to achieve samples which are consistent, and with low bias. The algorithm works on the principle of iteratively building clusters by finding the points with the maximal distance from the cluster center. The algorithm consistently gives a better result in lowering the bias by reducing the standard deviations in the means of each feature in a scaled data.",
    "creator" : "Microsoft® Word 2016"
  }
}