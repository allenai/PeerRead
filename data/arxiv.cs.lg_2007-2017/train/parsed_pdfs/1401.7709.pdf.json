{
  "name" : "1401.7709.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Joint Inference of Multiple Label Types in Large Networks",
    "authors" : [ "Deepayan Chakrabarti" ],
    "emails" : [ "DEEPAY@FB.COM", "SFUNIAK@FB.COM", "JONCHANG@FB.COM", "SOFMAC@FB.COM", "recall@1", "recall@3." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 1.\n77 09\nv1 [\ncs .L\nG ]\n3 0"
    }, {
      "heading" : "1. Introduction",
      "text" : "Inferring labels of nodes in networks is a common classification problem across a wide variety of domains ranging from social networks to bibliographic networks to biological networks and more. The typical goal is to predict a single label of low dimensionality for each node in the network (say, whether a webpage in a .edu domain belongs to a professor, student, or the department) given a partially labeled network and possibly attributes of the nodes. In this paper we instead consider the problem of inferring multiple\nThis is a technical report in preparation for submission to a peerreviewed conference.\ncurrent city=C\nNode u\nhometown=H\ncurrent city=C’ hometown=H\nFigure 1. An example graph of u and her friends: The hometown friends of u coincidentally contain a subset with current city C′. This swamps the group from u’s actual current city C, causing label propagation to infer C′ for u. However, our proposed model (called EDGEEXPLAIN) correctly explains all friendships by setting the hometown to be H and current city to be C.\nfields such as the hometowns, current cities, and employers of users of a social network, where users often only partially fill in their profile, if at all. Here, we have multiple types of missing labels, where each label type can be very high-dimensional and correlated. Joint inference of such label types is important for many ranking and relevance applications such as friend recommendation, ads and content targeting, and user-initiated searches for friends, motivating our focus on this problem.\nOne standard method of label inference is label propagation (Zhu & Ghahramani, 2002; Zhu et al., 2003), which tries to set the label probabilities of nodes so that friends have similar probabilities. While this method succinctly\ncaptures the essence of homophily (the more two nodes have in common, the more likely they are to connect (McPherson et al., 2001)), it optimizes for only a single type of label and assumes only a single category of relationships. It therefore fails to address the potential complexity of edge formation in networks, where nodes have different reasons to link to each other. As an example, consider the snapshot of a social network in Figure 1, where we want to predict the hometown and current city of node u, given what we know about u and u’s neighbors. Here, the labels of node u are completely unknown, but her friends’ labels are completely known. Label propagation would treat each label independently and infer the hometown of u to be the most common hometown among her friends, the current city to be the most common current city among friends, and so on. Hence, if the bulk of friends of u are from her hometown H , then inferences for current city will be dominated by the most common current city among her hometown friends (say, C′) and not friends from her actual current city C; indeed, the same will happen for all other label types as well.\nOur proposed method, named EDGEEXPLAIN, approaches the problem from a different viewpoint, using the following intuition: Two nodes form an edge for a reason that is likely to be related to them sharing the value of one or more label types (e.g., two users went to the same college). Using this intuition, we can go beyond standard label propagation in the following way: instead of taking the graph as given, and modeling labels as items that propagate over this graph, we consider the labels as factors that can explain the observed graph structure. For example, the inferences for u made by label propagation leave u’s edges from C completely unexplained. Our proposed method rectifies this, by trying to infer node labels such that for each edge u ∼ v, we can point to a reason why this is so — u and v are friends from the same hometown, or college, or the like. While we are primarily interested in inferring labels, we note that the inferred reason for each edge can be important applications by itself; e.g., if a new node u joins a network and forms and edge with v, knowledge of the reason can help with the well-known link prediction task — should we recommend v’s college friends, or friends from the same high school, etc.?\nWe note that a seemingly simple alternative solution — cluster the graph and then propagate the most common labels within a cluster — is in fact quite problematic. In addition to the computation cost, any clustering based solely on the graph structure ignores labels already available from user profiles, but any clustering that tries to use these labels must deal with incomplete and missing labels. The clustering must also be complex enough to allow many overlapping clusters. Hence, we believe that clustering does not readily lend itself to a solution for our problem.\nOur contributions are:\n1. We formulate the label inference problem as one of explaining the graph structure using the labels. We explicitly account for the fact that labels belong to a limited set of label types, whose properties we enumerate and incorporate into our model.\n2. Our gradient-based iterative method for inferring labels is easily implemented in large-scale messagepassing architectures. We empirically demonstrate its scalability on a billion-node subset of the Facebook social network, using publicly available user profiles and friendships.\n3. On this large real-world dataset, EDGEEXPLAIN significantly outperforms label propagation for several label types, with lifts of up to 120% for recall@1 and 60% for recall@3. These improvements in accuracy, combined with the scalability of EDGEEXPLAIN, clearly demonstrate its usefulness for label inference on large networks.\nThe paper is organized as follows. We survey related work in Section 2. Our proposed model is discussed in Section 3, followed by the inference method in Section 4, and generalizations of the model in Section 5. Empirical evidence proving the effectiveness of our method is presented in Section 6, followed by conclusions in Section 7."
    }, {
      "heading" : "2. Related Work",
      "text" : "We discuss prior work in semi-supervised learning, statistical relational learning, and in latent models for networks.\nSEMI-SUPERVISED LEARNING. Many graph-based approaches can be viewed as estimating a function over the nodes of the graph, with the function being close to the observed labels, and smooth (similar) at adjacent nodes. Label propagation (Zhu et al., 2003; Zhu & Ghahramani, 2002) uses a quadratic function, but other penalties are also possible (Zhou et al., 2004; Belkin et al., 2004; 2005). Other approaches modify the random walk interpretation of label propagation (Baluja et al., 2008; Talukdar & Crammer, 2009). In order to handle a large number of distinct label values, the label assignments can be summarized using countmin sketches (Talukdar & Cohen, 2014). None of the approaches consider interactions between multiple label types, and hence fail to capture the edge formation process in graphs considered here.\nSTATISTICAL RELATIONAL LEARNING. These algorithms typically predict a label based on (a) a local classifier that uses a node’s attributes alone, (b) a relational classifier that uses the labels at adjacent nodes,\nand (c) a collective inference procedure that propagates the information through the network (Chakrabarti et al., 1998; Perlich & Provost, 2003; Lu & Getoor, 2003; Macskassy & Provost, 2007). Macskassy et al. (2007) observe that the best algorithms (weighted-vote relational neighbor classifier (Macskassy & Provost, 2007) with relaxation labeling (Rosenfeld & Hummel, 1976; Hummel & Zucker, 1983)) tend to perform as well as label propagation, which we outperform. While there has been some work focusing on understanding how to combine and weigh different edge types for best prediction performance (Macskassy, 2007), the edge types (analogous to our reason for an edge) were given up front. We note that these algorithms typically focus on a single label type, while we explicitly model the interactions among multiple types.\nThere is also extensive work on probabilistic relational models, including Relational Bayesian Networks (Koller & Pfeffer, 1998; Friedman et al., 1999), Relational Dependency Networks (Neville & Jensen, 2007), and Relational Markov Networks (Taskar et al., 2002). These are very general formalisms, but it is our explicit modeling assumptions regarding multiple label types that yields gains in accuracy.\nLATENT MODELS. Graph structure has been modeled using latent variables (Hoff et al., 2002; Miller et al., 2009; Palla et al., 2012), but with an emphasis on link prediction. However, our goal is to make predictions about each individual user, and such latent features can be arbitrary combinations of user attributes, rather than concrete label types we wish to predict. Other models simultaneously explain the connections between documents as well as their word distributions (Nallapati et al., 2008; Chang & Blei, 2010; Ho et al., 2012). While we do not consider the problem of modeling text data, our model permits us to incorporate node attributes, such as group memberships. Finally, the number of distinct label values in our application is very large (on the order of millions), and we suspect that the latent variables would have to have a large dimension to explain the edges in our graph well."
    }, {
      "heading" : "3. Proposed Model",
      "text" : "In this section we first build intuition about our model using a running example. Suppose we want to infer the labels (e.g., “Palo Alto High School” and “Stanford University”) corresponding to several label types (e.g., high school and college) for a large collection of users. The available data consist of labels publicly declared by some users, and the (public) social network among users, as defined by their friendship network. While the desired set of label types may depend on the application, here we focus on five label types: hometown, high school, college, current city, and\nemployer.\nOur solution exploits three properties of these label types:\n(P1) They represent the primary situations where two people can meet and become friends, for example, because they went to the same high school or college.\n(P2) These situations are (mostly) mutually exclusive. While there may be occasional friendships sharing, say, hometown and high-school, we make the simplifying assumption that most edges can be explained by only one label type.\n(P3) Sharing the same label is a necessary but not sufficient condition. For example, “We are friends from Chicago” typically implies that the indicated individuals were, at some point in time, co-located in a small area within Chicago (say, lived in the same building, met in the same cafe), but hardly implies that two randomly chosen individuals from Chicago are likely to be friends.\n(P1) is a direct result of our application; our desired label types were targeted at friendship formation. Combined with (P2), our five label types can be considered a set of mutually exclusive and exhaustive “reasons” for friendship; while this is not strictly true for high school and hometown, empirical evidence suggests that it is a good approximation (shown later in Section 6) and we defer a discussion on this point to Section 5. However, as (P3) shows, we cannot simply cast the labels as features whose mere presence or absence significantly affects the probability of friendship; instead, a more careful analysis is called for.\nFormally, we are given a graph, G = (V,E) and a set of label types T = {t1, . . . , tk}. For each label type t, let L(t) denote the (high-dimensional) set of labels for that label type. Each node in the graph is associated with binary variables Sutℓ, where Sutℓ = 1 if node u ∈ V has label ℓ for label type t. Let SV and SH represent the sets of visible and hidden variables, respectively. We want to infer the correct values of SH , leveraging SV and G.\nA popular method for label inference is label propagation (Zhu & Ghahramani, 2002; Zhu et al., 2003). For a single label type, this approach represents the labeling by a set of indicator variables Suℓ, where Suℓ = 1 if node u is labeled as ℓ and 0 otherwise. Zhu et al. (2003) relax the labeling to real-valued variables fuℓ over all nodes u and labels ℓ that are clamped to one (or zero) for nodes known to possess that label (or not). They then define a quadratic energy function that assigns lower energy states to configurations where f at adjacent nodes are similar:\nE(f ) = 1\n2\n∑\nu∼v\nwuv ∑\nℓ\n(fuℓ − fvℓ) 2. (1)\nHere, u ∼ v means that u and v are linked by an edge, and wuv is a non-negative weight on the edge u ∼ v. The minimum of Eq. 1 is found by solving the fixed point equations\nfuℓ = 1\ndu\n∑\nu∼v\nwuvfvℓ, (2)\nwhere du = ∑\nu∼v wuv . This procedure encourages fuℓ of nodes connected to clamped nodes to be close to the clamped value and propagates the labels outwards to the rest of the graph. Multiple label types can be handled similarly by minimizing Eq. 1 independently for each type.\nWhile this formulation makes full use of (P1) and has the advantage of simplicity, it completely ignores (P2). Intuitively, label propagation assumes that friends tend to be similar in all respects (i.e., all label types), whereas what (P2) suggests is that each friendship tends to have a single reason: an edge u ∼ v exists because u and v share the same high school or college or current city, etc. This highly non-linear function is not easily expressed as a quadratic or similar variant.\nInstead, we propose a different probabilistic model, which we call EDGEEXPLAIN. As described above, let SV and SH represent the sets of visible and hidden variables respectively; the variable Sutℓ is known (visible) if user u has publicly declared the label ℓ for type t, and unknown (hidden) otherwise. Then, EDGEEXPLAIN is defined as follows:\nP (SV ,SH) = 1\nZ\n∏\nu∼v\nsoftmax t∈T (r(u, v, t)) (3)\nr(u, v, t) = ∑\nℓ∈L(t)\nSutℓSvtℓ (4)\nsoftmax t∈T (r(u, v, t)) = σ\n(\nα ∑\nt∈T\nr(u, v, t) + c\n)\n, (5)\nwhere Z is a normalization constant. Here, r(u, v, t) indicates whether a shared label type t is the reason underlying the edge u ∼ v (Eq. 4). The softmax(r1, . . . , r|T |) function should have three properties: (a) it should be monotonically non-decreasing in each argument, (b) it should achieve a value close to its maximum as long as any one of its parameters is “high”, and also (c) it should be differentiable, for ease of analysis. In Eq. 5, we use the sigmoid function to implement this: σ(x) = 1/(1 + e−x). This monotonically increases from 0 to 1, and achieves values greater than 1− ǫ once x is greater than an ǫ-dependent threshold. In addition, the sigmoid enables fine control of the degree of “explanation” required for each edge (discussed below) and allows for easy extensions to more complex label types and extra features (Section 5), all of which make it our preferred choice for the softmax.\nIn a nutshell, our modeling assumption can be stated as follows: It is better to explain as many friendships as possible,\nrather than to explain a few friendships really well. Eq. 3 is maximized if the softmax function achieves a high value for each edge u ∼ v, i.e., if each edge is “explained”. This is achieved if the sum ∑\nt∈T r(u, v, t) is more than the required threshold, which in turn is satisfied if the product SutℓSvtℓ is 1 for even one label ℓ — in other words, when there exists any label ℓ that both u and v share. The parameter α controls the degree of explanation needed for each edge; a small α forces the learning algorithm to be very sure that u and v share one or more label types, while with a large α, a single matching label type is enough. Empirical results shown later in Section 6 prove that largeα values perform better, suggesting that even a single matching label type is enough to explain the edge. The parameter c in Eq. 5 can be thought of as the probability of matching on an unknown label type, distinct from the five we consider. Higher values of c can be used to model uncertainty that the available label types form an exhaustive set of reasons for friendships. All our experiments use c = 0, and reflect our belief in property (P1); the accuracy of predictions under this setting (shown later in Section 6) suggests that (P1) indeed holds true.\nFurther intuition can be gained by considering a node u whose labels are completely unknown, but whose friends’ labels are completely known (see Figure 1). As we discussed earlier in Section 1, label propagation would infer the hometown of u to be the most common hometown among her friends (i.e., H), the current city to be the most common current city among friends (i.e., C′), and so on. However, such an inference leaves u’s friendships from C completely unexplained. Our proposed method rectifies this; Eq. 3 will be maximized by correctly inferring H and C as u’s hometown and current city respectively, since H is enough to explain all friendships with the hometown friends, and the marginal extra benefit obtained from explaining these same friendships a little better by using C′ as u’s current city is outweighed by the significant benefits obtained from explaining all the friendships from C by setting u’s current city to be C.\nTo summarize, Eq. 4 encapsulates property (P1) by trying to have matching labels between friends; Eq. 5 models property (P2) by enabling any one label type to explain each friendship; and the form of the probability distribution (Eq. 3) uses only existing edges u ∼ v and not all node pairs, and thus is not affected when, say, two nodes with Chicago as their current city are not friends, which reflects the idea that matching label types are necessary but not sufficient (P3)."
    }, {
      "heading" : "4. Inference",
      "text" : "The probabilistic description of EDGEEXPLAIN in Eqs. 3-5 can be restated as an optimization problem in the variables\nSutℓ ∈ {0, 1}. In the spirit of (Zhu et al., 2003), we propose a relaxation in terms of a real-valued function f , with futℓ ∈ [0, 1] representing the probability that Sutℓ = 1, i.e., the probability that user u has label ℓ for label type t. This yields the following optimization:\nMaximize ∑\nu∼v\nlog (\nsoftmax t∈T\n(r(u, v, t)) ) (6)\nwhere r(u, v, t) = ∑\nℓ∈L(t)\nfutℓfvtℓ (7)\n∑\nℓ∈L(t)\nfutℓ = 1 ∀t ∈ T (8)\nfutℓ ≥ 0 (9)\nwhere softmax(.) is defined as in Eq. 5, and the equation for r(.) is analogous to Eq. 4 but measures the total probability that u and v have the same label for a given label type t.\nThe problem is not convex in f , but is convex in fu = {futℓ|t ∈ T , ℓ ∈ L(t)} if the distributions fv are held fixed for all nodes v 6= u. Hence, we propose an iterative algorithm to infer f . Given fv for all v 6= u, finding the optimal fu corresponds to solving the following problem:\nMaximize g(fu) = ∑\nv∈Γ(u)\nlog (\nsoftmax t∈T\n(r(u, v, t)) ) ,\nwhere the summation is only over the set Γ(u) of the friends of u, and we again restrict fu to be a set of |T | probability distributions, one for each label type. We note that g(.) is convex and Lipschitz continuous with constant L = α · |Γ(u)|, where |Γ(u)| is the number of friends of u.\nThis is a constrained maximization problem with no closed form solution for fu. To solve it, we use proximal gradient ascent, which is an iterative method where in each step, we take a step in the direction of the gradient, and then project it back to the probability simplex ∆ = { futℓ | futℓ ≥ 0, ∑ ℓ∈L(t) futℓ = 1∀t ∈ T } . Specifically, let ∇g represent the gradient of g, with components given by:\n∂g(fu)\n∂futℓ =\n∑\nv∈Γ(u)\nαfvtℓ · σ\n(\n−α ∑\nt∈T\n∑\nℓ∈L(t)\nfutℓfvtℓ − c\n)\n.\nLet f (k−1)u = { f (k−1) utℓ |t ∈ T , ℓ ∈ L(t) }\nbe the estimated probability distributions for each of the T label types at the end of iteration k − 1, and let q(k)\nutℓ represent the (possibly\nimproper) ending point of the k-th gradient step:\nq(k)u = f (k−1) u + ck∇g,\nwhere ck is a step-size parameter that we could set to a constant ck = 1/L. The point q (k) u is now projected to the\nclosest point in ∆:\nf (k)u = argmin q′∈∆ ‖q(k)u − q ′‖2.\nThis can be easily achieved in expected linear time over the size of the label set ∑\nt L(t) (Duchi et al., 2008). If only\nsparse distributions can be stored for each label type (say, only the top k labels for each type), the optimal k-sparse projections can be obtained simply by setting to 0 all but the top k labels for each label type, and then projecting on to the simplex (Kyrillidis et al., 2013).\nThis algorithm converges to a fixed point, and the function values converge to the optimal at a 1/k rate (Beck & Teboulle, 2009):\ng∗ − g(k) ≤ L‖f (0)u − f ∗ u‖ 2\n2k ≤\nL|T |\nk ,\nwhere f∗u represents the optimal set of probability distributions, and g∗ is the optimal function value. An important consequence of the algorithm is that computation of fu only requires information from fv for the neighbors v of u. Thus, it is a “local” algorithm that can be easily implemented in distributed message-passing architectures, such as Giraph (Giraph; Ching, 2013)."
    }, {
      "heading" : "5. Generalizations",
      "text" : "We now discuss some aspects of EDGEEXPLAIN and some generalizations that demonstrate its wide applicability.\nRELATED LABEL TYPES. Property (P2) assumes that the reasons for friendship formation are mutually exclusive, but this need not be strictly true. For example, some high school friends could be a subset of hometown friends1. Let us again consider Figure 1, but with current city replaced by high school. Suppose that the solid-black nodes represent actual high school friends, and we are trying to infer u’s high school. If the small cluster on the right did not exist, then Eq. 3 would be maximized by picking the most common high school among u’s friends (i.e., the solidblack nodes), even if they are already explained by a shared hometown; thus, EDGEEXPLAIN would pick the correct high school. On the other hand, if some friendships would remain unexplained without a shared high school (e.g., the small cluster in Figure 1), then it is not obvious whether we should prefer a high school that explains these edges or a high school that represents a large segment of hometown friends. The parameter α modulates this trade-off, with a higher value of α emphasizing the explanation of all edges\n1The relationship between high school and hometown is in fact more complicated. The high school could be within driving distance of the hometown, but not in it; and sometimes even this does not hold.\nas against the explanation of several edges a little better. The choice of α must depend on the characteristics of the social network; for the Facebook network, the best empirical results are achieved for large α (shown later in Section 6), suggesting that many of our label types are indeed mutually exclusive.\nINCORPORATING USER FEATURES. EDGEEXPLAIN easily generalizes to broader settings with multiple user features, such as group memberships, topics of interest, keywords, or pages liked by the user. As an example, consider group memberships of users. Intuitively, if most members of a group come from the same college, then it is likely a college-friends group, and this can aid inference for group members whose college is unknown. This can be easily handled by creating a special node for each group, and creating “friendship” edges between the group node and its members. EDGEEXPLAIN will infer labels for the group node as well, and will explain its “friendships” via the college label. This, in turn, will influence college inference for group members with unknown college labels. The importance of such group membership features can also be tuned, as described next.\nINCORPORATING EDGE FEATURES. There are several situations where edge-specific features could be useful. First, we may want to give more importance to certain kinds of edges, such as the group-membership edges mentioned above. Second, some features could be important for one label type but not another: e.g., the age difference between friends could be useful for inferring high school but not employer. All these situations can be easily handled by modifying Eq. 4 to include an edge-specific and label typespecific weight. The corresponding modifications to the inference method are trivial."
    }, {
      "heading" : "6. Experiments",
      "text" : "Previously, we provided intuition and examples supporting the claim that EDGEEXPLAIN is better suited to inference of our desired label types than vanilla label propagation. In this section, we demonstrate this via empirical evidence on a billion-node graph.\nDATA. We ran experiments on a large subgraph of the Facebook social network, consisting of over 1.1 billion users and their friendship edges. From the public profile of each user, we extract the hometown, current city, high school, college, and employer, whenever these are available. The dimensionality of our five label types range from almost 900K to over 6.3M . We describe below in IMPLEMENTATION DETAILS our process for generating the edges. This forms our base dataset.\nEXPERIMENTAL METHODOLOGY. The set of users is ran-\ndomly split into five parts and experimental accuracy is measured via 5-fold cross-validation, with the known profile information from four folds being used to predict labels for all types for users in the fifth fold. Results over the various folds are identical to three decimal places. All differences are therefore significant and we do not show variances as they are too small to be noticeable.\nIn each experiment, we run inference on the training set and compute a ranking of labels for each label type for each user. This ranking is provided by f computed for label propagation (Eq. 1) and EDGEEXPLAIN (Eqs. 6-9) respectively. We then measure recall at the top-1 and top-3 positions, i.e., we measure the fraction of (user, label type) pairs in the test set where the predicted top-ranked label (or any of the top-3 labels) match the actual user-provided label. For reasons of confidentiality, we only present the lift in recall values of EDGEEXPLAIN as compared to label propagation.\nIMPLEMENTATION DETAILS. We implemented EDGEEXPLAIN in Giraph (Giraph; Ching, 2013) which is an iterative graph processing system based on the Bulk Synchronous Processing model (Malewicz et al., 2010; Valiant, 1990). The entire set of nodes is split among 200 machines, and in each iteration, every node u sends the probability distributions fu to all friends of u. To limit the communication overhead, we implemented two features: (a) for each user u and label type t, the multinomial distribution fut. was clipped to retain only the top 8 entries optimally (Kyrillidis et al., 2013), and (b) the friendship graph is sparsified so as to retain, for each user u, the top K friends whose ages are closest to that of u. This choice of friends is guided by the intuition that friends of similar age are most likely to share certain label types such as high school and college. We find that clipping the distributions makes little difference to accuracy while significantly improving running time. However, the value of K matters significantly, and we detail these effects next.\nRECALL OF EDGEEXPLAIN. Figure 2 shows recall as a function of varying number of friends K , against a baseline of EDGEEXPLAIN with K = 20. We observe that recall increases up to a certain K and then decreases — K = 100 for recall at 1, and K = 200 for recall at 3. This demonstrates both the importance and the limits of scalability: increasing the number of friends enables better inference but beyond a point, more friends increase noise. Thus, K = 100 friends appear to be enough for inference under EDGEEXPLAIN.\nFigure 2 also shows an increasing trend from hometown to employer in the degree of improvement obtained over the K = 20 baseline. This is because (a) the baseline itself is best for hometown and worst for employer, but also be-\ncause (b) Facebook users appear to have many more friends from label types other than from their current employer. The effect of this latter observation is that if we only have a small K , it is very likely that the few friends from the same current employer are not included in that limited set of friends (which we empirically verified). As K increases and such same-employer edges become available, EDGEEXPLAIN can easily learn the reason for these edges (hence the dramatic increase in recall), but label propagation will likely be confused by the overall distribution of different employers among all friends and therefore does not benefit equally from adding more friends, as we show next.\nCOMPARISON WITH LABEL PROPAGATION. Figure 3 shows the lift in recall achieved by EDGEEXPLAIN over Label Propagation as we increase K for both. We observe similar performance of both methods for hometown and current city, but increasing improvements for high school, college, and employer. This again points to the first two being easier to infer, with the difficulty of inference increase with the latter label types. With fewer employer-based friendships, the prototypical example of Figure 1 would also occur frequently, with Label Propagation likely picking common employers of (say) hometown friends instead of the less common friendships based on the actual employer. By attempting to explain each friendship, EDGEEXPLAIN is able to infer the employer even under such difficult circumstances, and the ability to perform well even for under-represented label types makes EDGEEXPLAIN particularly attractive.\nINCLUSION OF EXTRA FEATURES. In Section 5, we discussed how extra features could be used within the EDGEEXPLAIN framework. In particular, we showed how the fact that some users are members of groups can be used to infer (say) their college, if the group turns out to be collegespecific group. Group memberships are extensive and pro-\nvide information that is orthogonal to friendships; thus, a priori, one would expect the addition of group membership features to have significant impact on label inference.\nTable 1 shows the lift in recall for EDGEEXPLAIN when group memberships are used in addition to K = 100 friends. While the addition of group memberships increases the size of the graph by ≈ 25%, the observed benefits for recall are minor: a maximum lift of only 1.2% for employer inference, and indeed reduced recall at 1 for several label types. Note that the lift in recall would have appeared very significant had we compared it to Label Propagation with K = 100; however, this gain largely disappears when the friendships are considered in the framework of EDGEEXPLAIN. Thus, it is not merely the scalability of EDGEEXPLAIN, but also the careful modeling of properties (P1)-(P3) that makes group membership redundant.\nGiven the a priori expectations of the impact of group memberships, this surprising result suggests that information regarding our label types are already encoded in the structure of the social network and hence the orthogonal information from the group memberships actually turn out to be redundant.\nTHE LIMITS OF RESOLUTION. Our model theoretically should be able to handle any number of label types, but empirically this may not hold true for our network. How many friends sharing a certain label type (say, the same college) does a user need to have in order to correctly infer the value of that label type? To answer this, we select, for each user, the set of friends whose label for the given label type t is known, and we compute the fraction that actually shared the user’s label for t. Figure 4 shows the probability that EDGEEXPLAIN correctly infers the user’s label as a function of this fraction (i.e., the correct label is among the top 3 predictions). All label types are similar, though high school is somewhat easier and employer harder; having 10 − 15% of friends sharing a user’s label is sufficient to infer the label in our graph. Note that certain label types are more likely to be publicly declared than others, and this explains differences in recall observed earlier.\nEFFECT OF α. Figure 5 shows that the lift in recall at 1 for various values of the parameter α, with respect to α = 0.1. Performance generally improves with increasing α. Results for recall at 3 are qualitatively similar, though the effect is more muted. We find that α ∈ [10, 40] offer the best results, and EDGEEXPLAIN is robust to the specific choice\nof α within this range. Recall that with large α, a single matching label is enough to explain an edge, while with small α, multiple matching labels may be needed. Thus, the outperformance of large α provides strong empirical validation of property (P2) (on our network).\nRUNNING TIME. Figure 6 shows the wall-clock time as a function of K . The running time should depend linearly on the graph size, which grows almost linearly with K; as expected, the plot is linear, with deviations due to garbage collection stoppages in Java."
    }, {
      "heading" : "7. Conclusions",
      "text" : "We proposed the problem of jointly inferring multiple correlated label types in a large network and described the problems with existing single-label models. We noted that one particular failure mode of existing methods in our problem setting is that edges are often created for a reason associated with a particular label type (e.g., in a social network, two users may link because they went to the same high school, but they did not go to the same college). We identified three network properties that model this phenomenon: edges are created for a reason (P1), they are generally created only for one reason (P2), and sharing the same value for a label type is necessary but not sufficient for having an edge between two nodes (P3).\nWe introduced EDGEEXPLAIN, which carefully models these properties. It leverages a gradient-based method for collective inference which allows for fast iterative inference that is equivalent in running time to basic label propagation. Our experiments with a billion-node subset of the Facebook graph amply demonstrate the benefits of EDGEEXPLAIN, with significant improvements across a set of different label types. Our further analysis validates many of the properties and intuitions we had about modeling networks, primarily that one can achieve significant improvements if one considers and models the reason an edge exists. Whether one is interested in inferring one or multiple label types, modeling these explanations will have significant impact on the accuracy of the final predictions."
    } ],
    "references" : [ {
      "title" : "Video suggestion and discovery for YouTube: Taking random walks through the view graph",
      "author" : [ "S. Baluja", "R. Seth", "D. Sivakumar", "Y. Jing", "J. Yagnik", "S. Kumar", "D. Ravichandran", "M. Aly" ],
      "venue" : "In WWW,",
      "citeRegEx" : "Baluja et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Baluja et al\\.",
      "year" : 2008
    }, {
      "title" : "Gradient-based algorithms with applications to signal recovery",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : null,
      "citeRegEx" : "Beck and Teboulle,? \\Q2009\\E",
      "shortCiteRegEx" : "Beck and Teboulle",
      "year" : 2009
    }, {
      "title" : "Tikhonov regularization and semi-supervised learning on large graphs",
      "author" : [ "M. Belkin", "I. Matveeva", "P. Niyogi" ],
      "venue" : "In COLT, pp",
      "citeRegEx" : "Belkin et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Belkin et al\\.",
      "year" : 2004
    }, {
      "title" : "On manifold regularization",
      "author" : [ "M. Belkin", "P. Niyogi", "V. Sindhwani" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Belkin et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Belkin et al\\.",
      "year" : 2005
    }, {
      "title" : "Enhanced hypertext categorization using hyperlinks",
      "author" : [ "S. Chakrabarti", "B. Dom", "P. Indyk" ],
      "venue" : "In SIGMOD,",
      "citeRegEx" : "Chakrabarti et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Chakrabarti et al\\.",
      "year" : 1998
    }, {
      "title" : "Hierarchical relational models for document networks",
      "author" : [ "J. Chang", "D. Blei" ],
      "venue" : "The Annals of Applied Statistics,",
      "citeRegEx" : "Chang and Blei,? \\Q2010\\E",
      "shortCiteRegEx" : "Chang and Blei",
      "year" : 2010
    }, {
      "title" : "Scaling Apache Giraph to a trillion edges",
      "author" : [ "A. Ching" ],
      "venue" : "Facebook Engineering blog,",
      "citeRegEx" : "Ching,? \\Q2013\\E",
      "shortCiteRegEx" : "Ching",
      "year" : 2013
    }, {
      "title" : "Efcient projections onto the l1-ball for learning in high dimensions",
      "author" : [ "J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning Probabilistic Relational Models",
      "author" : [ "N. Friedman", "L. Getoor", "D. Koller", "A. Pfeffer" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Friedman et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 1999
    }, {
      "title" : "Document hierarchies from text and links",
      "author" : [ "Q. Ho", "J. Eisenstein", "E. Xing" ],
      "venue" : "In WWW, pp",
      "citeRegEx" : "Ho et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ho et al\\.",
      "year" : 2012
    }, {
      "title" : "Latent space approaches to social network analysis",
      "author" : [ "P. Hoff", "A. Raftery", "M. Handcock" ],
      "venue" : null,
      "citeRegEx" : "Hoff et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Hoff et al\\.",
      "year" : 2002
    }, {
      "title" : "On the foundations of relaxation labeling processes",
      "author" : [ "R. Hummel", "S. Zucker" ],
      "venue" : "IEEE PAMI,",
      "citeRegEx" : "Hummel and Zucker,? \\Q1983\\E",
      "shortCiteRegEx" : "Hummel and Zucker",
      "year" : 1983
    }, {
      "title" : "Probabilistic frame-based systems",
      "author" : [ "D. Koller", "A. Pfeffer" ],
      "venue" : "In AAAI, pp",
      "citeRegEx" : "Koller and Pfeffer,? \\Q1998\\E",
      "shortCiteRegEx" : "Koller and Pfeffer",
      "year" : 1998
    }, {
      "title" : "Sparse projections onto the simplex",
      "author" : [ "A. Kyrillidis", "S. Becker", "V. Cevher", "C. Koch" ],
      "venue" : null,
      "citeRegEx" : "Kyrillidis et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kyrillidis et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving learning in networked data by combining explicit and mined links",
      "author" : [ "S.A. Macskassy" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Macskassy,? \\Q2007\\E",
      "shortCiteRegEx" : "Macskassy",
      "year" : 2007
    }, {
      "title" : "Classification in networked data: A toolkit and a univariate case study",
      "author" : [ "S.A. Macskassy", "F. Provost" ],
      "venue" : "JMLR, 8:935–983,",
      "citeRegEx" : "Macskassy and Provost,? \\Q2007\\E",
      "shortCiteRegEx" : "Macskassy and Provost",
      "year" : 2007
    }, {
      "title" : "Pregel: A system for large-scale graph processing",
      "author" : [ "G. Malewicz", "M.H. Austern", "A.J.C. Bik", "I. Horn", "N. Leiser", "G. Czajkowski" ],
      "venue" : "In SIGMOD,",
      "citeRegEx" : "Malewicz et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Malewicz et al\\.",
      "year" : 2010
    }, {
      "title" : "Birds of a Feather: Homophily in Social Networks",
      "author" : [ "M. McPherson", "L. Smith-Lovin", "J.M. Cook" ],
      "venue" : "Annual Review of Sociology,",
      "citeRegEx" : "McPherson et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "McPherson et al\\.",
      "year" : 2001
    }, {
      "title" : "Nonparametric latent feature models for link prediction",
      "author" : [ "K. Miller", "T. Griffiths", "M. Jordan" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Miller et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2009
    }, {
      "title" : "Joint latent topic models for text and citations",
      "author" : [ "R. Nallapati", "A. Ahmed", "E. Xing", "W. Cohen" ],
      "venue" : "In KDD, pp",
      "citeRegEx" : "Nallapati et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2008
    }, {
      "title" : "An infinite latent attribute model for network data",
      "author" : [ "K. Palla", "D. Knowles", "Z. Ghahramani" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Palla et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Palla et al\\.",
      "year" : 2012
    }, {
      "title" : "Aggregation-based feature invention and relational concept classes",
      "author" : [ "C. Perlich", "F. Provost" ],
      "venue" : "In KDD, pp",
      "citeRegEx" : "Perlich and Provost,? \\Q2003\\E",
      "shortCiteRegEx" : "Perlich and Provost",
      "year" : 2003
    }, {
      "title" : "Scene labeling by relaxation",
      "author" : [ "A. Rosenfeld", "R. Hummel" ],
      "venue" : "operations. KDD,",
      "citeRegEx" : "Rosenfeld and Hummel,? \\Q1976\\E",
      "shortCiteRegEx" : "Rosenfeld and Hummel",
      "year" : 1976
    }, {
      "title" : "Scaling graph-based semi supervised learning to large number of labels using countmin sketch",
      "author" : [ "P. Talukdar", "W. Cohen" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Talukdar and Cohen,? \\Q2014\\E",
      "shortCiteRegEx" : "Talukdar and Cohen",
      "year" : 2014
    }, {
      "title" : "New regularized algorithms for transductive learning",
      "author" : [ "P. Talukdar", "K. Crammer" ],
      "venue" : "In ECML,",
      "citeRegEx" : "Talukdar and Crammer,? \\Q2009\\E",
      "shortCiteRegEx" : "Talukdar and Crammer",
      "year" : 2009
    }, {
      "title" : "Discriminative Probabilistic Models for Relational Data",
      "author" : [ "B. Taskar", "P. Abbeel", "D. Koller" ],
      "venue" : "In UAI, pp",
      "citeRegEx" : "Taskar et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Taskar et al\\.",
      "year" : 2002
    }, {
      "title" : "A bridging model for parallel computation",
      "author" : [ "L.G. Valiant" ],
      "venue" : null,
      "citeRegEx" : "Valiant,? \\Q1990\\E",
      "shortCiteRegEx" : "Valiant",
      "year" : 1990
    }, {
      "title" : "Learning with local and global consistency",
      "author" : [ "D. Zhou", "O. Bousquet", "T. Lal", "J. Weston", "B. Schölkopf" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2004
    }, {
      "title" : "Learning from labeled and unlabeled data with label propagation",
      "author" : [ "X. Zhu", "Z. Ghahramani" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Zhu and Ghahramani,? \\Q2002\\E",
      "shortCiteRegEx" : "Zhu and Ghahramani",
      "year" : 2002
    }, {
      "title" : "Semi-supervised learning using Gaussian fields and harmonic functions",
      "author" : [ "X. Zhu", "Z. Ghahramani", "J. Lafferty" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "One standard method of label inference is label propagation (Zhu & Ghahramani, 2002; Zhu et al., 2003), which tries to set the label probabilities of nodes so that friends have similar probabilities.",
      "startOffset" : 60,
      "endOffset" : 102
    }, {
      "referenceID" : 17,
      "context" : "captures the essence of homophily (the more two nodes have in common, the more likely they are to connect (McPherson et al., 2001)), it optimizes for only a single type of label and assumes only a single category of relationships.",
      "startOffset" : 106,
      "endOffset" : 130
    }, {
      "referenceID" : 29,
      "context" : "Label propagation (Zhu et al., 2003; Zhu & Ghahramani, 2002) uses a quadratic function, but other penalties are also possible (Zhou et al.",
      "startOffset" : 18,
      "endOffset" : 60
    }, {
      "referenceID" : 27,
      "context" : ", 2003; Zhu & Ghahramani, 2002) uses a quadratic function, but other penalties are also possible (Zhou et al., 2004; Belkin et al., 2004; 2005).",
      "startOffset" : 97,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : ", 2003; Zhu & Ghahramani, 2002) uses a quadratic function, but other penalties are also possible (Zhou et al., 2004; Belkin et al., 2004; 2005).",
      "startOffset" : 97,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "Other approaches modify the random walk interpretation of label propagation (Baluja et al., 2008; Talukdar & Crammer, 2009).",
      "startOffset" : 76,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "and (c) a collective inference procedure that propagates the information through the network (Chakrabarti et al., 1998; Perlich & Provost, 2003; Lu & Getoor, 2003; Macskassy & Provost, 2007).",
      "startOffset" : 93,
      "endOffset" : 190
    }, {
      "referenceID" : 14,
      "context" : "While there has been some work focusing on understanding how to combine and weigh different edge types for best prediction performance (Macskassy, 2007), the edge types (analogous to our reason for an edge) were given up front.",
      "startOffset" : 135,
      "endOffset" : 152
    }, {
      "referenceID" : 8,
      "context" : "There is also extensive work on probabilistic relational models, including Relational Bayesian Networks (Koller & Pfeffer, 1998; Friedman et al., 1999), Relational Dependency Networks (Neville & Jensen, 2007), and Relational Markov Networks (Taskar et al.",
      "startOffset" : 104,
      "endOffset" : 151
    }, {
      "referenceID" : 25,
      "context" : ", 1999), Relational Dependency Networks (Neville & Jensen, 2007), and Relational Markov Networks (Taskar et al., 2002).",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "and (c) a collective inference procedure that propagates the information through the network (Chakrabarti et al., 1998; Perlich & Provost, 2003; Lu & Getoor, 2003; Macskassy & Provost, 2007). Macskassy et al. (2007) observe that the best algorithms (weighted-vote relational neighbor classifier (Macskassy & Provost, 2007) with relaxation labeling (Rosenfeld & Hummel, 1976; Hummel & Zucker, 1983)) tend to perform as well as label propagation, which we outperform.",
      "startOffset" : 94,
      "endOffset" : 216
    }, {
      "referenceID" : 10,
      "context" : "Graph structure has been modeled using latent variables (Hoff et al., 2002; Miller et al., 2009; Palla et al., 2012), but with an emphasis on link prediction.",
      "startOffset" : 56,
      "endOffset" : 116
    }, {
      "referenceID" : 18,
      "context" : "Graph structure has been modeled using latent variables (Hoff et al., 2002; Miller et al., 2009; Palla et al., 2012), but with an emphasis on link prediction.",
      "startOffset" : 56,
      "endOffset" : 116
    }, {
      "referenceID" : 20,
      "context" : "Graph structure has been modeled using latent variables (Hoff et al., 2002; Miller et al., 2009; Palla et al., 2012), but with an emphasis on link prediction.",
      "startOffset" : 56,
      "endOffset" : 116
    }, {
      "referenceID" : 19,
      "context" : "Other models simultaneously explain the connections between documents as well as their word distributions (Nallapati et al., 2008; Chang & Blei, 2010; Ho et al., 2012).",
      "startOffset" : 106,
      "endOffset" : 167
    }, {
      "referenceID" : 9,
      "context" : "Other models simultaneously explain the connections between documents as well as their word distributions (Nallapati et al., 2008; Chang & Blei, 2010; Ho et al., 2012).",
      "startOffset" : 106,
      "endOffset" : 167
    }, {
      "referenceID" : 29,
      "context" : "A popular method for label inference is label propagation (Zhu & Ghahramani, 2002; Zhu et al., 2003).",
      "startOffset" : 58,
      "endOffset" : 100
    }, {
      "referenceID" : 29,
      "context" : "A popular method for label inference is label propagation (Zhu & Ghahramani, 2002; Zhu et al., 2003). For a single label type, this approach represents the labeling by a set of indicator variables Sul, where Sul = 1 if node u is labeled as l and 0 otherwise. Zhu et al. (2003) relax the labeling to real-valued variables ful over all nodes u and labels l that are clamped to one (or zero) for nodes known to possess that label (or not).",
      "startOffset" : 83,
      "endOffset" : 277
    }, {
      "referenceID" : 29,
      "context" : "In the spirit of (Zhu et al., 2003), we propose a relaxation in terms of a real-valued function f , with futl ∈ [0, 1] representing the probability that Sutl = 1, i.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "t L(t) (Duchi et al., 2008).",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "If only sparse distributions can be stored for each label type (say, only the top k labels for each type), the optimal k-sparse projections can be obtained simply by setting to 0 all but the top k labels for each label type, and then projecting on to the simplex (Kyrillidis et al., 2013).",
      "startOffset" : 263,
      "endOffset" : 288
    }, {
      "referenceID" : 6,
      "context" : "Thus, it is a “local” algorithm that can be easily implemented in distributed message-passing architectures, such as Giraph (Giraph; Ching, 2013).",
      "startOffset" : 124,
      "endOffset" : 145
    }, {
      "referenceID" : 6,
      "context" : "We implemented EDGEEXPLAIN in Giraph (Giraph; Ching, 2013) which is an iterative graph processing system based on the Bulk Synchronous Processing model (Malewicz et al.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : "We implemented EDGEEXPLAIN in Giraph (Giraph; Ching, 2013) which is an iterative graph processing system based on the Bulk Synchronous Processing model (Malewicz et al., 2010; Valiant, 1990).",
      "startOffset" : 152,
      "endOffset" : 190
    }, {
      "referenceID" : 26,
      "context" : "We implemented EDGEEXPLAIN in Giraph (Giraph; Ching, 2013) which is an iterative graph processing system based on the Bulk Synchronous Processing model (Malewicz et al., 2010; Valiant, 1990).",
      "startOffset" : 152,
      "endOffset" : 190
    }, {
      "referenceID" : 13,
      "context" : "was clipped to retain only the top 8 entries optimally (Kyrillidis et al., 2013), and (b) the friendship graph is sparsified so as to retain, for each user u, the top K friends whose ages are closest to that of u.",
      "startOffset" : 55,
      "endOffset" : 80
    } ],
    "year" : 2014,
    "abstractText" : "We tackle the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Our primary example, and the focus of this paper, is the joint inference of label types such as hometown, current city, and employers, for users connected by a social network. Standard label propagation fails to consider the properties of the label types and the interactions between them. Our proposed method, called EDGEEXPLAIN, explicitly models these, while still enabling scalable inference under a distributed message-passing architecture. On a billion-node subset of the Facebook social network, EDGEEXPLAIN significantly outperforms label propagation for several label types, with lifts of up to 120% for recall@1 and 60% for recall@3.",
    "creator" : "LaTeX with hyperref package"
  }
}