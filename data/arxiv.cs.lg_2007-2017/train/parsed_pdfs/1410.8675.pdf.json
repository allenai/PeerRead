{
  "name" : "1410.8675.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Partition-wise Linear Models",
    "authors" : [ "Hidekazu Oiwa", "Ryohei Fujimaki" ],
    "emails" : [ "hidekazu.oiwa@gmail.com", "rfujimaki@nec-labs.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Region-specific linear models are widely used in practical applications because of their non-linear but highly interpretable model representations. One of the key challenges in their use is non-convexity in simultaneous optimization of regions and region-specific models. This paper proposes novel convex region-specific linear models, which we refer to as partition-wise linear models. Our key ideas are 1) assigning linear models not to regions but to partitions (region-specifiers) and representing region-specific linear models by linear combinations of partitionspecific models, and 2) optimizing regions via partition selection from a large number of given partition candidates by means of convex structured regularizations. In addition to providing initialization-free globally-optimal solutions, our convex formulation makes it possible to derive a generalization bound and to use such advanced optimization techniques as proximal methods and decomposition of the proximal maps for sparsity-inducing regularizations. Experimental results demonstrate that our partition-wise linear models perform better than or are at least competitive with state-of-the-art region-specific or locally linear models."
    }, {
      "heading" : "1 Introduction",
      "text" : "Among pre-processing methods, data partitioning is one of the most fundamental. In it, an input space is divided into several sub-spaces (regions) and assigned a simple model for each region. In addition to better predictive performance resulting from the non-linear nature that arises from multiple partitions, the regional structure also provides a better understanding of data (i.e., better interpretability). Region-specific linear models learn both partitioning structures and predictors in each region.\nSuch models vary—from traditional decision/regression trees [1] to more advanced models [2, 3, 4]—depending on their region-specifiers (how they characterize regions), region-specific prediction models, and the objective functions to be optimized. One important challenge that remains in learning these models is the non-convexity that arises from the inter-dependency of optimizing regions and prediction models in individual regions. Most previous work suffers from disadvantages arising from non-convexity, including initialization-dependency (bad local minima) and lack of generalization error analysis.\nThis paper proposes convex region-specific linear models. We refer to them as partition-wise linear models. Our models have two distinguishing characteristics that help avoid the non-convexity problem.\n∗The work reported here was conducted when the first author was a visiting researcher at NEC Laboratories America.\nar X\niv :1\n41 0.\n86 75\nv1 [\nst at\n.M L\n] 3\n1 O\nct 2\n01 4\nPartition-wise Modeling We propose partition-wise linear models as a novel class of regionspecific linear models. Our models divide an input space by means of a small set of partitions1. Each partition possesses one weight vector, and this weight vector is only applied to one side of the divided space. It is trained to represent the local relationship between input vectors and output values. Region-specific predictors are constructed by linear combinations of these weight vectors. Our partition-wise parameterization enables us to construct convex objective functions.\nConvex Optimization via Sparse Partition Selection We optimize regions by selecting effective partitions from a large number of given candidates, using convex sparsity-inducing structured regularizations. In other words, we trade continuous region optimization for convexity. We allow partitions to locate only given discrete candidate positions, and are able to derive convex optimization problems. We have developed an efficient algorithm to solve structured-sparse optimization problems, and in it we utilize both a proximal method [5, 6] and the decomposition of proximal maps [7].\nAs a reliable partition-wise linear model, we have developed a global and local residual model that combines one global model and a set of partition-wise linear models. Further, our theoretical analysis gives a generalization bound for this model. Its large number of partition candidates enables us to obtain relatively low empirical error, but it leads to an increase in the risk of over-fitting. Our generalization bound analysis indicates that we can increase the number of partition candidates by less than an exponential order with respect to the number of samples, which is large enough to achieve good predictive performance in practice. Experimental results have demonstrated that our proposed models perform better than or are at least competitive with state-of-the-art region-specific or locally linear models."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Region-specific linear models and locally linear models are the most closely related models to our own. The former category, to which our models belong, assumes one predictor in a specific region and has an advantage in clear model interpretability, while the latter assigns one predictor to every single datum and has an advantage in higher model flexibility. Interpretable models are able to indicate clearly where and how the relationships between inputs and outputs change.\nWell-known precursors to region-specific linear models are decision/regression trees [1], which use rule-based region-specifiers and constant-valued predictors. Another traditional framework is a hierarchical mixture of experts [8], which is a probabilistic tree-based region-specific model framework. Recently, Local Supervised Learning through Space Partitioning (LSL-SP) has been proposed [3]. LSL-SP utilizes a linear-chain of linear region-specifiers as well as region-specific linear predictors. The highly important advantage of LSL-SP is the upper bound of generalization error analysis via the VC dimension. Additionally, a Cost-Sensitive Tree of Classifiers (CSTC) algorithm has also been developed [4]. It utilizes a tree-based linear localizer and linear predictors. This algorithm’s uniqueness among other region-specific linear models is in its taking “feature utilization cost” into account for test time speed-up. Although the developers’ formulation with sparsity-inducing structured regularization is, in a way, related to ours, their model representations and, more importantly, their motivation (test time speed-up) is different from ours.\n1In our paper, a region is a sub-space in an input space. Multiple regions do not intersect each other, and, in their entirety, they cover the whole input space. A partition is an indicator function that divides an input space into two parts.\nFast Local Kernel Support Vector Machines (FaLK-SVMs) represent state-of-the-art locally linear models. FaLK-SVMs produce test-point-specific weight vectors by learning local predictive models from the neighborhoods of individual test points [9]. It aims to reduce prediction time cost by preprocessing for nearest-neighbor calculations and local model sharing, at the cost of initializationindependency. Another advanced locally linear model is that of Locally Linear Support Vector Machines (LLSVMs) [10]. LLSVMs assign linear SVMs to multiple anchor points produced by manifold learning [11, 12] and construct test-point-specific linear predictors according to the weights of anchor points with respect to individual test points. When the manifold learning procedure is initialization-independent, LLSVMs become initial-value-independent because of the convexity of the optimization problem. Similarly, clustered SVMs (CSVMs) [13] assume given data clusters and learn multiple SVMs for individual clusters simultaneously. Although CSVMs are convex and generalization bound analysis has been provided, they cannot optimize regions (clusters).\nJoes et al. have proposed Local Deep Kernel Learning (LDKL) [2], which adopts an intermediate approach with respect to region-specific and locally linear models. LDKL is a tree-based local kernel classifier in which the kernel defines regions and can be seen as performing region-specification. One main difference from common region-specific linear models is that LDKL changes kernel combination weights for individual test points, and therefore the predictors are locally determined in every single region. Its aim is to speed up kernel SVMs’ prediction while maintaining the non-linear ability.\nTable 1 summarizes the above described state-of-the-art models in contrast with ours from a number of significant perspectives. Our proposed model uniquely exhibits three properties: joint optimization of regions and region-specific predictors, initialization-independent optimization, and meaningful generalization bound."
    }, {
      "heading" : "1.2 Notations",
      "text" : "Scalars and vectors are denoted by lower-case x. Matrices are denoted by upper-case X . Training samples and labels are denoted by xn ∈ RD and yn. The basic notations used in this paper are summarized in Table 2."
    }, {
      "heading" : "2 Partition-wise Linear Models",
      "text" : "This section explains partition-wise linear models under the assumption that effective partitioning is already known and fixed. We discuss how to optimize partitions and region-specific linear models in Section 3."
    }, {
      "heading" : "2.1 Framework",
      "text" : "Figure 1 illustrates the concept of partition-wise linear models. Suppose we have P partitions (red dashed lines) which essentially specify 2P regions. Partition-wise linear models are defined as follows. First, we assign a linear weight vector ap to the p-th partition. This partition has an activeness function, fp, which indicates whether the attached weight vector ap is applied to individual data\npoints or not. For example, in Figure 1, we set the weight vector a1 to be applied to the right-hand side of partition p1. In this case, the corresponding activeness function is defined as f1(x) = 1 when x is in the right-hand side of p1. Second, region-specific predictors (squared regions surrounded by partitions in Figure 1) are defined by a linear combination of active partition-wise weight vectors that are also linear models.\nLet us formally define the partition-wise linear models. We have a set of given activeness functions, f1, . . . , fP , which is denoted in a vector form as f(·) = (f1(·), . . . , fP (·))T . The p-th element fp(x) ∈ {0, 1} indicates whether the attached weight vector ap is applied to x or not. The activeness function f(·) can represent at most 2P regions, and f(x) specifies to which region x belongs. A linear model of an individual region is then represented as ∑P p=1 fp(·)ap. It is worth noting that partition-wise linear models use P linear weight vectors to represent 2P regions and restrict the number of parameters.\nThe overall predictor g(·) can be denoted as follows: g(x) = ∑ p fp(x) ∑ d adpxd. (1)\nLet us define A as A = (a1, . . . , aP ). The partition-wise linear model (1) simply acts as a linear model w.r.t. A while it captures the non-linear nature of data (individual regions use different linear models). Such non-linearity originates from the activeness functions fps, which are fundamentally important components in our models.\nBy introducing a convex loss function `(·, ·) (e.g., squared loss for regression, squared hinge or logistic loss for classification), we can represent an objective function of the partition-wise linear models as a convex loss minimization problem as follows:\nmin A ∑ n `(yn, ∑ p fp(xn) ∑ d adpxnd). (2)\nHere we give a convex formulation of region-specific linear models under the assumption that a set of partitions is given. In Section 3, we propose a convex optimization algorithm for partitions and regions as a partition selection problem, using sparsity-inducing structured regularization."
    }, {
      "heading" : "2.2 Partition Activeness Functions",
      "text" : "A partition activeness function fp divides the input space into two regions, and a set of activeness functions defines the entire region-structure. Although any function is applicable in principle to being used as a partition activeness function, we prefer as simple a region representation as possible\nbecause of our practical motivation of utilizing region-specific linear models (i.e., interpretability is a priority). This paper restricts them to being parallel to the coordinates, e.g., fp(x) = 1 (xi > 2.5) and fp(x) = 0 (otherwise) with respect to the i-th coordinate. Although this “rule-representation” is simpler than others [2, 3] which use dense linear hyperplanes as region-specifiers, our empirical evaluation (Section 5) indicates that our partition-wise linear models perform competitively with or even better than those others by appropriately optimizing the simple region-specifiers (partition activeness functions)."
    }, {
      "heading" : "2.3 Global and Local Residual Model",
      "text" : "As a special instance of partition-wise linear models, we here propose a model which we refer to as a global and local residual model. It employs a global linear weight vector a0 in addition to partition-wise linear weights. The global weight vector is active for all data. The predictor model (1) can be rewritten as:\ng(x) = aT0 x+ ∑ p fp(x) ∑ d adpxd . (3)\nThe integration of the global weight vector enables the model to determine how features affect outputs not only locally but also globally. Let us consider a new partition activeness function f0(x) that always returns to 1 regardless of x. Then, by setting f(·) = (f0(·), f1(·), . . . , fp(·), . . . , fP (·))T and A = (a0, a1, . . . , aP ), the global and local residual model can be represented using the same notation as is used in Section 2.1. Although a0 and ap have no fundamental difference here, they are different in terms of how we regularize them (Section 3.1)."
    }, {
      "heading" : "3 Convex Optimization of Regions and Predictors",
      "text" : "In Section 2, we presented a convex formulation of partition-wise linear models in (2) under the assumption that a set of partition activeness functions was given. This section relaxes this assumption and proposes a convex partition optimization algorithm."
    }, {
      "heading" : "3.1 Region Optimization as Sparse Partition Selection",
      "text" : "Let us assume that we have been given P +1 partition activeness functions, f0, f1, . . . , fP , and their attached linear weight vectors, a0, a1, . . . , aP , where f0 and a0 are the global activeness function and weight vector, respectively. We formulate the region optimization problem here as partition selection by setting setting most of aps to zero since ap = 0 corresponds to the situation in which the p-th partition does not exist.\nFormally, we formulate our optimization problem with respect to regions and weight vectors by introducing two types of sparsity-inducing constrains to (2) as follows:\nmin A ∑ n `(yn, g(xn)) s.t. ∑ p∈{1,...,P} 1{ap 6=0} ≤ µP , ‖ap‖0 ≤ µ0 ∀p. (4)\nThe former constraint restricts the number of effective partitions to at most µP . Note that we do not enforce this sparse partition constraint to the global model a0 so as to be able to determine local trends as residuals from a global trend. The latter constraint restricts the number of effective features of ap to at most µ0. We add this constraint because 1) it is natural to assume only a small number of features are locally effective in practical applications and 2) a sparser model is typically preferred for our purposes because of its better interpretability."
    }, {
      "heading" : "3.2 Convex Optimization via Decomposition of Proximal Maps",
      "text" : ""
    }, {
      "heading" : "3.2.1 The Tightest Convex Envelope",
      "text" : "The constraints in (5) are non-convex, and it is very hard to find the global optimum due to the indicator functions and L0 penalties. This makes optimization over a non-convex region a very complicated task, and we therefore apply a convex relaxation. One standard approach to convex\nrelaxation would be a combination of group L1 (the first constraint) and L1 (the second constraint) penalties. Here, however, we consider the tightest convex relaxation of (4) as follows:\nmin A ∑ n `(yn, g(xn)) s.t. P∑ p=1 ‖ap‖∞ ≤ µP , D∑ d=1 ‖adp‖∞ ≤ µ0 ∀p. (5)\nThe tightness of constraints in (5) can be shown as follows. The original constraints are non-convex cardinality functions on A, which can be equivalently rewritten in terms of a non-decreasing submodular function F on an index set S as:∑\n{Gp:S∩Gp 6=∅}\n1 ≤ ηP , ∑\n{Gdp:S∩Gdp 6=∅}\n1 ≤ η0, (6)\nS is a set of index pairs with respect to non-zero elements in A. Gp is a set of index pairs which correspond to individual features in the p-th partition, i.e., Gp = {(d, p) : d ∈ {1, . . . , D}}. Gdp is a set of single index pairs, i.e., Gdp = {(d, p)}. It is easy to confirm that the constraints in (5) represent the Lovász extension of the constraints in (4), and this extension gives the tightest convex envelope of non-decreasing sub-modular functions [14].\nThrough such a convex envelope of constraints, the feasible region becomes convex. Therefore, we can reformulate (5) to the following equivalent problem:\nmin A ∑ n `(yn, g(xn)) + Ω(A) where Ω(A) = λP P∑ p=1 ‖ap‖∞ + λ0 P∑ p=0 D∑ d=1 ‖adp‖∞. (7)\nwhere λP and λ0 are regularization weights corresponding to µP and µ0, respectively.\nThis paper derives an efficient optimization algorithm for (7) using the proximal method and the decomposition of proximal maps."
    }, {
      "heading" : "3.2.2 Proximal Method and FISTA",
      "text" : "The proximal method is a standard efficient tool for solving convex optimization problems with non-differential regularizers. It iteratively applies gradient steps and proximal steps to update parameters. This achieves O(1/t) convergence [5] under Lipschitz-continuity of the loss gradient, or even O(1/t2) convergence if an acceleration technique, such as a fast iterative shrinkage thresholding algorithm (FISTA) [6, 15], is incorporated.\nLet us defineA(t) as the weight matrix at the t-th iteration. In the gradient step, the weight vectors are updated to decrease empirical loss through the first-order approximation (gradient) of loss functions as follows:\nA(t+ 1 2 ) = A(t) − η(t) ∑ n ∂A(t)` (yn, g(xn)) , (8)\nwhere η(t) is a step size and ∂A(t)`(·, ·) is the gradient of loss functions evaluated at A(t). In the proximal step, we apply regularization to the current solution A(t+ 1 2 ) as follows:\nA(t+1) = M0(A (t+ 12 )) where M0(B) = argmin\nA\n( 1\n2 ‖A−B‖2F + η(t)Ω(A)\n) , (9)\nwhere ‖ · ‖F is the Frobenius norm. Furthermore, we have adopted a backtracking rule [6] to avoid the difficulty of calculating appropriate step widths beforehand.\nWe also employ FISTA [6] to achieve a faster convergence rate, O(1/t2), for weakly convex problems. In FISTA, the proximal operator step (9) is modified with an additional step width s(t) and a matrix V (t) as follows:\nV (t) = argmin V\n1 2 ‖V −A(t+ 12 )‖2F + ηtΩ(V ) ,\ns(t+1) = 1 +\n√ 1 + 4 ( s(t) )2\n2 , (10)\na (t+1) dp = v (t) dp + s(t) − 1 s(t+1) ( v (t) dp − v (t−1) dp ) .\nwhere we take V (0) = A(1) and t(1) = 1. V (t) is the same as the solution for the original proximal map. In both theoretical analysis and empirical evaluations, we have confirmed that (10) significantly improves convergence in learning partition-wise linear models."
    }, {
      "heading" : "3.2.3 Decomposition of Proximal Maps",
      "text" : "The computational cost of the proximal method depends strongly on the efficiency of solving the proximal step (9). A number of approaches have been developed for improving efficiency, including the minimum-norm-point approach [14] and the networkflow approach [16, 17]. Their computational efficiencies depend strongly on feature and partition size2, however, which makes them inappropriate for our formulation because of potentially large feature and partition sizes.\nAlternatively, this paper employs the decomposition of proximal maps [7]. The key idea here is to decompose the proximal step into a sequence of sub-problems that are easily solvable. We first introduce two easily-solvable proximal maps as follows:\nM1(B) = argmin A\n1 2 ‖A−B‖2F + η(t)λP P∑ p=1 ‖ap‖∞ , (11)\nM2(B) = argmin A\n1 2 ‖A−B‖2F + η(t)λ0 P∑ p=0 D∑ d=1 ‖adp‖∞\n= argmin A\n1 2 ‖A−B‖2F + η(t)λ0 P∑ p=0 D∑ d=1 |adp| . (12)\nThe theorem below guarantees that the decomposition of the proximal map (9) can be performed.\nTheorem 1 The original problem (9) can be decomposed by using the following two proximal maps as follows:\nA(t+1) = M0(A (t+ 12 )) = M2(M1(A (t+ 12 ))) . (13)\nProof: From the discussion of Theorem 1 in [7], we can determine the sufficient condition for satisfying this decomposition of proximal maps. The sufficient condition here is:\n∀B, p ∂‖ap‖∞ ⊇ ∂‖bp‖∞ where A = M2(B) , (14)\nwhere ∂‖ap‖∞ is a subdifferential set of ‖ap‖∞. Keys of this proof are 1) properties in the subdifferential of L∞ norm, and 2) no variance in the magnitude relationship among features as seen before and after soft-thresholding.\nThe subdifferential of ‖ap‖∞ is a convex hull of unit vectors with respect to a set of features that indicate the max absolute value in ap. A set of features having the max absolute value is denoted by Q in this proof. Therefore, formula (14) is satisfied when Q derived from ap includes all features in Q derived from bp. M2(·) is a well-known soft-thresholding operator. This problem can be decomposed into element-wise problems, and the update formula can be described as follows:\nap = sgn(bdp) max ( 0, |bdp − η(t)λ0| ) . (15)\nη(t)λ0 is a constant and is applied to all features. Therefore, Q derived from ap is the same as Q derived from bp when ‖bp‖∞ > η(t)λ0. When ‖bp‖∞ ≤ η(t)λ0, all weights become 0 and Q derived from ap includes all features. As a result, Q derived from ap always includes all features in Q derived from bp and the sufficient condition (14) is always satisfied.\nThe first proximal map (11) is the proximal operator with respect to the L1,∞-regularization. This problem can be decomposed into group-wise sub-problems. Each proximal operator with respect\n2For example, the fastest algorithm for the networkflow approach hasO(M(B+1) log(M2/(B+1))) time complexity, where B is the number of breakpoints determined by the structure of the graph (B ≤ D(P +1) = O(DP )) and M is the number of nodes, that is P + D(P + 1) = O(DP ) [16]. Therefore, the worst computational complexity is O(D2P 2 logDP ).\nAlgorithm 1 Iterative update procedure in the global and local residual model Require: Training data {(xn, yn)}, initial step width η(1), regularization hyperparameters λ0, λP\nInitialize A(1) ∈ RD×(P+1) if Apply warm start of the global model then\nsolve (17) and used its solution as a(1)0 end if for t = 1, . . . , T do\n/* Gradient Step */ Calculate gradient ∂A(t)` (yn, g(xn)) for all n Derive A(t+1/2) by calculating (8) /* Proximal Step */ Derive A(t+3/4) = M1(A(t+1/2)) by calculating (16) Derive A(t+1) = M2(A(t+3/4)) by calculating (15) if Apply acceleration method then\nUpdate A(t+1) by following (10) end if /* Backtracking */ if backtracking rule is violated then\nHalve the value of step width η(t+1) = η(t)/2 Restore previous weight matrix A(t+1) = A(t)\nelse Maintain the same step width η(t+1) = η(t) end if /* Termination */ if the gap of objective values between the current iteration and the previous iteration is less than 10−9 in 10 consecutive iterations then\nterminate algorithm end if\nend for\nto each group can be computed through a projection on an L1-norm ball (derived from the Moreau decomposition [14]), that is,\nap = bp − argmin c ‖c− bp‖2 s.t. ‖c‖1 ≤ η(t)λ . (16)\nThis projection problem can be efficiently solved [18].\nThe second proximal map (12) is a well-known proximal operator with respect to L1-regularization. The solution to element-wise problems is described in (15). These two sub-problems can be easily solved, and we can easily obtain the solution for the original proximal map (9)."
    }, {
      "heading" : "3.2.4 Time Complexity and Convergence Analysis",
      "text" : "Although the time complexity of a single gradient step (8) is O(NPD) with naive full gradient calculation, we can reduce the order by utilizing partition sparseness. In the t-th gradient step, we only need to calculate gradients of active partitions, which in practice are far fewer than with P . A two-stage gradient calculation, i.e., first searching for active partitions and then calculating their gradients, makes the time complexity O(NP + P̂D), where P̂ is the number of active partitions.\nThe time complexity in the proximal steps is dominated by the cost of the calculation of (11) and (12). The first is a simple soft-thresholding operation, and the computational complexity is O(PD). The second is an L∞-regularization proximal operation, and the dominant factor is the sorting of features in each group. The computational complexity of feature ordering is O(D logD), and the time complexity becomes O(PD logD).\nThe resulting computational complexity of partition-wise linear models is, then, O(NP + P̂D + PD logD)."
    }, {
      "heading" : "3.2.5 Warm Start of Global Model",
      "text" : "Although partition-wise linear models can derive global optimums by means of the proximal method from any initial point, the choice of the initial point considerably affects practical convergence speed. We initialize the global weight vector, using the L1-regularization solution, as:\na (1) 0 = argmin\na0 ∑ n ` ( yn, a T 0 xn ) + λ0‖a0‖1. (17)\nThe local weight vectors are uniformly initialized. Empirical comparisons among such few initialization methods as random initialization, zero initialization, etc. indicate that this initialization reliably achieves better empirical convergence. The result is denoted in Section 5.2.3.\nThe iterative update procedure for the global and local residual is expressed, then, in Algorithm 1."
    }, {
      "heading" : "4 Generalization Bound Analysis",
      "text" : "This section presents the derivation of a generalization error bound for partition-wise linear models and discusses how we can increase the number of partition candidates P over the number of samples N . Our bound analysis is related to that of [19], which gives bounds for general overlapping group Lasso cases, while ours is specifically designed for partition-wise linear models.\nLet us first derive an empirical Rademacher complexity [20] for a feasible weight space conditioned on (7). The definition of Rademacher complexity is as follows:\n<X(A) = 2\nn E [ sup A∈A N∑ n=1 ig(xi) ] . (18)\nwhere X = (x1, x2, . . . , xN ). The expectation is over all i, which are i.i.d. {±1}-valued random variables. We can derive Rademacher complexity for our model using the Lemma below. This Lemma is used to derive the upper bound of the expected loss.\nLemma 1 If Ω(A) ≤ 1 is satisfied and if almost surely ‖x‖∞ ≤ 1 with respect to x ∈ X , the empirical Rademacher complexity for partition-wise linear models can be bounded as:\n23/2√ N\n( 2 + √ ln(P +D(P + 1)) ) . (19)\nProof: Let us vectorize a weight matrix A into a = (aT1 , aT2 , . . . , aTP )T . By using this notation, (7) is reformulated as:\nΩ(A) = λP P∑ p=1 ‖up ⊗ a‖∞ + λ0 P∑ p=0 D∑ d=1 ‖udp ⊗ a‖∞ , (20)\nwhere ⊗ is a Kronecker product, udp is a basis vector w.r.t. the d-th feature of the p-th partition, and up = ∑ d udp. This is a special case of Theorem 2 of [19] in which the number of groups is P + D(P + 1). The assumptions here are Ω(A) ≤ 1, ‖fp(x)x‖∞ ≤ 1 for all p ≥ 1, and |fp(x)xd| ≤ 1 for all d, p almost surely with respect to x ∈ X . The second and third assumptions are satisfied when ‖x‖∞ ≤ 1 is satisfied. It can be proved from the definition of the L∞ norm that\n1 ≥ ‖x‖∞ = ‖f0(x)x‖∞ ≥ ‖fp(x)x‖∞ = max d |fp(x)xd| ≥ |fp(x)xd| . (21)\nApplying Theorem 2 of [19] to (20) gives us (19).\nThe next theorem shows the generalization bound of the global and local residual model. This bound is straightforwardly derived from Lemma 1 and the discussion of [20]. In [20], it has been shown that the uniform bound on the estimation error can be obtained through the upper bound of Rademacher complexity derived in Lemma 1. By using the uniform bound, the generalization bound of the global and local residual model defined in formula (4) can be derived.\nTheorem 2 Let us define a set of weights that satisfies Ωgroup(A) ≤ 1 as A where Ωgroup(A) is as defined in Section 2.5 in [19]. Let a datum (xn, yn) be i.i.d. sampled from a specific data distribution D and let us assume loss functions `(·, ·) to be L-Lipschitz functions with respect to a norm ‖ · ‖ and its range to be within [0, 1]. Then, for any constant δ ∈ (0, 1) and any A ∈ A, the following inequality holds with probability at least 1− δ.\nE(x,y)∼D [`(y, g(x))] ≤ 1\nN N∑ n=1 `(yn, g(xn))+\n23/2L√ N\n( 2 + √ ln(DP + P +D) ) + √ ln 1/δ\n2N . (22)\nThis theorem implies how we can increase the number of partition candidates. The third term of the right-hand side is obviously small if N is large. The second term converges to zero with N →∞ if the value of P is smaller than o(eN ), which is a sufficiently large number in practice. In summary, we expect to be able to handle a sufficient number of partition candidates for learning with little risk of over fitting."
    }, {
      "heading" : "5 Experiments",
      "text" : "We conducted two types of experiments: 1) evaluation of how partition-wise linear models perform, on the basis of a simple synthetic dataset and 2) comparisons with state-of-the-art region-specific and locally linear models on the basis of standard classification and regression benchmark datasets."
    }, {
      "heading" : "5.1 Evaluation Using Synthetic Dataset",
      "text" : "We generated a synthetic binary classification dataset as follows. xns were uniformly sampled from a 20-dimensional input space in which each dimension had values between [−1, 1]. The target variables were determined using the XOR rule over the first and second features (the other 18 features were added as noise for prediction purposes.), i.e., if the signs of first feature value and second feature value are the same, y = 1, otherwise y = −1. This is well known as a case in which linear models do not work. For example, L1-regularized logistic regression produced nearly random outputs where the error rate was 0.421.\nWe generated one partition for each feature except for the first feature. Each partition became active if the corresponding feature value was greater than 0.0. Therefore, the number of candidate partitions was 19. We used the logistic regression function for loss functions. Hyper-parameters3 were set as λ0 = 0.01 and λP = 0.001. The algorithm was run in 1, 000 iterations.\nFigure 2 illustrates results produced by the global and local residual model. The left-hand figure illustrates a learned effective partition (red line) to which the weight vector a1 = (10.96, 0.0, · · · ) was assigned. This weight a1 was only applied to the region above the red line. By combining a1 and the global weight a0, we obtained the piece-wise linear representation shown in the right-hand figure. While it is yet difficult for existing piece-wise linear methods to capture global structures4, our convex formulation makes it possible for the global and local residual model to easily capture the global XOR structures."
    }, {
      "heading" : "5.2 Comparisons Using Benchmark Datasets",
      "text" : "We next used benchmark datasets to compare our models with other state-of-the-art region-specific models. In these experiments, we simply generated partition candidates (activeness functions) as follows. For continuous value features, we calculated all 5-quantiles for each feature and generated partitions at each quantile point. Partitions became active if a feature value was greater than the corresponding quantile value. For binary categorical features, we generated two partitions in which\n3We conducted several experiments on other hyper-parameter settings and confirmed that variations in hyper-parameter settings did not significantly affect results.\n4For example, a decision tree cannot be used to find a “true” XOR structure since marginal distributions on the first and second features cannot discriminate between positive and negative classes.\none became active when the feature was 1 (yes) and the other became active only when the feature value was 0 (no).\nWe utilized several standard classification and regression benchmark datasets from UCI datasets (skin, winequality, census income, twitter, internet ad, energy heat, energy cool, communities), libsvm datasets (a1a, breast cancer), and LIACC datasets (abalone, kinematics, puma8NH, bank8FM). Table 3 summarizes specifications for each dataset."
    }, {
      "heading" : "5.2.1 Classification",
      "text" : "For classification, we compared the global and local residual model (Global/Local) with L1 logistic regression (Linear), LSL-SP with linear discrimination analysis5, LDKL supported by L2-\n5The source code is provided by the author of [3].\nregularized hinge loss6, FaLK-SVM with linear kernels7, and C-SVM with RBF kernel8. Note that C-SVM is neither a region-specific nor locally linear classification model; it is, rather, a non-linear model. We compared it with ours as a reference with respect to a common non-linear classification model.\nFor our models, we used logistic functions for loss functions. The max iteration number was set as 1000, and the algorithm stopped early when the gap in the empirical loss from the previous iteration became lower than 10−9 in 10 consecutive iterations. Hyperparameters9 were optimized through 10-fold cross validation. We fixed the number of regions to 10 in LSL-SP, tree-depth to 3 in LDKL, and neighborhood size to 100 in FaLK-SVM.\nTable 4 summarizes the classification errors. We observed:\n• Global/Local consistently performed well and achieved the best error rates foir four datasets out of seven. • LSL-SP performed well for census income and breast-cancer, but did significantly worse than Linear for skin, twitter, and a1a. Similarly, LDKL performed worse than Linear for census income, twitter, a1a and internet ad. This arose partly because of over fitting and partly because of bad local minima. Particularly noteworthy is that the standard deviations in LDKL were much larger than in the others, and the initialization issue would seem to become significant in practice. • FaLK-SVM performed well in most cases, but its computational cost was significantly higher than that of others, and it was unable to obtain results for census income and internet ad (we stopped the algorithm after 24 hours running)."
    }, {
      "heading" : "5.2.2 Regression",
      "text" : "For regression, we compared Global/Local with Linear, regression tree10 by CART (RegTree) [1], and epsilon-SVR with RBF kernel11. Target variables were standardized so that their mean was 0 and their variance was 1. Performance was evaluated using the root mean squared loss in the test data. Tree-depth of RegTree and in RBF-SVR were determined by means of 10-fold cross validation. Other experimental settings were the same as those used in the classification tasks.\nTable 5 summarizes RMSE values. In classification tasks, Global/Local consistently performed well. For the kinematics, RBF-SVR performed much better than Global/Local, but Global/Local was better than Linear and RegTree in many other datasets."
    }, {
      "heading" : "5.2.3 Warm Start Effect",
      "text" : "We also conducted time comparisons between our partition-wise linear models with and without warm start. We used the a1a dataset for the comparisons and considered the relationship between\n6https://research.microsoft.com/en-us/um/people/manik/code/LDKL/ download.html\n7http://disi.unitn.it/˜segata/FaLKM-lib/ 8We used a libsvm package for the experiment. http://www.csie.ntu.edu.tw/˜cjlin/ libsvm/ 9 λ1, λ2p in Global/Local,λ1 in Linear, λW , λθ, λθ‘, σ in LDKL, C in FaLK-SVM, and C, γ in RBF-SVM.\n10We used a scikit-learn package. http://scikit-learn.org/ 11We used a libsvm package.\ntraining error rate and computational time. Figure 3 shows the results. It indicates that our models with warm start achieved lower training error rates than did models without warm start."
    }, {
      "heading" : "6 Discussion",
      "text" : ""
    }, {
      "heading" : "6.1 Scaling Up Optimization",
      "text" : "In dealing with large-scale data in which the number of samples is very large, full gradient calculation (8) becomes a computational bottleneck. For scaling up our algorithm, stochastic and parallel optimization techniques appear promising. With respect to stochastic optimization, Mairal has recently proposed Minimization by Incremental Surrogate Optimization (MISO) [21] as an incremental optimization method for a majorization-minimization framework that includes proximal methods. Despite the fact that MISO stochastically approximates gradients (sampling), it has a linear convergence property for the global optimum when the data size is known in advance; this convergence rate is the same as the one in the full gradient case. Another promising direction is parallelization. Parallelization of full gradient calculations is a straightforward process and could be of insignificant importance in actual practice. Another possible future direction is a greedy solver for original non-convex problems (4), such as orthogonal matching pursuits [22]."
    }, {
      "heading" : "6.2 Advanced Partition Generation",
      "text" : "In Section 5, we generated partition candidates (activeness functions) on the basis of simple rules. Although this worked reasonably, more advanced partition generation methods might improve predictive accuracy. Taking locally linear SVMs [10] as an analogy, it is known that local coordinate (anchor) selection considerably affects predictive performance, and advanced coordinate generation\nmethods have been proposed [11, 12]. The algorithm proposed by Dekel and Shamir [23] gradually adds piece-wise regions on the basis of the detection of “poorly predictable” regions, and it trains piece-wise predictors. While this approach is not directly-applicable to partition-wise linear models, such a “boosting-type” approach to partition candidate generation is an interesting concept to consider."
    }, {
      "heading" : "6.3 Hierarchical Structured Sparseness",
      "text" : "In this study, we have treated partition candidates equally and enforced the group sparse penalty. In many real applications, data have structures, and it would be interesting to incorporate such structures into the learning process in partition-wise linear models. In this regard, the “tree-structured” sparsity-inducing regularization proposed by Huang et al. [24] is particularly notable. Defining hierarchical partition structures and automatically learning “hierarchical” region structures might give us an improved understanding of data structures. Note that such tree-structured regularization is also convex, and we might directly apply it to our optimization technique."
    }, {
      "heading" : "7 Summary",
      "text" : "We have proposed here a novel convex formulation of region-specific linear models that we refer to as partition-wise linear models. Our approach simultaneously optimizes regions and predictors using sparsity-inducing structured penalties. For the purpose of efficiently solving the optimization problem, we have derived an efficient algorithm based on the decomposition of proximal maps. Thanks to its convexity, our method is free from initialization dependency, and a generalization error bound can be derived. Empirical results demonstrate the superiority of partition-wise linear models over other region-specific and locally linear models."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The majority of the work was done during the internship of the first author at the NEC central research laboratories."
    } ],
    "references" : [ {
      "title" : "Classification and Regression Trees",
      "author" : [ "Leo Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1984
    }, {
      "title" : "Local deep kernel learning for efficient non-linear svm prediction",
      "author" : [ "Cijo Jose", "Prasoon Goyal", "Parv Aggrwal", "Manik Varma" ],
      "venue" : "In ICML,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Local supervised learning through space partitioning",
      "author" : [ "Joseph Wang", "Venkatesh Saligrama" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Cost-Sensitive Tree of Classifiers",
      "author" : [ "Zhixiang Xu", "Matt Kusner", "Minmin Chen", "Kilian Q. Weinberger" ],
      "venue" : "In ICML,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Approximation accuracy, gradient methods, and error bound for structured convex optimization",
      "author" : [ "Paul Tseng" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "Amir Beck", "Marc Teboulle" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "On decomposing the proximal map",
      "author" : [ "Yaoliang Yu" ],
      "venue" : "In NIPS, pages 91–99,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Hierarchical mixtures of experts and the em algorithm",
      "author" : [ "Michael I. Jordan", "Robert A. Jacobs" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1994
    }, {
      "title" : "Fast and scalable local kernel machines",
      "author" : [ "Nicola Segata", "Enrico Blanzieri" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Locally Linear Support Vector Machines",
      "author" : [ "Lubor Ladicky", "Philip H.S. Torr" ],
      "venue" : "In ICML,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Nonlinear learning using local coordinate coding",
      "author" : [ "Kai Yu", "Tong Zhang", "Yihong Gong" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Learning anchor planes for classification",
      "author" : [ "Ziming Zhang", "Lubor Ladicky", "Philip H.S. Torr", "Amir Saffari" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Clustered support vector machines",
      "author" : [ "Quanquan Gu", "Jiawei Han" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Structured sparsity-inducing norms through submodular functions",
      "author" : [ "Francis R. Bach" ],
      "venue" : "In NIPS, pages 118–126,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Gradient methods for minimizing composite objective function",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Core discussion papers,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "A fast parametric maximum flow algorithm and applications",
      "author" : [ "Giorgio Gallo", "Michael D. Grigoriadis", "Robert E. Tarjan" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1989
    }, {
      "title" : "Structured convex optimization under submodular constraints",
      "author" : [ "Kiyohito Nagano", "Yoshinobu Kawahara" ],
      "venue" : "In UAI,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Efficient online and batch learning using forward backward splitting",
      "author" : [ "John Duchi", "Yoram Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Structured sparsity and generalization",
      "author" : [ "Andreas Maurer", "Massimiliano Pontil" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Rademacher and gaussian complexities: risk bounds and structural results",
      "author" : [ "Peter L. Bartlett", "Shahar Mendelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    }, {
      "title" : "Optimization with first-order surrogate functions",
      "author" : [ "Julien Mairal" ],
      "venue" : "In ICML, pages 783–791,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition",
      "author" : [ "Y.C. Pati", "R. Rezaiifar", "P.S. Krishnaprasad" ],
      "venue" : "In ASILOMAR,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1993
    }, {
      "title" : "There’s a hole in my data space: Piecewise predictors for heterogeneous learning problems",
      "author" : [ "Ofer Dekel", "Ohad Shamir" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Learning with structured sparsity",
      "author" : [ "Junzhou Huang", "Tong Zhang", "Dimitris N. Metaxas" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Such models vary—from traditional decision/regression trees [1] to more advanced models [2, 3, 4]—depending on their region-specifiers (how they characterize regions), region-specific prediction models, and the objective functions to be optimized.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "Such models vary—from traditional decision/regression trees [1] to more advanced models [2, 3, 4]—depending on their region-specifiers (how they characterize regions), region-specific prediction models, and the objective functions to be optimized.",
      "startOffset" : 88,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "Such models vary—from traditional decision/regression trees [1] to more advanced models [2, 3, 4]—depending on their region-specifiers (how they characterize regions), region-specific prediction models, and the objective functions to be optimized.",
      "startOffset" : 88,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "Such models vary—from traditional decision/regression trees [1] to more advanced models [2, 3, 4]—depending on their region-specifiers (how they characterize regions), region-specific prediction models, and the objective functions to be optimized.",
      "startOffset" : 88,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "We have developed an efficient algorithm to solve structured-sparse optimization problems, and in it we utilize both a proximal method [5, 6] and the decomposition of proximal maps [7].",
      "startOffset" : 135,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "We have developed an efficient algorithm to solve structured-sparse optimization problems, and in it we utilize both a proximal method [5, 6] and the decomposition of proximal maps [7].",
      "startOffset" : 135,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "We have developed an efficient algorithm to solve structured-sparse optimization problems, and in it we utilize both a proximal method [5, 6] and the decomposition of proximal maps [7].",
      "startOffset" : 181,
      "endOffset" : 184
    }, {
      "referenceID" : 0,
      "context" : "Well-known precursors to region-specific linear models are decision/regression trees [1], which use rule-based region-specifiers and constant-valued predictors.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "Another traditional framework is a hierarchical mixture of experts [8], which is a probabilistic tree-based region-specific model framework.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "Recently, Local Supervised Learning through Space Partitioning (LSL-SP) has been proposed [3].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "Additionally, a Cost-Sensitive Tree of Classifiers (CSTC) algorithm has also been developed [4].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "FaLK-SVMs produce test-point-specific weight vectors by learning local predictive models from the neighborhoods of individual test points [9].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "Another advanced locally linear model is that of Locally Linear Support Vector Machines (LLSVMs) [10].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "LLSVMs assign linear SVMs to multiple anchor points produced by manifold learning [11, 12] and construct test-point-specific linear predictors according to the weights of anchor points with respect to individual test points.",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "LLSVMs assign linear SVMs to multiple anchor points produced by manifold learning [11, 12] and construct test-point-specific linear predictors according to the weights of anchor points with respect to individual test points.",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "Similarly, clustered SVMs (CSVMs) [13] assume given data clusters and learn multiple SVMs for individual clusters simultaneously.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "have proposed Local Deep Kernel Learning (LDKL) [2], which adopts an intermediate approach with respect to region-specific and locally linear models.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "Although this “rule-representation” is simpler than others [2, 3] which use dense linear hyperplanes as region-specifiers, our empirical evaluation (Section 5) indicates that our partition-wise linear models perform competitively with or even better than those others by appropriately optimizing the simple region-specifiers (partition activeness functions).",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "Although this “rule-representation” is simpler than others [2, 3] which use dense linear hyperplanes as region-specifiers, our empirical evaluation (Section 5) indicates that our partition-wise linear models perform competitively with or even better than those others by appropriately optimizing the simple region-specifiers (partition activeness functions).",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "It is easy to confirm that the constraints in (5) represent the Lovász extension of the constraints in (4), and this extension gives the tightest convex envelope of non-decreasing sub-modular functions [14].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 4,
      "context" : "This achieves O(1/t) convergence [5] under Lipschitz-continuity of the loss gradient, or even O(1/t) convergence if an acceleration technique, such as a fast iterative shrinkage thresholding algorithm (FISTA) [6, 15], is incorporated.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "This achieves O(1/t) convergence [5] under Lipschitz-continuity of the loss gradient, or even O(1/t) convergence if an acceleration technique, such as a fast iterative shrinkage thresholding algorithm (FISTA) [6, 15], is incorporated.",
      "startOffset" : 209,
      "endOffset" : 216
    }, {
      "referenceID" : 14,
      "context" : "This achieves O(1/t) convergence [5] under Lipschitz-continuity of the loss gradient, or even O(1/t) convergence if an acceleration technique, such as a fast iterative shrinkage thresholding algorithm (FISTA) [6, 15], is incorporated.",
      "startOffset" : 209,
      "endOffset" : 216
    }, {
      "referenceID" : 5,
      "context" : "Furthermore, we have adopted a backtracking rule [6] to avoid the difficulty of calculating appropriate step widths beforehand.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : "We also employ FISTA [6] to achieve a faster convergence rate, O(1/t), for weakly convex problems.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "A number of approaches have been developed for improving efficiency, including the minimum-norm-point approach [14] and the networkflow approach [16, 17].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 15,
      "context" : "A number of approaches have been developed for improving efficiency, including the minimum-norm-point approach [14] and the networkflow approach [16, 17].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 16,
      "context" : "A number of approaches have been developed for improving efficiency, including the minimum-norm-point approach [14] and the networkflow approach [16, 17].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 6,
      "context" : "Alternatively, this paper employs the decomposition of proximal maps [7].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "Proof: From the discussion of Theorem 1 in [7], we can determine the sufficient condition for satisfying this decomposition of proximal maps.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 15,
      "context" : "Each proximal operator with respect For example, the fastest algorithm for the networkflow approach hasO(M(B+1) log(M/(B+1))) time complexity, where B is the number of breakpoints determined by the structure of the graph (B ≤ D(P +1) = O(DP )) and M is the number of nodes, that is P + D(P + 1) = O(DP ) [16].",
      "startOffset" : 304,
      "endOffset" : 308
    }, {
      "referenceID" : 13,
      "context" : "to each group can be computed through a projection on an L1-norm ball (derived from the Moreau decomposition [14]), that is,",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 17,
      "context" : "This projection problem can be efficiently solved [18].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : "Our bound analysis is related to that of [19], which gives bounds for general overlapping group Lasso cases, while ours is specifically designed for partition-wise linear models.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 19,
      "context" : "Let us first derive an empirical Rademacher complexity [20] for a feasible weight space conditioned on (7).",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 18,
      "context" : "This is a special case of Theorem 2 of [19] in which the number of groups is P + D(P + 1).",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 18,
      "context" : "Applying Theorem 2 of [19] to (20) gives us (19).",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 19,
      "context" : "This bound is straightforwardly derived from Lemma 1 and the discussion of [20].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 19,
      "context" : "In [20], it has been shown that the uniform bound on the estimation error can be obtained through the upper bound of Rademacher complexity derived in Lemma 1.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 18,
      "context" : "5 in [19].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "sampled from a specific data distribution D and let us assume loss functions `(·, ·) to be L-Lipschitz functions with respect to a norm ‖ · ‖ and its range to be within [0, 1].",
      "startOffset" : 169,
      "endOffset" : 175
    }, {
      "referenceID" : 2,
      "context" : "The source code is provided by the author of [3].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : "2 Regression For regression, we compared Global/Local with Linear, regression tree10 by CART (RegTree) [1], and epsilon-SVR with RBF kernel11.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : "With respect to stochastic optimization, Mairal has recently proposed Minimization by Incremental Surrogate Optimization (MISO) [21] as an incremental optimization method for a majorization-minimization framework that includes proximal methods.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "Another possible future direction is a greedy solver for original non-convex problems (4), such as orthogonal matching pursuits [22].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "Taking locally linear SVMs [10] as an analogy, it is known that local coordinate (anchor) selection considerably affects predictive performance, and advanced coordinate generation",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "methods have been proposed [11, 12].",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "methods have been proposed [11, 12].",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : "The algorithm proposed by Dekel and Shamir [23] gradually adds piece-wise regions on the basis of the detection of “poorly predictable” regions, and it trains piece-wise predictors.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 23,
      "context" : "[24] is particularly notable.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2014,
    "abstractText" : "Region-specific linear models are widely used in practical applications because of their non-linear but highly interpretable model representations. One of the key challenges in their use is non-convexity in simultaneous optimization of regions and region-specific models. This paper proposes novel convex region-specific linear models, which we refer to as partition-wise linear models. Our key ideas are 1) assigning linear models not to regions but to partitions (region-specifiers) and representing region-specific linear models by linear combinations of partitionspecific models, and 2) optimizing regions via partition selection from a large number of given partition candidates by means of convex structured regularizations. In addition to providing initialization-free globally-optimal solutions, our convex formulation makes it possible to derive a generalization bound and to use such advanced optimization techniques as proximal methods and decomposition of the proximal maps for sparsity-inducing regularizations. Experimental results demonstrate that our partition-wise linear models perform better than or are at least competitive with state-of-the-art region-specific or locally linear models.",
    "creator" : "LaTeX with hyperref package"
  }
}