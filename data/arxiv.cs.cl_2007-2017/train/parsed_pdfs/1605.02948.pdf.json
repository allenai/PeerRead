{
  "name" : "1605.02948.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Bayesian Approach to Biomedical Text Summarization",
    "authors" : [ "Milad Moradi", "Nasser Ghadiri" ],
    "emails" : [ "milad.moradi@ec.iut.ac.ir,", "nghadiri@cc.iut.ac.ir", "nghadiri@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "A Bayesian Approach to Biomedical Text Summarization\nMilad Moradi, Nasser Ghadiri1\nDepartment of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan, Iran\nE-mail: milad.moradi@ec.iut.ac.ir, nghadiri@cc.iut.ac.ir\nAbstract―Many biomedical researchers and clinicians are faced with the information overload problem. Attaining desirable information from the ever-increasing body of knowledge is a difficult task without using automatic text summarization tools that help them to acquire the intended information in shorter time and with less effort. Although many text summarization methods have been proposed, developing domain-specific methods for the biomedical texts is a challenging task. In this paper, we propose a biomedical text summarization method, based on concept extraction technique and a novel sentence classification approach. We incorporate domain knowledge by utilizing the UMLS knowledge source and the naïve Bayes classifier to build our text summarizer. Unlike many existing methods, the system learns to classify the sentences without the need for training data, and selects them for the summary according to the distribution of essential concepts within the original text. We show that the use of critical concepts to represent the sentences as vectors of features, and classifying the sentences based on the distribution of those concepts, will improve the performance of automatic summarization. An extensive evaluation is performed on a collection of scientific articles in biomedical domain. The results show that our proposed method outperforms several well-known research-based, commercial and baseline summarizers according to the most commonly used ROUGE evaluation metrics.\nKeywords―Biomedical text summarization; Data mining; Naïve Bayes; Concept extraction; UMLS; Domain\nknowledge; Sentence classification;\n1 Corresponding author. Address: Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan, Iran. Phone : +98-31-3391-9058, Fax: +98-31-3391-2450, Alternate email: nghadiri@gmail.com\nManuscipt 2 26 April 2016"
    }, {
      "heading" : "1. Introduction",
      "text" : "Biomedical information available for researchers and clinicians is accessible from a variety of sources such as scientific literature databases, Electronic Health Record (EHR) systems, web documents, e-mailed reports and multimedia documents [1, 2]. The scientific literature provides a valuable source of information to researchers. It is widely used as a rich source for assessing the new comers in a particular field, gathering information for constructing research hypotheses and collecting information for interpretation of experimental results [3]. It is interesting to know that the US National Library of Medicine has indexed over 24 million citations from more than 5,500 biomedical journals in its MEDLINE bibliographic database [4]. However, the larger quantities of data cannot be used to attain the desirable information in a limited time. Required information must be accessed easily at the right time, and in the most appropriate form [2]. For clinicians and researchers, efficiently seeking useful information from the ever-increasing body of knowledge and other resources is excessively time-consuming. Managing this information overload is shown to be a difficult task without the help of automatic tools.\nAutomatic text summarization is a promising approach to overcome the information overload, by reducing the amount of text that must be read [5]. It can be used to obtain the gist efficiently on a topic of interest [1]. It helps the clinicians and researchers to save their time and efforts required to seek information. A good summary of text must have two main properties: it needs to be short, and it should preserve valuable information of source text [6].\nThe majority of summarization systems deal with domain-specific text (e.g. biomedical text) in the same way as other domain-independent texts are summarized. In other words, they are designed as general-purpose tools [7]. From the earliest basic methods, like position and word-frequency, to more recent methods that leverage artificial intelligence, machine learning, and graph-based algorithms, the general-purpose summarizers are not adequate for using in the biomedical domain. The characteristics of biomedical domain raise the need to analyze the source text at a conceptual level and to employ domain knowledge in summarization process [7]. Concept-level analysis of text, rather than term-level analysis, has been turned into a preliminary step in biomedical summarization process. This is required to extract a rich representation of source text [7-10]. The conceptual analysis of text is performed by focusing on concepts, rather than terms, as the building blocks of text. It can be facilitated by using biomedical knowledge sources and ontologies such as Unified Medical Language System (UMLS).\nA summarization system must decide which sentences are the best for the summary and which sentences can be ignored, based on its constructed model from the source text. From this point of view, text summarization can be modeled as a classification problem. However, there are some important questions. In biomedical summarization that utilizes domain knowledge rather than traditional measures, what features do determine which sentences are summary sentences and which are non-summary? Furthermore, in biomedical summarization that uses domain knowledge, a given document has its distinct concepts, and these concepts have a particular distribution in the given text. Thus, what data must be used as training data? Are there any training data available for this purpose? Can we model the biomedical summarization as a classification problem? In this paper, we provide answers to such questions and try to address the related issues. We employ a well-known classification method, namely naïve Bayes\nManuscipt 3 26 April 2016\nclassifier, in combination with biomedical concept extraction, to construct a classification model for biomedical text summarization.\nSome of the summarization systems have been proposed based on classification methods [11-14]. These systems require training data, to learn which part of the text should be selected for the summary and which part should be discarded. However, when a document is analyzed at the conceptual level, similar to our method, the source text is represented by its contained concepts. Every document may have its own set of concepts, and it is impractical to generalize a learned model to summarize new material. We propose a summarization method, which uses the distribution of important concepts within the source document to classify the sentences and to construct the final summary.\nIn our proposed method, biomedical concepts are extracted from input text by utilizing the UMLS [15], an important and well-known knowledge source in biomedical sciences, maintained by the US National Library of Medicine. Each sentence of the input document is represented as a vector of boolean features. The features are important concepts of the entry text, and a feature would be right if its similar concept appears in the sentence. Otherwise, it would be false. We use the naïve Bayes classifier [16] to label the sentences as summary or nonsummary. The distribution of important concepts within the text is known, as well as the number of summarylabeled sentences. Although, we do not initially know which sentences are labeled as a summary, we just know how many sentences must be selected (the compression rate specifies it). This information is enough to estimate the prior and likelihood probabilities for Bayesian inference. A primary assumption of our method is that the distribution of important concepts within the final summary must be similar to their distribution within the source text. Thus, we can estimate the posterior probabilities given the prior and likelihood probabilities that were calculated before. Consequently, the posterior odds ratio is calculated for each sentence. Eventually, the sentences with higher posterior odds ratio are selected to form the final summary.\nTo evaluate the performance of the proposed method, we conducted a set of experiments on a collection of articles from the biomedical domain and compared the results with other summarizers. The results demonstrate that our method performs better than similar research-oriented, commercial competitors and baseline methods in terms of the most commonly used ROUGE evaluation metrics [17].\nThe remainder of the paper is organized as follows. Section 2 gives an overview of text summarization and concept extraction from biomedical text, as well as a review of the related work in biomedical summarization. In Section 3, we introduce our biomedical summarization method based on the naïve Bayes classification method. Then, we describe the evaluation methodology. Section 4 presents the results of the assessment of the system configuration and the experiments that evaluate our system compared with other summarizers. Finally, Section 5 draws the conclusion and describes future lines of work."
    }, {
      "heading" : "2. Background and related work",
      "text" : "Early work on automatic text summarization dates back to the 1950s and 1960s with the pioneering work of Luhn [18] and Edmundson [19]. However, most of the progress in this field happened during the last two decades.\nManuscipt 4 26 April 2016\nThere are some well-known summarization methods, such as MEAD [20], MMR [21], LexRank [22], PageRank [23], TextRank [24] and HITS [25], widely referenced by the research community in the last two decades. In recent years, many works have been done in text summarization using Natural Language Processing (NLP), clustering, machine learning, statistical and graph-based methods. However, the biomedical text summarization is a relatively younger research area with a history of almost two decades. In this section, first, we present a commonly used categorization of text summarization methods. Then we focus on the concept extraction method in the biomedical domain. Finally, we review the previous work on biomedical text summarization."
    }, {
      "heading" : "2.1. Types of summarization",
      "text" : "Text summarization methods can be divided into abstractive and extractive approaches [1, 26]. An abstractive summarizer uses NLP methods to process and analyze the input text, then it infers and produces a new version. On the other hand, an extractive summarizer selects the most representative units (paragraphs, sentences, phrases) from the original wording and puts them together into shorter form. Another classification of text summarization differentiates single-document and multi-document inputs [1, 2]. Single-document summarizer produces the summary which is the result of condensing only one document. In contrast, a multi-document summarizer gets a cluster of papers and provides a single summary as the result of extracting the most representative contents from the input documents. Another classification of summarization methods is based on the requirements of user: generic vs. user-oriented (also known as query-focused summarizers) [1, 2, 27]. A general summary presents an overall implication of input document(s) without any specified preference in terms of content. While a useroriented summary is biased towards a given query or some keywords, to address a user’s specific information requirement. Our proposed biomedical summarization method is extractive, single-document and generic."
    }, {
      "heading" : "2.2. Concept extraction from biomedical text",
      "text" : "In the biomedical domain, there are several knowledge sources such as MeSH, SNOMED, GO, OMIM, UWDA and NCBI Taxonomy, which can be used in knowledge-intensive data and information processing tasks, as well as in text processing tasks related to the biomedical domain. These knowledge sources along with over 100 controlled vocabularies, classification systems, and additional information sources have been unified into the Unified Medical Language System (UMLS) [15] by the National Library of Medicine. The UMLS comprises three main components: the Specialist Lexicon, the Metathesaurus, and the Semantic Network.\nThe Specialist Lexicon [28] is a lexicographic information database, intended to use in NLP systems. It contains commonly occurring English words and biomedical vocabulary. For each word, a lexical entry records the syntactic, morphological and orthographic information. The Specialist Lexicon mainly addresses the high degree of variability in natural language words. The Metathesaurus [29] is a large, multi-lingual, and multi-purpose lexicon that contains millions of biomedical and health related concepts, their relationships and their synonymous names. It includes over 150 electronic versions of classifications, code sets, thesauri and lists of controlled terms in the biomedical domain. The Semantic Network [30] consists of a set of broad subject categories, known as semantic\nManuscipt 5 26 April 2016\ntypes, that provide a stable categorization of all concepts included in the Metathesaurus. It also contains a set of useful and valuable relationships, or semantic relations, that exist between semantic types.\nFor mapping biomedical text to the UMLS Metathesaurus concepts (known as automated concept annotation), the MetaMap program [31, 32] has been developed by the National Library of Medicine. MetaMap uses a knowledge-intensive approach based on NLP, computational linguistic and symbolic techniques to identify noun phrases in the text. First, lexical variations are generated, and phrases and concepts are partially matched, in order to compute the matches between each noun phrase and one or more Metathesaurus concepts. Then, based on the closeness of the matches between each noun phrase and the concepts, the candidate concepts are assigned scores. Eventually, the highest scoring concept and its semantic type are returned. For a noun phrase, it is possible to map to more than one concept, and MetaMap will return multiple concepts in this case."
    }, {
      "heading" : "2.3. Summarization in the biomedical domain",
      "text" : "In the biomedical field, various summarization methods have been proposed. These methods are reviewed in a\nsurvey of early work [2] and in a systematic review of recently published research [1].\nReeve et al. [9] applied the method of lexical chaining [33] to biomedical text, but they used concepts rather than terms. In their proposed method, named BioChain, automatically identified UMLS concepts in the original text are chained together based on their UMLS semantic types. Then, the strongest chains are identified through scoring. Strong concepts within each strong chain are determined, and sentences are scored based on the number of such concepts they contain. High scoring sentences are selected to form the summary. In BioChain, less frequent concepts that belong to strong chains participate in sentence scoring. More frequent concepts that do not belong to any strong chains are discarded for sentence scoring. As a result, the important concepts that demonstrate the main topics but do not belong to any strong chains will not participate in sentence scoring, and the accuracy of the summarizer may be affected negatively.\nFreqDist [10] is a context-sensitive approach, proposed to score the sentences according to a frequency distribution model, along with the ability to remove information redundancy. In the FreqDist method, the unit items (concepts and terms) within the original text are counted, and a frequency distribution model is formed. A summary frequency distribution model is also created based on the unit items found in the original wording. Then, in an iterative manner, sentences are selected for adding to the summary. Selection of a sentence must lead to the frequency distribution of the summary be closely aligned with the frequency distribution of the original text. Reeve et al. [5] combine the BioChain and the FreqDist and propose a hybrid method. In a feature-based method [34], in addition to commonly used traditional features, a vocabulary of cue terms and phrases unique to the medical domain is identified and is used as domain knowledge. The classic features used in summarization are word frequency, sentence position, the similarity with the title of the article, and sentence length. The presence of cue medical terms and phrases, as well as the presence of new terms, are two additional features. The sentences are scored based on these features, and the summary is generated by putting the high-scoring sentences together.\nManuscipt 6 26 April 2016\nA graph-based approach to biomedical summarization is proposed by Plaza et al. [7]. They use UMLS to identify concepts and semantic relations between them, and a semantic graph is constructed to represent the document. Different topics within the text are determined by applying a degree-based clustering algorithm on the semantic graph. Three different heuristics are intended for sentence selection according to identified topics. Moen et al. [35] present several text summarization methods for summarizing clinical notes. Most of their proposed methods are based on the word space models, resulted from distributional semantic modeling. They perform a meta-evaluation on the ROUGE metrics by developing a manual evaluation scheme, in order to assess the similarity between the automatic assessment and the opinions of health care professionals.\nAn investigation on the impact of the knowledge source used in a semantic graph-based summarization approach is performed by Plaza et al. [36], in terms of the quality of the automatically generated summaries. Different combinations of vocabularies and ontologies within the UMLS are used to retrieve domain concepts. Moreover, various types of relationships are considered to link the concepts in the semantic graph. They also show that the use of appropriate knowledge source to model the original text significantly improves the quality of the generated summaries.\nBesides extractive summarization methods, various abstractive methods have been proposed in the biomedical domain. Fiszman et al. [37] present a multi-document semantic abstraction summarization system for MEDLINE citations. Their system relies on the semantic predications extracted by SemRep [38], a parser based on linguistic analysis and domain knowledge contained in the UMLS. The system generates abstracts using four transformation principles: novelty, relevance, saliency, and connectivity. The output of the system is a graphical summary. Fiszman et al. [8] extend the semantic abstraction summarization system [37] for evidence-based medical treatment. Their focus is on the topic-based evaluation of summarization of drug interventions. Two other abstractive summarizers based on semantic abstraction summarization system [37] are proposed by Workman et al. [39] and Zhang et al. [40]. An abstractive graph-based clustering method [41] is presented for automatic identification of themes in multi-document summarization. The output of the system is a graph composed of semantic predications. The aim of their method is to summarize a large set of MEDLINE citations.\nUnlike domain-independent summarization methods such as SUMMA [42] and SweSum [43], our proposed method utilizes domain knowledge and analyzes the source text at a conceptual level. Existing approaches that rely on classification methods will require training data to learn the classifier. Moreover, the majority of the summarization methods use a number of general-purpose features such as sentence position, sentence length, keywords, and the presence of cue-words to represent the sentences as vectors of features. However, in text summarization methods that utilize domain knowledge and concept-level analysis of text, every document will have its particular set of concepts, leading to a potentially higher accuracy. A set of general features is not enough to summarize all new material. In our method, the naïve Bayes classification method helps to classify the sentences based on the distribution of concepts within the source text, without any requirements for training data.\nIn our proposed method, the important concepts are identified according to a threshold value, to demonstrate the main topics of the document and represent the sentences as vectors of features. Compared to the BioChain method\nManuscipt 7 26 April 2016\nthat ignores the important concepts that do not belong to any strong chains, our method is expected to perform more accurately. Moreover, our method assumes that the distribution of important concepts within the final summary is same as the source text. This assumption could improve the informativeness of the final summary. In order to classify the sentences of a document as summary and non-summary based on this assumption, the naïve Bayes classifier is a reliable method as it can discriminate the sentences based on the prior distribution of important concepts within the source document."
    }, {
      "heading" : "3. The proposed method",
      "text" : "Our proposed summarization scheme consists of a preprocessing phase and a classification phase. In the preprocessing step, the input document is mapped to UMLS concepts and is prepared for another stage. In the classification phase, the sentences represented as vectors of features are classified into summary and non-summary classes using the naïve Bayes classification method.\nOne of the main components of our summarization process is the naïve Bayes classifier. We begin this section with a brief review of the naïve Bayes classification method. Then, we explain our proposed biomedical summarization process in detail."
    }, {
      "heading" : "3.1. The naïve Bayes classifier",
      "text" : "The naïve Bayes [16] is an easy to build and robust classifier. It is known as a proven data mining algorithm [44]. Based on this method, the training phase, and the actual classification could be performed efficiently. There is no need to complicated iterative parameter estimation schemes. In general, Bayesian classifier is based on the Bayes theorem, defined by Eq. 1 below: | = | (1) where C and X are random variables. In classification tasks, they refer to observing class C and instance X,\nrespectively. X is a vector containing the values of features. | is the posterior probability of observing class C given instance X. In classification, it could be interpreted as the probability of instance X being in class C, and is\nwhat the classifier tries to determine. | is the likelihood, which is the probability of observing instance X given class C. It is computed from the training data. and are the prior probabilities of observing class C and instance X, respectively. They measure how frequent the class C and instance X are within the training data. Using Eq. 1, the classifier can compute the probability of each class of target variable C given instance X, and the\nmost probable class, the class that maximizes | , should be selected as the result of classification. This decision rule is known as Maximum A Posteriori or MAP. It is represented as follows:\n← argmax | = = (2)\nManuscipt 8 26 April 2016\nwhere is the jth class (or value) of target variable C. in Eq. 2, the denominator is removed because it is constant and does not depend on .\nWe represent the instance X as X=<x1, x2, …, xn>, where the xi is the ith feature of X. Assume each instance X\nhas a vector of values for 20 boolean features, and the target variable C is also boolean. When modeling | , we need to estimate approximately 2 × 2 = 2 = 2,097,152 parameters, that heavily increases the complexity of classifier. Using the naïve Bayes classifier dramatically reduces the number of parameters to be estimated to 2 × 20 = 40. The naïve Bayes classifier achieves this reduction in the number of parameters to be estimated by making a conditional independence assumption. It means that the probability of each value of feature xi is independent of the value of any other feature, given the class variable cj. In fact, it assumes that the effect of the value of predictor xi on a given class cj is independent of the values of other predictors. Therefore, the naïve Bayes classifier finds the most probable class for target variable by simplifying the joint probability calculation as follows:\n← argmax = | = (3) The conditional independence assumption plays a crucial role here because it simplifies the representation of | , and the problem of estimating this value from training data. A well-known measure to assess the confidence of classification in the naïve Bayes classifier is the Posterior Odds Ratio. The Posterior Odds Ratio shows a measure of the strength of evidence in favor of a particular classification compared to another method [45]. It is calculated as follows: = = | = | (4) where is the posterior odds ratio that measures the strength of evidence in favor of classifying the instance as a class variable = against classifying the instance as class variable = . A value of exactly 1.0 could be interpreted as the evidence from the posterior distribution supports both classes and equally. A value greater than 1.0 demonstrates that the posterior distribution favors the = classification, while a value less than 1.0 demonstrates that the posterior distribution favors the = classification."
    }, {
      "heading" : "3.2. Summarization Method",
      "text" : "In this subsection, we present our naïve Bayes summarization method. The process of document summarization is accomplished through six steps: (1) document preprocessing, (2) mapping text to biomedical concepts, (3) feature identification, (4) preparing sentences for classification, (5) sentence classification using naïve Bayes, and (6) creating the summary. Fig. 1 illustrates the architecture of our summarization method. A detailed description of each step will be given in the following subsections.\n3.2.1. Document preprocessing\nManuscipt 9 26 April 2016\nBefore applying the summarization process, a preliminary step is needed to be done, in order to prepare the\ninput document for the subsequent steps. In the preprocessing step, the following actions are done:\n• The portions of the text that seems to be unnecessary for inclusion in the summary are removed. These include\ntitle, the author information, abstract, keywords, heading of sections, competing interests, acknowledgments, and references. • Figures and tables are removed temporarily. If the final summary refers to a figure or table, it would be showed in\nthe summary. The figures and tables will be removed for evaluation.\n• If the document includes an abbreviations section, the shortened forms in the document body will be replaced\nwith their expansions. For example, if the abbreviations section includes GWAS and expands it as Genome-Wide Association Studies for a given document that contains the phrase the success of GWAS has led to paid less attention to linkage in complex disorder, and then that phrase would become the success of genome-wide association studies has led to paid less attention to linkage in complex disorder. • After replacing the shortened form of abbreviations with their expansions, the abbreviation section is also\nremoved, and the plain text of document body will remain.\nAlthough this preprocessing step is applied to biomedical articles, it can be customized for any textual document based on the logical structure of the text. We have customized the preprocessing step for biomedical articles because: (1) a vast amount of materials are commonly used in this domain, (2) one of the main reasons for proposing summarization methods in the biomedical field is to overcome the information overload in the biomedical literature, and (3) we will evaluate our method on biomedical articles.\n3.2.2. Mapping text to biomedical concepts\nManuscipt 10 26 April 2016\nIn this step, the document resulted from the preprocessing step is mapped to concepts of the UMLS Metathesaurus. Each concept has a semantic type extracted with it that determines the semantic category of concept. The semantic types are included in the UMLS Semantic Network. In this paper, we use the 2014 version of MetaMap program for the mapping step and the 2014AB UMLS release as the knowledge base.\nWhen the MetaMap is faced with lexical ambiguity, it often fails to specify a unique mapping for a given phrase [46]. For example, for the text fragment The significance of the identification of APOE, the MetaMap returns two candidate concepts with equal scores for APOE, i.e. Apolipoprotein E with semantic type aapp (Amino Acid, Peptide, or Protein), and APOE gene with semantic type gngm (Gene or Genome). This behavior occurs because some words may have multiple meanings, and each meaning depends on the context in which it appears [47]. The MetaMap returns all mappings in such cases that it cannot distinguish the context in which the phrase appears. If the MetaMap is invoked with the word sense disambiguation option, i.e. -y flag, it uses the Journal Descriptor Indexing (JDI) algorithm [48] to resolve Metathesaurus ambiguity. We use the -y flag to force the MetaMap to select a single mapping in cases that the number of candidate concepts for a given phrase is more than one. Although, there are cases that the JDI may fail to return a single mapping, and in such situations our method selects all mappings returned by MetaMap. It has been shown in [47] that the All Mappings is relatively an appropriate Word Sense Disambiguation (WSD) strategy for concept identification. In Fig. 2, a sample sentence and its identified concepts are illustrated.\nAfter mapping the document text to concepts, those concepts belong to semantic types which are very generic must be discarded, because they are excessively broad and almost frequently appear in every document. These semantic types have been identified empirically by [7], including functional concept, qualitative concept,\nquantitative concept, temporal concept, spatial concept, mental process, language, idea or concept, and intellectual product. Therefore, in Fig. 2, the following concepts are discarded: Widening, analysis aspect, Further, Relationships and Etiology aspects.\n3.2.3. Feature identification\nManuscipt 11 26 April 2016\nAfter concept extraction and dropping the generic concepts, the important concepts are identified and are selected as the classification features. First, all remained concepts are added to a list, named All_concept_list. Second, the frequency of each concept in the All_concept_list is calculated by counting the number of sentences which the concept has appeared. Third, the important concepts are specified based on this rule: a given concept is important if its frequency is equal or greater than the value of threshold . In the following, the value of threshold is presented: = !\"#$ + 2 × '()_)# !\"#$ (5) where the !\"#$ is the average of all concept frequencies in the All_concept_list, and the '()_)# !\"#$ is the standard deviation of all concept frequencies in the All_concept_list. We select the value of threshold presented in Eq. 5 based on a set of preliminary experiments (Section 3.3.3).\nFinally, concepts which their frequency is equal or greater than the in Eq. 5 are selected as features, in order to represent the sentences as vectors of features for the classification step. In Fig. 3, the identified important concepts along with their semantic types and frequencies are represented for a sample document concerning genetic overlap between autism, schizophrenia and bipolar disorder. For each concept, the semantic type is represented in brackets.\nFor this sample document, the !\"#$ , '()_)# !\"#$ and are equal to 2.435, 3.444 and 9.323, respectively. There are 234 concepts in the All_concept_list, eight of which are selected as features (Fig. 3)."
    }, {
      "heading" : "3.2.4. Preparing sentences for classification",
      "text" : "After identifying the main concepts and considering them as features, the sentences of the document must be represented in an appropriate form for classification. There are some sentences in which none of the important concepts appear. Thus, the value of all features for these sentences would be false, and they are discarded for this step and also for the next steps. For example, the sample document, which we pointed in the previous step, consists of 85 sentences of which 16 sentences do not contain any important concepts, and are identified as unimportant sentences. Thus, 69 sentences remain for preparation and classification. In this step, each remaining sentence must be represented as a vector of features. According to the order of sentences being appeared in the original document, each sentence has a number, and the number of each sentence is assigned to the corresponding vector of features. We point to the vector of features corresponding to the ith sentence as ith sentence-vector. For example, for the sample document here contains 69 sentence-vectors, each one has eight features. Every feature corresponds to an important concept, which was identified in the previous step. All features are boolean, and if an important concept appears in the ith sentence, the value of its corresponding feature in ith sentence-vector would be true. Otherwise, it would be false. For each sentence-vector, there is a target class or a target variable, named Summary, which is initially unknown for all sentence-vectors and would be determined in the classification step. After classification, the value of Summary class variable would identify as Yes or No.\nManuscipt 12 26 April 2016\nFor example, in the sample document, the 46th sentence is: “Therefore, just as for NRXN1 deletions, it is apparent that these large CNVs confer risk of a range of neurodevelopmental phenotypes, including autism, mental retardation, and schizophrenia”. In this sentence, four important concepts could be seen, including Autistic Disorder, Schizophrenia, Deletion Mutation and NRXN1 gene. Hence, in the 46th sentence-vector, the value of these four features would be True, and the value of other features would be False. The 46th sentence-vector, corresponding to the 46th sentence in the sample document, is represented in Fig. 4. The feature values for all sentence-vectors are assigned just in the same way as the example.\nAfter this step, we have a collection of sentence-vectors with their feature values specified. Their class variable is unknown, and they must be classified as summary sentences (True) or non-summary sentences (False). Every document has its particular set of concepts, and therefore, the features of each text are different from others. Thus, there are no training data in our method. In the next subsection, we will show how we give a hint to the naïve Bayes classifier, and it can attain any information required to classify the sentences."
    }, {
      "heading" : "3.2.5. Sentence classification using naïve Bayes",
      "text" : "As mentioned earlier, our proposed method does not use any training data for learning. On the other hand, the naïve Bayes classifier needs to know the distribution of feature values and the values of class variable in training data, in order to classify the previously unseen instances based on this information. We estimate the prior probabilities from those sentence-vectors which must be classified. Moreover, we make an assumption that simplifies the estimation of likelihood probabilities.\nIn summarization systems, there is a parameter called Compression Rate, which is used to determine what percentage of text must be extracted from the primary document as the final summary. Initially, we do not know = in Eq. 3, the prior probability of class variable values. However, we know what percentage of document sentences must be selected as summary sentences. In fact, with the hint that we give to the system as the\nCompression Rate, the classifier can estimate the '+,,-\". = /#0 and '+,,-\". = 12 . For instance, the total number of sentences in our sample document is 85. Suppose the Compression Rate is 0.3. It means that 30% of the text (about 26 sentences) should be selected for the final summary. In the preparation step, we discarded the sentences that did not include any important concepts, and 69 sentence-vectors remained for the classification step. The classifier does not know which of this 69 sentence-vectors has Yes value for the Summary class variable, but it\nManuscipt 13 26 April 2016\nknows 26 sentence-vectors have Yes value for the Summary class variable. Therefore, the '+,,-\". = /#0 would be equal to 26 / 69 = 0.377 and the '+,,-\". = 12 would be equal to 43 / 69 = 0.623. As noted earlier, we make an assumption about | = in Eq. 3 that simplifies the estimation of likelihood probabilities. We assume the distribution of important concepts within the final summary to be equal to their distribution within the main document. For example, if an important concept has appeared in 25% of main document sentences, it must appear in 25% of the summary sentences. Based on this assumption, we can estimate the likelihood probabilities, i.e. the probability of observing an important concept given class Yes or No. For example, the concept Schizophrenia has appeared in 30 sentences in the sample document. Its distribution within all sentence-vectors is equal to 30 / 69 = 0.435, and based on our assumption, we want to concept Schizophrenia be appeared in 43.5% of the final summary sentences. Therefore, the probability of observing concept Schizophrenia\ngiven class Yes, i.e. the ' 345263#\"74- = 8\"+#|'+,,-\". = /#0 , would be equal to 0.435. Likewise, the probability of not observing concept Schizophrenia given class Yes, i.e. ' 345263#\"74- = 9-:0#|'+,,-\". =/#0 , would be equal to 1 - 0.435 = 0.565. The likelihood probabilities of observing and not observing a concept given class value No are estimated as the same way.\nAt this step, the classifier can estimate the posterior probability of class values given a sentence-vector. If the classifier chooses the value that maximizes the posterior probability of Summary variable given ith sentence-vector similar to Eq. 3, the number of sentences which are classified as Yes may be less than the number of sentences which should be selected for the final summary. This comes true because, in a sentence, the number of important concepts which have feature value True is often less than the number of important concepts with a feature value of False. Therefore, the classifier will decide about the Summary class value of sentence-vectors in a different way compared with Eq. 3. We incorporate two coefficients that discriminate between presence and absence of less and more important concepts in the estimation of posterior probabilities.\nTo clarify the reason for using the coefficients, we express this example: assume that the classifier estimates the posterior probability of class value Yes for a given sentence-vector with two features (concept A and B). The prior probability of observing class value Yes is 0.29. Concept A occurs in the sentence-vector, and its frequency is 0.38. Concept B does not happen in the sentence-vector, and its frequency is 0.12. The prior probability of observing\nManuscipt 14 26 April 2016\nconcept A (0.38) is multiplied by the prior probability of not finding concept B (1 - 0.12 = 0.88). The posterior probability of class value Yes for the sentence-vector would be equal to 0.29 × 0.38 × 0.88 = 0.096976. However, if concept B occurs in the sentence-vector, the posterior probability of class value Yes for the sentence-vector would be equal to 0.29 × 0.38 × 0.12 = 0.013224. In this example, the posterior probability of class value Yes in the situation of presence of both concept A and B is less than the posterior probability of class value Yes in the case of absence of concept B. This happens because the classifier does not discriminate between the presence and absence of less relevant (low-frequent) and more important (high-frequent) concepts. Therefore, for estimating the posterior probabilities of class value Yes and No, the classifier must increase and decrease the impact of concepts based on their frequency and whether they occur in the sentence or not. We will assess the impact of using the aforementioned coefficients on the accuracy of our summarization method in a set of preliminary experiments (Section 3.3.3).\nFor estimating the posterior probabilities, first the posterior probability of class value Yes given ith sentence-\nvector is determined by rewriting the Eq. 3, as follows:\n'+,,-\". = /#0|'; = (6) '+,,-\". = /#0 ! |'+,,-\". = /#0\nwhere '; is the ith sentence-vector, and '+,,-\". = /#0|'; is the posterior probability of classifying the ith sentence-vector as Yes, given '; . ! is the kth feature in the ith sentence-vector, and ! |'+,,-\". = /#0 shows the likelihood probability, the probability of observing the feature ! = 8\"+# or ! = 9-:0#, given class variable '+,,-\". = /#0. The value of coefficient depends on whether the ! is True or False. The value of is specified as follows: = < !\"#$ 4! ! = 8\"+#1!\"#$ 4! ! = 9-:0# (7) where the !\"#$ is the frequency of concept corresponding to ! (the kth feature in the ith sentence-vector). Depending on whether the value of ! is True or False, the affects ! |'+,,-\". = /#0 in two ways: 1. When ! is True, the frequency of corresponding concept is multiplied by ! |'+,,-\". = /#0 . Thus, the ! |'+,,-\". = /#0 would be increased, and the '+,,-\". = /#0|'; would be also increased.\nConsequently, the presence of more frequent concepts increases the chance of selecting a sentence for the final summary.\n2. When ! is False, the inverted frequency of corresponding concept is multiplied by ! |'+,,-\". = /#0 , and as a result, the value of ! |'+,,-\". = /#0 would be decreased. The value of '+,,-\". = /#0|'; would be also decreased. It could be seen that a higher frequency will decrease\nManuscipt 15 26 April 2016\n! |'+,,-\". = /#0 with the higher rate. Hence, the absence of more frequent concepts decreases the chance of selecting a sentence for the final summary.\nIn the next step, the posterior probability of class value No given ith sentence-vector is estimated as follows:\n'+,,-\". = 12|'; = (8) '+,,-\". = 12 ! |'+,,-\". = 12\nwhere the '; is the ith sentence-vector, and '+,,-\". = 12|'; is the posterior probability of classifying the ith sentence-vector as No, given '; . The ! is the kth feature in the ith sentence-vector, and ! |'+,,-\". = 12 is the likelihood probability, the probability of observing the feature ! = 8\"+# or ! = 9-:0#, given class variable '+,,-\". = 12. The value of coefficient depends on whether the ! is True or False. The value of is specified as follows:\n= >? @ 1!\"#$ 4! ! = 8\"+# !\"#$ 4! ! = 9-:0# (9)\nwhere the !\"#$ is the frequency of concept corresponding to ! (the kth feature in the ith sentence-vector). Similar to , depending on whether the value of ! is True or False, the affects the ! |'+,,-\". = 12 in two ways: 1. When ! is True, the inverted frequency of corresponding concept is multiplied by ! |'+,,-\". = 12 , and as a result, the ! |'+,,-\". = 12 would be decreased. The value of '+,,-\". = 12|'; would be also decreased. The higher frequency decreases the ! |'+,,-\". = 12 with the higher rate. Hence, the presence of more frequent concepts decreases the probability of not selecting a sentence for the final summary.\n2. When ! is False, the frequency of corresponding concept is multiplied by ! |'+,,-\". = 12 . Thus, the value of ! |'+,,-\". = 12 would be increased, and '+,,-\". = 12|'; would be also increased. Consequently, the absence of more frequent concepts increases the probability of not selecting a sentence for the\nfinal summary.\nAfter estimating the probability of classifying each sentence-vector as Yes and No, we need to decide which phrases to select for inclusion in the final summary. As mentioned earlier, if the classifier chooses the class value which maximizes the posterior probability of Summary class variable given ith sentence-vector, the number of sentences which are classified as Yes may be less than the number of sentences required for the final summary. Therefore, we employ the Posterior Odds Ratio, introduced in Section 3.1, to classify the sentence-vectors. For the ith sentence-vector, it could be calculated by rewriting Eq. 4 as follows:\n= '+,,-\". = /#0|'; '+,,-\". = 12|'; (10)\nManuscipt 16 26 April 2016\nwhere the is the posterior odds ratio of ith sentence-vector. The values of '+,,-\". = /#0|'; and '+,,-\". = 12|'; are the posterior probability of classifying ith sentence-vector as Yes and No given '; , as estimated earlier.\nAs mentioned earlier, The Posterior Odds Ratio demonstrates a measure of the strength of evidence in favor of a particular classification. Therefore, the greater POR for a sentence-vector indicates the higher strength of evidence in favor of classifying the sentence-vector as Yes.\nAfter calculating the POR value for all sentence-vectors, the classifier can decide which sentences to select for inclusion in the final summary. The sentence-vectors are sorted in descending order of their POR. The top-ranked N sentence-vectors are classified as Yes, and the remaining sentence-vectors are classified as No, where the N is the number of sentences which must be selected for the final summary and is specified by the Compression Rate. The Summary class value is determined for all sentence-vectors, and the summarizer can build the final summary."
    }, {
      "heading" : "3.2.6. Creating the summary",
      "text" : "The last step is summary creation. It has been determined that which sentences should be selected to make the final summary. Those sentences with their corresponding sentence-vector classified as Yes are added to the summary. The sentences are arranged in the same order as they appear in the primary document. Finally, the figures and tables in the main document that are referred to in the summary are added to finalize the summarization process."
    }, {
      "heading" : "3.3. Evaluation methodology",
      "text" : "The evaluation methods of summarization systems could be divided into two broad categories: Intrinsic and Extrinsic [49]. For the intrinsic evaluation, the quality of generated summaries is assessed according to certain criteria such as accuracy, relevancy, comprehensiveness, and readability. Such criteria could be represented by two main properties: informativeness and coherence. In intrinsic evaluation, the generated summaries are evaluated by comparing with a gold standard or rating by a human.\nIn the extrinsic evaluation, the impact of a summarization system on the performance of a specific informationseeking task is assessed. Extrinsic evaluation could be performed according to measures such as decision-making accuracy, success rate, and time-to-completion. We evaluated the performance of our biomedical summarization method using intrinsic evaluation."
    }, {
      "heading" : "3.3.1. Evaluation corpus",
      "text" : "The most common method of evaluating the summaries generated by an automatic summarizer (also known as system or peer summaries) is to compare them against manually generated summaries (also called model or reference summaries). The metric of such evaluation method is the similarity between the content of system and model summaries. The more content shared between the system and model summaries, the better the system summary is assumed to be. Obtaining the manually generated summaries is a challenging and time-consuming task because they have to be written by human experts. Moreover, the human-generated model summaries are highly\nManuscipt 17 26 April 2016\nsubjective. To the authors’ knowledge, there is no corpus of model summaries for biomedical documents. However, most scientific papers have an abstract which is usually considered as model summary for evaluation.\nTo evaluate our proposed method, we used a collection of 80 biomedical scientific articles, randomly selected and downloaded from the BioMed Central online library (http://www.biomedcentral.com). According to [50], the size of evaluation corpus is large enough to allow the results of the assessment to be significant. The abstract of each paper was used as model summary for evaluating the system summary generated for that paper."
    }, {
      "heading" : "3.3.2. Evaluation metrics: ROUGE",
      "text" : "As noted earlier, in the intrinsic evaluation of summarization methods, two properties are regarded as the measure of summary quality: coherence and informativeness. Coherence is a property for measuring the readability and cohesion of summary. Informativeness is a feature for representing how much information from the original text is provided by the summary [51]. In spite of advances in evaluating the coherence and readability of automatic summaries [52-54], this evaluation approach is still very preliminary, and the research community has not adopted any standard readability assessment approach yet. On the other hand, advances in automatic evaluation of informativeness are more impressive [55, 56], and the research community has agreed upon a standard approach for this evaluation approach.\nFor performance evaluation, in terms of the informativeness of automatic summaries, we used the RecallOriented Understudy for Gisting Evaluation (ROUGE) package [17]. ROUGE compares a system-generated summary with one or more model summaries and estimates the shared content between the system and model summaries by calculating the proportion of n-grams in common between them. In a comparison of two system summaries generated for the same document by two different summarizers, a system summary is assumed to be better if it contains more shared data with the model summary. The ROUGE metrics produce a value between 0 and 1, and a higher value is preferred as it demonstrates a greater content overlap between the system and model summaries. In this paper, we used four ROUGE metrics: ROUGE-1, ROUGE-2, ROUGE-W-1.2, and ROUGESU4. ROUGE-1 and ROUGE-2 compute the number of shared unigrams (1-grams) and bigrams (2-grams) between the system and model summaries. ROUGE-W-1.2 computes the union of the longest common subsequences between the system and model summaries. It takes into account the presence of consecutive matches. ROUGE-SU4 will measure the overlap of skip-bigrams (pairs of words having intervening word gaps) between the system and model summaries. It allows a skip distance of four between the bigrams.\nIt is worth to note that the Document Understanding Conference (DUC) has adopted ROUGE as the official evaluation metric for text summarization. In spite of its simplicity, ROUGE has shown high correlation with the human judges [17]. In DUC 2005 conference, it achieved a Pearson correlation of 0.97 and a Spearman correlation of 0.95 compared with human evaluation. Nevertheless, ROUGE has a significant drawback. For measuring the overlap between the system and model summaries, ROUGE metrics assess the lexical matching instead of semantic matching. It means for a given document if a system summary is worded differently compared with other system summaries, but carries the identical information, the assigned ROUGE scores may be different.\nManuscipt 18 26 April 2016"
    }, {
      "heading" : "3.3.3. System configuration",
      "text" : "We performed two sets of experiments to evaluate our proposed method. In this subsection, we describe the first and preliminary set of experiments that determines the best system configuration. In Section 3.3.4, we will define the second round of experiments that compares our proposed method against other summarizers.\nWe performed a set of preliminary experiments, in order to determine the optimal value for the threshold involved in recognizing the important concepts for feature selection (Section 3.2.2). A possible choice for the value of this parameter could be calculated using Eq. 5 that we selected for the evaluation. We evaluated the performance of our summarization method under two other possible choices for the value of this parameter. The two other choices could be calculated using (11) and (12) as follows:\n= !\"#$ (11) = !\"#$ + '()_)# !\"#$ (12)\nIn our preliminary experiments, we also assessed the impact of the coefficients and on the performance of our summarization method (Section 3.2.5). We eliminated the from the Eq. 6 and the from the Eq. 8 and evaluated the system with and without the coefficients. We combined the two configurations of the impact assessment of coefficients with the three configurations of the value of threshold , hence a total of six configurations were evaluated.\nThe preliminary experiments were performed according to ROUGE scores. For evaluating the system configuration, we used a separate development set consisted of 25 papers, randomly selected and downloaded from the BioMed Central online library. The abstracts of the articles were used as model summaries."
    }, {
      "heading" : "3.3.4. Comparison with other summarizers",
      "text" : "We compared our biomedical summarization method against six summarizers. Three summarizers are research prototypes, namely SUMMA, SweSum, and BioChain. One of the summarizers is a commercial application, Microsoft AutoSummarize, and two summarizers are baseline, namely Lead baseline and Random baseline. The size of summaries generated by all of the summarizers is set to 30% of the original document. The choice of 30% as the compression rate is based on a well-accepted de facto standard that says the size of a summary should be between 15% and 35% of the size of original text [57]. In the following, a brief description of the six summarizers is presented.\n• SUMMA: SUMMA [42] is a popular research summarizer and is available for public usage. It is used as a plugin\nin the GATE architecture for text engineering [58], and must be implemented as processing resources and language resources. It could be utilized as both single-document and multi-document summarizer. SUMMA is customizable based on several statistical and similarity-based features. The customized features are used for scoring the sentences and extracting them for the summary. The features we used for the evaluation were the\nManuscipt 19 26 April 2016\nfrequency of sentences’ terms, the position of sentences within the document, the similarity of sentences to the first sentence, and the overlap of sentences with the title. • SweSum: SweSum [43] is a multi-lingual summarizer with its text summarization for English, Danish,\nNorwegian and Swedish considered to be state-of-the-art and for Persian, French, German, Spanish and Greek is in a prototype state. SweSum uses several features to score the sentences, and the user can specify the weight of each feature. We used the online version of SweSum (http://swesum.nada.kth.se/index-eng-adv.html) for the evaluation. The type of text was set to ‘Academic’, and these features were used: sentences in the first line of text, sentences containing numerical values, sentences containing keywords extracted by the summarizer. SweSum provides a function named ‘User keywords’ that considers user-defined keywords as a measure to score the sentences. We did not use this feature in our evaluation. • BioChain: BioChain [9] is a biomedical summarizer that uses an NLP method, named Lexical Chaining, for\nsummarization. However, BioChain uses a set of concepts instead of terms and changes the lexical chaining to concept chaining. The concepts are extracted from the original document using UMLS, the semantic types are considered as the head of chains, and concepts with the same semantic type are chained together. Those chains that contain the core concepts of text are identified as strong chains. Then, the most common concepts of each strong chain are identified and used to score the sentences. The high-scoring sentences are extracted, and the final summary is returned. • Microsoft AutoSummarize: Microsoft AutoSummarize is a feature of the Microsoft Word software [59].\nMicrosoft AutoSummarize is based on a word frequency algorithm, and a score is assigned to each sentence of a document based on the words it contains. However, the algorithm is not documented in detail; it is stated in the online help for the product that a higher score are assigned to sentences which contain frequently-used words, compared with sentences which contain less frequent words. Although the word frequency is a simple measure, it is identified as a well-accepted heuristic for summarization. • Lead baseline: The lead baseline algorithm selects the first N sentences of each input document and returns it as\nthe summary of that document.\n• Random baseline: The random baseline generates summaries by randomly selecting N sentences from each\ndocument.\nIn order to test the statistical significance of the results, we used a Wilcoxon signed-rank test with a 95%\nconfidence interval."
    }, {
      "heading" : "4. Results and discussion",
      "text" : "In this section, we first present the results of configuration and the effect of the aforementioned coefficients in the proposed model. The second subsection presents the results of evaluating the system and comparing to existing methods.\n4.1. Configuration results\nManuscipt 20 26 April 2016\nWe performed a set of preliminary experiments, in order to select the best setting for the naïve Bayes summarizer. The initial experiments were conducted to find: (1) the optimal value for the threshold , that is used in Section 3.2.2 for identifying main concepts and selecting them as features, and (2) the impact of two coefficients ( and ), involved in Section 3.2.5, on the summarization performance. The central concepts identified by the algorithm are used as features for the classification step. A higher value of threshold leads to less concepts to be identified as important concepts, hence less features would be used for the classification. We assessed three possible values for the threshold to determine the value with more positive impact on the performance of summarization. The two coefficients used in sentence classification affect the posterior probability of a class value given a sentence vector based on the degree of importance of the concepts appeared in the sentence. We evaluated the impact of presence and absence of the two coefficients on the performance of summarization. The two groups of experiments were performed together, in order to select the best combination of the value of threshold and the coefficients. The results of the experiments are presented in Table 1. For legibility reasons, only ROUGE-2 and ROUGE-SU4 scores are shown.\nIt can be observed from Table 1 that according to the ROUGE scores, the use of coefficients improves the performance of the summarizer, and the best value among the three values of the threshold is two standard deviations above the average of frequencies. We discuss the results of system configuration, shown in Table 1, in two cases: without and with the coefficients.\nWithout the coefficients: when the coefficients are not used, the best value among the three values of threshold is one standard deviation above the average of frequencies. In this case, the choice of the average of frequencies as threshold , increases the number of features. The classifier does not discriminate between the more important and less important features (concepts), and the high number of features negatively affects the performance of the summarizer. This indicates that the high number of features will mislead the classifier and it would classify the sentences according to the distribution of concepts that the majority of them are indeed unimportant. The choice of two standard deviations above the average of frequencies as threshold , will decrease the number of features, and the low number of features also will have a negative impact on the performance of the summarizer. The classifier does not know which features (concepts) are more important and which are less important. Moreover, the number of features is low. Therefore, the classifier’s knowledge is incomplete and the performance of summarizer will decrease. However, the choice of one standard deviation above the average of frequencies, slightly moderates the deficiencies of two other choices. Although, it still suffers from the shortage of knowledge about the importance of features.\nWith the coefficients: when the coefficients are used, the best value among the three values of threshold is two standard deviations above the average of frequencies. In this case, the worst value is the average of frequencies, which increases the number of features. The classifier knows the importance of features (concepts), but the high number of features will mislead the classifier. The classifier decides about the class of each sentence according to the frequencies of a large number of concepts, while only some of them are really important, and the others are redundant and illusory. The choice of one standard deviation above the average of frequencies reduces\nManuscipt 21 26 April 2016\nthe number of features and improves the performance of the summarizer. The number of unimportant concepts is reduced and the classifier can decide more accurately. However, it seems that some of the features are still redundant, and the performance of the summarizer can be further improved. The choice of two standard deviations above the average of frequencies decreases the number of features again, and improves the accuracy of the summarizer. However, as the number of features is reduced, the knowledge about the importance of features will help the classifier to decide more accurate. Based on the results, the positive impact of the coefficients on the accuracy of the classifier, and consequently on the performance of the summarizer, could be observed. The best\ncombination of the value of threshold and the coefficients could be specified as !\"#$ + 2 ×'()_)# !\"#$ for the value of threshold , with the presence of the coefficients."
    }, {
      "heading" : "4.2. Evaluation results",
      "text" : "To evaluate the performance of our summarization method, we compare the ROUGE scores obtained by our method with the ROUGE scores of the other six summarizers. Other summarizers, as described in Section 3.3.4, are SUMMA, SweSum, BioChain, Microsoft AutoSummarize, Lead baseline and Random baseline. For each summarizer, the system-generated summaries were compared with the model summaries by the ROUGE toolkit, and each summarizer was assigned four different scores. The ROUGE scores for all summarizers are shown in\nManuscipt 22 26 April 2016\nTable 2. It can be observed that the naïve Bayes summarizer reports higher ROUGE scores than the other summarizers and baselines. The naïve Bayes summarizer significantly improves all ROUGE metrics compared with SUMMA, SweSum, AutoSummarize, Lead baseline and Random baseline, according to Wilcoxon signedrank test (p < 0.05). Compared with BioChain, the naïve Bayes summarizer significantly improves three ROUGE scores, namely ROUGE-1, ROUGE-2 and ROUGE-SU4, but the improvement is not significant for ROUGE-W1.2.\nThe results in Table 2 show that the two summarizers that use domain knowledge in summarization process, i.e. our naïve Bayes summarizer and BioChain, perform better than the general purpose and baseline summarizers. Moreover, the proposed naïve Bayes summarizer increases the accuracy of summarization, in terms of generated summaries’ informative content quality, as compared to another biomedical summarizer. The results obtained by our naïve Bayes summarizer show its effectiveness as a classification method for such modeling requirements. In many cases, several concepts within a biomedical textual document represent the main topics of text. It seems that identifying these important concepts and utilizing them to show the sentences as vectors of features, is a more accurate approach to model the biomedical summarization problem. Moreover, the simplicity of naïve Bayes classification method helps the summarizer to select the most informative sentences based on the distribution of important concepts within the source text. Therefore, the informativeness of generated summaries is increased, and consequently, the performance of summarization is improved."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this paper, a novel biomedical text summarization method was proposed based on the naïve Bayes classifier. Our method extracts biomedical concepts within the document using UMLS and identifies the important concepts to show the main topics of text. The identified important concepts are then used as features to classify the sentences as summary and non-summary. There is no need to training data, and the naïve Bayes classifier estimates the prior and posterior probabilities based on the distribution of important concepts within the original document. Besides, a useful hint that helps the estimation of probabilities is the distribution of important concepts within the summary must be same as within the source text. The proposed method was evaluated by summarizing a collection of 80 scientific biomedical papers, selected from BioMed Central online library. Comparing the results showed that the proposed naïve Bayes summarizer actually improved the performance of summarization, compared with generalpurpose summarizers and baselines. It confirms that in the biomedical domain, the use of domain knowledge and concept-level analysis rather than term-level analysis of text can be very useful to improve the informativeness of automatically generated summaries. Moreover, our proposed method performed better than BioChain, which also uses domain knowledge. It indicates that the use of essential concepts to classify the sentences by the naïve Bayes classifier could be a viable approach to automatic summarization. There is no need for training data to estimate the required probabilities, and the method uses the distribution of important concepts within the source text to calculate the probabilities. It also showed a considerable improvement in the quality of summarization.\nMore accurately, we can now answer the questions raised in Section 1. Important concepts are the features that determine which sentences are summary sentences and which are non-summary. There are no training data for such\nManuscipt 23 26 April 2016\ntype of summarization, and in fact, a learned model is not applicable to be generalized to classify the sentences of a new document. It is possible to model this summarization approach as a classification problem and deal with it by the naïve Bayes classifier. The classifier estimates the probabilities and classifies the sentences according to the distribution of essential concepts within the original document.\nWhile our proposed biomedical summarization method performs well in single-document summarization, it seems that in multi-document summarization that the redundant information is inevitable, the performance of summarizer may be decreased due to selecting the sentences containing redundant information. We will concentrate on addressing this problem in future work."
    } ],
    "references" : [ {
      "title" : "Text summarization in the biomedical domain: a systematic review of recent research",
      "author" : [ "R. Mishra", "J. Bian", "M. Fiszman", "C.R. Weir", "S. Jonnalagadda", "J. Mostafa" ],
      "venue" : "Journal of biomedical informatics, vol. 52, pp. 457-467, 2014.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Summarization from medical documents: a survey",
      "author" : [ "S. Afantenos", "V. Karkaletsis", "P. Stamatopoulos" ],
      "venue" : "Artificial intelligence in medicine, vol. 33, pp. 157-177, 2005.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Application of text mining in the biomedical domain",
      "author" : [ "W.W. Fleuren", "W. Alkema" ],
      "venue" : "Methods, vol. 74, pp. 97-106, 2015.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The use of domain-specific concepts in biomedical text summarization",
      "author" : [ "L.H. Reeve", "H. Han", "A.D. Brooks" ],
      "venue" : "Information Processing & Management, vol. 43, pp. 1765-1776, 2007.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Text summarization using Wikipedia",
      "author" : [ "Y. Sankarasubramaniam", "K. Ramanathan", "S. Ghosh" ],
      "venue" : "Information Processing & Management, vol. 50, pp. 443-461, 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A semantic graph-based approach to biomedical summarisation",
      "author" : [ "L. Plaza", "A. Díaz", "P. Gervás" ],
      "venue" : "Artificial intelligence in medicine, vol. 53, pp. 1-14, 2011.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Automatic summarization of MEDLINE citations for evidence-based medical treatment: A topic-oriented evaluation",
      "author" : [ "M. Fiszman", "D. Demner-Fushman", "H. Kilicoglu", "T.C. Rindflesch" ],
      "venue" : "Journal of biomedical informatics, vol. 42, pp. 801- 813, 2009.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "BioChain: lexical chaining methods for biomedical text summarization",
      "author" : [ "L. Reeve", "H. Han", "A.D. Brooks" ],
      "venue" : "Proceedings of the 2006 ACM symposium on Applied computing, 2006, pp. 180-184.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Concept frequency distribution in biomedical text summarization",
      "author" : [ "L.H. Reeve", "H. Han", "S.V. Nagori", "J.C. Yang", "T.A. Schwimmer", "A.D. Brooks" ],
      "venue" : "Proceedings of the 15th ACM international conference on Information and knowledge management, 2006, pp. 604-611.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A trainable document summarizer",
      "author" : [ "J. Kupiec", "J. Pedersen", "F. Chen" ],
      "venue" : "Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, 1995, pp. 68-73.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Automatic text summarization using a machine learning approach",
      "author" : [ "J.L. Neto", "A.A. Freitas", "C.A. Kaestner" ],
      "venue" : "Advances in Artificial Intelligence, ed: Springer, 2002, pp. 205-215.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Applying regression models to query-focused multi-document summarization",
      "author" : [ "Y. Ouyang", "W. Li", "S. Li", "Q. Lu" ],
      "venue" : "Information Processing & Management, vol. 47, pp. 227-237, 2011.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Automatic extraction and learning of keyphrases from scientific articles",
      "author" : [ "Y. HaCohen-Kerner", "Z. Gross", "A. Masa" ],
      "venue" : "Computational Linguistics and Intelligent Text Processing, ed: Springer, 2005, pp. 657-669.  Manuscipt  24  26 April 2016",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "The unified medical language system (umls) project",
      "author" : [ "S.J. Nelson", "T. Powell", "B. Humphreys" ],
      "venue" : "Encyclopedia of library and information science, pp. 369-378, 2002.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Generative and discriminative classifiers: naive Bayes and logistic regression, 2005",
      "author" : [ "T. Mitchell" ],
      "venue" : "Manuscript available at <http://www.cs. cm. edu/~ tom/NewChapters.html>.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "C.-Y. Lin" ],
      "venue" : "Text summarization branches out: Proceedings of the ACL-04 workshop, 2004.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The automatic creation of literature abstracts",
      "author" : [ "H.P. Luhn" ],
      "venue" : "IBM Journal of research and development, vol. 2, pp. 159- 165, 1958.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1958
    }, {
      "title" : "New methods in automatic extracting",
      "author" : [ "H.P. Edmundson" ],
      "venue" : "Journal of the ACM (JACM), vol. 16, pp. 264-285, 1969.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1969
    }, {
      "title" : "Centroid-based summarization of multiple documents",
      "author" : [ "D.R. Radev", "H. Jing", "M. Styś", "D. Tam" ],
      "venue" : "Information Processing & Management, vol. 40, pp. 919-938, 2004.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The use of MMR, diversity-based reranking for reordering documents and producing summaries",
      "author" : [ "J. Carbonell", "J. Goldstein" ],
      "venue" : "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, 1998, pp. 335-336.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "LexRank: Graph-based lexical centrality as salience in text summarization",
      "author" : [ "G. Erkan", "D.R. Radev" ],
      "venue" : "Journal of Artificial Intelligence Research, pp. 457-479, 2004.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The PageRank citation ranking: bringing order to the web",
      "author" : [ "L. Page", "S. Brin", "R. Motwani", "T. Winograd" ],
      "venue" : "1999.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "TextRank: Bringing order into texts",
      "author" : [ "R. Mihalcea", "P. Tarau" ],
      "venue" : "2004.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Authoritative sources in a hyperlinked environment",
      "author" : [ "J.M. Kleinberg" ],
      "venue" : "Journal of the ACM (JACM), vol. 46, pp. 604- 632, 1999.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A survey of text summarization extractive techniques",
      "author" : [ "V. Gupta", "G.S. Lehal" ],
      "venue" : "Journal of Emerging Technologies in Web Intelligence, vol. 2, pp. 258-268, 2010.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "MCMR: Maximum coverage and minimum redundant text summarization model",
      "author" : [ "R.M. Alguliev", "R.M. Aliguliyev", "M.S. Hajirahimova", "C.A. Mehdiyev" ],
      "venue" : "Expert Systems with Applications, vol. 38, pp. 14514-14522, 2011.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program",
      "author" : [ "A.R. Aronson" ],
      "venue" : "Proceedings of the AMIA Symposium, 2001, p. 17.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Using lexical chains for text summarization",
      "author" : [ "R. Barzilay", "M. Elhadad" ],
      "venue" : "Advances in automatic text summarization, pp. 111-121, 1999.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Using domain knowledge for text summarization in medical domain",
      "author" : [ "K. Sarkar" ],
      "venue" : "International Journal of Recent Trends in Engineering, vol. 1, pp. 200-205, 2009.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Comparison of automatic summarisation methods for clinical free text notes",
      "author" : [ "H. Moen", "L.-M. Peltonen", "J. Heimonen", "A. Airola", "T. Pahikkala", "T. Salakoski" ],
      "venue" : "Artificial Intelligence in Medicine, 2016.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Comparing different knowledge sources for the automatic summarization of biomedical literature",
      "author" : [ "L. Plaza" ],
      "venue" : "Journal of biomedical informatics, vol. 52, pp. 319-328, 2014.  Manuscipt  25  26 April 2016",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Abstraction summarization for managing the biomedical research literature",
      "author" : [ "M. Fiszman", "T.C. Rindflesch", "H. Kilicoglu" ],
      "venue" : "Proceedings of the HLT-NAACL workshop on computational lexical semantics, 2004, pp. 76-83.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text",
      "author" : [ "T.C. Rindflesch", "M. Fiszman" ],
      "venue" : "Journal of biomedical informatics, vol. 36, pp. 462- 477, 2003.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Text summarization as a decision support aid",
      "author" : [ "T.E. Workman", "M. Fiszman", "J.F. Hurdle" ],
      "venue" : "BMC medical informatics and decision making, vol. 12, p. 41, 2012.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Degree centrality for semantic abstraction summarization of therapeutic studies",
      "author" : [ "H. Zhang", "M. Fiszman", "D. Shin", "C.M. Miller", "G. Rosemblat", "T.C. Rindflesch" ],
      "venue" : "Journal of biomedical informatics, vol. 44, pp. 830-838, 2011.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Clustering cliques for graph-based summarization of the biomedical research literature",
      "author" : [ "H. Zhang", "M. Fiszman", "D. Shin", "B. Wilkowski", "T.C. Rindflesch" ],
      "venue" : "BMC bioinformatics, vol. 14, p. 1, 2013.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A robust and adaptable summarization tool",
      "author" : [ "H. Saggion" ],
      "venue" : "Traitement Automatique des Langues, vol. 49, 2008.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Swesum: A text summerizer for swedish: KTH",
      "author" : [ "H. Dalianis" ],
      "venue" : null,
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2000
    }, {
      "title" : "Top 10 algorithms in data mining",
      "author" : [ "X. Wu", "V. Kumar", "J.R. Quinlan", "J. Ghosh", "Q. Yang", "H. Motoda" ],
      "venue" : "Knowledge and information systems, vol. 14, pp. 1-37, 2008.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Data mining and predictive analytics",
      "author" : [ "D.T. Larose", "C.D. Larose" ],
      "venue" : null,
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2015
    }, {
      "title" : "National library of medicine",
      "author" : [ "M.J. Shooshan SE", "Aronson AR" ],
      "venue" : "Technical report. Ambiguity in the UMLS Metathesaurus; 2009 Edition,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2009
    }, {
      "title" : "Resolving ambiguity in biomedical text to improve summarization",
      "author" : [ "L. Plaza", "M. Stevenson", "A. Díaz" ],
      "venue" : "Information Processing & Management, vol. 48, pp. 755-766, 2012.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Word sense disambiguation by selecting the best semantic type based on Journal Descriptor Indexing: Preliminary experiment",
      "author" : [ "S.M. Humphrey", "W.J. Rogers", "H. Kilicoglu", "D. Demner‐Fushman", "T.C. Rindflesch" ],
      "venue" : "Journal of the American Society for Information Science and Technology, vol. 57, pp. 96-113, 2006.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Evaluating natural language processing systems: An analysis and review",
      "author" : [ "K.S. Jones", "J.R. Galliers" ],
      "venue" : "vol. 1083: Springer Science & Business Media,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 1995
    }, {
      "title" : "Looking for a few good metrics: Automatic summarization evaluation-how many samples are enough",
      "author" : [ "C.-Y. Lin" ],
      "venue" : "NTCIR, 2004.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Summarization evaluation: An overview",
      "author" : [ "I. Mani" ],
      "venue" : "2001.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Automatic evaluation of linguistic quality in multi-document summarization",
      "author" : [ "E. Pitler", "A. Louis", "A. Nenkova" ],
      "venue" : "Proceedings of the 48th annual meeting of the Association for Computational Linguistics, 2010, pp. 544-554.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Revisiting readability: A unified framework for predicting text quality",
      "author" : [ "E. Pitler", "A. Nenkova" ],
      "venue" : "Proceedings of the conference on empirical methods in natural language processing, 2008, pp. 186-195.",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Quantitative evaluation of grammaticality of summaries",
      "author" : [ "R. Vadlapudi", "R. Katragadda" ],
      "venue" : "Computational Linguistics and Intelligent Text Processing, ed: Springer, 2010, pp. 736-747.",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Summarization evaluation using transformed basic elements",
      "author" : [ "S. Tratz", "E. Hovy" ],
      "venue" : "Proceedings TAC, 2008.",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Summarization system evaluation revisited: Ngram graphs",
      "author" : [ "G. Giannakopoulos", "V. Karkaletsis", "G. Vouros", "P. Stamatopoulos" ],
      "venue" : "ACM Transactions on Speech and Language Processing (TSLP), vol. 5, p. 5, 2008.",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The Oxford handbook of computational linguistics",
      "author" : [ "R. Mitkov" ],
      "venue" : null,
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Biomedical information available for researchers and clinicians is accessible from a variety of sources such as scientific literature databases, Electronic Health Record (EHR) systems, web documents, e-mailed reports and multimedia documents [1, 2].",
      "startOffset" : 242,
      "endOffset" : 248
    }, {
      "referenceID" : 1,
      "context" : "Biomedical information available for researchers and clinicians is accessible from a variety of sources such as scientific literature databases, Electronic Health Record (EHR) systems, web documents, e-mailed reports and multimedia documents [1, 2].",
      "startOffset" : 242,
      "endOffset" : 248
    }, {
      "referenceID" : 2,
      "context" : "It is widely used as a rich source for assessing the new comers in a particular field, gathering information for constructing research hypotheses and collecting information for interpretation of experimental results [3].",
      "startOffset" : 216,
      "endOffset" : 219
    }, {
      "referenceID" : 1,
      "context" : "Required information must be accessed easily at the right time, and in the most appropriate form [2].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "Automatic text summarization is a promising approach to overcome the information overload, by reducing the amount of text that must be read [5].",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "It can be used to obtain the gist efficiently on a topic of interest [1].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "A good summary of text must have two main properties: it needs to be short, and it should preserve valuable information of source text [6].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "In other words, they are designed as general-purpose tools [7].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "The characteristics of biomedical domain raise the need to analyze the source text at a conceptual level and to employ domain knowledge in summarization process [7].",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "This is required to extract a rich representation of source text [7-10].",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 6,
      "context" : "This is required to extract a rich representation of source text [7-10].",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "This is required to extract a rich representation of source text [7-10].",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "This is required to extract a rich representation of source text [7-10].",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "Some of the summarization systems have been proposed based on classification methods [11-14].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 10,
      "context" : "Some of the summarization systems have been proposed based on classification methods [11-14].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "Some of the summarization systems have been proposed based on classification methods [11-14].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 12,
      "context" : "Some of the summarization systems have been proposed based on classification methods [11-14].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "In our proposed method, biomedical concepts are extracted from input text by utilizing the UMLS [15], an important and well-known knowledge source in biomedical sciences, maintained by the US National Library of Medicine.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "We use the naïve Bayes classifier [16] to label the sentences as summary or nonsummary.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "The results demonstrate that our method performs better than similar research-oriented, commercial competitors and baseline methods in terms of the most commonly used ROUGE evaluation metrics [17].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 16,
      "context" : "Early work on automatic text summarization dates back to the 1950s and 1960s with the pioneering work of Luhn [18] and Edmundson [19].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 17,
      "context" : "Early work on automatic text summarization dates back to the 1950s and 1960s with the pioneering work of Luhn [18] and Edmundson [19].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 18,
      "context" : "Manuscipt 4 26 April 2016 There are some well-known summarization methods, such as MEAD [20], MMR [21], LexRank [22], PageRank [23], TextRank [24] and HITS [25], widely referenced by the research community in the last two decades.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : "Manuscipt 4 26 April 2016 There are some well-known summarization methods, such as MEAD [20], MMR [21], LexRank [22], PageRank [23], TextRank [24] and HITS [25], widely referenced by the research community in the last two decades.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 20,
      "context" : "Manuscipt 4 26 April 2016 There are some well-known summarization methods, such as MEAD [20], MMR [21], LexRank [22], PageRank [23], TextRank [24] and HITS [25], widely referenced by the research community in the last two decades.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "Manuscipt 4 26 April 2016 There are some well-known summarization methods, such as MEAD [20], MMR [21], LexRank [22], PageRank [23], TextRank [24] and HITS [25], widely referenced by the research community in the last two decades.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 22,
      "context" : "Manuscipt 4 26 April 2016 There are some well-known summarization methods, such as MEAD [20], MMR [21], LexRank [22], PageRank [23], TextRank [24] and HITS [25], widely referenced by the research community in the last two decades.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 23,
      "context" : "Manuscipt 4 26 April 2016 There are some well-known summarization methods, such as MEAD [20], MMR [21], LexRank [22], PageRank [23], TextRank [24] and HITS [25], widely referenced by the research community in the last two decades.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "Text summarization methods can be divided into abstractive and extractive approaches [1, 26].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : "Text summarization methods can be divided into abstractive and extractive approaches [1, 26].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "Another classification of text summarization differentiates single-document and multi-document inputs [1, 2].",
      "startOffset" : 102,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "Another classification of text summarization differentiates single-document and multi-document inputs [1, 2].",
      "startOffset" : 102,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "user-oriented (also known as query-focused summarizers) [1, 2, 27].",
      "startOffset" : 56,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : "user-oriented (also known as query-focused summarizers) [1, 2, 27].",
      "startOffset" : 56,
      "endOffset" : 66
    }, {
      "referenceID" : 25,
      "context" : "user-oriented (also known as query-focused summarizers) [1, 2, 27].",
      "startOffset" : 56,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "These knowledge sources along with over 100 controlled vocabularies, classification systems, and additional information sources have been unified into the Unified Medical Language System (UMLS) [15] by the National Library of Medicine.",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 26,
      "context" : "For mapping biomedical text to the UMLS Metathesaurus concepts (known as automated concept annotation), the MetaMap program [31, 32] has been developed by the National Library of Medicine.",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "These methods are reviewed in a survey of early work [2] and in a systematic review of recently published research [1].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "These methods are reviewed in a survey of early work [2] and in a systematic review of recently published research [1].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "[9] applied the method of lexical chaining [33] to biomedical text, but they used concepts rather than terms.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 27,
      "context" : "[9] applied the method of lexical chaining [33] to biomedical text, but they used concepts rather than terms.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "FreqDist [10] is a context-sensitive approach, proposed to score the sentences according to a frequency distribution model, along with the ability to remove information redundancy.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 3,
      "context" : "[5] combine the BioChain and the FreqDist and propose a hybrid method.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 28,
      "context" : "In a feature-based method [34], in addition to commonly used traditional features, a vocabulary of cue terms and phrases unique to the medical domain is identified and is used as domain knowledge.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "[7].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 29,
      "context" : "[35] present several text summarization methods for summarizing clinical notes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[36], in terms of the quality of the automatically generated summaries.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[37] present a multi-document semantic abstraction summarization system for MEDLINE citations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "Their system relies on the semantic predications extracted by SemRep [38], a parser based on linguistic analysis and domain knowledge contained in the UMLS.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "[8] extend the semantic abstraction summarization system [37] for evidence-based medical treatment.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 31,
      "context" : "[8] extend the semantic abstraction summarization system [37] for evidence-based medical treatment.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 31,
      "context" : "Two other abstractive summarizers based on semantic abstraction summarization system [37] are proposed by Workman et al.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 33,
      "context" : "[39] and Zhang et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "[40].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "An abstractive graph-based clustering method [41] is presented for automatic identification of themes in multi-document summarization.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 36,
      "context" : "Unlike domain-independent summarization methods such as SUMMA [42] and SweSum [43], our proposed method utilizes domain knowledge and analyzes the source text at a conceptual level.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 37,
      "context" : "Unlike domain-independent summarization methods such as SUMMA [42] and SweSum [43], our proposed method utilizes domain knowledge and analyzes the source text at a conceptual level.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 14,
      "context" : "The naïve Bayes [16] is an easy to build and robust classifier.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 38,
      "context" : "It is known as a proven data mining algorithm [44].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 39,
      "context" : "The Posterior Odds Ratio shows a measure of the strength of evidence in favor of a particular classification compared to another method [45].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 40,
      "context" : "When the MetaMap is faced with lexical ambiguity, it often fails to specify a unique mapping for a given phrase [46].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 41,
      "context" : "This behavior occurs because some words may have multiple meanings, and each meaning depends on the context in which it appears [47].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 42,
      "context" : "-y flag, it uses the Journal Descriptor Indexing (JDI) algorithm [48] to resolve Metathesaurus ambiguity.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 41,
      "context" : "It has been shown in [47] that the All Mappings is relatively an appropriate Word Sense Disambiguation (WSD) strategy for concept identification.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "These semantic types have been identified empirically by [7], including functional concept, qualitative concept,",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 43,
      "context" : "The evaluation methods of summarization systems could be divided into two broad categories: Intrinsic and Extrinsic [49].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 44,
      "context" : "According to [50], the size of evaluation corpus is large enough to allow the results of the assessment to be significant.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 45,
      "context" : "Informativeness is a feature for representing how much information from the original text is provided by the summary [51].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 46,
      "context" : "In spite of advances in evaluating the coherence and readability of automatic summaries [52-54], this evaluation approach is still very preliminary, and the research community has not adopted any standard readability assessment approach yet.",
      "startOffset" : 88,
      "endOffset" : 95
    }, {
      "referenceID" : 47,
      "context" : "In spite of advances in evaluating the coherence and readability of automatic summaries [52-54], this evaluation approach is still very preliminary, and the research community has not adopted any standard readability assessment approach yet.",
      "startOffset" : 88,
      "endOffset" : 95
    }, {
      "referenceID" : 48,
      "context" : "In spite of advances in evaluating the coherence and readability of automatic summaries [52-54], this evaluation approach is still very preliminary, and the research community has not adopted any standard readability assessment approach yet.",
      "startOffset" : 88,
      "endOffset" : 95
    }, {
      "referenceID" : 49,
      "context" : "On the other hand, advances in automatic evaluation of informativeness are more impressive [55, 56], and the research community has agreed upon a standard approach for this evaluation approach.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 50,
      "context" : "On the other hand, advances in automatic evaluation of informativeness are more impressive [55, 56], and the research community has agreed upon a standard approach for this evaluation approach.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "For performance evaluation, in terms of the informativeness of automatic summaries, we used the RecallOriented Understudy for Gisting Evaluation (ROUGE) package [17].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 15,
      "context" : "In spite of its simplicity, ROUGE has shown high correlation with the human judges [17].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 51,
      "context" : "The choice of 30% as the compression rate is based on a well-accepted de facto standard that says the size of a summary should be between 15% and 35% of the size of original text [57].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 36,
      "context" : "• SUMMA: SUMMA [42] is a popular research summarizer and is available for public usage.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 37,
      "context" : "• SweSum: SweSum [43] is a multi-lingual summarizer with its text summarization for English, Danish, Norwegian and Swedish considered to be state-of-the-art and for Persian, French, German, Spanish and Greek is in a prototype state.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 7,
      "context" : "• BioChain: BioChain [9] is a biomedical summarizer that uses an NLP method, named Lexical Chaining, for summarization.",
      "startOffset" : 21,
      "endOffset" : 24
    } ],
    "year" : 2016,
    "abstractText" : "Many biomedical researchers and clinicians are faced with the information overload problem. Attaining desirable information from the ever-increasing body of knowledge is a difficult task without using automatic text summarization tools that help them to acquire the intended information in shorter time and with less effort. Although many text summarization methods have been proposed, developing domain-specific methods for the biomedical texts is a challenging task. In this paper, we propose a biomedical text summarization method, based on concept extraction technique and a novel sentence classification approach. We incorporate domain knowledge by utilizing the UMLS knowledge source and the naïve Bayes classifier to build our text summarizer. Unlike many existing methods, the system learns to classify the sentences without the need for training data, and selects them for the summary according to the distribution of essential concepts within the original text. We show that the use of critical concepts to represent the sentences as vectors of features, and classifying the sentences based on the distribution of those concepts, will improve the performance of automatic summarization. An extensive evaluation is performed on a collection of scientific articles in biomedical domain. The results show that our proposed method outperforms several well-known research-based, commercial and baseline summarizers according to the most commonly used ROUGE evaluation metrics. Keywords―Biomedical text summarization; Data mining; Naïve Bayes; Concept extraction; UMLS; Domain knowledge; Sentence classification; 1 Corresponding author. Address: Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan, Iran. Phone : +98-31-3391-9058, Fax: +98-31-3391-2450, Alternate email: nghadiri@gmail.com",
    "creator" : "PDFCreator 2.3.0.103"
  }
}