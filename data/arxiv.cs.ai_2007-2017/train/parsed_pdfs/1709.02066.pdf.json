{
  "name" : "1709.02066.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Formulation of Deep Reinforcement Learning Architecture Toward Autonomous Driving for On-Ramp Merge",
    "authors" : [ "Pin Wang", "Ching-Yao Chan" ],
    "emails" : [ "pin_wang@berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "in production automated driving systems (ADS) that offer freeway-pilot functions. This type of ADS is typically limited to restricted-access freeways only, that is, the transition from manual to automated modes takes place only after the ramp merging process is completed manually. One major challenge to extend the automation to ramp merging is that the automated vehicle needs to incorporate and optimize long-term objectives (e.g. successful and smooth merge) when near-term actions must be safely executed. Moreover, the merging process involves interactions with other vehicles whose behaviors are sometimes hard to predict but may influence the merging vehicle’s optimal actions. To tackle such a complicated control problem, we propose to apply Deep Reinforcement Learning (DRL) techniques for finding an optimal driving policy by maximizing the long-term reward in an interactive environment. Specifically, we apply a Long Short-Term Memory (LSTM) architecture to model the interactive environment, from which an internal state containing historical driving information is conveyed to a Deep Q-Network (DQN). The DQN is used to approximate the Q-function, which takes the internal state as input and generates Q-values as output for action selection. With this DRL architecture, the historical impact of interactive environment on the long-term reward can be captured and taken into account for deciding the optimal control policy. The proposed architecture has the potential to be extended and applied to other autonomous driving scenarios such as driving through a complex intersection or changing lanes under varying traffic flow conditions.\nKeywords— Autonomous Driving; Highway On-Ramp\nMerge; Deep Reinforcement Learning; Long Short-Term Memory; Deep Q-Network; Control Policy\nI. INTRODUCTION\nHighly or fully automated systems, such as Tesla\nAutopilot [1] and google self-driving car [2], are not widely available yet from major automakers. Some automakers are likely to offer partially or highly automated features at Automation Level 2, 3 or 4 in the very near future, such as those provided by Volvo [3] in the Swedish “Drive Me” experiment. Despite the advancement of high automation levels being proposed and demonstrated, the implementation of autonomous driving for highway on-ramp merge still presents considerable challenges. First, the ADS needs to decide on immediate actions with the consideration of delayed impacts on the future vehicle states. For example, the actions such as accelerating, decelerating, or steering that the ego vehicle takes at the current time will affect the success or failure of the merge mission. This process can be handled at relative ease in most cases by experienced human drivers but the algorithms for automatic execution of the merge maneuver in a consistently smooth, safe, and reliable manner can become complex. Second, the ego vehicle’s merge maneuver depends not only on its own states and actions but also on its interaction with surrounding vehicles, which may be cooperative or adversarial. A typical scenario is that a vehicle on the mainline arriving at the merge point from behind may operate cooperatively (e.g. decelerate or change lane) to let the merging vehicle merge or it may act adversarially (e.g. speed\nup) to deter the merging vehicle from entering into the mainline traffic. Such interaction is not trivial, and serious risks may emerge if the ego vehicle fails to respond to the potentially complicated interaction.\nIn this paper, we propose a machine learning framework,\nDeep Reinforcement Learning, to achieve a robust and reliable merging policy. With this approach, it is of vital importance to learn the interactive environment and to optimize long term cumulative rewards. In our study, we formulate an architectural framework based on Deep Reinforcement Learning techniques for the on-ramp merge problem to tackle the issues of (1) attaining long-term effects, (2) mitigating unexpected and adversarial impacts by other agents, and (3) dealing with a system of continuous states/actions. A literature view of related works is described in the next section, followed by the proposed architecture and the methodology. Then, the implementation procedure of the ramp-merge problem is presented. Finally, concluding remarks and discussions are given in the closing section.\nII. LITUREATURE REVIEW\nSeveral modeling methods have been previously\nsuggested to solve the autonomous on-ramp merging problem by assuming some specific rules. Davis [4] presented a cooperative merging strategy, in which vehicles on the mainline always slow down to create enough gap space to let the on-ramp vehicle merge into. Marinescu et al. [5] proposed a slot-based merging algorithm, which defined a slot’s occupancy status (e.g. free and occupied) based on the mainline vehicles’ speed, position, and behavior of acceleration or deceleration. Chen et al. [6] used driving rules and a gap acceptance theory to model the decision-making process of the urban expressway on-ramp merge problem. These rule-based models are conceptually comprehensible but are pragmatically vulnerable due to their inability to adapt to unforeseen situations in the real world. In other words, merge maneuvers can only be executed under predefined rules, and for cases that are outside the domains of these rules, the models may fail and lead to hazardous situations, e.g. crashes or severe disturbance to traffic flows.\nMachine learning models, in contrast, have the potential\nto be superior in dealing with complex situations without resorting to detailed hard-coded rules or pre-determined models. Particularly, reinforcement learning, different from standard supervised learning techniques, which need ground truth input/output pairs, can efficiently learn optimal actions by itself through trials and errors [7]. A reinforcement learning agent observes its environment, and interacts with it by taking actions that are balanced between exploration of uncharted territory and exploitation of current knowledge. Then it receives immediate rewards and the system moves to a new state. By maximizing the cumulative reward, the agent finds an optimal policy to achieve its goal.\nReinforcement learning has been extensively applied to\nthe field of robotics and recently been applied to vehicle and traffic control problems. Fares et al. [8] designed a Reinforcement Learning based Density Control Agent (RLCA) to control the number of vehicles entering the mainline from the ramp merging area. Yang et al. [9] developed a rampmetering control algorithm based on reinforcement learning to increase the capacity at weaving sections. These applications use the basic Q-learning in which the state space and action space are discrete and the problem is considered as a Markov Decision Process (MDP).\nRamp merging is much more complex. The driving\nenvironment includes not only the merging vehicle’s state but also the dynamic states of other agents, which are not necessarily predictable in the view of the merging agent. Therefore, the on-ramp merge case is an intrinsically nonMarkovian problem. Moreover, the vehicle’s state space and action space are continuous, which makes it impractical to use tabular settings as in basic Q-learning. Instead, Q-function approximation is a good way to deal with non-MDP or Partially Observed Markov Decision Process (POMDP) in a continuous state/action space. For example, Google DeepMind [10] has successfully applied a Deep Q-network (a convolutional neural network, CNN) to play Atari games with only screen images and game scores as inputs. Some other studies applied a similar reinforcement learning framework on browser-based car simulators to implement autonomous\nvehicle control under some specific scenarios. For instance, Sallab et al. [11] used end-to-end deep reinforcement learning for lane-keeping assist on an open-source simulator for Racing called (TORCS). Yu et al. [12] investigated the use of deep reinforcement learning for training an agent to control a simulated car running on the track in JavaScript Racer. In these applications, the inputs of game screens only represent simplified real-world driving rules so that the policy learned can only be applied on virtual video games. Mobileye [13] used another approach that employed two supervised learning models to describe an interactive environment and a recurrent neural network based on these two models to learn an optimal policy. Shalev-Shwartz et al. [14] decompose the driving strategy into a learnable part which estimate the comfort of driving, and a non-learnable part which is hard constrains on the safety of driving.\nThe aforementioned research and associated\nshortcomings show that basic Q-learning is not appropriate for handling problems with non-MDP properties and continuous states/actions, and that Deep Q-network with CNN structure is limited to image inputs and not ready to be implemented on real-world driving scenarios. Besides, the historical driving information has not been extensively incorporated in these studies. It is important to develop a highly representative model to describe the real-world environment and design an appropriate Q-function approximator to employ long dependencies of the history for learning a robust and reliable driving policy.\nIII. METHODOLOGY\nIn our study, we use Deep Reinforcement Learning to\nincorporate the influence of the historical information of the driving environment on the merging policy optimization. Specifically, we model the environment with a Long ShortTerm Memory (LSTM) architecture to learn the internal relations between the ego vehicle and other surrounding vehicles based a relatively long duration of the past time. An internal state representation from LSTM at each time step is then fed into a Deep Q-network for action selection. After that, the Q-network is immediately updated by an experience replay\nand a second target Q-network to avoid local optima and divergence problems. In this way, an interactive merging policy can be learned. The overview of the architecture is shown in Fig. 1. We will first introduce the architecture of LSTM and then the architecture of Deep Q-learning."
    }, {
      "heading" : "A. LSTM",
      "text" : "LSTM is a special recurrent neural network designed to\nhandle long-term dependency problems [15]. LSTM has the ability to remember values for both long and short durations. As mentioned earlier, the driving environment of the on-ramp merge scenario involves interactions with the surrounding vehicles, which are not necessarily predictable in the view of the ego vehicle. With the use of LSTM, the historical driving information can be incorporated into an internal state that is an adequate representation of the interactive environment. In other words, the internal state given by an LSTM cell gives a compact and fixed-sized representation of the history that can be fed into the Q-network.\nIn our study, we train the LSTM model by supervised\nlearning. The architecture of the LSTM model is shown in Fig. 2. Each LSTM unit includes two modules, an observation module and a state module. The observation module is used to estimate the next observation (\uD835\uDC5C\") based on the inputs of the internal state (\uD835\uDC60\"%&) and action (\uD835\uDC4E\"%&) from the last time step. The state module is used to map the current observation (\uD835\uDC5C\") into a more informative internal state ( \uD835\uDC60\" ) by employing historical environment information retained from previous steps. The representative internal state is then input to the next\nLSTM unit as well as to Q-network for action decision, which is described in the following section."
    }, {
      "heading" : "B. Deep Q-Learning",
      "text" : "Deep Q-learning is a model-free approach to deal with\nreinforcement learning problems. Deep Q-learning uses neural networks, parameterized by \uD835\uDF03, to approximate the Q-function. Q-values, denoted as \uD835\uDC44(\uD835\uDC60, \uD835\uDC4E; \uD835\uDF03), can be used to get the best action for a given state. The architecture of Deep Q-learning in our study is depicted in Fig. 3.\nIn our approach, the Q-learning process consists of two\nparts at each time step. One is the Q-value approximation for action selection (left part in Fig. 3) in which the internal state \uD835\uDC60-, obtained from LSTM based on \uD835\uDC5C-, is used as the input to the Q-network to get the chosen action \uD835\uDC4E-. The other part is the Q-network update (right part in Fig.3) where the loss between predicted Q-values and target Q-values is used to update Q-network parameters \uD835\uDF03 and \uD835\uDF03%.\nTo guarantee there is always an optimal action in a given\nstate and that the learning process can converge fast, in our work, we design the Q-function approximator as a quadratic function, \uD835\uDC44 \uD835\uDC60, \uD835\uDC4E = \uD835\uDC34(\uD835\uDC60) ∗ \uD835\uDC35 \uD835\uDC60 − \uD835\uDC4E ∗∗ 2 + \uD835\uDC36(\uD835\uDC60) , where \uD835\uDC34, \uD835\uDC35, and \uD835\uDC36 are designed with neural networks.\nThe vehicle action is composed of the longitudinal control\n(acceleration) and the lateral control (steering). Note that the action cannot be arbitrarily large or small values due to the vehicle physical mechanics, therefore, we restrain the acceleration and steering angle in certain ranges, and within the range the acceleration and steering can be any real value. When the best action is obtained with the highest Q-value, a random noise is added to it and the new value is the chosen action, which is similar with the concept of \uD835\uDF00 − \uD835\uDC54\uD835\uDC5F\uD835\uDC52\uD835\uDC52\uD835\uDC51\uD835\uDC66 used in the action exploration process.\n\uD835\uDC4E- = \uD835\uDC4E\uD835\uDC5F\uD835\uDC54\uD835\uDC5A\uD835\uDC4E\uD835\uDC65>? \uD835\uDC44(\uD835\uDC60-, \uD835\uDC4E?) + \uD835\uDC5B\uD835\uDC5C\uD835\uDC56\uD835\uDC60\uD835\uDC52 (1)\nThe reward function measures the safeness, smoothness,\nand timeliness of the merging maneuver, and is formulated as a function of the merging vehicle’s acceleration, steering angle, speed, and the distance to its surrounding vehicles, shown in equation (2). \uD835\uDC45 \uD835\uDC60-, \uD835\uDC4E- = \uD835\uDC5F1 ∗ \uD835\uDC4E\uD835\uDC50\uD835\uDC50\uD835\uDC52\uD835\uDC59 + \uD835\uDC5F2 ∗ \uD835\uDC60\uD835\uDC61\uD835\uDC52\uD835\uDC52\uD835\uDC5F\uD835\uDC56\uD835\uDC5B\uD835\uDC54 + \uD835\uDC5F3 ∗ \uD835\uDC51\uD835\uDC56\uD835\uDC60\uD835\uDC61\uD835\uDC4E\uD835\uDC5B\uD835\uDC50\uD835\uDC52 + \uD835\uDC5F4 ∗ \uD835\uDC60\uD835\uDC5D\uD835\uDC52\uD835\uDC52\uD835\uDC51 (2) where \uD835\uDC5F1 and \uD835\uDC5F2 are the smoothness factors; \uD835\uDC5F3 is the safeness factor; and \uD835\uDC5F4 is the timeliness factor. Large acceleration/deceleration, small distance to surrounding vehicles, and low speed under free flow condition are penalized by large negative rewards.\nThe loss function is defined by the mean square error\nbetween predicted Q-values \uD835\uDC44-K and target Q-values \uD835\uDC44-L , equation (3). \uD835\uDC44-L is calculated by the immediate reward \uD835\uDC5F- and the maximum Q-value of the next internal state \uD835\uDC60-M&.\n\uD835\uDC3F \uD835\uDF03 = \uD835\uDC44-L − \uD835\uDC44-K O (3)\nwhere \uD835\uDC44-K = \uD835\uDC44 \uD835\uDC60-, \uD835\uDC4E- \uD835\uDF03 , \uD835\uDC44-L = \uD835\uDC5F- + \uD835\uDEFE\uD835\uDC5A\uD835\uDC4E\uD835\uDC65>Q \uD835\uDC44 \uD835\uDC60-M&, \uD835\uDC4E ? \uD835\uDF03% . \uD835\uDEFE is a discounted factor, \uD835\uDEFE ∈ 0,1 . The Q-network parameters are updated by the backpropagation on \uD835\uDC3F \uD835\uDF03 :\n\uD835\uDF03 ≔ \uD835\uDF03 + ∇W \uD835\uDC3F \uD835\uDF03 (4)\nIt is worth mentioning that there may be stability issues\nwith such a Q-learning approach due to the fact that the data\nwe used in the Q-network training is sequential and that slight changes in Q-values can lead to large oscillations in control policies. With these considerations, we apply an experience replay and use a different set of Q-function parameters \uD835\uDF03% for \uD835\uDC44-L , to break data correlation and to avoid sticking in local optima. At each time step t, a state transition element can be saved as \uD835\uDC52- = (\uD835\uDC60-, \uD835\uDC4E-, \uD835\uDC5F-, \uD835\uDC60-M&) after the action execution, and is stored in a replay memory \uD835\uDC37. A mini-batch is sampled uniformly at random as \uD835\uDC52&, … , \uD835\uDC52Z , which are used for the parameter updates as in supervised learning. The parameters used for the estimation of target Q-values are fixed for a number of iterations (e.g. C iterations where C=500), and updated periodically with the Q-network parameters at that step, as \uD835\uDF03% ← \uD835\uDF03.\nIV. IMPLEMENTATION ON ON-RAMP MERGE CASE\nThe implementation of the proposed Deep Reinforcement\nLearning architecture for the highway on-ramp merge problem is described below.\nA typical scenario of the on-ramp merging involves three\nvehicles. The merging vehicle on the ramp, denoted as \uD835\uDC49] , executes the merge maneuver when it observes an appropriate gap formed by two vehicles on the mainline, denoted as the gap lag vehicle \uD835\uDC49 and the gap front vehicle \uD835\uDC49_, as shown in Fig. 4(a) (a simulated scenario of a section of US Interstate Highway I-80). The training is based on such simulated scenarios of real-world merging areas.\nIn the current study, we only consider the interaction\namong the three vehicles (the merging vehicle, the gap lag\nvehicle and the gap front vehicle). For the merging vehicle, we use 5 variables to describe its driving state that are speed (\uD835\uDC63]), position (\uD835\uDC5D]), heading angle (\uD835\uDF11]), and distances to the right (\uD835\uDC5F]) and left (\uD835\uDC59]) lane makings of the current lane. For the other two vehicles, we assume we can only observe their speeds (\uD835\uDC63_ and \uD835\uDC63^) and positions (\uD835\uDC5D_ and \uD835\uDC5D^). In total, there are 9 variables in the state representation. The observation at time step t is denoted as \uD835\uDC5C- =\n\uD835\uDC63-], \uD835\uDC5D-], \uD835\uDF11-], \uD835\uDC59], \uD835\uDC5F], \uD835\uDC63- _, \uD835\uDC5D- _, \uD835\uDC63-^, \uD835\uDC5D-^ .\nThe action is the acceleration and the steering angle taken\nby the merge vehicle, and is denoted as \uD835\uDC4E- = (\uD835\uDC4E-], \uD835\uDF0E-]). The immediate reward is measured by the \uD835\uDC5C- and \uD835\uDC4E- . The threshold for action variables and for negative rewards can be fine-tuned for specific design criteria.\nTo guarantee the representativeness of the driving environment, the data to train the LSTM model are obtained from the real-world driving data. Cameras, placed on a high pole, are used to record the on-ramp merging scenarios in a bird view. Video analysis (e.g. scene understanding, object detection, motion estimation) is performed to obtain the aforementioned observation and action variables. In the video analysis, the inputs are video images and the output are vehicle dynamics. The interactive driving behaviors of the merge vehicle and the other two vehicles (the lag vehicle and the front vehicle) are learned as internal state through the LSTM units. In the Deep Q-learning process, a couple of neural networks are used to calculate Q values based on the internal state at each time step. The parameter update of the Q-network is conducted through gradient descent. The pseudo codes are shown below.\n1 Initialize experience replay memory D 2 Initialize Q-Network parameters \uD835\uDF03 3 for episode p = 1: M do 4 Initialize environment state \uD835\uDC5C& 5 for time t= 1:T do 6 get the internal state \uD835\uDC60- from LSTM 7 input \uD835\uDC60- to Q-function approximator 8 get the best action \uD835\uDC4E-∗ 9 get the chosen action \uD835\uDC4E- = \uD835\uDC4E-∗ + \uD835\uDC5B\uD835\uDC5C\uD835\uDC56\uD835\uDC60\uD835\uDC52 10 Execute \uD835\uDC4E- and obtain immediate reward \uD835\uDC5F- and\nnext observation \uD835\uDC5C-M& 11 Store transition element (\uD835\uDC60-, \uD835\uDC4E-, \uD835\uDC5F-, \uD835\uDC60-M&) into D 12 Sample mini-batch (\uD835\uDC52&,…, \uD835\uDC52Z) uniformly at random from D (\uD835\uDC52\" = \uD835\uDC60\", \uD835\uDC4E\", \uD835\uDC5F\", \uD835\uDC60\"M& , ∀\uD835\uDC56 ∈ (1, \uD835\uDC41)) 13 for i = 1 to N, do 14 Set target Q as \uD835\uDC44L = \uD835\uDC5F\", \uD835\uDC56\uD835\uDC53 \uD835\uDC60\"M& \uD835\uDC56\uD835\uDC60 \uD835\uDC61\uD835\uDC52\uD835\uDC5F\uD835\uDC5A\uD835\uDC56\uD835\uDC5B\uD835\uDC4E\uD835\uDC59\n\uD835\uDC5F\" + \uD835\uDEFE ∗ \uD835\uDC5A\uD835\uDC4E\uD835\uDC65>Q \uD835\uDC44 \uD835\uDC60\"M&, \uD835\uDC4E?; \uD835\uDF03% , \uD835\uDC56\uD835\uDC53 \uD835\uDC60\"M& \uD835\uDC56\uD835\uDC60 \uD835\uDC5B\uD835\uDC5C\uD835\uDC61 \uD835\uDC61\uD835\uDC52\uD835\uDC5F\uD835\uDC5A\uD835\uDC56\uD835\uDC5B\uD835\uDC4E\uD835\uDC59\n15 Calculate \uD835\uDC3F \uD835\uDF03 = \uD835\uDC3F \uD835\uDF03 + (\uD835\uDC44L − \uD835\uDC44(\uD835\uDC60\", \uD835\uDC4E\"; \uD835\uDF03))O 16 Perform gradient descent on \uD835\uDC3F \uD835\uDF03 17 Set \uD835\uDF03% = \uD835\uDF03 after C iterations\nThis paper describes our preliminary work on\nautonomous ramp merging. The verification and validation of the implemented methodologies is our next step work.\nV. CONCLUSIONS AND DISCUSSION\nIn this work, we propose a Deep Reinforcement Learning\narchitecture for learning an on-ramp merge driving policy. The driving environment is based on a LSTM architecture to incorporate the influence of historical and interactive driving behaviors on the action selection. In the Deep Q-learning process, the internal state from LSTM is taken as the input to the Q-function approximator, which is used for the action selection based on more past information. The Q-network parameters are updated with an experience replay and a second target Q-network is used to relieve the problems of local optima and instability. Model training and fine-tuning, refinement of proposed methodologies, and performance evaluation of deep learning approaches remain topics of our further studies.\nWe also envision that there is another research topic,\nwhich we call “supervisory control in highly automated driving systems”, that is worth pursuing. A sensible operational concept of ADS is a system, in which manual and automated modes may cooperatively co-exist, instead of forced switching between the two modes. The “supervisory control” term per Sheridan [16], suggests that human-machine systems can exist in a spectrum of automation, and shift across the spectrum of control levels in real time to suit the situation\nat hand. In the use case of ramp merge, the driver may selectively provide control inputs while the automated driving system seeks to maximize the reward and achieve a successful and effective merge maneuver. To accommodate a balanced and common reward in the machine learning approach, a cooperative inverse reinforcement learning concept (CIRL) [17] may be considered and adopted into the framework."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "The authors thank the support from Berkeley Deep Drive\nprogram and the helpful discussion with Dr. Yi-Ta Chuang.\nREFERENCES [1] https://www.tesla.com/autopilot [2] https://www.google.com/selfdrivingcar/ [3] http://www.volvocars.com/intl/about/our-innovationbrands/intellisafe/autonomous-driving/drive-me. [4] Davis, L.C. Effect of adaptive cruise control systems on mixed traffic\nflow near an on-ramp. Physica A: Statistical Mechanics and its Applications, Vol. 379, No. 1, pp. 274–290, 2007.\n[5] Marinescu, D., Čurn, J., Bouroche, M., Cahill, V. On-ramp traffic merging using cooperative intelligent vehicles: a slot-based approach. 15th International IEEE Conference on Intelligent Transportation Systems, Alaska, USA, 2012. [6] Chen, X., Jin, M., Chan, C., Mao, Y., Gong, W. Decision-making analysis during urban expressway ramp merge for autonomous vehicle. 96th Annual Meeting of the Transportation Research Board, Washington, D.C., 2017. [7] https://en.wikipedia.org/wiki/Reinforcement_learning [8] Fares, A., Gomaa, W. Freeway Ramp-Metering Control based on\nReinforcement Learning. 11th IEEE International Conference on Control & Automation (ICCA), 2014, Taiwan.\n[9] Yang, H., Rakha, H. Reinforcement learning ramp metering control for weaving sections in a connected vehicle environment. 96th Annual Meeting of the Transportation Research Board, 2017. [10] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A., Veness, J., Bellemare, M., Graves, A., Riedmiller, M., Fidjeland, A., Ostrovski, G., et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015. [11] Sallab, A., Abdou, M., Perot, E., Yogamani, S. End-to-End Deep Reinforcement Learning for Lane Keeping Assist. 30th Conference on Neural Information Processing Systems, Barcelona, Spain, 2016. [12] Yu, A., Palefsky-Smith, R., Bedi, R. Deep Reinforcement Learning for Simulated Autonomous Vehicle Control. Stanford University. StuDocu. 2016. [13] Shalev-Shwartz, S., Ben-Zrihem, N., Cohen, A., Shashua, A. Longterm Planning by Short-term Prediction. 2016, arXiv preprint arXiv:1602.01580. [14] Shalev-Shwartz, S., Shammah, S., Shashua, A.. Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving. arXiv:1610.03295v1. [15] Hochreiter, S., Schmidhuber, J. Long short-term memory. Neural computation, 1997, 9(8):1735–1780. [16] Sheridan, T.B. Telerobotics, Automation, and Human Supervisory Control, Cambridge, MA: MIT Press, 1992. [17] Hadfield-Menell, D., Dragan, A. Abbeel, P., Russell, S. Cooperative Inverse Reinforcement Learning, 30th Conference on Neural Information Processing Systems (NIPS), Barcelona, Spain, 2016."
    } ],
    "references" : [ {
      "title" : "Effect of adaptive cruise control systems on mixed traffic flow near an on-ramp",
      "author" : [ "L.C. Davis" ],
      "venue" : "Physica A: Statistical Mechanics and its Applications,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "On-ramp traffic merging using cooperative intelligent vehicles: a slot-based approach",
      "author" : [ "D. Marinescu", "J. Čurn", "M. Bouroche", "V. Cahill" ],
      "venue" : "15th International IEEE Conference on Intelligent Transportation Systems, Alaska,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Decision-making analysis during urban expressway ramp merge for autonomous vehicle",
      "author" : [ "X. Chen", "M. Jin", "C. Chan", "Y. Mao", "W. Gong" ],
      "venue" : "Annual Meeting of the Transportation Research",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2017
    }, {
      "title" : "Freeway Ramp-Metering Control based on Reinforcement Learning",
      "author" : [ "A. Fares", "W. Gomaa" ],
      "venue" : "IEEE International Conference on Control & Automation (ICCA),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Reinforcement learning ramp metering control for weaving sections in a connected vehicle environment",
      "author" : [ "H. Yang", "H. Rakha" ],
      "venue" : "Annual Meeting of the Transportation Research Board,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2017
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Rusu", "J. Veness", "M. Bellemare", "A. Graves", "M. Riedmiller", "A. Fidjeland", "G Ostrovski" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "End-to-End Deep Reinforcement Learning for Lane Keeping Assist",
      "author" : [ "A. Sallab", "M. Abdou", "E. Perot", "S. Yogamani" ],
      "venue" : "Conference on Neural Information Processing Systems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Deep Reinforcement Learning for Simulated Autonomous Vehicle Control",
      "author" : [ "A. Yu", "R. Palefsky-Smith", "R. Bedi" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Longterm Planning by Short-term Prediction. 2016, arXiv preprint arXiv:1602.01580",
      "author" : [ "S. Shalev-Shwartz", "N. Ben-Zrihem", "A. Cohen", "A. Shashua" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1997
    }, {
      "title" : "Telerobotics, Automation, and Human Supervisory Control",
      "author" : [ "T.B. Sheridan" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1992
    }, {
      "title" : "Cooperative Inverse Reinforcement Learning, 30",
      "author" : [ "D. Hadfield-Menell", "Dragan", "P.A. Abbeel", "S. Russell" ],
      "venue" : "Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Davis [4] presented a cooperative merging strategy, in which vehicles on the mainline always slow down to create enough gap space to let the on-ramp vehicle merge into.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 1,
      "context" : "[5] proposed a slot-based merging algorithm, which defined a slot’s occupancy status (e.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[6] used driving rules and a gap acceptance theory to model the decision-making process of the urban expressway on-ramp merge problem.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[8] designed a Reinforcement Learning based Density Control Agent (RLCA) to control the number of vehicles entering the mainline from the ramp merging area.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[9] developed a rampmetering control algorithm based on reinforcement learning to increase the capacity at weaving sections.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "For example, Google DeepMind [10] has successfully applied a Deep Q-network (a convolutional neural network, CNN) to play Atari games with only screen images and game scores as inputs.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : "[11] used end-to-end deep reinforcement learning for lane-keeping assist on an open-source simulator for Racing called (TORCS).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[12] investigated the use of deep reinforcement learning for training an agent to control a simulated car running on the track in JavaScript Racer.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "Mobileye [13] used another approach that employed two supervised learning models to describe an interactive environment and a recurrent neural network based on these two models to learn an optimal policy.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 9,
      "context" : "LSTM is a special recurrent neural network designed to handle long-term dependency problems [15].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 10,
      "context" : "The “supervisory control” term per Sheridan [16], suggests that human-machine systems can exist in a spectrum of automation, and shift across the spectrum of control levels in real time to suit the situation at hand.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 11,
      "context" : "To accommodate a balanced and common reward in the machine learning approach, a cooperative inverse reinforcement learning concept (CIRL) [17] may be considered and adopted into the framework.",
      "startOffset" : 138,
      "endOffset" : 142
    } ],
    "year" : 2017,
    "abstractText" : "Multiple automakers have in development or in production automated driving systems (ADS) that offer freeway-pilot functions. This type of ADS is typically limited to restricted-access freeways only, that is, the transition from manual to automated modes takes place only after the ramp merging process is completed manually. One major challenge to extend the automation to ramp merging is that the automated vehicle needs to incorporate and optimize long-term objectives (e.g. successful and smooth merge) when near-term actions must be safely executed. Moreover, the merging process involves interactions with other vehicles whose behaviors are sometimes hard to predict but may influence the merging vehicle’s optimal actions. To tackle such a complicated control problem, we propose to apply Deep Reinforcement Learning (DRL) techniques for finding an optimal driving policy by maximizing the long-term reward in an interactive environment. Specifically, we apply a Long Short-Term Memory (LSTM) architecture to model the interactive environment, from which an internal state containing historical driving information is conveyed to a Deep Q-Network (DQN). The DQN is used to approximate the Q-function, which takes the internal state as input and generates Q-values as output for action selection. With this DRL architecture, the historical impact of interactive environment on the long-term reward can be captured and taken into account for deciding the optimal control policy. The proposed architecture has the potential to be extended and applied to other autonomous driving scenarios such as driving through a complex intersection or changing lanes under varying traffic flow conditions. Keywords— Autonomous Driving; Highway On-Ramp Merge; Deep Reinforcement Learning; Long Short-Term Memory; Deep Q-Network; Control Policy",
    "creator" : "'Certified by IEEE PDFeXpress at 07/31/2017 4:20:40 PM'"
  }
}