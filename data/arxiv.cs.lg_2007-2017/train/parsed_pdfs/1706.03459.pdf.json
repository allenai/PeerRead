{
  "name" : "1706.03459.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimal Auctions through Deep Learning∗",
    "authors" : [ "Paul Dütting", "Zhe Feng", "Harikrishna Narasimhan", "David C. Parkes" ],
    "emails" : [ "p.d.duetting@lse.ac.uk.", "feng@g.harvard.edu.", "hnarasimhan@g.harvard.edu.", "parkes@g.harvard.edu." ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Optimal auction design is one of the cornerstones of economic theory, and has also received a lot of attention in computer science in recent years. It is of great practical importance, as auctions are used within industry and by the public sector to organize the sale of many products and services. A basic question is that of designing a protocol for selling one or more items so as to maximize expected revenue. Myerson’s [45] seminal work solved the problem of optimal auction design for a single item setting. The problem of designing optimal auctions for more than one item is considerably more challenging, and has defied a thorough theoretical understanding for several decades. In fact, despite a flurry of recent breakthroughs [1, 22, 24, 11, 23, 31], many important questions remain unsolved. For instance, while there is a Myerson-like characterization of optimal Bayesian incentive compatible mechanisms, which can also be leveraged algorithmically, relatively little is known about the exact structure of these mechanisms (see, e.g., [21]). More importantly, although a lot of progress has been made in designing dominant strategy incentive compatible approximation mechanisms [34, 41, 56, 50, 2], the design of exactly optimal, dominant strategy incentive compatible mechanisms is largely open.\n∗We would like to thank Vincent Conitzer, Constantinos Daskalakis, Alexander Rush, participants in the Economics and Computer Science Reunion workshop at Simons Institute, as well as anonymous reviewers on an earlier version of this paper for their helpful feedback. †Department of Mathematics, London School of Economics, Houghton Street, London WC2A 2AE, UK. Email: p.d.duetting@lse.ac.uk. ‡John A. Paulson School of Engineering and Applied Sciences, Harvard University, 33 Oxford Street, Cambridge, MA 02138, USA. Email: {zhe feng,hnarasimhan,parkes}@g.harvard.edu.\nar X\niv :1\n70 6.\n03 45\n9v 1\n[ cs\n.G T\n] 1\n2 Ju\nn 20\n17"
    }, {
      "heading" : "1.1 Optimal Auctions: Structure and Challenges",
      "text" : "Let us start by formulating the problem and stating some basic results. In the standard model, there are n bidders N and m items M . Each bidder i has a valuation function vi : 2\nM → R+ drawn independently from some distribution Fi over possible valuation functions Vi. Let V = V1×. . .×Vn. The auctioneer knows the distributions Fi, but not the bidders’ valuation functions. The auctioneer runs a mechanismM = (g, p) consisting of a collection of allocation rules gi : V → 2M and payment rules pi : V → R≥0. The auction collects bids bi ∈ Vi from the bidders, and then computes an allocation g(b) and payments p(b), with bid profile b = (b1, . . . , bn). A feasible mechanism is one that allocates each item to at most one bidder. Allocation rules may also be randomized.\nBidders act strategically, and seek to maximize their utility ui(vi, b) = vi(gi(b)) − pi(b). Let v−i denote the valuation profile v = (v1, . . . , vn) without element vi, similarly for b−i. Let V−i = V1 × . . .× Vi−1 × Vi+1 × . . .× Vn.\nA mechanism is Bayesian incentive compatible (BIC) if Ev−i [ui(vi, (vi, v−i))] ≥ Ev−i [ui(vi, (bi, v−i))] for every bidder i, every valuation vi, and every bid bi. That is, for each bidder bidding truthfully yields the highest expected utility provided that the other bidders are truthful. A stronger incentive property than BIC is dominant strategy incentive compatibility. A mechanism is dominant strategy incentive compatible (DSIC) if ui(vi, (vi, b−i)) ≥ ui(vi, (bi, b−i)) for every bidder i, every valuation vi, every bid bi, and all bids b−i from others. That is, the bidder’s utility is maximized by reporting truthfully no matter what the other bidders do.\nThe revenue of a BIC/DSIC mechanism is ∑\ni pi(v). A revenue-maximizing (or optimal) BIC/DSIC mechanism is one that maximizes expected revenue within its class, while ensuring individual rationality (IR), which is the property that bidders have non-negative utility for participating and bidding truthfully. This IR property may be stated ex post, meaning that it holds for any bids of others (in expectation with respect to any randomization of the allocation rule). Alternatively, IR may be stated interim, meaning that it holds in expectation with respect to the equilibrium bids of others. In this paper, we insist on ex post IR, i.e., ui(vi, (vi, b−i)) ≥ 0, ∀vi ∈ Vi, ∀b−i ∈ V−i, ∀i ∈ N .\nA special case of the problem of finding a revenue-maximizing mechanism is that of finding an optimal auction for selling a single item.\nTheorem 1.1 (Myerson [45]). There exist a collection of monotonically increasing functions φi : Vi → R, called the ironed virtual valuation functions, such that the optimal BIC mechanism for selling a single item is the DSIC mechanism that assigns the item to the bidder i with the highest ironed virtual value φi(vi) assuming this quantity is positive and charges the winning bidder the smallest bid that would keep its ironed virtual value at least that of the other bidders.\nMyerson’s result is remarkable for a number of reasons. First, it says that the optimal mechanism among all BIC mechanisms is a DSIC mechanism. Second, the optimal mechanism is deterministic and not randomized.1 And third, it gives a very crisp description of the optimal mechanism, which can be surprisingly simple. For regular distributions, for example, where vi − (1 − Fi(vi))/fi(vi) is increasing (where fi is the density function corresponding to distribution Fi), the ironed virtual valuation function is φi(vi) = vi − (1− Fi(vi))/fi(vi) and the optimal mechanism is a second-price auction with bidder-specific reserve prices φ−1i (0) (see Figure 1(a)).\nA crisp characterization of the revenue-maximizing mechanism does not exist for more general auction problems, although some special cases are understood. A famous example— for the case of just a single bidder —is the following (also see Figure 1(b)).\nTheorem 1.2 (Manelli and Vincent [42], Pavlov [48]). The optimal BIC mechanism for selling two items to a single bidder with additive preferences over the items and values on each item that are i.i.d. draws from the uniform distribution on [0, 1] is the DSIC mechanism that offers each individual item at a price of 2/3, and the bundle of both items at a price of (4− √ 2)/3.\nAs in Myerson’s result, the optimal BIC mechanism for this setting is DSIC, deterministic, and relatively easy to state. On the other hand, this result also highlights one of the complications that arise when going beyond one item, namely the decision between selling items individually and grouping them into larger bundles. Indeed, optimal mechanisms for selling more than one item to even a single bidder can be rather complicated and exhibit some counterintuitive properties. In general, they need to be randomized (in the above example, if values are uniform on [c, c+ 1] and c > 0) and the revenue gap between deterministic and randomized mechanisms can be arbitrarily large [35, 9]. Moreover, it may be impossible to describe them via finitely many lotteries over allocations and prices [22], they may fail to possess intuitive monotonicity properties [36], and they may exhibit curious properties in regard to bundling [24].\nThe problem of finding a revenue-optimal mechanism for more than one item is challenging because of the absence of nice characterizations of BIC/DSIC mechanisms for multidimensional settings. Myerson [45] gives an explicit characterization via monotonicity and payment identity for problems where the private information of a bidder is one dimensional, as in the single item auction. For fully general settings there are only rather implicit characterizations, e.g., via cyclic monotonicity [49]. For multi-item auctions, the most definitive characterization is duality-based and given for the single additive bidder case [24]. This has been leveraged for understanding the optimality of bundling and for the optimal design of particular two-item examples, but not more generally. There is also a generalized virtual valuation characterization for the general problem of BIC (but not DSIC) optimal auction design [13]. Furthermore, although it can be leveraged computationally in problems with small, discrete valuation spaces, it does not provide the analytical form of mappings from valuations to virtual valuations.\nConsiderably less is known about DSIC revenue-optimal mechanisms as compared to BIC auctions, except that there is a provable gap between revenue in DSIC and BIC mechanisms [57], and that in many settings the revenue gap is a constant multiplicative factor [10, 34, 2].\n1In the case of a tie for the bidder with the highest virtual valuation, this can be broken arbitrarily."
    }, {
      "heading" : "1.2 Our Approach and Results",
      "text" : "Our Approach. In light of these difficulties we advocate a data-driven approach to optimal auction design that uses deep machine learning for modeling an auction. We focus on the design of revenue optimal mechanisms amongst the family of DSIC mechanisms because these mechanisms provide a sweetspot for practically-motivated advances in auction design: the distributional information about bidder valuations is used to promote revenue, but without relying on common knowledge of value distributions or rationality amongst bidders for the equilibrium solution concept.\nAt the heart of our approach are the feed-forward multi-layer neural networks that have gained renewed attention in machine learning in recent years. In our setting, the inputs to the neural networks are the bidders’ valuations and the outputs encode the allocation and pricing decisions. This approach seems particularly fit for the problem of identifying optimal designs for several reasons. First, by the so-called “universal approximation theorem,” these networks are in principle able to encode any mapping from inputs to outputs [20, 38]. Second, deep networks have been applied successfully to a variety of challenging problems (including those in vision and naturallanguage processing), and one of their strengths is that they are able to automatically identify relevant features [6, 32]. Third, recent theoretical developments provide some support for why stochastic gradient descent succeeds in finding globally optimal solutions [40, 54]. While this is a statement about minimizing the training error, in practice one can use regularization techniques to ensure that the learned network does not overfit the training set and has low generalization error. Indeed in our problem where training and test data are generated from known distributions and training data is abundant, we expect to avoid issues of overfitting.\nA feed-forward multi-layer neural network consists of an input layer, one or more hidden layers, and an output layer (see Figure 2). In our case, the input is bidder valuations (v1 through v3 in the figure). Each hidden layer and the output layer consists of a number of units, each of which apply a non-linear activation function h to a weighed sum of inputs x1, . . . , xk from the previous layer (these weights are parameters). For different weights, a network thus computes different mappings from inputs to outputs (o1, o2 in the figure), which in our case encode allocation and pricing decisions. The weights are adjusted during training (in our case, the training data is valuation profiles v(1), . . . v(L), sampled i.i.d. from F ), in order to minimize a loss function that is defined on the inputs and outputs of the network.\nOur goal is to understand whether we can train a feed-forward multi-layer neural network that recovers the revenue-maximizing DSIC mechanism, and to explore different architectural choices. We are interested both in reproducing current results from the theoretical literature as well as demonstrating that we can apply the framework to currently unsolved problems.\nOur Results. In this paper, we describe and explore two approaches to designing revenue-optimal auctions through deep learning:\n(1) A fully agnostic approach. In this case, we proceed without making use of characterization results. Rather, we use the negated, expected revenue as the loss function, and optimize it subject to a constraint that the expected regret to bidders for bidding truthfully is zero. The regret defines the maximum improvement in utility available to a bidder, fixing the bids of others, and considering all possible deviations from making a truthful bid. (2) A characterization-based approach. In this case, we make use of characterization results to model neural network architectures that satisfy necessary and sufficient conditions for DSIC, so that any mechanism learned by the networks will be truthful. With the negated, expected revenue as the loss function, minimizing the loss corresponds to finding a revenue-maximizing auction.\nIn both cases, we train the networks based on valuation profiles sampled from the true value distribution, which we assume is available (this could be estimated, for example, from bids in previous auctions).\nWe show through experiments that:\n(1) Even without using characterization results, feed-forward neural networks are able to learn almost-optimal auctions (with neglible approximation to IC and IR) for settings for which there exist analytical solutions to optimal auction design— the single item setting; the multiple item, single additive bidder setting; and the two item, multiple, additive bidders (with two values in the support of their valuation distributions) setting. (2) We can leverage characterizations, such as Myerson’s monotonicity condition or Rochet’s characterization via induced utilities and their gradients, to construct neural network architectures that provide more precise fits to the optimal design. (3) We can design auctions with good revenue properties in settings for which an analytical description of the optimal mechanism is not available, such as single additive bidder settings with m > 6 items, and a setting with two items and two bidders with a continuous valuation space."
    }, {
      "heading" : "1.3 Related work",
      "text" : "Given our goals, we focus the discussion of related work on the literature on obtaining revenueoptimal auctions rather than competitive approximation results, as well as providing a discussion of the current state-of-art in regard to using computation for the design of optimal auctions. We also point the interested reader to excellent surveys on recent theoretical advances on optimal auction design [37, 21, 15].\nA sequence of results have made progress in characterizing the optimal mechanism for the special, single additive bidder problem [22, 30, 31, 24]. In particular, Giannakopoulos and Koutsoupias [31] and Daskalakis et al. [24] develop a duality-based framework for verifying the optimality of mechanisms and use it to characterize the optimal mechanism for several two-item settings as well to gain an understanding of when bundling auctions are optimal.\nBeyond the single item or single bidder case, only very limited settings have so far admitted analytical solutions. Yao [57], for example, provides an analytical solution for the optimal BIC and DSIC mechanisms with any number of bidders and two items when preferences are additive and all item valuations are identically distributed on a support of size two. He also proves a formal separation between the revenue from the optimal DSIC and BIC mechanism. For instance, he shows that with two bidders and two items and i.i.d. values of 1 or 2 each with probability 1/2 the revenue gap between the optimal DSIC mechanism and the optimal BIC mechanism is precisely 2%. There are also results that seek to understand this problem through approximation [34, 41, 56, 50, 2, 10], showing that for rather general multi-bidder and multi-item settings the revenue gap between BIC and DSIC designs is only a constant.\nA recent series of breakthrough results have been made in developing a computational framework for optimal, BIC auctions [11, 12, 1, 23, 8, 13]. Cai et al. [11], for example, show that in settings with additive valuations subject to arbitrary feasibility constraints (including unit-demand settings), the optimal mechanism is still a virtual welfare maximizer in the sense of Myerson [45]. A related advance brings general, multi-dimensional optimal auction design into the framework, and leverages an interim representation of a mechanism’s rules [13]. But the characterization result is not analytical, in that it does not provide the form of the virtual valuation mapping, and also that the optimal mappings sometimes depend on the distributions of all bidders.\nUnlike our work, these results are restricted because of the interim representation (which seems to be crucial for computational efficiency) to BIC and cannot be used to obtain DSIC mechanisms. They also rely on a small valuation space and an explicit value distribution representation. Our work, by contrast, makes use of standard pipelines for training deep nets (we use TensorFlow and GPUs for our experiments), applies to the more practically-relevant DSIC solution concept, and works with continuous valuations that are sampled from a generative model. Obtaining revenueoptimal, DSIC mechanisms responds to an open problem in Daskalakis [21].\nOur work on deep learning for optimal auction design fits into the agenda of automated mechanism design [18, 19, 33], albeit using new tools. Conitzer and Sandholm [18] formulated the mechanism design problem as a mathematical program, and obtained exact solutions for a variety of instances (including single and multi-item auctions). Their approach, however, restricts bidders to a finite number of possible values and does not scale up because it represents the set of all valuation profiles explicitly. Sandholm and Likhodedov [51] use heuristic search algorithms to find optimal or near-optimal mechanisms but restrict attention to weighted affine maximizers.\nThe use of machine learning for automated mechanism design was pioneered by Dütting et al. [28], who use support vector machines to design payment rules that render the resulting mechanism maximally DSIC given an allocation rule. Their approach, however, does not directly apply to revenue maximization and need not provide DSIC mechanisms. Narasimhan et al. [47] use methods from machine learning and especially convex optimization to design DSIC social choice and matching mechanisms, but their goal is to minimize distance to a target mechanism and is tailored to specific parameterized classes of mechanisms.\nThere is a recent literature on the sample complexity of revenue-optimal auctions [29, 17, 44, 39, 25]. This work typically looks for a uniform bound that applies for any distribution in a class, and thus negative results do not preclude deep learning (or other statistical machine learning methods) working well on particular distributions. Balcan et al. [3] gave an early application of statistical learning theory to prior-free, digital auctions. See also Baliga and Vohra [5] and Segal [52] for asymptotic, single-item auction results, Cesa-Bianchi et al. [14], Mohri and Medina [43] for setting reserve prices, Balcan et al. [4] for results on combinatorial auctions, Dughmi et al. [27] for single bidder, multi-item auctions and Narasimhan and Parkes [46] for allocation problems (both with and without money). Of these, Dughmi et al. [27] consider general, correlated valuation distributions, and prove bounds on sample complexity based on the representation complexity of an auction.\nFor the single item auction problem, there is an interesting separation between independent and identical distributions and independent, asymmetric distributions (the former but not the latter needing a linear dependence of samples on the number of bidders) [17, 26]. As in our work, it is common to assume that the data used for training does not depend on inputs from the same bidders who will use a mechanism that is trained as a result of the data; see Chawla et al. [16] for work that explicitly considers the coupling of inferential power and revenue."
    }, {
      "heading" : "2 Characterization Based Approach",
      "text" : "We begin with auction design settings where there are known characterizations for DSIC mechanisms. We make use of these characterization results to construct neural networks that represent DSIC mechanisms for all choices of the network parameters. In this context, we optimize the parameters of the network with expected, negated revenue as the loss function.\nWe illustrate this approach on two settings: (i) single-item auctions, where we use Myerson’s monotonicity condition to model the allocation and payment rules as neural networks; and (ii) a setting with a single bidder with additive preferences over multiple items, where we use Rochet’s implicit characterization through induced utilities and their gradients to recover the optimal auction. We refer to the first architecture as the MyersonNet and the second architecture as the RochetNet."
    }, {
      "heading" : "2.1 Single-item Auctions",
      "text" : "In the single-item setting, each bidder has a private value vi ∈ R≥0 for the item. For the purpose of training, we consider a randomized mechanism (g, p) that maps a reported bid profile b ∈ Rn≥0 to a vector of allocation probabilities g(b) ∈ [0, 1]n, where gi(b) denotes the probability that bidder i is allocated the item and ∑n i=1 gi(b) ≤ 1. We represent the payment rule pi via a price ti(b) conditioned on the item being allocated to the bidder, so that pi(b) = gi(b) ti(b). For regular distributions, from Myerson’s characterization the optimal auction can be described by a set of (strictly monotone) virtual value transformations φ1, . . . , φn : R≥0 → R≥0. The Myerson auction can be interpreted as transforming the bids, b̄i = φi(bi), and then using a second price auction (SPA) with zero reserve price, (g0, t0), on these transformed bids. In particular, let t0i (b̄) for bidder i denote the maximum of 0 and the transformed bids of others. The conditional payment for agent i is the bid for which the corresponding virtual value equals t0i (b̄); i.e., ti = φ −1 i (t 0 i (b̄)).\nMyerson provides an analytical functional form for the transforms. However, the auction remains DSIC for any choice of strictly monotone transforms:\nTheorem 2.1. For any set of strictly monotonically increasing functions φ1, . . . , φn : R≥0 → R≥0, an auction defined by outcome rule gi = g 0 i ◦ φ and conditional payments ti = φ −1 i ◦ t0i is DSIC and IR.\nWe leverage this characterization to constrain the network to learn auctions of this kind, but without specifying the precise functional form of the transforms. Rather, the network learns the appropriate transforms in order to maximize expected revenue. See Figures 3 and 4 for the neural network architecture. A neural network is usually a continuous function of its inputs, so that its parameters can be optimized efficiently. Since the allocation rule is a discrete mapping (from bids to the winning bidder), for the purpose of training we employ a smooth approximation to the allocation rule. Specifically, in the SPA with zero reserve, we approximate the winner decision using a softmax function, and this is where the randomization comes in (see Figure 4 (a)). Once training is complete, we use the learned monotone transforms together with an exact SPA with zero reserve to define the final, learned mechanism. The experimental results, in regard to revenue of the learned mechanism, are reported on this non-randomized, DSIC auction.\nIn the case of irregular distributions, the optimal Myerson auction is characterized by ironed virtual value transformations, which need not be strictly monotone or invertible. The experimental results will show that the framework can still be used to design mechanisms that yield revenue very close to the optimal revenue in this irregular case.\nModeling monotone transforms. We model each virtual value function φi as a two-layer feedforward network with min and max operations over linear functions. For K groups of J linear\nfunctions, with strictly positive slopes wikj ∈ R>0, k = 1, . . . ,K, j = 1, . . . , J and intercepts βikj ∈ R, k = 1, . . . ,K, j = 1, . . . , J , we define:\nφi(bi) = min k∈[K] max j∈[J ]\nwikj bi + β i kj . (1)\nSince each linear function is strictly non-decreasing, so is this min-max expression and thus φi. In practice, we can set each wikj = e αikj , with parameters αikj ∈ [−B,B] and limited to a bounded range. A graphical representation of the neural network used for this transform is shown in Figure 3(b), where the intermediate functions are hkj(bi) = e αikjbi +β i kj , ∀k ∈ [K], j ∈ [J ]. For sufficiently large K and J , this neural network can be used to approximate any continuous, bounded monotone function (that satisfies a mild regularity condition) to an arbitrary degree of accuracy [53].\nA particular advantage of this representation is that the inverse transform φ−1 can be directly obtained from the parameters for the forward transform:\nφ−1i (y) = max k∈[K] min j∈[J ] e−α i kj (y − βikj). (2)\nThis neural network is somewhat analogous to an autoencoder neural network, insofar as containing two parts, one for transforming the input bids to a different representation, and the other for inverting the transform [32].\nModeling SPA with zero reserve. We also need to model an SPA with zero reserve (SPA-0) within the neural network architecture. The SPA-0 allocation rule g0 allocates the item to the bidder with the highest virtual value if the virtual value is greater than 0, and leaves the item unallocated otherwise. This can be approximated using a ‘softmax’ function on the virtual values b̄1, . . . , b̄n and an additional dummy input b̄n+1 = 0:\ng0i (b̄) = softmaxi(b̄1, . . . , b̄n+1; κ) = eκb̄i∑n+1 j=1 e κb̄j , ∀i ∈ N, (3)\nwhere κ > 0 is a constant that is fixed a priori, and determines the quality of the approximation. The higher the value of κ, the better is the approximation but the less smooth is the resulting allocation function (and thus it becomes harder to optimize).\nThe SPA-0 payment to bidder i (conditioned on being allocated) is the maximum of the virtual values from the other bidders, and zero:\nt0i (b̄) = max {\nmax j 6=i\nb̄j , 0 } , i ∈ N. (4)\nThe allocation rule for the overall mechanism (in Figure 3(a)) can be seen as a feed-forward network with a single layer of linear functions, followed by a layer of max-min operations and a softmax activation. The payment rule for the mechanism can be seen as a network with a layer of linear functions and a layer of max-min operations (both shared with the allocation rule), followed by a layer of max operations, another layer of linear functions (with parameters shared with the first layer), and finally a layer of min-max computations.\nLet gα,β and tα,β denote the allocation and conditional payment rules, where (α, β) are the parameters of the forward monotone transform. The goal is to optimize the parameters using the expected, negated revenue as the loss function:\nL(α, β) = −Ev∼F [ n∑ i=1 gα,βi (v) t α,β i (v) ] . (5)\nIn practice, given a sample of valuation profiles S = {v(1), . . . , v(L)} drawn i.i.d. from F , we work with an empirical loss function:\nL̂(α, β) = − 1 L L∑ `=1 n∑ i=1 gα,βi (v (`)) tα,βi (v (`)). (6)\nWe optimize the empirical loss over parameters (α, β) using a mini-batch stochastic gradient descent solver. We defer the implementation details to Section 4."
    }, {
      "heading" : "2.2 Multi-item Auction with Single Additive Bidder",
      "text" : "Our second setting concerns a single bidder with additive preferences over multiple items. Despite having received a lot of attention in the literature, there is no analytical solution to this problem\nfor more than six items [21]. The bidder holds a private value for each item v1, . . . , vm ∈ R≥0, and the valuation for a bundle is the sum of the values on individual items. In this case, a bid b ∈ Rm specifies the bid of the single bidder, rather than the bids from multiple bidders. It is known that the optimal auction in this setting may require randomization.\nWe consider a mechanism (g, p) with outcome rule g : Rm≥0 → [0, 1]m that map a bid b ∈ Rm≥0 to a lottery vector g(b) ∈ [0, 1]m, where the bidder receives item j independently with probability gj(b). The payment rule p : Rm≥0 → R≥0 maps the input values to the payment p(b) ∈ R≥0 the bidder will make for this lottery of items.\nWe make use of an implicit characterization for a single bidder problem that involves the bidder’s induced utility and its gradient [49].2 The utility function u : Rm≥0 → R induced by a mechanism (g, p) for a single bidder is given by:\nu(v) = m∑ j=1 gj(v) vj − p(v). (7)\nThis is the bidder’s utility for bidding truthfully when the bidder’s valuation is v. We say that the utility function is monotonically non-decreasing if u(v) ≤ u(v′) whenever vj ≤ v′j , ∀j ∈M .\nThe following theorem explains the connection between a DSIC mechanism and its induced utility function:\nTheorem 2.2 (Rochet [49]). A utility function u : Rm≥0 → R is induced by a DSIC mechanism iff u is 1-Lipschitz w.r.t. the `1-norm, non-decreasing, and convex. Moreover, for such a utility function u, ∇u(v) exists almost everywhere in Rm≥0, and wherever it exists, ∇u(v) gives the allocation probabilities for valuation v, and ∇u(v) · v − u(v) is the corresponding payment.\nTo find the optimal mechanism, we need to search over all utility functions that satisfy the above conditions, and pick the one that maximizes expected revenue. We also need to impose IR constraints. We define the penalty for violating IR as:\nirp(u) = Ev∼F [max{0,−u(v)}]. (8)\nThis captures the expected ex post IR violation. We want to solve the following optimization problem over all utility functions:\nsup u\nEv∼F [ ∇u(v) · v − u(v) ] (9)\ns.t. |u(v)− u(v′)| ≤ |v − v′|1, ∀v, v′ ∈ Rm≥0 u is monotonically non-decreasing, convex\nirp(u) = 0.\nTo model a convex, monotone and Lipschitz utility function, we use a max of J linear functions with non-negative coefficients:\nuα,β(v) = max j∈[J ] {wj · v + βj}, (10)\nwhere each wjk = 1/(1 + e −αjk), for αjk ∈ R, j ∈ [J ], k ∈ M , and βj ∈ R. By bounding the hyperplane coefficients to [0, 1], we guarantee that the function is 1-Lipschitz.\n2Daskalakis et al. [22] and Daskalakis et al. [24] also make use of Rochet’s characterization in their duality-based characterizations for the single-bidder multi item problem.\nTheorem 2.3. For any α ∈ RmJ and β ∈ RJ , the function uα,β is monotonically non-decreasing, convex and 1-Lipschitz w.r.t. the `1-norm.\nSee Appendix B for the proof. The utility function, represented as a single layer neural network, is illustrated in Figure 5, where each hj(b) = wj · b + βj . By using a large number of hyperplanes, one can use this neural network architecture to search over a sufficiently rich class of monotone, convex 1-Lipschitz utility functions. Once trained, the mechanism (g, p) can be derived from the gradient of the utility function, with the allocation rule given by:\ng(b) = ∇uα,β(b), (11)\nand the payment rule given by the difference between the expected value to the bidder from the allocation and the bidder’s utility:\np(b) = ∇uα,β(b) · b − uα,β(b). (12)\nHere the utility gradient can be computed as: ∇juα,β(b) = wj∗ , for j∗ ∈ argmaxj∈[J ]{wj · b + βj}. We seek to minimize the negated, expected revenue:\n−Ev∼F [ ∇uα,β(v) · v − uα,β(v) ] . (13)\nTo ensure that the objective is a continuous function of the parameters α and β (so that the parameters can be optimized efficiently), the gradient term is computed approximately by using a ‘softmax’ operation in place of the argmax. The loss function that we use is given by the negated revenue with approximate gradients:\nL(α, β) = −Ev∼F [ ∇̃uα,β(v) · v − uα,β(v) ] , (14)\nwhere ∇̃kuα,β(v) = ∑ j∈[J ] wjk softmaxj(w1 · v + β1, . . . ,wJ · v + βJ ; κ), (15)\nand κ > 0 is a constant that controls the quality of the approximation. We seek to optimize the parameters of the neural network to minimize loss subject to the IR penalty being zero:\ninf α∈RmJ ,β∈RJ\nL(α, β) s.t. irp(α, β) = 0, (16)\nwhere irp(α, β) = Ev∼F [max{0,−uα,β(v)}]. Given a sample S = {v(1), . . . , v(L)} drawn from F , we solve an empirical version of the above problem, where the IR constraint is enforced only on the valuations in S:\nmin α∈RmJ , β∈RJ\nL̂(α, β) s.t. îrp(α, β) = 0 (17)\nwhere\nL̂(α, β) = − 1 L L∑ `=1 ∇̃uα,β(v(`)) · v(`) − uα,β(v(`)) and îrp(α, β) = 1 L L∑ `=1 max{0,−uα,β(v(`))}.\nAs long as the sample is sufficiently large, we expect the trained mechanism to have low IR violations on valuations drawn from the distribution. The final mechanism is derived from the parameters of the trained neural network (see (11) and (12)), using the exact utility gradient.\nWe solve the above training problem using the augmented Lagrangian method. This method formulates a sequence of unconstrained optimization problems, where the IR constraint is enforced through a weighted term in the objective. More specifically, the solver works with the Lagrangian function, augmented with a quadratic penalty term for violating the IR constraint:\nCρ(α, β;λ) = L̂(α, β) + λ îrp(α, β) + ρ\n2\n( îrp(α, β) )2 , (18)\nwhere λ is a Lagrange multiplier, and ρ > 0 is a parameter that controls the weight on the quadratic penalty. The solver operates in multiple iterations, and performs the following updates in each iteration t:\n(αt+1, βt+1) ∈ argminα,β Cρ(α, β; λt) (19)\nλt+1 = λt − ρ îrp(α, β), (20)\nwhere the inner optimization in (19) is performed using mini-batch stochastic subgradient descent. We provide more details about this augmented Lagrangian method in Appendix A.\nA possible interpretation of the RochetNet architecture is that the network maintains a menu of (randomized) allocations and prices, and chooses the option from the menu that maximizes the bidder’s utility based on the bidder’s bid. Each linear function hj(b) = wj · b + βj in RochetNet corresponds to an option on the menu, with the allocation probabilities and payments encoded through the parameters wj and βj respectively. In our experiments, we find that RochetNet is able to almost precisely recover the optimal mechanism by finding the optimal menu of options."
    }, {
      "heading" : "3 Fully Agnostic Approach",
      "text" : "In the previous section, we developed a framework for using neural networks to design revenueoptimal auctions by exploiting known characterization results. We now develop a framework for using deep learning techniques to design mechanisms that have almost-optimal revenue and almostzero regret (and thus are almost DSIC). Although the learned auctions do not match quite so precisely existing analytical results in the theoretical literature, it is this direction that we consider most exciting, and most likely to yield the most significant progress in DSIC, revenue-optimal auction design. In this case, we model a mechanism using general feed-forward networks, and provide feedback to the learning algorithm through the revenue and regret from the network. We refer to these as the RegretNet architectures.\nWe describe our approach for a general setting with n bidders N and m items M . For ease of exposition, we consider additive bidders, though the approach easily extends to more general bidder valuations. We consider a randomized allocation rule g : Rnm≥0 → [0, 1]nm that takes a bid profile b ∈ Rnm≥0 as input, where bij is the bid from bidder i for item j. The rule outputs a vector of allocation probabilities g(b) ∈ [0, 1]nm, where gij(b) denotes the probability that bidder i is allocated item j, and ∑n i=1 gij(b) ≤ 1, ∀j ∈ M . The payment rule p : Rnm≥0 → Rn≥0 maps the bid profile to the expected payment for each bidder pi(b). We measure the deviation of a mechanism from DSIC and IR using the following metrics: Regret. We define the expected ex post regret from a mechanism (g, p) to bidder i as the expected maximum gain in utility that the bidder can receive through a non-truthful bid (knowing the bids of others):\nrgt i(g, p) = Ev∼F [\nmax v′i∈Vi\nui(v ′ i, v−i)− ui(vi, v−i)\n] . (21)\nIR penalty. We also define the penalty for violating individual rationality for bidder i as: irpi(g, p) = Ev∼F [ max{0,−ui(v)} ] . (22)\nLetM be a class of mechanisms described by neural networks, and that possibly do not satisfy DSIC or IR. As before, the loss function for a mechanism is defined as the negated expected revenue:\nL(g, p) = −Ev∼F [ n∑ i=1 pi(v) ] .\nThe goal is to minimize the loss over class M, subject to the regret and IR penalty of the mechanism being zero for each bidder:\nmin (g,p)∈M\nL(g, p) (23)\ns.t.[IC] rgt i(g, p) = 0, ∀i ∈ N, [IR] irpi(g, p) = 0, ∀i ∈ N.\nIn practice, the loss, regret and IR penalty are estimated from a sample S = {v(1), . . . , v(L)} drawn i.i.d. from F . The empirical loss is given by L̂(g, p) = 1L ∑L `=1 ∑n i=1 pi(v\n(`)). To estimate the regret, we use additional samples of deviating valuation profiles S` drawn i.i.d. from F for each profile v(`) in S, and compute the maximum utility gain over these deviating profiles:\nr̂gt i(g, p) = 1\nL L∑ `=1 max v′∈S` ui(v ′ i, v (`) −i )− ui(v (`)). (24)\nThe IR-penalty is estimated as:\nîrpi(g, p) = 1\nL L∑ `=1 max{0,−ui(v(`))}. (25)\nFor a large samples S and S` (for each `), we expect a mechanism that has zero empirical regret and zero empirical IR penalty will have low regret and IR penalty on the distribution.\nWe now describe the RegretNet architecture (see Figure 6). We use general feed-forward neural networks for both the allocation and payment rule.\nAllocation rule. The allocation rule is modeled as a feed-forward neural network containing R fully-connected hidden layers with sigmoidal activations and a fully-connected output layer. For a given bidder profile b, the network outputs a vector of allocation probabilities g1j(b), . . . , gnj(b) for each item j through a softmax activation function, with ∑n i=1 gij(b) ≤ 1. Bundling of items is possible because the output units corresponding to allocating items to the same bidder can be correlated.\nThe allocation network is parametrized as follows:\nh (1) j = σ(w (1) j · b), ∀j = 1, . . . , J1 h (k) j = σ(w (k) j · h (k−1)), ∀k = 2, . . . , R, j = 1, . . . , Jk\nsij = w (R+1) ij · h (R), ∀i = 1, . . . , n+ 1, j = 1, . . . ,m gwij (b) = softmax i(s1j , . . . , snj , sn+1,j) ∀i = 1, . . . , n, j = 1, . . . ,m,\nwhere weights w (1) j ∈ Rnm, w (k) j ∈ RJk−1 , for k = 2, . . . , R, and j = 1, . . . , Jk, and weights w (R+1) ij ∈ RJR , for i = 1, . . . , n, and j = 1, . . . ,m, and σ(z) = 1/(1 + e−z) is the sigmoid activation function. The softmax output for each item j: softmax i(s1j , . . . , snj , sn+1,j) = esij∑n+1\nk=1 e skj\ndenotes\nthe probability that bidder i gets the item (here sn+1,j is a dummy input that corresponds to the item not being allocated to any bidder). We use w ∈ Rd to denote the complete vector of parameters.\nPayment rule. The payment rule is modeled using a feed-forward neural network with T fullyconnected hidden layers and a fully-connected output layer. The network outputs a payment for each bidder i through a ReLU activation unit. The payment network is parameterized as follows:\nc (1) j = σ(w ′(1) j · b), ∀j = 1, . . . , J ′ 1 c (k) j = σ(w ′(k) j · c\n(k−1)), ∀k = 2, . . . , T, j = 1, . . . , J ′k si = w ′(T+1) i · c\n(T ), ∀i = 1, . . . , n pw ′\ni (b) = relu(si), ∀i = 1, . . . , n,\nwhere relu(z) = max{z, 0} ensures that the payments are non-negative, and weights w′(1)j ∈ Rnm,\nw ′(k) j ∈ R J ′k−1 , for k = 2, . . . , T + 1, and j = 1, . . . , J ′k. We use w ′ ∈ Rd′ to denote the complete vector of parameters. In our experiments, we use allocation and payment networks with two hidden layers (R = T = 2) and an output layer. Training. The training problem on sample S of valuation profiles is:\nmax w∈Rd,w′∈Rd′\nL̂(gw, pw′) (26)\ns.t.[IC] r̂gt i(g w, pw\n′ ) = 0, ∀i ∈ N,\n[IR] îrpi(g w, pw\n′ ) = 0, ∀i ∈ N.\nWe again use augmented Lagrangian optimization for training (see Section 2.2). This proceeds by solving a sequence of unconstrained optimization problems that combine the revenue, regret and IR penalty terms, with the relative weight on the regret and IR penalty terms adjusted across iterations. The details are deferred to Appendix A.\nSpecialized architectures. We also provide neural network architectures for the special cases of single-item auctions (Figure 7) and single additive bidder with multiple items. In each case, the payment rules continue to be modeled through a general feed-forward network.\nFor the allocation rule in the single-item setting, we apply monotone tranformation to each bid followed by a softmax allocation (see Figure 7). The transforms are modeled through a min-max over linear functions (see (1)), but unlike with the MyersonNet they need not be strictly monotone. By considering this specialized architecture, which prevents “entanglement” between the inputs of each bidder, we can interpret the trained allocation rule as a maximization over virtual valuation style transforms and compare to the optimal Myerson design.\nThe allocation rule in the single additive bidder, multiple items setting is modeled as a general feed-forward network, but specialized so that the network outputs a probability gwj (b) ∈ [0, 1] for each item j ∈ M , indicating if the item is allocated to the bidder, and hence uses a sigmoidal activation output for each item instead of a softmax function. This reflects that there is only a single bidder in this setting."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "We present experimental results that demonstrate that deep learning can be used to recover almostoptimal mechanisms for a variety of distributions, as well as to find new mechanisms for settings where there is no analytical solution for the optimal design.\nSetup. We use the TensorFlow deep learning library for implementing the neural network learning algorithms. We incorporate L2 regularization, with the regularization parameter set to 0.01. The learning rate for the solvers is set to 0.001, and the solvers are run for a maximum of 400,000 iterations. The batch size in the mini-batch stochastic gradient solver is set to 16 for RegretNet and to 100 for the other neural networks. In some cases, the parameters were tuned differently to obtain faster convergence (details deferred to Appendix C). Most experiments were run on a compute cluster with NVDIA GPU cores.\nEvaluation. We generate training and test sets from various value distributions, optimize the neural network design on the training set, and evaluate the revenue on the test set. In experiments within the agnostic framework (RegretNet), we also evaluate the regret, averaged across all bidders and test valuation profiles, Regret = 1n ∑n i=1 r̂gt i(f, p), and the IR penalty, averaged across all\nbidders and test valuation profiles, IR penalty = 1n ∑n\ni=1 îrpi(f, p) (see (24) and (25)). We use a sufficient number of valuation profiles and deviating bids in the training set to ensure that the learned auction mechanism does not “overfit” the training set, and yields similar values of revenue, IR penalty and regret on the test set (the specific sample sizes are provided below)."
    }, {
      "heading" : "4.1 Characterization Based Approaches",
      "text" : ""
    }, {
      "heading" : "4.1.1 Single-item Auction",
      "text" : "We evaluate MyersonNet for the design of single-item auctions on three regular distributions:\n(a) symmetric uniform distribution with 3 bidders and each vi ∼ U [0, 1] (b) asymmetric uniform distribution with 5 bidders and each vi ∼ U [0, i] (c) exponential distribution with 3 bidders and each vi ∼ Exp(3).\nWe study auctions with a small number of bidders because this is where revenue-optimal auctions are meaningfully different from efficient auctions. The optimal auctions for these distributions involve virtual valuations φi that are strictly monotone. We also consider an irregular distribution Firregular:\n(d) each vi is drawn from U [0, 3] with probability 3/4 and from U [3, 8] with probability 1/4.\nIn this irregular case, the optimal auction uses ironed virtual valuations that are not strictly monotone.\nThe training set and test set each have 1000 valuation profiles, sampled i.i.d. from the respective valuation distribution. We model each transform φi in the MyersonNet architecture using 5 sets of 10 linear functions, and set κ = 103. While we do not explicitly restrict the network parameters to a bounded range, the presence of L2 regularization in the training problem ensures that the parameters are bounded.\nThe results are summarized in Table 1. For comparison, we also report the revenue obtained by the optimal Myerson auction and the second price auction (SPA) without reserve. The virtual valuation functions inferred by the neural network are shown in Figure 9. For all three regular distributions, the auction trained by the neural network yields revenue close to the optimal and the monotone transforms closely match the optimal transforms.\nFor the irregular distribution, the trained auction also has revenue close to optimal. However, in this case, because of the strictness imposed on the value transforms, the learned mechanism converges to a design that is close to optimal but not quite optimal. Indeed, as can be seen in Figure 9(d), the network’s virtual valuation function contains a “kink” in a similar location as the optimal transform, but a strictly monotonic curve replaces the flattened portion of the optimal transform. We will later see that we are able to improve on the revenue obtained with MyersonNet by using the fully agnostic approach."
    }, {
      "heading" : "4.1.2 Multi-item Setting with Single Additive Bidder",
      "text" : "We also evaluate the RochetNet for designing multi-item mechanisms with a single bidder. We consider the following settings:\n(a) Additive Uniform I : single additive bidder with preferences over two items, where the item values v1, v2 ∼ U [0, 1], (b) Additive Uniform II : single additive bidder with preferences over two non-identically distributed items, where v1 ∼ U [4, 16] and v2 ∼ U [4, 7]. (c) Unit-demand Uniform I : single unit-demand bidder with preferences over two items, where the item values v1, v2 ∼ U [0, 1], (d) Unit-demand Uniform II : single unit-demand bidder with preferences over two items, where the item values v1, v2 ∼ U [2, 3], (e) Additive Uniform III : single additive bidder with preferences over ten items, where each vi ∼ U [0, 1].3\nFor the first distribution, we show that our approach is able to almost exactly recover the optimal mechanism of Manelli and Vincent [42]. For the second distribution, we show that the approach almost exactly recovers the optimal mechanism of Daskalakis et al. [24]. For the third and forth distributions, we show that the approach almost exactly recovers the optimal mechanisms of Pavlov [48].\nTo our knowledge, an analytical solution for the optimal mechanism for the fifth distribution is not available [21]. In this case, our approach finds a new mechanism that yields higher revenue than both a Myerson auction on each item and a Myerson on the entire bundle.\nThe training and test set each contain 5000 valuations. We model the induced utility function as a max network over 10 linear functions. In this case, we explicitly impose IR constraints in the training problem, and evaluate both the revenue and IR violations from the trained mechanism on the test set. For the unit-demand setting, we further constrain the weights of the utility network to ensure that the corresponding allocation rule assigns at most one item to each bidder.4\nThe results for the two-item distributions are summarized in Tables 2 and 3. In each case, the revenue of the trained mechanism is close to the optimal revenue, while incurring a very small IR penalty of 0.003 or less. We compare the allocation rules learned by the neural network with the\n3In a unit-demand setting, each bidder can be assigned at most one item. Hence, for this setting, we consider allocation rules that assign at most one item per bidder. 4This is done by constraining the incoming weights for each hidden unit in RochetNet to sum up to at most 1, i.e.∑m k=1 wjk ≤ 1, ∀j ∈ [J ]. It can be verified that the network is monotonically non-decreasing, convex, and Lipschitz even with these constraints on the weights.\noptimal rule in Figures 10-13. We can see that the allocation rule closely resembles the optimal rule. Moreover, the neural network not only matches the optimal revenue, but is also able to recover nontrivial decision regions in the optimal allocation rule. This is because for the valuation distributions considered, the optimal mechanism can be described by a finite menu of allocations and payments, and RochetNet effectively recovers the optimal menu of options for these distributions (see the discussion at the end of Section 2.2)\nIn Figure 14, we show the progress in test revenue and IR violations for the trained mechanism with increasing solver iterations when applied to the first distribution. The solver adaptively tunes the relative weight (Lagrange multiplier) on the IR penalty, focusing on the revenue in the initial iterations and on IR penalty in later iterations.\nIn the case of the uniform distribution on ten items, no analytical description of the optimal mechanism is available. Here, we use a RochetNet with 200 linear functions, and compare the resulting mechanism against two standard baselines: allocating using a Myerson auction for each item (in this case, a posted price mechanism with a price of 0.5), and using a Myerson auction on the bundle. The results are summarized in Table 4. The RochetNet is able to learn a new mechanism that yields higher revenue than standard mechanisms for this setting, while incurring small IR violations. This demonstrates the power of our framework in designing new mechanisms."
    }, {
      "heading" : "4.2 Fully Agnostic Approach",
      "text" : "We next apply the regret-based approach to the distributions evaluated above, as well as new distributions for which there are no characterization results, and show that we can recover a mechanism with revenue close to the optimal and with very low regret.5"
    }, {
      "heading" : "4.2.1 Single-item Auctions",
      "text" : "For single-item auctions, the RegretNet allocation rule passes each bid through a monotone transform (comprising 5 groups of 10 linear functions), followed by a softmax activation function to compute the allocation probabilities (see Figure 7). The payment rule uses a general feed-forward network with two hidden layers of 10 nodes each (T = 2), followed by an output layer that computes the payment to each bidder (conditioned on allocation).\n5 The RegretNet training problem in our experiments (see (26)) uses a stricter definition for regret than the one presented in (24). Specifically, we evaluate the regret for agent i as the maximum utility gain for the agent over all training profiles, intead of the mean utility gain: r̂gt i(g, p) = max`∈[L] maxv′∈S` ui(v ′ i, v (`) −i ) − ui(v\n(`)). This allows us to lay greater emphasis against DSIC violations in the training problem. On the other hand, we use the original definition for regret in (24) while evaluating the learned mechanism on the test set, where the regret is averaged across all bidders and test valuation profiles.\nThe results on the four distributions described previously are shown in Table 1. The RegretNet yields revenue close to optimal, while incurring a small regret of around 0.02 and a small IR penalty. In the case of the symmetric uniform distribution, the network is able to achieve a slightly higher revenue than the optimal auction because it has a non-zero regret and IR violations. The trained mechanism also has higher revenue than MyersonNet for the irregular distribution. Recall that MyersonNet is constrained to strictly increasing, monotone transforms.\nIn Figure 15, we show the monotone transforms that are learned by RegretNet. In most cases, the transforms are close to Myerson’s optimal virtual valuation functions. Note that Myerson’s virtual value functions are invariant to scaling by the same (positive) multiplicative factor across all bidders. The difference in slope for the exponential distribution can be explained by this scale invariance; otherwise the zero intercepts for the learned and optimal transforms are close. For the irregular distribution, the neural network approximately recovers the ironed portion of the optimal virtual valuation function. We also show how the revenue, regret and IR penalty vary with solver iterations in Figure 16. The solver aggressively reduces regret in the initial iterations and improves on revenue in later iterations."
    }, {
      "heading" : "4.2.2 Multi-item Setting with a Single Additive Bidder",
      "text" : "We also apply the regret-based approach to the single additive bidder setting. Here, the RegretNet allocation and payment networks each use two hidden layers with 10 nodes (R = T = 2 in Figure 8), followed by an output layer. The results for the two-item additive bidder distributions are shown in Table 2. Even without the aid of characterization results, RegretNet is able to identify mechanisms that have revenue close to the optimal revenue, and with low regret and IR violations. The results for the ten item distribution are shown in Table 4, where we can see that RegretNet identifies a new mechanism that yields higher revenue than the baselines, while incurring very small regret."
    }, {
      "heading" : "4.2.3 Multi-item Setting with Multiple Bidders",
      "text" : "Finally, we apply the regret-based approach to settings for which there are no characterization results. We consider a setting containing two bidders having additive preferences over two items,\nwith the following valuation distributions:\n(a) Discrete Uniform I : the item values for each bidder are drawn from identical uniform distributions over two values {0.5, 1.0} (b) Discrete Uniform II : the item values for each bidder are drawn from identical uniform distributions over three values {0.5, 1.0, 1.5} (c) Continuous Uniform: the item values for each bidder are drawn i.i.d. from identical uniform distributions over [0, 1].\nEven for these simple distributions, this setting is analytically difficult to solve. In fact, the optimal mechanism for this setting is only known for the first distribution [57]. For the discrete distributions, we compare the trained mechanism against a Myerson auction on each item and a Myerson auction on the entire bundle of items.\nFor the third distribution, Sandholm and Likhodedov [51] provide optimal mechanisms over specialized families of weighted affine maximizer mechanisms. We compare the trained mechanism with the optimal mechanisms obtained in their work, namely those from the VVCA and AMAbsym families.\nThe training and test set each contain 5000 valuations. Both the RegretNet allocation and payment networks contain two hidden layers with 10 nodes each (R = T = 2 in Figure 6), followed by an output layer. The results are summarized in Table 5 for the discrete distributions and in Table 6 for the continuous distribution. For the first distribution, RegretNet recovers the optimal mechanism, yielding revenue close to the optimal and incurring a small regret and IR penalty. For the second distribution, RegretNet is able to find a new mechanism that yields higher revenue than both the baselines, while incurring very small regret and IR violations. For the third distribution, RegretNet finds a new mechanism that is better than the results in Sandholm and Likhodedov\n[51], while incurring a small regret and IR penalty. This shows that RegretNet is able to discover new mechanisms in settings for which there are no useful characterizations of the space of optimal mechanisms.\nWe also show in Figure 17 how the test revenue and regret of the trained mechanism, when applied to the first distribution, change with increasing solver iterations. We also report the progress in the average absolute error between the learned allocation rule g and Yao’s [57] optimal allocation rule g∗ on the test set:\n1\nLmn L∑ `=1 n∑ i=1 m∑ j=1 |gij(v(`))− g∗ij(v(`))|,\nwhere gij(v) denotes the probability that allocation rule g assigns item j to bidder i for bid profile v. Not only does the learned mechanism converge to the optimal revenue with a negligible regret, but the structure of the learned allocation rule also closely approximates to the optimal allocation rule in terms of the allocation error."
    }, {
      "heading" : "5 Conclusion",
      "text" : "Our work shows that tools from machine learning can re-discover theoretical results such as Myerson [45]’s optimal single-item auction or the Manelli and Vincent [42] mechanism for a single additive bidder and two items, and provides some evidence for the helpfulness of techniques from deep learning beyond these settings.\nWe believe that this is the starting point of a very fruitful research agenda of machine-aided mechanism design, which could lead to new theoretical insight as well as new, practical mechanisms. It would be interesting, for example, to see if our framework can be used to obtain additional insights into the structure of optimal DSIC mechanisms. A particularly promising direction are auction settings with n > 1 bidders and two items, where Yao [57] provides a partial characterization.\nMore generally, the power of deep learning is that it can, through representation learning, find good or close-to-optimal designs in complex settings for which clean analytical characterizations are unlikely. The framework can, in principle, be extended to the design of mechanisms for settings with correlated values, to problems without money or with budget constraints, as well as to embrace\nadditional desiderate such as envy-free and stable outcomes. Technical questions of interest include whether, by training for longer periods of time, we can reliably drive regret down to essentially zero? Can we come up with useful representations for problems with combinatorial outcome spaces? Can we provide optimality guarantees for the augmented Lagrangian solver, perhaps along the lines of Kawaguchi [40]? Can the networks be constrained in some way to make the trained mechanisms interpretable?"
    }, {
      "heading" : "A Augmented Lagrangian Method for Constrained Optimization",
      "text" : "We give a brief description of the Augmented Lagrangian method for solving constrained optimization problems [7]. We use this method for solving neural network training problems involving equality constraints.\nConsider the following optimization problem with s equality constraints:\nmin w∈Rd C(w) (27)\ns.t. gj(w) = 0, ∀j = 1, . . . , s.\nThe augmented Lagrangian method formulates an unconstrained objective, involving the Lagrangian for the above problem, augmented with additional quadratic penalty terms that penalize violations in the equality constaints:\nCρ(w, λ) = C(w) + s∑ j=1 λjgj(w) + ρ 2 s∑ j=1 (gj(w)) 2,\nwhere λ = [λ1, . . . , λs] is a vector of Lagrange multipliers associated with the equality constraints, and ρ > 0 is a parameter that controls the weight on the penalty terms for violating the constraints. The method then performs the following sequence of updates:\nwt+1 ∈ argminw∈Rd Cρ(w, λt) (28) λt+1j = λ t j − ρ gj(wt+1). (29)\nOne can set the penalty parameter ρ to a very large value (i.e. set a high cost for violating the equality constraints), so that method converges to a (locally) optimal solution to the original constrained problem (27). However, in practice, this can lead to numerical issues in applying the solver updates. Alternatively, the theory shows that under some conditions on the iterates of the solver, any value of ρ above a certain threshold will take the solver close to a locally optimal solution to (27) (see e.g. Theorem 17.6 in [55]).\nIn our experiments, we apply the augmented Lagrangian method to solve neural network revenue optimization problems, where we implement the inner optimization within the solver updates using mini-batch stochastic subgradient descent. We find that even for small values of ρ, with sufficient number of iterations, the solver converges to auction designs that yield near-optimal revenue while closely satisfying the IR/regret constraintss (see experimental results in Sections 4.1.2, 4.2.1 and 4.2.2). The specific choices of ρ and number of solver iterations in our experiments are provided in Appendix C.\nFinally, we point out that the described method can also be applied to optimization problems with inequality constraints hj(w) ≤ 0 by formulating equivalent equality constraints of the form max{0, hj(w)} = 0."
    }, {
      "heading" : "B Proof for Theorem 2.3",
      "text" : "Proof. The convexity of uα,β follows from the fact it is a ‘max’ of linear functions. We now show that uα,β is monotonically non-decreasing. Let hj(v) = wj · v + βj . Since wj is non-negative in all entries, for any vi ≤ v′i, ∀i ∈M , we have hj(v) ≤ hj(v′). Then\nuα,β(v) = max j∈[J ] hj(v) = hj∗(v) ≤ hj∗(v′) ≤ max j∈[J ] hj(v ′) = uα,β(v ′),\nwhere j∗ ∈ argminj∈[J ] hj(v). It remains to be shown that uα,β is 1-Lipschitz. For any v, v′ ∈ Rm≥0,\n|uα,β(v)− uα,β(v′)| = |max j∈[J ] hj(v) − max j∈[J ] hj(v ′)| ≤ max j∈[J ] |hj(v′) − hj(v)|\n= max j∈[J ] |wj · (v′ − v)| ≤ max j∈[J ] ‖wj‖∞ |v′ − v|1 ≤ |v′k − vk|1\nwhere the last inequality holds because each component wjk = σ(αjk) ≤ 1."
    }, {
      "heading" : "C Supplementary Experimental Details",
      "text" : "The learning rate in MyersonNet is set to 0.01 for all the single-item distributions. The learning rate in RochetNet is to 0.001 for the 2-item distributions and to 0.01 for the 10-item distribution. The learning rate in RegretNet is set to 0.001 for all the single-item distributions except the exponential distribution, to 0.005 for the single-item exponential distribution, to 0.001 for the 2- item 1-bidder symmetric uniform distribution, to 0.005 for the 2-item 1-bidder asymmetric uniform distribution, to 0.001 for the 10-item 1-bidder uniform distribution, to 0.005 for the 2-item 2-bidder discrete uniform I distribution, to 0.005 for the 2-item 2-bidder discrete uniform II distribution, and to 0.002 for the 2-item 2-bidder continuous uniform distribution. We sometimes were able to converge to a mechanism with higher revenue (and lower regret) by reducing the learning rate after a few thousand iterations.\nFor RochetNet, the parameter ρ in the augmented Lagrangian solver is set to 0.1 for the 2-item distributions, and to 0.005 for the 10-item distributions. For RegretNet, the parameter ρ in the augmented Lagrangian solver is set to 0.001 for the symmetric uniform single-item auction, to 0.01 for the discrete uniform I distribution, and to 0.005 for all other distributions.\nD Additional Experimental Results"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "<lb>Designing an auction that maximizes expected revenue is an intricate task. Indeed, as of<lb>today—despite major efforts and impressive progress over the past few years—only the single-<lb>item case is fully understood. In this work, we initiate the exploration of the use of tools<lb>from deep learning on this topic. The design objective is revenue optimal, dominant-strategy<lb>incentive compatible auctions. We show that multi-layer neural networks can learn almost-<lb>optimal auctions for settings for which there are analytical solutions, such as Myerson’s auction<lb>for a single item, Manelli and Vincent’s mechanism for a single bidder with additive preferences<lb>over two items, or Yao’s auction for two additive bidders with binary support distributions and<lb>multiple items, even if no prior knowledge about the form of optimal auctions is encoded in<lb>the network and the only feedback during training is revenue and regret. We further show how<lb>characterization results, even rather implicit ones such as Rochet’s characterization through<lb>induced utilities and their gradients, can be leveraged to obtain more precise fits to the optimal<lb>design. We conclude by demonstrating the potential of deep learning for deriving optimal<lb>auctions with high revenue for poorly understood problems.",
    "creator" : "LaTeX with hyperref package"
  }
}