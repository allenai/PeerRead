{
  "name" : "1704.03079.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "WRPN: Training and Inference using Wide Reduced-Precision Networks",
    "authors" : [ "Asit Mishra", "Jeffrey J Cook", "Eriko Nurvitadhi", "Debbie Marr" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n03 07\n9v 1\n[ cs\n.L G\n] 1\n0 A\npr 2\n01 7"
    }, {
      "heading" : "1. Introduction",
      "text" : "Deep learning for robotics vision demands highly efficient real-time solutions. A promising approach to deliver one such extremely efficient solution is through the use of low numeric precision deep learning algorithms. Operating in lower precision mode reduces computation as well as data movement and storage requirements. Due to such efficiency benefits, there are many existing works that have proposed low-precision deep neural networks (DNNs), even down to 2-bit ternary mode [5] and 1-bit mode [4, 1]. However, the majority of existing works in low-precision DNNs sacrifice accuracy over the baseline full-precision networks. Further, most prior works target reducing the precision of the model parameters (network weights). This primarily benefits the inference step only when batch sizes are small.\nWe observe that activationmaps (neuron outputs) occupy more memory compared to the model parameters for batch sizes typical during training. This observation holds even during inference when batch size is 8 or more for modern networks. Based on this observation, we study schemes for training and inference using low-precision DNNs where we reduce the precision of activation maps as well as the model parameterswithout sacrificing the network accuracy. We reduce the precision of activation maps (along with model pa-\nrameters) and increase the number of filter maps in a layer. We call networks using this scheme wide reduced-precision networks (WRPN) and find that this scheme compensates or surpasses the accuracy of the baseline full-precision network. Although the number of raw compute operations increase as we increase the number of filter maps in a layer, the compute bits required per operation is now a fraction of what is required when using full-precision operations.\nWe present results using our scheme on AlexNet and ResNet on ILSVRC-12 dataset. Our results show that the proposed scheme offer better accuracies on ILSVRC-12 dataset, while being computationally less expensive compared to previously reported reduced-precision networks. Further, our reduced-precision quantization scheme is hardware friendly allowing for efficient hardware implementations of such networks for servers, deeply-embedded and real-time deployments."
    }, {
      "heading" : "2. WRPN Scheme",
      "text" : "While most prior works proposing reduced-precision networks work with low precision weights (e.g. [1, 5, 4, 3]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs. Using minibatches of inputs is typical in training of DNNs and for multi-modal inference [2]. Figure 1 shows the memory footprint of activation maps and filter maps as batch size changes for 4 different networks during the training and inference steps. As batch-size increases, because of the filter reuse aspect across batches of inputs, the activation maps occupy significantly larger fraction of memory compared to the filter weights.\nBased on this observation, we reduce the precision of activation maps for DNNs to speed up training and inference steps as well as cut down on memory requirements. However, a straightforward reduction in precision of activation maps leads to significant reduction in model accuracy. This has been reported in prior work as well [4]. We conduct a sensitivity study where we reduce the precision of activation maps and model weights for AlexNet network using ILSVRC-12 dataset. Table 1 reports our findings. We find that reducing the precision of activation maps hurts model accuracymuchmore than reducing the precision of the filter parameters.\nTo re-gain the model accuracy while working with reduced-precision operands, we increase the number of filter maps in a layer. Increasing the number of filter maps increases the layer compute complexity linearly. Another knob we tried was to increase the size of the filter maps. However increasing the filter map spatial dimension in-\n1\ncreases the compute complexity quadratically. Although the number of raw compute operations increase linearly as we increase the number of filter maps in a layer, the compute bits required per operation is now a fraction of what is required when using full-precision operations. As a result, with appropriate support, one can significantly reduce the dynamic memory requirements, memory bandwidth, computational energy and speed up the training and inference process.\nTable 1 also reports the accuracy of AlexNet when we double the number of filter maps in a layer. We find that with doubling of filter maps, AlexNet with 4b weights and 2b activations exhibits accuracy at-par with full-precision networks. Operating with 4b weights and 4b activations, surpasses the baseline accuracy by 1.44%. When doubling the number of filter maps, AlexNet’s raw compute operations grow by 3.9x compared to the baseline full-precision network, however by using reduced-precision operands the overall compute complexity is a fraction of the baseline. For example, with 4b operands for weights and activations and\n2x the number of filters, reduced-precision AlexNet is just 14% of the total compute cost1 of the full-precision baseline.\nWe also study how our scheme applies to bigger networks. For this we study widening the number of filters in ResNet-34. When doubling the number of filters in ResNet34, we find that 4b weights and 8b activations surpass the baseline accuracy (baseline full-precision ResNet-34 is 73.2% top-1 accuracy and our reduced-precision ResNet-34 is 73.8% top-1 accuracy). The reduced-precision ResNet is 15.1% the compute cost of full-precision ResNet."
    }, {
      "heading" : "2.1. Hardware friendly quantization scheme",
      "text" : "For quantizing tensor values, we first constrain the weight values (W) to lie within the range {-1,+1} and activation values (A) within the range {0,1}. This is followed by a k-bit quantization of the values within the respective interval. Since a k-bit number can represent 2k numbers, we represent the quantizedweight values as round((2k−1−1)∗W )\n2k−1−1\nand the quantized activation values as round((2k−1)∗A)\n2k−1 . One\nbit is reserved for sign-bit in case of weight values hence the use of 2k−1 for these quantized values. With appropriate affine transformations, the convolution operations (the bulk of the compute operations in the network) can be done using quantized values followed by scaling with floatingpoint constants. This makes our scheme hardware friendly and can be used in embedded systems. This is the quantization scheme for all the results reported above for AlexNet and ResNet."
    }, {
      "heading" : "3. Conclusions",
      "text" : "Vision, speech and NLP based applications have seen tremendous success with DNNs. However, DNNs are compute intensive workloads. In this paper, we present WRPN scheme to reduce the compute requirements of DNNs. While most prior works look at reducing the precision of weights, we find that activations contribute significantly more to the memory footprint than weight parameters when using mini-batches and thus aggressively reduce the precision of activation values. Further, we increase the number of filter maps in a layer and show that widening of filters and reducing precision of network parameters does not sacrifice model accuracy. Our scheme is hardware friendly making it viable for deeply embedded system deployments which is useful for robotics applications."
    } ],
    "references" : [ {
      "title" : "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1",
      "author" : [ "M. Courbariaux", "Y. Bengio" ],
      "venue" : "CoRR, abs/1602.02830,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Multi-modal variational encoder-decoders",
      "author" : [ "I.V. Serban", "A.G.O. II", "J. Pineau", "A.C. Courville" ],
      "venue" : "CoRR, abs/1612.00377,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Accelerating deep convolutional networks using low-precision and sparsity",
      "author" : [ "G. Venkatesh", "E. Nurvitadhi", "D. Marr" ],
      "venue" : "CoRR, abs/1610.00324,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients",
      "author" : [ "S. Zhou", "Z. Ni", "X. Zhou", "H. Wen", "Y. Wu", "Y. Zou" ],
      "venue" : "CoRR, abs/1606.06160,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Trained ternary quantization. CoRR, abs/1612.01064, 2016. 1 Compute cost is the product of the number of FMA operations and the width of the activation and weight operands",
      "author" : [ "C. Zhu", "S. Han", "H. Mao", "W.J. Dally" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Due to such efficiency benefits, there are many existing works that have proposed low-precision deep neural networks (DNNs), even down to 2-bit ternary mode [5] and 1-bit mode [4, 1].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 3,
      "context" : "Due to such efficiency benefits, there are many existing works that have proposed low-precision deep neural networks (DNNs), even down to 2-bit ternary mode [5] and 1-bit mode [4, 1].",
      "startOffset" : 176,
      "endOffset" : 182
    }, {
      "referenceID" : 0,
      "context" : "Due to such efficiency benefits, there are many existing works that have proposed low-precision deep neural networks (DNNs), even down to 2-bit ternary mode [5] and 1-bit mode [4, 1].",
      "startOffset" : 176,
      "endOffset" : 182
    }, {
      "referenceID" : 0,
      "context" : "[1, 5, 4, 3]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 4,
      "context" : "[1, 5, 4, 3]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 3,
      "context" : "[1, 5, 4, 3]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 2,
      "context" : "[1, 5, 4, 3]), we find that activation maps occupy a larger memory footprint when using mini-batches of inputs.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 1,
      "context" : "Using minibatches of inputs is typical in training of DNNs and for multi-modal inference [2].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "This has been reported in prior work as well [4].",
      "startOffset" : 45,
      "endOffset" : 48
    } ],
    "year" : 2017,
    "abstractText" : "For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks but also that reducing the precision of activations hurts model accuracy much more than reducing the precision of model parameters. We study schemes to train networks from scratch using reduced-precision activations without hurting the model accuracy. We reduce the precision of activation maps (along with model parameters) using a novel quantization scheme and increase the number of filter maps in a layer, and find that this scheme compensates or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly reduce the dynamic memory footprint, memory bandwidth, computational energy and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN wide reduced-precision networks. We report results using our proposed schemes and show that our results are better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.",
    "creator" : "LaTeX with hyperref package"
  }
}