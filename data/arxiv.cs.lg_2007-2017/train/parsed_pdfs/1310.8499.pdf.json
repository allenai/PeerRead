{
  "name" : "1310.8499.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Deep AutoRegressive Networks",
    "authors" : [ "Karol Gregor", "Andriy Mnih", "Daan Wierstra" ],
    "emails" : [ "KAROL@DEEPMIND.COM", "ANDRIY@DEEPMIND.COM", "DAAN@DEEPMIND.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Modelling data distributions is one of the primary goals of machine learning. There exists now a substantial literature in this domain. One class of models, based on undirected generative models, includes examples such as Restricted Boltzmann Machines (RBMs; Smolensky, 1986), Deep Belief Networks (DBNs; Hinton et al., 2006), and Deep Boltzmann Machines (DBMs; Salakhutdinov & Hinton, 2009). These models have been shown to produce good samples, yet rely on an iterative procedure of Markov Chain Monte Carlo for both training and sampling, which makes sampling prohibitively slow for many applications. Another premier class of models, regularized autoencoders (Bengio et al., 2013; Bengio & Thibodeau-Laufer, 2013), rely on\nsimilar iterative methods for sampling and suffer from similar difficulties.\nIn this paper, we propose Deep AutoRegressive Networks (DARN), a new generative latent variable model for binary high-dimensional data. The model consists of one or more hidden layers of stochastic binary units, followed by a visible layer. All layers have autoregressive connections, so that each unit receives input from the preceding units in the same layer in addition to input from the units in the layer immediately above.\nOne major advantage of DARN over the aforementioned models is that generating observations from it is efficient. To produce an exact sample from the model, we simply perform a top-down pass, starting at the deepest hidden layer and sampling one unit at a time, layerwise and in autoregressive fashion. During training the model encodes a given data sample as hidden unit activations, and it optimizes its parameters to be able to minimize the total information that needs to be stored in order to reconstruct the input, that is, to maximize the compression of the data.\nDARN is closely related to a family of autoregressive models, of which the Neural Autoregressive Distribution Estimator (NADE) (Larochelle & Murray, 2011; Gregor & LeCun, 2011) is a prime example. NADE and other such models order the input into a sequence and sample every given input unit from the previously sampled ones. The key distinguishing feature of DARN relative to other autoregressive methods is that autoregressivity now applies to the latent variables as well as the visible units.\nThe following sections first describe the model’s objective, after which the model is described in detail. Subsequently, experiments and analysis are presented, including results using a deep locally connected version of DARN which enables the algorithm to scale up to large images. The paper concludes with a short discussion of both the merits and disadvantages of the method.\nar X\niv :1\n31 0.\n84 99\nv1 [\ncs .L\nG ]\n3 1\nO ct\n2 01\n3"
    }, {
      "heading" : "2. Using models to compress data",
      "text" : "A probabilistic model can be seen as a means of compressing data. A good model can describe any given observation compactly in terms of the states of its latent variables. Prefixing the observation with the states of the latent variables produces a sequence that is easier to compress than the observation alone because the redundancy in the data is made more explicit. Training a model then simply amounts to adjusting its parameters to minimize the amount of information needed to encode the sequence. In this section we explain how to measure this information.\nThe basic unit of information is a bit. It is the amount of information we need to store to describe the state of a binary random variable that is equally likely to be 0 or 1. If one of the two values of the variable is much more likely than the other, we will be able to describe its state with less than one bit on average. Suppose we would like to compress a binary sequence, for example s = 00001000000000010000000100001000..., losslessly. Intuitively, this should be possible as it does not look very random and 0s are much more frequent in it than 1s. For example, if we are certain that the next digit is zero then we do not need to use a bit for it. However, typically we will not be completely certain about the next digit in the sequence, so instead of specifying the digit directly, we quantify our uncertain belief by specifying the probability of the digit being 1. These probabilistic predictions can then be used to compress the sequence, with better predictions leading to better compression.\nSuppose we have a model that specifies the probability pi of ith bit being 1 given the values of previous bits 1, . . . , i−1. We can then apply arithmetic coding (MacKay, 2003) to the probabilities produced by the model to store the sequence using the following number of bits (on average): L(s) = ∑ i L(si) = ∑ i (−si log2(pi)−(1−si) log2(1−pi)), (1) plus a small fixed overhead. For example, if we predict with probability 1 − pi = 0.99 that the bit is 0 and the bit is actually 0, then we “pay” Li = − log2(1−pi) = 0.0145 bits for that bit, but if we are wrong and the bit is 1 we pay Li = − log(pi) = 6.64 bits. If the bit is random with probability 0.99 of being 0, then on average we would pay 0.99 ∗ 0.0145 + 0.01 ∗ 6.64 = 0.081 bits – a lot less then if it were encoded directly with 1 bit. The model introduced in this paper is exactly such a predictive model – it successively predicts each bit from the previous ones."
    }, {
      "heading" : "3. Deep AutoRegressive Networks",
      "text" : "We will now describe our approach to compression-based probabilistic modelling. After giving the abstract outline\nof the approach, we will successively apply it to a model with a single hidden layer, a deep model, and a model with partial connectivity.\nThe models presented in this paper are meant to be applied to binary sequential data. If the input is not a binary sequence, we turn it into one. If the model is a binary image, we produce a sequence by reading it from top left to bottom right as if reading a page, but one can use any order."
    }, {
      "heading" : "3.1. The general model",
      "text" : "The model operates as follows:\n1) Apply a nonlinear transformation to a data vector x to produce a new binary sequence h(x). Concatenate the two sequences to produce s = s(x) = h(x), x. Note that the transformation need not be deterministic.\n2) Create a model that predicts every bit in the sequence s from previous bits. That is, specify a set of functions gi such that pi ≡ p(si = 1|s1, . . . , si−1) = gi(s1, . . . , si−1).\n3) Optimize the parameters of functions h and g to make them better at predicting the sequence s, minimizing L(s) (1). This is done using stochastic gradient descent (SGD) with gradients calculated by back-propagation.\nTo generate observations from the model, we start by sampling s1 from p1 (that is setting s1 = 1 with probability p1 and s1 = 0 with probability 1 − p1). Having sampled s1, . . . , si−1 we sample si by setting it to 1 with probability pi = gi(s1, . . . , si−1), computed from the already sampled values. Discarding the part of the generated s that corresponds to h(x), we obtain an independent sample from the model.\nThe model we described here is exactly the type of model that an arithmetic compressor uses, as discussed in Section 2. The size of the compressed observation in bits is given by the cost obtained in step 3) up to a small constant.\nThe above approach can also be interpreted in the context of graphical models. From that perspective it consists of two parts: 1) a specific type of directed graphical model with hidden layers and autoregressive connections as described in the next subsections and 2) a tractable inference and training procedure based on back-propagation through binary units."
    }, {
      "heading" : "3.2. A one-hidden-layer instantiation",
      "text" : "In this section we describe a one-hidden-layer instantiation of DARN. All the related computations are also shown in Figure 1. For this model the steps from the previous subsection are as follows.\n1) The encoding function h becomes\nh(x) = β(σ(U · x+ b)) (2)\nwhere U is a weight matrix, b a bias vector, σ(x) = 1/(1 + exp(−x)) is the logistic function and β is a binarizing function, which is either 1) stochastic: β(p) = 1 with probability p and 0 otherwise, or 2) deterministic: β(p) = 0 if p < 0.5 and β(p) = 1 otherwise;\n2) The bits of h(x) are predicted using\nphi ≡ p(h(x)i = 1|h(x)1, . . . , h(x)i−1) = σ( ∑ j<i Shijh(x)j + b h i ), (3)\nwhile the bits of x are predicted using\npxi ≡ p(xi = 1|h, x1, . . . , xi−1) = σ((W · h)i + ∑ j<i Sxijxj + b x i ). (4)\nHere Sh and Sx are autoregressive matrices, W is a weight matrix, bh and bx are vectors, and · denotes matrix multiplication.\n3) We minimize the objective\nL(s) = − ∑ i (hi log2 p h i + (1− hi) log2(1− phi ))\n− ∑ i (xi log2 p x i + (1− xi) log2(1− pxi ))(5)\nwith respect to parameters θ = {U, b, Sh, Sx,W, bh, bx}.\nWe optimize Eq. 5 using SGD with gradients computed by back-propagation. Though the functions given by Eqns. 2 and 4 are differentiable, the binarizing functions are not. We back-propagate through the binarizing function by copying gradients, that is if y = β(x) then dLdx ← dL dy . We also apply a sparsity penalty to the hidden unit activations for some number of updates after the start of training (typically one sweep through the training data). We use either the KL-divergence sparsity penalty or the following procedure: if a fraction of the inputs is larger then the desired maximum fraction, we apply a gradient proportional to the excess activation to suppress it. We also found that not propagating gradients from Eq. 3 when updating U, b early in training improved performance. A reason is might be that at the beginning, the features produced by U and b are not useful for compression so the model learns this – predicting the h to be zero. Once this has happened it is very difficult for the model to learn to use the hidden units to make better predictions."
    }, {
      "heading" : "3.3. A deep instantiation",
      "text" : "In this section we present a model with several hidden layers. The model is obtained from the general one, by forming h(x) as a concatenation of the hidden layers. In the first step a given layer is obtained from the lower layer(s)/input using a nonlinear function. In the second step the probability of encoding a bit in a given layer is calculated from the hidden layer(s) above and from the bits already predicted in the given layer. In the third step, optimization is performed in an analogous fashion to the previous subsection.\nWe assume we have n hidden layers denoted by ha and the input layer denoted by h0 = x. Between the layers it is easy to implement arbitrary differentiable functions, which we denote by fa and ga for a = 1, . . . , n. The following\nare typical simple examples:\nf, g(y) = W · y + b (6) f, g(y) = W · tanh(W̃ · y + b̃) + W̄ · y + b (7)\nwhere the parameters to be learned are the matrices W , W̃ , W̄ and vectors b, b̃ (those that are present). These parameters are different for different layers and different between f and g. For simplicity we omitted the layer indices in the formulas above.\n1) h(x) is obtained by applying the following formula for a = 1, . . . , n:\nha = β(σ(fa(ha−1))). (8)\n2) Given the values of the preceding units, the probability that unit i in layer a is 1, is\np(hai = 1|previous) = σ(ga+1(ha+1)i + ∑ j<i Saijh a j ) (9)\nwhere gn+1(hn+1) is a vector and Sas are the autoregressive matrices.\n3) Optimization. The optimization is analogous to the single-layer case from the previous subsection. At the beginning of the training, the gradients from the costs only flow to the layer above, requiring the hidden layers to “reconstruct” layer below, but not penalizing the activations."
    }, {
      "heading" : "3.4. Partial connectivity",
      "text" : "We can make the model scale better to high-dimensional data by connecting units only to a subset of units in the adjacent layers (or, autoregressively the same layer). This is particularly useful for modelling large images. A common way of dealing with images is through local connectivity, either fully convolutional (LeCun et al., 1998) or with less or even no weight sharing. We use the periodic local connectivity of (Gregor & LeCun, 2010) in the large input example below. Such local connectivity is obtained by laying out the hidden units onto a two dimensional grid “over” the image and connecting a given unit only to a neighbourhood of inputs underneath it. If weight sharing is desired, the weights are shared if their locations are shifted from each other by a multiple of integer distances in each of the x and y directions (see the reference for details). This reduces to a convolutional net when these integer distances are both 1.\nOur algorithm operates naturally with any partial connectivity pattern – we simply make all the matrices, including the S matrices, partially connected. Specifically, given a matrix W that calculates yi = ∑ j Wijxj , partial connectivity means that for every i the sum includes j if and only if the yi is connected to xj ."
    }, {
      "heading" : "4. Analysis",
      "text" : "In this section, we first discuss the computational cost of the model and then elaborate on the relation to graphical models and the estimation of the data log-likelihood."
    }, {
      "heading" : "4.1. Computational cost",
      "text" : "We look at the computational cost of the deep model, of which the one-hidden-layer model is a special case. We will consider the cost of calculating the hidden sequence, calculating the probabilities, and optimization. All operations required to compute the hidden sequence are matrix multiplications and point-wise nonlinearities. When computing probabilities, we use matrix multiplications between layers and predict one unit at a time in each layer. However, the latter process can also be implemented as matrix multiplication – we simply set Sij = 0 if j ≥ i and predict a given unit from all the units in this layer. In the training phase we update this matrix as if it were a full matrix and then zero out the appropriate entries. Zeroing out is done by precomputing a masking matrix (Smij = 0 if j ≥ i and Smij = 1 otherwise) at the beginning of training and multiplying the S matrix by the masking matrix after the weight update. All other operations are matrix operations and point-wise nonlinearities. These are very common operations and thus the training of the whole system can be easily implemented on graphical processing units (GPUs).\nOne key property of DARN models is their ability to quickly generate independent samples. Let us look the computation cost of the one-hidden-layer model from Section 3.2. Let nx and nh be the input dimensionality and the hidden layer dimensionality respectively. The computational cost of generation (and also of training) is O(n2x + nxnh + n 2 h). Importantly, there is no hidden multiplying constant – we iterate over the units only once. This is different from many other models such as RBMs, DBMs, and autoencoders, where an iterative process is needed to obtain samples and where samples are not independent but a new sample is obtained as a “step on a manifold” from an old sample. The generative cost of DARN is similar to the cost of NADE except that the nonlinearity in the hidden layer is evaluated nh times in DARN and nhnx times in NADE. In our estimate, evaluating the nonlinearity takes about 5 times as much time as multiplication.\nHowever, we can reduce the cost of sampling from DARN even further. First, the representations in the hidden layers of DARN are sparse. Let ns be a typical number of nonzero hidden units (in a typical experiment below, we have ns = 20 with nh = 400). During generation we first check if a unit is nonzero and only then we need to perform an appropriate vector operation. The cost of hidden-tohidden (autoregressive) computation goes down from n2h to nsnh, and that of hidden-to-input computation from nhnx\nto nsnx. The cost of input-to-input (autoregressive) computation is still the same, n2x, except when the input is also sparse and we can use the same trick. However, we find that model generates high-quality samples even without the input autoregressive matrix. We call the sparse model without input autoregressive matrix fast DARN (fDARN). The computational cost of generation from it is ns(nh + nx)."
    }, {
      "heading" : "4.2. Relation to graphical models",
      "text" : "DARN can be seen as a generative model over the extended sequence s = h, x (the concatenation of the hidden layers and the input), which predicts each bit from the previous ones. Over this sequence the model naturally factorizes as p(s) = p(s1)p(s2|s1)p(s3|s1, s2) . . ., where each term is parameterized as a deterministic feedforward network. Graphical models are typically trained by maximizing likelihood or, equivalently, minimizing the negative log-likelihood of the data L = − log(p(x)). Here p(x) is the probability of input, which equals p(x) =∑\nh p(x|h)p(h). This amounts to choosing the setting of parameters that assigns the highest probability to the training data under the model. This choice of parameters also results in the theoretically optimal compression rate for the data. However, this compression rate cannot be achieved in practice because it requires computing p(x) by summing out h, which is an intractable operation in general, and so approximations have to be used. As the generative part of DARN is a graphical model, the approximate learning and inference methods developed for graphical models can be applied to it.\nSteps 1) and 3) of DARN constitute inference and training procedures, respectively. Given an input x we obtain a hidden layer sequence h(x) using a (possibly stochastic) function and optimize for coding cost L̄ = − log(p̄(x)) where p̄(x) = p(x|h(x))p(h(x)) (maximizing compression or minimizing description length). These steps allow us to optimize the coding cost using back-propagation. To understand how this criterion relates to the log-likelihood, let us consider deterministic encoding (with the stochastic case being similar). The likelihood of generating an input x is given by p(x) = ∑ h p(x|h)p(h) ≥ p(x|h(x))p(h(x)) = p̄(x), which implies L ≤ L̄. Thus the coding cost L̄ is an upper bound on the negative log-likelihood L. We can get a better estimate of the negative log-likelihood under our model by summing over an arbitrary set of h. Including more hs, especially the ones contributing larger p(x|h)p(h) values, leads to better estimates. We will use this procedure to evaluate our models below.\nNoting that adding extra hidden layers to models such as RBMs strictly improves their representational power (Le Roux & Bengio, 2008), we could ask whether that is also the case for DARN. Adding a layer cannot make the\ncoding cost worse because we can initialize the added layer to always have zero activity, thus not adding any bits to the cost. The optimization algorithm will then change the parameters to turn the units on if they help to compress the data better. We do expect this to happen because extra layers make the model more expressive and thus better at prediction."
    }, {
      "heading" : "5. Results",
      "text" : "We trained our models on three datasets. For the first experiment, we used the MNIST handwritten digits dataset, consisting of 60000 training and 10000 testing 28× 28 images of hand-written digits. After being rescaled to the [0, 1] interval, the image intensities are nearly binary, with most values close to either 0 or 1. We make the dataset fully binary in two ways: we either 1) set pixels with values greater then 0.5 to 1 and the remaining pixels to zero, (Gregor & LeCun, 2011), Figure 2 top left; or 2) use the samplingbased binarization of (Larochelle & Murray, 2011). In this paper we mostly use the former, except for the results in Table 1 obtained using the latter. We find that the results fairly consistent between the two.\nWe started by trained models with and without autoregres-\nsive matrix on the inputs (DARN and fDARN respectively). We compared the coding costs for the resulting models to that of NADE (Larochelle & Murray, 2011) and FVSBN (Frey, 1998), a model consisting of only an autoregressive matrix in the input layer, which can be seen as DARN without the hidden layer. The coding costs in bits on the testing (training) set were as follows: FVSBN 109 (109), DARN 103.5 (93.8), fDARN 116.6 (106.2), NADE 91.4. However, ideally we would like to compare the models based on their negative log-likelihoods and while these are the same as the coding costs for FVSBN and NADE, for (f)DARN the coding cost is only an upper bound on the negative loglikelihood. We can get a better estimate of likelihood by using the identity p(x) = ∑ h p(x|h)p(h), as discussed in Section 4.2. We approximate this sum using about 1600 hidden vectors for a given x which we obtain as follows. First, we sample 1600 samples from the stochastic encoder and prune the duplicates. Then we consider random 1- or 2-bit flips around these samples and prune the duplicates. The resulting numbers for the test set are DARN: 97.26 bits and fDARN: 107.16 bits. For the comparison on the stochastic binarization of MNIST see Table 1.\nThough these numbers are not quite on par with those of NADE, the samples from (f)DARN look just as good. One possible explanation for this is that our bound on the negative log-likelihood is quite loose and its true value is considerably lower. To test this theory we will need a better way of computing the bound. Another possibility is that the model likelihood is not a very good measure of model quality, especially if we are interested in realisticlooking image samples. This is suggested by the fact that though the likelihoods for fDARN and FVSBN are very close, samples from DARN look like digits, while samples from FVSBN are much less digit-like. Moreover, samples from fDARN do not look considerably worse then DARN, except for exhibiting more pixel noise (less smooth edges). If we are primarily interested in learning high-level representations, achieving pixel-level accuracy required for high model likelihoods might be an unnecessarily stringent requirement.\nThus we found that DARNs generate good samples, but with negative log-likelihood somewhat higher then NADE. On the other hand we found in Section 4.1 that sampling from fDARN is significantly faster than for the competing models. For example, with 400 hidden units of which only 20 are typically on, the sampling costs in multiplication operations are: NADE 2.2×106; DARN: 1.1×106; fDARN: 2.4×104. These results are summarized in Table 1 columns 3, 4. Generating samples from RBMs, DBMs, and autoencoders is significantly slower than from NADE.\nIf we are only interested in generating realistic-looking samples, we do not need to binarize MNIST before train-\ning. Thus we trained fDARN on the unbinarized intensities (using real valued x in (5)) and generated smoother samples by recording the pixel probabilities produced by fDARN instead of sampling from them. The resulting samples are shown along with some randomly selected training cases in Figure 2.\nOur second experiment was a demonstration of learning feature hierarchies from high-dimensional data. As large binary images are not common, as a proof of concept, we apply DARN to images from a computer game. In particular, we took frames of the Atari 2600 (Bellemare et al., 2013) game called Freeway and binarized them by extracting edges. This resulted in 200 × 160 binary images, one of which is shown in Figure 3 (left). It shows cars moving in the lanes of a freeway.\nWe applied a three-hidden-layer network to this data. The first layer is a locally connected net with a small kernel of size 7× 7 pixels and periodicity 8× 8 (both in connection to the input and to itself). The density of the hidden units is the same as the density of pixels (so the number of hidden units is equal to the number of input units minus a boundary effect). The second layer has larger kernel of size 21 × 21 and periodicity 8× 8 but with a density 4 times lower than the layer below. The final third layer is fully connected to the second layer with 100 hidden units. The activations of each layer are shown in Figure 3, images 2-4 (in the last image, each row is an activation of the hidden layer for a different input; several inputs are shown to show variation in activations). We see that in the first two hidden layers the units are active around car locations, with the road handled by biases in the input layer.\nSamples generated from this network are shown in Figure 4, image 2. We see that the generated data seems to capture the training data well. To understand what each layer captured, we also trained a network with the top layer removed (Figure 4, image 3) and with the top two layers removed (Figure 4, image 4). We see that the last network was unable to produce a car due to the small size of the kernel in the first layer (7 × 7). The second layer, while having fewer units, had a larger kernel size and thus was able to combine the lower layer features into a car. The top-most layer helped in the placement of the cars. In the original game there is at most one car per lane. By simple counting from one hundred images, we found that the networks placed extra cars in 14% and 30% of cases respectively and didn’t place any cars in 16% and 36% cases respectively.\nIn the final experiment, we trained DARN on the Silhouettes dataset from (Marlin et al., 2010), consisting of silhouettes extracted from the Caltech 101 dataset and subsampled down to 28 × 28 pixels. Some examples of the training data and samples from the model are shown in the Figure 5. The training set is quite small consisting of only\naround 4000 images of 101 classes, so, not surprisingly, there is a lot of variation present in the generated samples.\nWe summarize the properties of several generative models in Table 1. The third column provides the sampling speed in terms of the number of multiplications. The cs constant is the number of unit updates needed before samples become roughly independent. The factor of 7 for NADE was obtained from two parts: 2 for the two matrices and 5 (estimate) for running through the nonlinearity (which is run nhnx times). The first four numbers in the log-likelihood column are estimates. NADE has a unique advantage over all other models in the table as its loglikelihood is tractable. The numbers given for DARN and fDARN are upper bounds which can only get better with more sophisticated estimation methods."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We introduced a generative model that has the following desirable properties: 1) Generation of high-quality samples; 2) Fast generation of independent samples; 3) The ability to learn a hierarchy of representations; 4) Support for partial connectivity for dealing with high-dimensional data; 5) Efficient training on GPUs using standard matrix operations. Though existing models satisfy some of these properties, none of them satisfy them all. RBMs/DBNs, DBMs, and regularized autoencoders satisfy all the properties except 2). Input-space autoregressive models such as NADE do not satisfy 3) and 5). While we have not been able to achieve state-of-the-art results on all datasets, sampling from fDARN is orders of magnitude faster then for the aforementioned undirected models and is considerably faster than for NADE. It also successfully captures high-level structure in the data as demonstrated by the samples DARN generates. We believe that its properties makes DARN an interesting and potentially useful model."
    } ],
    "references" : [ {
      "title" : "Deep generative stochastic networks trainable by backprop",
      "author" : [ "Bengio", "Yoshua", "Thibodeau-Laufer", "Éric" ],
      "venue" : "arXiv preprint arXiv:1306.1091,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Generalized denoising auto-encoders as generative models",
      "author" : [ "Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal" ],
      "venue" : "arXiv preprint arXiv:1305.6663,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Graphical models for machine learning and digital communication",
      "author" : [ "Frey", "Brendam J" ],
      "venue" : "The MIT press,",
      "citeRegEx" : "Frey and J.,? \\Q1998\\E",
      "shortCiteRegEx" : "Frey and J.",
      "year" : 1998
    }, {
      "title" : "Emergence of complexlike cells in a temporal product network with local receptive fields",
      "author" : [ "Gregor", "Karol", "LeCun", "Yann" ],
      "venue" : "arXiv preprint arXiv:1006.0448,",
      "citeRegEx" : "Gregor et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning representations by maximizing compression",
      "author" : [ "Gregor", "Karol", "LeCun", "Yann" ],
      "venue" : "arXiv preprint arXiv:1108.1169,",
      "citeRegEx" : "Gregor et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2011
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee-Whye" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "The neural autoregressive distribution estimator",
      "author" : [ "Larochelle", "Hugo", "Murray", "Iain" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Larochelle et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Larochelle et al\\.",
      "year" : 2011
    }, {
      "title" : "Representational power of restricted boltzmann machines and deep belief networks",
      "author" : [ "Le Roux", "Nicolas", "Bengio", "Yoshua" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Roux et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Roux et al\\.",
      "year" : 2008
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "LeCun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Information theory, inference and learning algorithms",
      "author" : [ "MacKay", "David JC" ],
      "venue" : "Ch 6. Cambridge university press,",
      "citeRegEx" : "MacKay and JC.,? \\Q2003\\E",
      "shortCiteRegEx" : "MacKay and JC.",
      "year" : 2003
    }, {
      "title" : "Inductive principles for restricted boltzmann machine learning",
      "author" : [ "Marlin", "Benjamin M", "Swersky", "Kevin", "Chen", "Bo", "Freitas", "Nando D" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Marlin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Marlin et al\\.",
      "year" : 2010
    }, {
      "title" : "Deep boltzmann machines",
      "author" : [ "Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics, pp",
      "citeRegEx" : "Salakhutdinov et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Salakhutdinov et al\\.",
      "year" : 2009
    }, {
      "title" : "Information processing in dynamical systems: Foundations of harmony theory",
      "author" : [ "P. Smolensky" ],
      "venue" : null,
      "citeRegEx" : "Smolensky,? \\Q1986\\E",
      "shortCiteRegEx" : "Smolensky",
      "year" : 1986
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "One class of models, based on undirected generative models, includes examples such as Restricted Boltzmann Machines (RBMs; Smolensky, 1986), Deep Belief Networks (DBNs; Hinton et al.",
      "startOffset" : 116,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "One class of models, based on undirected generative models, includes examples such as Restricted Boltzmann Machines (RBMs; Smolensky, 1986), Deep Belief Networks (DBNs; Hinton et al., 2006), and Deep Boltzmann Machines (DBMs; Salakhutdinov & Hinton, 2009).",
      "startOffset" : 162,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "Another premier class of models, regularized autoencoders (Bengio et al., 2013; Bengio & Thibodeau-Laufer, 2013), rely on similar iterative methods for sampling and suffer from similar difficulties.",
      "startOffset" : 58,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "A common way of dealing with images is through local connectivity, either fully convolutional (LeCun et al., 1998) or with less or even no weight sharing.",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "In the final experiment, we trained DARN on the Silhouettes dataset from (Marlin et al., 2010), consisting of silhouettes extracted from the Caltech 101 dataset and subsampled down to 28 × 28 pixels.",
      "startOffset" : 73,
      "endOffset" : 94
    } ],
    "year" : 2017,
    "abstractText" : "We introduce a multilayer deep generative model capable of learning hierarchies of sparse distributed representations from data. The model consists of several layers of stochastic units, with autoregressive connections within each layer, which allows for efficient exact sampling. We train the model efficiently using an algorithm derived from the Minimum Description Length (MDL) principle, which minimizes the amount of information contained in the joint vector of data and hidden unit configurations for the training set. As we are not given the hidden unit configurations corresponding to the training data, we use a feedforward network to map data vectors to configurations of hidden units that are jointly probable with them and train it jointly with the model. Our approach can also be seen as maximizing a lower bound on the log-likelihood, with the feedforward network implementing approximate inference.",
    "creator" : "LaTeX with hyperref package"
  }
}