{
  "name" : "1610.03793.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Daniel Hein", "Alexander Hentschel", "Volkmar Sterzing", "Michel Tokic", "Steffen Udluft" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 0.\n03 79\n3v 1\n[ cs\n.L G\n] 1\n2 O\nct 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "The scientific method requires that a hypothesis is tested by experiments. This holds true in the field of machine learning, when algorithms are to be developed or improved. Such a test can be, to run the algorithm on a real system, in order to observe its performance, or to run it on a simulation, a virtual system implemented as a computer program. The latter method has several advantages: it is usually faster, cheaper, and of course more safe to test the algorithm on a simulation. In addition the simulation can be manipulated much more freely than a real system. Internal states can be observed, stored, restored, and set, thus given the freedom to test special aspects with little effort. The amount of data can usually be enlarged such that all results gain statistical significance. This stands in drastic contrast to the situation when testing the algorithm on a large scale industrial system, like a power plant.\nThere are some disadvantages with simulation based testing though. First of all, the final success of an improved algorithm will be defined by the performance on the real system. Thus any deviation of the simulation from the real system might cause the development of new algorithms and the process of improving to go not in the right direction. The simulation might be too simple, underestimating the challenges posed by the real system. Or, it might focus on less relevant aspects of the task, posing artificial difficulties. Therefore it would be desirable to use a most realistic simulation for benchmarking. On the other hand, when developing an algorithm one usually targets for methods that are applicable for a broad spectrum of systems and fine tuning for a specific system is fruitful from the perspective of that system only, while it does not help to decide for the algorithm which is “superior in general”. It can be doubted that it is possible to create an algorithm that is “superior in general”. Usually special cases exist where the seemingly inferior method is superior, but still it seems good practice to aim for a method that performs well in a wide variety of simulation benchmarks. This dilemma of specific versus general solutions will not be solved here, but we want to contribute a software benchmark1 that\n1Java source code: http://github.com/siemens/industrialbenchmark\ncaptures many aspects that we found to be vital in industrial applications. The basic task, we are considering is the optimization of operating an industrial system. This task will be described in the theoretical framework of reinforcement learning [1]. The proposed simulation, called Industrial Benchmark, will serve as environment, i.e. the system to be controlled. Independently of reinforcement learning, the Industrial Benchmark can also be used to evaluate regression, forecasting, and system identification capabilities of different machine learning methods, as well as specific challenges like transfer learning, active learning, feature selection, or change detection."
    }, {
      "heading" : "2 Industrial Benchmark",
      "text" : "The Industrial Benchmark aims at being realistic in the sense, that it includes a variety of aspects that we found to be vital in industrial applications. It is not designed to be an approximation of any real system, but to pose the same hardness and complexity. State- and action-space are continuous, the state-space is rather high-dimensional, and only partially observable. The actions consist of three continuous components and effect three steerings. There are delayed effects. The optimization task is multi-criterial in the sense that there are two reward-components that show opposite dependencies on the actions. The dynamical behaviour is heteroscedastic with state dependent observation noise and state dependent probability distributions, based on latent variables. The dynamical behaviour is dependent on an external driver, that cannot be influenced by the actions. The Industrial Benchmark is designed such that the optimal policy will not approach a fixed operation point in the three steerings. Any specific choice is driven by our experience with industrial challenges."
    }, {
      "heading" : "3 Detailed description",
      "text" : "At any time step t the reinforcement learning agent can influence the environment (Industrial Benchmark) via actions ~a(t) that are three dimensional vectors in [−1,1]3. Each action can be interpreted as three proposed changes to the three observable state variables called current steerings. Those current steerings are named velocity v, gain g, and shift s. Each of those is limited to [0,100].\n~a(t) = (∆v,∆g,∆s)⊤ , (1)\nv(t+ 1) = min(0.max(100,v +∆v)) , (2)\ng(t+ 1) = min(0.max(100,g + 10∆g)) , (3)\ns(t + 1) = min(0.max(100,s+ αs∆s)) . (4)\nWhere the step size for changing shift is αs = 20 sin(150)/0.9 ≈ 5.75. After applying the action ~a(t) the environment transitions to the next time step t + 1 in which it is in an internal state ~s(t + 1)2. State ~s(t) and successor state ~s(t + 1) (also written as ~s′(t)) are the markovian states of the environment that are only partially observable to the agent.\nThree observable state variables we have already discussed, the current steerings v, gain g, and shift s. There are three more observable state variables. One is the external driver named set point p, that influences the dynamical behaviour of the environment but cannot be changed by the actions. In the setting discussed here, the set point is kept constant (Constant Set Point Setting).\n2Not to be mixed up with the shift s, which is one of the variables that build the state ~s.\nAn extension will be that the set point will change with time (Variable Set Point Setting), i.e. p = p(t). In this Variable Set Point Setting the changes in the set point will be influenced externally and not by the actions. In the Constant Set Point Setting the agent does not need to predict set point changes, as they do not occur. This setting is therefore not one partially observable markov decision problem (POMDP), but a family (german: Schar) of POMDPs, parameterized by the set point. Learning to act optimal for any set point in the Constant Set Point Setting can also be seen as a multi-task or transfer learning task [2, 3].\nThe set of observable state variables is completed by the two reward relevant variables consumption c(t) and fatigue f(t). In the general reinforcement learning setting a reward r(t) is drawn for each transition t → t+ 1 from state ~s(t) via action ~a(t) to the successor state ~s(t+ 1) from a probability distribution depending on ~s(t), ~a(t), and ~S(t+1). In the Industrial Benchmark the reward is given by a deterministic function of the successor state r(t) = r(~s(t + 1)), i.e.\nr(t) = −(c(t + 1) + f(t+ 1)) . (5)\nIn the real world tasks that motivated the Industrial Benchmark the reward function has always been known explicitly. In some cases it was subject to optimization itself and had to be adjusted to properly express the optimization goal. For the Industrial Benchmark we therefore assume that the reward function is known and all variables influencing it are observable.\nThus the observation vector ~O(t) at time t comprises current values of the set of observable state variables, which is a subset of all the variables of state ~s(t), i.e.\n1. the current steerings, velocity v(t), gain g(t), and shift s(t), 2. the external driver, set point p, 3. and the reward relevant variables consumption c(t) and fatigue f(t). The data base for learning comprises of tuples ( ~O(t),~a(t), ~O(t + 1),r(t)), which, by introducing the notation ~O′ for the observation vector of the successor state, will be written as ( ~O(i),~a(i), ~O′(i),r(i)) or, in short, ( ~O,~a, ~O′,r)(i).\nThe agent is allowed to use all previous observation vectors and actions to estimate the markovian state ~s(t)."
    }, {
      "heading" : "4 Description of the dynamical behaviour",
      "text" : "The dynamical behaviour of the Industrial Benchmark is determined by the three steerings velocity v, gain g, and shift s, the external driver set point p, and five latent variables. The dynamics can be decomposed in three different sub-dynamics named operational costs, mis-calibration, and fatigue."
    }, {
      "heading" : "4.1 Dynamics of operational cost",
      "text" : "The sub-dynamics of operational cost is influenced by the external driver set point p and two of the three steerings, namely velocity v and gain g. The current operational cost o(t) is calculated as\no(t) = exp\n(\n2p(t) + 4v(t) + 2.5g(t)\n100\n)\n. (6)\nThe current operational cost o(t) cannot be observed, the observation is delayed and smeared out by a convolution\noc(t) = 0o(t) + 0o(t− 1) + 0o(t− 2) + 0o(t− 3) + 0o(t− 4) + 1\n9 o(t− 5) +\n2 9 o(t− 6) + 3 9 o(t− 7) + 2 9 o(t− 8) + 1 9 o(t− 9) (7)\nThe convoluted operational cost oc(t) still cannot be observed directly, it is modified by the second sub-dynamics, called mis-calibration, and finally subject to observation noise. The motivation for this dynamical bahaviour is that it is non-linear, depends on more than one influence, is delayed and smeared. All those effects have been observed in industrial applications."
    }, {
      "heading" : "4.2 Dynamics of mis-calibration",
      "text" : "The sub-dynamics of mis-calibration is influenced by the external driver, set point p, and only one steering, namely shift s. Set point p and shift s are combined to an effective shift se\nse = min(1.5,max(−1.5,s/20− p/50− 1.5)) , (8)\nwhich influences the three latent variables ml1, m l 2, and m l 3. The resulting mis-calibration m is a function of effective shift and the latent variables\nm = f(se,ml1, m l 2, m l 3) . (9)\nThe resulting mis-calibration m(t) is added to the convoluted operational cost oc(t), giving ĉ,\nĉ = oc(t) + 25m(t) , (10)\nBefore being observable as consumption c the modified operational cost ĉ is subject to heteroskedastic observation noise\nc = ĉ+ gauss(0,1 + 0.02 ĉ) , (11)\ni.e. a gaussian noise with zero mean and a standard deviation of σ = 1 + 0.02 ĉ."
    }, {
      "heading" : "4.3 Dynamics of fatigue",
      "text" : "The sub-dynamic of fatigue is influenced by the same variables as the sub-dynamic of operational cost, i.e. set point p, velocity v, and gain g. The Industrial Benchmark is designed in such a way that when changing the steerings velocity v and gain g as to reduce the operational cost, fatigue will be increased, leading to the desired multi-criterial task, with two reward-components showing opposite dependencies on the actions. The basic fatigue fb is computed as\nfb = max\n(\n0, 30000\n5 v + 100 − 0.01 g2\n)\n. (12)\nFrom the basic fatigue fb, the fatigue f is calculated by\nf = fb(1 + 2α) , (13)\nwhere α is an amplification. The amplification depends on two latent variables hv and hg, an effective velocity ve, an effective gain ge, and is affected by noise,\nα =\n{\n1 1+exp(−gauss(2.4,0.4) for max(hv,hg) = 1.2 max(ηv,ηg) else . (14)\nThe noise components ηv and ηg, as well as the latent variables hv and hg, depend on effective velocity ve, and effective gain ge. These are calculated by set point dependent transformation functions\nTv(v,g,p) = g + p+ 2\nv − p+ 101 , (15)\nTg(g,p) = 1\ng + p+ 1 . (16)\nBased on this transformation functions, effective velocity ve and effective gain ge are computet as follows:\nve = Tv(v,g,p)− Tv(0,100,p)\nTv(100,0,p)− Tv(0,100,p) (17)\nge = Tg(g,p)− Tg(100,p)\nTg(0,p)− Tg(100,p) . (18)\nTo compute the noise components ηv and ηg, six random numbers are drawn from different random distributions: ηve and η g e are drawn from an exponential distribution with mean 0.05, η v b and ηgb are drawn from binominial distributions Binom(1, v e) and Binom(1, ge), respectively, ηvu and ηgu are drawn from an uniform distribution in [0,1]. These random numbers are combined to two noise components ηv and ηg by\nηv = ηve + (1− η v e )η v uη v bv e (19) ηg = ηge + (1− η g e)η g uη g bg e . (20)\nThe latent variables hv and hg are caclulated as\nhv(t) =\n\n \n \nve for ve ≤ 0.05\nmin(5,1.1hv(t− 1)) for ve > 0.05 ∧ hv(t− 1) ≥ 1.2 0.9hv(t− 1) + ηv\n3 else\n(21)\nhg(t) =\n\n \n \nge for ge ≤ 0.05\nmin(5,1.1hg(t− 1)) for ge > 0.05 ∧ hg(t− 1) ≥ 1.2 0.9hg(t− 1) + ηg\n3 else .\n(22)\nThe sub-dynamic of fatigue results in a value for fatigue f , which is relevant for the reward function. (see Eq. 5)."
    }, {
      "heading" : "5 State definitions",
      "text" : "To give an overview on possible state definitions a small summary is given."
    }, {
      "heading" : "5.1 The observation vector",
      "text" : "Only a part of the state variables is observable. This observation vector is also called observable state, but one has to keep in mind, that it does not fulfill the markov property. The observation vector ~O(t) at time t comprises current values of velocity v(t), gain g(t), shift s(t), set point p(t), consumption c(t), and fatigue f(t)."
    }, {
      "heading" : "5.2 The preferred minimal markovian state",
      "text" : "The preferred minimal markovian state fulfills the markov property with the minimum number of variables. It comprises 20 values. These are the observation vector (velocity v(t), gain g(t), shift s(t), set point p(t), consumption c(t), and fatigue f(t)) plus some latent variables of the sub-dynamics. The sub-dynamics of operational cost adds a list of previous operational costs, o(t − i) with i ∈ 1, · · · ,9. Note that the current operational cost o(t) is not part of this state definition. It would be redundant, as it can be calculated by v(t), gain g(t), and set point p. The sub-dynamics of mis-calibration needs 3 additional latent variables, m1, m2, and m3, (Sec. 4.2). The sub-dynamics of fatigue adds 2 additional latent variables hv and hg, (Eq. 21 and 22)."
    }, {
      "heading" : "5.3 The extended state",
      "text" : "The extended state, also called the internal markovian state, contains in addition to all the variables of the preferred minimal markovian state also some variables, which are of useful for data analysis purposes."
    }, {
      "heading" : "6 Experimental setup",
      "text" : "To test the algorithms in an initial batch mode, off-policy setting, data is generated by the maximum entropy policy,\ndP (a|s)\nda = const (23)\nThe benchmark is initialized for ten different set points p ∈ 10,20, · · · ,90,100 with the latent variables in their default values and the three steerings at 50 each. Then for each set point value the maximum entropy policy is applied on the benchmark for 1000 time steps, resulting in 10.000 data points. This data can then be used to train system identification models and/or policies. The goal is to build a policy π that maximizes the average reward on the same setting, where instead of the maximum entropy policy, the policy π is applied."
    }, {
      "heading" : "7 Interfaces",
      "text" : "The main interfaces are defined in com.siemens.rl.interfaces. They are DataVector, Environment, and ExternalDriver."
    }, {
      "heading" : "7.1 Interface DataVector",
      "text" : "This interface lists all necessary methods to implement a data vector, which might be a state- or action-vector.\nModifier and Type Method and Description DataVector clone()\nReturns a copy of the data vector.\nList <String> getKeys() Returns a list containing the data-vector dimension names.\nDouble getValue(String_key) Returns the value for a given data-vector dimension.\ndouble[] getValuesArray() Returns a double[] array containing the values.\nvoid setValue(String_key, double_value) Sets the current value of a given data-vector dimension."
    }, {
      "heading" : "7.2 Interface Environment",
      "text" : "This interface describes all relevant methods for implementing the dynamics of an environment.\nModifier and Type Method and Description void addExternalDriver(ExternalDriver_extDriver)\nThis function adds an external driver to the environment, which affect/filter state variables during the call of step().\nDataVector getInternalMarkovState() Returns the internal Markovian state.\ndouble getReward() Returns the reward.\nDataVector getState() Returns the observable state.\nvoid reset() Function for resetting the environment.\ndouble step(DataVector_action) Performs an action within the environment and returns the reward."
    }, {
      "heading" : "7.3 Interface ExternalDriver",
      "text" : "Abstract interface for attaching external drivers to the Environment, that affect/filter certain state dimensions (e.g. such as set point).\nModifier and Type Method and Description void filter(DataVector_state)\nApplies \"in-place\" the external drivers to the given data vector.\nDataVector getState() Returns the current configuration.\nvoid setConfiguration() Sets the external driver configuration from within the given data vector.\nvoid setSeed(long_seed) Sets the random seed."
    }, {
      "heading" : "8 Example usage",
      "text" : ""
    }, {
      "heading" : "8.1 Class ExampleMain",
      "text" : "An example is implemented in com.siemens.industrialsim.ExampleMain.\nModifier and Type Method and Description static void main(String[]_args)\nRun example simulation with random actions."
    }, {
      "heading" : "9 First results",
      "text" : "First results with 10.000 data vectors as described in section 6 indicate, that the reward (named RewardTotal can be estimated from current and past values of velocity, gain, shift, and set point as inputs by a recurrent neural network with a mean relative absolute deviation (MRABD) of approximately 10%. Consumption c can be estimated with a MRABD of approximately 3.6%, and fatigue f with a MRABD of approximately 24% (for f > 1).\nThe average reward of the maximum entropy policy in the setting described in section 6 is -290.8± 0.6 with a standard deviation of 20.\nFirst results with the policy gradient neural rewards regression (PGNRR) [4] and with an extension to continues actions of the neural fitted Q-iteration (NFQ) [5] lead to average rewards of roughly -270."
    } ],
    "references" : [ {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1998
    }, {
      "title" : "Is learning the n-th thing any easier than learning the first",
      "author" : [ "S. Thrun" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1996
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "S.J. Pan", "Q. Yang" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Improving optimality of neural rewards regression for data-efficient batch near-optimal policy identification",
      "author" : [ "Daniel Schneegaß", "Steffen Udluft", "Thomas Martinetz" ],
      "venue" : "Artificial Neural Networks – ICANN 2007: 17th International Conference,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Neural fitted Q-iteration - first experiences with a data efficient neural reinforcement learning method",
      "author" : [ "Martin Riedmiller" ],
      "venue" : "In Proceedings of the 16th European Conference on Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This task will be described in the theoretical framework of reinforcement learning [1].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "Learning to act optimal for any set point in the Constant Set Point Setting can also be seen as a multi-task or transfer learning task [2, 3].",
      "startOffset" : 135,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "Learning to act optimal for any set point in the Constant Set Point Setting can also be seen as a multi-task or transfer learning task [2, 3].",
      "startOffset" : 135,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "05, η v b and η b are drawn from binominial distributions Binom(1, v ) and Binom(1, g), respectively, η u and η u are drawn from an uniform distribution in [0,1].",
      "startOffset" : 156,
      "endOffset" : 161
    }, {
      "referenceID" : 3,
      "context" : "First results with the policy gradient neural rewards regression (PGNRR) [4] and with an extension to continues actions of the neural fitted Q-iteration (NFQ) [5] lead to average rewards of roughly -270.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "First results with the policy gradient neural rewards regression (PGNRR) [4] and with an extension to continues actions of the neural fitted Q-iteration (NFQ) [5] lead to average rewards of roughly -270.",
      "startOffset" : 159,
      "endOffset" : 162
    } ],
    "year" : 2016,
    "abstractText" : "A novel reinforcement learning benchmark, called Industrial Benchmark, is introduced. The Industrial Benchmark aims at being be realistic in the sense, that it includes a variety of aspects that we found to be vital in industrial applications. It is not designed to be an approximation of any real system, but to pose the same hardness and complexity.",
    "creator" : "LaTeX with hyperref package"
  }
}