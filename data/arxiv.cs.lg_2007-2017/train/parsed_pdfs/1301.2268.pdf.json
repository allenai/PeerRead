{
  "name" : "1301.2268.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Incorporating Expressive Graphical Models in Variational Approximations: Chain-Graphs and Hidden Variables",
    "authors" : [ "Tal El-Hay", "Nir Friedman" ],
    "emails" : [ "@cs.huji.ac.il" ],
    "sections" : null,
    "references" : [ {
      "title" : "Tractable variational structures for approximating graphical models",
      "author" : [ "D. Barber", "W. Wiegerinck" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1999
    }, {
      "title" : "Elements of Informa­",
      "author" : [ "T.M. Cover", "J. A Thomas" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1991
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Saul and Jordan [ 1 0] sug­ gest to circumvent this problem by using structured variationa! approximation.",
      "startOffset" : 16,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "This idea can be generalized for various factored forms for Q, such as Bayesian networks and Markov networks [1, 1 1 ].",
      "startOffset" : 109,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "This idea can be generalized for various factored forms for Q, such as Bayesian networks and Markov networks [1, 1 1 ].",
      "startOffset" : 109,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "This idea can be generalized for various factored forms for Q, such as Bayesian networks and Markov networks [1, 1 1 ].",
      "startOffset" : 109,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "A common measure of distance is the KL divergence [2] between Q(T : 8) and the posterior distribution P(T I o).",
      "startOffset" : 50,
      "endOffset" : 53
    } ],
    "year" : 2011,
    "abstractText" : "Global variational approximation methods in graphical models allow efficient approximate inference of com­ plex posterior distributions by using a simpler model. The choice of the approximating model determines a tradeoff between the complexity of the approximation procedure and the quality of the approximation. In this paper, we consider variational approximations based on two classes of models that are richer than standard Bayesian networks, Markov networks or mixture mod­ els. As such, these classes allow to find better tradeoffs in the spectrum of approximations. The first class of models are elwin graphs, which capture distributions that are partially directed. The second class of mod­ els are directed graphs (Bayesian networks) with addi­ tional latent variables. Both classes allow representa­ tion of multi-variable dependencies that cannot be eas­ ily represented within a Bayesian network.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}