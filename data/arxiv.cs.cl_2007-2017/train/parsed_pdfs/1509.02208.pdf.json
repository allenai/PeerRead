{
  "name" : "1509.02208.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "UNSUPERVISED DISCOVERY OF LINGUISTIC STRUCTURE INCLUDING TWO-LEVEL ACOUSTIC PATTERNS USING THREE CASCADED STAGES OF ITERATIVE OPTIMIZATION",
    "authors" : [ "Cheng-Tao Chung", "Chun-an Chan", "Lin-shan Lee" ],
    "emails" : [ "r01921031@ntu.edu.tw,", "chunanchan@gmail.com,", "lslee@gate.sinica.edu.tw" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— unsupervised learning, hidden Markov models, spoken term detection, zero resource speech recognition, iterative optimization\n1. INTRODUCTION\nSupervised training of HMMs for automatic speech recognition relies on not only collecting huge quantities of acoustic data, but also obtaining the corresponding precise labels. Such supervised training method yields adequate performance in most circumstances but with high cost, and in many situations such annotated data sets are simply not available. This is why substantial effort [1]-[15] has been made for unsupervised discovery of acoustic patterns from huge quantities of acoustic data which may be easily obtained nowadays, without manual labels and corresponding knowledge. Most of such effort discovered only one level of phoneme like acoustic patterns. However, it is well known that speech signals have multi-level structure including at least phoneme and words, and such structure are very helpful in analysing or decoding speech[16].\nIn this paper we propose an approach for unsupervised discovery of structured two-level acoustic patterns including subword-like patterns and word-like patterns (concatenation of several subwordlike patterns). Not only the HMMs for these patterns, the number of the subword-like patterns and the lexicon size of word-like patterns can be automatically learned from data, but more knowledge about the language such as the N-gram language model and the word-like pattern lexicon, jointly referred to as the linguistic structure in this paper, can all be obtained directly from the acoustic signals of a corpus. This is achieved by integrating a dynamic lexicon into the process of the conventional supervised HMM-training, and performing three stages of iterative optimization between the labels and the models, such that the models, parameters, and the linguistic structure can then collect knowledge from the corpus layer after layer iteratively and adjust themselves accordingly. In this way, we are able to develop semantic building blocks of the target spoken language represented by the corpus with word-like patterns and acoustic building blocks of the target spoken language with subword-like patterns."
    }, {
      "heading" : "2. PROPOSED APPROACH: CASCADED THREE STAGES",
      "text" : "OF ITERATIVE OPTIMIZATION\nThe goal is to find the parameter set θ = {θa, θx, θl} for the linguistic structure and the word-like pattern labelW given the observed acoustic feature vector sequences Ō for the corpus considered. The parameter set θ includes three parts: θa for acoustic HMMs of subword-like patterns, θx for lexicon of word-like patterns in terms of subwordlike pattern sequences, and θl for N-gram word-like pattern language model. This is achieved by first finding an initial label W0 for the observation Ō as in (1). In each iteration i, we train the parameters θi with the label Wi−1 obtained in the previous iteration as in (2) and decode the label Wi with the obtained parameters θi as in (3).\nW0 = initialization(Ō), (1) θi = arg max\nθ P (Ō|θ,Wi−1), (2)\nWi = arg max W\nP (Ō|θi,W ). (3)\nThe iterations above are organized as an initialization step followed by three cascaded stages (I)(II)(III) respectively for acoustic, linguistic and lexical optimization as shown in Fig. 1. In Fig. 1, the number of iterations for each stage are Ia, Il and Ix respectively. When the difference between Wi−1, Wi becomes insignificant, the process then advances to the next stage. The parameters θai are generated by EM training as in (2), while the other parameters θli or θ x i\nar X\niv :1\n50 9.\n02 20\n8v 1\n[ cs\n.C L\n] 7\nS ep\n2 01\nare generated directly from the labels Wi−1 obtained in the previous iteration. However, not all of θa, θx, and θl are used in each stage. The detailed updating procedure is depicted in Fig. 2 and will be explained shortly.\nThe basic idea behind the procedure in Fig. 1 is to gradually construct and update the parameters layer after layer. This prevents the parameters from being caught in local optimal situations which often happen when too many parameters are optimized at once. First, the HMM parameters for the subword-like patterns are trained alone in stage (I), because these HMMs are the primary building blocks of the whole linguistic structure and reliable estimate for their parameters is the key. With reliable enough HMMs for subword-like patterns, we then in stage (II) use N-gram parameters for word-like patterns to better decode those word-like patterns frequently appearing together while continuously updating the HMM parameters. Finally in the stage (III), we break the word-like patterns into subword-like patterns and reconstruct better word-like patterns. The number of word-like patterns in the lexicon may shrink in the iterations of the first two stages because some less frequent patterns can be absorbed by other patterns, but this number can be changed significantly in the third stage. The time alignment for the subword-like patterns are updated in all iterations when the the labels Wi are decoded."
    }, {
      "heading" : "2.1. Initialization Step",
      "text" : "Here we initialize the labels in a top-down fashion by first breaking each utterance into word-like segments based on the discontinuities in a parameter evaluated from energy and MFCC features. For each word-like segment, we further divide it into subword-like segments in the following way. We perform a watershed transform on the filtered self-similarity dotplot [17] for acoustic features of each hypothesized word-like segment. Watershed transformation is able to capture the number of objects and their borders in a gray scale image [18]. So, the intersections of the diagonal entries of the dot-plot with the watershed transform object borders are taken as the boundaries between subword-like segments. An example dotplot and its watershed transform including the hypothesized subword-like segment boundaries is shown in Fig. 3.\nWe then extract an average representative feature vector for every hypothesized subword-like segment, and perform global k-means\nclustering on these representative vectors obtained from the whole corpus. The number of clusters (the initial number of subword-like patterns) is determined by the ratio of the within-cluster total scattering to the between-cluster total scattering. A subword-like pattern ID is then assigned to each cluster. A distinct sequence of consecutive subword-like patterns for word-like segments then defines a wordlike pattern, and the total number of distinct word-like patterns in the corpus is the initial vocabulary size of the lexicon. The corpus is thus represented by its initial labels W0."
    }, {
      "heading" : "2.2. Stage(I):Acoustic Optimization",
      "text" : "The process in stage(I) is shown in Fig. 2(a). In each iteration, the acoustic model set θai are the HMMs trained from the corpus based on Wi with the ML criterion. The lexicon θxi is derived by collecting all word-like patterns appearing in Wi with counts exceeding a threshold. Free word decoding is then performed on the whole corpus Ō based on θai and θ x i , producing an updated label Wi+1. When Wi is updated to Wi+1, not only the HMM parameters of θai and HMM segmentation boundaries are updated, but the vocabulary size of θxi may shrink when the counts of some word-like patterns become small enough."
    }, {
      "heading" : "2.3. Stage(II):Linguistic Optimization",
      "text" : "This stage is shown in Fig. 2(b), which is very similar to the previous stage. The only difference is an N-gram language model θli for\nthe word-like patterns is estimated from the label Wi and is used in decoding to produce the updated labelsWi+1. The N-grams help produce better labels Wi+1 especially for word-like patterns appearing together frequently."
    }, {
      "heading" : "2.4. Stage(III):Lexical Optimization",
      "text" : "We reconstruct new word-like patterns in this step as in Fig. 2(c). This is done by breaking the word-like patterns in θxi−1 into subwordlike patterns, and then reconstructing new word-like patterns based on Wi. Those segments of several consecutive subword-like patterns appearing frequent enough and with high enough right and left context variation are taken as word-like patterns. This can be achieved by constructing an efficient data structure called PAT-Tree using the labels Wi[19]. In this way, the lexicon θxi can be updated significantly in each iteration. This updated lexicon θxi is then used in freeword decoding to produce the labels Wi+1. The whole process is completed when there is no significant difference between Wi and Wi+1. This gives the automatically discovered linguistic structure θ = {θa, θx, θl}, where θl is trained from the final version of Wi+1.\n3. EXPERIMENTS"
    }, {
      "heading" : "3.1. Experimental Setup",
      "text" : "The proposed approach was tested in the preliminary experiments performed on a corpus of Mandarin broadcast news collected in Taiwan in 2001 with length of 4 hours including 5034 utterances. The HMMs used for each sub-word like pattern had 13 states, each with only 1 Gaussian component. This configuration was selected due to the assumption that the subword-like patterns of interest should describe more signal trajectory variation and less acoustic variation. Signal segments with larger acoustic variation should be classified as different patterns. The final linguistic structure including all patterns, models and parameters was obtained by performing 30 iterations in each stage (I)(II)(III) in Fig. 1 on the entire corpus."
    }, {
      "heading" : "3.2. Initial Observations and Analysis",
      "text" : "It is interesting that almost all the 208 subword-like patterns obtained here roughly correspond to Mandarin syllables (each Chinese character is pronounced as a Mandarin syllable). A global view of the exact mapping relation from the 208 subword-like patterns to the total of 399 Mandarin syllables manually labelled for the corpus is shown in Fig. 4. The Mandarin syllables on the horizontal scale of the figure have been sorted according to acoustic similarity (only a quarter of them are explicitly printed due to limited space). Every circle here represents 35 or more subword-like patterns on the vertical scale whose central feature frame belonged to the Mandarin syllable in the horizontal scale. This figure implied a very-closeto one-to-one mapping relation with some fuzziness around neighbouring syllables with similar acoustic behaviour. The 362 word-like pattern obtained corresponded to roughly 154 frequently occurring multi-syllable words and 208 monosyllables (or mono-subword-like patterns). Those words occurring not frequently enough couldn’t be discovered and as a result were represented as one to several monosubword-like patterns.\nFig. 5 further illustrates how the number of subword-like patterns, lexicon size of word-like patterns, the consistency between Wi−1 and Wi at word-like pattern level and utterance level changed with respect to iterations. In a global perspective, lexicon size of word-like patterns dropped in the stages (I) and (II), and jumped and\noscillated in stage (III). Although most word-like patterns in stage (I) did not survive by the end of stage (II), the main purpose of them was to provide some context guidance for the training of subword-like HMMs."
    }, {
      "heading" : "3.3. Justification of the Initialization and Iterative Stages",
      "text" : "We performed further tests with configurations slightly different from the proposed approach on a subset of 942 utterances out of the 5034 in the tested corpus. We evaluated the syllable accuracy by mapping every discovered subword-like pattern to a corresponding Mandarin syllable (as was done in Fig. 4) for each configuration considered. In the first part, we initialized W0 with 3 different methods and then applied 50 iterations of stage (I) only. The three methods are (1) the proposed two-level top-down labelling started with word-like segments, (2) subword initialization with only watershed transform, but without higher level word-like segments, (3) same as (2) but without k-means clustering, with same number of subword-like pattern IDs randomly assigned to each subword-like segment. The main difference between methods (1)(2) was the two-level pattern structure. Method (1) brought us halfway through the proposed approach (initialization and stage (I)) producing two-level patterns, while method (2) was similar to the unsupervised initialization methods used previously with one-level patterns only [1][17]. The results are in the left half of Table 1. Although method (1) was only 1.03% better than method (2), the patterns obtained with method (1) manual auditing tests suggest that the improvement is non-trivial. This verified the word-like pattern constraints were useful in the acoustic optimization process. The random ID assignments without clustering in method (3) also offered relatively high accuracy. This implied the acoustic optimization iterations in stage (I) was quite helpful.\nIn the second part, we initialized W0 with the two-layered method then applied 3 different iteration sequences: (1) (Ia, Il, Ix) = (30, 20, 0), (2) (Ia, Il, Ix) = (50, 0, 0), (3) (Ia, Il, Ix) = (0, 50, 0). Method (1) brought us halfway through the proposed\napproach wile method (3) was actually the intuitive joint optimization of both acoustic and linguistic parameters similar to previously proposed approaches [3][4]. The results are in the right half of Table 1. The proposed method (1) was 2.37% better than the joint optimization method (3). The proposed method (1) was also better than the applying method (2) alone, which implies that the transition was the source of improvement. This verified that gradually learning later after layer yielded more reliable results. The benefits of the lexical optimization in stage (III), on the other hand, are better observed in a companion paper on semantic retrieval of spoken content also submitted to ICASSP 2013[20], since the word-like patterns carried semantics."
    }, {
      "heading" : "3.4. Spoken Term Detection",
      "text" : "We also applied the discovered patterns on a task of spoken term detection [22]-[27] and compared to a set of Mandarin syllable models trained on a manually annotated corpus of 24.5 hours of Mandarin Broadcast News with a trigram for 72k vocabulary used in recognition. The performance of the supervised HMMs serves as an upper bound for the performance of our unsupervised HMMs. We tested the performance of the supervised and unsupervised models under the same scenario. The query set consisted of 52 name entities of countries, organizations and political leaders. For each query, we decoded their corresponding utterances in the corpus and selected the most frequent HMM sequence to represent each query (equivalent to query by one example of the best query utterance). Syllable HMMs were used for the supervised case, and subword-like pattern HMMs were used for the unsupervised case. This query HMM sequence\nwas then compared with the HMM sequences of all utterances in the corpus for evaluation of the relevance scores for retrieval. We first computed offline the distance between each pair of two HMMs. The distance between two HMMs was defined to be the DTW-distance between the two state sequences. One state in a HMM can be matched with several states in another HMM and vice versa. The distance metric used for DTW was the KL-divergence between the two Gaussian mixtures [21]. We then calculate the distance between the query HMM sequences and corpus HMM sequences online. The distance between two HMM sequences was defined to be the sum of distances for matched pairs of models for the two sequences. Since most computation was done offline, this method was as fast as text information retrieval.\nWe took the weighted sum of the supervised distance ds and unsupervised distance du, and performed spoken term detection based on the combined distance dλ = λ×ds+(1−λ)×du. The results in Fig. 6 show that reasonable detection performance was achieved for the unsupervised model on its own (λ = 0). More importantly, the combined distance can yield better results in all the three measures than using only supervised or unsupervised distances. This implies that the proposed method has successfully harvested information directly from the data that was lost during recognition with the supervised models. In other words, the proposed method not only performs reasonably well on its own, but it is also complimentary to standard supervised ASR systems."
    }, {
      "heading" : "4. CONCLUSION",
      "text" : "This work presents an approach for unsupervised discovery of linguistic structure including two-level acoustic patterns from a corpus. The main difference from similar approaches proposed earlier [1][2][3][4][5][6][7] lies in the two-level acoustic patterns and the layer-after-layer gradual learning of the model parameters with cascaded stages of iterative optimization. Although some earlier approaches [1] also took hierarchical knowledge into consideration, our work used 13-state single Gaussian HMMs as compared to the conventional HMMs with smaller number of states and multi-Gaussian [1][2][3][4] to model the trajectories of acoustic patterns with less acoustic variation. The preliminary experiment on spoken term detection on subword-like pattern sequences indicated that the proposed system is complimentary to existing ASR systems. A more complete experiment on spoken term detection in a companion paper submitted to ICASSP 2013 [15] demonstrates how our model can outperform the segmental DTW approach. Also, the second level of word-like patterns are aimed to capture some semantic features in the acoustic signal, which can be verified in a companion paper on Semantic\nretrieval of spoken content also submitted to ICASSP 2013 [20].\n5. REFERENCES\n[1] A. Jansen and K. Church “Towards Unsupervised Training of Speaker Independent Acoustic Models” in InterSpeech, 2011, pp. 1693–1696.\n[2] C. Lee and J. Glass, “A Nonparametric Bayesian Approach to Acoustic Model Discovery” in Proc. The Association for Computer Linguistics, 2012, vol. 1, pp. 40–49.\n[3] H. Gish, M. Siu, A. Chan, and B. Belfield, “Unsupervised training of an HMM-based Speech Recognizer for Topic Classification” in InterSpeech, 2009, pp. 1935–1938.\n[4] M. Siu, H. Gish, A. Chan, and W. Belfield, “Improved Topic Classification and Keyword Discovery using an HMM-based Speech Recognizer Trained without Supervision” in InterSpeech, 2010, pp. 2838–2841.\n[5] M. Huijbregts, M. McLaren, and D. van Leeuwen, “Unsupervised acoustic sub-word unit detection for query-by-example spoken term detection,” in ICASSP, 2011, pp. 4436–4439.\n[6] S. Novotney, R. Schwartz, and J. Ma, “Unsupervised acoustic and language model training with small amounts of labelled data,” in ICASSP, 2009, pp. 4297–4300.\n[7] C. Chan and L. Lee, “Unsupervised Hidden Markov Modeling of Spoken Queries for Spoken Term Detection without Speech Recognition” in InterSpeech, 2011, pp. 2141–2144.\n[8] O. J. Rasanen, U. K. Laine, T. Altosaar “Computational language acquisition by statistical bottom-up processing,” in InterSpeech, 2008, pp. 1980–1983.\n[9] O. J. Rasanen, U. K. Laine, T. Altosaar “A noise robust method for pattern discovery in quantized time series: the concept matrix approach,” in InterSpeech, 2009, pp. 3035–3038.\n[10] O. J. Rasanen, U. K. Laine, T. Altosaar, “Self-learning vector quantization for pattern discovery from speech,” in InterSpeech, 2009, pp. 852–855.\n[11] O. J. Rasanen, Fully unsupervised word learning from continuous speech using transitional probabilities of atomic acoustic events in InterSpeech, 2010, pp. 2922–2925.\n[12] C. Chan, “Unsupervised Spoken Term Detection with Spoken Queries ” Ph.D dissertation, National Taiwan University, July, 2012.\n[13] Y. Qiao, N. Shimomura, and N. Minematsu, “Unsupervised optimal phoneme segmentation: objectives, algorithm and comparisons,” in ICASSP, 2008, pp. 3989–3992.\n[14] C. Chan and L. Lee, “Integrating Frame-based and Segmentbased Dynamic Time Warping for Unsupervised Spoken Term Detection with Spoken Queries” in ICASSP, 2011, pp. 5652– 5655.\n[15] C. Chan, C. Chung, Y. Kuo and L. Lee, “Toward Unsupervised Model-based Spoken Term Detection with Spoken Queries without Annotated Data” submitted to ICASSP, 2013\n[16] Y.-c. Pan and L.-s. Lee, “Performance analysis for lattice-based speech indexing approaches using word and subword units,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 6, August 2010, pp. 1562–1574.\n[17] A. Jansen, K. Church, and H. Hermansky, “Towards Spoken Term Discovery At Scale With Zero Resources” in InterSpeech, 2010, pp. 1676–1679.\n[18] M. Couprie, G. Bertrand, “Topological gray-scale watershed transform,” in Proc. of SPIE Vision Geometry V, 1997, vol. 3168, pp. 136–146.\n[19] T. Ong and H. Chen, “Updateable PAT-Tree Approach to Chinese Key Phrase Extraction using Mutual Information: A Linguistic Foundation for Knowledge Management,” in Proc. the Second Asian Digital Library Conference, 1999, pp. 63–84.\n[20] H. Lee, Y. Li, C. Chung, and L. Lee, “Enhancing Query Expansion for Semantic Retrieval of Spoken Content with Automatically Discovered Acoustic Patterns,” submitted to ICASSP, 2013\n[21] J. Hershey and P. Olsen, “Approximating the Kullback Liebler Divergence between Gaussain Mixture Models” in ICASSP, 2007, vol. 4, pp. 317–320.\n[22] F. Metze, N. Rajput et al., “The spoken web search task at Mediaeval 2011,” in ICASSP, 2012, pp. 5165–5168.\n[23] H. Wang, C.-C. Leung, T. Lee, B. Ma, and H. Li, “An acoustic segment modeling approach to query-by-example spoken term detection,” in ICASSP, 2012, pp. 5157–5160.\n[24] A. Garcia and H. Gish, “Keyword spotting of arbitrary words using minimal speech resources,” in ICASSP, 2006.\n[25] R. Wallace, R. Vogt, and S. Sridharan, “A phonetic search approach to the 2006 NIST spoken term detection evaluation,” in InterSpeech, 2007, pp. 2385–2388.\n[26] M. Terao, T. Koshinaka, S. Ando, R. Isotani, and A. Okumura, “Open vocabulary spoken-document retrieval based on query expansion using related web documents,” in InterSpeech, 2008, pp. 2171–2174.\n[27] W. Shen, C. M. White, and T. J. Hazen, “A comparison of queryby example methods for spoken term detection,” in InterSpeech, 2009, pp. 2143–2146.\n[28] M. Ostendorf, V. Digalakis, and O. A. Kimball, “From hmms to segment models: A unified view of stochastic modeling for speech recognition,” IEEE Transactions on Speech and Audio Processing, vol. 4, pp. 360V-378, 1995.\n[29] Y. Zhang and J. R. Glass, “A piecewise aggregate approximation lowerbound estimate for posteriorgram-based dynamic time warping,” in InterSpeech, 2011, pp. 1909–1912.\n[30] M.-W. Koo, C.-H. Lee, and B.-H. Juang, “Speech recognition and utterance verification based on a generalized confidence score,” IEEE Transactions on Speech and Audio Processing, vol. 9, no. 8, pp. 821V-832, 2001.\n[31] Y. Tsao, H. Sun, H. Li, and C.-H. Lee, “An acoustic segment model approach to incorporating temporal information into speaker modeling for text-independent speaker recognition,” in ICASSP, 2010, pp. 4422–4425."
    } ],
    "references" : [ {
      "title" : "Unsupervised Training of Speaker Independent Acoustic Models",
      "author" : [ "A. Jansen", "K. Church “Towards" ],
      "venue" : "InterSpeech,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "A Nonparametric Bayesian Approach to Acoustic Model Discovery",
      "author" : [ "C. Lee", "J. Glass" ],
      "venue" : "in Proc. The Association for Computer Linguistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Unsupervised training of an HMM-based Speech Recognizer for Topic Classification",
      "author" : [ "H. Gish", "M. Siu", "A. Chan", "B. Belfield" ],
      "venue" : "InterSpeech,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Improved Topic Classification and Keyword Discovery using an HMM-based Speech Recognizer Trained without Supervision",
      "author" : [ "M. Siu", "H. Gish", "A. Chan", "W. Belfield" ],
      "venue" : "in Inter- Speech,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Unsupervised acoustic sub-word unit detection for query-by-example spoken term detection",
      "author" : [ "M. Huijbregts", "M. McLaren", "D. van Leeuwen" ],
      "venue" : "ICASSP, 2011, pp. 4436–4439.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Unsupervised acoustic and language model training with small amounts of labelled data",
      "author" : [ "S. Novotney", "R. Schwartz", "J. Ma" ],
      "venue" : "ICASSP, 2009, pp. 4297–4300.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Unsupervised Hidden Markov Modeling of Spoken Queries for Spoken Term Detection without Speech Recognition",
      "author" : [ "C. Chan", "L. Lee" ],
      "venue" : "InterSpeech,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Altosaar “Computational language acquisition by statistical bottom-up processing,",
      "author" : [ "O.J. Rasanen", "T.U.K. Laine" ],
      "venue" : "in Inter- Speech,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "A noise robust method for pattern discovery in quantized time series: the concept matrix approach,",
      "author" : [ "O.J. Rasanen", "U.K. Laine", "T. Altosaar" ],
      "venue" : "InterSpeech,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Self-learning vector quantization for pattern discovery from speech",
      "author" : [ "O.J. Rasanen", "U.K. Laine", "T. Altosaar" ],
      "venue" : "InterSpeech, 2009, pp. 852–855.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Rasanen, Fully unsupervised word learning from continuous speech using transitional probabilities of atomic acoustic events",
      "author" : [ "J. O" ],
      "venue" : "InterSpeech,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Unsupervised Spoken Term Detection with Spoken Queries ",
      "author" : [ "C. Chan" ],
      "venue" : "Ph.D dissertation, National Taiwan University,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Unsupervised optimal phoneme segmentation: objectives, algorithm and comparisons",
      "author" : [ "Y. Qiao", "N. Shimomura", "N. Minematsu" ],
      "venue" : "ICASSP, 2008, pp. 3989–3992.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Integrating Frame-based and Segmentbased Dynamic Time Warping for Unsupervised Spoken Term Detection with Spoken Queries",
      "author" : [ "C. Chan", "L. Lee" ],
      "venue" : "in ICASSP,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Toward Unsupervised Model-based Spoken Term Detection with Spoken Queries without Annotated Data",
      "author" : [ "C. Chan", "C. Chung", "Y. Kuo", "L. Lee" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Performance analysis for lattice-based speech indexing approaches using word and subword units",
      "author" : [ "Y.-c. Pan", "L.-s. Lee" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 6, August 2010, pp. 1562–1574.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Topological gray-scale watershed transform",
      "author" : [ "M. Couprie", "G. Bertrand" ],
      "venue" : "Proc. of SPIE Vision Geometry V, 1997, vol. 3168, pp. 136–146.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Updateable PAT-Tree Approach to Chinese Key Phrase Extraction using Mutual Information: A Linguistic Foundation for Knowledge Management",
      "author" : [ "T. Ong", "H. Chen" ],
      "venue" : "Proc. the Second Asian Digital Library Conference, 1999, pp. 63–84.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Enhancing Query Expansion for Semantic Retrieval of Spoken Content with Automatically Discovered Acoustic Patterns",
      "author" : [ "H. Lee", "Y. Li", "C. Chung", "L. Lee" ],
      "venue" : "submitted to ICASSP, 2013",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Approximating the Kullback Liebler Divergence between Gaussain Mixture Models",
      "author" : [ "J. Hershey", "P. Olsen" ],
      "venue" : "in ICASSP,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "The spoken web search task at Mediaeval 2011",
      "author" : [ "F. Metze", "N. Rajput" ],
      "venue" : "ICASSP, 2012, pp. 5165–5168.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An acoustic segment modeling approach to query-by-example spoken term detection",
      "author" : [ "H. Wang", "C.-C. Leung", "T. Lee", "B. Ma", "H. Li" ],
      "venue" : "ICASSP, 2012, pp. 5157–5160.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Keyword spotting of arbitrary words using minimal speech resources",
      "author" : [ "A. Garcia", "H. Gish" ],
      "venue" : "ICASSP, 2006.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A phonetic search approach to the 2006 NIST spoken term detection evaluation",
      "author" : [ "R. Wallace", "R. Vogt", "S. Sridharan" ],
      "venue" : "InterSpeech, 2007, pp. 2385–2388.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Open vocabulary spoken-document retrieval based on query expansion using related web documents",
      "author" : [ "M. Terao", "T. Koshinaka", "S. Ando", "R. Isotani", "A. Okumura" ],
      "venue" : "InterSpeech, 2008, pp. 2171–2174.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A comparison of queryby example methods for spoken term detection",
      "author" : [ "W. Shen", "C.M. White", "T.J. Hazen" ],
      "venue" : "InterSpeech, 2009, pp. 2143–2146.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "From hmms to segment models: A unified view of stochastic modeling for speech recognition",
      "author" : [ "M. Ostendorf", "V. Digalakis", "O.A. Kimball" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing, vol. 4, pp. 360V-378, 1995.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "A piecewise aggregate approximation lowerbound estimate for posteriorgram-based dynamic time warping",
      "author" : [ "Y. Zhang", "J.R. Glass" ],
      "venue" : "InterSpeech, 2011, pp. 1909–1912.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Speech recognition and utterance verification based on a generalized confidence score",
      "author" : [ "M.-W. Koo", "C.-H. Lee", "B.-H. Juang" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing, vol. 9, no. 8, pp. 821V-832, 2001.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "An acoustic segment model approach to incorporating temporal information into speaker modeling for text-independent speaker recognition",
      "author" : [ "Y. Tsao", "H. Sun", "H. Li", "C.-H. Lee" ],
      "venue" : "ICASSP, 2010, pp. 4422–4425.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This is why substantial effort [1]-[15] has been made for unsupervised discovery of acoustic patterns from huge quantities of acoustic data which may be easily obtained nowadays, without manual labels and corresponding knowledge.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 14,
      "context" : "This is why substantial effort [1]-[15] has been made for unsupervised discovery of acoustic patterns from huge quantities of acoustic data which may be easily obtained nowadays, without manual labels and corresponding knowledge.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 15,
      "context" : "However, it is well known that speech signals have multi-level structure including at least phoneme and words, and such structure are very helpful in analysing or decoding speech[16].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 16,
      "context" : "Watershed transformation is able to capture the number of objects and their borders in a gray scale image [18].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 17,
      "context" : "This can be achieved by constructing an efficient data structure called PAT-Tree using the labels Wi[19].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "Method (1) brought us halfway through the proposed approach (initialization and stage (I)) producing two-level patterns, while method (2) was similar to the unsupervised initialization methods used previously with one-level patterns only [1][17].",
      "startOffset" : 238,
      "endOffset" : 241
    }, {
      "referenceID" : 2,
      "context" : "approach wile method (3) was actually the intuitive joint optimization of both acoustic and linguistic parameters similar to previously proposed approaches [3][4].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "approach wile method (3) was actually the intuitive joint optimization of both acoustic and linguistic parameters similar to previously proposed approaches [3][4].",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 18,
      "context" : "optimization in stage (III), on the other hand, are better observed in a companion paper on semantic retrieval of spoken content also submitted to ICASSP 2013[20], since the word-like patterns carried semantics.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 20,
      "context" : "We also applied the discovered patterns on a task of spoken term detection [22]-[27] and compared to a set of Mandarin syllable models trained on a manually annotated corpus of 24.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 25,
      "context" : "We also applied the discovered patterns on a task of spoken term detection [22]-[27] and compared to a set of Mandarin syllable models trained on a manually annotated corpus of 24.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 19,
      "context" : "The distance metric used for DTW was the KL-divergence between the two Gaussian mixtures [21].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "The main difference from similar approaches proposed earlier [1][2][3][4][5][6][7] lies in the two-level acoustic patterns and the layer-after-layer gradual learning of the model parameters with cascaded stages of iterative optimization.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "The main difference from similar approaches proposed earlier [1][2][3][4][5][6][7] lies in the two-level acoustic patterns and the layer-after-layer gradual learning of the model parameters with cascaded stages of iterative optimization.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "The main difference from similar approaches proposed earlier [1][2][3][4][5][6][7] lies in the two-level acoustic patterns and the layer-after-layer gradual learning of the model parameters with cascaded stages of iterative optimization.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "The main difference from similar approaches proposed earlier [1][2][3][4][5][6][7] lies in the two-level acoustic patterns and the layer-after-layer gradual learning of the model parameters with cascaded stages of iterative optimization.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "The main difference from similar approaches proposed earlier [1][2][3][4][5][6][7] lies in the two-level acoustic patterns and the layer-after-layer gradual learning of the model parameters with cascaded stages of iterative optimization.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "The main difference from similar approaches proposed earlier [1][2][3][4][5][6][7] lies in the two-level acoustic patterns and the layer-after-layer gradual learning of the model parameters with cascaded stages of iterative optimization.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "The main difference from similar approaches proposed earlier [1][2][3][4][5][6][7] lies in the two-level acoustic patterns and the layer-after-layer gradual learning of the model parameters with cascaded stages of iterative optimization.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "Although some earlier approaches [1] also took hierarchical knowledge into consideration, our work used 13-state single Gaussian HMMs as compared to the conventional HMMs with smaller number of states and multi-Gaussian [1][2][3][4] to model the trajectories of acoustic patterns with less acoustic variation.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "Although some earlier approaches [1] also took hierarchical knowledge into consideration, our work used 13-state single Gaussian HMMs as compared to the conventional HMMs with smaller number of states and multi-Gaussian [1][2][3][4] to model the trajectories of acoustic patterns with less acoustic variation.",
      "startOffset" : 220,
      "endOffset" : 223
    }, {
      "referenceID" : 1,
      "context" : "Although some earlier approaches [1] also took hierarchical knowledge into consideration, our work used 13-state single Gaussian HMMs as compared to the conventional HMMs with smaller number of states and multi-Gaussian [1][2][3][4] to model the trajectories of acoustic patterns with less acoustic variation.",
      "startOffset" : 223,
      "endOffset" : 226
    }, {
      "referenceID" : 2,
      "context" : "Although some earlier approaches [1] also took hierarchical knowledge into consideration, our work used 13-state single Gaussian HMMs as compared to the conventional HMMs with smaller number of states and multi-Gaussian [1][2][3][4] to model the trajectories of acoustic patterns with less acoustic variation.",
      "startOffset" : 226,
      "endOffset" : 229
    }, {
      "referenceID" : 3,
      "context" : "Although some earlier approaches [1] also took hierarchical knowledge into consideration, our work used 13-state single Gaussian HMMs as compared to the conventional HMMs with smaller number of states and multi-Gaussian [1][2][3][4] to model the trajectories of acoustic patterns with less acoustic variation.",
      "startOffset" : 229,
      "endOffset" : 232
    }, {
      "referenceID" : 14,
      "context" : "A more complete experiment on spoken term detection in a companion paper submitted to ICASSP 2013 [15] demonstrates how our model can outperform the segmental DTW approach.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 18,
      "context" : "retrieval of spoken content also submitted to ICASSP 2013 [20].",
      "startOffset" : 58,
      "endOffset" : 62
    } ],
    "year" : 2015,
    "abstractText" : "Techniques for unsupervised discovery of acoustic patterns are getting increasingly attractive, because huge quantities of speech data are becoming available but manual annotations remain hard to acquire. In this paper, we propose an approach for unsupervised discovery of linguistic structure for the target spoken language given raw speech data. This linguistic structure includes two-level (subwordlike and word-like) acoustic patterns, the lexicon of word-like patterns in terms of subword-like patterns and the N-gram language model based on word-like patterns. All patterns, models, and parameters can be automatically learned from the unlabelled speech corpus. This is achieved by an initialization step followed by three cascaded stages for acoustic, linguistic, and lexical iterative optimization. The lexicon of word-like patterns defines allowed consecutive sequence of HMMs for subword-like patterns. In each iteration, model training and decoding produces updated labels from which the lexicon and HMMs can be further updated. In this way, model parameters and decoded labels are respectively optimized in each iteration, and the knowledge about the linguistic structure is learned gradually layer after layer. The proposed approach was tested in preliminary experiments on a corpus of Mandarin broadcast news, including a task of spoken term detection with performance compared to a parallel test using models trained in a supervised way. Results show that the proposed system not only yields reasonable performance on its own, but is also complimentary to existing large vocabulary ASR systems.",
    "creator" : "LaTeX with hyperref package"
  }
}