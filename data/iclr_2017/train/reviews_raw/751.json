{"conference": "ICLR 2017 conference submission", "title": "Combating Deep Reinforcement Learning's Sisyphean Curse with Intrinsic Fear", "abstract": "To use deep reinforcement learning in the wild, we might hope for an agent that can avoid catastrophic mistakes. Unfortunately, even in simple environments, the popular deep Q-network (DQN) algorithm is doomed by a Sisyphean curse. Owing to the use of function approximation, these agents eventually forget experiences as they become exceedingly unlikely under a new policy. Consequently, for as long as they continue to train, DQNs may periodically relive catastrophic mistakes. Many real-world environments where people might be injured exhibit a special structure. We know a priori that catastrophes are not only bad, but that agents need not ever get near to a catastrophe state. In this paper, we exploit this structure to learn a reward-shaping that accelerates learning and guards oscillating policies against repeated catastrophes. First, we demonstrate unacceptable performance of DQNs on two toy problems. We then introduce intrinsic fear, a new method that mitigates these problems by avoiding dangerous states. Our approach incorporates a second model trained via supervised learning to predict the probability of catastrophe within a short number of steps. This score then acts to penalize the Q-learning objective, shaping the reward function away from catastrophic states.", "reviews": [{"is_meta_review": true, "comments": "This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).\n\n1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of \u201crobust RL\u201d. A good place to start might be with the work of Shie Mannor.\n\n2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as \u201ccatastrophic\u201d if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I\u2019d want to see plots for \u201caverage reward\u201d in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible \u201cdanger states\u201d?\n\n3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I\u2019m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant \u201cstate-of-the-art\u201d variant of DQN are you using?\n\nThe good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.\n\nGiven my comment a couple of weeks ago, and the prompt response (\u201cwe implemented expected SARSA\u201d), I would have expected that the paper had been revised with the new results by now? In any case, I\u2019m open to discussing all these points and revising my opinion based on an updated version of the paper.\n\nMinor comment: the bibliography is done sloppily, with missing years, conference venues and missing/misspelled author lists, e.g. \u201cSergey et al. Levine\u201d. I also think it is good form to cite the actual conference publications instead of arXiv where applicable.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "This paper presents a few interesting ideas, namely the idea of keeping around a set of \"danger states\" and treating these states with some special consideration in reply to make sure that their impact is not neglected after collecting a lot of additional data.\n \n However, there are two main problems: 1) the actual implementation here seems fairly ad-hoc, and it's not at all clear to me that this particular algorithm (building a classifier with equal numbers of good and danger states, and then injecting an additional reward into the Q-learning task based upon this classifier), is the right way to go about this. The presentation is also difficult to follow, and the final results imply aren't that compelling (though this is improving after the revisions, but still has a way to go. We therefore encourage the authors to resubmit their work at a future conference venue.\n \n Pros:\n + Interesting idea of keeping around danger states and injecting them into training\n \n Cons:\n - Algorithm doesn't seem that well motivated\n - Presentation is a bit unclear, takes until page 6 to actually present the basic approach.\n - Experiments aren't that convincing (better after revisions, but still need work)", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Some high-level comments", "OTHER_KEYS": "(anonymous)", "comments": "Thanks for an interesting take on an important question :) \nSome high level comments/questions about the issue of \"safety\" in RL:\n\n1. What is the justification that there is any hope of behaving safely when a model is unavailable? Some amount of exploration is needed anyway, and without exploration you are doomed to not find the optimal policy. Essentially you either get stuck with a sub-optimal policy or accept some amount of failures. Whether such failures are acceptable or not is highly application specific, and hence I would encourage the authors to consider a concrete application that has some use (as opposed to toy problems) and provide a reasonable solution tailored to the same. A general solution is unlikely to exist, or be useful in practice.\n\n2. If a model is available, then much of the motivations raised become irrelevant. A simulated car can hit simulated pedestrians many times, and learn from the mistake in simulation -- it does not pose any threat. Further, measures like risk sensitive RL or robust RL can be put in place to ensure sim2real transfer.\n\n3. In light of (2), I would actually encourage the authors to think of their danger model as a form of reward shaping. Can we think of fear as a way to guide the exploration space when a model is made available? That way, the question shifts away from \"safety\" to sample efficiency and transfer. I think the danger model could be a good approach for these considerations (e.g. similar to SARSA). Though these have been mentioned in the paper, I look forward to an expanded discussion along these lines.", "IS_META_REVIEW": false, "DATE": "26 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "heuristic for avoiding large negative rewards", "is_meta_review": false, "comments": "This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a \"danger model\". The paper is well written including some rather poetic language [*].\n\nThe heuristic is evaluated in two toy domains. I would think that in order to properly evaluate this one would use a well known benchmark e.g. Atari. Atari seems particularly apt since those games are full of catastrophes (i.e. sudden death).\n\n[*] this reviewer's favourite quotes:\n\"Imagine a self-driving car that had to periodically hit a few pedestrians in order to remember that it\u2019s undesirable.\"\n\"The child can learn to adjust its behaviour without actually having to stab someone.\"\n\"... the catastrophe lurking just past the optimal shave.\"", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "21 Dec 2016", "REVIEWER_CONFIDENCE": 2}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "is_meta_review": false, "comments": "- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. \n\n- There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. \"Reinforcement learning with unsupervised auxiliary tasks.\", Blundell, Charles, et al. \"Model-free episodic control.\" , Narasimhan et al. \"Language understanding for text-based games using deep reinforcement learning\"), which could perhaps reinforce the agent to avoid revisiting catastrophic states. \n\n- Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the learner instead of a separate model? ", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "is_meta_review": false, "comments": "This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).\n\n1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of \u201crobust RL\u201d. A good place to start might be with the work of Shie Mannor.\n\n2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as \u201ccatastrophic\u201d if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I\u2019d want to see plots for \u201caverage reward\u201d in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible \u201cdanger states\u201d?\n\n3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I\u2019m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant \u201cstate-of-the-art\u201d variant of DQN are you using?\n\nThe good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.\n\nGiven my comment a couple of weeks ago, and the prompt response (\u201cwe implemented expected SARSA\u201d), I would have expected that the paper had been revised with the new results by now? In any case, I\u2019m open to discussing all these points and revising my opinion based on an updated version of the paper.\n\nMinor comment: the bibliography is done sloppily, with missing years, conference venues and missing/misspelled author lists, e.g. \u201cSergey et al. Levine\u201d. I also think it is good form to cite the actual conference publications instead of arXiv where applicable.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "baselines", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "SARSE", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 4, "authors": "Zachary C. Lipton, Jianfeng Gao, Lihong Li, Jianshu Chen, Li Deng", "KEYWORDS": "Owing to function approximation, DRL agents eventually forget about dangerous transitions once they learn to avoid them, putting them at risk of perpetually repeating mistakes. We propose techniques to avert catastrophic outcomes.", "accepted": false, "id": ""}
