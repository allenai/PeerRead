{
  "name" : "1609.05610.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Enhancing LambdaMART Using Oblivious Trees",
    "authors" : [ "Marek Modrý", "Michal Ferov" ],
    "emails" : [ "marek.modry@firma.seznam.cz", "michal.ferov@firma.seznam.cz" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 9.\n05 61\n0v 1\n[ cs\n.I R\n] 1\n9 Se\np 20\nKeywords: Document Retrieval for Search Engines, LambdaMART, Learning to Rank, Oblivious Decision Trees."
    }, {
      "heading" : "1 Introduction",
      "text" : "Learning to rank (LTR) is a machine learning technique broadly used in many areas, such as document retrieval in search engines, collaborative filtering in recommender systems or question answering [22,9,15]. Generally speaking, LTR is a subject of interest for any system that needs to order intermediate or final results with respect to a given utility function [6].\nThe focus of this paper is document retrieval in the sense of search engine results ranking, which is also reflected by the data on which the experiments were conducted. There are many algorithms that approach the task in different ways but LambdaMART is the state-of-the-art LTR algorithm. It has been recently revealed that it is being used as a part of Facebook’s machine learning framework [2]. In contrast to the most of LTR algorithms, LambdaMART uses direct optimisation with respect to a given objective function. In this paper we consider Normalised Discounted Cumulative Gain. On the other hand, LambdaMART is prone to overfitting when training complicated models consisting of hundreds of trees.\nThis paper proposes a modification of the standard LambdaMART algorithm which utilises oblivious decision trees instead of standard regression trees in the\nprocess of learning. Oblivious tree is a special kind of a decision tree with a constraint on the selection of decision rules. LambdaMART iteratively builds a model by constructing a sequence of decision trees whose predictions are summed up to obtain the final prediction. The introduction of oblivious trees brings many benefits like making the algorithm less prone to overfitting, simplifying and speeding up the use of the model and potentially decreasing the number of features included in the model.\nWe present experimental results which suggest that the performance of the current state-of-the-art LTR algorithm LambdaMART can be improved if standard regression trees are replaced by oblivious trees. The experimental results were obtained both on a public LTR dataset and on a dataset originating from Czech search engine Seznam.cz. We performed the comparison using different parameter setups and also verified whether the size of the training set can influence the dominance of the oblivious trees variant.\nThe paper is structured as follows. Existing literature and research is examined and described in Section 2. Formal definition of LTR framework is given in Section 3 along with the description of performance evaluation for LTR models and description of LambdaMART algorithm and oblivious decision trees. Experiments, datasets and results are presented in Section 4. Finally, in Section 5 we provide summary of our results and suggestions for future work."
    }, {
      "heading" : "2 Related Work",
      "text" : "When solving standard classification and regression problems, the most common approach aims at direct optimisation of a given objective function. However, due to the non-differentiability of a performance measure function, direct optimisation is a challenging task in LTR. Hence, researchers approach LTR task in many different ways.\nFor instance, LTR problem can be reformulated as a regression task of the relevance label prediction which approaches the problem in a pointwise manner. Each query-document pair is then considered a single data sample and the relations between documents belonging to a particular query are not taken into account. Mean Squared Error (MSE) is then usually used as the objective function [5]. Random Forest [5] or Multiple Additive Regression Trees (MART) [13] can be utilised to solve the task in the aforementioned manner. Similarly, PRank algorithm proposed in [10] uses a neural network to predict the relevance label. However, the authors of [10] extend the task to ordinal regression, where the relevance score is converted to the relevance class (resp. label) in the end. Besides, there are also pointwise algorithms that treat the problem as a classification problem. For instance, McRank algorithm [22] uses gradient boosting tree algorithm and reformulates the task as a multiple ordinal classification.\nAlgorithms applying a pairwise approach formalise the problem as classification or regression on pairs of query-documents. As was pointed out in [9], even though pairwise formalisations benefit from the possibility of using existing classification or regression methods, the results can be suboptimal as the models\noptimise surrogate loss functions, the computation efficiency can be a problem and the results can be potentially biased towards queries with more documents. In [16], RankingSVM algorithm employs ordinal regression to determine relative relevance of document pairs. RankBoost [12] is a boosting algorithm based on AdaBoost’s idea and uses a sequence of weak learners in order to minimise the number of incorrectly ordered pairs. Burges et al. [6] proposed RankNet algorithm that learns a neural network to predict the relevance score of a single query-document in such a way that the score can be used to correctly order any pair of query-document samples. The network is optimised using gradient descent on a probabilistic cost function defined on pairs of documents.\nAlgorithms taking the whole ranking list into account belong to the group of list-wise algorithms. The approach is straightforward and uses all information about the ranked list to further improve the model. On the other hand, direct optimisation is very challenging. Authors of PermuRank [31] use SVM technique to minimise a hinge loss function on permutations of documents. Similarly, AdaRank [30] repeatedly constructs weak rankers in order to minimise an exponential loss which is derived from the original performance measure. Examples of other algorithms that employ list-wise approach are ListMLE [29], ListNet [9], RankCosine [24], LambdaRank [8] or LambdaMART [7]. Note that LambdaRank was the first algorithm to propose using lambdas to define gradients. LambdaMART algorithm is the main focus of this paper.\nOblivious decision tree is a special kind of a decision tree with constraints on the selection of a decision rule. Experimental results of Almuallim and Dietterich [3] demonstrated that standard decision trees, e.g. those built using ID3 algorithm, can perform poorly on datasets with many irrelevant features. This problem is addressed by Langley and Sage in [20] where they proposed tackling the problem of irrelevant features by using oblivious decision trees. The constraints on decision rules selection were introduced also by Schlimmer in [26]. Although our modification uses a basic greedy top-down induction of oblivious trees, there have been several methods of oblivious tree construction proposed (see [25,18,21]). Authors of YetiRank algorithm [14] introduced oblivious trees into LTR task. However, YetiRank works in a different way and utilises oblivious trees differently than LambdaMART."
    }, {
      "heading" : "3 Learning to Rank",
      "text" : ""
    }, {
      "heading" : "3.1 Formal description of LTR",
      "text" : "LTR can be formally described as follows. In the training phase of LTR process, a set of queries Q = {q1, q2, . . . , qn}, where n denotes the number of queries, is given. For every query qi there is a set of documents di = {d i 1, d i 2, . . . , d i m(qi) }, where m(qi) is the number of documents given for the query qi. For every querydocument pair (qi, d i j), where i ∈ {1, . . . , n} and j ∈ {1, . . . ,m(qi)}, a label y i j is given. Similarly, a feature vector xij ∈ R l, where l is the number of features, is specified for each query-document pair (qi, d i j), where i ∈ {1, 2, . . . , n} and\nj ∈ {1, . . . ,m(qi)}. Finally, the dataset S can be defined as\nS = {(qi,di,yi)} n i=1 , (1)\nwhere yi = {y i 1, . . . , y i m(qi) } for i ∈ {1, . . . , n}.\nIn our formal framework, the model is a function f mapping a feature vectors corresponding to samples to score values, i.e. f : Rl → R. By evaluation of the model f on the query-document sample (qi, d i j) we mean application of the function f on the feature vector xij . When applying the model, all querydocument pairs (qi, d i j) in the list corresponding to the query qi are evaluated and have their score assigned. The list is then ordered according to the score in the descending order.\nA permutation πi(di, f) of the set of integers {1, . . . ,m(qi)} is created based on the document list di and the model function f . To be more specific, the permutation represents a ranked (ordered) list and πi(j) denotes the position of the document dij in the ranking (ordering). When applying the model, f is utilised to rank elements of documents lists for each single query in the dataset S and, finally, the performance of the model is evaluated using given evaluation measure (e.g. NDCG)."
    }, {
      "heading" : "3.2 Performance evaluation",
      "text" : "Unlike the models built during classification or regression tasks, a quality of LTR model depends primarily on the order of a sorted list. As a result, LTR evaluation functions usually are not smooth or differentiable, which makes the direct optimisation very challenging as we cannot use gradient descent methods. In the following paragraphs, Normalised Discounted Cumulative Gain (NDCG) measure is introduced. Our experiments described in the text below are evaluated using NDCG.\nAs described in [17], NDCG is applicable to multi-graded relevance scale. Discounted Cumulative Gain (DCG) can be defined as follows:\nDCG(f ;dj,yj) =\nm ∑\ni=1\nG(y(dπf (i)))) disc(i), (2)\nwhere dj = {d1, . . . , dm} is a set of query-document pairs belonging to query qj and yj = {y(d1), . . . , y(dm)} is the set of corresponding relevance labels, G : R → R is an increasing gain function, disc : R → R is a decreasing position discount function, πf is the resulting ranked list and dπf (i) is the document at i th position in the ranked list. Ideal Discounted Cumulative Gain (IDCG) represents the DCG score of a list that was ranked in the best way possible:\nIDCG(f ;dj,yj) = max πf\n{\nm ∑\ni=1\nG(y(dπf (i)))) disc(i)\n}\n(3)\nSubsequently, NDCG is defined in a straightforward way:\nNDCG(f ;dj,yj) = DCG(f ;dj,yj)\nIDCG(f ;dj,yj) . (4)\nFollowing [28] and [22], the gain function G is defined as G(r) = 2r − 1 and the discount function disc is defined as\ndisc(i) =\n{\n1 log 2 (i+z) if i ≤ C, 0 if i > C,\nwhere C is a fixed integer cut-off constant that specifies on how many top documents does the measure focus. The use of a cut-off constant can be motivated for instance by the paging of search engine results and limited number of initially displayed documents as noted in [6,28]. The usage of cut-off constant k and limiting the evaluation only to the top k documents is denoted by NDCG@k. When calculating performance on multiple queries, the resulting performance is the mean of all performance scores averaged over the corresponding set of queries."
    }, {
      "heading" : "3.3 LambdaMART",
      "text" : "LambdaMART is a LTR algorithm introduced in [28]. It combines regression trees boosting technique that is utilised in MART3 algorithm [13] and LambdaRank’s idea of using lambda trick that avoids the non-smoothness problem of ranking performance measures [8].\nThere is no known way to derive the gradients from the performance measure directly. LambdaMART overcomes this problem by computing the gradients based on the current state of the model and desirable changes in the order of elements in the list, so called lambdas. Lambda coefficient represents how the model’s prediction for a particular document should change in the next iteration in order to improve the model performance. As LambdaMART builds one tree per iteration, lambdas representing the target function tk+1 : R\nl → R for the (k + 1)th tree are computed after building k trees. Note that the direct optimisation following the computed gradients (i.e. lambdas) can be further modified by using Newton’s method (see [27]). Newton’s method is implemented by adjusting values in the leaves of a newly built tree. For detailed explanation of the Newton’s method step in LambdaMART see [7]."
    }, {
      "heading" : "3.4 Oblivious Decision Trees",
      "text" : "Oblivious decision tree is a particular kind of a decision tree. It is characterised by the constraint which allows to select only one feature in a particular level of the decision tree, i.e. all the decision rules in the specific level can involve\n3 Multiple Additive Regression Trees\nonly one selected feature. Although the definition of oblivious decision trees in [25] permits to use different threshold values for the selected feature, our implementation goes even further and all nodes in the given depth use only a single rule (i.e. feature and threshold is uniform on levels).\nIn [19] Kohavi et al. proposed a way to represent oblivious decision trees by decision tables. A decision table can be implemented very efficiently and can be processed much faster than a standard decision tree with no constraints. Such efficiency advantage can be essential for commercial search engines (such as Seznam.cz) which have to process hundreds of requests per second.\nFurthermore, as the authors of [4,23,26] have noted, the constraint of oblivious decision trees has a positive effect on the effectiveness of feature selection which is a desired property, especially when dealing with datasets containing many uninformative irrelevant features.\nOur implementation uses top-down greedy algorithm for oblivious trees induction. A decision rule is selected for each level of the tree in a top-down manner. Each rule consists of a feature and a threshold value which splits k disjoint sets into 2k disjoint subsets. Note that as we are working with complete binary trees k = 2k ′\nwhere k′ is the number of levels of the tree. Formally speaking, a splitting rule r is a pair (i, v), where i ∈ {1, . . . , l} is a index of a feature and v ∈ R is a threshold value. A set of feature vectors X ⊆ R can be then split into two disjoint subsets XL, XR such that for every x = (x1, . . . , xl) ∈ X we have x ∈ XL if xi ≤ v and x ∈ X\nR if xi > v. Naturally, splitting rule can be applied to any collection of k disjoint sets X1, . . . , Xk ⊆ R\nl to obtain a collection of 2k disjoint sets XL1 , X R 1 , . . . , X L k , X R k . When given a collection of sets X = {X1, . . . , Xk} ⊆ P(R l) and a target function t : Rl → R, the splitting rule is then chosen as argmin\nr\n{M(r, t,X )} , (5)\nwhere M is a non-negative real-valued function measuring the optimality of the splitting rule r on the collection X1, . . . , Xk with respect to the target function t (with a slight abuse of notation we can assume that M is defined for every k ∈ N). In our implementation, the optimality measure M was defined as\nM(r, t,X ) = 1\n∑k\ni=1 |Xi|\n(\nk ∑\ni=1\n|XLi |Var ( t(XLi ) ) + |XRi |Var ( t(XRi ) )\n)\n, (6)\nwhere t(Xi) = {t(x) | x ∈ Xi ⊆ R l} is the set of target scores corresponding to the samples in the set (resp. leaf node) Xi."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Data",
      "text" : "The experiments were performed using two datasets. MSLR-WEB10k dataset4, which is a public LTR dataset released in 2010 by Microsoft, and the sec-\n4 Microsoft Learning to Rank Datasets, 2010. Available at http://research.microsoft.com/mslr\nond dataset originating from Czech search engine Seznam.cz. Statistics of both datasets are provided in Tab. 1.\nMSLR-WEB10k dataset was chosen due to its availability, its sufficient size and also its multi-graded relevance labels. On the other hand, as the authors of the dataset claim (see [1]), features in MSLR-WEB10k dataset are ”those widely used in the research community”. Although Seznam.cz’s dataset is not publicly accessible, it allowed us to carry out the experiments on real-life data with a subset of features that are used in a commercial search engine.\nThe datasets, similarly to the most of Learning to rank datasets, use LibSVM (also called SVMlight) format, i.e. each line represents a query-document pair, specifying its relevance label, query ID and features. Both datasets use multigraded scale of relevance labels that were annotated by human assessors. The higher the label, the more relevant the document is to a given query. While MSLR-WEB10k uses 5 grades scale, Seznam.cz uses 6 grades. Both datasets were aligned to use 1 as the minimal relevance label in the range, i.e. in MSLRWEB10k dataset yij = 5 means that the document dj is maximally relevant to the query qi whereas y i j = 1 means that dj is completely irrelevant to qi.\nBoth datasets were split into 5 folds that were subsequently used for crossvalidation. Training dataset, validation set and testing set therefore consist of 60%, 20% and 20% of queries, respectively. Note that each query with its documents represent a compact data sample which means that a single query cannot be divided into more folds and therefore the dataset is split query-wise."
    }, {
      "heading" : "4.2 Methodology",
      "text" : "In this paper, two LambdaMART algorithm variants are compared, one using standard regression trees (as described in [28]) and one using oblivious decision trees instead. While RankLib [11] library was used to train standard LambdaMART models, Seznam.cz’s own implementation of LambdaMART with oblivious decision trees called RCRank was used to train the other variant.\nAs noted above, the comparison was performed using cross-validation with training, validation and test sets consisting of 60%, 20% and 20% queries, respectively. There were 5 runs of each experiment using the same parameter setup but different data folds. Validation dataset was used to find the optimal number of trees in order to avoid overfitting and final test set performance was obtained by averaging the scores over experiments with the same parameter setup.\nSince the optimal parameters for both LambdaMART variants can differ, experiments on various parameter setups for each of the algorithm variants were performed and subsequently compared. Total number of leaves in a tree and learning rate were subjects of the change in particular setups. With 1000 trees being the maximal size of the forest, the length of each forest was cut according to the validation set to the size with highest validation quality (measured by NDCG@10). Lambda gradients in both algorithm variants also optimise NDCG@10 measure.\nThe second set of experiments was focused on the influence of the size of the training set. Although the setup of parameters were the same as in the previous experiments, the size of the training set was decreased. Originally, the training set consisted of 60% (i.e. approximately 700 000 document-query pairs) of all samples which was lowered to only 6% of samples (i.e. approximately 70 000 query-document pairs) and the training set in a subsequent experiment consisted only of 0.85%. samples (i.e. approximately 10 000 query-document pairs). Note that, apart from the size of the training set, nothing was changed with respect to the the first set of experiments. Therefore the parameters, just like the size and splitting strategy of validation set and test set, remained the same. This experiment was performed only on MSLR-WEB10k dataset.\nAnalysis of how the number of features can influence the performance of the algorithms was performed on a modified dataset (the dataset with 10 times smaller training set was used). The list of features was sorted by the number of occurrences in models in the first set of experiments and only 50 most frequent features were kept in the dataset. We aimed at removing such features that are irrelevant or noisy."
    }, {
      "heading" : "4.3 Results",
      "text" : "The first set of experiments aimed at comparing the two algorithm variants, LambdaMART (using regression trees) and RCRank (using oblivious trees). Tab. 2 and Tab. 3 presents the results of the comparison on both, public and internal dataset. LambdaMART does not outperform RCRank in no parameter setup. When comparing the best results achieved by each of the algorithms, RCRank reached NDCG@10 score of 0.7135 and 0.5706, while LambdaMART’s score was 0.7110 and 0.5582, on Seznam.cz and MSLR-WEB10k datasets, respectively, which means improvement by 0.35% and 2.22%. Despite the improvement on Seznam.cz dataset being slightly lower, the improvement is consistent over all parameter setups.\nIn order to analyse the influence of the size of the training set on RCRank’s superiority, two experiments set on smaller training sets were performed. Tab. 4 presents results of models built using training set ten times smaller. It can be observed that RCRank outperforms LambdaMART even when much smaller data set is being used. While NDCG score of RCRank models decreased by 5.12% on average, the scores of LambdaMART models decreased by 5.64% which could be a sign of oblivious trees being less prone to overfitting on smaller datasets.\nSimilar results were achieved in experiment using 70 times less samples. The score of RCRank decreased by 10.44% on average and the score of LambdaMART models decreased by 11.15% in comparison to the full-size dataset. Best models’ NDCG@10 scores for RCRank and LambdaMART were 0.5115 and 0.4988.\nThe last set of experiments analysed the influence of number of features. Our hypothesis was that the removal of less important features will affect RCRank and LambdaMART differently. As mentioned above, standard regression trees can perform worse when irrelevant features are present in the dataset, whereas oblivious trees should not be influenced by irrelevant features as much as standard regression trees. Tab. 5 presents the results of the experiments performed on MSLR-WEB10k dataset with smaller training sets.\nInterestingly, even though 86 features were removed from the dataset, overall performance of LambdaMART improved with average improvement 0.37% (averaged over all parameter setups). Since RCRank’s overall performance did not improve for each of the parameters setup and the average score increased only by 0.09%, LambdaMART is more prone to be affected by the presence of less relevant features. Note that results of all comparison experiments are statistically significant (p-value 0.01) except for a single one.5\n5 Experiment on trees with 64 leaves and learning rate 0.11 on Seznam.cz dataset."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we proposed an enhancement of LambdaMART algorithm substituting standard regression trees for oblivious decision trees which have several desirable properties. Using both, public MSLR-WEB10k dataset and internal dataset of Seznam.cz search engine, we experimentally compared performances of both variants. Moreover, the influence of various changes in the size of the dataset was examined.\nDepending on the dataset, we showed that by using oblivious trees instead of standard regression trees, the performance can be increased by more than 2.2%. Our experimental results further demonstrate that oblivious trees variant deals slightly better with smaller training datasets, as NDCG score of LambdaMART decreases faster than the score of RCRank6 with the decrease of the size of training set.\nOn the other hand, when potentially noisy and irrelevant features are removed from the dataset, LambdaMART’s score improves more than the score of RCRank. This can be a sign confirming the aforementioned fact that standard regression trees can perform worse when irrelevant features are present in the training data.\n6 Our implementation of LambdaMART using oblivious trees\nFuture work could focus on better theoretical understanding of the relation between standard regression trees and oblivious trees. Furthermore, all analysis could be also performed using various ranking measures, such as Expected Reciprocal Rank. Last but not least, it would be very useful to specify the properties of a dataset which is favourable to model using LambdaMART and not RCRank and vice versa."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to thank their colleagues from Seznam.cz research group for many insightful discussions, in particular to the authors of the original RCRank implementation that was based on Gradient Boosted Regression Trees without lambdas, Tomáš Cı́cha and Roman Rožńık."
    } ],
    "references" : [ {
      "title" : "Learning with many irrelevant features",
      "author" : [ "H. Almuallim", "T.G. Dietterich" ],
      "venue" : "In Proceedings of the ninth National conference on Artificial intelligence-Volume 2, pages 547–552. AAAI Press,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Learning boolean concepts in the presence of many irrelevant features",
      "author" : [ "H. Almuallim", "T.G. Dietterich" ],
      "venue" : "Artificial Intelligence, 69(1-2):279–305,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Random forests",
      "author" : [ "L. Breiman" ],
      "venue" : "Machine Learning, 45(1):5–32,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Learning to rank using gradient descent",
      "author" : [ "C. Burges", "T. Shaked", "E. Renshaw", "M. Deeds", "N. Hamilton", "G. Hullender" ],
      "venue" : "In In ICML, pages 89–96,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "From RankNet to LambdaRank to LambdaMART: An overview",
      "author" : [ "C.J.C. Burges" ],
      "venue" : "Technical report, Microsoft Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning to Rank with Nonsmooth Cost Functions",
      "author" : [ "C.J.C. Burges", "R. Ragno", "Q.V. Le" ],
      "venue" : "In B. Schölkopf, J. C. Platt, T. Hoffman, B. Schölkopf, J. C. Platt, and T. Hoffman, editors, NIPS, pages 193–200. MIT Press,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning to rank: From pairwise approach to listwise approach",
      "author" : [ "Z. Cao", "T. Qin", "T.-Y. Liu", "M.-F. Tsai", "H. Li" ],
      "venue" : "In Proceedings of the 24th International Conference on Machine Learning, ICML ’07, pages 129–136, New York, NY, USA,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Pranking with ranking",
      "author" : [ "K. Crammer", "Y. Singer" ],
      "venue" : "In Advances in Neural Information Processing Systems 14, pages 641–647. MIT Press,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "RankLib",
      "author" : [ "V. Dang" ],
      "venue" : "Online,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An efficient boosting algorithm for combining preferences",
      "author" : [ "Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer" ],
      "venue" : "J. Mach. Learn. Res., 4:933–969, Dec.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Greedy function approximation: A gradient boosting machine",
      "author" : [ "J.H. Friedman" ],
      "venue" : "Annals of Statistics, 29:1189–1232,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Winning the transfer learning track of yahoo!’s learning to rank challenge with yetirank",
      "author" : [ "A. Gulin", "I. Kuralenok", "D. Pavlov" ],
      "venue" : "In Yahoo! Learning to Rank Challenge, pages 63–76,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A short introduction to learning to rank",
      "author" : [ "L. Hang" ],
      "venue" : "IEICE TRANSACTIONS on Information and Systems, 94(10):1854–1862,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Large margin rank boundaries for ordinal regression",
      "author" : [ "R. Herbrich", "T. Graepel", "K. Obermayer" ],
      "venue" : "In A. Smola, P. Bartlett, B. Schölkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 115–132, Cambridge, MA,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Cumulated gain-based evaluation of ir techniques",
      "author" : [ "K. Järvelin", "J. Kekäläinen" ],
      "venue" : "ACM Trans. Inf. Syst., 20(4):422–446, Oct.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Oblivious decision trees, graphs, and top-down pruning",
      "author" : [ "R. Kohavi", "C.-H. Li" ],
      "venue" : "In IJCAI, pages 1071–1079. Citeseer,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Targeting business users with decision table classifiers",
      "author" : [ "R. Kohavi", "D. Sommerfield" ],
      "venue" : "In KDD, pages 249–253,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Oblivious decision trees and abstract cases",
      "author" : [ "P. Langley", "S. Sage" ],
      "venue" : "Technical report, DTIC Document,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Improving stability of decision trees",
      "author" : [ "M. Last", "O. Maimon", "E. Minkov" ],
      "venue" : "International Journal of Pattern Recognition and Artificial Intelligence, 16(02):145–159,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Mcrank: Learning to rank using multiple classification and gradient boosting",
      "author" : [ "P. Li", "C.J.C. Burges", "Q. Wu" ],
      "venue" : "In J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, NIPS. MIT Press,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Improving supervised learning by feature decomposition",
      "author" : [ "O. Maimon", "L. Rokach" ],
      "venue" : "In Foundations of Information and Knowledge Systems, pages 178–196. Springer,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Query-level loss functions for information retrieval",
      "author" : [ "T. Qin", "X.-D. Zhang", "M.-F. Tsai", "D.-S. Wang", "T.-Y. Liu", "H. Li" ],
      "venue" : "INFORMATION PROCESSING AND MANAGEMENT, 44(2),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Oblivious decision trees",
      "author" : [ "L. Rokach", "O. Maimon" ],
      "venue" : "In Data mining with decision trees, pages 76–77. World Scientific, Singapore,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Efficiently inducing determinations: A complete and systematic search algorithm that uses optimal pruning",
      "author" : [ "J.C. Schlimmer" ],
      "venue" : "In In Proceedings of the Tenth International Conference on Machine Learning,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "The newton-raphson method",
      "author" : [ "E. Whittaker", "G. Robinson" ],
      "venue" : "The calculus of observations: a treatise on numerical mathematics, 4th ed. Dover, New York, pages 84–87,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "Adapting boosting for information retrieval measures",
      "author" : [ "Q. Wu", "C.J. Burges", "K.M. Svore", "J. Gao" ],
      "venue" : "Inf. Retr., 13(3):254–270, June",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Listwise approach to learning to rank: Theory and algorithm",
      "author" : [ "F. Xia", "T.-Y. Liu", "J. Wang", "W. Zhang", "H. Li" ],
      "venue" : "In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages 1192–1199, New York, NY, USA,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Adarank: A boosting algorithm for information retrieval",
      "author" : [ "J. Xu", "H. Li" ],
      "venue" : "In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’07, pages 391–398, New York, NY, USA,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "T",
      "author" : [ "J. Xu" ],
      "venue" : "yan Liu, M. Lu, H. Li, and W. ying Ma. Directly optimizing evaluation measures in learning to rank. In SIGIR 2008 - Proceedings of the 31th annual international ACM SIGIR conference. ACM Press,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Learning to rank (LTR) is a machine learning technique broadly used in many areas, such as document retrieval in search engines, collaborative filtering in recommender systems or question answering [22,9,15].",
      "startOffset" : 198,
      "endOffset" : 207
    }, {
      "referenceID" : 6,
      "context" : "Learning to rank (LTR) is a machine learning technique broadly used in many areas, such as document retrieval in search engines, collaborative filtering in recommender systems or question answering [22,9,15].",
      "startOffset" : 198,
      "endOffset" : 207
    }, {
      "referenceID" : 12,
      "context" : "Learning to rank (LTR) is a machine learning technique broadly used in many areas, such as document retrieval in search engines, collaborative filtering in recommender systems or question answering [22,9,15].",
      "startOffset" : 198,
      "endOffset" : 207
    }, {
      "referenceID" : 3,
      "context" : "Generally speaking, LTR is a subject of interest for any system that needs to order intermediate or final results with respect to a given utility function [6].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 2,
      "context" : "Mean Squared Error (MSE) is then usually used as the objective function [5].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "Random Forest [5] or Multiple Additive Regression Trees (MART) [13] can be utilised to solve the task in the aforementioned manner.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 10,
      "context" : "Random Forest [5] or Multiple Additive Regression Trees (MART) [13] can be utilised to solve the task in the aforementioned manner.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "Similarly, PRank algorithm proposed in [10] uses a neural network to predict the relevance label.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "However, the authors of [10] extend the task to ordinal regression, where the relevance score is converted to the relevance class (resp.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 19,
      "context" : "For instance, McRank algorithm [22] uses gradient boosting tree algorithm and reformulates the task as a multiple ordinal classification.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 6,
      "context" : "As was pointed out in [9], even though pairwise formalisations benefit from the possibility of using existing classification or regression methods, the results can be suboptimal as the models",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "In [16], RankingSVM algorithm employs ordinal regression to determine relative relevance of document pairs.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 9,
      "context" : "RankBoost [12] is a boosting algorithm based on AdaBoost’s idea and uses a sequence of weak learners in order to minimise the number of incorrectly ordered pairs.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 3,
      "context" : "[6] proposed RankNet algorithm that learns a neural network to predict the relevance score of a single query-document in such a way that the score can be used to correctly order any pair of query-document samples.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 28,
      "context" : "Authors of PermuRank [31] use SVM technique to minimise a hinge loss function on permutations of documents.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 27,
      "context" : "Similarly, AdaRank [30] repeatedly constructs weak rankers in order to minimise an exponential loss which is derived from the original performance measure.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 26,
      "context" : "Examples of other algorithms that employ list-wise approach are ListMLE [29], ListNet [9], RankCosine [24], LambdaRank [8] or LambdaMART [7].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "Examples of other algorithms that employ list-wise approach are ListMLE [29], ListNet [9], RankCosine [24], LambdaRank [8] or LambdaMART [7].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 21,
      "context" : "Examples of other algorithms that employ list-wise approach are ListMLE [29], ListNet [9], RankCosine [24], LambdaRank [8] or LambdaMART [7].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : "Examples of other algorithms that employ list-wise approach are ListMLE [29], ListNet [9], RankCosine [24], LambdaRank [8] or LambdaMART [7].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 4,
      "context" : "Examples of other algorithms that employ list-wise approach are ListMLE [29], ListNet [9], RankCosine [24], LambdaRank [8] or LambdaMART [7].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "Experimental results of Almuallim and Dietterich [3] demonstrated that standard decision trees, e.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : "This problem is addressed by Langley and Sage in [20] where they proposed tackling the problem of irrelevant features by using oblivious decision trees.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 23,
      "context" : "The constraints on decision rules selection were introduced also by Schlimmer in [26].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "Although our modification uses a basic greedy top-down induction of oblivious trees, there have been several methods of oblivious tree construction proposed (see [25,18,21]).",
      "startOffset" : 162,
      "endOffset" : 172
    }, {
      "referenceID" : 15,
      "context" : "Although our modification uses a basic greedy top-down induction of oblivious trees, there have been several methods of oblivious tree construction proposed (see [25,18,21]).",
      "startOffset" : 162,
      "endOffset" : 172
    }, {
      "referenceID" : 18,
      "context" : "Although our modification uses a basic greedy top-down induction of oblivious trees, there have been several methods of oblivious tree construction proposed (see [25,18,21]).",
      "startOffset" : 162,
      "endOffset" : 172
    }, {
      "referenceID" : 11,
      "context" : "Authors of YetiRank algorithm [14] introduced oblivious trees into LTR task.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 14,
      "context" : "As described in [17], NDCG is applicable to multi-graded relevance scale.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 25,
      "context" : "Following [28] and [22], the gain function G is defined as G(r) = 2 − 1 and the discount function disc is defined as",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : "Following [28] and [22], the gain function G is defined as G(r) = 2 − 1 and the discount function disc is defined as",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "The use of a cut-off constant can be motivated for instance by the paging of search engine results and limited number of initially displayed documents as noted in [6,28].",
      "startOffset" : 163,
      "endOffset" : 169
    }, {
      "referenceID" : 25,
      "context" : "The use of a cut-off constant can be motivated for instance by the paging of search engine results and limited number of initially displayed documents as noted in [6,28].",
      "startOffset" : 163,
      "endOffset" : 169
    }, {
      "referenceID" : 25,
      "context" : "LambdaMART is a LTR algorithm introduced in [28].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "It combines regression trees boosting technique that is utilised in MART algorithm [13] and LambdaRank’s idea of using lambda trick that avoids the non-smoothness problem of ranking performance measures [8].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "It combines regression trees boosting technique that is utilised in MART algorithm [13] and LambdaRank’s idea of using lambda trick that avoids the non-smoothness problem of ranking performance measures [8].",
      "startOffset" : 203,
      "endOffset" : 206
    }, {
      "referenceID" : 24,
      "context" : "lambdas) can be further modified by using Newton’s method (see [27]).",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "For detailed explanation of the Newton’s method step in LambdaMART see [7].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 22,
      "context" : "Although the definition of oblivious decision trees in [25] permits to use different threshold values for the selected feature, our implementation goes even further and all nodes in the given depth use only a single rule (i.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 16,
      "context" : "In [19] Kohavi et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, as the authors of [4,23,26] have noted, the constraint of oblivious decision trees has a positive effect on the effectiveness of feature selection which is a desired property, especially when dealing with datasets containing many uninformative irrelevant features.",
      "startOffset" : 31,
      "endOffset" : 40
    }, {
      "referenceID" : 20,
      "context" : "Furthermore, as the authors of [4,23,26] have noted, the constraint of oblivious decision trees has a positive effect on the effectiveness of feature selection which is a desired property, especially when dealing with datasets containing many uninformative irrelevant features.",
      "startOffset" : 31,
      "endOffset" : 40
    }, {
      "referenceID" : 23,
      "context" : "Furthermore, as the authors of [4,23,26] have noted, the constraint of oblivious decision trees has a positive effect on the effectiveness of feature selection which is a desired property, especially when dealing with datasets containing many uninformative irrelevant features.",
      "startOffset" : 31,
      "endOffset" : 40
    }, {
      "referenceID" : 25,
      "context" : "In this paper, two LambdaMART algorithm variants are compared, one using standard regression trees (as described in [28]) and one using oblivious decision trees instead.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 8,
      "context" : "While RankLib [11] library was used to train standard LambdaMART models, Seznam.",
      "startOffset" : 14,
      "endOffset" : 18
    } ],
    "year" : 2016,
    "abstractText" : "Learning to rank is a machine learning technique broadly used in many areas such as document retrieval, collaborative filtering or question answering. We present experimental results which suggest that the performance of the current state-of-the-art learning to rank algorithm LambdaMART, when used for document retrieval for search engines, can be improved if standard regression trees are replaced by oblivious trees. This paper provides a comparison of both variants and our results demonstrate that the use of oblivious trees can improve the performance by more than 2.2%. Additional experimental analysis of the influence of a number of features and of a size of the training set is also provided and confirms the desirability of properties of oblivious decision trees.",
    "creator" : "LaTeX with hyperref package"
  }
}