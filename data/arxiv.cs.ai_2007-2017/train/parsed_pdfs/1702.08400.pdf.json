{
  "name" : "1702.08400.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Asymmetric Tri-training for Unsupervised Domain Adaptation",
    "authors" : [ "Kuniaki Saito", "Yoshitaka Ushiku", "Tatsuya Harada" ],
    "emails" : [ "<k-saito@mi.t.u-tokyo.ac.jp>,", "<ushiku@mi.t.u-tokyo.ac.jp>,", "<harada@mi.t.u-tokyo.ac.jp>." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n08 40\n0v 3\n[ cs\n.C V\n] 1\n3 M\nay 2\n01 7\nof labeled samples boost the accuracy of many tasks. It is important to apply such models to different domains because collecting many labeled samples in various domains is expensive. In unsupervised domain adaptation, one needs to train a classifier that works well on a target domain when provided with labeled source samples and unlabeled target samples. Although many methods aim to match the distributions of source and target samples, simply matching the distribution cannot ensure accuracy on the target domain. To learn discriminative representations for the target domain, we assume that artificially labeling target samples can result in a good representation. Tri-training leverages three classifiers equally to give pseudo-labels to unlabeled samples, but the method does not assume labeling samples generated from a different domain. In this paper, we propose an asymmetric tri-training method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train neural networks as if they are true labels. In our work, we use three networks asymmetrically. By asymmetric, we mean that two networks are used to label unlabeled target samples and one network is trained by the samples to obtain targetdiscriminative representations. We evaluate our method on digit recognition and sentiment analysis datasets. Our proposed method achieves state-of-the-art performance on the benchmark digit recognition datasets of domain adaptation."
    }, {
      "heading" : "1. Introduction",
      "text" : "With the development of deep neural networks including deep convolutional neural networks (CNN)\n1The University of Tokyo, Tokyo, Japan. Correspondence to: Kuniaki Saito <k-saito@mi.t.u-tokyo.ac.jp>, Yoshitaka Ushiku <ushiku@mi.t.u-tokyo.ac.jp>, Tatsuya Harada <harada@mi.t.u-tokyo.ac.jp>.\n(Krizhevsky et al., 2012), the recognition abilities of images and languages have improved dramatically. Training deep-layered networks with a large number of labeled samples enables us to correctly categorize samples in diverse domains. In addition, the transfer learning of CNN is utilized in many studies. For object detection or segmentation, we can transfer the knowledge of a CNN trained with a large-scale dataset by fine-tuning it on a relatively small dataset (Girshick et al., 2014; Long et al., 2015a). Moreover, features from a CNN trained on ImageNet (Deng et al., 2009) are useful for multimodal learning tasks including image captioning (Vinyals et al., 2015) and visual question answering (Antol et al., 2015).\nOne of the problems of neural networks is that although they perform well on the samples generated from the same distribution as the training samples, they may find it difficult to correctly recognize samples from different distributions at the test time. One example is images collected from the Internet, which may come in abundance and be fully labeled. They have a distribution different from the images taken from a camera. Thus, a classifier that performs well on various domains is important for practical use. To realize this, it is necessary to learn domain-invariantly discriminative representations. However, acquiring such representations is not easy because it is often difficult to collect a large number of labeled samples and because samples from different domains have domain-specific characteristics.\nIn unsupervised domain adaptation, we try to train a classifier that works well on a target domain on the condition that we are provided labeled source samples and unlabeled target samples during training. Most of the previous deep domain adaptation methods have been proposed mainly under the assumption that the adaptation can be realized by matching the distribution of features from different domains. These methods aimed to obtain domain-invariant features by minimizing the divergence between domains as well as a category loss on the source domain (Ganin & Lempitsky, 2014; Long et al., 2015b; 2016). However, as shown in (Ben-David et al., 2010), theoretically, if a classifier that works well on both the source and the target domains does not exist, we cannot expect a discriminative classifier for the target domain. That is, even if the distributions are matched on the nondiscriminative representations, the classifier may not work\nwell on the target domain. Since directly learning discriminative representations for the target domain, in the absence of target labels, is considered very difficult, we propose to assign pseudo-labels to target samples and train targetspecific networks as if they were true labels.\nCo-training and tri-training (Zhou & Li, 2005) leverage multiple classifiers to artificially label unlabeled samples and retrain the classifiers. However, the methods do not assume labeling samples from different domains. Since our goal is to classify unlabeled target samples that have different characteristics from labeled source samples, we propose asymmetric tri-training for unsupervised domain adaptation. By asymmetric, we mean that we assign different roles to three classifiers.\nIn this paper, we propose a novel tri-training method for unsupervised domain adaptation, where we assign pseudolabels to unlabeled samples and train neural networks utilizing the samples. As described in Fig. 1, two networks are used to label unlabeled target samples and the remaining network is trained by the pseudo-labeled target samples. Our method does not need any special implementations. We evaluate our method on the digit classification task, traffic sign classification task and sentiment analysis task using the Amazon Review dataset, and demonstrate state-of-the-art performance in nearly all experiments. In particular, in the adaptation scenario, MNIST→SVHN, our method outperformed other methods by more than 10%."
    }, {
      "heading" : "2. Related Work",
      "text" : "As many methods have been proposed to tackle various tasks in domain adaptation, we present details of the research most closely related to our paper.\nA number of previous methods attempted to realize adaptation by utilizing the measurement of divergence between different domains (Ganin & Lempitsky, 2014; Long et al., 2015b; Li et al., 2016). The methods are based on the theory proposed in (Ben-David et al., 2010), which states that the expected loss for a target domain is bounded by three terms: (i) expected loss for the source domain; (ii) domain divergence between source and target; and (iii) the minimum value of a shared expected loss. The shared expected\nloss means the sum of the loss on the source and target domain. As the third term, which is usually considered to be very low, cannot be evaluated when labeled target samples are absent, most methods try to minimize the first term and the second term. With regards to training deep architectures, the maximum mean discrepancy (MMD) or a loss of domain classifier network is utilized to measure the divergence corresponding to the second term (Gretton et al., 2012; Ganin & Lempitsky, 2014; Long et al., 2015b; 2016; Bousmalis et al., 2016). However, the third term is very important in training CNN, which simultaneously extract representations and recognize them. The third term can easily be large when the representations are not discriminative for the target domain. Therefore, we focus on how to learn target-discriminative representations considering the third term. In (Long et al., 2016) the focus was on the point we have stated and a target-specific classifier was constructed using a residual network structure. Different from their method, we constructed a target-specific network by providing artificially labeled target samples.\nSeveral transductive methods use similarity of features to provide labels for unlabeled samples (Rohrbach et al., 2013; Khamis & Lampert, 2014). For unsupervised domain adaptation, in (Sener et al., 2016), a method was proposed to learn labeling metrics by using the k-nearest neighbors between unlabeled target samples and labeled source samples. In contrast to this method, our method explicitly and simply backpropagates the category loss for target samples based on pseudo-labeled samples. Our approach does not require any special modules.\nMany methods proposed to give pseudo-labels to unlabeled samples by utilizing the predictions of a classifier and retraining it including the pseudo-labeled samples, which is called self-training. The underlying assumption of selftraining is that one’s own high-confidence predictions are correct (Zhu, 2005). As the predictions are mostly correct, utilizing samples with high confidence will further improve the performance of the classifier. Co-training utilizes two classifiers, which have different views on one sample, to provide pseudo-labels (Blum & Mitchell, 1998; Tanha et al., 2011). Then, the unlabeled samples are added to training set if at least one classifier is confident about the predictions. The generalization ability of co-training is theoretically ensured (Balcan et al., 2004; Dasgupta et al., 2001) under some assumptions and applied to various tasks (Wan, 2009; Levin et al., 2003). In (Chen et al., 2011), the idea of co-training was incorporated into domain adaptation. Tri-training can be regarded as the extension of co-training (Zhou & Li, 2005). Similar to co-training, tritraining uses the output of three different classifiers to give pseudo-labels to unlabeled samples. Tri-training does not require partitioning features into different views; instead, tri-training initializes each classifier differently. However,\ntri-training does not assume that the unlabeled samples follow the different distributions from the ones which labeled samples are generated from. Therefore, we develop a tritraining method suitable for domain adaptation by using three classifiers asymmetrically.\nIn (Lee, 2013), the effect of pseudo-labels in a neural network was investigated. They argued that the effect of training a classifier with pseudo-labels is equivalent to entropy regularization, thus leading to a low-density separation between classes. In addition, in our experiment, we observe that target samples are separated in hidden features."
    }, {
      "heading" : "3. Method",
      "text" : "In this section, we provide details of the proposed model for domain adaptation. We aim to construct a targetspecific network by utilizing pseudo-labeled target samples. Simultaneously, we expect two labeling networks to acquire target-discriminative representations and gradually increase accuracy on the target domain.\nWe show our proposed network structure in Fig. 2. Here F denotes the network which outputs shared features among three networks, F1 and F2 classify features generated from F . Their predictions are utilized to give pseudo-labels. The classifier Ft classifies features generated from F , which is a target-specific network. Here F1, F2 learn from source and pseudo-labeled target samples and Ft learns only from pseudo-labeled target samples. The shared network F learns from all gradients from F1, F2, Ft. Without such a shared network, another option for the network architecture we can think of is training three networks separately, but this is inefficient in terms of training and implementation. Furthermore, by building a shared network F , F1 and F2 can also harness the target-discriminative represen-\ntations learned by the feedback from Ft.\nThe set of source samples is defined as { (xi, yi) }ms\ni=1 ∼ S,\nthe unlabeled target set is { (xi) }mt\ni=1 ∼ T , and the pseudo-\nlabeled target set is { (xi, ŷi) }nt\ni=1 ∼ Tl."
    }, {
      "heading" : "3.1. Loss for Multiview Features Network",
      "text" : "In the existing works (Chen et al., 2011) on co-training for domain adaptation, given features are divided into separate parts and considered to be different views.\nAs we aim to label target samples with high accuracy, we expect F1, F2 to classify samples based on different viewpoints. Therefore, we make a constraint for the weight of F1, F2 to make their inputs different to each other. We add the term |W1\nTW2| to the cost function, whereW1,W2 denote fully connected layers’ weights of F1 and F2 which are first applied to the feature F (xi). Each network will learn from different features with this constraint. The objective for learning F1, F2 is defined as\nE(θF , θF1 , θF2) = 1\nn\nn ∑\ni=1\n[\nLy(F1 ◦ F (xi)), yi)\n+ Ly(F2 ◦ (F (xi)), yi) ] + λ|W1 TW2|\n(1)\nwhere Ly denotes the standard softmax cross-entropy loss function. We decided the trade-off parameter λ based on validation split."
    }, {
      "heading" : "3.2. Learning Procedure and Labeling Method",
      "text" : "Pseudo-labeled target samples will provide targetdiscriminative information to the network. However, since they certainly contain false labels, we have to pick up reliable pseudo-labels. Our labeling and learning method is aimed at realizing this.\nThe entire procedure of training the network is shown in Algorithm 1. First, we train the entire network with source training set S. Here F1, F2 are optimized by Eq. (1) and Ft is trained on standard category loss. After training on S, to provide pseudo-labels, we use predictions of F1 and F2, namely y\n1, y2 obtained from xk. When C1, C2 denote the class which has the maximum predicted probability for y1, y2, we assign a pseudo-label to xk if the following two conditions are satisfied. First, we require C1 = C2 to give pseudo-labels, which means two different classifiers agree with the prediction. The second requirement is that the maximizing probability of y1 or y2 exceeds the threshold parameter, which we set as 0.9 or 0.95 in the experiment. We suppose that unless one of two classifiers is confident of the prediction, the prediction is not reliable. If the two requirements are satisfied, ( xk, ŷk = C1 = C2 ) is added to Tl. To prevent the overfitting to pseudo-labels, we resample the candidate for labeling samples in each step. We set the\nAlgorithm 1 iter denotes the iteration of training. The function Labeling means the method of labeling. We assign pseudo-labels to samples when the predictions of F1 and F2 agree and at least one of them is confident of their predictions.\nInput: data S = { (xi, ti) }m\ni=1 , T =\n{ (xj) }n\nj=1\nTl = ∅ for j = 1 to iter do Train F, F1, F2, Ft with mini-batch from training set S end for Nt = Ninit Tl = Labeling(F, F1, F2,T , Nt) L = S ∪ Tl for k steps do\nfor j = 1 to iter do Train F, F1, F2 with mini-batch from training set L Train F, Ft with mini-batch from training set Tl end for Tl = ∅, Nt = k/20 ∗ n Tl = Labeling(F, F1, F2,T , Nt) L = S ∪ Tl\nend for\nnumber of the initial candidatesNinit as 5,000. We gradually increase the number of the candidatesNt = k/20 ∗ n, where n denotes the number of all target samples and k denotes the number of steps, and we set the maximum number of pseudo-labeled candidates as 40,000. After the pseudo-labeled training set Tl is composed, F, F1, F2 are updated by the objective Eq. (1) on the labeled training set L = S ∪ Tl. Then, F, Ft are simply optimized by the category loss for Tl.\nDiscriminative representations will be learned by constructing a target-specific network trained only on target samples. However, if only noisy pseudo-labeled samples are used for training, the network may not learn useful representations. Then, we use both source samples and pseudo-labeled samples for training F, F1, F2 to ensure the accuracy. Also, as the learning proceeds, F will learn target-discriminative representations, resulting in an improvement in accuracy in F1, F2. This cycle will gradually enhance the accuracy in the target domain."
    }, {
      "heading" : "3.3. Batch Normalization for Domain Adaptation",
      "text" : "Batch normalization (BN) (Ioffe & Szegedy, 2015), which whitens the output of the hidden layer in a CNN, is an effective technique to accelerate training speed and enhance the accuracy of the model. In addition, in domain adaptation, whitening the hidden layer’s output is effective for improving the performance, which make the distribution in\ndifferent domains similar (Sun et al., 2016; Li et al., 2016).\nInput samples of F1, F2 include both pseudo-labeled target samples and source samples. Introducing BN will be useful for matching the distribution and improves the performance. We add the BN layer in the last layer in F ."
    }, {
      "heading" : "4. Analysis",
      "text" : "In this section, we provide a theoretical analysis to our approach. First, we provide an insight into existing theory, then we introduce a simple expansion of the theory related to our method.\nIn (Ben-David et al., 2010), an equation was introduced showing that the upper bound of the expected error in the target domain depends on three terms, which include the divergence between different domains and the error of an ideal joint hypothesis. The divergence between source and target domain,H∆H-distance, is defined as follows:\ndH∆H(S, T )\n= 2 sup (h,h′)∈H2\n∣ ∣ ∣ E\nx∼S [h(x) 6= h′(x)] − E x∼T [h(x) 6= h′(x)]\n∣ ∣ ∣\nThis distance is frequently used to measure the adaptability between different domains.\nThe ideal joint hypothesis is defined as h∗ = arg min\nh∈H\n(\nRS(h ∗) + RT (h ∗) ) , and its corresponding error\nis C = RS(h ∗) + RT (h ∗), where R denotes the expected error on each hypothesis. The theorem is as follows.\nTheorem 1. (Ben-David et al., 2010) LetH be the hypothesis class. Given two different domains S, T , we have\n∀h ∈ H,RT (h) ≤ RS(h) + 1 2dH∆H(S, T ) + C\nThis theorem means that the expected error on the target domain is upper bounded by three terms, the expected error on the source domain, the domain divergence measured by the disagreement of the hypothesis, and the error of the ideal joint hypothesis. In the existing work (Ganin & Lempitsky, 2014; Long et al., 2015b),C was disregarded because it was considered to be negligibly small. If we are provided with fixed features, we do not need to consider the term because the term is also fixed. However, if we assume that xs ∼ S, xt ∼ T are obtained from the last fully connected layer of deep models, we note that C is determined by the output of the layer, and further note the necessity of considering this term.\nWe consider the pseudo-labeled target samples set Tl = {\n(xi, ŷi) }mt\ni=1 given false labels at the ratio of ρ. The shared\nerror of h∗ on S, Tl is denoted as C′. Then, the following inequality holds:\n∀h ∈ H,RT (h) ≤ RS(h) + 1 2dH∆H(S, T ) + C\n≤ RS(h) + 1 2dH∆H(S, T ) + C ′ + ρ\nWe show a simple derivation of the inequality in the Supplementary material. In Theorem 1, we cannot measure C in the absence of labeled target samples. We can approximately evaluate and minimize it by using pseudo-labels. Furthermore, when we consider the second term on the right-hand side, our method is expected to reduce this term. This term intuitively denotes the discrepancy between different domains in the disagreement of two classifiers. If we regard certain h and h′ as F1 and F2, respectively, E x∼Sx [h(x) 6= h′(x)] should be very low because training is based on the same labeled samples. Moreover, for the same reason, E x∼Tx [h(x) 6= h′(x)] is expected to be low, although we use the training set Tl instead of genuine labeled target samples. Thus, our method will consider both the second and the third term in Theorem 1."
    }, {
      "heading" : "5. Experiment and Evaluation",
      "text" : "We perform extensive evaluations of our method on image datasets and a sentiment analysis dataset. We evaluate the accuracy of target-specific networks in all experiments.\nVisual Domain Adaptation For visual domain adaptation, we perform our evaluation on the digits datasets and traffic signs datasets. Digits datasets include MNIST (LeCun et al., 1998), MNIST-M (Ganin & Lempitsky, 2014), Street View House Numbers (SVHN) (Netzer et al., 2011), and Synthetic Digits (SYN DIGITS) (Ganin & Lempitsky, 2014). We further evaluate our method on traffic sign datasets including Synthetic Traffic Signs (SYN SIGNS) (Moiseev et al., 2013) and German Traffic Signs Recognition Benchmark (Stallkamp et al., 2011) (GTSRB). In total, five adaptation scenarios are evaluated in this experiment. As the datasets used for evaluation are varied in previous works, we extensively evaluate our method on the five scenarios.\nWe do not evaluate our method on Office (Saenko et al., 2010), which is the most commonly used dataset for visual domain adaptation. As pointed out by (Bousmalis et al., 2016), some labels in that dataset are noisy and some images contain other classes’ objects. Furthermore,many previous studies have evaluated the fine-tuning of pretrained networks using ImageNet. This protocol assumes the existence of another source domain. In our work, we want to evaluate the situation where we have access to only one source domain and one target domain.\nAdaptation in Amazon Reviews To investigate the behavior on language datasets, we also evaluated our method on the Amazon Reviews dataset (Blitzer et al., 2006) with the same preprocessing as used by (Chen et al., 2011; Ganin et al., 2016). The dataset contains reviews on four\ntypes of products: books, DVDs, electronics, and kitchen appliances. We evaluate our method on 12 domain adaptation scenarios. The results are shown in Table 1.\nBaseline Methods We compare our method with five methods for unsupervised domain adaptation including state-of-the art methods in visual domain adaptation; Maximum Mean Discrepancy (MMD) (Long et al., 2015b), Domain Adversarial Neural Network (DANN) (Ganin & Lempitsky, 2014), Deep Reconstruction Classification Network (DRCN) (Ghifary et al., 2016), Domain Separation Networks (DSN) (Bousmalis et al., 2016), and k-Nearest Neighbor based adaptation (kNN-Ad) (Sener et al., 2016). We cite the results of MMD from (Bousmalis et al., 2016). In addition, we compare our method with CNN trained only on source samples. We compare our method with Variational Fair AutoEncoder (VFAE) (Louizos et al., 2015) and DANN (Ganin et al., 2016) in the Amazon Reviews experiment."
    }, {
      "heading" : "5.1. Implementation Detail",
      "text" : "In experiments on image datasets, we employ the architecture of CNN used in (Ganin & Lempitsky, 2014). For a fair comparison, we separate the network at the hidden layer from which (Ganin & Lempitsky, 2014) constructed discriminator networks. Therefore, when considering one classifier, for example, F1 ◦ F , the architecture is identical to previous work. We also follow (Ganin & Lempitsky, 2014) in the other protocols. We set the threshold value for the labeling method as 0.95 in MNIST→SVHN. In other scenarios, we set it as 0.9. We use MomentumSGD for optimization and set the momentum as 0.9, while the learning rate is determined on validation splits and uses either [0.01, 0.05]. λ is set 0.01 in all scenarios. In our Supplementary material, we provide details of the network architecture and hyper-parameters.\nFor experiments on the Amazon Reviews dataset, we use a similar architecture to that used in (Ganin et al., 2016): with sigmoid activated, one dense hidden layer with 50 hidden units, and softmax output. We extend the architecture to our method similarly in the architecture of CNN. λ is set as 0.001 based on the validation. Since the input is sparse, we use Adagrad (Duchi et al., 2011) for optimization. We repeat this evaluation 10 times and report mean accuracy."
    }, {
      "heading" : "5.2. Experimental Result",
      "text" : "In Tables 1 and 3, we show the main results of the experiments. When training only on source samples, the effect of the BN is not clear as in Tables 1. However, in all image recognition experiments, the effect of BN in our method is clear; at the same time, the effect of our method is also clear when we do not use BN in the network architecture. The effect of the weight constraint is obvious in MNIST→SVHN.\nMNIST→MNIST-M First, we evaluate the adaptation scenario between the hand-written digits dataset MNIST and its transformed dataset MNIST-M. MNIST-M is composed by merging the clip of the background from BSDS500 datasets (Arbelaez et al., 2011). A patch is randomly taken from the images in BSDS500, merged to MNIST digits. Even with this simple domain shift, the adaptation performance of CNN is much worse than the case where it was trained on target samples. From 59,001 target training samples, we randomly select 1,000 labeled target samples as a validation split and tuned hyper-parameters.\nOur method outperforms the other existing method by about 7%. Visualization of features in the last pooling layer is shown in Fig. 3(a)(b). We can observe that the red target samples are more dispersed when adaptation is achieved. We show the comparison of the accuracy between the ac-\ntual labeling accuracy on target samples during training and the test accuracy in Fig. 4. The test accuracy is very low at first, but as the steps increase, the accuracy becomes closer to that of the labeling accuracy. In this adaptation, we can clearly see that the actual labeling accuracy gradually improves with the accuracy of the network.\nSVHN↔MNIST We increase the gap between distributions in this experiment. We evaluate adaptation between SVHN (Netzer et al., 2011) and MNIST in a ten-class classification problem. SVHN and MNIST have distinct appearance, thus this adaptation is a challenging scenario especially in MNIST→SVHN. SVHN is colored and some images contain multiple digits. Therefore, a classifier trained on SVHN is expected to perform well on MNIST, but the reverse is not true. MNIST does not include any samples containing multiple digits and most samples are\n0 5 10 15 20 25 30\nNumber of steps\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nA c c u ra\nc y\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nN u m\nb e r\no f s a m\np le\ns\n!10 4\nAccuracy of labeling method Accuracy of learned network Number of labeled samples\n(a) MNIST→MNIST-M\n0 5 10 15 20 25 30\nNumber of steps\n0.7\n0.75\n0.8\n0.85\n0.9\nA c c u ra\nc y\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nN u m\nb e r\no f s a m\np le\ns\n!10 4\nAccuracy of labeling method Accuracy of learned network Number of labeled samples\n(b) SVHN→MNIST\n0 5 10 15 20 25 30\nNumber of steps\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\nA c c u ra\nc y\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nN u m\nb e r\no f s a m\np le\ns\n!10 4\nAccuracy of labeling method Accuracy of learned network Number of labeled samples\n(c) MNIST→SVHN\n0 5 10 15 20 25 30\nNumber of steps\n0.84\n0.86\n0.88\n0.9\n0.92\n0.94\nA c c u\nra c y\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nN u\nm b\ne r\no f\ns a\nm p\nle s\n!10 4\nAccuracy of labeling method Accuracy of learned network Number of labeled samples\n(d) SYNDIGITS→SVHN\n0 5 10 15 20 25 30\nNumber of steps\n0.75\n0.8\n0.85\n0.9\n0.95 1 A c c u ra c y\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nN u\nm b\ne r\no f\ns a\nm p\nle s\n!10 4\nAccuracy of labeling method Accuracy of learned network Number of labeled samples\n(e) SYNSIGNS→GTSRB\n0 5 10 15 20 25 30\nNumber of steps\n0.7\n0.75\n0.8\n0.85\nA c c u\nra c y\nAccuracy of Target Network Accuracy of Network1 Accuracy of Network2\n(f) Comparision of accuracy of three network on SVHN→MNIST (g) A-distance in MNIST→MNISTM\nFigure 4. (a) ∼ (e): Comparison of the actual accuracy of pseudo-labels and learned network accuracy during training. The blue curve is the pseudo-label accuracy and the red curve is the learned network accuracy. Note that the labeling accuracy is computed using (the number of correctly labeled samples)/(the number of labeled samples). The green curve is the number of labeled target samples in each step. (f): Comparison of the accuracy of three networks in our model. Three networks almost simultaneously improve accuracy. (g): Comparison of the A-distance of different methods. Our model slightly reduced the divergence of the domain compared with source-only trained CNN.\ncentered in images, thus adaptation fromMNIST to SVHN is rather difficult. In both settings, we use 1,000 labeled target samples to find the optimal hyperparameters.\nWe evaluate our method on both adaptation scenarios and achieved state-of-the-art performance on both datasets. In particular, for the adaptation MNIST→SVHN, we outperformed other methods by more than 10%. In Fig. 3(c)(d), we visualize the representations in MNIST→SVHN. Although the distributions seem to be separated between domains, the red SVHN samples become more discriminative using our method compared with non-adapted embedding. We also show the comparison between actual labeling method accuracy and testing accuracy in Fig. 4(b)(c). In this figure, we can see that the labeling accuracy rapidly drops in the initial adaptation stage. On the other hand, testing accuracy continues to improve, and finally exceeds the labeling accuracy. There are two questions about this interesting phenomenon. The first question is why does the labeling method continue to decrease despite the increase in the test accuracy? Target samples given pseudo-labels always include mistakenly labeled samples whereas those given no labels are ignored in our method. Therefore, the error will be reinforced in the target samples that are included in training set. The second question is why does the test accuracy continue to increase despite the lower labeling accuracy? The assumed reasons are that the network already acquires target discriminative representations in this\nphase and they can improve the accuracy using source samples and correctly labeled target samples.\nIn Fig. 4(f), we also show the comparison of accuracy of the three networks F1, F2, Ft in SVHN→MNIST. The accuracy of three networks is nearly the same in every step. The same thing is observed in other scenarios. From this result, we can state that the target-discriminative representations are shared in all three networks.\nSYN DIGITS→SVHN In this experiment, we aimed to address a common adaptation scenario from synthetic images to real images. The datasets of synthetic numbers (Ganin & Lempitsky, 2014) consist of 500,000 images generated from Windows fonts by varying the text, positioning, orientation, background and stroke colors, and the amount of blur. We use 479,400 source samples and 73,257 target samples for training, and 26,032 target samples for testing. We use 1,000 SVHN samples as a validation set.\nOur method also outperforms other methods in this experiment. In this experiment, the effect of BN is not clear compared with other scenarios. The domain gap is considered small in this scenario as the performance of the source-only classifier shows. In Fig. 4(d), although the labeling accuracy is dropping, the accuracy of the learned network’s prediction is improving as in MNIST↔SVHN.\nSYN SIGNS→GTSRB This setting is similar to the pre-\nvious setting, adaptation from synthetic images to real images, but we have a larger number of classes, namely 43 classes instead of 10. We use the SYN SIGNS dataset (Ganin & Lempitsky, 2014) for the source dataset and the GTSRB dataset (Stallkamp et al., 2011) for the target dataset, which consist of real traffic sign images. We select randomly 31,367 samples for target training samples and evaluate accuracy on the rest of the samples. A total of 3,000 labeled target samples are used for validation.\nIn this scenario, our method outperforms other methods. This result shows that our method is effective for the adaptation from synthesized images to real images, which have diverse classes. In Fig. 4(e), the same tendency as in MNIST↔SVHN is observed in this adaptation scenario.\nGradient Stop Experiment We evaluate the effect of the target-specific network in our method. We stop the gradient from upper layer networks F1, F2, and Ft to examine the effect of Ft. Table 2 shows three scenarios including the case where we stop the gradient from F1, F2, and Ft. In all scenarios, when we backward all gradients fromF1, F2, Ft, we obtain clear performance improvements.\nIn the experimentMNIST→MNIST-M,we can assume that only the backpropagation fromF1, F2 cannot construct discriminative representations for target samples and confirm the effect of Ft. For the adaptation MNIST→SVHN, the best performance is realized when F receives all gradients from upper networks. Backwarding all gradients will ensure both target-specific discriminative representations in difficult adaptations. In SYN SIGNS→GTSRB, backwarding only from Ft produces the worst performance because these domains are similar and noisy pseudo-labeled target samples worsen the performance.\nA-distance From the theoretical results in (Ben-David et al., 2010), A-distance is usually used as a measure of domain discrepancy. The way of estimating empirical A-distance is simple, in which we train a classifier to classify a domain from each domains’ feature. Then, the approximate distance is calculated\nas d̂A = 2(1 − 2ǫ), where ǫ is the generalization error of the classifier. In Fig. 4(g), we show the A-distance calculated from each CNN features. We used linear SVM to calculate the distance. From this graph, we can see that our method certainly reduces the A-distance compared with the CNN trained on only source samples. In addition, when comparing DANN and our method, although DANN reduces A-distance much more than our method, our method shows superior performance. This indicates that minimizing the domain discrepancy is not necessarily an appropriate way to achieve better performance.\nAmazon Reviews Reviews are encoded in 5,000 dimensional vectors of bag-of-words unigrams and bigrams with binary labels. Negative labels are attached to the samples if they are ranked with 1–3 stars. Positive labels are attached if they are ranked with 4 or 5 stars. We have 2,000 labeled source samples and 2,000 unlabeled target samples for training, and between 3,000 and 6,000 samples for testing. We use 200 of labeled target samples for validation.\nFrom the results in Table 3, our method performs better than VFAE (Louizos et al., 2015) and DANN (Ganin et al., 2016) in nine settings out of twelve. Our method is effective in learning a shallow network on different domains."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this paper, we have proposed a novel asymmetric tri-training method for unsupervised domain adaptation, which is simply implemented. We aimed to learn discriminative representations by utilizing pseudo-labels assigned to unlabeled target samples. We utilized three classifiers, two networks assign pseudo-labels to unlabeled target samples and the remaining network learns from them.\nWe evaluated our method both on domain adaptation on a visual recognition task and a sentiment analysis task, outperforming other methods. In particular, our method outperformed the other methods by more than 10% in the MNIST→SVHN adaptation task."
    }, {
      "heading" : "7. Acknowledgement",
      "text" : "This work was funded by ImPACT Program of Council for Science, Technology and Innovation (Cabinet Office, Government of Japan) and supported by CREST, JST."
    } ],
    "references" : [ {
      "title" : "Vqa: Visual question answering",
      "author" : [ "Antol", "Stanislaw", "Agrawal", "Aishwarya", "Lu", "Jiasen", "Mitchell", "Margaret", "Batra", "Dhruv", "C. Lawrence Zitnick", "Parikh", "Devi" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Antol et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Contour detection and hierarchical image segmentation",
      "author" : [ "Arbelaez", "Pablo", "Maire", "Michael", "Fowlkes", "Charless", "Malik", "Jitendra" ],
      "venue" : null,
      "citeRegEx" : "Arbelaez et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Arbelaez et al\\.",
      "year" : 2011
    }, {
      "title" : "Cotraining and expansion: Towards bridging theory and practice",
      "author" : [ "Balcan", "Maria-Florina", "Blum", "Avrim", "Yang", "Ke" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2004
    }, {
      "title" : "A theory of learning from different domains",
      "author" : [ "Ben-David", "Shai", "Blitzer", "John", "Crammer", "Koby", "Kulesza", "Alex", "Pereira", "Fernando", "Vaughan", "Jennifer Wortman" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Ben.David et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 2010
    }, {
      "title" : "Domain adaptation with structural correspondence learning",
      "author" : [ "Blitzer", "John", "McDonald", "Ryan", "Pereira", "Fernando" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Blitzer et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2006
    }, {
      "title" : "Combining labeled and unlabeled data with co-training",
      "author" : [ "Blum", "Avrim", "Mitchell", "Tom" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Blum et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Blum et al\\.",
      "year" : 1998
    }, {
      "title" : "Domain separation networks",
      "author" : [ "Bousmalis", "Konstantinos", "Trigeorgis", "George", "Silberman", "Nathan", "Krishnan", "Dilip", "Erhan", "Dumitru" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Bousmalis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bousmalis et al\\.",
      "year" : 2016
    }, {
      "title" : "Co-training for domain adaptation",
      "author" : [ "Chen", "Minmin", "Weinberger", "Kilian Q", "Blitzer", "John" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Chen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2011
    }, {
      "title" : "Pac generalization bounds for co-training",
      "author" : [ "Dasgupta", "Sanjoy", "Littman", "Michael L", "McAllester", "David" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Dasgupta et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2001
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Deng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "Duchi", "John", "Hazan", "Elad", "Singer", "Yoram" ],
      "venue" : "JMLR, 12(7):2121–2159,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Unsupervised domain adaptation by backpropagation",
      "author" : [ "Ganin", "Yaroslav", "Lempitsky", "Victor" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ganin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ganin et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep reconstructionclassification networks for unsupervised domain adaptation",
      "author" : [ "Ghifary", "Muhammad", "Kleijn", "W Bastiaan", "Zhang", "Mengjie", "Balduzzi", "David", "Li", "Wen" ],
      "venue" : null,
      "citeRegEx" : "Ghifary et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ghifary et al\\.",
      "year" : 2016
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Girshick et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2014
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "Gretton", "Arthur", "Borgwardt", "Karsten M", "Rasch", "Malte J", "Schölkopf", "Bernhard", "Smola", "Alexander" ],
      "venue" : "JMLR, 13(3):723–773,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2012
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Ioffe", "Sergey", "Szegedy", "Christian" ],
      "venue" : null,
      "citeRegEx" : "Ioffe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe et al\\.",
      "year" : 2015
    }, {
      "title" : "Coconut: Co-classification with output space regularization",
      "author" : [ "Khamis", "Sameh", "Lampert", "Christoph H" ],
      "venue" : "In BMVC,",
      "citeRegEx" : "Khamis et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Khamis et al\\.",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "LeCun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
      "author" : [ "Lee", "Dong-Hyun" ],
      "venue" : "In ICML workshop on Challenges in Representation Learning,",
      "citeRegEx" : "Lee and Dong.Hyun.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lee and Dong.Hyun.",
      "year" : 2013
    }, {
      "title" : "Unsupervised improvement of visual detectors using co-training",
      "author" : [ "Levin", "Anat", "Viola", "Paul A", "Freund", "Yoav" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Levin et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Levin et al\\.",
      "year" : 2003
    }, {
      "title" : "Revisiting batch normalization for practical domain adaptation",
      "author" : [ "Li", "Yanghao", "Wang", "Naiyan", "Shi", "Jianping", "Liu", "Jiaying", "Hou", "Xiaodi" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Long et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning transferable features with deep adaptation networks",
      "author" : [ "Long", "Mingsheng", "Cao", "Yue", "Wang", "Jianmin", "Jordan", "Michael I" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Long et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised domain adaptation with residual transfer networks",
      "author" : [ "Long", "Mingsheng", "Zhu", "Han", "Wang", "Jianmin", "Jordan", "Michael I" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Long et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2016
    }, {
      "title" : "The variational fair autoencoder",
      "author" : [ "Louizos", "Christos", "Swersky", "Kevin", "Li", "Yujia", "Welling", "Max", "Zemel", "Richard" ],
      "venue" : null,
      "citeRegEx" : "Louizos et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Louizos et al\\.",
      "year" : 2015
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Maaten", "Laurens van der", "Hinton", "Geoffrey" ],
      "venue" : "JMLR, 9(11):2579–2605,",
      "citeRegEx" : "Maaten et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Maaten et al\\.",
      "year" : 2008
    }, {
      "title" : "Evaluation of traffic sign recognition methods trained on synthetically generated data",
      "author" : [ "Moiseev", "Boris", "Konev", "Artem", "Chigorin", "Alexander", "Konushin", "Anton" ],
      "venue" : "ACIVS,",
      "citeRegEx" : "Moiseev et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Moiseev et al\\.",
      "year" : 2013
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y" ],
      "venue" : "In NIPS workshop on deep learning and unsupervised feature learning,",
      "citeRegEx" : "Netzer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Netzer et al\\.",
      "year" : 2011
    }, {
      "title" : "Transfer learning in a transductive setting",
      "author" : [ "Rohrbach", "Marcus", "Ebert", "Sandra", "Schiele", "Bernt" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Rohrbach et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Rohrbach et al\\.",
      "year" : 2013
    }, {
      "title" : "Adapting visual category models to new domains",
      "author" : [ "Saenko", "Kate", "Kulis", "Brian", "Fritz", "Mario", "Darrell", "Trevor" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Saenko et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Saenko et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning transferrable representations for unsupervised domain adaptation",
      "author" : [ "Sener", "Ozan", "Song", "Hyun Oh", "Saxena", "Ashutosh", "Savarese", "Silvio" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sener et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sener et al\\.",
      "year" : 2016
    }, {
      "title" : "The german traffic sign recognition benchmark: a multi-class classification competition",
      "author" : [ "Stallkamp", "Johannes", "Schlipsing", "Marc", "Salmen", "Jan", "Igel", "Christian" ],
      "venue" : "In IJCNN,",
      "citeRegEx" : "Stallkamp et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Stallkamp et al\\.",
      "year" : 2011
    }, {
      "title" : "Return of frustratingly easy domain adaptation",
      "author" : [ "Sun", "Baochen", "Feng", "Jiashi", "Saenko", "Kate" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Sun et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2016
    }, {
      "title" : "Ensemble based co-training",
      "author" : [ "Tanha", "Jafar", "van Someren", "Maarten", "Afsarmanesh", "Hamideh" ],
      "venue" : "BNAIC,",
      "citeRegEx" : "Tanha et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tanha et al\\.",
      "year" : 2011
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Co-training for cross-lingual sentiment classification",
      "author" : [ "Wan", "Xiaojun" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Wan and Xiaojun.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wan and Xiaojun.",
      "year" : 2009
    }, {
      "title" : "Tri-training: Exploiting unlabeled data using three",
      "author" : [ "Zhou", "Zhi-Hua", "Li", "Ming" ],
      "venue" : "classifiers. TKDE,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2005
    }, {
      "title" : "Semi-supervised learning literature survey",
      "author" : [ "Zhu", "Xiaojin" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Zhu and Xiaojin.,? \\Q2005\\E",
      "shortCiteRegEx" : "Zhu and Xiaojin.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "(Krizhevsky et al., 2012), the recognition abilities of images and languages have improved dramatically.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "For object detection or segmentation, we can transfer the knowledge of a CNN trained with a large-scale dataset by fine-tuning it on a relatively small dataset (Girshick et al., 2014; Long et al., 2015a).",
      "startOffset" : 160,
      "endOffset" : 203
    }, {
      "referenceID" : 9,
      "context" : "Moreover, features from a CNN trained on ImageNet (Deng et al., 2009) are useful for multimodal learning tasks including image captioning (Vinyals et al.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 35,
      "context" : ", 2009) are useful for multimodal learning tasks including image captioning (Vinyals et al., 2015) and visual question answering (Antol et al.",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : ", 2015) and visual question answering (Antol et al., 2015).",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "However, as shown in (Ben-David et al., 2010), theoretically, if a classifier that works well on both the source and the target domains does not exist, we cannot expect a discriminative classifier for the target domain.",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 21,
      "context" : "A number of previous methods attempted to realize adaptation by utilizing the measurement of divergence between different domains (Ganin & Lempitsky, 2014; Long et al., 2015b; Li et al., 2016).",
      "startOffset" : 130,
      "endOffset" : 192
    }, {
      "referenceID" : 3,
      "context" : "The methods are based on the theory proposed in (Ben-David et al., 2010), which states that the expected loss for a target domain is bounded by three terms: (i) expected loss for the source domain; (ii) domain divergence between source and target; and (iii) the minimum value of a shared expected loss.",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "With regards to training deep architectures, the maximum mean discrepancy (MMD) or a loss of domain classifier network is utilized to measure the divergence corresponding to the second term (Gretton et al., 2012; Ganin & Lempitsky, 2014; Long et al., 2015b; 2016; Bousmalis et al., 2016).",
      "startOffset" : 190,
      "endOffset" : 287
    }, {
      "referenceID" : 6,
      "context" : "With regards to training deep architectures, the maximum mean discrepancy (MMD) or a loss of domain classifier network is utilized to measure the divergence corresponding to the second term (Gretton et al., 2012; Ganin & Lempitsky, 2014; Long et al., 2015b; 2016; Bousmalis et al., 2016).",
      "startOffset" : 190,
      "endOffset" : 287
    }, {
      "referenceID" : 24,
      "context" : "In (Long et al., 2016) the focus was on the point we have stated and a target-specific classifier was constructed using a residual network structure.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 29,
      "context" : "Several transductive methods use similarity of features to provide labels for unlabeled samples (Rohrbach et al., 2013; Khamis & Lampert, 2014).",
      "startOffset" : 96,
      "endOffset" : 143
    }, {
      "referenceID" : 31,
      "context" : "For unsupervised domain adaptation, in (Sener et al., 2016), a method was proposed to learn labeling metrics by using the k-nearest neighbors between unlabeled target samples and labeled source samples.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 34,
      "context" : "Co-training utilizes two classifiers, which have different views on one sample, to provide pseudo-labels (Blum & Mitchell, 1998; Tanha et al., 2011).",
      "startOffset" : 105,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "The generalization ability of co-training is theoretically ensured (Balcan et al., 2004; Dasgupta et al., 2001) under some assumptions and applied to various tasks (Wan, 2009; Levin et al.",
      "startOffset" : 67,
      "endOffset" : 111
    }, {
      "referenceID" : 8,
      "context" : "The generalization ability of co-training is theoretically ensured (Balcan et al., 2004; Dasgupta et al., 2001) under some assumptions and applied to various tasks (Wan, 2009; Levin et al.",
      "startOffset" : 67,
      "endOffset" : 111
    }, {
      "referenceID" : 20,
      "context" : ", 2001) under some assumptions and applied to various tasks (Wan, 2009; Levin et al., 2003).",
      "startOffset" : 60,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "In (Chen et al., 2011), the idea of co-training was incorporated into domain adaptation.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "In the existing works (Chen et al., 2011) on co-training for domain adaptation, given features are divided into separate parts and considered to be different views.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 33,
      "context" : "In addition, in domain adaptation, whitening the hidden layer’s output is effective for improving the performance, which make the distribution in different domains similar (Sun et al., 2016; Li et al., 2016).",
      "startOffset" : 172,
      "endOffset" : 207
    }, {
      "referenceID" : 21,
      "context" : "In addition, in domain adaptation, whitening the hidden layer’s output is effective for improving the performance, which make the distribution in different domains similar (Sun et al., 2016; Li et al., 2016).",
      "startOffset" : 172,
      "endOffset" : 207
    }, {
      "referenceID" : 3,
      "context" : "In (Ben-David et al., 2010), an equation was introduced showing that the upper bound of the expected error in the target domain depends on three terms, which include the divergence between different domains and the error of an ideal joint hypothesis.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "(Ben-David et al., 2010) LetH be the hypothesis class.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 18,
      "context" : "Digits datasets include MNIST (LeCun et al., 1998), MNIST-M (Ganin & Lempitsky, 2014), Street View House Numbers (SVHN) (Netzer et al.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 28,
      "context" : ", 1998), MNIST-M (Ganin & Lempitsky, 2014), Street View House Numbers (SVHN) (Netzer et al., 2011), and Synthetic Digits (SYN DIGITS) (Ganin & Lempitsky, 2014).",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 27,
      "context" : "We further evaluate our method on traffic sign datasets including Synthetic Traffic Signs (SYN SIGNS) (Moiseev et al., 2013) and German Traffic Signs Recognition Benchmark (Stallkamp et al.",
      "startOffset" : 102,
      "endOffset" : 124
    }, {
      "referenceID" : 32,
      "context" : ", 2013) and German Traffic Signs Recognition Benchmark (Stallkamp et al., 2011) (GTSRB).",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 30,
      "context" : "We do not evaluate our method on Office (Saenko et al., 2010), which is the most commonly used dataset for visual domain adaptation.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 6,
      "context" : "As pointed out by (Bousmalis et al., 2016), some labels in that dataset are noisy and some images contain other classes’ objects.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "Adaptation in Amazon Reviews To investigate the behavior on language datasets, we also evaluated our method on the Amazon Reviews dataset (Blitzer et al., 2006) with the same preprocessing as used by (Chen et al.",
      "startOffset" : 138,
      "endOffset" : 160
    }, {
      "referenceID" : 7,
      "context" : ", 2006) with the same preprocessing as used by (Chen et al., 2011; Ganin et al., 2016).",
      "startOffset" : 47,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : ", 2015b), Domain Adversarial Neural Network (DANN) (Ganin & Lempitsky, 2014), Deep Reconstruction Classification Network (DRCN) (Ghifary et al., 2016), Domain Separation Networks (DSN) (Bousmalis et al.",
      "startOffset" : 128,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : ", 2016), Domain Separation Networks (DSN) (Bousmalis et al., 2016), and k-Nearest Neighbor based adaptation (kNN-Ad) (Sener et al.",
      "startOffset" : 42,
      "endOffset" : 66
    }, {
      "referenceID" : 31,
      "context" : ", 2016), and k-Nearest Neighbor based adaptation (kNN-Ad) (Sener et al., 2016).",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "We cite the results of MMD from (Bousmalis et al., 2016).",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 25,
      "context" : "We compare our method with Variational Fair AutoEncoder (VFAE) (Louizos et al., 2015) and DANN (Ganin et al.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "Since the input is sparse, we use Adagrad (Duchi et al., 2011) for optimization.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "7 DRCN (Ghifary et al., 2016) 82.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "1 DSN (Bousmalis et al., 2016) 83.",
      "startOffset" : 6,
      "endOffset" : 30
    }, {
      "referenceID" : 31,
      "context" : "1 kNN-Ad (Sener et al., 2016) 86.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "In source only results, we show the results reported in (Bousmalis et al., 2016) and (Ghifary et al.",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : ", 2016) and (Ghifary et al., 2016) in parentheses.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "MNIST-M is composed by merging the clip of the background from BSDS500 datasets (Arbelaez et al., 2011).",
      "startOffset" : 80,
      "endOffset" : 103
    }, {
      "referenceID" : 28,
      "context" : "We evaluate adaptation between SVHN (Netzer et al., 2011) and MNIST in a ten-class classification problem.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 32,
      "context" : "We use the SYN SIGNS dataset (Ganin & Lempitsky, 2014) for the source dataset and the GTSRB dataset (Stallkamp et al., 2011) for the target dataset, which consist of real traffic sign images.",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "A-distance From the theoretical results in (Ben-David et al., 2010), A-distance is usually used as a measure of domain discrepancy.",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 25,
      "context" : "The accuracy (%) of the proposed method is shown with the result of VFAE (Louizos et al., 2015) and DANN (Ganin et al.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 25,
      "context" : "From the results in Table 3, our method performs better than VFAE (Louizos et al., 2015) and DANN (Ganin et al.",
      "startOffset" : 66,
      "endOffset" : 88
    } ],
    "year" : 2017,
    "abstractText" : "Deep-layered models trained on a large number of labeled samples boost the accuracy of many tasks. It is important to apply such models to different domains because collecting many labeled samples in various domains is expensive. In unsupervised domain adaptation, one needs to train a classifier that works well on a target domain when provided with labeled source samples and unlabeled target samples. Although many methods aim to match the distributions of source and target samples, simply matching the distribution cannot ensure accuracy on the target domain. To learn discriminative representations for the target domain, we assume that artificially labeling target samples can result in a good representation. Tri-training leverages three classifiers equally to give pseudo-labels to unlabeled samples, but the method does not assume labeling samples generated from a different domain. In this paper, we propose an asymmetric tri-training method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train neural networks as if they are true labels. In our work, we use three networks asymmetrically. By asymmetric, we mean that two networks are used to label unlabeled target samples and one network is trained by the samples to obtain targetdiscriminative representations. We evaluate our method on digit recognition and sentiment analysis datasets. Our proposed method achieves state-of-the-art performance on the benchmark digit recognition datasets of domain adaptation.",
    "creator" : "LaTeX with hyperref package"
  }
}