{
  "name" : "1604.03010.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semi-supervised learning of local structured output predictors",
    "authors" : [ "Xin Du" ],
    "emails" : [ "xindu.njtu@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we study the problem of semi-supervised structured output prediction, which aims to learn predictors for structured outputs, such as sequences, tree nodes, vectors, etc., from a set of data points of both inputoutput pairs and single inputs without outputs. The traditional methods to solve this problem usually learns one single predictor for all the data points, and ignores the variety of the different data points. Different parts of the data set may have different local distributions, and requires different optimal local predictors. To overcome this disadvantage of existing methods, we propose to learn different local predictors for neighborhoods of different data points, and the missing structured outputs simultaneously. In the neighborhood of each data point, we proposed to learn a linear predictor by minimizing both the complexity of the predictor and the upper bound of the structured prediction loss. The minimization is conducted by gradient descent algorithms. Experiments over four benchmark data sets, including DDSM mammography medical images, SUN natural image data set, Cora research paper data set, and Spanish news wire article sentence data set, show the advantages of the proposed method.\nKeywords: Machine learning, Structured output, Semi-supervised learning, Local linear regression, Gradient descent"
    }, {
      "heading" : "1. Introduction",
      "text" : "Machine learning refers to the problem of learning a predictive model to predict a output from a input data point [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\nEmail address: xindu.njtu@gmail.com (Xin Du)\nPreprint submitted to Neurocomputing April 12, 2016\nar X\niv :1\n60 4.\n03 01\n0v 1\n[ cs\n.L G\n] 1\n1 A\npr 2\n11, 12]. The forms of output are various, usually including binary class label and continues response. The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25]. Both of these two problems have many applications, such as computer vision, natural language processing, bioinformatics, and finance. However, in many these applications, the forms of outputs of the prediction may be beyond binary class labels and continues responses. For example, in the part-ofspeech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [26, 27, 28, 29, 30]. In the problem of hierarchical image classification problem, the class labels of images are organized as a tree structure, and the outputs of the prediction problem are the leaves of a tree [31, 32, 33, 34]. In this case, the predictive models designed for binary class labels and continues responses cannot handle these output forms, and new predictive model should be developed. The output forms other than binary labels and continues responses are called structured outputs. The structured outputs include a wide range of types of outputs, such as sequences, vectors, graph nodes, tree leaves, etc. The problem of learning predictive models to predict unknown structured outputs are called as structured output prediction [35, 36, 37, 38, 39]. Given a set of input-output data points, where the outputs are structured, this problem usually learns a predictive model to match the input-output relationship. Most existing methods designed to solve assumes that in the training set, all the input data points have their corresponding outputs. However, in realworld application, many output are not available for the inputs [40, 41, 42, 43]. The training set is composed of two parts. One part is a set of inputoutput pairs, which is called labeled set. The other part is a set of single input data points with missing outputs, and this set is called unlabeled data set. Learning from such a training set is call semi-supervised learning [44, 45, 46]. In this paper, we invest the problem of learning structured output predictors from such a training set. This problem is called semi-supervise structured output prediction."
    }, {
      "heading" : "1.1. Related works",
      "text" : "Our work is a novel semi-supervised structured output prediction method, thus we introduce the related works of this direction. There are a number of existing algorithms for this problem, which are briefly introduced as follows.\n1. Altun et al. [40] proposed the problem of semi-supervised learning with structured outputs. Moreover, a novel discriminative approach was also proposed to use the manifold of input features of both labeled and unlabeled data points. This approach is based on the semi-supervised maximum-margin formulation. It is an inductive algorithm, and it can be easily extended to new-coming test data points.\n2. Brefeld and Scheffer [41] proposed to solve the problem of semi-supervised structured output prediction by learning in the space of input-output space, and using co-training method. This method is based on the assumption that the multiple structured output predictors should be consent with each other. Based on this assumption, the structural support vector machine is extended to the argued input-output space.\n3. Suzuki et al. [42] proposed a hybrid method to solve the problem of semi-supervised structured output learning. This method combines both the generative and discriminative methods. The objective of this method is composed of log-linear forms of both discriminative structured predictor and generative model. The generative model is used to incorporate unlabeled data points. The discriminant functions is enhanced by the unlabeled data points provided by the generative model.\n4. Jiang et al. [43] proposed to regularize the structured outputs by the manifold constructed from the input space directly. This method constructs a nearest neighbor graph from the input features, and use it to represent the manifold. Then the manifold is used to regularize the learning of the missing outputs of the unlabeled data points. The outputs and the predictor are learned simultaneously, and they regularize each other in the learning process.\nOur work approximate the upper bound of the structured loss, and is inspired by the lower bound approximation of the structure learning of the Bayesian network [16, 4]. Thus we also discuss the works of bound approximation technologies of [16, 4].\n1. Fan et al. [16] proposed to tighten the upper and lower bounds of the breadth-first branch and bound algorithm for the learning of Bayesian network structures. The informed variable groupings is used to create the pattern databases to tighten the lower bounds, while the anytime learning algorithm is used to tighten the upper bound. These strategies show good performance in the learning process of the Bayesian\nnetwork structures. The work of [16] is a contribution of major significance to the bound approximation community, and our upper bound approximation method is also based on these strategies.\n2. Fan et al. [4] further proposed to improve the lower bound function of static k-cycle conflict heuristic for the learning of Bayesian network structures. This work is used to guild the search of the most promising search spaces. It use a partition of the random variables of a data set, and the further research is based on the importance of the partition. A new partition method was proposed, and it uses the information extracted from the potentially optimal parent sets."
    }, {
      "heading" : "1.2. Our contributions",
      "text" : "All the mentioned semi-supervised structured output prediction methods learns one single predictor for the entire data set. However, we observe that a training set, the local distributions of neighborhoods play important roles in the problem of modeling of both input and structured outputs. It is extremely important to respect the local distributions when the structured output predictor are learned. This is even more important for learning from semi-supervised data sets. This is because for this type of data set, only a few data points have available structured outputs, and the structured outputs of all other data points are missing. To learn the missing structured outputs, we need to explore the connections between different data points, so that we may propagate the structured outputs from the labeled set to the unlabeled set. It has been shown that using local connections is an effective way to model the connections among different data points [43]. To explore the local distributions, one option is to construct a nearest neighbor graph, and use it to regularize the learning of the predictors. More specifically, with the nearest neighbor graph, we hope that the neighboring data points can obtain similar structured outputs from the predictor [47, 43]. However, one single predictor is usually not enough to characterize multiple local distributions, thus even we use the neighborhood graph to regularize the learning the predictor, it is still not guaranteed that the local distributions are sufficiently modeled with regard to the structured output prediction problem. This is an even more serious problem when it is applied to a semi-supervised data set. With such a data set, only a few data points have corresponding structured outputs, while most of the data points do not. The learned predictor can easily fits to the labeled data points.\nTo solve this problem, we propose to learn multiple local linear structured output predictor for different neighborhoods to model the local distributions, instead of learning one single predictor for the entire data [48, 49]. Moreover, we also propose to learn the missing structured outputs for a semi-supervised data set simultaneously. For each data point, we propose to present the local distribution around this data point by its k nearest neighborhood, and model it by learning a local linear structured output predictor. To learn the parameters of this local predictor, we propose to minimize a upper bound of the structured losses of the data points in this neighborhood, and the squared `2 norm of the predictor parameter vector. In this process, the predicted structured outputs are compared to the learned structured outputs. The learning of the structured outputs are simultaneously by the true structured outputs of the labeled data points, and the local predictors. Some data points are shared by different neighborhoods, and they play the role of bridging different local distributions to learn a complete manifold. To solve the problem, we develop an iterative algorithm, by using gradient descent method."
    }, {
      "heading" : "1.3. Paper organization",
      "text" : "The rest parts of this paper are organized as follows. In section 2, we model the learning problem, and present the optimization methods for the problem. In section 3, the iterative algorithm for the learning process, and the algorithm for the test process are both introduced. In section 4, the experiments in four benchmark data sets are presented, including DDSM Mammography medical image data set, SUN natural image data set, Cora research paper data set, and Spanish news wire article sentence data set. In section 5, the paper is concluded. In section 6, we discuss the future works."
    }, {
      "heading" : "2. Problem modeling and optimization",
      "text" : ""
    }, {
      "heading" : "2.1. Problem modeling",
      "text" : "Suppose we have a training set of n data points, X = L ∪ U , which is composed of a labeled subset, L, and a unlabeled subset, U . L contains l data points of input-output pairs, L = {(xi, yi)}li=1, where xi ∈ Rd is the d-th dimensional feature vector input of the i-th data point, yi ∈ Y is the true structured output of the i-th data point, Y is the space of structured outputs. U contains u = n − l data points of inputs without outputs, U = {xi}ni=l+1. The structured output prediction problem is to learn predictors to predict the structured outputs from the inputs from the training set. We\nproposed to learn a local predictor for the neighborhood of each data points, instead of learning one single predictor for all the data points. We present the neighborhood of the i-th data point as the set of its k nearest neighbors,\nNi = {xj : xj is the l − th nearest neighbor of xi, and l ≤ k}. (1)\nGiven a data point of the i-th neighborhood, xj ∈ Ni, to predict is structured output, we match it against all the possible structured outputs. Given a candidate structured output, y ∈ Y , and an input feature vector, xj, we use a joint representation to match them. The joint representation of y and xj is denoted as Φ(xj, y) ∈ Rm, where m is the dimension of the joint representation. We design a local linear predictor for the joint representations of the data points of the neighborhood of each data point, Ni, and use it to predict the structured outputs,\ny∗j = arg max y∈Y w>i Φ(xj, y), j : xj ∈ Ni, (2)\nwhere wi is the parameter of the local predictor of the i-th neighborhood, Ni, and y∗j is the predicted structured output of a data point in Ni, xj. Apparently, this local predictor match the joint representations of a input vector and each candidate output by a linear function, and return the output candidate which gives the maximum matching response.\nBecause the training set is composed of labeled and unlabeled subsets, the outputs of the data points in U are missing, we also proposed to learn the complete outputs simultaneously. The learned output set is denoted as {yi}|ni=1, where yi is the outputs of the i-th data point. To guarantee the learned outputs are consistent with the given true outputs, we impose that the outputs of the data points in L is equal to the true outputs,\nyi = yi, i : (xi, yi) ∈ L. (3)\nWe propose to learn the parameters of the local predictors, wi|ni=1, and the complete outputs, yi|ni=1 simultaneously. To this end, we decompose the learning problem to each neighborhood. In the i-th neighborhood, we learn the local predictor parameter, wi, and the outputs of the data points in Ni, yj|j:xj∈Ni . We use a structured loss function to measure the loss of predicting the j-th structured output as y∗j , while the corresponding output is yj. The loss function is denoted as ∆(yj, y ∗ j ). Moreover, we also propose to keep the\npredictor as simple as possible, by minimizing the squared `2 norm of the local predictor, wi. By minimizing this loss function with regard to wi and yj over the i-th neighborhood, and minimizing squared `2 norm of wi, we obtain the optimal local predictor parameter,\nmin wi,yj |j:xj∈Ni 1k ∑ j:xj∈Ni ∆(yj, y ∗ j ) + C 2 ‖wi‖22  , s.t. yj = yj, j : (xj, yi) ∈ L,\n(4)\nwhere C is a tradeoff parameter to balance the weights of the first and second terms of the objective.\nBecause the structured loss function is usually complex and difficult to optimize directly, we seek its upper bound and minimize it instead. According to (2), we have,\nw>i Φ(xj, y ∗ j ) ≥ w>i Φ(xj, yj), j : xj ∈ Ni, ⇒ w>i ( Φ(xj, y ∗ j )− Φ(xj, yj) ) ≥ 0,\n⇒ w>i ( Φ(xj, y ∗ j )− Φ(xj, yj) ) + ∆(yj, y ∗ j ) ≥ ∆(yj, y∗j ), ⇒ max y′j∈Y [ w>i ( Φ(xj, y ′ j)− Φ(xj, yj) ) + ∆(yj, y ′ j) ] ≥ ∆(yj, y∗j ).\n(5)\nThus the upper bound of ∆(yj, y ∗ j ) can be given as\nmax y′j∈Y\n[ w>i ( Φ(xj, y ′ j)− Φ(xj, yj) ) + ∆(yj, y ′ j) ]\n= [ w>i ( Φ(xj, z ∗ j )− Φ(xj, yj) ) + ∆(yj, z ∗ j ) ] ,\n(6)\nwhere z∗j is defined as\nz∗i,j = arg max y′j∈Y\n[ w>i ( Φ(xj, y ′ j)− Φ(xj, yj) ) + ∆(yj, y ′ j) ] , (7)\nand it is an important parameter to define the upper bound of the loss function. With the upper bound, the minimization problem can be transferred to the following problem,\nmin wi,yj |j:xj∈Ni 1k ∑ j:xj∈Ni [ w>i ( Φ(xj, z ∗ i,j)− Φ(xj, yj) ) + ∆(yj, z ∗ i,j) ] + C 2 ‖wi‖22  s.t. yj = yj, j : (xj, yi) ∈ L.\n(8) To bridge the learning problems of different neighborhoods, we propose\nto combine them into one single problems over the entire data set,\nmin (wi,yi)|ni=1 n∑ i=1 1k ∑ j:xj∈Ni [ w>i ( Φ(xj, z ∗ i,j)− Φ(xj, yj) ) + ∆(yj, z ∗ i,j) ] + C 2 ‖wi‖22  s.t. yi = yi, i : (xi, yi) ∈ L. (9) The motive to combine the local learning problems to one single over all problem is to connect these local problems by the overlapping data points of different neighborhoods. For example, Ni and Ni′ have overlapping data points, and one of them is xj, i.e., xj ∈ Ni, and xj ∈ Ni′ . Then the learning of yi will be a common process of both the local problems of Ni and Ni′ , and it can also regularize the learning of both wi and wi′ ."
    }, {
      "heading" : "2.2. Problem optimization",
      "text" : "To solve the problem in (9), we propose to use an iterative algorithm. In this algorithm, we use an alternate optimization strategy. Each iteration has two steps. In the first step, we fix yi|ni=1 and update wi|ni=1 by gradient descent algorithm. In the first step, we fix the updated wi|ni=1 and update yi|ni=1 one by one.\n2.2.1. Updating wi|ni=1 When the outputs yi|ni=1 are fixing, the problem in (9) is transferred to\nmin wi|ni=1 n∑ i=1 1k ∑ j:xj∈Ni [ w>i ( Φ(xj, z ∗ i,j)− Φ(xj, yj) ) + ∆(yj, z ∗ i,j) ]\n+ C\n2 ‖wi‖22 = g(wi)  , (10)\nwhere g(wi) is the objective of the problem of the i-th neighborhood. To update wi, we use the gradient descent algorithm. However, in the objective, z∗j is also a function of wi, thus it is difficult to obtain the gradient function directly. To solve this problem, instead of seeking the gradient function, we seek the sub-gradient function of g(wi) with regard to wi. We first update each z∗j according to previously updated wi, and then fix it to obtain the sub-gradient function with regard to wi,\n∇g(wi) = 1\nk ∑ j:xj∈Ni [( Φ(xj, z ∗ i,j)− Φ(xj, yj) )] + Cwi. (11)\nWith the sub-gradient function, the updating rule of wi is given as follows,\nwi ← wi − η∇g(wi)\n= wi − η 1 k ∑ j:xj∈Ni [( Φ(xj, z ∗ i,j)− Φ(xj, yj) )] + Cwi  = (1− ηC)wi + η\nk ∑ j:xj∈Ni [( Φ(xj, yj)− Φ(xj, z∗i,j) )] .\n(12)\nWe can see that the updating function of wi is a combination of a weighted wi and a function of the joint representations of the data points in Ni.\n2.2.2. Updating yi|ni=1 When wi|ni=1 are fixed and only yi|ni=1 are considered, we remove the terms irrelevant to yi|ni=1 in problem of (9), and transfer it to the following problem,\nmin yi|ni=1 n∑ i=1 1k ∑ j:xj∈Ni [ ∆(yj, z ∗ i,j)−w>i Φ(xj, yj) ] s.t. yi = yi, i : (xj, yi) ∈ L.\n(13)\nTo solve this problem, we propose to solve the n outputs one by one. When one output yi is considered, other outputs are fixed, yi′|i′=i. When only yi is considered, the problem in (13) is reduced to\nmin yi ∑ i′:xi∈Ni′ { 1 k [ ∆(yi, z ∗ i′,i)−w>i′ Φ(xi, yi) ]} s.t. yi = yi, if (xi, yi) ∈ L.\n(14)\nTo solve this problem, we discuss it in two cases.\n• Case I, (xi, yi) ∈ L: In this case, according to the constraint of (16), we assign yi = yi,\nyi = yi, if (xi, yi) ∈ L. (15)\n• Case II, xi ∈ U : In this case, we obtain the follow optimal output as follows,\nyi = arg min y∈Y ∑ i′:xi∈Ni′ { 1 k [ ∆(y, z∗i′,i)−w>i′ Φ(xi, y) ]} , if xi ∈ U .\n(16) This is a minimization problem, and the objective function is a combination of functions over several neighborhoods which xi belongs to. Each function is composed of two terms, one of them is a loss function between a candidate and a upper bound parameter. The other term is a negative function of the local predictor."
    }, {
      "heading" : "3. Algorithms",
      "text" : ""
    }, {
      "heading" : "3.1. Iterative learning algorithm",
      "text" : "Based on the optimization problem, we develop an iterative algorithm to learn the local structured output predictor and the outputs jointly. In each iteration, we first fix the both the local predictor parameters and the outputs to update the upper bound parameters, then update the local predictor parameters by fixing the outputs and the upper bound parameter, and finally fix local predictor parameters and the upper bound parameters to update the outputs. The iterations are repeated for T times. The developed iterative algorithm is given in Algorithm 1.\n• Algorithm 1. Iterative training algorithm of semi-supervised learning of local structured output predictor.\n• Inputs: Training set, X .\n• Inputs: Maximum iteration number, T .\n• Initialize (wi, yi)|ni=1;\n• For t = 1, · · · , T\n– Update the upper bound parameters\n– For i = 1, · · · , n\n∗ For j : xj ∈ Ni\nz∗i,j ← arg max y′j∈Y\n[ w>i ( Φ(xj, y ′ j)− Φ(xj, yj) ) + ∆(yj, y ′ j) ] ,\n(17)\n– Update the local predictor parameters\n– For i = 1, · · · , n\nwi ← (1− ηC)wi + η\nk ∑ j:xj∈Ni [( Φ(xj, yj)− Φ(xj, z∗i,j) )] . (18)\n– Update the structured outputs\n– For i = 1, · · · , n\n∗ If (xi, yi) ∈ L\nyi ← yi; (19)\n∗ Else\nyi ← arg min y∈Y ∑ i′:xi∈Ni′ { 1 k [ ∆(y, z∗i′,i)−w>i′ Φ(xi, y) ]} ;\n(20)\n• Output: wi|ni=1."
    }, {
      "heading" : "3.2. Algorithm of predicting structured output of test data point",
      "text" : "We have discussed how to learn local predictors from a training set. In this section, we discuss how to use these local predictors for the task of predicting structured output of a new-coming test data point. Suppose the input feature vector of the new-coming data point is x, to predict its output, we first find which neighborhoods it belongs to. To this end, we first find its k nearest neighbors from the training set, and denote the set of its k nearest neighbors as Nx. We assume x are in the neighbors of the data points in Nx, and use their local predictors to predict the structured outputs of x. Given a candidate output, y, we use these local predictors to match it to x to obtain k matching scores. The average matching score is used as the final matching score to match x and y. The candidate output which gives the maximum matching score is choose as the final structured output of x,\ny∗ = arg max y∈Y\n1\nk ∑ i:xi∈Nx w>i Φ(x, y). (21)\nThe prediction algorithm for new-coming data point is given in Algorithm 2.\n• Algorithm 2: Predicting the structured output of a new-coming test data point.\n• Input: x and X ;\n• Find the k-th nearest neighbor set of x from X , Nx;\n• Initialize maximum matching score s∗ = −∞\n• For y ∈ Y\nsy = 1\nk ∑ i:xi∈Nx w>i Φ(x, y); (22)\n– If sy ≥ s∗\ns∗ = sy, y ∗ = y; (23)\n• Output: y∗"
    }, {
      "heading" : "4. Experiments",
      "text" : "In this section, we evaluate the performance of the proposed semi-supervised structured output prediction method. The experiments are conducted on three benchmark data sets. We first compare it to several state-of-the-art semi-supervised structured output prediction method, and then analyze its sensitivity to parameters, k and C, experimentally."
    }, {
      "heading" : "4.1. Benchmark data sets",
      "text" : "We used three benchmark data sets in our experiments, which are discussed as follows.\n• Data set I - DDSM mammography image data set: The last data set is a medical imaging data set which contains Mammography images [50]. This data set contains 2620 pairs of images and they belong to three different classes, normal, cancer, and benign. Thus this is a three class classification problem. Some example images of this data set are given in Fig. 1. To present the class of each image pair, we code it as a three-demesnial binary vector. For the i-th image pair, its structured output is a vector is yi = [yi1, yi2, yi3] >, where\nyi1 = { 1, if the i− th image pair belongs to normal class, 0, otherwise.\nyi2 = { 1, if the i− th image pair belongs to cancer class, 0, otherwise.\nyi3 = { 1, if the i− th image pair belongs to benign class, 0, otherwise. (24)\nTo compare a given input and its corresponding prediction, y, and y∗, we use the 0-1 loss,\n∆(y, y∗) = 1, if y 6= y∗, and 0 otherwise. (25)\nTo construct the feature vector of the i-th image pair, we use the bagof-feature method. The images are split into small image patches, the image patches are quantized to a dictionary, and the quantization histogram is used as the feature vector of the image pair, xi. Moreover,\nthe joint representation of a input vector x and a output vector y is their tensor product,\nΦ(x, y) = x⊗ y. (26)\n• Data set II - SUN natural image data set: The second data set is a image data set [51]. The class labels of the images of this data set is organized as a tree structure. The tree has 15 different leaves. For each class, we randomly select 200 images from the data set to conduct our experiments, thus there are 3,000 images in our data set in total. Some example images of this data set are given in Fig. 2. To extract features from each image, we calculate the HOG features. The structured output of a data point is a leave of the tree. We code it as a vector κ dimension, y = [y1, · · · , yκ]> ∈ {1, 0}κ, where κ is the number of nodes of the tree. The ι-th element of the vector, yι = 1 if the ι-th node is its predecessor or itself, and 0 otherwise. Given a input x and a output y, the joint representation is\nΦ(x, y) = x⊗ y. (27)\nThe loss function between y and y∗, ∆(y, y∗), is defined as the height of their first common ancestor.\n• Data set III - Cora research paper data set: This data set is a set of computer science research papers [52]. It contains 9,947 papers. To represent the papers, we extract two types of features. The first type is the frequencies of words of the papers, and the second type is the outlinks of the citations of the papers. To extract the second type features, we remover the papers which do not have links to other papers, and a data set of 9,555 is used for the experiments. These papers belong to 7 different non-overlapping classes. Thus the structured output of each paper is coded as d 7-dimensional vector. The loss function defined for this structured output is the 0-1 loss. Moreover, the joint representation of a input vector x and a output vector y is also their tensor product.\n• Data set IV - Spanish news wire article sentence data set: This data set is a data set for named entity recognition problem of natural language processing [53]. It contains 300 sentences, and each sentence\nis used as a data point in our experiment. The length of each sentence is 9, and the corresponding output of a sentence, y, is a sequence of labels of non-name and named entities. The joint representation Φ(x, y) of a sentence, x, and a sequence of labels, y, is defined as the histogram of state transition, and a set of emission features. The loss function to compare y and y∗ is defined as a 0-1 loss, i.e., ∆(y, y∗) = 1 if y 6= y∗, and 0 otherwise."
    }, {
      "heading" : "4.2. Experiment setup",
      "text" : "To conduct the experiments, we use the 10-fold cross validation strategy to split the training and test subsets. Given an entire data set, we randomly split it to ten subsets of equal sizes. Each subset is used as the test set, while the remaining nine subsets are combined and used as a training set. The training set are also randomly split to a labeled set and an unlabeled set. The Labeled set contains about 30% of the training data points, and the unlabeled set contains the remaining 70% data points. The proposed algorithm is performed to the training set to learn the local predictors, and then the local predictors are used to predict the structured outputs of the data points of the test set. The average structured loss over the test set is used to evaluate the performance of the proposed algorithm. Given a test set, T with nT test data points, the average structured loss is defined as follows,\nAverage Loss = 1\nnT ∑ i:xi∈T ∆(yi, y ∗ i ) (28)\nwhere yi is the true structured output of the i-th test data point, while y ∗ i is its corresponding predicted structured output."
    }, {
      "heading" : "4.3. Comparison to state-of-the-arts",
      "text" : "We first compare the proposed algorithm to some state-of-the-art semisupervised structured output prediction algorithms, including the algorithms proposed by Altun et al. [40], Brefeld and Scheffer [41], Suzuki et al. [42], and Jiang et al. [43]. All the four competing algorithm learn a single global\npredictor for all the data points, and our algorithm is the only algorithm that explores the local distribution of the data set, and learns the local predictors for different neighborhoods. Our algorithm is named as semi-supervised local structured output prediction algorithm (SSLSOP). The average losses of the compared algorithms over three different data sets are given in Table 1. From the results in Table 1, it is obvious that the proposed algorithm outperforms better than all competing algorithms significantly. For example, over data set I, only SSLSOP archives an average structured loss lower than 0.400. The method of Jiang et al. [43] is the second best method. It is slightly better than the other mothers. This method also try to explore the local structure of the data set, and use it to regularize the learning of the structured output predictors. However, it still learns a global predictor, thus it cannot compete with the proposed method, SSLSOP. The results of the other algorithms are comparable to each other, and they are inferior to the SSLSOP. In summary, the results clearly show that multiple local predictors work better than one single predictor in the problems of structured output prediction."
    }, {
      "heading" : "4.4. Sensitivity to parameters",
      "text" : "In our objective, there are two important parameters, k, the size of each neighborhood, and C, the tradeoff parameter. To study the sensitivity of the algorithm to these two parameters, we plot the curves of the results with different values of the parameters. These curves of sensitivity to parameter k over four benchmark data sets are shown in Fig. 3. From the figures, we observe that the algorithm is stable to the changes of the parameter k. For different data sets, the optimal neighborhood sizes are different. For example, over the data set I, the lowest average structured loss is obtained at k = 20, while for data set II, it is obtained at k = 100. The sensitivity curves of C are given in Fig. 4. From this figure, we also can see that the performances of the proposed algorithm are stable against the changes of the\nparameter C, especially over data set II. We cannot see a clear trend of the changes of the results corresponding to the changes of the values of C. This means that the algorithm is robust to the selection of parameter C."
    }, {
      "heading" : "4.5. Repainting time analysis",
      "text" : "We are also interested in the running time of the proposed method, SSLSOP, and its competing methods. The running time of these methods over four benchmark data sets are given in Fig. 5. From the figure, we can see that the proposed method, SSLSOP, consumes the second shortest running time over three data sets. The only exception is the results over data set II. The least time consuming algorithm is the one proposed by Altun et al. [40], however, its prediction results are not satisfying. The most time consuming algorithm is the one proposed by Jiang et al. [39], but its prediction results are not as good as SSLSOP."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this paper, we investigate the problem of semi-supervised learning of structured output predictor. To handle the problem of diversity of the local distributions, we propose to learn local structured output predictors for neighborhoods of different data points. Moreover, we also propose to learn the missing outputs of the unlabeled data points. We build a new minimization problem to learn the local structured output predictors and the missing structured outputs simultaneously. This problem is modeled as the joint minimization of the local predictor complexity and the local structured output loss. The problem is optimized by gradient descent, and we designed an iterative algorithm to learn the local predictors. The experiments over benchmark data sets of medical image classification, natural image classification, computer science paper classification, and sentence part-of-speech tagging."
    }, {
      "heading" : "6. Future work",
      "text" : "In the future, we will study how to fit the proposed algorithm to big data sets, by using big data processing framework, such as Map-Reduce of Hadoop software. When the data set is big, i.e., the number of data points, n, is large, we can the Hadoop distributed file system (HDFS) to store the data set. The entire data set is split into some small sub-sets, and different sub-sets are stored in different clusters. The proposed algorithm has three basic steps, and each of them can be parallelized easily. The three steps are listed as follows:\n1. finding the k nearest neighbors of each data point,\n2. updating the local structured output predictor parameter of each neighborhood, and\n3. updating the outputs of each data point.\nTo find the k nearest neighbors of one data point from the distributed big data set, we can use the Map-Reduce framework. We use the Map function to find the k nearest neighbors from each sub-set, and then use the Reduce function to find the final k nearest neighbors from the results of the Map functions. To update the local structured output predictor parameter of one neighborhood, according to (12), only the data points of the considered neighborhood is used. We can store the data points of each neighborhood in the same cluster, and use a Map function to update the parameter simultaneously for all the local structured output predictors. Similarly, the structured outputs also are calculated by exploring the data points of the neighborhoods, by using Map functions."
    } ],
    "references" : [ {
      "title" : "Multiple parameter control for ant colony optimization applied to feature selection problem",
      "author" : [ "G. Wang", "H. Chu", "Y. Zhang", "H. Chen", "W. Hu", "Y. Li", "X. Peng" ],
      "venue" : "Neural Computing and Applications 26 (7) ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Margin-based over-sampling method for learning from imbalanced datasets",
      "author" : [ "X. Fan", "K. Tang", "T. Weise" ],
      "venue" : "in: Advances in Knowledge Discovery and Data Mining, Springer",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An improved lower bound for bayesian network structure learning",
      "author" : [ "X. Fan", "C. Yuan" ],
      "venue" : "in: Twenty-Ninth AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Feedforward kernel neural networks",
      "author" : [ "S. Wang", "Y. Jiang", "F.-L. Chung", "P. Qian" ],
      "venue" : "generalized least learning machine, and its deep learning with application to image classification, Applied Soft Computing Journal 37 ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An intrusion detection system using network traffic profiling and online sequential extreme learning machine",
      "author" : [ "R. Singh", "H. Kumar", "R. Singla" ],
      "venue" : "Expert Systems with Applications 42 (22) ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Regularized minimum class variance extreme learning machine for language recognition, Eurasip",
      "author" : [ "J. Xu", "W.-Q. Zhang", "J. Liu", "S. Xia" ],
      "venue" : "Journal on Audio, Speech, and Music Processing",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Multiple kernel multivariate performance learning using cutting plane algorithm",
      "author" : [ "J. Wang", "H. Wang", "Y. Zhou", "N. McDonald" ],
      "venue" : "in: Systems, Man and Cybernetics (SMC), 2015 IEEE International Conference on, IEEE",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An effective image representation method using kernel classification",
      "author" : [ "H. Wang", "J. Wang" ],
      "venue" : "in: 2014 IEEE 26th International Conference on Tools with Artificial Intelligence ",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Supervised learning of sparse context reconstruction coefficients for data representation and classification",
      "author" : [ "X. Liu", "J. Wang", "M. Yin", "B. Edwards", "P. Xu" ],
      "venue" : "Neural Computing and Applications ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multi-kernel learning for multivariate performance measures optimization",
      "author" : [ "F. Lin", "J. Wang", "N. Zhang", "J. Xiahou", "N. McDonald" ],
      "venue" : "Neural Computing and Applications ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Classification of unsteady flow patterns in a rotodynamic blood pump: Introduction of non-dimensional regime map",
      "author" : [ "F. Shu", "S. Vandenberghe", "J. Brackett", "J. Antaki" ],
      "venue" : "Cardiovascular Engineering and Technology 6 (3) ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Enhanced maximum auc linear classifier",
      "author" : [ "X. Fan", "K. Tang" ],
      "venue" : "in: Fuzzy Systems and Knowledge Discovery (FSKD), 2010 Seventh International Conference on, Vol. 4, IEEE",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Finding optimal bayesian network structures with constraints learned from data",
      "author" : [ "X. Fan", "B. Malone", "C. Yuan" ],
      "venue" : "in: Proceedings of the 30th Annual Conference on Uncertainty in Artificial Intelligence (UAI-14)",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Tightening bounds for bayesian network structure learning",
      "author" : [ "X. Fan", "C. Yuan", "B. Malone" ],
      "venue" : "in: Proceedings of the 28th AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A bayesian approach for sleep and wake classification based on dynamic time warping method",
      "author" : [ "C. Fu", "P. Zhang", "J. Jiang", "K. Yang", "Z. Lv" ],
      "venue" : "Multimedia Tools and Applications ",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Supervised cross-modal factor analysis for multiple modal data classification",
      "author" : [ "J. Wang", "Y. Zhou", "K. Duan", "J.J.-Y. Wang", "H. Bensmail" ],
      "venue" : "in: Systems, Man and Cybernetics (SMC), 2015 IEEE International Conference on, IEEE",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Representing data by sparse combination of contextual data points for classification",
      "author" : [ "J. Wang", "Y. Zhou", "M. Yin", "S. Chen", "B. Edwards" ],
      "venue" : "in: Advances in Neural Networks–ISNN 2015, Springer",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Biomarker binding on an antibodyfunctionalized biosensor surface: The influence of surface properties",
      "author" : [ "Y. Zhou", "W. Hu", "B. Peng", "Y. Liu" ],
      "venue" : "electric field, and coating density, The Journal of Physical Chemistry C 118 (26) ",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Achieving profitable biological sludge disintegration through phase separation and predicting its anaerobic biodegradability by non linear regression model",
      "author" : [ "S. Kavitha", "S. Adish Kumar", "S. Kaliappan", "I. Yeom", "J. Banu" ],
      "venue" : "Chemical Engineering Journal 279 ",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Combined use of milp and multi-linear regression to simplify lca studies",
      "author" : [ "J. Pascual-Gonzlez", "C. Pozo", "G. Guilln-Goslbez", "L. Jimnez-Esteller" ],
      "venue" : "Computers and Chemical Engineering 82 ",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Studies of the adaptive network-constrained linear regression and its application",
      "author" : [ "H. Yang", "D. Yi" ],
      "venue" : "Computational Statistics and Data Analysis 92 ",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Weibull and lognormal taguchi analysis using multiple linear regression",
      "author" : [ "M. Pia-Monarrez", "J. Ortiz-Yaez" ],
      "venue" : "Reliability Engineering and System Safety 144 ",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The relationship of dependency relations and parts of speech in hungarian",
      "author" : [ "V. Vincze" ],
      "venue" : "Journal of Quantitative Linguistics 22 (1) ",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Evaluating word embeddings and a revised corpus for part-of-speech tagging in portuguese",
      "author" : [ "E. Fonseca", "J. G Rosa", "S. Alusio" ],
      "venue" : "Journal of the Brazilian Computer Society 21 (1) ",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Leveraging topic modeling and part-of-speech tagging to support combinational creativity in requirements engineering",
      "author" : [ "T. Bhowmik", "N. Niu", "J. Savolainen", "A. Mahmoud" ],
      "venue" : "Requirements Engineering ",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multilingual part-of-speech tagging with weightless neural networks",
      "author" : [ "H. Carneiro", "F. Frana", "P. Lima" ],
      "venue" : "Neural Networks 66 ",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Pos-rs: A random subspace method for sentiment classification based on part-of-speech analysis",
      "author" : [ "G. Wang", "Z. Zhang", "J. Sun", "S. Yang", "C. Larson" ],
      "venue" : "Information Processing and Management 51 (4) ",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multifractal characterisation and classification of bread crumb digital images",
      "author" : [ "R. Baravalle", "C. Delrieux", "J. Gmez" ],
      "venue" : "Eurasip Journal on Image and Video Processing 2015 (1) ",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A novel information transferring approach for the classification of remote sensing images",
      "author" : [ "J. Gao", "L. Xu", "J. Shen", "F. Huang", "F. Xu" ],
      "venue" : "Eurasip Journal on Advances in Signal Processing 2015 (1) ",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A computer vision approach for automated analysis and classification of microstructural image data",
      "author" : [ "B. Decost", "E. Holm" ],
      "venue" : "Computational Materials Science 110 ",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Bayesian sample steered discriminative regression for biometric image classification",
      "author" : [ "G. Gao", "J. Yang", "S. Wu", "X. Jing", "D. Yue" ],
      "venue" : "Applied Soft Computing Journal 37 ",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning distributed representations for structured output prediction",
      "author" : [ "V. Srikumar", "C. Manning" ],
      "venue" : "Vol. 4",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The impact of incomplete knowledge on the evaluation of protein function prediction: A structured-output learning perspective",
      "author" : [ "Y. Jiang", "W. Clark", "I. Friedberg", "P. Radivojac" ],
      "venue" : "Bioinformatics 30 (17) ",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Augmenting image descriptions using structured prediction output",
      "author" : [ "Y. Han", "X. Wei", "X. Cao", "Y. Yang", "X. Zhou" ],
      "venue" : "IEEE Transactions on Multimedia 16 (6) ",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Structured output prediction with hierarchical loss functions for seafloor imagery taxonomic categorization",
      "author" : [ "N. Nourani-Vatani", "R. Lpez-Sastre", "S. Williams" ],
      "venue" : "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 9117 ",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Manifold regularization in structured output space for semi-supervised structured output prediction",
      "author" : [ "F. Jiang", "L. Jia", "X. Sheng", "R. LeMieux" ],
      "venue" : "Neural Computing and Applications ",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Maximum margin semisupervised learning for structured variables",
      "author" : [ "Y. Altun", "M. Belkin", "D.A. Mcallester" ],
      "venue" : "in: Advances in neural information processing systems",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Semi-supervised learning for structured output variables",
      "author" : [ "U. Brefeld", "T. Scheffer" ],
      "venue" : "in: Proceedings of the 23rd international conference on Machine learning, ACM",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "H",
      "author" : [ "J. Suzuki", "A. Fujino" ],
      "venue" : "Isozaki, Semi-supervised structured output learning based on a hybrid generative and discriminative approach., in: EMNLP-CoNLL",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Manifold regularization in structured output space for semi-supervised structured output prediction",
      "author" : [ "F. Jiang", "L. Jia", "X. Sheng", "R. LeMieux" ],
      "venue" : "Neural Computing and Applications ",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Towards a probabilistic semisupervised kernel minimum squared error algorithm",
      "author" : [ "H. Gan", "R. Huang", "Z. Luo", "Y. Fan", "F. Gao" ],
      "venue" : "Neurocomputing 171 ",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Semi-supervised feature selection based on local discriminative information",
      "author" : [ "Z. Zeng", "X. Wang", "J. Zhang", "Q. Wu" ],
      "venue" : "Neurocomputing 173 ",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Enhancing semi-supervised learning through label-aware base kernels",
      "author" : [ "Q. Wang", "K. Zhang", "Z. Chen", "D. Wang", "G. Jiang", "I. Marsic" ],
      "venue" : "Neurocomputing 171 ",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Manifold regularization for structured outputs via the joint kernel",
      "author" : [ "C. Hu", "J.T. Kwok" ],
      "venue" : "in: Neural Networks (IJCNN), The 2010 International Joint Conference on, IEEE",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Semi-supervised distance metric learning based on local linear regression for data clustering",
      "author" : [ "H. Zhang", "J. Yu", "M. Wang", "Y. Liu" ],
      "venue" : "Neurocomputing 93 ",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Local ridge regression for face recognition",
      "author" : [ "H. Xue", "Y. Zhu", "S. Chen" ],
      "venue" : "Neurocomputing 72 (4) ",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The digital database for screening mammography",
      "author" : [ "M. Heath", "K. Bowyer", "D. Kopans", "R. Moore", "W.P. Kegelmeyer" ],
      "venue" : "in: Proceedings of the 5th international workshop on digital mammography, Citeseer",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A",
      "author" : [ "J. Xiao", "J. Hays", "K. Ehinger", "A. Oliva" ],
      "venue" : "Torralba, et al., Sun database: Large-scale scene recognition from abbey to zoo, in: Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, IEEE",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Efficient clustering of highdimensional data sets with application to reference matching",
      "author" : [ "A. McCallum", "K. Nigam", "L.H. Ungar" ],
      "venue" : "in: Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, ACM",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Introduction to the conll-2002 shared task: Languageindependent named entity recognition",
      "author" : [ "T.K. Sang" ],
      "venue" : "in: Proceedings of the 6th conference on natural language learning,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 11,
      "context" : "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 15,
      "context" : "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].",
      "startOffset" : 180,
      "endOffset" : 196
    }, {
      "referenceID" : 19,
      "context" : "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].",
      "startOffset" : 180,
      "endOffset" : 196
    }, {
      "referenceID" : 20,
      "context" : "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].",
      "startOffset" : 180,
      "endOffset" : 196
    }, {
      "referenceID" : 21,
      "context" : "The problem of predicting binary class label is called classification [13, 14, 15, 16, 17, 18, 19, 20, 21], while the problem of predicting continues response is called regression [22, 23, 24, 25].",
      "startOffset" : 180,
      "endOffset" : 196
    }, {
      "referenceID" : 22,
      "context" : "For example, in the part-ofspeech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [26, 27, 28, 29, 30].",
      "startOffset" : 239,
      "endOffset" : 259
    }, {
      "referenceID" : 23,
      "context" : "For example, in the part-ofspeech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [26, 27, 28, 29, 30].",
      "startOffset" : 239,
      "endOffset" : 259
    }, {
      "referenceID" : 24,
      "context" : "For example, in the part-ofspeech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [26, 27, 28, 29, 30].",
      "startOffset" : 239,
      "endOffset" : 259
    }, {
      "referenceID" : 25,
      "context" : "For example, in the part-ofspeech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [26, 27, 28, 29, 30].",
      "startOffset" : 239,
      "endOffset" : 259
    }, {
      "referenceID" : 26,
      "context" : "For example, in the part-ofspeech tagging problem of natural language processing, given a sequence of words, we want to predict the tags of the part-of-speech of the works, and the output of the prediction is a sequence of parts-of-speech [26, 27, 28, 29, 30].",
      "startOffset" : 239,
      "endOffset" : 259
    }, {
      "referenceID" : 27,
      "context" : "In the problem of hierarchical image classification problem, the class labels of images are organized as a tree structure, and the outputs of the prediction problem are the leaves of a tree [31, 32, 33, 34].",
      "startOffset" : 190,
      "endOffset" : 206
    }, {
      "referenceID" : 28,
      "context" : "In the problem of hierarchical image classification problem, the class labels of images are organized as a tree structure, and the outputs of the prediction problem are the leaves of a tree [31, 32, 33, 34].",
      "startOffset" : 190,
      "endOffset" : 206
    }, {
      "referenceID" : 29,
      "context" : "In the problem of hierarchical image classification problem, the class labels of images are organized as a tree structure, and the outputs of the prediction problem are the leaves of a tree [31, 32, 33, 34].",
      "startOffset" : 190,
      "endOffset" : 206
    }, {
      "referenceID" : 30,
      "context" : "In the problem of hierarchical image classification problem, the class labels of images are organized as a tree structure, and the outputs of the prediction problem are the leaves of a tree [31, 32, 33, 34].",
      "startOffset" : 190,
      "endOffset" : 206
    }, {
      "referenceID" : 31,
      "context" : "The problem of learning predictive models to predict unknown structured outputs are called as structured output prediction [35, 36, 37, 38, 39].",
      "startOffset" : 123,
      "endOffset" : 143
    }, {
      "referenceID" : 32,
      "context" : "The problem of learning predictive models to predict unknown structured outputs are called as structured output prediction [35, 36, 37, 38, 39].",
      "startOffset" : 123,
      "endOffset" : 143
    }, {
      "referenceID" : 33,
      "context" : "The problem of learning predictive models to predict unknown structured outputs are called as structured output prediction [35, 36, 37, 38, 39].",
      "startOffset" : 123,
      "endOffset" : 143
    }, {
      "referenceID" : 34,
      "context" : "The problem of learning predictive models to predict unknown structured outputs are called as structured output prediction [35, 36, 37, 38, 39].",
      "startOffset" : 123,
      "endOffset" : 143
    }, {
      "referenceID" : 35,
      "context" : "The problem of learning predictive models to predict unknown structured outputs are called as structured output prediction [35, 36, 37, 38, 39].",
      "startOffset" : 123,
      "endOffset" : 143
    }, {
      "referenceID" : 36,
      "context" : "However, in realworld application, many output are not available for the inputs [40, 41, 42, 43].",
      "startOffset" : 80,
      "endOffset" : 96
    }, {
      "referenceID" : 37,
      "context" : "However, in realworld application, many output are not available for the inputs [40, 41, 42, 43].",
      "startOffset" : 80,
      "endOffset" : 96
    }, {
      "referenceID" : 38,
      "context" : "However, in realworld application, many output are not available for the inputs [40, 41, 42, 43].",
      "startOffset" : 80,
      "endOffset" : 96
    }, {
      "referenceID" : 39,
      "context" : "However, in realworld application, many output are not available for the inputs [40, 41, 42, 43].",
      "startOffset" : 80,
      "endOffset" : 96
    }, {
      "referenceID" : 40,
      "context" : "Learning from such a training set is call semi-supervised learning [44, 45, 46].",
      "startOffset" : 67,
      "endOffset" : 79
    }, {
      "referenceID" : 41,
      "context" : "Learning from such a training set is call semi-supervised learning [44, 45, 46].",
      "startOffset" : 67,
      "endOffset" : 79
    }, {
      "referenceID" : 42,
      "context" : "Learning from such a training set is call semi-supervised learning [44, 45, 46].",
      "startOffset" : 67,
      "endOffset" : 79
    }, {
      "referenceID" : 36,
      "context" : "[40] proposed the problem of semi-supervised learning with structured outputs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "Brefeld and Scheffer [41] proposed to solve the problem of semi-supervised structured output prediction by learning in the space of input-output space, and using co-training method.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 38,
      "context" : "[42] proposed a hybrid method to solve the problem of semi-supervised structured output learning.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "[43] proposed to regularize the structured outputs by the manifold constructed from the input space directly.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "Our work approximate the upper bound of the structured loss, and is inspired by the lower bound approximation of the structure learning of the Bayesian network [16, 4].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 2,
      "context" : "Our work approximate the upper bound of the structured loss, and is inspired by the lower bound approximation of the structure learning of the Bayesian network [16, 4].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 13,
      "context" : "Thus we also discuss the works of bound approximation technologies of [16, 4].",
      "startOffset" : 70,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "Thus we also discuss the works of bound approximation technologies of [16, 4].",
      "startOffset" : 70,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : "[16] proposed to tighten the upper and lower bounds of the breadth-first branch and bound algorithm for the learning of Bayesian network structures.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "The work of [16] is a contribution of major significance to the bound approximation community, and our upper bound approximation method is also based on these strategies.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 2,
      "context" : "[4] further proposed to improve the lower bound function of static k-cycle conflict heuristic for the learning of Bayesian network structures.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 39,
      "context" : "It has been shown that using local connections is an effective way to model the connections among different data points [43].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 43,
      "context" : "More specifically, with the nearest neighbor graph, we hope that the neighboring data points can obtain similar structured outputs from the predictor [47, 43].",
      "startOffset" : 150,
      "endOffset" : 158
    }, {
      "referenceID" : 39,
      "context" : "More specifically, with the nearest neighbor graph, we hope that the neighboring data points can obtain similar structured outputs from the predictor [47, 43].",
      "startOffset" : 150,
      "endOffset" : 158
    }, {
      "referenceID" : 44,
      "context" : "To solve this problem, we propose to learn multiple local linear structured output predictor for different neighborhoods to model the local distributions, instead of learning one single predictor for the entire data [48, 49].",
      "startOffset" : 216,
      "endOffset" : 224
    }, {
      "referenceID" : 45,
      "context" : "To solve this problem, we propose to learn multiple local linear structured output predictor for different neighborhoods to model the local distributions, instead of learning one single predictor for the entire data [48, 49].",
      "startOffset" : 216,
      "endOffset" : 224
    }, {
      "referenceID" : 46,
      "context" : "• Data set I - DDSM mammography image data set: The last data set is a medical imaging data set which contains Mammography images [50].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 47,
      "context" : "• Data set II - SUN natural image data set: The second data set is a image data set [51].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 48,
      "context" : "• Data set III - Cora research paper data set: This data set is a set of computer science research papers [52].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 49,
      "context" : "• Data set IV - Spanish news wire article sentence data set: This data set is a data set for named entity recognition problem of natural language processing [53].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 36,
      "context" : "[40], Brefeld and Scheffer [41], Suzuki et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "[40], Brefeld and Scheffer [41], Suzuki et al.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 38,
      "context" : "[42], and Jiang et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "[43].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "[43] 0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[40] 0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "504 Brefeld and Scheffer [41] 0.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 38,
      "context" : "[42] 0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 39,
      "context" : "[43] is the second best method.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[40], however, its prediction results are not satisfying.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[39], but its prediction results are not as good as SSLSOP.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we study the problem of semi-supervised structured output prediction, which aims to learn predictors for structured outputs, such as sequences, tree nodes, vectors, etc., from a set of data points of both inputoutput pairs and single inputs without outputs. The traditional methods to solve this problem usually learns one single predictor for all the data points, and ignores the variety of the different data points. Different parts of the data set may have different local distributions, and requires different optimal local predictors. To overcome this disadvantage of existing methods, we propose to learn different local predictors for neighborhoods of different data points, and the missing structured outputs simultaneously. In the neighborhood of each data point, we proposed to learn a linear predictor by minimizing both the complexity of the predictor and the upper bound of the structured prediction loss. The minimization is conducted by gradient descent algorithms. Experiments over four benchmark data sets, including DDSM mammography medical images, SUN natural image data set, Cora research paper data set, and Spanish news wire article sentence data set, show the advantages of the proposed method.",
    "creator" : "LaTeX with hyperref package"
  }
}