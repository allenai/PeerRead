{
  "name" : "1602.05112.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Patient Flow Prediction via Discriminative Learning of Mutually-Correcting Processes",
    "authors" : [ "Hongteng Xu", "Weichang Wu", "Shamim Nemati", "Hongyuan Zha" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Patient flow; mutually-correcting process; discriminative learning; logistic regression; group lasso; imbalanced data.\nF"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "R ECENT reports have highlighted an increasing demand forcare units in the United States due to an improved life expectancy and a larger aging population [1]. Patient management and reducing waiting time, particularly in the Emergency Department (ED) [2], [3] and intensive care unit (ICU) [4], [5], is crucially important to improving quality of care, outcomes, and the overall patient satisfaction. The so-called practice of “patient boarding” refers to temporarily keeping critically-ill patients in their existing hospital location, such as the emergency department or the post anesthesia unit, while awaiting available CU bed [4], which may result in suboptimal care, and increase both length of stay (LOS) and hospital mortality [6], [7]. System-level management of medical resources becomes even more critical for large numbers of critically-ill patients in the case of disasters and pandemics [8].\nSuch an urgent requirement gives rise to an important problem of predicting the transition processes of patients, known as the “patient flow” (see Fig. 1(a)), which has not been explored via existing machine learning techniques. The patient flow includes patients’ duration time within each care unit and transition probability among different units, and is determined by a number of factors including patient’s underlying condition and clinical state, disease progression, and availability of care team and care resources. With the advent of comprehensive electronic health\n• H. Xu is with the School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, 30332. E-mail: hxu42@gatech.edu • W. Wu is with Shanghai Key Lab of Digital Media Transmission and Processing, Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China, 200240. E-mail: blade091@sjtu.edu.cn • S. Nemati is with the School of Medicine, Emory University, Atlanta, GA, 30322. E-mail: shamim.nemati@emory.edu • H. Zha are with the College of Computing, Georgia Institute of Technology, Atlanta, GA, 30332. E-mail: zha@cc.gatech.edu\nrecords (EHRs) and real-time streaming analytics [9], much of these factors can be captured and utilized to jointly model flow of patients within many care units. Therefore, the problem we aim to address in this work involves predicting patients’ destination CUs and durations simultaneously based on their medical records and continuously-documented clinical status. Solving this problem may enable early planning and optimization of hospital resources.\nHowever, predicting patient flow is a difficult task due to a number of factors, e.g., the collection and the storage of a huge amount of data, the lack of a systematic approach to resource management, etc. Additionally, from the viewpoint of machine learning, the main challenges include:\nTime-sensitivity. The prediction of patient flow is a timesensitive learning task, which requires us to both predict the destination care unit of a patient (i.e., the transition) and the dwell time within that care unit (i.e., the duration).\nFeature selection. The patient flow can be viewed as a timevarying transition process in continuous time, which is influenced by many medical factors, e.g., patients’ health profiles, diagnoses, medications, nursing, etc. However, the relationships between these factors and the transition process are not fully explored and their importance for predicting patient flow is unknown. Although modern EHRs may include complete or partial information pertaining to most of these factors, taking advantage of EHRs involves feature selection and fusion, all of which are highly dependent on the model used to describe the patient flow process.\nData sparsity and case imbalance. Because most patients more often stay in general wards than transfer to other CUs (or moved around within the same CU), models and learning algorithms may suffer from sparse and imbalanced data — the general ward appears in most of transition processes while a certain CU may only appear in very few of them.\nConsidering the challenges above, we need a predictive model\nar X\niv :1\n60 2.\n05 11\n2v 3\n[ cs\n.L G\n] 1\n0 N\nov 2\n01 6\nthat jointly captures the transitions and durations in patient flow. Moreover, the model should consider all influential factors and be robust to data sparsity and imbalance. To the best of our knowledge, no existing work has been proposed to deal with such a challenging situation. For achieving this aim, in this paper we propose a novel and efficient method that utilizes both timeinvariant and time-varying features from patients’ EHRs, to predict the patient flow, as depicted in Fig. 1(b).\nBased on the unique characteristics of patient flow, we consider the transitions among the care units and the dwell time within each care unit as two separate events, which are jointly modeled via a novel parametric point process model called mutually-correcting process. Applying our mutually-correcting process model with the EHR-based features, the instantaneous rate of patient being transferred to a given patient care unit and that of staying certain days in the unit are captured via two parameterized conditional intensity functions. Compared with traditional models, such as discrete Markov chain [10], vector auto-regressive model [11] and hidden Markov model [12], which can only deal with time-invariant transition process formulated as discrete time series, our point process model is able to describe time-varying transition processes in continuous time. Compared with other continuous model, such as the continuous-time Markov chain [13], our model captures the mutually-correcting patterns among states over time using all historical data, which does not need to set the order of model in advance. In other words, our model is more robust to sparse data and model misspecification.\nBesides proposing a mutually-correcting process to model the patient flow, another technical contribution of our work is the development of a methodology for learning a parametric point process model in a discriminative way. Specifically, traditional generative point processes model the joint distribution of events\nin continuous time and parameters are learned via the maximum likelihood estimation. In this work, however, we focus on learning the conditional distribution of transition and that of duration given historical events. We analyze the relationship between the conditional distribution and the conditional intensity function, showing that by using the proposed mutually-correcting process, we can formulate the learning problem as learning a multinomial logistic regression model that greatly simplifies the learning task. Thousands of factors, i.e., diagnoses, treatments and medications, are generated and accompany the patient flow, while only few of them are highly influential. Moreover, these key factors generally have influences on both patients’ destination CUs and duration at the same time. Therefore, feature selection is introduced into the framework of our learning algorithm. Specifically, we formulate these factors as high-dimensional features, which are shared via the logistic regressor for predicting destination CUs and that for predicting duration. The parameters of these two models are associated with these features and learned jointly. We treat each dimension of feature (i.e., a factor influencing patient flow) as a “group” and regularize the parameters via l1,2-norm. It guarantees the group sparsity of parameters so that only the parameters corresponding to the features of important dimensions are non-zero and shared via both models. Leveraging the alternating direction method of multipliers (ADMM) [14] with group-lasso [15], we propose an efficient algorithm to learn the model.\nFor overcoming the data imbalance problem, we investigate several robust learning methods for imbalanced data and make comparisons for them. We focus on applying a pre-processing on our imbalanced patient flow data, which shows its superiority in our experiments: for the classes with extremely few samples, we synthesize some auxiliary samples from original ones to increase the number of training samples. Taking original samples and\nauxiliary ones as training samples, we can improve the robustness of our learning method greatly and obtain better performance in the testing phase.\nIn summary, the contributions of our method include: 1) We propose a flexible mutually-correcting process model to capture the properties of patient flow. 2) We propose a discriminative algorithm to learn point processes. In certain cases, e.g., the proposed mutually-correcting processes, the algorithm can be implemented via logistic regression. 3) Combining group-lasso with ADMM, we achieve feature selection and learning model jointly in the training phase. 4) The influence of data imbalance is considered, and a preprocessing step is applied to synthesize auxiliary data for training. The preprocessing helps us to improve the robustness of learning algorithm.\nOur method can be viewed as a point process-based interpretation of multinomial logistic regression model for continuous-time transition processes. We test our method on real-world patient flow data set and compare it with several alternative methods on the prediction accuracy and robustness to imbalanced data. Additionally, we analyze the functions of various parameters and investigate their impacts on our learning algorithm. We demonstrate the robustness of our method to those parameters. Multiple metrics are applied to evaluate the performance of various methods, including the prediction accuracy of the destination CUs and duration days, and the relative simulation error of the patient flow. Experimental results demonstrate that our method significantly outperforms its competitors, especially in predicting those unique transitions and usages of CUs."
    }, {
      "heading" : "2 BACKGROUND AND DATA ANALYSIS",
      "text" : ""
    }, {
      "heading" : "2.1 Notations and Problem Statement",
      "text" : "Suppose that we have U patients in a hospital having C CU departments. For each patient u, u = 1, ..., U , her transition process among CUs is represented via an event sequence in continuous time, denoted as su = {(cui , dui , tui )}N u i=1. Here, t u i ∈ (0, Tu] is the time when a transition event happened, Tu is the length of observation time window, cui ∈ C, C = {1, ..., C}, is the destination CU of the transition, dui ∈ D, D = {1, ..., D}, is the dwell time (measured by the number of duration days) of the patient in the previous CU (i.e., the cui−1-th CU) before the transition, and Nu is the number of transitions1. The set of historical transitions before time t is denoted as Hut = {(cui , dui , tui )|tui < t}.\nEach event (c, d, t), which means that a patient stays in a CU for d days before transferred to the c-th CU, is always accompanied by a series of medical services. According to the EHRs of patients, we classify various medical services into three categories: treatment, medication and nursing. The treatment contains Mtreat items, including various medical tests, surgeries and therapies. The medication contains Mmed items, including various medicines and their various usage methods. The nursing contains Mnurse items, including various nursing programs and records of patients’ liquid inputs and outputs. We can extract binary feature vectors for patient u from her EHRs, denoted as fui ∈ {0, 1}Mtreat+Mmed+Mnurse , i = 1, ..., Nu. Here fui is a binary vector corresponding to the EHR of patient u when staying in the cui -th CU, in which the elements corresponding to received services are 1’s. It is the concatenation of three binary vectors corresponding to the three categories above. Besides the time-varying\n1. When i = 1, we do not consider the duration and set dui = NULL.\nfeatures mentioned above, a patient’s EHR also contains Mp timeinvariant features, including personal health profile like gender, age, chronic diseases, and diagnoses2. Similarly, we can extract a binary feature vector for the patient, denoted as fu0 ∈ {0, 1}Mp .\nThe event sequence of patient can be modeled using point process methodology [16]. Specifically, we capture the temporal dynamics of event sequences via the conditional intensity function defined as follows:\nλ(t)dt = E(dN(t)|Ht), (1)\nwhere N(t) is the number of events occurred in time range (−∞, t], Ht contains historical events before time t, and E(dN(t)|Ht) is the expectation of the number of events happening in the interval (t, t + dt] given historical observations Ht. The conditional intensity function in Eq. (1) represents the expected instantaneous rate of future events at time t. Based on conditional intensity function, the conditional probability that an event happens at time t given historical record is computed as\np(t|Ht) = λ(t) exp ( − ∫ t tI λ(s)ds ) . (2)\nHere tI is the time stamp of the last event before time t. Problem statement. For each patient u, given historical record Hutui−1 and EHR features {f u 0 ,f u 1 , ...,f u i−1}, we aim to predict the destination CU of the next transition (i.e., cui ) and the duration before the transition (i.e., dui )."
    }, {
      "heading" : "2.2 Data and Basic Statistics",
      "text" : "We focus on the real-world data from MIMIC II database [17], from which 30, 685 patients staying in CUs are selected for training and testing. The CUs are categorized into C = 8 departments, including the Coronary care unit (CCU), the Anesthesia care unit (ACU), the Fetal ICU (FICU), the Cardiac surgery recovery unit (CSRU), the Medical ICU (MICU), the Trauma Surgical ICU (TSICU), the Neonatal ICU (NICU), and the general ward (GW). According to the EHRs of the patients, the number of treatment items is Mtreat = 5, 627, the number of medication items is Mmed = 405, the number of nursing items is Mnurse = 6, 808, and the number of time-invariant features is Mp = 4, 832.\nThe data is representative, which reflects the following natures of patient flow. For each department, the number of patients ever staying in it and the number of transitions directing to it are shown in Table 1. We can find that the data for various departments is imbalanced. On the one hand, most of the patients and transitions concentrate on certain CUs, e.g., GW, CCU, and CSRU, etc. On the other hand, few patients and transitions involve ACU and TSICU. The average duration days for each department is also listed. Except for NICU, the average dwell time of other department is within one week. To simplify our treatment, we categorize the duration times into D = 8 time intervals, include 1 day, 2 days, ...., 7 days and more than 1 week.\nInterestingly we also observed that the transitions and the durations are weakly correlated with each other. The correlation coefficient between the transition and the duration is about 0.2. Fig. 2 further gives the normalized histograms of various CUs w.r.t. the categories of duration days. We can find that in each category of duration, the frequency of occurrence for various CUs generally do not have large variance. It should be noted that the\n2. In our data set the diagnose is time-invariant because the patient flow for each patient is collected after a single diagnose.\nnature of the weak correlation between the transition and the duration is important for us to simplify our model, which will be shown in the following section.\nTable 2 gives the proportions of nonzero elements in different feature domains w.r.t various CUs. Specifically, we count the number of nonzero elements in different feature domains for each CU and normalize the counts. The proportions reflect the importance of feature domains. We can find that patient’s profile, treatment, and nursing are relatively important for all CUs, which contain most of nonzero features. On the contrary, the proportion of nonzero features from medication is relatively low. For TSICU and GW, most of nonzero features concentrate in the domain of treatment."
    }, {
      "heading" : "3 PROPOSED METHOD",
      "text" : "In this section, we take advantage of the properties of patient flow and propose a mutually-correcting point process to describe the transitions among CUs and the durations in them respectively. The proposed model can be viewed as a specialization of a generalized parametric point process model. It has higher capability and can represent more complicated temporal dynamics of event sequences than existing popular point processes, e.g., modulated Poisson process [10], Hawkes process [18] and self-correcting process [19]. A discriminative learning algorithm for the point process model is proposed, which combines the alternating direction method\nof multipliers (ADMM) and the group-lasso. Both the feature selection problem and the imbalance of data are considered in our learning algorithm. Finally, a pre-processing method for training samples is proposed to handle the data imbalance problem."
    }, {
      "heading" : "3.1 Mutually-correcting Process Model",
      "text" : "As aforementioned, patient flow is a time-varying transition process in continuous time. It generally has two important properties. Again, take the patient flow in Fig. 1(a) as an example:\nHigh correlation between EHRs and patient flow. A typical EHR consists of a patient’s profile (i.e., gender, age), her diagnose of certain diseases (i.e., ICD code), and her treatment process, e.g., medications, nursing information, the transitions and durations in various care units. It reflects the patient’s status and contains very useful information for predicting patient flow. Recall the previous cases shown in Fig. 1(a). For a man having coronary heart disease, the probability staying in the Coronary care unit is relatively high, while the probability staying in the Neonatal ICU is zero. On the contrary, for a premature baby, the probability staying in the Neonatal ICU is high while the probability staying in the Coronary care unit is very low. In more general cases, most of patients whose treatments involve surgeries are likely to have transitions among the Anesthesia care unit, the surgery recovery unit, and the general ward. Similarly, the duration of a patient in a CU is also dependent on her health record. The patients having chronic diseases may spend a lot of time at the general ward. The patients after surgeries may stay at the surgery recovery units for varying time according to their feedback of treatments and recovery. In summary, the patient flow is highly correlated with their EHRs. The patients’ EHR can help us to predict what types of CUs they need and how long will they stay at different CUs.\nMutually-correcting across CUs. Staying in the Coronary care unit is likely to increase the probability transferring to the Cardiac surgery recovery unit while suppress the probability transferring to the Neonatal ICU. It reflects that the duration of previous CU has a positive or negative influence on the transitions to following CUs, which is called mutually-correcting in our work.\nTherefore, both the transitions among CUs and the durations in different CUs contain mutually-correcting patterns, which are highly dependent on EHR-based features. Additionally, taking the weak correlation between the transition and the duration (Fig. 2) into consideration, we propose a new point process model called mutually-correcting process to model the transitions and the durations respectively. Specifically, given the event sequence su = {cui , dui , tui } Nu i=1 of patient u, we decouple the event (c, d) into two independent events c and d, which correspond to two counting processes {Nuc (t)}Cc=1 and {Nud (t)}Dd=1. Here Nuc (t) is the number of events that transferring patient u to the c-th CU after time t, while Nud (t) is the number of events that staying in a CU d days after time t. We propose a generalized parametric model for the conditional intensity functions of these two counting processes as follows,3\nλuc (t) = f(α > c f u 0 g(t)− β>c ∑ i:tui <t fui h(t, t u i )), λud(t) = f(α > d f u 0 g(t)− β>d ∑ i:tui <t fui h(t, t u i )).\n(3)\n3. It should be noted that Eq. (3) can be further generalized by replacing αg(t), βh(t, tui ) with functional α(t), β(t). Then, the model becomes nonparametric, which is out-of-range in this paper.\nλuc (t) represents the instantaneous rate of the event transferring patient u to the c-th CU at time t, while λud(t) represents the instantaneous rate of the event staying in a CU d days. Here {fu0 ,fui } are time-invariant and time-varying features defined in Section 2. The term α>fu0 g(t) represents the temporal influence of time-invariant feature of the patient on event. The term β> ∑ i:tui <t\nfui represents the temporal influences of historical transitionsHut on event. Here f(·), g(·) and h(·, ·) are predefined time functions, which describes the increase or the decay of influences over time.\nEq. (3) provides a unified framework for many useful point processes, e.g., modulated Poisson processes [20], Hawkes processes [21], [22], as Table 3 shows. In our mutually-correcting process model, we set f(·) = exp(·), g(t) = t − tuI , and h(t, t′) = exp(− (t−t ′)2\nσ2 ), where t u I is the time stamp of the last\nevent before time t for patient u. Our model extends traditional self-correcting process model [23] to multivariate case and further considers the temporal decay of influence from historical record. Compared with existing models, our model is more flexible. Firstly, different from self-correcting process, whose historical influence is time-invariant, i.e., h(·, ·) ≡ 1, our model considers the time-varying historical influence as Hawkes process does. Secondly, for guaranteeing models to be physically-meaningful and stable, the self-correcting process requires all parameters α = [α1, ...,αC ], β = [β1, ...,βC ] to be nonnegative while the modulated Poisson and Hawkes process require α ≥ 0 and β ≤ 0. Our model, however, does not have such constraints. Such a relaxation increases the flexibility of our model and enhances the description power of conditional intensity function. Fig. 3 shows that the dynamics of conditional intensity function for various point processes in 1-dimensional case. We can find that the conditional intensity function of modulated Poisson process is piecewise constant. A jump happens when a new event comes. However, the change of event’s happening rate between adjacent\nevents cannot be captured. Hawkes process and self-correcing process can only describe the change of event’s happening rate via fixed pattern — the conditional intensity always decreases for Hawkes process and increases for self-correcting process till new event comes. Our mutually-correcting process, however, is more flexible, which can capture both the increase and decrease of intensity function between adjacent events.\nObviously, the conditional intensity function of our mutuallycorrecting process model can be rewritten as\nλuc (t) = exp(θ > c f u t ), λ u d(t) = exp(θ > d f u t ). (4)\nfut = [f u> 0 (t − tuI ), ( ∑ tui <t\nexp(−(t − tui )2/σ2)fui )>]> ∈ RM , θd = [α>d ,β>d ]>, θc = [α>c ,β>c ]>, M = Mtreat + Mmed +Mnurse +Mp. Such a simple representation inspires us to propose the following discriminative learning method for our model with the help of multinomial logistic regression."
    }, {
      "heading" : "3.2 Discriminative Learning of Model",
      "text" : "Traditional learning methods for point processes are generative, which aim to estimate the joint probability of all events via a maximum likelihood estimator, i.e., maxΘ ∏ u,i p(c u i , d u i , t u i |Hutui )(1−P (T\nu)), where p(c, d, t|Hut ) is the conditional probability of event (c, d) given historical record Hut , and P (Tu) is the cumulative probability transferring before Tu. The parameters of the model is represented as a matrix Θ = {θc,θd}c∈C,d∈D ∈ RM×(C+D). However, the generative learning methods may lack discrimination power because it naturally cares more about the happening of the whole event sequence, than the classification or the prediction of individual events given historical record. The information of labels, e.g., the transition destination and the duration, is not fully used in the model. Additionally, the sparse and imbalanced data, e.g., the patient flow data we deal with, is insufficient for estimating the joint probability, so that the generative learning methods will be at high risk of over-fitting.\nAccording to the analysis above, we propose a discriminative learning method for our model. Recall the problem we have: given current time tui−1 and historical recordHuti−1 , we aim to maximize the probability that the patient u stay in a CU dui days before being transferred to the cui -th CU, i.e., p(c u i , d u i |tui−1,Huti−1). Therefore, instead of estimating p(c, d, t|Hut ) directly, we focus on the conditional probability p(c, d|t,Hut ), which is the probability of event (c, d) given current time t and historical record. As shown in Eq. (3), we decouple the event (c, d) into two independent events c and d, so we can specialize Eq. (2) as\np(c, d, t|Hut ) =λuc,d(t) exp ( − C∑ c′=1 D∑ d′=1 ∫ t tuI λuc′,d′(s)ds )\n= λuc,d(t)∑ λuc′,d′(t)\n× ∑ c′,d′ λ u c′,d′(t)\nexp (∑∫ t\ntuI λuc′,d′(s)ds ) =p(c, d|t,Hut )× p(t|Hut ) =p(c|t,Hut )× p(d|t,Hut )× p(t|Hut ) = λuc (t)∑ c′ λ u c′(t) × λ u d(t)∑ d′ λ u d′(t) × p(t|Hut ),\n(5)\nwhere p(t|Hut ) is the conditional probability that there is an event happening at time t given historical record, and λuc,d(t) measures the instantaneous happening rate of the event that the patient u\nstay in a CU d days before being transferred to the c-th CU. Focusing on the first two terms in the last row, we can find that the formulas of p(c|t,Hut ) and p(c|t,Hut ) in Eq. (5) are actually the normalized intensity functions.\nBased on Eq. (5), we propose the following cross-entropybased loss function for our learning task.\nL(Θ) =− U∑ u=1 Nu∑ i=1 { C∑ c=1 1{cui = c} log p(c|tui−1,Hutui−1)\n+ D∑ d=1 1{dui = d} log p(d|tui−1,Hutui−1) }\n=− U∑ u=1 Nu∑ i=1 log\n( λucui (t u i−1)λ u cui (tui−1)∑\nc′ λ u c′(t u i−1) ∑ d′ λ u d′(t u i−1)\n) .\n(6)\nHere 1{statement} is an indicator of returning to 1 if the statement is truth, otherwise to 0.\nAdditionally, for exploring the relationship between the EHRbased feature and the patient flow, we consider the group sparsity of the parameter matrix of proposed model, denoted as ‖Θ‖1,2. ‖Θ‖1,2 = ∑M m=1 ‖Θm‖2 sums the l2-norms of Θ’s rows Θm, m = 1, ...,M . Here each dimension of feature is treated as a group. Introducing this term as a regularizer into the loss function, we achieve feature selection simultaneously when learning model — the rows corresponding to insignificant and noisy features will be suppressed to all zeros. Because the parameters of the model for predicting destination CUs and those for predicting durations are concatenated in Θ, the regularizer ensures that the useful features are shared via the two models. Such a feature selection strategy is also be used in [24], [25]. In summary, we learn our discriminative point process model via solving the following optimization problem:\nmin Θ\nL(Θ) + γ‖Θ‖1,2, (7)\nwhere γ ≥ 0 is the weight controlling the significance of regularizer. Recalling the formula of conditional intensity function in Eq. (4), we can easily find that Eq. (7) corresponds to a problem like multinomial logistic regression with group-lasso regularization [15]. From the viewpoint of Bayesian inference, the loss function L(Θ) corresponds to the negative log-likelihood function of Θ given a series of samples, and the group-lasso regularizer imposes a structural prior distribution on Θ [26], [27] such that the prior probability p(Θ) ∝ exp(−γ ∑M m=1 ‖Θm‖2).\nWe apply the idea of alternating direction method of multipliers (ADMM) [14] to convert the optimization problem to several sub-problems that are easier to solve. Specifically, by introducing an auxiliary variable X and a dual variable Y , we obtain the augmented Lagrangian of Eq. (7) as follows:\nmin Θ\nL(Θ) + γ‖X‖1,2 + ρtr(Y >(Θ−X)) + ρ\n2 ‖Θ−X‖2F ,\nwhere ρ > 0 is the penalty parameter. It mainly controls the convergence of ADMM algorithm [28]. tr(·) computes the trace of matrix. We solve it via optimizing the following sub-problems iteratively:\nUpdate Θ: In the k-th iteration, we optimize the following problem:\nΘ(k+1) = argmin Θ\nL(Θ) + ρ\n2 ‖Θ−X(k) + Y (k)‖2F .\nApplying gradient descent algorithm, we update Θ as\nΘ(k+1) = Θ(k) − β∇L|Θ(k) − βρ(Θ(k) −X(k) + Y (k)), (8)\nwhere parameter β > 0 is the learning rate for updating parameters. ∇L|Θ(k) is the gradient of loss function L(Θ(k)) given current parameters Θ(k), which is computed as\n∇L| θ (k) c = U∑ u=1 Nu∑ i=1 ( λ u,(k) c (tui−1)∑ c′ λ u,(k) c′ (t u i−1) − 1{cui = c} ) futi−1 ,\n∇L| θ (k) d = U∑ u=1 Nu∑ i=1\n( λ u,(k) d (t\nu i−1)∑\nd′ λ u,(k) d′ (t u i−1)\n− 1{dui = d} ) futi−1 .\nHere λu,(k)c (t) and λ u,(k) d (t) are estimates of conditional intensity functions given current parameters. UpdateX: The optimization problem is a simple linear model with group-lasso penalty [15], [29], [30]:\nX(k+1) = argmin X\nρ 2 ‖Θ(k+1) −X + Y (k)‖2F + γ‖X‖1,2.\nDenote Xm as the m-th row of X . Its subgradient equations are\nρ(Xm − (Θ(k+1)m + Y (k)m )) + γs = 0, (9)\nwhere s = Xm ‖X(k)m ‖2 ifX(k)m 6= 0 and s is a vector with ‖s‖2 < 1 otherwise. The solution of Eq. (9) is\nX̂m =\n( 1 +\nρ\nγ‖X(k)m ‖2\n)−1 (Θ(k+1)m + Y (k) m ),\nand then, X(k+1)m is updated via\nX(k+1)m = { 0, if ‖X̂m − (Θ(k+1)m + Y (k)m )‖2 ≤ γρ , X̂m, otherwise. (10)\nUpdate Y : Y (k+1) = Y (k) + (Θ(k+1) −X(k+1)). (11)\nRepeating the steps above until convergence, we learn the parameter matrix of the model, and obtain p(c|t,Hut ) and p(d|t,Hut ) jointly. In summary, we give the scheme of our learning algorithm in Algorithm 1.\nOur model and algorithm can be viewed as a trade-off between learning joint probability p(c, d|t,Hut ) directly and learning the probabilities of transition and duration (p(c|t,Hut ) and p(d|t,Hut )) independently. On one hand, learning p(c, d|t,Hut ) requires O(CD) parameters, which might lead to the overfitting result. Our model, however, merely requires O(C + D) parameters.\nOn the other hand, although we relax the weak correlation between the transition and the duration to an independence assumption, we do not really learn p(c|t,Hut ) and p(d|t,Hut ) independently. With the help of the group-lasso in Eq. (7), their correlation is preserved to some degree — the group sparsity of parameters is shared via p(c|t,Hut ) and p(d|t,Hut ) and the parameters are updated simultaneously.\nIt should be noted that our discriminative algorithm is not only suitable for mutually-correcting processes. Actually, we can use conditional intensity functions from arbitrary point processes to compute the conditional probabilities in Eq. (5) and the loss function in Eq. (6).\nAlgorithm 1 Discriminative Learning of Mutually-Correcting Processes (DMCP)\nInput: Patient flow {su}Uu=1, parameters γ, ρ, β, error bound = 0.01. Output: Θ. Initialize Θ(0) randomly, X(0) = Θ(0), Y (0) = 0, outer iteration number k = 0 repeat\nInner iteration number l = 0, Θ(k,l) = Θ(k). repeat\nUpdate Θ(k,l+1) via Eq. (8). l = l + 1.\nuntil ‖Θ (k,l)−Θ(k,l−1)‖2 ‖Θ(k,l)‖2\n≤ Θ(k+1) = Θ(k,l). Update X(k+1) via Eq. (10). Update Y (k+1) via Eq. (11). k = k + 1.\nuntil ‖Θ (k)−Θ(k−1)‖2 ‖Θ(k)‖2\n≤ . Θ = Θ(k)."
    }, {
      "heading" : "3.3 Enhancing Robustness to Imbalanced Data",
      "text" : "As aforementioned, the imbalance of the data has a remarkable impact on the overall performance of patient work flow prediction, leading to the poor performance of duration and transition prediction of classes with minority samples (i.e., in the following experiments, the prediction accuracy of destination CUs with only a few patients transferring to CUs like ACU, FICU, TSICU, is relatively lower than other CUs with more patients like CCU, SCRU, MICU, NICU). As the 2-D case in Fig. 4(a) shows, the classifier trained on imbalanced data will focus more on the classification accuracy of the class having sufficient samples while ignore the errors of the class having extremely few samples.\nFor suppressing the negative influence of data imbalance problem, several potential solutions are proposed and analyzed in depth.\nWeighted data. A reason of the low prediction accuracy of the classes with few samples is that these classes are insignificant compared to the the classes with sufficient samples when we optimize the likelihood or loss function of the classifier. A possible way to increase the significance of the classes with few samples is\nadding the weights of the samples in the training phase [31], [32], [33], [34]. Specifically, we can rewrite the likelihood function in\nEq. (6) as − ∑U u=1 ∑Nu i=1 wi log\n( λucu i (tui−1)λ u cu i\n(tui−1)∑ c′ λ u c′ (t u i−1) ∑ d′ λ u d′ (t u i−1)\n) ,\nwhere the weight wi aims at suppressing the imbalance of data. It should be large for the samples in the minor classes and small for those in the major ones (i.e., in our case, counting the number of labels {(c, d)} in the training set, denoted as #{(c, d)}, we calculate wi = 1log(1+#{(c,d)}) if c u i = c and d u i = d). Fig. 4(b) visualizes the weighted data, where the enlarged squares and dots are the samples with large weights. Such a simple method might increase the classification accuracy of the classes with few training samples, while decrease the classification accuracy of the classes with sufficient samples at the same time. Because simply weighting samples may wrongly change the distribution of samples in minor classes, the outliers of the classes are enhanced and wrong boundaries between classes are learned.\nHierarchical data. Instead of learning one multi-class classifier directly with imbalanced data, we can rank classes according to the number of training samples and learn binary classifiers hierarchically [35], [36]. Specifically, in each step, we take the class with the largest number of training samples as “MAJORITY”, and the rest samples as a single class called “MINORITY”. Then, a binary classifier is trained on them and the samples of “MAJORITY” is removed from the training set. Repeating the steps above, we obtain a series of binary classifier from hierarchical data. The principle of this method is re-balancing data via merging minor classes. However, in practice, the merging step may lead the classes to be linear-inseparable, which increase the difficulty of training phase. In this case, as Fig. 4(c) shows, nonlinear binary classifier is required in each step, which relies on more complicated learning algorithm, e.g., kernel-based methods. When training linear classifier insistently, the classification accuracy may not be improved.\nSynthetic data. For overcoming the weaknesses of the two methods above, we propose a new method to solve the data imbalance problem. Recalling the classifier trained from weighted data, we can view the weighted data as sampling minor class repeatedly and generating identical samples. Different from directly sampling identical samples, we propose a data synthesis method: for the samples (feature vectors) in a minor class, we synthesize auxiliary samples for the class by sampling each element according to the distribution of corresponding elements of existing samples. Therefore, the auxiliary samples are similar but not identical to original ones. Supplementing these auxiliary samples to the minor classes as training samples4, as shown in Fig. 4(d), we can enhance the robustness of the learning method to imbalanced data.\nOur data synthesis method is actually based on an assumption that the dimensions of feature are independent with each other. As long as the assumption is held by original data, our method can guarantee that the auxiliary samples yields to the distribution of original data. On the contrary, the two competitors mentioned above change the distribution of data: the weighted data implicitly increases the probability of those samples in minor classes; the hierarchical data also changes the distribution of minor classes in each step. As a result, the models learned based on the data generated via these two methods have higher risk of model misspecification. In the following experiments, we will show that applying our data synthesis method as a pre-processing in the\n4. The numbers of samples in different classes are equal after the preprocessing.\ntraining phase, we can enhance the robustness of learning method and obtain superior testing results to its competitors."
    }, {
      "heading" : "3.4 Patient Flow Prediction",
      "text" : "Given learned model Θ, we can predict patient flow for each patient u simply. Specifically, given historical record Hti−1 , we compute p(c|tui−1,Huti−1) and p(d|t u i−1,Huti−1) for c ∈ C and d ∈ D, respectively. The predicts of cui and dui are given as\nĉui = argmax c∈C p(c|tui−1,Huti−1), d̂ui = argmax d∈D p(d|tui−1,Huti−1)."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "4.1 Baselines and Evaluations",
      "text" : "Although there is no existing method proposed to predict patient flow based on a large amount of EHRs, we consider several alternatives that can be potentially adapted to solve our problem. These potential methods are designed for modeling transition processes in discrete or continuous time. Taking these methods as baselines, we compare our method (DMCP) with them and demonstrate its superiority.\nMarkov chain (MC). Taking C CUs and D duration days as states, the simplest method is treating the event sequences as two independent Markov chains for the transition and the duration, respectively. Two one-order MCs are trained, whose transition matrices are calculated via counting the transitions among various states. In the prediction phase, given initial state (i.e., current CU and previous duration time), we use the transition matrices to predict next states (i.e., current duration time and next CU).\nVector auto-regressive model (VAR). Similar to the MC model, the VAR model used in this paper also captures the transitions among CUs and the durations in CUs as two independent transition processes, whose transition matrices are learned via the method in [11]. Different from the MC model, the transition matrix of the VAR model does not have probabilistic interpretation but is more flexible.\nContinuous-time Markov Chain (CTMC). The CTMC [13], as a special type of semi-Markov model [37], also models the transition among CUs as a markov process. In this application, the transition process among CUs is modeled as a Markov chain in continuous time, whose transition probability is time-varying. In the prediction phase, the destination CU is predicted according to previous CU and current transition matrix, and the duration in current CU is predicted via the interval between adjacent transitions.\nLogistic regression (LR). Using the feature extracted from EHRs, we can treat the prediction of CU patient flow as a classification problem. Specifically, two multi-class classifiers are trained independently via multinomial logistic regression (or called softmax regression) for destination CUs and duration days, respectively. In the training set, for each label cui (or d u i ), the feature is [fu>0 ,f u> i ] >.\nHawkes processes (HP). Taking the transitions among CUs as event sequences, the parametric Hawkes process model [21] is implemented, where the conditional intensity function is shown in Table 3. Different from our method, the Hawkes process is learned in a generative way — the likelihood of the whole event sequence is maximized via the maximum likelihood estimator (MLE), i.e., maxΘ ∏ u ∏ i p(c u i , d u i |Hut )(1−P (Tu)). In the prediction phase,\ngiven historical record Hut , we compute the intensity of CUs, λuc (t), is computed in the time interval [t, t+D], the predictions of next event (c, d) are obtained via max(c,d)∈C×D ∫ t+d t+d−1 λ u c (s)ds.\nModulated Poisson processes (MPP). The MPP method replaces our mutually-correcting process with the modulated Poisson process shown in Table 3. In the learning phase, the multinomial logistic regression is applied as we did while the grouplasso is not considered. From the viewpoint of methodology, this method can be viewed as a point process-based interpretation of the generalized logit model of Markov chain in [10].\nSelf-correcting process (SCP). Similar to the MPP method, the SCP method replaces our mutually-correcting process with the self-correcting process shown in Table 3. In the learning phase, the multinomial logistic regression is applied as we did while the group-lasso is not considered.\nThe baselines above can be categorized into three classes: the MC, VAR, and CTMC methods are feature-independent, which merely rely on temporal information; the LR is historyindependent, which merely relies on the EHR-based feature generated at current time while ignores historical record; the HP, MPP, SCP, including our DMCP are point process-based methods. Specifically, the MPP, SCP, and our DMCP can be viewed as extensions of the LR method, which merge current features with historical ones via various point process models. Additionally, our method is the only one introducing group-lasso into learning algorithm.\nFor evaluating the significance and the performance of preprocessing of imbalanced data, we consider our DMCP method with various pre-processing methods, including the weighted data+DMCP (WDMCP), the hierarchical data+DMCP (HDMCP), and the proposed synthetic data+DMCP (SDMCP). The SCP with synthetic data SSCP is also tested to prove the universality of our pre-processing method.\nUsing the proposed data representation method, we can extract a large amount of feature-label pairs from event sequences, e.g., (futi−1 , c u i , d u i ), where f u ti−1 is the feature of patient u containing her historical information before time ti−1, cui is her destination CU after ti−1, and dui is the duration time in c u i accordingly. Given all these pairs, we train and test all the methods via 10-fold cross validation. Specifically, we use 90% of the data for training and the remaining 10% for testing randomly. The training data is further divided into 10 folds. For each method, its model is trained via 10 trials. In each trial, the 9-fold data is used to train the model while the rest is for validation. The final model is the average of 10 training results.\nFor evaluating various methods comprehensively, we apply the following measurements:\nPrediction accuracy: The prediction accuracy ACc for each CU c and the overall accuracy ACC are calculated as\nACc = #{right prediction} #{transitions to c} , ACC = C∑ c=1 #{transitions to c} #{total transitions} ACc.\nThe prediction accuracy ACd for each duration category d and the overall accuracy ACD are calculated in the same way.\nRelative simulation error: Given trained model, we can simulate patient flow following existing data. Specifically, given historical patient data, we simulate the daily number of patients in each CU within the following week. The relative simulation error\nof patient flow Errc for each CU c and the overall relative error ErrC are calculated as\nErrc = 1\n7 7∑ d=1 |Nc,d − N̂c,d| Nc,d , ErrC = 1 7 7∑ d=1 |Nd − N̂d| Nd ,\nwhere Nc,d (Nd) is the real number of patient in each CU (all CUs) in the d-th day, and N̂c,d (N̂d) is the simulation result.\nIt should be noted that we also try to learn joint probability p(c, d|t,Hut ) directly. As we analyzed in the end of section 3.3, such a method will lead to serious over-fitting problem — even on the pre-processed data, the prediction accuracy for each (c, d) pair is no more than 0.31, and the simulation error is larger than 0.45. Compared with the result of our method shown below, the performance is too bad to be applicable.\nThe robustness of algorithm to parameters: The influences of parameters on our learning algorithm are investigated. Specifically, we give a strategy for selecting learning rate β and analyze the function of the bandwidth of Gaussian kernel σ in our mutually-correcting process model. The weight of regularizer γ and the weight of augmented Lagrangian ρ are also analyzed."
    }, {
      "heading" : "4.2 Comparison Results",
      "text" : "We compare our DMCP method with other competitors on predicting destination CUs and duration days in current CUs, and simulating the dynamics of patient flow. The prediction results are shown in Fig. 5, and the relative simulation errors are shown in\nFig. 6. The numerical results of overall prediction accuracy and simulation error are shown in Tables 4, 5, and 6. Experimental results of these three tasks show that our DMCP method obtains superior results in most situations and outperforms other methods. Furthermore, adding proposed data synthesis method as the preprocessing of training data, our SDMCP method further improves the testing results. Specifically, we can find that:\n1) According to Fig. 5, Fig. 6, and Tables 4, 5, we can find that our DMCP methods obtain the highest overall prediction accuracy and the lowest simulation error. Compared with the second best methods, i.e., the HP for predicting destination CUs and the MPP for predicting duration days, our DMCP achieves improvements over 4% and 11% respectively. The encouraging results demonstrate that our mutually-correcting process model is suitable for describing patient flow.\n2) The feature-independent methods (MC, VAR and CTMC) perform poorly in all three tasks. Because of the imbalance of data, there are insufficient transition processes involving those rarelyused CUs. For these CUs, the transition probabilities learned via MC and CTMC and the transition coefficients learned via VAR are unreliable. For example, in Fig. 5(a), we can find that these methods only obtain high accuracy for general ward because it is contained via most patients’ transition processes. For other CUs, however, the prediction accuracy is almost zero in most situations. Similar phenomenon can also be observed in the prediction results of duration days — only the 1-day situation is predicted with high accuracy while the rest situations cannot be predicted.\n3) Compared with feature-independent methods, the LR method improves the testing results greatly, which demonstrates the importance of EHR-based features for predicting patient flow. Applying EHR-based features suppresses the negative influence caused by imbalanced data and improves the prediction results of the classes having insufficient samples. Specifically, in Fig. 5 we can find that LR outperforms MC, VAR, and CTMC in most situations, whose overall accuracy is improved over 20% in both prediction tasks.\n4) The point process-based methods (HP, MPP, SCP, and our DMCP) further improve the prediction accuracy for both two learning tasks because of considering the temporal influences of historical features on current predictions. Specifically, the HP method trains a Hawkes process model in a generative way, and the joint probability p(c, d, t|Hut ) is estimated. However, as aforementioned, such a generative learning method is sensitive to the insufficiency and imbalance of data. As a result, the predictive\nmodel does not work when it comes to predict the classes having few samples, i.e., ACU, FICU, and TSICU in Fig. 5(a), and the duration with 7-day in Fig. 5(b). On the contrary, the discriminative learning methods (MPP, SCP and our DMCP) are more robust, which improves prediction results in most situations, especially the classes having few samples.\n5) Adding suitable pre-processing in the training phase indeed enhances the robustness of our DMCP method to imbalanced data and improves the testing results. In Tables 4, 5, and 6, we can find that because of the weaknesses analyzed in Section 3.3, WDMCP and HDMCP are slightly inferior to original DMCP method. Fig. 5 illustrates the reason obviously: while the prediction accuracy for those minor classes, i.e., the ACU, FICU in Fig. 5(a) and the 4-day in Fig. 5(b), is improved, the performance on major classes degrades more, i.e., the CSRU, NICU in Fig. 5(a) and the 2-and 3-day in Fig. 5(b). For WDMCP, increasing the weight of some training samples, especially for some minority sample classes with only a few samples, may lead to over-fitting of the classifier, so the performance is bad when it comes to the testing set that may only have a slight difference from the training set. For HDMCP, the performance is degraded because the linearinseparable property of MINORITY class in each step increases\nthe difficulty of training. The proposed SDCMP, on the contrary, improves the result of minor classes and avoids the degradation of the result of major classes jointly, which obtains even better results than original DMCP — both the ACC and ACD increase over 3% and the ErrC is reduced to 0.183.\nAdditionally, all the methods above are stable with the change of training data. In the case of Using 10-fold cross validation, the fluctuations of their testing results are all within ±0.01."
    }, {
      "heading" : "4.3 Feature Selection Result",
      "text" : "As aforementioned, our method achieves feature selection via group lasso. Treating each dimension of feature as a group, we measure the importance of each group via the amplitude of the coefficient associated with the group, denoted as |Θm|. The large amplitude means that the change of feature corresponding to the coefficient has a large influence on the prediction result. Specifically, when the coefficient is zero, it means that the corresponding feature does not change the conditional intensity function, and therefore, has no influence on the transition to the destination CU and the duration time. When the coefficient is positive, it means that the corresponding feature will increase the conditional intensity function. Such a feature (profile, treatment,\nnursing operation, or medication) increases the probability that transiting patients to certain CUs and staying certain days. On the contrary, when the coefficient is negative, the corresponding feature decreases the probability of certain transition events.\nFigs. 7(a) and 7(b) visualize the coefficients in different feature domains w.r.t. various learning tasks. We can find that most of the time-varying features related to treatments are selected via at least one learning task while the time-invariant features (personal profile) and the time-varying features related to nursing programs and medications focused on certain parts. Another interesting observation is that many features have negative coefficients. It means that these features suppress the transitions among CUs and lengthen the duration in current CU. We think these phenomena are reasonable based on the following reasons. 1) The treatments are the most influential factors for the patient flow, whose progresses and feedbacks impact on the transitions between CUs and the durations in them greatly. Therefore, it is natural that most of features in this domain are with large parameters. 2) The features across different dimensions in the personal profile domain are likely to be correlated with each other, i.e., a certain disease’s diagnose is correlated with patient’s age and gender. Therefore, only a part of features in this domain are selected. 3) Similarly, nursing programs and medications are highly correlated with the treatments. When most of features related to treatments are selected, only a part of them are useful. 4) Some diseases and corresponding treatments require patients to stay at certain CUs for a long time. When the treatments, nursing operations, or medications happen, the patients are unlikely to transit to other CUs in few days."
    }, {
      "heading" : "4.4 Impacts of Parameters",
      "text" : "The parameters in our method are the learning rate of gradient descent β, the bandwidth of Gaussian kernel σ in our mutuallycorrecting process model, the weight of group-lasso γ, and the weight of augmented Lagrangian ρ. The learning rate β controls the step length of gradient descent. Too large β will lead our algorithm to be unstable while too small β will lead our algorithm to converge too slowly. Following the work in [38], we set the learning rate β decays with rate O(k−1), where k is the number of iteration. Its initial value for our work is set as 10−4.\nThe parameter σ controls the importance of historical EHRbased features. When σ is large, the kernel exp(− (t−t ′)2\nσ2 ) decays slowly, which means the temporal influence of historical events will exist for a long time. In an extreme case that σ → ∞, the kernel will tend to be 1, and our mutually-correcting process model will ignore the temporal difference among historical events and degrade to a self-correcting process. On the contrary, when σ is small, the kernel decays rapidly and the influence of historical events will be short. In the case that σ → 0, our model will only consider the feature at current time and our learning algorithm will be similar to the LR method mentioned above. For achieving a trade-off, we set σ as the mean of duration days in our work.\nWe also investigate the robustness of our method to the changes of γ and ρ. The parameter γ controls the importance of group-lasso. In the case that the features of data indeed yield to the assumption of group sparsity, a suitable γ will regularize model well and improve the result of feature selection, while too large or too small ρ will cause the misspecification of model. Fig. 8(a) gives the overall AC of our method w.r.t. the change of γ. We can find that the learning result is relatively stable in a wide range of\nγ and the result corresponding to γ = 1 is slightly better than others.\nThe parameter ρ reflects the importance of augmented Lagrangian. It mainly controls the convergence rate of ADMM algorithm [28]. Fig. 8(b) gives the overall AC of our method w.r.t. the change of ρ. We can find that the learning result is very stable in a wide range of ρ. A slightly degradation happens when we set a large ρ. In such a situation, the step length in Eq. (8) will be too large and cause the oscillatory updating around the optimal point.\nIn summary, our SDMCP method is robust to these two parameters. According to Fig. 8, we set γ = 1 and ρ = 1 empirically."
    }, {
      "heading" : "5 RELATED WORK",
      "text" : ""
    }, {
      "heading" : "5.1 EHR and Feature Representation",
      "text" : "A typical electronic health record consists of a patient’s profile (i.e., gender, age), her diagnose of certain diseases (i.e., ICD code), and her treatment process, e.g., medications, nursing information, the transitions and durations in various care units. An important application driven via EHRs is extracting characteristic features of physiology in clinical data, or called phenotyping [39]. In [40], the temporal phenotyping from EHRs is achieved by a graph-based model, where a temporal graph of patients’ events (i.e., diagnoses and treatments of diseases) is constructed and phenotypes are extracted via decomposing the adjacent matrix of the graph. In [41], a binary tensor indicating patients’ diagnose and the medications they used is given and phenotypes are extracted via non-negative factorization of the tensor with sparse constraints. In [42], the deep computational phenotyping is achieved via stacked autoencoder. All these works can be viewed as feature extraction methods for EHRs. The feature obtained via these works can be further applied to other problems like constructing disease network [43] and modeling patient flow [44] that we care in this paper."
    }, {
      "heading" : "5.2 Patient Flow and Traditional Models",
      "text" : "Many patient flow models based on EHRs have been proposed for recent years. The early work in [45] models patient flows from a viewpoint of treatment processes and proves that the treatment clustering information helps to model patient flow in emergency departments indeed. Following this strategy, the information of patients’ treatment types is used to estimate the crowdedness of emergency departments in [46]. For example, the workflow of emergency departments is modeled based on the features extracted\nfrom patients’ EHRs in [47] and the work is further specialized for pediatric asthma patients in [44]. Additionally, the visualization and analysis of patient flow are achieved jointly in [48], [49] based on patients’ EHRs. Most of methods above are based on EHRs formulated as time series. Many traditional models, such as Markov chain (MC) model [10], vector auto-regressive (VAR) model [11], [50], [51] and hidden Markov model (HMM) [12], [52], [53], can be used to model patients’ transition processes among different states. However, the works above mainly focus on modeling the flow of patients having a certain kind of diseases from discrete time series or aggregate data. None of them attempt to model general patient flow in continuous time."
    }, {
      "heading" : "5.3 Continuous-time Models",
      "text" : "Recently, many efforts have been made to extend the models above from discrete time domain to continuous one. The continuous-time Markov chain (CTMC) is proposed in [54] to model the Markov chain in continuous time domain, which can be viewed as a special case of semi-Markov models [37]. Similarly, a hidden Markov model in continuous time domain is proposed in [55]. Focusing on e-health related applications, these continuous-time models have been widely used to analyze EHRs. For example, in [43], [56], Hawkes process-based models are proposed to capture the\ntemporal triggering patterns between diseases. A continuous-time HMM is proposed in [55] to model the progression of diseases.\nPoint processes are a kind of classic tools for modeling continuous-time event sequences [16]. Many different point processes have been proposed for various applications, e.g., the Hawkes processes for social network modeling [21], [57], [58] and information system analysis [18], [22], and the self-correcting processes for earthquake prediction [23], [59] and vision perception model [19]. An advantage of these point process models is considering the influence of all historical events on current one, which make these models outperform traditional low-order Markovian models. Recently, some works start to apply point process-based model to analyze EHRs for health information systems [56], [60].\nAs aforementioned, it is surprising that very few works make attempts to model and predict patient flow via a continuoustime model. Additionally, from the viewpoint of methodology, all the methods above are generative. The joint distribution of all transitions in the continuous domain are learned via the maximum likelihood estimator. However, because of the following two reasons, sometimes it is necessary for us to propose a discriminative model. One reason is in some learning tasks, e.g., predicting future transitions, we care more about the conditional probability of\ncurrent transition given historical transitions rather than the joint probability of all transition events. The other is facing sparse or imbalanced data, learning a generative model may suffer to serious over-fitting problem. Unfortunately, the discriminative learning methods for continuous-time models like point processes are not explored in depth."
    }, {
      "heading" : "5.4 Imbalanced Data Processing",
      "text" : "Many methods have been proposed to learn models from imbalanced data. Generally, these methods can be categorized into two classes. One kind of the methods is merging minor classes together and learning binary classifiers step-by-step [35], [36]. Another is weighting training samples to re-balance data [31], [32], [34], where the samples in the minor classes have large weights while those in the major ones have small weights. This kind of methods are extended recently in [33]. The weights are added to unlabeled samples when training logistic regression, which can be viewed as the prior knowledge of model. More recently, the imbalanced data processing methods based on auxiliary samples are proposed. In [61], a classifier based on semi-supervised dictionary learning is proposed for the classes with extremely few samples. Unlabeled samples are used as auxiliary samples in the training phase and added to minor classes adaptively. Focusing on the problem of data synthesis, auxiliary data is generated based on the manifold learning in [62], [63]. However, they do not consider the data imbalance problem in the classification task."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "Focusing on predicting patient flow, we propose a novel mutuallycorrecting process model and its discriminative learning algorithm in this paper. Our mutually-correcting process model improves the flexibility of existing parametric point process models, which reflects the properties of patient flow. The proposed discriminative learning algorithm combines multinomial logistic regression with group-lasso, and achieves feature selection during learning model. We also consider the data imbalance problem in the real-world dataset and propose a novel pre-processing method for training samples, which greatly improves the learning result. Compared with the state-of-art methods, our method obtains superior prediction results on real-world data set, which has potential to predict overcrowdedness or conflicted usage of CUs in practical situations. Our method is applicable to modeling a patient’s need for various “care teams” within the CU (critical care nurses, a pharmacist, a nutritionist, respiratory therapists, consultants, social workers and case managers, clergy, etc), which will further improve care management and coordination for patients with multiple chronic conditions.\nIt should be noted that the proposed work is a first step towards our goal that predicting and managing patient flow. Many problems are not completely solved, which will be our future work. For example, although our method is superior to other competitors in most situations, we can find that for the transitions and the duration days happening with low frequency, the prediction results obtained by our method are still unsatisfying. It means that the robustness problem to imbalanced data is still not completely solved, which is one direction of our future research work. Another problem is the prediction accuracy of duration time. Currently, we can merely predict the duration time accurate to “day”, which is too coarse for practical situations. In the future, we will make efforts to extend our methodology and further improve the prediction accuracy of\nduration time.Additionally, we also plan to extend our mutuallycorrecting process to a nonparametric model."
    }, {
      "heading" : "7 ACKNOWLEDGMENT",
      "text" : "This work is supported in part by NIH/NSF BIGDATA R01 GM108341, NSF IIS-1639792, NSF DMS-1620345, and NIH early career development award in biomedical big data science (1K01ES025445-01A1)."
    } ],
    "references" : [ {
      "title" : "Medicine Committee on the Future of Emergency Care in the US Health System et al., “The future of emergency care in the united states health system.",
      "author" : [ "I. of" ],
      "venue" : "Annals of emergency medicine,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Emergency department overcrowding in the united states: an emerging threat to patient safety and public health",
      "author" : [ "S. Trzeciak", "E. Rivers" ],
      "venue" : "Emergency medicine journal, vol. 20, no. 5, pp. 402–405, 2003.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Managing emergency department overcrowding",
      "author" : [ "J.S. Olshaker" ],
      "venue" : "Emergency medicine clinics of North America, vol. 27, no. 4, pp. 593–603, 2009.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "There s no place like home: Boarding surgical icu patients in other icus and the effect of distances from the home unit",
      "author" : [ "J.L. Pascual", "N.W. Blank", "D.N. Holena", "M.P. Robertson", "M. Diop", "S.R. Allen", "N.D. Martin", "B.A. Kohl", "C.A. Sims", "C.W. Schwab" ],
      "venue" : "The journal of trauma and acute care surgery, vol. 76, no. 4, p. 1096, 2014.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Complications during intrahospital transport of critically ill patients: Focus on risk identification and prevention",
      "author" : [ "P.H. Knight", "N. Maheshwari", "J. Hussain", "M. Scholl", "M. Hughes", "T.J. Papadimos", "W.A. Guo", "J. Cipolla", "S.P. Stawicki", "N. Latchana" ],
      "venue" : "International Journal of Critical Illness and Injury Science, vol. 5, no. 4, p. 256, 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Impact of delayed transfer of critically ill patients from the emergency department to the intensive care unit",
      "author" : [ "D.B. Chalfin", "S. Trzeciak", "A. Likourezos", "B.M. Baumann", "R.P. Dellinger", "D.-E. study group" ],
      "venue" : "Critical care medicine, vol. 35, no. 6, pp. 1477–1483, 2007.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The postanaesthesia care unit as a temporary admission location due to intensive care and ward overflow",
      "author" : [ "A. Ziser", "M. Alkobi", "R. Markovits", "B. Rozenberg" ],
      "venue" : "British journal of anaesthesia, vol. 88, no. 4, pp. 577–579, 2002.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "System-level planning, coordination, and communication: care of the critically ill and injured during pandemics and disasters: Chest consensus statement",
      "author" : [ "J.R. Dichter", "R.K. Kanter", "D. Dries", "V. Luyckx", "M.L. Lim", "J. Wilgis", "M.R. Anderson", "B. Sarani", "N. Hupert", "R. Mutter" ],
      "venue" : "CHEST Journal, vol. 146, no. 4 suppl, pp. e87S–e102S, 2014.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Real-time analysis for intensive care: development and deployment of the artemis analytic system",
      "author" : [ "M. Blount", "M.R. Ebling", "J.M. Eklund", "A.G. James", "C. McGregor", "N. Percival", "K.P. Smith", "D. Sow" ],
      "venue" : "Engineering in Medicine and Biology Magazine, IEEE, vol. 29, no. 2, pp. 110–118, 2010.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A multistate markov chain model for longitudinal, categorical quality-of-life data subject to non-ignorable missingness",
      "author" : [ "B.F. Cole", "M. Bonetti", "A.M. Zaslavsky", "R.D. Gelber" ],
      "venue" : "Statistics in Medicine, vol. 24, no. 15, pp. 2317–2334, 2005.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Transition matrix estimation in high dimensional time series",
      "author" : [ "F. Han", "H. Liu" ],
      "venue" : "ICML, 2013, pp. 172–180.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The analysis of hospital infection data using hidden markov models",
      "author" : [ "B. Cooper", "M. Lipsitch" ],
      "venue" : "Biostatistics, vol. 5, no. 2, pp. 223–237, 2004.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Continuous-time Markov chains: An applicationsoriented approach",
      "author" : [ "W.J. Anderson" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "A dual algorithm for the solution of nonlinear variational problems via finite element approximation",
      "author" : [ "D. Gabay", "B. Mercier" ],
      "venue" : "Computers & Mathematics with Applications, vol. 2, no. 1, pp. 17–40, 1976.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "A sparse-group lasso",
      "author" : [ "N. Simon", "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Journal of Computational and Graphical Statistics, vol. 22, no. 2, pp. 231–245, 2013.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Physiobank, physiotoolkit, and physionet components of a new research resource for complex physiologic signals",
      "author" : [ "A.L. Goldberger", "L.A. Amaral", "L. Glass", "J.M. Hausdorff", "P.C. Ivanov", "R.G. Mark", "J.E. Mietus", "G.B. Moody", "C.-K. Peng", "H.E. Stanley" ],
      "venue" : "Circulation, vol. 101, no. 23, pp. e215–e220, 2000.  IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. X, SEPTEMBER 201X  14",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Multitask multi-dimensional hawkes processes for modeling event sequences",
      "author" : [ "D. Luo", "H. Xu", "Y. Zhen", "X. Ning", "H. Zha", "X. Yang", "W. Zhang" ],
      "venue" : "IJCAI. AAAI Press, 2015, pp. 3685–3691.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Trailer generation via a point processbased visual attractiveness model",
      "author" : [ "H. Xu", "Y. Zhen", "H. Zha" ],
      "venue" : "IJCAI. AAAI Press, 2015, pp. 2198–2204.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Variational inference for gaussian process modulated poisson processes",
      "author" : [ "C. Lloyd", "T. Gunter", "M.A. Osborne", "S.J. Roberts" ],
      "venue" : "ICML, 2015, pp. 1814–1822.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning parametric models for social infectivity in multi-dimensional hawkes processes",
      "author" : [ "L. Li", "H. Zha" ],
      "venue" : "AAAI, 2014, pp. 101–107.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On machine learning towards predictive sales pipeline analytics",
      "author" : [ "J. Yan", "C. Zhang", "H. Zha", "M. Gong", "C. Sun", "J. Huang", "S. Chu", "X. Yang" ],
      "venue" : "AAAI, 2015, pp. 1945–1951.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A self-correcting point process",
      "author" : [ "V. Isham", "M. Westcott" ],
      "venue" : "Stochastic Processes and Their Applications, vol. 8, no. 3, pp. 335–347, 1979.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Multi-task feature learning via efficient l2,1-norm minimization",
      "author" : [ "J. Liu", "S. Ji", "J. Ye" ],
      "venue" : "UAI. AUAI Press, 2009, pp. 339–348.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "l2,1-norm regularized discriminative feature selection for unsupervised learning",
      "author" : [ "Y. Yang", "H.T. Shen", "Z. Ma", "Z. Huang", "X. Zhou" ],
      "venue" : "IJCAI. AAAI Press, 2011, pp. 1589–1594.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The bayesian group-lasso for analyzing contingency tables",
      "author" : [ "S. Raman", "T.J. Fuchs", "P.J. Wild", "E. Dahl", "V. Roth" ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009, pp. 881–888.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Bayesian variable selection and estimation for group lasso",
      "author" : [ "X. Xu", "M. Ghosh" ],
      "venue" : "Bayesian Analysis, vol. 10, no. 4, pp. 909–936, 2015.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A general analysis of the convergence of admm",
      "author" : [ "R. Nishihara", "L. Lessard", "B. Recht", "A. Packard", "M.I. Jordan" ],
      "venue" : "arXiv preprint, 2015.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Solving structured sparsity regularization with proximal methods",
      "author" : [ "S. Mosci", "L. Rosasco", "M. Santoro", "A. Verri", "S. Villa" ],
      "venue" : "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2010, pp. 418–433.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Proximal algorithms",
      "author" : [ "N. Parikh", "S.P. Boyd" ],
      "venue" : "Foundations and Trends in optimization, vol. 1, no. 3, pp. 127–239, 2014.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Logistic regression in rare events data",
      "author" : [ "G. King", "L. Zeng" ],
      "venue" : "Political Analysis, pp. 137–163, 2001.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Neighbor-weighted k-nearest neighbor for unbalanced text corpus",
      "author" : [ "S. Tan" ],
      "venue" : "Expert Systems with Applications, vol. 28, no. 4, pp. 667–671, 2005.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Learning with positive and unlabeled examples using weighted logistic regression",
      "author" : [ "W.S. Lee", "B. Liu" ],
      "venue" : "ICML, vol. 3, 2003, pp. 448–455.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Efficient text classification by weighted proximal svm",
      "author" : [ "D. Zhuang", "B. Zhang", "Q. Yang", "J. Yan", "Z. Chen", "Y. Chen" ],
      "venue" : "ICDM. IEEE, 2005, pp. 8–15.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Hierarchical classifier design using mutual information",
      "author" : [ "I.K. Sethi", "G. Sarvarayudu" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, no. 4, pp. 441–445, 1982.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "A fast scoring algorithm for maximum likelihood estimation in unbalanced mixed models with nested random effects",
      "author" : [ "N.T. Longford" ],
      "venue" : "Biometrika, vol. 74, no. 4, pp. 817–827, 1987.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Semimarkov: An r package for parametric estimation in multi-state semi-markov models",
      "author" : [ "A. Król", "P. Saint-Pierre" ],
      "venue" : "Journal of Statistical Software, vol. 66, no. 1, pp. 1–16, 2015.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "No more pesky learning rates",
      "author" : [ "T. Schaul", "S. Zhang", "Y. LeCun" ],
      "venue" : "ICML, 2013, pp. 343–351.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Next-generation phenotyping of electronic health records",
      "author" : [ "G. Hripcsak", "D.J. Albers" ],
      "venue" : "Journal of the American Medical Informatics Association, vol. 20, no. 1, pp. 117–121, 2013.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Temporal phenotyping from longitudinal electronic health records: A graph based framework",
      "author" : [ "C. Liu", "F. Wang", "J. Hu", "H. Xiong" ],
      "venue" : "SIGKDD. ACM, 2015, pp. 705–714.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Rubik: Knowledge guided tensor factorization and completion for health data analytics",
      "author" : [ "Y. Wang", "R. Chen", "J. Ghosh", "J.C. Denny", "A. Kho", "Y. Chen", "B.A. Malin", "J. Sun" ],
      "venue" : "SIGKDD. ACM, 2015, pp. 1265–1274.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep computational phenotyping",
      "author" : [ "Z. Che", "D. Kale", "W. Li", "M.T. Bahadori", "Y. Liu" ],
      "venue" : "SIGKDD. ACM, 2015, pp. 507–516.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Constructing disease network and temporal progression model via context-sensitive hawkes process",
      "author" : [ "E. Choi", "N. Du", "R. Chen", "L. Song", "J. Sun" ],
      "venue" : "ICDM. IEEE, 2015, pp. 721–726.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Characterizing workflow for pediatric asthma patients in emergency departments using electronic health records",
      "author" : [ "M. Ozkaynak", "O. Dziadkowiec", "R. Mistry", "T. Callahan", "Z. He", "S. Deakyne", "E. Tham" ],
      "venue" : "Journal of biomedical informatics, vol. 57, pp. 386–398, 2015.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Facilitating decision support in hospital emergency departments: A process orientd perspective",
      "author" : [ "R. Ceglowski", "L. Churilov", "J. Wasserthiel" ],
      "venue" : "ECIS Proceedings, p. 55, 2005.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "An investigation of emergency department overcrowding using data mining and simulation: a patient treatment type perspective",
      "author" : [ "A.S. Ceglowski" ],
      "venue" : "Ph.D. dissertation, Monash University. Faculty of Business and Economics. Department of Accounting and Finance, 2006.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Patient-centered care requires a patientoriented workflow model",
      "author" : [ "M. Ozkaynak", "P.F. Brennan", "D.A. Hanauer", "S. Johnson", "J. Aarts", "K. Zheng", "S.N. Haque" ],
      "venue" : "Journal of the American Medical Informatics Association, vol. 20, no. e1, pp. e14–e16, 2013.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Outflow: Visualizing patient flow by symptoms and outcome",
      "author" : [ "K. Wongsuphasawat", "D. Gotz" ],
      "venue" : "IEEE VisWeek Workshop on Visual Analytics in Healthcare. American Medical Informatics Association, 2011, pp. 25–28.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Toward automated workflow analysis and visualization in clinical environments",
      "author" : [ "M. Vankipuram", "K. Kahol", "T. Cohen", "V.L. Patel" ],
      "venue" : "Journal of biomedical informatics, vol. 44, no. 3, pp. 432–440, 2011.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Temporal causal modeling with graphical granger methods",
      "author" : [ "A. Arnold", "Y. Liu", "N. Abe" ],
      "venue" : "SIGKDD. ACM, 2007, pp. 66–75.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Toward learning graphical and causal process models",
      "author" : [ "C. Meek" ],
      "venue" : "UAI Workshop Causal Inference: Learning and Prediction, 2014, pp. 43–48.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "An introduction to hidden markov models",
      "author" : [ "L.R. Rabiner", "B.-H. Juang" ],
      "venue" : "ASSP Magazine, IEEE, vol. 3, no. 1, pp. 4–16, 1986.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Combining hidden markov models and latent semantic analysis for topic segmentation and labeling: Method and clinical application",
      "author" : [ "F. Ginter", "H. Suominen", "S. Pyysalo", "T. Salakoski" ],
      "venue" : "International Journal of Medical Informatics, vol. 78, no. 12, pp. e1–e6, 2009.",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Continuous-time markov chains",
      "author" : [ "M. Iannelli", "A. Pugliese" ],
      "venue" : "An Introduction to Mathematical Population Dynamics. Springer, 2014, pp. 329–334.",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Efficient learning of continuous-time hidden markov models for disease progression",
      "author" : [ "Y.-Y. Liu", "S. Li", "F. Li", "L. Song", "J.M. Rehg" ],
      "venue" : "NIPS, 2015, pp. 3599–3607.",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Mining medical records with a klipi multi-dimensional hawkes model",
      "author" : [ "Y. Zhao", "X. Qi", "Z. Liu", "Y. Zhang", "T. Zheng" ],
      "venue" : "SIGKDD Workshop on Health Informatics. ACM, 2015, pp. 1–8.",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Mixture of mutually exciting processes for viral diffusion",
      "author" : [ "S.-H. Yang", "H. Zha" ],
      "venue" : "ICML, 2013, pp. 1–9.",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Seismic: A self-exciting point process model for predicting tweet popularity",
      "author" : [ "Q. Zhao", "M.A. Erdogdu", "H.Y. He", "A. Rajaraman", "J. Leskovec" ],
      "venue" : "SIGKDD. ACM, 2015, pp. 1513–1522.",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Inference for earthquake models: a selfcorrecting model",
      "author" : [ "Y. Ogata", "D. Vere-Jones" ],
      "venue" : "Stochastic processes and their applications, vol. 17, no. 2, pp. 337–347, 1984.",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "A multitask point process predictive model",
      "author" : [ "W. Lian", "R. Henao", "V. Rao", "J. Lucas", "L. Carin" ],
      "venue" : "ICML, 2015, pp. 2030–2038.",
      "citeRegEx" : "60",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Dictionary learning with mutually reinforcing group-graph structures",
      "author" : [ "H. Xu", "L. Yu", "D. Luo", "H. Zha", "Y. Xu" ],
      "venue" : "AAAI, 2015, pp. 3101– 3107.",
      "citeRegEx" : "61",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Manifold based face synthesis from sparse samples",
      "author" : [ "H. Xu", "H. Zha" ],
      "venue" : "ICCV. IEEE, 2013, pp. 2208–2215.",
      "citeRegEx" : "62",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Manifold based dynamic texture synthesis from extremely few samples",
      "author" : [ "H. Xu", "H. Zha", "M. Davenport" ],
      "venue" : "CVPR. IEEE, 2014, pp. 3019–3026.",
      "citeRegEx" : "63",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "R ECENT reports have highlighted an increasing demand for care units in the United States due to an improved life expectancy and a larger aging population [1].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 1,
      "context" : "Patient management and reducing waiting time, particularly in the Emergency Department (ED) [2], [3] and intensive care unit (ICU) [4], [5], is crucially important to improving quality of care, outcomes, and the overall patient satisfaction.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 2,
      "context" : "Patient management and reducing waiting time, particularly in the Emergency Department (ED) [2], [3] and intensive care unit (ICU) [4], [5], is crucially important to improving quality of care, outcomes, and the overall patient satisfaction.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "Patient management and reducing waiting time, particularly in the Emergency Department (ED) [2], [3] and intensive care unit (ICU) [4], [5], is crucially important to improving quality of care, outcomes, and the overall patient satisfaction.",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "Patient management and reducing waiting time, particularly in the Emergency Department (ED) [2], [3] and intensive care unit (ICU) [4], [5], is crucially important to improving quality of care, outcomes, and the overall patient satisfaction.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : "The so-called practice of “patient boarding” refers to temporarily keeping critically-ill patients in their existing hospital location, such as the emergency department or the post anesthesia unit, while awaiting available CU bed [4], which may result in suboptimal care, and increase both length of stay (LOS) and hospital mortality [6], [7].",
      "startOffset" : 230,
      "endOffset" : 233
    }, {
      "referenceID" : 5,
      "context" : "The so-called practice of “patient boarding” refers to temporarily keeping critically-ill patients in their existing hospital location, such as the emergency department or the post anesthesia unit, while awaiting available CU bed [4], which may result in suboptimal care, and increase both length of stay (LOS) and hospital mortality [6], [7].",
      "startOffset" : 334,
      "endOffset" : 337
    }, {
      "referenceID" : 6,
      "context" : "The so-called practice of “patient boarding” refers to temporarily keeping critically-ill patients in their existing hospital location, such as the emergency department or the post anesthesia unit, while awaiting available CU bed [4], which may result in suboptimal care, and increase both length of stay (LOS) and hospital mortality [6], [7].",
      "startOffset" : 339,
      "endOffset" : 342
    }, {
      "referenceID" : 7,
      "context" : "System-level management of medical resources becomes even more critical for large numbers of critically-ill patients in the case of disasters and pandemics [8].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 8,
      "context" : "edu records (EHRs) and real-time streaming analytics [9], much of these factors can be captured and utilized to jointly model flow of patients within many care units.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "Compared with traditional models, such as discrete Markov chain [10], vector auto-regressive model [11] and hidden Markov model [12], which can only deal with time-invariant transition process formulated as discrete time series, our point process model is able to describe time-varying transition processes in continuous time.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 10,
      "context" : "Compared with traditional models, such as discrete Markov chain [10], vector auto-regressive model [11] and hidden Markov model [12], which can only deal with time-invariant transition process formulated as discrete time series, our point process model is able to describe time-varying transition processes in continuous time.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "Compared with traditional models, such as discrete Markov chain [10], vector auto-regressive model [11] and hidden Markov model [12], which can only deal with time-invariant transition process formulated as discrete time series, our point process model is able to describe time-varying transition processes in continuous time.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 12,
      "context" : "Compared with other continuous model, such as the continuous-time Markov chain [13], our model captures the mutually-correcting patterns among states over time using all historical data, which does not need to set the order of model in advance.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : "Leveraging the alternating direction method of multipliers (ADMM) [14] with group-lasso [15], we propose an efficient algorithm to learn the model.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 14,
      "context" : "Leveraging the alternating direction method of multipliers (ADMM) [14] with group-lasso [15], we propose an efficient algorithm to learn the model.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "We focus on the real-world data from MIMIC II database [17], from which 30, 685 patients staying in CUs are selected for training and testing.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : ", modulated Poisson process [10], Hawkes process [18] and self-correcting process [19].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 16,
      "context" : ", modulated Poisson process [10], Hawkes process [18] and self-correcting process [19].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : ", modulated Poisson process [10], Hawkes process [18] and self-correcting process [19].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 18,
      "context" : ", modulated Poisson processes [20], Hawkes processes [21], [22], as Table 3 shows.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : ", modulated Poisson processes [20], Hawkes processes [21], [22], as Table 3 shows.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 20,
      "context" : ", modulated Poisson processes [20], Hawkes processes [21], [22], as Table 3 shows.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : "Our model extends traditional self-correcting process model [23] to multivariate case and further considers the temporal decay of influence from historical record.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 22,
      "context" : "Such a feature selection strategy is also be used in [24], [25].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 23,
      "context" : "Such a feature selection strategy is also be used in [24], [25].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "(7) corresponds to a problem like multinomial logistic regression with group-lasso regularization [15].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 24,
      "context" : "From the viewpoint of Bayesian inference, the loss function L(Θ) corresponds to the negative log-likelihood function of Θ given a series of samples, and the group-lasso regularizer imposes a structural prior distribution on Θ [26], [27] such that the prior probability p(Θ) ∝ exp(−γ ∑M m=1 ‖Θm‖2).",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 25,
      "context" : "From the viewpoint of Bayesian inference, the loss function L(Θ) corresponds to the negative log-likelihood function of Θ given a series of samples, and the group-lasso regularizer imposes a structural prior distribution on Θ [26], [27] such that the prior probability p(Θ) ∝ exp(−γ ∑M m=1 ‖Θm‖2).",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 13,
      "context" : "We apply the idea of alternating direction method of multipliers (ADMM) [14] to convert the optimization problem to several sub-problems that are easier to solve.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : "It mainly controls the convergence of ADMM algorithm [28].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "UpdateX: The optimization problem is a simple linear model with group-lasso penalty [15], [29], [30]:",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 27,
      "context" : "UpdateX: The optimization problem is a simple linear model with group-lasso penalty [15], [29], [30]:",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 28,
      "context" : "UpdateX: The optimization problem is a simple linear model with group-lasso penalty [15], [29], [30]:",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 29,
      "context" : "A possible way to increase the significance of the classes with few samples is adding the weights of the samples in the training phase [31], [32], [33], [34].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 30,
      "context" : "A possible way to increase the significance of the classes with few samples is adding the weights of the samples in the training phase [31], [32], [33], [34].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 31,
      "context" : "A possible way to increase the significance of the classes with few samples is adding the weights of the samples in the training phase [31], [32], [33], [34].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 32,
      "context" : "A possible way to increase the significance of the classes with few samples is adding the weights of the samples in the training phase [31], [32], [33], [34].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 33,
      "context" : "Instead of learning one multi-class classifier directly with imbalanced data, we can rank classes according to the number of training samples and learn binary classifiers hierarchically [35], [36].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 34,
      "context" : "Instead of learning one multi-class classifier directly with imbalanced data, we can rank classes according to the number of training samples and learn binary classifiers hierarchically [35], [36].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 10,
      "context" : "Similar to the MC model, the VAR model used in this paper also captures the transitions among CUs and the durations in CUs as two independent transition processes, whose transition matrices are learned via the method in [11].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 12,
      "context" : "The CTMC [13], as a special type of semi-Markov model [37], also models the transition among CUs as a markov process.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 35,
      "context" : "The CTMC [13], as a special type of semi-Markov model [37], also models the transition among CUs as a markov process.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 19,
      "context" : "Taking the transitions among CUs as event sequences, the parametric Hawkes process model [21] is implemented, where the conditional intensity function is shown in Table 3.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : "From the viewpoint of methodology, this method can be viewed as a point process-based interpretation of the generalized logit model of Markov chain in [10].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 36,
      "context" : "Following the work in [38], we set the learning rate β decays with rate O(k−1), where k is the number of iteration.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 26,
      "context" : "It mainly controls the convergence rate of ADMM algorithm [28].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 37,
      "context" : "An important application driven via EHRs is extracting characteristic features of physiology in clinical data, or called phenotyping [39].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 38,
      "context" : "In [40], the temporal phenotyping from EHRs is achieved by a graph-based model, where a temporal graph of patients’ events (i.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 39,
      "context" : "In [41], a binary tensor indicating patients’ diagnose and the medications they used is given and phenotypes are extracted via non-negative factorization of the tensor with sparse constraints.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 40,
      "context" : "In [42], the deep computational phenotyping is achieved via stacked autoencoder.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 41,
      "context" : "The feature obtained via these works can be further applied to other problems like constructing disease network [43] and modeling patient flow [44] that we care in this paper.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 42,
      "context" : "The feature obtained via these works can be further applied to other problems like constructing disease network [43] and modeling patient flow [44] that we care in this paper.",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 43,
      "context" : "The early work in [45] models patient flows from a viewpoint of treatment processes and proves that the treatment clustering information helps to model patient flow in emergency departments indeed.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 44,
      "context" : "Following this strategy, the information of patients’ treatment types is used to estimate the crowdedness of emergency departments in [46].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 45,
      "context" : "from patients’ EHRs in [47] and the work is further specialized for pediatric asthma patients in [44].",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 42,
      "context" : "from patients’ EHRs in [47] and the work is further specialized for pediatric asthma patients in [44].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 46,
      "context" : "Additionally, the visualization and analysis of patient flow are achieved jointly in [48], [49] based on patients’ EHRs.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 47,
      "context" : "Additionally, the visualization and analysis of patient flow are achieved jointly in [48], [49] based on patients’ EHRs.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "Many traditional models, such as Markov chain (MC) model [10], vector auto-regressive (VAR) model [11], [50], [51] and hidden Markov model (HMM) [12], [52], [53], can be used to model patients’ transition processes among different states.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "Many traditional models, such as Markov chain (MC) model [10], vector auto-regressive (VAR) model [11], [50], [51] and hidden Markov model (HMM) [12], [52], [53], can be used to model patients’ transition processes among different states.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 48,
      "context" : "Many traditional models, such as Markov chain (MC) model [10], vector auto-regressive (VAR) model [11], [50], [51] and hidden Markov model (HMM) [12], [52], [53], can be used to model patients’ transition processes among different states.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 49,
      "context" : "Many traditional models, such as Markov chain (MC) model [10], vector auto-regressive (VAR) model [11], [50], [51] and hidden Markov model (HMM) [12], [52], [53], can be used to model patients’ transition processes among different states.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 11,
      "context" : "Many traditional models, such as Markov chain (MC) model [10], vector auto-regressive (VAR) model [11], [50], [51] and hidden Markov model (HMM) [12], [52], [53], can be used to model patients’ transition processes among different states.",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 50,
      "context" : "Many traditional models, such as Markov chain (MC) model [10], vector auto-regressive (VAR) model [11], [50], [51] and hidden Markov model (HMM) [12], [52], [53], can be used to model patients’ transition processes among different states.",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 51,
      "context" : "Many traditional models, such as Markov chain (MC) model [10], vector auto-regressive (VAR) model [11], [50], [51] and hidden Markov model (HMM) [12], [52], [53], can be used to model patients’ transition processes among different states.",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 52,
      "context" : "The continuous-time Markov chain (CTMC) is proposed in [54] to model the Markov chain in continuous time domain, which can be viewed as a special case of semi-Markov models [37].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 35,
      "context" : "The continuous-time Markov chain (CTMC) is proposed in [54] to model the Markov chain in continuous time domain, which can be viewed as a special case of semi-Markov models [37].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 53,
      "context" : "Similarly, a hidden Markov model in continuous time domain is proposed in [55].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 41,
      "context" : "For example, in [43], [56], Hawkes process-based models are proposed to capture the temporal triggering patterns between diseases.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 54,
      "context" : "For example, in [43], [56], Hawkes process-based models are proposed to capture the temporal triggering patterns between diseases.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 53,
      "context" : "A continuous-time HMM is proposed in [55] to model the progression of diseases.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 19,
      "context" : ", the Hawkes processes for social network modeling [21], [57], [58] and information system analysis [18], [22], and the self-correcting processes for earthquake prediction [23], [59] and vision perception model [19].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 55,
      "context" : ", the Hawkes processes for social network modeling [21], [57], [58] and information system analysis [18], [22], and the self-correcting processes for earthquake prediction [23], [59] and vision perception model [19].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 56,
      "context" : ", the Hawkes processes for social network modeling [21], [57], [58] and information system analysis [18], [22], and the self-correcting processes for earthquake prediction [23], [59] and vision perception model [19].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : ", the Hawkes processes for social network modeling [21], [57], [58] and information system analysis [18], [22], and the self-correcting processes for earthquake prediction [23], [59] and vision perception model [19].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 20,
      "context" : ", the Hawkes processes for social network modeling [21], [57], [58] and information system analysis [18], [22], and the self-correcting processes for earthquake prediction [23], [59] and vision perception model [19].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 21,
      "context" : ", the Hawkes processes for social network modeling [21], [57], [58] and information system analysis [18], [22], and the self-correcting processes for earthquake prediction [23], [59] and vision perception model [19].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 57,
      "context" : ", the Hawkes processes for social network modeling [21], [57], [58] and information system analysis [18], [22], and the self-correcting processes for earthquake prediction [23], [59] and vision perception model [19].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 17,
      "context" : ", the Hawkes processes for social network modeling [21], [57], [58] and information system analysis [18], [22], and the self-correcting processes for earthquake prediction [23], [59] and vision perception model [19].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 54,
      "context" : "Recently, some works start to apply point process-based model to analyze EHRs for health information systems [56], [60].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 58,
      "context" : "Recently, some works start to apply point process-based model to analyze EHRs for health information systems [56], [60].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 33,
      "context" : "One kind of the methods is merging minor classes together and learning binary classifiers step-by-step [35], [36].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 34,
      "context" : "One kind of the methods is merging minor classes together and learning binary classifiers step-by-step [35], [36].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 29,
      "context" : "Another is weighting training samples to re-balance data [31], [32], [34], where the samples in the minor classes have large weights while those in the major ones have small weights.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 30,
      "context" : "Another is weighting training samples to re-balance data [31], [32], [34], where the samples in the minor classes have large weights while those in the major ones have small weights.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 32,
      "context" : "Another is weighting training samples to re-balance data [31], [32], [34], where the samples in the minor classes have large weights while those in the major ones have small weights.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 31,
      "context" : "This kind of methods are extended recently in [33].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 59,
      "context" : "In [61], a classifier based on semi-supervised dictionary learning is proposed for the classes with extremely few samples.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 60,
      "context" : "Focusing on the problem of data synthesis, auxiliary data is generated based on the manifold learning in [62], [63].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 61,
      "context" : "Focusing on the problem of data synthesis, auxiliary data is generated based on the manifold learning in [62], [63].",
      "startOffset" : 111,
      "endOffset" : 115
    } ],
    "year" : 2016,
    "abstractText" : "Over the past decade the rate of care unit (CU) use in the United States has been increasing. With an aging population and ever-growing demand for medical care, effective management of patients’ transitions among different care facilities will prove indispensible for shortening the length of hospital stays, improving patient outcomes, allocating critical care resources, and reducing preventable re-admissions. In this paper, we focus on an important problem of predicting the so-called “patient flow” from longitudinal electronic health records (EHRs), which has not been explored via existing machine learning techniques. By treating a sequence of transition events as a point process, we develop a novel framework for modeling patient flow through various CUs and jointly predicting patients’ destination CUs and duration days. Instead of learning a generative point process model via maximum likelihood estimation, we propose a novel discriminative learning algorithm aiming at improving the prediction of transition events in the case of sparse data. By parameterizing the proposed model as a mutually-correcting process, we formulate the estimation problem via generalized linear models, which lends itself to efficient learning based on alternating direction method of multipliers (ADMM). Furthermore, we achieve simultaneous feature selection and learning by adding a group-lasso regularizer to the ADMM algorithm. Additionally, for suppressing the negative influence of data imbalance on the learning of model, we synthesize auxiliary training data for the classes with extremely few samples, and improve the robustness of our learning method accordingly. Testing on real-world data, we show that our method obtains superior performance in terms of accuracy of predicting the destination CU transition and duration of each CU occupancy.",
    "creator" : "LaTeX with hyperref package"
  }
}