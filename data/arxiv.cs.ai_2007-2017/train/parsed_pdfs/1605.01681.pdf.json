{
  "name" : "1605.01681.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "chaotic time series inspired by the brain emotional learning of mammals. We describe the structure and function of this model, which is referred to as BELPM (Brain Emotional LearningBased Prediction Model). Structurally, the model mimics the connection between the regions of the limbic system, and functionally it uses weighted k nearest neighbors to imitate the roles of those regions. The learning algorithm of BELPM is defined using steepest descent (SD) and the least square estimator (LSE). Two benchmark chaotic time series, Lorenz and Henon, have been used to evaluate the performance of BELPM. The obtained results have been compared with those of other prediction methods. The results show that BELPM has the capability to achieve a reasonable accuracy for long-term prediction of chaotic time series, using a limited amount of training data and a reasonably low computational time.\nIndex Terms—Brain emotional learning, Chaotic time series,\nLong-term prediction, Weighted k-nearest-neighbors.\nI. INTRODUCTION\nrediction models have applications in many different areas of science and technology: business, economics, healthcare, and welfare services. Well-known data-driven methodologies such as neural networks and neuro-fuzzy models have shown reasonable accuracy in the nonlinear prediction of chaotic time series [1]. According to the VapnikChervonenkis (VC) theory [2], a sufficient size of training samples for achieving arbitrary prediction accuracy is proportional to the number of the models’ learning parameters. Thus, a data-driven model with high model complexity and high number of learning parameters requires a large number of training samples to achieve high accuracy in a prediction application. The prediction accuracy of predicting chaotic time series depends on the characteristics of the applied models (i.e., the model complexity and the number of learning parameters) as well as the features of the prediction applications (i.e., the number of training samples, chaos degree, embedding dimension, and the horizon of prediction) [1]–[5].\nThis study suggests a prediction model that could be feasible for long-term prediction of chaotic systems (chaotic time series) with a limited amount of training data samples; i.e., a prediction model with a small number of learning parameters. The suggested model is inspired by the brain\nstands for Brain Emotional Learning-Based Prediction Model. Its simple architecture has been inspired by the brain emotional system, emphasizing the interaction between those parts that have significant roles in emotional learning. The model mimics the emotional learning by merging weighted kNearest Neighbor (Wk-NN) method and adaptive neural networks. The learning algorithm of BELPM is based on the steepest descent (SD) and the least square estimator (LSE). The model aims at continuing the recent studies that have suggested computational models of emotional processing for control and prediction applications [6]-[16].\nThe rest of the paper is organized as follows: Section II gives an introduction of emotional processing in the brain, reviews the anatomical aspects of emotional learning, and gives an overview of related work in the area of computational models of brain emotional learning. Section III describes the BELPM’s architecture and illustrates its learning algorithm. In Section IV, two benchmark chaotic time series, Lorenz and Henon, are used to evaluate the performance of BELPM and the results are compared with the obtained results from other nonlinear learning methods (e.g. Adaptive-Network-Based Fuzzy Inference System (ANFIS) [3], MultiLayer Perceptron (MLP) Network [1], [2], Radial Bias Function (RBF) Networks [1], [2], and Local Linear Neuro-Fuzzy (LLNF) Models [1]). Finally, conclusions about the BELPM model and further improvements to the model are stated in Section V."
    }, {
      "heading" : "II. BACKGROUND",
      "text" : "One of the most challenging topics in machine learning research area is development of high generalization algorithms for accurate prediction of chaotic systems. Recently, bioinspired models, in particular, emotion-based learning models [6], [8], [10], [12]-[14], have shown acceptable generalization capability in modeling and predicting the chaotic behavior of dynamic systems. In fact, this capability is obtained in emotion-based learning models by integrating machine learning algorithms with the computational model of emotional learning. In the following, we explain how emotional learning can be modeled as a computer-based tool and how it can be integrated with learning algorithms. The difference between BELPM and several well-known datadriven methods will also be indicated."
    }, {
      "heading" : "A. Related Works in Modeling Emotional Learning",
      "text" : "Since 1988, emotion and emotional processing have been\nP\nactive research topics for neuroscientists and psychologists. A lot of efforts have been made to analyze emotional behavior and describe emotion on the basis of different hypotheses, e.g., psychological, neurobiological, philosophy, and learning hypothesis. These hypotheses that have contributed to the present computer-based models of emotional processing [18], have imitated certain aspects of emotional learning, and can be classified on the basis of their fundamental theories and applications. For example, a computer-based model that is based on the central theory [18] (i.e., which explains how a primary evaluation of emotional stimuli forms emotional experiences) is called a computational model of emotional learning and imitates the associative learning aspect of emotional processing [18] that is based on fear conditioning [19], [ 20].\nEmotional processing has also been described using different anatomical structures: “MacLean’s limbic system, Cannon’s structure, and Papez circuit” [21]. The first anatomical representation is based on studies on cognitive neuroscience [21], [22], and has been developed on the basis of Cannon’s structure and Papez circuit emphasizing the role of the limbic system (i.e., a group of the brain regions) for emotional processing. The Cannon’s structure suggested that the hypothalamus of the brain plays the most significant role in emotional learning, while Papez circuit [21] emphasized the role of the cingulate cortex in emotional processing. In this study, we have focused on the limbic system, which is the basis of our suggested model."
    }, {
      "heading" : "1) Anatomical Structure of Emotional Learning",
      "text" : "The limbic system is a group of brain regions, which includes the hippocampus, amygdala, thalamus, and sensory cortex [22]-[24]. The roles of the main regions of the limbic system with regard to emotional learning can be summarized as follows:\na) Thalamus receives emotional stimuli and is responsible for the provision of high-level information, i.e., determining the effective values of stimuli [22]-[28]. It then passes the generated signals to the amygdala and sensory cortex [28]. The thalamus includes different parts that process the emotional stimuli separately [27].\nb) Sensory cortex is a part of the sensory area of the brain and is responsible for analysis and processing of the received signals. The sensory cortex distributes its output signals between the amygdala and orbitofrontal region [18]-[21], [27].\nc) Amygdala is the central part of the limbic system of mammals and has a principal role in emotional learning [18]- [26]. The amygdala consists of several parts with different functional roles (see Fig. 1), and it connects through them to other regions of the brain (e.g., the insular cortex, orbital cortex, and frontal lobe). It has connections to the thalamus, orbitofrontal cortex, and hypothalamus [25], [26]. During emotional learning, the amygdala participates in reacting to emotional stimuli, storing emotional responses [29], evaluating positive and negative reinforcement [30], learning the association between unconditioned and conditioned stimuli [19], [20], [31], predicting the association between stimuli and\nfuture reinforcement [31], and forming an association between neutral stimuli and emotionally charged stimuli [30].\nThalamous\nLateral of Amygdala\nBasal of Amygdala\nCentromedial of\nAmygdala\nEmotional Response\nEmotional Stimulus\nOrbitofrontal\nAMYGDALA\nBasolateral of\nAmygdala\nAccesory Basal of\nAmygdala\nFig.1.The parts of amygdala and their pathways. The diagram shows the pattern of fear conditioning, which needs to be clarified [17].\nThe two main parts of the amygdala are the basolateral part (the largest portion of the amygdala) and the centeromedial part. The basolateral part has the bidirectional link to the insular cortex and orbital cortex [18], [20], [21], [25], [26] and performs the main role in mediating memory consolidation [32] and providing the primary response, and is divided into three parts: the lateral, basal, and accessory basal [25], [29]. The lateral is the part through which stimuli enter the amygdala. The lateral region not only passes the stimuli to other regions, but also memorizes them to form the stimulus– response association [31]. This part also takes some roles in spreading the sensor’s information to other parts, forming the association between the conditioned and unconditioned stimuli, inhabiting and reflecting the external stimuli, and memorizing the emotional experiences. The basal and accessory basal parts participate in mediating the contextual conditioning [20], [25].\nThe centeromedial part, which is the main output for the basaloteral part [26], is divided into the central and medial parts [18], [20], [25], [26]. It is responsible for the hormonal aspects of emotional reactions [25] or for mediating the expression of the emotional responses [25], [26].\nd) Orbitofrontal cortex is located close to the amygdala and has a bidirectional connection to the amygdala. This part plays roles in processing stimulus [25], decoding the primary reinforcement, representing the negative reinforcement, and learning the stimulus–reinforcement association. It also evaluates and corrects reward and punishment [18]-[21], [33]- [37], selects goals, makes decisions for a quick response to punishment [18], [23], [25]-[36], and prevents inappropriate responses of the amygdala. The orbitofrontal cortex encompasses two parts, the medial and lateral. The medial part forms and memorizes reinforcement–stimulus association, and also has role in providing responses and monitoring them, whereas the lateral part evaluates the response and provides punishment [37]."
    }, {
      "heading" : "2) Emotion-Inspired Computational Models",
      "text" : "Computational models of emotional learning, which are computer-based models, have been developed to represent the associative learning aspect of emotional processing. From the\napplication perspective, they can be categorized into three groups: emotion-based decision-making model, emotion-based controller, and emotion-based machine-learning approaches.\na) Emotion-based decision-making model: This model is the basis of artificial intelligent (AI) emotional agent that integrates emotional reactions with rational reactions. EMAI (Emotionally Motivated Artificial Intelligence) was one of the first attempts to develop emotion-based agents. It was applied for simulating artificial soccer playing [38], and its results were fairly good. The Cathexis model [39] was another emotional agent developed that reacted to an environment by imitating an emotional decision-making process in humans. The model of the mind [40] was developed as a modular artificial agent to generate emotional behavior for making decisions. An agent architecture that was called Emotionbased Robotic Agent Development (in reverse order, DARE) was developed on the basis of the somatic marker theory; it was tested in a multi-agent system and showed ability in modeling social and emotional behavior [41].\nb) Emotion-based controller: The first practical implementation of an emotion-based controller is BELBIC (Brain Emotional Learning-Based Intelligent Controller) [7]. It was developed on the basis of Moren and Balkenius computational model [7], [23], [42]. The BELBIC has been successfully employed for a number of applications: controlling heating and air conditioning [43] of aerospace launch vehicles [44], intelligent washing machines [45], and trajectory tracking of stepper motor [46]. Another emotionbased intelligent controller is a neuro-fuzzy controller [47], which was integrated with emotion-based performance measurement to tune the parameters of the controller. Application of an emotion-based controller robotics was proposed in [48], which is an interesting example of applying emotional concepts in robotic applications and imitated the reinforcement learning aspect of emotional processing. The results of applying emotion-based controllers have shown that they have the capability to overcome uncertainty and complexity issues of control applications. Specifically, the BELBIC has been proven to outperform others in terms of simplicity, reliability, and stability [7], [39]-[42].\nc) Emotion-based machine-learning approach: Developing machine-learning approaches by imitating emotional processing of the brain has captured the attention of researchers in the AI area. So far, some studies have been carried out to develop new neural networks by imitating some aspects of emotional learning. Hippocampus-neocortex and amygdala hippocampus model have been proposed as neural network models [49], [50]. They combine associative neural network with emotional learning concepts. Several emotionbased prediction models [6], [8]-[15] have been developed to model the complex systems. Most of them are based on the amygdala-orbitofrontal subsystem [23] that was proposed by Moren and Balkenius. They have been applied for different applications, e.g., auroral electrojec (AE) index prediction [9], solar activity prediction [6], [8], [10], [11]-[13], [14], and financial and chaotic time series prediction [6], [8]-[14].\nd) Amygdala-orbitofrontal system\nThe fundamental model of emotion-based machine-learning approaches and emotion-based controllers is the amygdalaorbitofrontal system. It is a type of computational model of emotional learning with a simple structure that has been defined using the theory of the limbic system [23]. Its structure is inherited from some parts of the limbic system (e.g., the amygdala, thalamus, sensory cortex), and imitates the interaction between those parts of the limbic systems and formulates the emotional response using mathematical equations [23]. The amygdala-orbitofrontal subsystem consists of two subsystems: the amygdala and orbitofrontal subsystem. Each subsystem has several linear neurons and receives a feedback (a reward). The model’s output function has been defined as subtracting the orbitofrontal’s response from the amygdala’s response. To update the weights, learning rules are defined for both the amygdala and orbitofrontal subsystem. Due to its simplicity, it has been the basis of most controllers and prediction models inspired by emotional learning."
    }, {
      "heading" : "B. A Brief Overview of the Data- Driven Methods",
      "text" : "One straightforward way to evaluate the data-driven learning approaches (e.g., neural network and neuro-fuzzy models) is to apply them to predict chaotic time series. Different types of neural network and neuro fuzzy models (e.g., RBF, ANFIS, LLNF) have been applied to model and predict the short-term and long-term behavior of chaotic time series (e.g., Lorenz, Henon, Mackey-Glass, Ikeda) [1]-[15], [51]-[55].\nWe have suggested a model that differs from the previously proposed models in terms of prediction accuracy, structural simplicity, and generalization capability. In the following, we explain the differences between BELPM and other wellknown data-driven models.\n1) Radial Bias Function (RBF) differs from BELPM in terms of the underlying structure, inputs of the neurons, connection between neurons, and number of learning parameters and learning algorithms.\n2) Generalization Regression Neural Network (GRNN) [1] differs from BELPM in its number of neurons (i.e., the number of neurons of GRNN are equal to the size of training samples). Moreover, GRNN has no learning algorithm to optimize its performance and increase its generalization capability.\n3) Adaptive Neuro Fuzzy Inference System (ANFIS) and BELPM are not similar because of different structures, functions, and some aspects of learning algorithms. Due to the learning algorithm and the large number of learning parameters (linear and nonlinear) that are spread through the layers, ANFIS has the capability to obtain very accurate results for complex applications. However, its learning algorithm has a significant effect on its computational complexity and it also causes over-fitting problems. The curse of dimensionality is another issue of ANFIS and increases the computational time of ANFIS for high-dimension application. Although the number of learning parameters of BELPM is not dependent on the dimension of input data, as mentioned before, BELPM uses Wk-NN; consequently, the computational time of BELPM only depends on the number of\nneighbors. To decrease its time complexity in high-dimension cases, we can choose the small number of neighbors for the BELPM.\n4) Local Linear Neuro Fuzzy Models (LLNF) and BELPM can both be considered as types of “local modeling” [2] algorithms. They both combine an optimization-learning algorithm and LSE to train the learning parameters. However, LLNF uses Local Linear Model Tree (LoLiMoT) algorithm, instead of Wk-NN method of BELPM. The number of learning parameters of LoLiMoT has a linear relationship with the dimension of input samples and number of epochs; thus, its computational complexity has no exponential growth for high-dimension applications.\n5) Modular neural network is a combination of several modules with different inputs [2] without any connection with others. There is no algorithm to update the learning parameters of the modules.\n6) Hybrid structures that are defined in [1], differ from BELPM in receiving the input data. The submodules of a hybrid structure can also be designed in parallel or series.\nIII. BRAIN EMOTIONAL LEARNING-BASED PREDICTION MODEL\nThe architecture of BELPM is inspired by the brain emotional learning system. It mimics some functional and structural aspects of the limbic system regions. To describe BELPM’s architecture and its underlying learning algorithms, we used the machine-learning terminology, instead of neuroscientific terms. For example, an input–output pair of training data and an input–output pair of test data are equivalent to an unconditioned stimulus–response pair and a conditioned stimulus–response pair in neuro-scientific terminology, respectively. Thus, we used two subscripts u and c to distinguish the training data set and the test data set that are defined as , , 1{ , } N c c c j c j jI ri and , , 1{ , } N u u u j u j jI ri ,\nrespectively, with cN and uN data samples. Before explaining\nthe architecture of BELPM, let us briefly review the W-kNN algorithm that is the basic of BELPM. The following steps\nexplain how the output value of an input vector testi is calculated by using the W-kNN algorithm [56]:\n1) Calculate the Euclidean distance as, j test u , j 2 d i i\nfor each u , ji that is a member of the training data set\nu , , ...,u ,1 u ,2 u ,N{ }i i i .\n2) Determine the k minimum values of 1 2 N u d d d,{ , ..., }d\nas mind . The data samples corresponding to mind are shown as\nk\nmin min, j min, j j 1{ ,r }I i , where minI denotes the samples of\nthe training data set that are nearest neighbors to the test sample, testi . 3) Calculate the output of testi as given in (1).\nk k\ntest j min, j j j 1 j 1\nr w r w( / ) (1)\nThe weight, jw , is calculated as j jw K( d ) , where K(.) is\nknown as the kernel function that makes the transition from Euclidean distances to the weights. Any arbitrary function that holds the following properties given can be considered as the kernel function [56],[57 ]:\n1) For all d , K( d ) 0 .\n2) If d 0 , then K( d ) gets the maximum value. 3) If d , then K( d ) gets the minimum value. Some typical kernel functions are the Gaussian kernel (2), Inversion kernel (3), and the weighted kernel function that is defined as (4). 2\n1 d K( d ) exp( )\n22 (2)\n1 K( d )\nd (3)\njmax( ) ( d min( )) K( d )\nmax( )\nd d\nd (4)\nAs mentioned earlier, they transform the Euclidean distances into the weights; thus, the neighbors that are closer\nto the test sample testi have higher weights on estimating the\noutput, testr [56],[57]."
    }, {
      "heading" : "A. Architecture of BELPM",
      "text" : "As shown in Fig. 2, the BELPM’s architecture consists of four main parts that are named as TH, CX, AMYG, and ORBI, and are referred to as THalamus, sensory CorteX, AMYGdala, and ORBItofrontal, respectively.\nMAX_MIN\nAGG\n,\n_\nu j\nMAX MIN th\nOuput\n(Unconditioned\nresponse )\nju,i ja ,p\nju, s\nCX\nLOMO\nBL CM\njo,p\nj,o re ja ,p\nORBI\nAMYG TH\n(a)During first learning Phase\nj rja ,p\nj,a r\n,u j\nAGG th\nInput\n(Unconditioned\nStimulus)\nMAX_MIN\nAGG\n,\n_\nc j\nMAX MIN th\nOuput\n(Conditioned\nresponse )\nc, ji ja ,p\nc, js CX LOMO\nBL CM\njo,p\nj,o re ja ,p\nORBI\nAMYG TH (a)During second learning\nPhase\nja ,p j,a\nr\n,c j\nAGG th\nInput\n(Conditioned\nStimulus)\nFig.2. The architecture of BELPM showing the structure of each part and its connection to other parts. (a) An input from training set, unconditioned stimulus, enters the BELPM. (b) An input of test data, conditioned stimulus, enters the BELPM.\nLet us assume an unseen input c, j cIi enters BELPM that provides the corresponding output using the following steps:\n1) The input vector c , ji is fed to TH, which is the entrance\npart of the BELPM structure. This part consists of two components: MAX_MIN (MAXimum_MINimum) and AGG\n(AGGregation). The MAX_MIN can be described as a modular neural network. It has two neural networks, each of which has two layers with a competitive function for the neuron of the first layer and a linear function for the neuron of the second layer. The output of MAX_MIN that is referred to as _MAX MIN\nc, j th is calculated according to (5) and is fed to AGG\nand AMYG. Equation (5) calculates the highest and lowest values of the input vector with R dimensions. The AGG can be described as a neural network with R 2 linear neurons ( R is the dimension of c , ji ); the output of AGG, AGG c, j th , is equal to\nc , ji and is fed to CX as shown in (6).\nMAX _ MIN\nc, j c, j[ Max( ),Min( )]c, jth i i (5)\n,[ ] AGG c jic, jth (6)\n2) The AGG\nc, j th is sent to CX, which is a pre-trained neural\nnetwork with one layer of linear function. The role of CX is to\nprovide c , js and distribute it between AMYG and ORBI. It\nshould be noted that c , ji and c , js have the same entity; however,\nthey have been originated from different parts.\n3) Both c , js and _MAX MIN c, j th are sent to AMYG that is the\nmain part of the BELPM structure and is divided into two components: BL(BasoLateral) and CM (CenteroMedial). The subpart that is referred to as BL corresponds to the set of the lateral and basal, while the other subpart, CM, corresponds to the accessory basal and centromedial part. There is a bidirectional connection between AMYG and ORBI; this connection is utilized to exchange the information, and contains AMYG’s expected punishment and ORBI’s response. The functionalities of AMYG have been defined to mimic some roles of the amygdala (e.g., storing unconditioned stimulus–response pairs, making the association between the conditioned and unconditioned stimuli, and generating reward and punishment). Thus, AMYG has a main role in providing the primary and final responses. The structure and function of BL and CM are given as follows: a) BL, which is responsible for the provision of the primary\nresponse of the AMYG, calculates a,i u , j c, j u , j c, jd - 2 2 s s + th - th . Here, u , js and _MAX MIN u, j th are the output of TH and CX for each\nmember of the training data set uu,1 u,2 u,N { , ,.., }i i i , respectively.\nIt must be noted that c , js and u , js are the output of the CX for\nc , ji and u , ji , respectively. The BL encompasses an adaptive\nnetwork with four layers (see Fig. 3(a)). The first layer\nconsists of ak nodes (“adaptive or square” [3] nodes) with K(.) function (kernel function). Each node has an input\nthat is an entity from the a a min a min,1 a min,2 a min,k d ,d ,...,dd\n(which is a set of ak minimum distances of\nu a,1 a,2 a,N\nd ,d ,...,d a d ). The output vector of the first layer\nis 1\nan ; the output value of each node of this layer is calculated\nusing (7), where the input to m th\nnode is m d ,amin .\n1\na,m a min,mn K( d ) (7) In general, the kernel function for m th node can be defined as\n(8), (9), and (10). The input and the parameter of K(.) are\ndetermined using md and mb , which are the m th entity of d and b , respectively. We used the subscript a to distinguish BL’s\nkernel function and its related parameters ( a mind and ab ). m m mK(d ) exp( d b ) (8)\nm 2z\nm m\n1 K( d )\n(1 ( d b ) ) (9)\nm m\nmax( ) ( d min( )) K( d )\nmax( )\nd d\nd\n(10)\nThe second layer is a normalized layer and has ak nodes (fixed\nor circle), which are labeled as to calculate the normalized value of 1\nan as (11).\na\n1\na ,m2 a ,m k\n1 a ,m\n(n ) n\nn m 1\n(11)\nThe third layer has ak circle nodes with functions given in (12). This layer has two input vectors, 1\nan and uar ; the latter is a\nvector that is extracted from uu u,1 u,2 u,N r ,r ,...,rr and is\nrelated to the ak nearest neighbors.\na\n1\na,m3 a,m ua,mk\n1 a,m\n(n ) n r\nn m 1\n(12)\nThe fourth layer has a single node (circle) that calculates the summation of its input vector to produce a , jr , the primary\noutput (response). The function of the third and fourth layers can be formulated according to (13), i.e., the inner product of\nthe third layer’s output, 3 an , and aru . 3 a, j a uar n r (13)\nThe provided primary response, a , jr , is sent to the CM to\ncalculate the reinforcement signal. It should be noted that the connection between CM and BL (see Fig. 2(a)) does not exist between centromedial nuclei and the other parts of the amygdala. However, we assumed its existence in the BELPM’s architecture.\nb) CM is responsible to provide the final output; thus, it is an important component of AMYG. It has inputs from BL and ORBI, and performs different functions during the first leaning phase and the second learning phase of the BELPM.\n CM during the first learning phase: The first learning phase of BELPM begins when the input of BELPM is\nchosen from the training sets,\nuu, j u ,1 u,2 u,Nu , ,...,i I i i i (see Fig. 2(a)). After receiving this input, BL starts performing its function\nand provides ra, j and ,u jr that are sent to CM, which\nhas three square nodes (see Fig. 3(b)). The function of the first node is defined according to (14) to\nprovide jr , i.e., the output of BELPM (emotional response). The functions of the second and third nodes are defined according to (15) and (16), which\nprovide reinforcement (punishment), jap , , and\nexpected reinforcement (expected punishment), e\na,j p ,\nrespectively.\nj 1 a, j 2 o, j 3r w r w r w (14)\na, j a,1 u, j a,2 a, j a,3p w r w r w (15)\ne\na, j u , j a, jp r r (16) A supervised learning algorithm determines appropriate values for the weights.\n CM during the second learning phase: The second learning phase of BELPM begins when the input is\nchosen from the test data sets c , ji (see Fig. 2(b)). As BL has no information about the desired output (the target response), it does not send any information about the desired output to the CM. In this phase, the first node of the CM has the same input and performs the same function as it does in the first learning phase. However, the input and the connection of the second square node are different. In this phase, the\nreinforcement, jap , , is calculated according to (17),\nwhich differs from (15) in terms of its input. The third node has no input and function, and it can be removed.\na, j a,1 j a,2 a, j a,3p w r w r w (17)\n4) The expected reinforcements, e ap , and c , js are sent to\nORBI that is connected to CX and AMYG. The function of this part is defined to emulate certain roles of the orbitofrontal cortex. These roles include forming a stimulus–reinforcement association, evaluating reinforcement, and providing an output. Before explaining how ORBI performs, it should be noted that ORBI starts performing its functions after receiving the vector of the expected reinforcement, e ap , which means that BL of AMYG must have fulfilled its functions. The ORBI is composed of MO and LO corresponding to the lateral and medial parts of the orbitofrontal cortex, and their functions are described as follows: a)MO, which is responsible for the provision of the secondary\nresponse of the BELPM, receives c , js and calculates\no, j c , j u , j\n2\nd s s for each u , js . The MO consists of a four-\nlayer adaptive network. The first layer has ok nodes (square), and the input vector and output vector of the first layer’s nodes\nare o mind (the ok minimum values of the distance\nvector, uo o,1 o,2 o,N\nd ,d ,...,dd ) and 1\non , respectively. The\nfunction of m th node is the kernel function given in (18).\nomin,m\n1\no,mn K(d ) (18)\nThe second layer consists of ok nodes. Each node calculates an output as shown in (19) and sends it to the third layer.\no\n1\no,m2 o,m k\n1 o,m\n(n ) n\nn m 1\n(19)\nThe third layer has an input vector e\na minp , which is a vector of\nok minimum values of e ap corresponding to o mind . The third layer’s nodes have the function to multiply e\na minp and 2 on , as\ngiven in (20). e\namin\n3 2\no on n p (20) The fourth layer has a single node with a summation function that provides the output of ORBI (the secondary response). The result of the third and fourth layers can be obtained\naccording to (21). The output ,o jr is fed to the LO and CM.\n2 e\no, j o aminr n p (21)\nb) LO evaluates the output of MO, generates o, jp as reinforcement (punishment), and sends it to MO. It has one node (square) with a summation function given in (22).\no, j o,1 o, j o,2p w r w (22)\nThe main structure of BELPM is similar to the amygdalaorbitofrontal model. However, the connection between the components, their functions, and definition of reinforcement functions are significantly different from the amygdalaorbitofrontal model. Moreover, in this study, we have used pre-trained neural networks to explain the functionality of TH and CX; however, to increase the functionality and adaptability of BELPM, they can be defined by a multi-layer NN with trainable weights.\nK(.)\nK(.) N\nN\nNK(.)\n×\n×\n×\n∑ . . . . . . . . .\n1min,a d\nkamin, d\n2min,a d\n1\na,1 n\n1\na,2 n\n1\nka, n\nua,1 r\nua,2 r\nkua, r\na r\nAdaptive network of\nAMYG\n(a)"
    }, {
      "heading" : "B. Learning Algorithms of BELPM",
      "text" : "In the following, we explain how the BELPM uses the combination of two learning methods: the SD [3] and LSE to learn the input–output mapping (the stimulus–response association). The learning parameters are the weights\n( 1 2 3w ,w ,w , etc.) and the parameters of kernel functions\n( ob and ab ). As mentioned earlier, the learning algorithm of BELPM is divided into two phases: the first learning phase and the second learning phase. Each of them uses different learning rules to adjust the learning parameters.\n1) First learning phase: At the first learning phase, a hybrid learning algorithm [3] that is a combination of SD and LSE is used to update the learning parameters of AMYG (e.g., ab ,\n,a ,1 a,2w w and 1 2 3w ,w ,w ) and ORBI ( o o,1 o,2,w ,wb ). Under the\nassumption that the linear parameters, 1 2 3 o,1 o,2 a,1 a,2 a,3w ,w ,w ,w ,w ,w ,w ,w have fixed values, the\nnonlinear parameters (kernel parameters), ob and ab , are updated by SD in a batch-learning manner. Thus, SD is applied to minimize the two loss functions, which have been defined on the basis of a p and op . Equations (23) and (24) are SD-based learning rules used to calculate the derivatives of the loss functions, with respect to ob and ab .\na\nit 1 it it it\na a aηb b b (23)\no\nit 1 it it it\no o oηb b b (24)\nThe parameter it denotes the current values of learning parameters, where it ab and it ob are the gradients of loss functions to the parameters ob and ab (25) and (26). Two\nlearning rates a itη and o itη are defined as functions of a p and\na p .\na\na\na\nf (p ) b\nb (25)\na\na\na\nf (p ) b\nb\n(26)\nwhere a p and op are defined using (27) and (28).\na,1 u,a a,2 a a,3w w wap r r (27)\no o,1 o o,2w wp r (28) The ORBI and AMYG have their own loss function and update their own learning parameters ob and ab separately. The derivatives of (25) and (26) are formulated by using the chain rules of (29) and (30). Here, we have ignored it and just mentioned about the chain rules, but in the algorithm, it has been calculated for the current iteration, it .\namin a\n1 u ,aa a\na,2 a k 1\na a,m\nw 2 K( n )\nm 1\nrp n r d\nb (29)\nomin o\n1 u ,oo o\no,2 o k 1\no o,m\nw 2 K( n )\nm 1\nrp n r d\nb (30)\nAn offline version of LSE is used to update the linear parameters under the assumption that the nonlinear parameters have been updated and their values are fixed. The output of BELPM, jr , is parameterized by the weights,{ , ,1 2 3w w w }.\nFurthermore, a, jp and o,jp have been formulated using the\nlinear parameters, , ,a,1 a,2 a,3w w w , o,1w , and o,2w , according to\n(27) and (28). The LSE updates the weights 1 2 3w ,w ,w , , ,a,1 a,2 a,3w w w o,1w , and o,2w by assuming that each triple of the\nset a , j o , j u, j u {( r ,r ,r ), j 1,...N } is substituted into (14).\nEach triple of a , j u, j a, j u {( r ,r , p ), j 1,...N } and each pair of\no , j o, j u ( r , p ),i 1,...N are also substituted into (15) and\n(22), respectively; thus, uN linear equations such as (31), (32), and (33) are derived.\nu, j 1 a, j 2 o, j 3r w r w r w (31)\na, j u,i a,1 a,i a,2 a,3p r w r w w (32)\no, j o, j o,1 o,2p r w w (33)\nEquations (31), (32), and (33) can be rewritten in matrix form\nas N u\na, j o, j j 1 r r 1A ,\nuN\nu, j a, j j 1 r r 1B and\nuN\no, j j 1 r 1C to\ndefine (34), (35), and (36), and update the linear parameters using LSE. Here, the weights are defined as ][ 321 ,w,www ,\n][ a,3a,2a,1 ,w,wwaw , and ][ o,2o,1,wwow .\nT 1 T\nuw (A A) A r (34)\nT 1 T e\na aw (B B) B p (35)\nT 1 T e\noow (C C) C p (36)\nDuring the first learning phase, the learning parameters, linear and nonlinear, can be updated by using one of the following methods:\n All parameters can be updated using SD.  The nonlinear parameters can be updated using SD and the initial values of linear parameters can be adjusted\nusing LSE.\n The linear parameters are updated using LSE and the initial values of parameters of kernel functions are\nchosen by using a heuristic method.\n The nonlinear parameters are updated using SD and LSE are applied to update the linear parameters. Certainly, these methods differ in terms of time complexity and prediction accuracy, and a tradeoff between high accuracy and low computational time must be considered to choose a feasible method. The batch mode or online mode of each method can be considered for the first learning phase. 2) Second learning phase: At the second learning phase, the nonlinear and kernel parameters are updated by a reinforcement-learning algorithm. The nonlinear parameters are updated by SD. The SD algorithm minimizes the loss functions, which are defined based on the\nreinforcements, a, jp and o,jp , using (37) and (28). It must be\nnoted that a p is calculated using the obtained output of BELPM, r , as given in (37), which differs from that calculated using (27). The adjusting rules that update ob and ab are calculated according to (21) and (22). The derivatives of the loss functions with respect to ob and ab are calculated according to (30) and (38). Here, the weights a,4 a,5 a,6w ,w ,w are equal to [1,-1,0].\na,4 a,5 a,6w w wa ap r r (37)\n1 u ,a a\n4 5 k 2\nn w w\nK ( n )\na\na, a, a amin\na a a,m\nm 1\nrp r r d\nb b (38)"
    }, {
      "heading" : "IV. CASE STUDIES: CHAOTIC TIME SERIES",
      "text" : "In this section, BELPM is evaluated as a prediction model by using two benchmark chaotic time series, Lorenz and Henon. To provide a careful comparison with other methods, we used various data sets with different initialized points and sizes of training samples. We also utilized two error measures:\nnormalized mean square error (NMSE) and mean square error (MSE), as given in (39), (40), to assess the performance of the prediction models and provide results comparable with other studies. N\n2\nj j\nj 1 N 2\nj j\nj 1\nˆ(y y )\nNMSE\n(y y )\n(39)\nN 2\nj j j 1\n1 ˆMSE (y y )\nN (40)\nWhere ŷ and y refer to the observed values and desired\ntargets, respectively. The parameter y is the average of the\ndesired targets. For all experiments, one-fold cross-validation was chosen; the number of samples in one-fold crossvalidation was equal to the size of the test data set."
    }, {
      "heading" : "A. Lorenz Time Series",
      "text" : "The Lorenz time series [5], [9], [58] was chosen as the first test, given by using (41) and (42). In this case study, the initialized point is according to x(0) 15,y(0) 0,z(0) 0 .\n. x a(y x) . y bx y xz . z xy cz\n(41)\na 10,b 28,c 8 / 3,T 0.01s (42)\nTo produce the time series, the sampling period is equal to 0.01 s [5], [9], [58], and the embedded dimension is selected as three. The BELPM is tested for four sets of data from the Lorenz time series. The first data set is selected from 32 nd to 51 st s, and is employed for long-term prediction. For the training set, 500 samples are used and the next 1400 samples are considered as the test data set and validation data set. Table I presents the NMSEs obtained from applying different methods for 10, 30, and 40 steps ahead prediction of this data set. It also indicates that the NMSE of ANFIS is lower than the NMSE of BELPM for 10 steps ahead prediction. However, when the prediction horizon increases to 30 and 40 steps ahead, the NMSEs of BELPM are lower than ANFIS. The presented results in Table I also show that an increase in the prediction horizon causes a decrease in the prediction accuracy for all methods. It is important to note that the NMSEs of BELPM for predicting 30 and 40 steps ahead are less than those of the other methods.\nFor the second data set, ANFIS, Wk-NN, and BELPM are tested for 1–20 steps ahead prediction, using 1500 samples as the training data set and 1000 samples as the test data set. In\nFig. 4, the NMSE values vs. the prediction steps are depicted; it can be observed that the NMSE values of BELPM are lower than those of the other methods, especially, in the case of 15 steps ahead prediction and longer.\nTABLE I THE COMPARISONS OF NMSES OF DIFFERENT METHODS TO PREDICT MULTISTEP AHEAD OF LOREN TIME SERIES (THE FIRST DATA SET)\nLearning Method\nTen Thirty Forty\nBELPM 0.0125 0.2473 0.2447 ANFIS 0.006 0.3559 0.3593 RBF 0.4867 [58] 0.3405 0.6887 LLNF 0.1682 [58] 0.4946 0.5341 Wk-NN 0.0235 0.2599 0.3830\n0 2 4 6 8 10 12 14 16 18 20 0\n0.005\n0.01\n0.015\n0.02\n0.025\nSteps Ahead\nN M\nS E\nBELPM ANFIS WKNN\nFig.4. The NMSE values of multi-step ahead prediction of Lorenz system vs. prediction horizon for 1000 samples of test data using 1500 samples as training data (the second data set). The dotted curves show the NMSE of BELPM.\nIn the third data set, ANFIS, Wk-NN, and BELPM are compared for 30 steps ahead prediction, with different sizes of training data set. In this case, the complete sample set is\nchosen from 30 th to 55 th s, and the training is conducted using 500, 1000, and 1500 samples to predict 1000 samples of test data set from 45 th to 55 th s. The validation set is 1000 samples from 56 th to 65 th s. Table II compares the values of NMSE for the different methods and once again shows that NMSE of BELPM has the lowest value among that of all the methods tested.\nFor the fourth data set, the training set and test data are\npicked from 15 th to 45 th s and 46 th to 55 th s, respectively. BELPM is applied for 25 steps ahead prediction and the NMSE values, CPU time, and structures of different models are compared in Table III. Again, it can be observed that the NMSE of BELPM is lower than that of the other methods. The time complexity of BELPM is less than most of the other methods, and only Wk-NN is found to have a little lower time complexity than BELPM.\nLearning Method\nNMSE TIME(Sec) STRUCTURE\nBELPM 0.0325 11.81 10neuron ANFIS 0.0802 12.91 4 rule LoLiMoT 0.2059 18.74 7 neuron RBF 0.1193 26.07 32 neuron Wk-NN 0.0342 1.23 5 neighbor\nAs mentioned earlier, the Lorenz chaotic time series is a wellknown benchmark time series and has been tested with numerous data-driven models to evaluate the models’ performance. Table IV presents the obtained NMSEs of several data-driven methods for noiseless and noisy data. The data-driven models are: Nonlinear Autoregressive model with eXogenous input (Hybrid NARX-Elman RNN) [59], Evolving Recurrent Neural Networks (ERNN) [60], Radial Basis Function (RBF), multilayer perceptron (MLP) [5], Support Vector Regression (SVR), Tapped Delay Line Multilayer Perceptron (TDL-MLP), Distributed Local Experts based on Vector_Quantization using Information Theoretic learning (DLE-VQIT) [61], Cooperative Coevolution of Elman Recurrent Neural Networks (CCRNN) [62], Functional Weights Wavelet Neural Network-based state-dependent AutoRegressive (FWWNN-AR) [63], Recurrent Neural Network trained with Real-time Recurrent Learning (RNNRTRL), Recurrent Neural Network trained with the secondorder Extended Kalman Filter (RNN-EKF), Recurrent Neural Network trained with the algorithm and BackPropagation Through Time (BPTT), feedforward Multi layer Perceptron trained with the Bayesian Levenberg–Marquardt (MLP-BLM), and recursive second-order training of Recurrent Neural Networks via a Recursive Bayesian Levenberg–Marquardt (RBLM-RNN) algorithm [65]. It can be noted that the table is sorted according to the obtained NMSEs, and that the NMSE of BELPM is not excellent as that of the other methods. However, this table compares the NMSEs for short-term prediction, which is not a feasible application for BELPM."
    }, {
      "heading" : "B. Henon Time Series",
      "text" : "The second benchmark of this study is the Henon time\nseries that is constructed by using (43). 2\nx(t 1) 1 ax(t) y(t)\ny(t 1) bx(t)\na 1.4,b 0.3\n(43)\nThe embedded dimension are considered as 0.01s [5], [58] and three, respectively. In this case study, the initialized point is x(0 ) 0,y(0 ) 0 . Three data sets of the Henon time series\nare used for evaluating the BELPM. The first data set is selected from 9 th to 18 th s and the training data set and the test data set consist of 800 and 100 samples, respectively. The BELPM, ANFIS, and Wk-NN are tested to predict three steps ahead of this data set.\ndata samples\nFWWNN[63] 9.8e-15 1500,1000 NN+AR, 1 step noiseless NARX[59] 1.9e-10 1500,1000 AR, 1 step noiseless BELRFS[12] 4.9e-10 1500,1000 NF+BEL, 1 step noiseless ERNN[60] 9.9e-10 1500,1000 NN, 1 step noiseless RBF[5] 1.4e-9 1500,1000 NN, 1 step noiseless MLP[5] 5.2e-8 1500,1000 NN, 1 step noiseless BELPM 2.9e-6 1500,1000 W-kNN+BEL,1step noiseless TDL-MLP[61] 1.6e-4 ----- NN, 1 step noiseless DLE-VQIT[61] 2.6e-4 ----- -----, 1 step noiseless LSSVMs[64] 6.4e-5 1000,250 NF, 1 step noisy-STD 0.05 BELPM 3.7e-4 1500,1000 W-kNN+BEL,1stepnoisy-\nSTD 0.001\nBELPM 4.4e-4 1500,1000 W-kNN+BEL,1stepnoisy-\nSTD 0.01\nRBLMRNN[65]\n9.0e-4 1000,250 RNN,1 step noisy-STD 0.05\nCCRNN [62] 7.7e-4 500,500 NN,2 step noiseless LLNF[64] 2.9e-4 1000,250 RNN, 1 step noisy-STD 0.05 MLP_BLM[65] 8.1e-4 1000,250 NN, 1 step noisy-STD 0.05 MLP_EKF[65] 1.6e-3 1000,250 NN, 1 step noisy-STD 0.05 RNNRTRL[65] 1.7e-3 1000,250 NN, 1 step noisy-STD 0.05 RNN-BPTT[65] 1.8e-3 1000,250 NN, 1 step noisy-STD 0.05 RNN-EKF[65] 1.2e-3 1000,250 RNN,1 step noisy-STD 0.05 BELPM 1.0e-3 1500,1000 W-kNN+BEL,1stepnoisy-\nSTD 0.1\nTable V presents the NMSEs, structure, and computational time (CPU time) for the tested methods. It can be noted that the NMSE of BELPM is lower than that of the other methods. This table also indicates that the number of neurons in BELPM is more than the number of rules in ANFIS; however, the CPU time for BELPM is lesser than that of ANFIS. The graph in Fig. 5 displays the obtained errors (the difference between the target values and obtained outputs) from ANFIS and BELPM for the test data set. We have also compared the effect of different structures of W-kNN, ANFIS, and BELPM on their prediction accuracy. The structures of these methods have been changed by increasing the number of neighbors in BELPM and Wk-NN and the number of rules in ANFIS. As Fig. 6 shows the value of NMSE for ANFIS decreases when the number of rules increases. In contrast, increasing the number of neighbors of Wk-NN increases the value of NMSE. For BELPM, the value of NMSE decreases slowly when the number of neighbors increases. This experiment verifies the generalization capability of BELPM and shows that different structures of BELPM do not make a noticeable difference between the obtained prediction errors. To further evaluate the performance of the BELPM and verify its robustness, white noise with standard deviation 0.1 is added to the first data set, and the results of multi-step ahead prediction are listed in Table V. The effect of using the second learning phase of BELPM has been presented in Table VI. It is clear that there is a reduction in the obtained NMSEs of BELPM because of the second learning phase (SLP).\nThe graph in Fig. 7 shows the values of NMSE of two steps ahead prediction of the first data set during the learning phases. It can be observed that the NMSE decreases continuously during the first and second learning phases. The graph in Fig. 8 depicts the mean square error (MSE) that is obtained from the training samples and test samples during the first learning phase. In each epoch, the learning parameters have been updated and the values of MSE for the training data and the test data have been calculated. It is noticeable that the values of the dotted curve that is related to the MSE of the test data are lower than those of the solid curve, i.e., the MSE of training data.\n0 5 10 15 20 25 30 35 40 45 0.0256\n0.0258\n0.026\n0.0262\n0.0264\n0.0266\n0.0268\nepochs\nN M\nS E\ne rr\no r\nin d e x e s\nprediction error of noisy data\n(First and second learning phase)\nErrors of second learning phase Errors of first learning phase\nFig.7. The NMSE of BELPM to predict the two steps ahead of Henon time that is added white noise with standard deviation 0.1. The solid line is related to the NMSE during 35 epochs of the first learning phase and the dashed line is the NMSE during 10 epochs of the second learning phase.\n0 20 40 60 80 100 0\n0.2\n0.4\n0.6\n0.8\n1\n1.2 x 10\n-5\nepochs\nM e a n S\nq u a re\nE rr\no r\nTest Train\nFig.8. Curves of the mean square error during the first learning phase; the solid line is the MSE curve, which is related to training data; the dashed line is the MSE curve, which is related to test data.\nThe second data set of the Henon time series is selected\nfrom 8 th to 18 th s, and three different sizes of data samples are selected as training data. Table VII compares the obtained results from examining different methods that are trained by different training sets: 300, 600, and 900 training samples, and have been used to predict 100 samples of test data. As shown in Table VII, the prediction accuracy of BELPM is higher than that of the other methods in all cases; thus, it is a suitable method for the prediction application with a small number of training samples. The bar chart in Fig. 9 shows the NMSE for the compared methods. It can be seen that the values of NMSE of BELPM are once again lowest among those of all the presented methods. Examination of this data set revealed that a decrease in the number of training samples causes an increase in the values of NMSE of all methods (e.g., BELPM, ANFIS, Wk-NN, and RBF). However, the rate of increasing the values of NMSE of BELPM is lower than that of other methods. The third data set of the Henon time series is selected to compare the time complexities (CPU time) of various learning methods when the size of the training set is large. A training set of 3000 samples are selected from 100 th to 130 th s, and the following 1000 samples from 130 th to 140 th s are considered as the test data set. Table VIII summarizes the NMSEs, CPU time, and the applied structures of the different methods. Although the CPU time of Wk-NN is 1.128 s, which is less than that of BELPM, the prediction accuracy of\nBELPM is higher than that of Wk-NN and other methods.\nFig.10. The prediction errors of ANFIS and BELPM for three steps ahead prediction of Henon time series using 3000 training samples.\n0 50 100 150 200 1.978\n1.979\n1.98\n1.981\n1.982\n1.983\n1.984 x 10\n-3\nepochs\nN M\nS E\ne rr\no r\nin d e x e s\nError of training data (First learning phase)\n0 50 100 150 200 7.8\n7.805\n7.81\n7.815\n7.82\n7.825\n7.83\n7.835\n7.84\n7.845 x 10\n-3\nepochs\nN M\nS E\ne rr\no r\nin d e x e s\nError of Test data\n(First learning phase)\n(b) Fig.11. The NMSE of three steps ahead predictions during first learning phase. (a) For training data. (b) For test data.\nThe Henon chaotic time series is also a well-known benchmark time series, and has been examined by a large number of data-driven models. Table IX lists the obtained NMSEs of several data-driven methods. It becomes obvious that BELPM has a fairly good performance in predicting noisy data time series."
    }, {
      "heading" : "V. DISCUSSION AND CONCLUSION",
      "text" : "This study has presented a prediction model inspired by brain emotional processing and, particularly, has investigated this model for chaotic time series prediction. We have described the architecture of this model using feedforward neural networks and adaptive networks. Furthermore, we have also explained the function and learning algorithms of the model that is referred to as BELPM. The accuracy of the BELPM has been extensively evaluated by different data sets\nof two benchmark chaotic time series, Lorenz and Henon, and the results strongly indicate that the model can predict the long-term state of chaotic time series rapidly and more accurately than other well-known methods, i.e., RBF, LoLiMoT, and ANFIS. The results also show that BELPM is more efficient than the other methods when large training data sets are not available. In comparison with other data-driven approaches, we can summarize the highlighted features of BELPM as follows: 1) It converges very fast into its optimal structure (see Tables III, IV, and VII). 2) It has high generalization capability (see dotted lines in Figs. 7 and 8), high robustness from the perspective of learning algorithms (see solid lines in Figs. 7, 8, 11, and 12), low noise sensitivity (see Table V), relatively low computational time (compare the values of time columns in Tables III, IV, and VII), and low model complexity. 3)The number of neighbors in AMYG and ORBI is not dependent on the input’s dimension, which indicates that the dimension of data does not have a direct effect on the model complexity. However, the input’s dimension increases the computational time of calculating the Euclidean distance and searching the nearest neighbors; thus, it has an indirect effect on the computational time of BELPM. The experiments have also shown that when there are a limited number of samples\nfor training the model, the number of neighbours, ok and\nak should be raised to get accurate results (compare Table IV and VII). A general observation is that a better prediction\nresult could be achieved when the number of neighbours, ok , in the ORBI part is approximately twice the number of\nneighbours, ak , in the AMYG part. The feasible values and\neffective combination of ak and ok can be determined using meta-heuristic optimization methods (e.g., genetic algorithm). 4) The second learning phase of BELPM provides online adaptation and continuously increases the prediction accuracy and offers the capability to overcome both over-fitting and under-fitting problems. 5) For long-term prediction using a small number of training samples, the accuracy of BELPM is higher than other datadriven models, such as ANFIS and LoLiMoT (see Tables I, II, IV, and V). In the case of using a large number of training samples, it is also noticeable that the computational time of BELPM is not greater than that of ANFIS and LoLiMoT (see Tables III and VII). It can also be concluded that when the degree of chaos is high, even using a large number of data and neuro-fuzzy methods (ANFIS and LoLiMoT) would not achieve higher prediction accuracy than the BELPM (see Table VII). As future works, the authors consider adding some optimization methods (e.g., genetic algorithm) to find optimal values of the fiddle parameters, e.g., the number of neighbors\nak and ok and the initial values of nonlinear parameters. Other improvements in the model would be made on the basis of kdTree data structure [47] to address “the curse of dimensionality” [1] problem and decrease the computational time complexity of BELPM. To adjust the nonlinear\nparameters, different types of optimization methods (e.g., Quasi-Newton or Conjugate Directions) for ORBI and AMYG can be utilized. In addition, the Temporal Difference (TD) learning algorithm can also be used as a reinforcement method for the second learning phase to update the linear learning parameters. The good results obtained by employing the BELPM for predicting the chaotic time series are a motivation for applying this model as a classification method as well as to identify complex systems."
    } ],
    "references" : [ {
      "title" : "Nonlinear System Identification:From classicical Approches to Neural Networks and Fuzzy Models. Berlin, Germany",
      "author" : [ "O. Nelles" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2001
    }, {
      "title" : "Neural Networks: A Comperhensive Foundation.Upper Saddle River, NJ:Prentice Hall, 2 ed",
      "author" : [ "S. Haykin" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1999
    }, {
      "title" : "Neuro-Fuzzy and Soft Computing: A computational approach to Learning and Machine Intelligence.Upper Saddle River, NJ",
      "author" : [ "R. Jang", "C. Sun", "E. Mizutani" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1997
    }, {
      "title" : "Time Series Prediction and Neural Networks",
      "author" : [ "R.J. Frank", "N. Davey", "S.P. Hunt" ],
      "venue" : "J. Intell Robot Syst., vol. 31, no. 1-3, pp. 91-103, 2001.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Predicting Chaotic Time Series Using Neuraland Neurofuzzy Models A Comparative Study",
      "author" : [ "A. Golipour", "B.N.Araabi.", "C. Lucas" ],
      "venue" : "J. Neural. Process Lett., vol. 24, no. 3, pp. 217-239, 2006.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Neuro-fuzzy models, BELRFS and LoLiMoT, for prediction of chaotic time series",
      "author" : [ "M M. Parsapoor", "U. Bilstrup" ],
      "venue" : "Proc. IEEE Int. Conf. INISTA., pp.1-5, 2012.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Introducing BELBIC: brain emotional learning based intelligent controller",
      "author" : [ "C. Lucas", "D. Shahmirzadi", "N. Sheikholeslami" ],
      "venue" : "J. INTELL. AUTOM. SOFT. COMPUT., vol. 10, no. 1, pp. 11-22, 2004.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Enhancing the performance of neurofuzzy predictors by emotional learning algorithm",
      "author" : [ "C. Lucas", "A. Abbaspour", "A. Gholipour", "B. Nadjar Araabi", "M. Fatourechi" ],
      "venue" : "J. Informatica (Slovenia)., vol. 27, no. 2 pp.165–174, 2003.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Learning based brain emotional intelligence as a new aspect for development of an alarm system",
      "author" : [ "T. Babaie", "R. Karimizandi", "C. Lucas" ],
      "venue" : "J. Soft Computing., vol. 9, issue 9, pp.857-873, 2008.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Reinforcement _recurrent fuzzy rule based system based on brain emotional learning structure to predict the complexity dynamic system",
      "author" : [ "M. Parsapoor", "C. Lucas", "S. Setayeshi" ],
      "venue" : "Proc. IEEE Int. Conf. ICDIM, , pp.25-32, 2008.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Brain Emotional Learning Based Fuzzy Inference System (BELFIS) for Solar Activity Forecasting",
      "author" : [ "M. Parsapoor", "U. Bilstrup" ],
      "venue" : "Proc. IEEE Int. Conf. ICTAI 2012, 2012.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Chaotic Time Series Prediction Using Brain Emotional Learning Based Recurrent Fuzzy System (BELRFS)",
      "author" : [ "M. Parsapoor", "U. Bilstrup" ],
      "venue" : "to be published in International Journal of Reasoning-based Intelligent Systems, 2013.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Prediction the price of Virtual Supply Chain Management with using emotional methods",
      "author" : [ "M. Parsapoor" ],
      "venue" : "M.S. thesis, Dept. Computer. Eng., Science and research Branch, IAU., Tehran, Iran,2008.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Application of emotional learning fuzzy inference systems and locally linear neuro-fuzzy models for prediction and simulation in dynamic systems",
      "author" : [ "M. Abdollahzade", "A. Miranian", "S. Faraji" ],
      "venue" : "Proc. IEEE Int. Joint Conf. Fuzzy Syst., pp. 1–8,2012",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Supervised brain emotional learning",
      "author" : [ "E. Lotfi", "M.R. Akbarzadeh-T" ],
      "venue" : "Proc. IJCNN, pp.1-6, 2012.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Emotional controller (BELBIC) based DTC for encoderless Synchronous Reluctance Motor drives",
      "author" : [ "H", "A. Zarchi", "E. Daryabeigi", "G.R. A .Markadeh", "J. Soltani" ],
      "venue" : "Proc. Int. Conf. PEDSTC, pp.478-483 2011.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Emotional Circuits and Computational Neuroscience",
      "author" : [ "J.M.Fellous", "J.L.Armony", "J.E. LeDoux" ],
      "venue" : "The Handbook of Brain Theory and Neural Networks, The MIT Press, Cambridge, MA, 2003.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "How the Brain Processes Emotional Information",
      "author" : [ "J.L. Armony", "J.E. LeDoux" ],
      "venue" : "J. Ann. N. Y. Acad., no. 821, pp. 259-270, 1997.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "The Imbalanced Brain: From Normal Behavior To Schizophrenia",
      "author" : [ "S. Grossberg" ],
      "venue" : "J. Biol. Psychiatry., vol. 48, no. 2, pp. 81-98, 2000.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "The emotional brain",
      "author" : [ "T.Dalgleish" ],
      "venue" : "J. NAT REV NEUROSCI., vol. 5, no. 7, pp. 583–589, 2004.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Color and Contrast Sensitivity in the Lateral Geniculate Body and Primary Visual Cortex of the Macaque Monkey",
      "author" : [ "D.H. Hubel", "M.S. Livingstone" ],
      "venue" : "J., Neuroscience. vol. 10, no.7, pp. 2223-2237, 1990.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "The Neural Basis of Perception and Movement, Principles of Neural Science",
      "author" : [ "J.P. Kelly" ],
      "venue" : "London: Prentice Hall. 1991.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Cytoarchitectonic mapping of the human amygdala, hippocampal region and entorhinal cortex : intersubject variability and probability maps",
      "author" : [ "K. Amunts.", "O. Kedo.", "M. Kindler.", "P. Pieperhoff.", "H. Mohlberg.", "N. Shah.", "U. Habel.", "F. Schneider.", "K. Zilles." ],
      "venue" : "J. Anatomy and Embryology., vol. 21, no. 5-6, pp. 343-352, 2005.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Amygdala Response to Facial Expressions Reflects Emotional Learning",
      "author" : [ "C.I. Hooker.", "L.T. Germine.", "R.T. Knight.", "M.D. Esposito." ],
      "venue" : "Neuroscience. J., vol. 26, no.35, pp. 8915-8930, Aug. 2006.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Primate Orbitofrontal Cortex and Adaptive Behavior,’",
      "author" : [ "AC. Robert" ],
      "venue" : "J. TRENDS. COGN. SCI,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2006
    }, {
      "title" : "The orbitofrontal cortex: linking reward to hedonic experience",
      "author" : [ "M.L. Kringelbach." ],
      "venue" : "J., Nat. Rev. Neurosci., vol. 6, pp. 691-702,2005.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "The Brain and Emotion by Edmund T. Rolls",
      "author" : [ "A.G. Phillips." ],
      "venue" : "J. TRENDS. COGN. SCI., vol. 3, pp. 281-282, 1999.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Schultz.,“The Mysterios of Orbitofrontal Cortex. Foreword",
      "author" : [ "W.C. Cavada" ],
      "venue" : "Cereb Cortex,’’, J. Cerebr. Cortex. vol. 10,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2000
    }, {
      "title" : "Constracting Roles of Basolateral Amygdala and Orbitofrontal Cortex in Impulsive Choice",
      "author" : [ "C.A. Winstanley", "D.E.H. Theobald", "R.N. Cardinal", "T.W. Robbins" ],
      "venue" : "J. Neurosci., vol. 24, no. 20, pp. 4718-4722, 2004.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The functional neuroanatomy of the human orbitofrontal cortex: evidence from neuroimaging and neuropsychology",
      "author" : [ "M.L. Kringelbach", "E.T. Rolls" ],
      "venue" : "J., Prog. Neurobiol, vol. 72, pp. 341–372, 2004.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Emotion-Based Decision and Learning Using Associative Memory and Statistical Estimation",
      "author" : [ "B.D. Damas", "L. Custódio" ],
      "venue" : "J. Informatica (Slovenia), vol. 27, no. 2, pp. 145-156, 2004.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "When Robots Weep: Emotional Memories and Decision-Making",
      "author" : [ "J.D. Velásquez." ],
      "venue" : "Proc. Conf. on Artifitial Intelligence, pp.70-75. 1997.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Emotional behaviour: A resourcemanagement approach",
      "author" : [ "S.H. Zadeh", "S.B. Shouraki", "R R. Halavati" ],
      "venue" : "J. Adaptive Behaviour, vol. 14, pp. 357-380, 2006.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Multiple Emotion-Based Agents Using an Extension of DARE Architecture",
      "author" : [ "M. Maçãs", "L. Custódio" ],
      "venue" : "J. Informatica (Slovenia), vol. 27, no. 2, pp. 185-196, 2004.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Lucas,\"Emotional controller (BELBIC) for electric drives — A review,",
      "author" : [ "E. Daryabeigi", "C.G.R.A. Markadeh" ],
      "venue" : "in Proc. Annual Conference on IEEE Industrial Electronics,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2010
    }, {
      "title" : "Applying Brain Emotional Learning Algorithm for Multivariable Control of HVAC Systems,",
      "author" : [ "N. Sheikholeslami", "D. Shahmirzadi", "E. Semsar", "C. Lucas" ],
      "venue" : "J. INTELL. FUZZY",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2005
    }, {
      "title" : "Roshanian,\"Aerospace Launch Vehicle Control: An Intelligent Adaptive Approach",
      "author" : [ "A.R. Mehrabian", "J.C. Lucas" ],
      "venue" : "J. Aerosp. Sci. Technol.,vol.10,pp",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2006
    }, {
      "title" : "Intelligent Modeling and Control of Washing Machines Using LLNF Modeling and Modified BELBIC",
      "author" : [ "R.M. Milasi", "C. Lucas", "B.N. Araabi" ],
      "venue" : "Proc. Int. Conf. Control and Automation., pp.812-817, 2005,.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Brain emotional learning based intelligent controller for stepper motor trajectory tracking",
      "author" : [ "A.M. Yazdani1", "S. Buyamin1", "S. Mahmoudzadeh2", "Z. Ibrahim1", "M.F. Rahmat1." ],
      "venue" : "J. IJPS., vol. 7, no. 15, pp. 2364- 2386, 2012.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Emotional Learning as a New Tool for Development of Agent-based System",
      "author" : [ "M. Fatourechi", "C. Lucas", "A.K. Sedigh" ],
      "venue" : "J. Informatica (Slovenia), vol. 27, no. 2, pp.137-144., 2004.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning Behavior-selection in a Multigoal Robot Task",
      "author" : [ "S.C. Gadanho", "L. Custódio" ],
      "venue" : "J. Informatica (Slovenia), vol. 27, no. 2, pp. 175-184, 2003.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A dynamic associative memory system by adopting amygdala model",
      "author" : [ "T. Kuremoto", "K.T. Ohta", "M. Kobayashi", "Obayashi" ],
      "venue" : "J. AROB, vol.13, pp.478–482, 2009.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A functional model of limbic system of brain",
      "author" : [ "T. Kuremoto", "T. Ohta", "K.K. Kobayashi", "M. Obayashi" ],
      "venue" : "Proc. Int. Conf. Brain informatics, pp.135-146, 2009.,.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Recursive Bayesian recurrent neural networks for time-series modeling",
      "author" : [ "D. Mirikitani", "N. Nikolaev" ],
      "venue" : "IEEE Trans. Neural Netw., vol. 21, no. 2, pp. 262–274, Feb. 2010.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Input variables selection using mutual information for neuro fuzzy modeling with the application to time series forecasting",
      "author" : [ "M.M.R. Yousefi", "M. Mirmomeni", "C. Lucas" ],
      "venue" : "Proc. Int. Joint Conf. Neural Netw., pp. 1121–1126, 2007.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Time-series prediction using a local linear wavelet neural network",
      "author" : [ "Y. Chen", "B. Yang", "J. Dong" ],
      "venue" : "J. Neurocomputing, vol. 69, nos. 4–6, pp. 449–465, 2006.",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Time series prediction using support vector machines: A survey",
      "author" : [ "N.I. Sapankevych", "R. Sankar" ],
      "venue" : "IEEE Comput. Intell. Mag., vol. 4, no. 2, pp. 24–38, May 2009.",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Chaotic time series prediction based on a novel robust echo state network",
      "author" : [ "D. Li", "M. Han", "J. Wang" ],
      "venue" : "IEEE Trans. Neural Netw Learn. Syst., vol. 23, no. 5, pp. 787–799, May 2012.",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Weighted k-Nearest-Neighbor Techniques and Ordinal Classification,",
      "author" : [ "S. Hechenbichler" ],
      "venue" : "Discussion Paper 399,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2004
    }, {
      "title" : "P.Indyk, Nearest-Neighbor Methods in Learning and Vision:Theory and Practice",
      "author" : [ "G. Shakhnarovich", "T. Darrell" ],
      "venue" : null,
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2006
    }, {
      "title" : "Extracting the main patterns of natural time series for long-term neurofuzzy prediction",
      "author" : [ "A.Gholipour", "C.Luca", "B.N.Araabi", "M.Mirmomeni", "M.Shafiee" ],
      "venue" : "J. Neural Computing & Applications., vol. 16, Issue, 4-5, pp. 383-393, 2007",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Chaotic time series prediction with residual analysis method using hybrid Elman–NARX neural networks",
      "author" : [ "M. Ardalani-Farsa", "S S. Zolfaghari" ],
      "venue" : "J. Neurocomputing, vol.73, issues 13–15, pp.2540-2553, 2010.",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Chaotic Time Series Prediction Based on Evolving Recurrent Neural Networks",
      "author" : [ "M. Qian-Li", "Z. Qi-lun", "P. Hong", "Z. Tan-Wei", "X. Li-Qiang" ],
      "venue" : "Proc. Int. Conf. Machine Learning and Cybernetics (ICMLC.2007), vol.6,no.,pp.3496,3500, 2007.",
      "citeRegEx" : "60",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Cooperative coevolution of elman recurrent neural networks for chaotic time series prediction,’’J",
      "author" : [ "R. Chandra", "M. Zhang" ],
      "venue" : "Neurocomputing, vol. 86,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2012
    }, {
      "title" : "Nonlinear time series modeling and prediction using functional weights wavelet neural network-based statedependent AR model",
      "author" : [ "G. Inoussa", "H. Peng", "J. Wu" ],
      "venue" : "J. Neurocomputing Journal, vol. 86, pp. 59- 74,2012.",
      "citeRegEx" : "63",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Developing a Local Least-Squares Support Vector Machines-Based Neuro-Fuzzy Model for Nonlinear and Chaotic Time Series Prediction",
      "author" : [ "A. Miranian", "M. Abdollahzade" ],
      "venue" : "IEEE Trans. Neural Netw, vol.24, no.2, pp. 207-218, 2013.",
      "citeRegEx" : "64",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Recursive Bayesian Recurrent Neural Networks for Time-Series Modeling,",
      "author" : [ "D.T. Mirikitani", "N. Nikolaev" ],
      "venue" : "IEEE Trans. Neural Netw, vol.21,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Well-known data-driven methodologies such as neural networks and neuro-fuzzy models have shown reasonable accuracy in the nonlinear prediction of chaotic time series [1].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "According to the VapnikChervonenkis (VC) theory [2], a sufficient size of training samples for achieving arbitrary prediction accuracy is proportional to the number of the models’ learning parameters.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : ", the number of training samples, chaos degree, embedding dimension, and the horizon of prediction) [1]–[5].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : ", the number of training samples, chaos degree, embedding dimension, and the horizon of prediction) [1]–[5].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "The model aims at continuing the recent studies that have suggested computational models of emotional processing for control and prediction applications [6]-[16].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 15,
      "context" : "The model aims at continuing the recent studies that have suggested computational models of emotional processing for control and prediction applications [6]-[16].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 2,
      "context" : "Adaptive-Network-Based Fuzzy Inference System (ANFIS) [3], MultiLayer Perceptron (MLP) Network [1], [2], Radial Bias Function (RBF) Networks [1], [2], and Local Linear Neuro-Fuzzy (LLNF) Models [1]).",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "Adaptive-Network-Based Fuzzy Inference System (ANFIS) [3], MultiLayer Perceptron (MLP) Network [1], [2], Radial Bias Function (RBF) Networks [1], [2], and Local Linear Neuro-Fuzzy (LLNF) Models [1]).",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "Adaptive-Network-Based Fuzzy Inference System (ANFIS) [3], MultiLayer Perceptron (MLP) Network [1], [2], Radial Bias Function (RBF) Networks [1], [2], and Local Linear Neuro-Fuzzy (LLNF) Models [1]).",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "Adaptive-Network-Based Fuzzy Inference System (ANFIS) [3], MultiLayer Perceptron (MLP) Network [1], [2], Radial Bias Function (RBF) Networks [1], [2], and Local Linear Neuro-Fuzzy (LLNF) Models [1]).",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "Adaptive-Network-Based Fuzzy Inference System (ANFIS) [3], MultiLayer Perceptron (MLP) Network [1], [2], Radial Bias Function (RBF) Networks [1], [2], and Local Linear Neuro-Fuzzy (LLNF) Models [1]).",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "Adaptive-Network-Based Fuzzy Inference System (ANFIS) [3], MultiLayer Perceptron (MLP) Network [1], [2], Radial Bias Function (RBF) Networks [1], [2], and Local Linear Neuro-Fuzzy (LLNF) Models [1]).",
      "startOffset" : 194,
      "endOffset" : 197
    }, {
      "referenceID" : 5,
      "context" : "Recently, bioinspired models, in particular, emotion-based learning models [6], [8], [10], [12]-[14], have shown acceptable generalization capability in modeling and predicting the chaotic behavior of dynamic systems.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "Recently, bioinspired models, in particular, emotion-based learning models [6], [8], [10], [12]-[14], have shown acceptable generalization capability in modeling and predicting the chaotic behavior of dynamic systems.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "Recently, bioinspired models, in particular, emotion-based learning models [6], [8], [10], [12]-[14], have shown acceptable generalization capability in modeling and predicting the chaotic behavior of dynamic systems.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 11,
      "context" : "Recently, bioinspired models, in particular, emotion-based learning models [6], [8], [10], [12]-[14], have shown acceptable generalization capability in modeling and predicting the chaotic behavior of dynamic systems.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "Recently, bioinspired models, in particular, emotion-based learning models [6], [8], [10], [12]-[14], have shown acceptable generalization capability in modeling and predicting the chaotic behavior of dynamic systems.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "These hypotheses that have contributed to the present computer-based models of emotional processing [18], have imitated certain aspects of emotional learning, and can be classified on the basis of their fundamental theories and applications.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "For example, a computer-based model that is based on the central theory [18] (i.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : ", which explains how a primary evaluation of emotional stimuli forms emotional experiences) is called a computational model of emotional learning and imitates the associative learning aspect of emotional processing [18] that is based on fear conditioning [19], [ 20].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 18,
      "context" : ", which explains how a primary evaluation of emotional stimuli forms emotional experiences) is called a computational model of emotional learning and imitates the associative learning aspect of emotional processing [18] that is based on fear conditioning [19], [ 20].",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 19,
      "context" : ", which explains how a primary evaluation of emotional stimuli forms emotional experiences) is called a computational model of emotional learning and imitates the associative learning aspect of emotional processing [18] that is based on fear conditioning [19], [ 20].",
      "startOffset" : 261,
      "endOffset" : 266
    }, {
      "referenceID" : 22,
      "context" : ", determining the effective values of stimuli [22]-[28].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "It then passes the generated signals to the amygdala and sensory cortex [28].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 21,
      "context" : "The thalamus includes different parts that process the emotional stimuli separately [27].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "The sensory cortex distributes its output signals between the amygdala and orbitofrontal region [18]-[21], [27].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 21,
      "context" : "The sensory cortex distributes its output signals between the amygdala and orbitofrontal region [18]-[21], [27].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 17,
      "context" : "c) Amygdala is the central part of the limbic system of mammals and has a principal role in emotional learning [18][26].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "c) Amygdala is the central part of the limbic system of mammals and has a principal role in emotional learning [18][26].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "It has connections to the thalamus, orbitofrontal cortex, and hypothalamus [25], [26].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 23,
      "context" : "During emotional learning, the amygdala participates in reacting to emotional stimuli, storing emotional responses [29], evaluating positive and negative reinforcement [30], learning the association between unconditioned and conditioned stimuli [19], [20], [31], predicting the association between stimuli and future reinforcement [31], and forming an association between neutral stimuli and emotionally charged stimuli [30].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 18,
      "context" : "During emotional learning, the amygdala participates in reacting to emotional stimuli, storing emotional responses [29], evaluating positive and negative reinforcement [30], learning the association between unconditioned and conditioned stimuli [19], [20], [31], predicting the association between stimuli and future reinforcement [31], and forming an association between neutral stimuli and emotionally charged stimuli [30].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 19,
      "context" : "During emotional learning, the amygdala participates in reacting to emotional stimuli, storing emotional responses [29], evaluating positive and negative reinforcement [30], learning the association between unconditioned and conditioned stimuli [19], [20], [31], predicting the association between stimuli and future reinforcement [31], and forming an association between neutral stimuli and emotionally charged stimuli [30].",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 16,
      "context" : "The diagram shows the pattern of fear conditioning, which needs to be clarified [17].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : "The basolateral part has the bidirectional link to the insular cortex and orbital cortex [18], [20], [21], [25], [26] and performs the main role in mediating memory consolidation [32] and providing the primary response, and is divided into three parts: the lateral, basal, and accessory basal [25], [29].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 19,
      "context" : "The basolateral part has the bidirectional link to the insular cortex and orbital cortex [18], [20], [21], [25], [26] and performs the main role in mediating memory consolidation [32] and providing the primary response, and is divided into three parts: the lateral, basal, and accessory basal [25], [29].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 20,
      "context" : "The basolateral part has the bidirectional link to the insular cortex and orbital cortex [18], [20], [21], [25], [26] and performs the main role in mediating memory consolidation [32] and providing the primary response, and is divided into three parts: the lateral, basal, and accessory basal [25], [29].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 24,
      "context" : "The basolateral part has the bidirectional link to the insular cortex and orbital cortex [18], [20], [21], [25], [26] and performs the main role in mediating memory consolidation [32] and providing the primary response, and is divided into three parts: the lateral, basal, and accessory basal [25], [29].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 23,
      "context" : "The basolateral part has the bidirectional link to the insular cortex and orbital cortex [18], [20], [21], [25], [26] and performs the main role in mediating memory consolidation [32] and providing the primary response, and is divided into three parts: the lateral, basal, and accessory basal [25], [29].",
      "startOffset" : 299,
      "endOffset" : 303
    }, {
      "referenceID" : 19,
      "context" : "The basal and accessory basal parts participate in mediating the contextual conditioning [20], [25].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 20,
      "context" : "The centeromedial part, which is the main output for the basaloteral part [26], is divided into the central and medial parts [18], [20], [25], [26].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "The centeromedial part, which is the main output for the basaloteral part [26], is divided into the central and medial parts [18], [20], [25], [26].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 19,
      "context" : "The centeromedial part, which is the main output for the basaloteral part [26], is divided into the central and medial parts [18], [20], [25], [26].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 20,
      "context" : "The centeromedial part, which is the main output for the basaloteral part [26], is divided into the central and medial parts [18], [20], [25], [26].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 20,
      "context" : "It is responsible for the hormonal aspects of emotional reactions [25] or for mediating the expression of the emotional responses [25], [26].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 17,
      "context" : "It also evaluates and corrects reward and punishment [18]-[21], [33][37], selects goals, makes decisions for a quick response to punishment [18], [23], [25]-[36], and prevents inappropriate responses of the amygdala.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 25,
      "context" : "It also evaluates and corrects reward and punishment [18]-[21], [33][37], selects goals, makes decisions for a quick response to punishment [18], [23], [25]-[36], and prevents inappropriate responses of the amygdala.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 29,
      "context" : "It also evaluates and corrects reward and punishment [18]-[21], [33][37], selects goals, makes decisions for a quick response to punishment [18], [23], [25]-[36], and prevents inappropriate responses of the amygdala.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 17,
      "context" : "It also evaluates and corrects reward and punishment [18]-[21], [33][37], selects goals, makes decisions for a quick response to punishment [18], [23], [25]-[36], and prevents inappropriate responses of the amygdala.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 28,
      "context" : "It also evaluates and corrects reward and punishment [18]-[21], [33][37], selects goals, makes decisions for a quick response to punishment [18], [23], [25]-[36], and prevents inappropriate responses of the amygdala.",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 29,
      "context" : "The medial part forms and memorizes reinforcement–stimulus association, and also has role in providing responses and monitoring them, whereas the lateral part evaluates the response and provides punishment [37].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 30,
      "context" : "It was applied for simulating artificial soccer playing [38], and its results were fairly good.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 31,
      "context" : "The Cathexis model [39] was another emotional agent developed that reacted to an environment by imitating an emotional decision-making process in humans.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 32,
      "context" : "The model of the mind [40] was developed as a modular artificial agent to generate emotional behavior for making decisions.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 33,
      "context" : "An agent architecture that was called Emotionbased Robotic Agent Development (in reverse order, DARE) was developed on the basis of the somatic marker theory; it was tested in a multi-agent system and showed ability in modeling social and emotional behavior [41].",
      "startOffset" : 258,
      "endOffset" : 262
    }, {
      "referenceID" : 6,
      "context" : "b) Emotion-based controller: The first practical implementation of an emotion-based controller is BELBIC (Brain Emotional Learning-Based Intelligent Controller) [7].",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 6,
      "context" : "It was developed on the basis of Moren and Balkenius computational model [7], [23], [42].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 34,
      "context" : "It was developed on the basis of Moren and Balkenius computational model [7], [23], [42].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 35,
      "context" : "The BELBIC has been successfully employed for a number of applications: controlling heating and air conditioning [43] of aerospace launch vehicles [44], intelligent washing machines [45], and trajectory tracking of stepper motor [46].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 36,
      "context" : "The BELBIC has been successfully employed for a number of applications: controlling heating and air conditioning [43] of aerospace launch vehicles [44], intelligent washing machines [45], and trajectory tracking of stepper motor [46].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 37,
      "context" : "The BELBIC has been successfully employed for a number of applications: controlling heating and air conditioning [43] of aerospace launch vehicles [44], intelligent washing machines [45], and trajectory tracking of stepper motor [46].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 38,
      "context" : "The BELBIC has been successfully employed for a number of applications: controlling heating and air conditioning [43] of aerospace launch vehicles [44], intelligent washing machines [45], and trajectory tracking of stepper motor [46].",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 39,
      "context" : "Another emotionbased intelligent controller is a neuro-fuzzy controller [47], which was integrated with emotion-based performance measurement to tune the parameters of the controller.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 40,
      "context" : "Application of an emotion-based controller robotics was proposed in [48], which is an interesting example of applying emotional concepts in robotic applications and imitated the reinforcement learning aspect of emotional processing.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "Specifically, the BELBIC has been proven to outperform others in terms of simplicity, reliability, and stability [7], [39]-[42].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 31,
      "context" : "Specifically, the BELBIC has been proven to outperform others in terms of simplicity, reliability, and stability [7], [39]-[42].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 34,
      "context" : "Specifically, the BELBIC has been proven to outperform others in terms of simplicity, reliability, and stability [7], [39]-[42].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 41,
      "context" : "Hippocampus-neocortex and amygdala hippocampus model have been proposed as neural network models [49], [50].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 42,
      "context" : "Hippocampus-neocortex and amygdala hippocampus model have been proposed as neural network models [49], [50].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "Several emotionbased prediction models [6], [8]-[15] have been developed to model the complex systems.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "Several emotionbased prediction models [6], [8]-[15] have been developed to model the complex systems.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "Several emotionbased prediction models [6], [8]-[15] have been developed to model the complex systems.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : ", auroral electrojec (AE) index prediction [9], solar activity prediction [6], [8], [10], [11]-[13], [14], and financial and chaotic time series prediction [6], [8]-[14].",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : ", auroral electrojec (AE) index prediction [9], solar activity prediction [6], [8], [10], [11]-[13], [14], and financial and chaotic time series prediction [6], [8]-[14].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : ", auroral electrojec (AE) index prediction [9], solar activity prediction [6], [8], [10], [11]-[13], [14], and financial and chaotic time series prediction [6], [8]-[14].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 9,
      "context" : ", auroral electrojec (AE) index prediction [9], solar activity prediction [6], [8], [10], [11]-[13], [14], and financial and chaotic time series prediction [6], [8]-[14].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 10,
      "context" : ", auroral electrojec (AE) index prediction [9], solar activity prediction [6], [8], [10], [11]-[13], [14], and financial and chaotic time series prediction [6], [8]-[14].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 12,
      "context" : ", auroral electrojec (AE) index prediction [9], solar activity prediction [6], [8], [10], [11]-[13], [14], and financial and chaotic time series prediction [6], [8]-[14].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : ", auroral electrojec (AE) index prediction [9], solar activity prediction [6], [8], [10], [11]-[13], [14], and financial and chaotic time series prediction [6], [8]-[14].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : ", auroral electrojec (AE) index prediction [9], solar activity prediction [6], [8], [10], [11]-[13], [14], and financial and chaotic time series prediction [6], [8]-[14].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 7,
      "context" : ", auroral electrojec (AE) index prediction [9], solar activity prediction [6], [8], [10], [11]-[13], [14], and financial and chaotic time series prediction [6], [8]-[14].",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : ", auroral electrojec (AE) index prediction [9], solar activity prediction [6], [8], [10], [11]-[13], [14], and financial and chaotic time series prediction [6], [8]-[14].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : ", Lorenz, Henon, Mackey-Glass, Ikeda) [1]-[15], [51]-[55].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : ", Lorenz, Henon, Mackey-Glass, Ikeda) [1]-[15], [51]-[55].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 43,
      "context" : ", Lorenz, Henon, Mackey-Glass, Ikeda) [1]-[15], [51]-[55].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 47,
      "context" : ", Lorenz, Henon, Mackey-Glass, Ikeda) [1]-[15], [51]-[55].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "2) Generalization Regression Neural Network (GRNN) [1] differs from BELPM in its number of neurons (i.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "4) Local Linear Neuro Fuzzy Models (LLNF) and BELPM can both be considered as types of “local modeling” [2] algorithms.",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "5) Modular neural network is a combination of several modules with different inputs [2] without any connection with others.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "6) Hybrid structures that are defined in [1], differ from BELPM in receiving the input data.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 48,
      "context" : "calculated by using the W-kNN algorithm [56]:",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 48,
      "context" : "Any arbitrary function that holds the following properties given can be considered as the kernel function [56],[57 ]: 1) For all d , K( d ) 0 .",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 49,
      "context" : "Any arbitrary function that holds the following properties given can be considered as the kernel function [56],[57 ]: 1) For all d , K( d ) 0 .",
      "startOffset" : 111,
      "endOffset" : 116
    }, {
      "referenceID" : 48,
      "context" : "output, test r [56],[57].",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 49,
      "context" : "output, test r [56],[57].",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "The first layer consists of a k nodes (“adaptive or square” [3] nodes)",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "In the following, we explain how the BELPM uses the combination of two learning methods: the SD [3] and LSE to learn the input–output mapping (the stimulus–response association).",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "1) First learning phase: At the first learning phase, a hybrid learning algorithm [3] that is a combination of SD and LSE is used to update the learning parameters of AMYG (e.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "equal to [1,-1,0].",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "Lorenz Time Series The Lorenz time series [5], [9], [58] was chosen as the first test, given by using (41) and (42).",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : "Lorenz Time Series The Lorenz time series [5], [9], [58] was chosen as the first test, given by using (41) and (42).",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 50,
      "context" : "Lorenz Time Series The Lorenz time series [5], [9], [58] was chosen as the first test, given by using (41) and (42).",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "01 s [5], [9], [58], and the embedded dimension is selected as three.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 8,
      "context" : "01 s [5], [9], [58], and the embedded dimension is selected as three.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 50,
      "context" : "01 s [5], [9], [58], and the embedded dimension is selected as three.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 50,
      "context" : "4867 [58] 0.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 50,
      "context" : "1682 [58] 0.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 51,
      "context" : "The data-driven models are: Nonlinear Autoregressive model with eXogenous input (Hybrid NARX-Elman RNN) [59], Evolving Recurrent Neural Networks (ERNN) [60], Radial Basis Function (RBF), multilayer perceptron (MLP) [5], Support Vector Regression (SVR), Tapped Delay Line Multilayer Perceptron (TDL-MLP), Distributed Local Experts based on Vector_Quantization using Information Theoretic learning (DLE-VQIT) [61], Cooperative Coevolution of Elman Recurrent Neural Networks (CCRNN) [62], Functional Weights Wavelet Neural Network-based state-dependent AutoRegressive (FWWNN-AR) [63], Recurrent Neural Network trained with Real-time Recurrent Learning (RNNRTRL), Recurrent Neural Network trained with the secondorder Extended Kalman Filter (RNN-EKF), Recurrent Neural Network trained with the algorithm and BackPropagation Through Time (BPTT), feedforward Multi layer Perceptron trained with the Bayesian Levenberg–Marquardt (MLP-BLM), and recursive second-order training of Recurrent Neural Networks via a Recursive Bayesian Levenberg–Marquardt (RBLM-RNN) algorithm [65].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 52,
      "context" : "The data-driven models are: Nonlinear Autoregressive model with eXogenous input (Hybrid NARX-Elman RNN) [59], Evolving Recurrent Neural Networks (ERNN) [60], Radial Basis Function (RBF), multilayer perceptron (MLP) [5], Support Vector Regression (SVR), Tapped Delay Line Multilayer Perceptron (TDL-MLP), Distributed Local Experts based on Vector_Quantization using Information Theoretic learning (DLE-VQIT) [61], Cooperative Coevolution of Elman Recurrent Neural Networks (CCRNN) [62], Functional Weights Wavelet Neural Network-based state-dependent AutoRegressive (FWWNN-AR) [63], Recurrent Neural Network trained with Real-time Recurrent Learning (RNNRTRL), Recurrent Neural Network trained with the secondorder Extended Kalman Filter (RNN-EKF), Recurrent Neural Network trained with the algorithm and BackPropagation Through Time (BPTT), feedforward Multi layer Perceptron trained with the Bayesian Levenberg–Marquardt (MLP-BLM), and recursive second-order training of Recurrent Neural Networks via a Recursive Bayesian Levenberg–Marquardt (RBLM-RNN) algorithm [65].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 4,
      "context" : "The data-driven models are: Nonlinear Autoregressive model with eXogenous input (Hybrid NARX-Elman RNN) [59], Evolving Recurrent Neural Networks (ERNN) [60], Radial Basis Function (RBF), multilayer perceptron (MLP) [5], Support Vector Regression (SVR), Tapped Delay Line Multilayer Perceptron (TDL-MLP), Distributed Local Experts based on Vector_Quantization using Information Theoretic learning (DLE-VQIT) [61], Cooperative Coevolution of Elman Recurrent Neural Networks (CCRNN) [62], Functional Weights Wavelet Neural Network-based state-dependent AutoRegressive (FWWNN-AR) [63], Recurrent Neural Network trained with Real-time Recurrent Learning (RNNRTRL), Recurrent Neural Network trained with the secondorder Extended Kalman Filter (RNN-EKF), Recurrent Neural Network trained with the algorithm and BackPropagation Through Time (BPTT), feedforward Multi layer Perceptron trained with the Bayesian Levenberg–Marquardt (MLP-BLM), and recursive second-order training of Recurrent Neural Networks via a Recursive Bayesian Levenberg–Marquardt (RBLM-RNN) algorithm [65].",
      "startOffset" : 215,
      "endOffset" : 218
    }, {
      "referenceID" : 53,
      "context" : "The data-driven models are: Nonlinear Autoregressive model with eXogenous input (Hybrid NARX-Elman RNN) [59], Evolving Recurrent Neural Networks (ERNN) [60], Radial Basis Function (RBF), multilayer perceptron (MLP) [5], Support Vector Regression (SVR), Tapped Delay Line Multilayer Perceptron (TDL-MLP), Distributed Local Experts based on Vector_Quantization using Information Theoretic learning (DLE-VQIT) [61], Cooperative Coevolution of Elman Recurrent Neural Networks (CCRNN) [62], Functional Weights Wavelet Neural Network-based state-dependent AutoRegressive (FWWNN-AR) [63], Recurrent Neural Network trained with Real-time Recurrent Learning (RNNRTRL), Recurrent Neural Network trained with the secondorder Extended Kalman Filter (RNN-EKF), Recurrent Neural Network trained with the algorithm and BackPropagation Through Time (BPTT), feedforward Multi layer Perceptron trained with the Bayesian Levenberg–Marquardt (MLP-BLM), and recursive second-order training of Recurrent Neural Networks via a Recursive Bayesian Levenberg–Marquardt (RBLM-RNN) algorithm [65].",
      "startOffset" : 480,
      "endOffset" : 484
    }, {
      "referenceID" : 54,
      "context" : "The data-driven models are: Nonlinear Autoregressive model with eXogenous input (Hybrid NARX-Elman RNN) [59], Evolving Recurrent Neural Networks (ERNN) [60], Radial Basis Function (RBF), multilayer perceptron (MLP) [5], Support Vector Regression (SVR), Tapped Delay Line Multilayer Perceptron (TDL-MLP), Distributed Local Experts based on Vector_Quantization using Information Theoretic learning (DLE-VQIT) [61], Cooperative Coevolution of Elman Recurrent Neural Networks (CCRNN) [62], Functional Weights Wavelet Neural Network-based state-dependent AutoRegressive (FWWNN-AR) [63], Recurrent Neural Network trained with Real-time Recurrent Learning (RNNRTRL), Recurrent Neural Network trained with the secondorder Extended Kalman Filter (RNN-EKF), Recurrent Neural Network trained with the algorithm and BackPropagation Through Time (BPTT), feedforward Multi layer Perceptron trained with the Bayesian Levenberg–Marquardt (MLP-BLM), and recursive second-order training of Recurrent Neural Networks via a Recursive Bayesian Levenberg–Marquardt (RBLM-RNN) algorithm [65].",
      "startOffset" : 576,
      "endOffset" : 580
    }, {
      "referenceID" : 56,
      "context" : "The data-driven models are: Nonlinear Autoregressive model with eXogenous input (Hybrid NARX-Elman RNN) [59], Evolving Recurrent Neural Networks (ERNN) [60], Radial Basis Function (RBF), multilayer perceptron (MLP) [5], Support Vector Regression (SVR), Tapped Delay Line Multilayer Perceptron (TDL-MLP), Distributed Local Experts based on Vector_Quantization using Information Theoretic learning (DLE-VQIT) [61], Cooperative Coevolution of Elman Recurrent Neural Networks (CCRNN) [62], Functional Weights Wavelet Neural Network-based state-dependent AutoRegressive (FWWNN-AR) [63], Recurrent Neural Network trained with Real-time Recurrent Learning (RNNRTRL), Recurrent Neural Network trained with the secondorder Extended Kalman Filter (RNN-EKF), Recurrent Neural Network trained with the algorithm and BackPropagation Through Time (BPTT), feedforward Multi layer Perceptron trained with the Bayesian Levenberg–Marquardt (MLP-BLM), and recursive second-order training of Recurrent Neural Networks via a Recursive Bayesian Levenberg–Marquardt (RBLM-RNN) algorithm [65].",
      "startOffset" : 1064,
      "endOffset" : 1068
    }, {
      "referenceID" : 4,
      "context" : "01s [5], [58] and three, respectively.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 50,
      "context" : "01s [5], [58] and three, respectively.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 54,
      "context" : "FWWNN[63] 9.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 51,
      "context" : "8e-15 1500,1000 NN+AR, 1 step noiseless NARX[59] 1.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 11,
      "context" : "9e-10 1500,1000 AR, 1 step noiseless BELRFS[12] 4.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 52,
      "context" : "9e-10 1500,1000 NF+BEL, 1 step noiseless ERNN[60] 9.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "9e-10 1500,1000 NN, 1 step noiseless RBF[5] 1.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "4e-9 1500,1000 NN, 1 step noiseless MLP[5] 5.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 55,
      "context" : "6e-4 ---------, 1 step noiseless LSSVMs[64] 6.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 56,
      "context" : "01 RBLMRNN[65] 9.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 53,
      "context" : "CCRNN [62] 7.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 55,
      "context" : "7e-4 500,500 NN,2 step noiseless LLNF[64] 2.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 56,
      "context" : "05 MLP_BLM[65] 8.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 56,
      "context" : "05 MLP_EKF[65] 1.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 56,
      "context" : "05 RNNRTRL[65] 1.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 56,
      "context" : "RNN-BPTT[65] 1.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 56,
      "context" : "05 RNN-EKF[65] 1.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "3315 RBF[5] 0.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 4,
      "context" : "0872 42 neuron ------LoLiMoT[5] 0.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : "0590[5] 0.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 4,
      "context" : "RBF[5] 1.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "4e-9 1500,1000 NN, 1 step noiseless MLP[5] 5.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 55,
      "context" : "001 LSSVMs[64] 4.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 56,
      "context" : "05 RBLMRNN[65] 6.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 56,
      "context" : "RNN-EKF[65] 8.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 56,
      "context" : "05 MLP-BLM[65] 8.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 56,
      "context" : "01 RNN-BPTT[65] 1.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 56,
      "context" : "05 RNNRTRL[65] 1.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 39,
      "context" : "improvements in the model would be made on the basis of kdTree data structure [47] to address “the curse of dimensionality” [1] problem and decrease the computational time complexity of BELPM.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "improvements in the model would be made on the basis of kdTree data structure [47] to address “the curse of dimensionality” [1] problem and decrease the computational time complexity of BELPM.",
      "startOffset" : 124,
      "endOffset" : 127
    } ],
    "year" : 2016,
    "abstractText" : "Abstract— This study suggests a new prediction model for chaotic time series inspired by the brain emotional learning of mammals. We describe the structure and function of this model, which is referred to as BELPM (Brain Emotional LearningBased Prediction Model). Structurally, the model mimics the connection between the regions of the limbic system, and functionally it uses weighted k nearest neighbors to imitate the roles of those regions. The learning algorithm of BELPM is defined using steepest descent (SD) and the least square estimator (LSE). Two benchmark chaotic time series, Lorenz and Henon, have been used to evaluate the performance of BELPM. The obtained results have been compared with those of other prediction methods. The results show that BELPM has the capability to achieve a reasonable accuracy for long-term prediction of chaotic time series, using a limited amount of training data and a reasonably low computational time.",
    "creator" : "Microsoft® Word 2010"
  }
}