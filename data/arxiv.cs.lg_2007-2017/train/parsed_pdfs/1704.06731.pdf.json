{
  "name" : "1704.06731.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Batch-Expansion Training: An Efficient Optimization Paradigm for Machine Learning",
    "authors" : [ "Michał Dereziński" ],
    "emails" : [ "MDEREZIN@UCSC.EDU", "DHRUVM@FB.COM", "KEERTHI@MICROSOFT.COM", "VISHY@UCSC.EDU", "MWEIMER@MICROSOFT.COM" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n06 73\n1v 1\n[ cs\na framework for running a batch optimizer on a gradually expanding dataset. As opposed to stochastic approaches, batches do not need to be resampled i.i.d. at every iteration, thus making BET more resource efficient in a distributed setting, and when disk-access is constrained. Moreover, BET can be easily paired with most batch optimizers, does not require any parameter-tuning, and compares favorably to existing stochastic and batch methods. We show that when the batch size grows exponentially with the number of outer iterations, BET achieves optimal Õ(1/ǫ) data-access convergence rate for strongly convex objectives."
    }, {
      "heading" : "1. Introduction",
      "text" : "State-of-the-art optimization algorithms used in machine learning broadly tend to fall into two main categories: batch methods, which visit the entire dataset once before performing an expensive parameter update, and stochastic methods, which rely on a small subset of training data, to apply quick parameter updates at a much greater frequency. Both approaches present different trade-offs. Stochastic updates often provide very good early performance, since they can update the parameters a considerable number of times before even a single batch update finishes. On the other hand, by accessing the full dataset at each step,\nbatch algorithms can better utilize second-order information about the loss function, while taking advantage of parallel and distributed computing architectures. Finding approaches that provide the best of both worlds is an important area of research. In this paper, we propose a new framework which –while closer to batch in spirit – enjoys the benefits of stochastic methods. In addition, our framework addresses a practical issue observed in real-world industrial settings, which we now describe.\nIn industrial server farms, compute resources for large jobs become available only in a phased manner. When running a batch optimizer, which requires loading the entire dataset, one has to wait until all the machines become available before beginning the computation. Moreover, the training data, which typically consists of user logs, is distributed across multiple locations. This data needs to be normalized, often by communicating summary statistics or subsets of the data across the network. Since stochastic optimization algorithms only deal with a subset of the data at a time, one can largely avoid the bottleneck of data normalization by preparing mini-batches at a time. However, now each data point needs to be visited multiple times randomly, and this requires performingmany random accesses from a hard-disk or network attached storage (NAS), which is inherently slow. Moreover, extending stochastic optimization to the distributed setting is still an active area of research. This raises the question of whether the compute and dataavailability delays could be avoided in batch methods, and how that would affect the training time.\nWe propose Batch-Expansion Training (BET), a new op-\ntimization framework which addresses the above issues. Our framework hinges on the following observation: initially, when only a subset of the data is available, the statistical error (the error that arises because one is observing a sample from the true underlying data distribution) is large. Therefore, we can tolerate a large optimization error (the error that arises because of the iterative nature of the batch optimizer). However, as the sample size increases, the statistical error decreases and the corresponding optimization error that we can tolerate also decreases. BET exploits this by initially training models with large optimization error on smaller subsets of the data, and iteratively loading more training data, and driving down the optimization error. Relying on classical results in statistical learning theory, we show that for optimal performance, the data size should grow exponentially with the number of iterations. Moreover, we propose a simple and efficient algorithm, which can easily be paired with most batch optimizers, and does not require any parameter-tuning. Experiments using Nonlinear Conjugate Gradient (CG), as well as a Newton-CG method, demonstrate the versatility of our framework. We show that for strongly convex losses BET achieves the same asymptotic Õ(1/ǫ) convergence rate as SGD in terms of data accesses (and is strictly faster than regular batch updates). However, unlike stochastic methods, BET reuses all of the data that has already been loaded into memory, so when we take the cost of data-loading into account, experiments on multiple datasets show that our method outperforms stochastic as well as batch techniques."
    }, {
      "heading" : "2. Related Work",
      "text" : "There has been a recent explosion of interest in optimization methods for machine learning, both in the batch and stochastic paradigms. Algorithms like Stochastic Variance Reduced Gradient method (Johnson & Zhang, 2013) and related approaches (Schmidt et al., 2013; Defazio et al., 2014)mix SGD-like steps with some batch computations to control the stochastic noise. Others have proposed to parallelize stochastic training through large mini-batches (Li et al., 2014; Dekel et al., 2012). However, these methods do not address the issues of compute and data-availability delays that we discussed above.\nAdditionally, two-stage approaches have been proposed (Agarwal et al., 2014; Shalev-Shwartz & Zhang, 2013), which employ SGD at the beginning followed by a batch optimizer (e.g., L-BFGS). These methods are much more limited than BET, which allows for multiple stages of optimization, with clear practical benefits.\nInterleaving computation with data loading was shown to have significant practical benefits by Matsushima et al. (2012). However, that work is confined to training on a single machine and did not provide any theoretical conver-\ngence guarantees. In contrast, we largely focus on the distributed setting and provide convergence guarantees.\nThe most relevant to our work is a stochastic variant of the Newton-CG method with increasing mini-batches (Byrd et al., 2012) as well as a similar approach proposed in (Friedlander & Schmidt, 2012). The algorithms proposed there still rely on stochastic sampling, which makes them less resource-efficient than BET, and not applicable to certain distributed settings (see Section 5 for a detailed comparison). The convergence guarantees offered in (Byrd et al., 2012) are limited to using gradient descent as the inner optimizer, and heavily rely on the independence conditions present in stochastic sampling. We use different proof techniques for time complexity analysis, which make the results applicable to a wider range of inner optimizers and do not require i.i.d. sampling."
    }, {
      "heading" : "3. Batch-Expansion Training",
      "text" : "We consider a standard composite convex optimization problem arising in regularized linear prediction. Given a dataset Z = {(xi, yi)}Ni=1, we aim to approximately minimize the average regularized loss\nf̂ , 1\nN\nN∑\ni=1\nℓzi(w) + λ\n2 ‖w ‖2, (1)\nwhere zi = (xi, yi) and ℓzi(w) , ℓ(〈w, φ(xi)〉, yi) is the loss of predicting with a linear model 〈w, φ(xi)〉 against a target label yi. Any iterative optimization algorithm in this setting will produce a sequence of models {wt}Tt=1 with the goal that wT has small optimization error ĝ(wT ) with respect to the exact optimum ŵ ∗ , where\nĝ(w) , f̂(w)− f̂(ŵ∗) and ŵ∗ , argmin w f̂(w).\nNote that our true goal is to predict well on an unseen example z = (x, y) coming from an underlying distribution. The right regularization λ for this task can be determined experimentally or from the statistical guarantees of loss function ℓ, as discussed in (Sridharan et al., 2009; Shalev-Shwartz& Srebro, 2008). In this paper, however, we will assume that an acceptable λwas chosen and concentrate on the problem of minimizing f̂ ."
    }, {
      "heading" : "3.1. Linear convergence of the batch optimizer",
      "text" : "Adding 2-norm regularization in function f̂ makes it λstrongly convex. In this setting, many popular batch optimization algorithms enjoy linear convergence rate (Nocedal & Wright, 2006). Namely, given arbitrary model w and any c > 1, after at most O(log(c)) iterations - where a single iteration can look at the entire dataset - we can reduce its optimization error by a multiplicative factor of c,\nobtainingw′ such that\nĝ(w′) ≤ c−1 · ĝ(w).\nNote, that the runtime of a single iteration will depend on data size N , but the number of needed iterations does not.\nSetting c = 2, we observe that only a constant number of batch iterations is necessary for a linearly converging optimizer to halve the optimization error of w. This constant depends on the convergence rate enjoyed by the method. For example, in the case of batch gradient descent we need O(1/λ) iterations to reduce the optimization error by a factor of 2 (Bubeck, 2015) (note the dependence on the strong convexity coefficient), while other methods (like L-BFGS) can achieve better rates of convergence (Lin et al., 2007). Furthermore, the time complexity of performing a single iteration for many of those algorithms (including GD and LBFGS) is linearly proportional to the data size. We refer to methods exhibiting both linear convergence (with respect to a given loss) and linear time complexity of a single iteration as linear optimizers. From now on, we only consider this class of methods."
    }, {
      "heading" : "3.2. Dataset size selection",
      "text" : "The general task of an optimization algorithm is to return a model with small optimization error ĝ(w) ≤ ǫ. For selecting an effective optimization error tolerance ǫ in a machine\nlearning problem, we often look at the estimation error exhibited by the objective, i.e. how much f̂ deviates from the expected regularized loss measured on a random unseen example. As discussed in (Shalev-Shwartz & Srebro, 2008), the effective optimization error tolerance should be proportional to the estimation error of f̂ , because optimizing beyond that point does not yield any improvement on unseen data. Note that under standard statistical assumptions, the more data we use, the smaller the estimation error becomes (Sridharan et al., 2009), and thus, the effective optimization error tolerance should also be decrease (see Figure 1a).\nConsider the scenario where data is abundant, but we have a limited time budget for training the model1. In this case, a practitioner would select the data size so that we can reach the smallest effective optimization error tolerance (dashed curve in Figure 1a) within the allotted training time. Vertical line in Figure 1a shows the reduction in optimization error that the algorithm has to achieve to obtain the desired tolerance. If we use a batch optimizer for this task, the training time is determined by two factors. First, as the dataset grows, each iteration takes longer (e.g. for linear optimizers the iteration time is proportional to the data size, as discussed in Section 3.1). Second, the algorithm has to perform larger number of iterations, the smaller the effective optimization error tolerance. Combined, those two effects result in training time, which, for linear optimizers, grows faster than linearly with the data size (see Figure 1c).\nIn this paper, we propose that instead of finding the optimal dataset size at the beginning, we first load a small subset of the data, train the model until we reach the corresponding effective optimization error tolerance, then we load more data, and optimize further, etc. Vertical lines in Figure 1b illustrate how the optimization error is reduced in the multiple stages working with increasing dataset sizes. This procedure benefits from inexpensive iterations in the early stages, as shown in Figure 1d, with vertical lines corresponding to the training time for each data size, and the horizontal line showing the time budget (note that the budget shown for this method is the same as the one used in Figure 1c). Thus, our approach is able to reach smaller effective optimization error tolerance within the same time budget compared to fixing the dataset size at the beginning, as seen by comparing Figures 1a and 1b."
    }, {
      "heading" : "3.3. Exponentially increasing batches",
      "text" : "We now precisely formulate the idea of training with gradually increasing data size. Suppose our goal is to return a model ŵ with optimization error ĝ(ŵ) ≤ ǫ. In the procedure described above, we seek to obtain a sequence of gradually improving models w1, . . . ,wT−1 before we reach\n1This is a reasonable implementation practice in many web applications.\nwT = ŵ. Any of the intermediate models (say, model wt for t < T ) has a large optimization error relative to ŵ, so to computewt we can minimize an objective function with correspondingly large estimation error. Thus, we will first obtainw1 with optimization error 2 ǫ1 using n1 data points, next we pick a smaller error tolerance ǫ2 < ǫ1 and bigger data size n2 > n1, computing a better model w2, etc. so that in the end we reach ǫT = ǫ. Algorithm 1 demonstrates a simple instantiation of this strategy, where at each stage we double the data size:\nAlgorithm 1 Batch-Expansion Training\nPick initial modelw0 Load first n1 data points for t = 1..T do wt ← run κt iterations on all nt loaded data points nt+1 ← btnt increase data size, bt > 1 Load additional nt+1 − nt data points end for return wT\nThe basis for the number of inner iterations κt will be explained below. Note that at stage t of the process we are working with an estimate of the loss function\nf̂t(w) , 1\nnt\nnt∑\ni=1\nℓzi(w) + λ\n2 ‖w ‖2,\nwhich tends to f̂ with increasing t. At this stage, the optimizer is in fact converging to an approximate optimum\nŵ ∗ t , argmin\nw f̂t(w),\nrather than to the minimizer of f̂ , ŵ∗. To decide how much data is needed at each stage we will describe the relationship between data size nt and the desired optimization error ǫt. Note, that model wt obtained at stage t is assumed to satisfy ĝt(wt) ≤ ǫt, where ĝt represents optimization error for the given data subset, i.e. ĝt(w) = f̂t(w) − f̂t(ŵ∗t ). When going to the next stage, the data size increases, and thus we can use optimization error function ĝt+1, which is a better estimate of ĝ, than ĝt is. In Appendix B.2 of (Authors, 2017), we show that function ĝt+1 can be uniformly bounded by ĝt, plus an additional term which can be interpreted as the estimation error (of ĝt with respect to ĝt+1):\nĝt+1(w) ≤ 2 · ĝt(w) +O (1/(λnt)) .\nAs discussed earlier, the effective optimization error for data size nt is proportional to the estimation error suffered by ĝt, hence it is sufficient to demand that\nĝt(wt) ≤ ǫt , O (1/(λnt)) . (2) 2The optimization error for intermediate models wt is com-\nputed using only the loaded portion of the data.\nNote that this makes the optimization error tolerance ǫt inversely proportional to the subset size nt, confirming our intuition that for t < T , since ǫt > ǫ, we can work with a batch of size nt that is smaller thanN .\nTo establish the correct rate of growth for the data size nt, we combine (2) with the observations made in Section 3.1. First, note that the rate of decay of sequence {ǫt} should match the convergence rate of the optimization algorithm, so that inequality ĝt(wt) ≤ ǫt can be satisfied for all t. Recall from Section 3.1, that a linear optimizer takes only a constant number of iterations (call it κ) to reduce the optimization error by a factor of 2. This suggests the following simple strategy: at each stage perform κ iterations of the optimizer, then divide the tolerance level by 2 (matching the convergence rate). Note that based on Equation (2), this corresponds to doubling the data size nt. Thus, we obtain the following simple scheme for data expansion, which maintains the desired relationship between nt and ǫt:\nǫt+1 = ǫt/2, nt+1 = 2 · nt.\nIt is important that nt grows exponentially with t. This allows for a considerable improvement in runtime. Let us say that ǫ0 = O(1) and T stages are needed to reach the final desired tolerance ǫ, i.e. T = log(ǫ0/ǫ) = O(log(1/ǫ)). Moreover, using (2) the suitable size of the full dataset is N = O(1/(λǫ)). If we assume that one iteration of the linear optimizer takes time proportional to the data size, then the time complexity of the optimization when using batchexpansion is given by\nT∑\nt=1\nκnt = κn0\nT∑\nt=1\n2t = O(κN) = O ( κ λǫ ) .\nOn the other hand, when running the same optimizer on full dataset from the beginning, the time complexity becomes\nT∑\nt=1\nκN = κN · T = O ( κ λǫ · log(1/ǫ) ) .\nNote that to establish convergence of the proposed algorithm, it is only required that the dataset is randomly permuted, i.e. that each subset {zi}nti=1 represents a random portion of the data. However, the batches used in different stages do not need to be independent of each other, which is why we can reuse data from previous stages. Section 4 precisely formulates the ideas discussed above. Moreover, a careful analysis of the time complexity for this approach is given in Theorem 4.1, showing that the method indeed exhibits Õ(κ/(λǫ)) convergence rate for optimal values of its input parameters. Next, we will discuss a parameter-free approach, which works well in practice."
    }, {
      "heading" : "3.4. Two-track algorithm",
      "text" : "How many iterations of the proposed expansion procedure should be performed at each stage? From our high-level analysis, we concluded that a roughly constant number of updates should be sufficient for any stage (using a linear optimizer), since each time we aim to make the same multiplicative improvement to the optimization error suffered by our model. However, that constant may depend on the type of loss function, the dataset, as well as the optimizer used, and moreover, in practice the right number of iterations may in fact vary to some extent between the stages. Therefore, we need a practical method of deciding the right time to double the data. Consider the following experiment: we run two optimization tracks in parallel, first one for the batch of size nt, the other for half of that batch. For one slower step on the bigger batch, two faster steps are performed on the smaller one. Which track will make better progress towards the optimum of the bigger batch? If the starting model is far enough from the optimum ŵ ∗ t , then the faster updates will initially have an advantage. However, as the convergence proceeds, only the slower track can get arbitrarily close, so at some point it will move ahead of the fast one (in terms of the loss f̂t(w)). Denote the starting model as wt,0. The secondary track (running on half of the batch) also starts at the same point, denoted as w ′ t−1,0 = wt,0, and they are both updated as follows:\nwt,s+1 ← Update(wt,s, nt), w\n′ t−1,s+1 ← Update(w′t−1,s, nt−1),\nwhere Update(w, n) is one step of the optimizer, with respect to model w, on the batch {zi}ni=1. To reflect faster update time for the second track, we let it make twice as many updates when comparing to the first track. Thus, after s iterations of the above procedure, we can use the following easily testable condition for when the first track becomes better than the second one:\nf̂t(wt,⌊s/2⌋) < f̂t(w ′ t−1,s). (3)\nAlgorithm 2 describes Batch-Expansion Training implemented using the Two-Track strategy. Note that in the algorithm for each step of the first track, we run one step of the second track (as opposed to 2 steps), which reduces the amount of extra computation per iteration, that is used to evaluate Condition (3)."
    }, {
      "heading" : "3.5. Discussion",
      "text" : "The choice to increase data size by a factor of 2 at each stage (rather than by a different factor) is not crucial for the optimization performance (both theoretically and in practice), therefore this parameter does not require tuning. The initial subset size n0 also does not affect performance sig-\nAlgorithm 2 Two-Track Optimizer\nInitializew1,0 = w ′ 0,0 arbitrarily Pick any 2 ≤ n1 = 2n0 < N Initialize s ← 0 and t ← 1 while nt < N do\nwt,s+1 ← Update(wt,s, nt) w\n′ t−1,s+1 ← Update(w′t−1,s, nt−1)\ns ← s+ 1 if f̂t(wt,⌊s/2⌋) < f̂t(w ′ t−1,s) then\nnt+1 ← 2nt w\n′ t,0,wt+1,0 ← wt,s\nt ← t+ 1 s ← 0\nend if\nend while while stopping condition not met do\nwt,s+1 ← Update(wt,s, N) s ← s+ 1\nend while return wt,s\nnificantly - generally, the larger n0 we select, the more updates will be performed before first data expansion, but as long as the initial subset is small enough, total optimization time will remain close to optimal. Thus, Algorithm 2 does not require any tuning to achieve good performance. Moreover, our method can be paired with many popular batch optimizers, and it will automatically adapt its behavior to the selected inner optimizer, as shown in Appendix A.1 of (Authors, 2017).\nIt is important to note that the fraction of data accessed by the algorithm is only gradually expanded as optimization proceeds. Moreover, BET iterates multiple times over the data points that have already been loaded. Thus, it is very resource efficient in a way that can be beneficial with:\nSlow disk-access. Loading data from disk to memory can be a significant bottleneck (Matsushima et al., 2012). Performing multiple iterations over the data points while extra data is being loaded in parallel provides speed-up.\nResource ramp-up. In distributed computing, often not all resources are made available immediately at the beginning of the optimization (Narayanamurthy et al., 2013), which similarly leads to gradual data availability."
    }, {
      "heading" : "4. Complexity Analysis",
      "text" : "In this section, we provide theoretical guarantees for the time complexity of Batch-Expansion Training and compare them to other approaches. For the remainder of this section, we assume that the inner optimizer for some κ > 1 and for\nevery t,w exhibits linear convergence:\nĝt(Update(w, nt)) ≤ (1− (1/κ)) ĝt(w)."
    }, {
      "heading" : "4.1. Data-Access Complexity",
      "text" : "For the sake of complexity analysis, we discuss a parameterized variant of our approach, described in Algorithm 3, and establish complexity results for it. Here, the number of updates needed at each stage is a fixed parameter κ̂.\nAlgorithm 3 Optimal BET\nInput: Target tolerance ǫ Pick ǫ0, n0 Initializew0 ← 0, t ← 0 and κ̂ = ⌈κ log(6)⌉ while 3 · ǫt > ǫ do nt+1 ← 2nt wt,0 ← wt for s = 1..κ̂ do wt,s ← Update(wt,s−1, nt+1)\nend for wt+1 ← wt,κ̂ ǫt+1 ← ǫt/2 t ← t+ 1\nend while T ← t return wT\nAssumptions. We assume that the feature mapping φ(·) (see (1) and the discussion below it) is B-bounded, i.e. it satisfies ‖φ(xi)‖ ≤ B, and that the loss function ℓzi(w) = ℓ(zi,w) is L-Lipschitz in zi for all w. Moreover, we will use the fact that f̂ is λ-strongly convex, due to the use of 2- norm regularization. The result below (proven in Appendix B of (Authors, 2017)) holds with high probability with respect to a random permutation of data.\nTheorem 4.1 For any n0, δ, ǫ there exists ǫ0 such that data-access complexity of Algorithm 3 is\nO ( κ λǫ · (log log(1/ǫ) + log(1/δ)) )\nand w.p. at least 1− δ we have f̂(wT )− f̂(ŵ∗) ≤ ǫ.\nUsing gradient descent as the inner optimizer, we have κ = O(1/λ), so Algorithm 3 reaches data-access complexity Õ(1/(λ2ǫ)) . However, the general nature of this approach allows us to choose a different linear optimizer\nwith better guarantees, like κ = O(1/ √ λ) for accelerated gradient descent. Methods like L-BFGS and other approximate Newton algorithms have been shown to exhibit linear convergence (Lin et al., 2007; Erdogdu&Montanari, 2015) with a rate that does not suffer from such strict dependence on the strong convexity coefficient λ. Hence, when using\nthose optimizers, for most problems we should expect κ to be a small constant factor, in which case data-access complexity becomes Õ(1/(λǫ))."
    }, {
      "heading" : "4.2. Time Complexity",
      "text" : "We will now discuss a simple model of time complexity for BET, which incorporates such aspects as hardwareacceleration and data-loading. Our aim is to highlight certain regimes in the computational architecture landscape which allow for a meaningful comparison of stochastic and batch methods. We compare BET with three other paradigms for running an inner batch optimizer update:\n1. Batch: using the inner optimizer as a regular batch\nmethod on the entire dataset.\n2. Dynamic Sample Method (DSM): applying stochas-\ntic sampling with dynamically increasing sample sizes, proposed in (Byrd et al., 2012), 3. Mini-Batch: applying stochastic sampling with mini-\nbatches of fixed size b (Li et al., 2014).\nFirst, we compare data-access complexity of the methods with their sample complexity N∗(ǫ) (number of unique data points needed to reach generalization error ǫ). For stochastic methods, sample complexity essentially corresponds to their data-access complexity. On the other hand, for batch methods time complexity depends both on the sample sizeN∗(ǫ) and separately on ǫ itself (for example to incorporate linear convergence rate κ log(1/ǫ)). For BatchExpansion Training, data-access complexity also does not equal sample complexity, but the gap does not depend on ǫ.\nAll of the considered methods exhibit sample complexity Õ(1/(λǫ)), however with considerably different constant factors in the stochastic and batch settings, which makes a direct comparisonmore challenging. To that end, we define multiplicative factors for DSM and Mini-Batch3:\nDSM: κd , NDSM(ǫ)/NBET(ǫ), (4) Mini-Batch: κm , NMB(ǫ)/NBET(ǫ). (5)\nTo derive time complexity T∗(ǫ) for the methods, we make the following assumptions on the computing architecture:\n1. Processing one data point takes 1/p units of time, where p is a hardware-acceleration factor. 2. Data points are loaded sequentially, as the optimiza-\ntion progresses, one point per every a units of time. 3. We incur an overhead of s units in between each two\nconsecutive calls to the inner optimizer.\nNote, that this model is by no means exhaustive, for example it does not take into account aspects related to the dimensionality of the data. We can think of this dimension-\n3For BET and Batch, we use the same factor κ, which corresponds to the convergence rate of the inner optimizer\ndependence as being absorbed into the time units used in the analysis, which is reasonable as we are only considering optimizers with linear time dependence on the dimension.\nTable 1 compares time complexities under the proposed model, normalized by the BET sample complexityNBET(ǫ) (thus, the term 1/(λǫ) cancels out from the formulas). Unlike for BET, the available theoretical guarantees for DSM and Mini-Batch apply only to particular inner optimizers (e.g. gradient descent). Note that Mini-Batch time complexity is based on the convergence rate of the EMSO method (Li et al., 2014), (which is better than the rate of simple mini-batch SGD for large b). In this big-picture analysis, for the sake of comparison we extrapolated those results to cover all inner optimizers exhibiting linear convergence (the convergence rate gets absorbed into the factor κ∗). Note, that some bounds for stochastic methods carry an additional log(1/ǫ) term, however there are results for SGD, which avoid that term under additional smoothness assumptions (Bottou & Bousquet, 2007), so for clarity of presentation we elected to leave it out of this comparison. Factors κ, κd and κm dependmainly on the inner optimizer. In our experiments, their values were comparable for all methods, ranging between 2 and 4.\nThe formulas in Table 1 can essentially be divided into three components: data-availability, computation time and sequentiality cost. The last one is only present for MiniBatch, where it can dominate the optimization time, especially when mini-batch size b is small. For other methods, this cost is negligible for small enough ǫ. Moreover, note that by the very nature of non-stochastic approaches (Batch and BET), their dependence on data-availability is only limited by sample complexity of the learning problem, hence there are no additionalmultiplicative factors there. In practice, data-loading can happen concurrently to the computations. BET is especially well suited in that case, since it allows the κ/p term to be absorbed by a.\nAs for the computation time, each method carries a κ∗/p term, with the exception of Batch, which suffers an additional log(1/ǫ) factor, clearly observed experimentally in the comparison between Batch and BET. As we can see, the preferred optimization method depends on the parameters of the architecture as well as the learning problem, but\nwe can reach a few general conclusions:\n1. BET is asymptotically superior to Batch for any\nparametrization of the time complexity model.\n2. If data availability is slow relative to numerical com-\nputations and κd > 1, then BET is faster than DSM. 3. Mini-Batch is slower than other methods when the se-\nquentiality cost is high."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section we present experimental results supporting the theoretical time complexity analysis from Section 4. As the optimization problem we use squared hingeloss for SVMwith 2-norm regularization trained on several standard LIBSVM datasets (see Table 2). The regularization parameter for each dataset was selected by tuning to achieve highest test-set accuracy. All algorithms start with the initial model vectorw set to all zeros. BET was implemented as shown in Algorithm 2. The initial subset size n0 was tested in the range between 100 and 2000 data-points, with minimal effect on performance. As the main inner optimizer for BET we used Sub-sampled Newton-CG (SN) (Byrd et al., 2011). Additional experiments in Appendix A.1 of (Authors, 2017) show how our algorithm performs when paired with nonlinear Conjugate Gradient (Fletcher & Reeves, 1964). We compare BET against:\n1. Fixed Batch strategy using SN as the optimizer, 2. Dynamic Sample Method (DSM) (Byrd et al., 2012), 3. Adagrad (Duchi et al., 2011).\nBoth DSM and Adagrad were tuned to obtain optimal parameter values for each algorithm. Performance of DSM varied considerably depending on its parameters, as discussed in Appendix A.2 of (Authors, 2017). BET does not require any tuning."
    }, {
      "heading" : "5.1. Optimization Time",
      "text" : "In this section, we evaluate algorithms using time complexity model described in Section 4.2. We present the results in terms of log Relative Functional Value Difference\nlog RFVD: log ( (f̂(w)− f̂(ŵ∗))/f̂(ŵ∗) ) . (6)\nFigure 2 plots the simulated runtime with a = 1, p = 10 and s = 5, which is based on the system characteristics of a standard hardware setup. As expected from the time complexity analysis, DSM and Adagrad pay a significant cost for resampling the data at every iteration. We also show wallclock times until reaching test-set accuracy of 0.985 (within 1% of optimum) and 0.995 (within 0.05% of optimum) for BET, DSM and Batch, when ran on the same platform, for the webspam dataset (see Figure 3). Wallclock times for Adagrad were much larger on our platform, and so they are ommitted in Figure 3. We observe that Batch has a considerable disadvantage when the chosen error tolerance is large, hence it is not suited for early stopping. BET achieves best performance for any tolerance.\nTo analyze our time complexity model more generally, first note that for the sake of performance comparison we can reduce it to just two degrees of freedom: p̄ = a · p and s̄ = s/a. Increasing p̄ means reducing computation time relative to the data availability rate. This can be caused by slow disk-loading time relative to performing operations on\nthe memory, as well as hardware acceleration through parallel and distributed computing. Parameter s̄ corresponds to the time overhead spent in between each iteration of the algorithm. We observe that:\n1. Increasing p̄ allows BET to take advantage of running multiple updates on the same data subset (Figure 4). 2. With increasing s̄, Adagrad’s relative performance degrades, since it relies on significantly more sequential\nupdates than either BET or DSM."
    }, {
      "heading" : "5.2. Parallel Experiments",
      "text" : "In this section, we present preliminary results for parallel experiments comparing Batch-Expansion Training with the standard Batch strategy, when using L-BFGS as the inner optimizer. Both methods were implemented in the PETSc (Balay et al., 2016a;b; 1997) framework, with the data split between multiple computing cores. Figure 5 shows the runtime results, in terms of the training objective, for optimizing the regularized logistic loss on the SUSY dataset. Dashed lines correspond to sequential experiments, whereas solid lines describe the convergence when two cores are performing computations in parallel. In this experiment, BET achieved 1.84x speed-up, whereas Batch improved 1.78x. Moreover, we can see that the advantage of BET over Batch increases when dealing with large sample sizes (4 million instances), which follows the asymptotic analysis from Section 4.2."
    }, {
      "heading" : "5.3. Test Set Accuracy",
      "text" : "Finally, we look at how the optimization performance translates to classification accuracy on the test set. Figure 6 shows test set accuracy against simulated runtime under the same time complexity model as Figure 2. We observe similar results, as BET performs better than other methods. On the plots, we marked the moment when BET reaches full dataset. In most cases, by this point the algorithm reaches close to optimum test accuracy. Thus, in many practical applications we can end the optimization by relying on this simple stopping criterion. Moreover, in problems when the full dataset is too abundant, BET will attain optimal performance well before the full data size is reached."
    }, {
      "heading" : "6. Conclusions",
      "text" : "We proposed Batch-Expansion Training, a simple parameter-free method of running batch optimizers in a way that is both theoretically and experimentally competitive with stochastic approaches. BET does not need any tuning and can be paired with various optimizers, while offering advantages in parallel and distributed settings."
    }, {
      "heading" : "A. Additional Experiments",
      "text" : "A.1. Inner Optimizers\nTo demonstrate the flexibility of our framework, in this section we use two different inner optimizers with BET:\n1. Nonlinear Conjugate Gradient method (CG), using\nFletcher-Reeves (Fletcher & Reeves, 1964) formula, with exact line-search; 2. The Sub-sampled Newton-CG (SN) algorithm, de-\nscribed in (Byrd et al., 2012).\nBoth of those methods are linear optimizers that employ strategies for enhancing the basic gradient descent direction. Note, that CG uses a memory vector updated at each iteration, which becomes invalid after every batch expansion, when the loss function changes from f̂t to f̂t+1. To overcome this, we restart the CG update at each stage of the training. Despite this, we show that BET can still be effective at accelerating memory-based algorithms. Subsampled Newton-CG does not require memory, since its updates are self-contained. Instead, it sub-samples a portion of the available data to get an estimate Hessian, and approximately find the Newton direction by performing a number of linear CG steps.\nFigure 7 shows the performance comparison of using BET with the two inner optimizers, contrasted with both of them ran in regular batch mode. First, we observe that subsampled Newton-CG is a much more effective optimizer than Nonlinear CG. Moreover, both of them significantly benefit\nfrom BET. In particular, note that in the cases where data is especially poorly conditioned for CG (e.g. Webspam), using BET helps overcome this problem, putting the method in the rough range of Newton-CG performance.\nNote that, similarly to BET, DSM is a general framework which can be paired with different inner optimizers, however it has an additional limitation that the optimizer cannot use memory accross updates, due to resampling. Thus, this method cannot be used with the CG update unless we restart CG at every step, turning it into plain gradient descent.\nA.2. Parameter Tuning\nMost stochastic optimization methods require parameter tuning to exhibit good performance. This requires restarting the algorithm many times for different parameter values, affecting the overall computation time and ease of use. Batch optimizers, on the other hand, tend to be much less sensitive to their initialization. Here, we look at how the two hybrid methods, DSM and BET, compare in this regard. Both of the approaches rely on an inner batch optimizer. The Sub-sampled Newton-CG algorithm does require us to select the fraction R ∈ [0, 1] of data used to approximate the Hessian, and how many linear CG steps to perform on that fraction. As discussed in (Byrd et al., 2012; 2011), the method is relatively robust to those parameters, and in our experiments setting R = 0.1 and using R−1 = 10 steps of CG worked well on all datasets.\nTurning to the hybrid meta-algorithms, as we noted earlier, BET is essentially parameter free, since it is very robust to the choice of initial subset size n0 (which may be as small as it is practical for a given computational setting). DSM also requires setting the initial sample size n0, and in addition has an extra parameter θ, both of which require some tuning due to the stochastic nature of this method.\nThe sample size selection in DSM is based on comparing two quantities: norm of the gradient of the objective, and a variance estimate of that gradient with respect to the randomness of the sample. The intuition is that if variance is smaller than the norm, then the current sample size is sufficient to perform an effective batch update. Otherwise, we should increase the sample size so that the condition is satisfied once again. Thus, we need a parameter θ describing the variance-to-norm ratio above which we decide to increase the sample size. As shown in Figure 8, the choice of θ considerably affects the performance (and even convergence) of DSM. The value optimal for one dataset may give subpar results for another. Moreover, setting the initial sample size n0 too small may lead to the gradient variance estimate being inadequate, which further disrupts the algorithm. Note, that BET does not suffer from any of those issues, making it much easier to use in practice."
    }, {
      "heading" : "B. Proof Details for Theorem 4.1",
      "text" : "B.1. Proof of Theorem 4.1\nRecall that we denote the approximation error estimate at stage t by ĝt(w) , f̂t(w) − f̂t(ŵ∗t ) and the full approximation error as ĝ(w) , f̂(w) − f̂(ŵ∗). Moreover, note that Algorithm 3 defines the optimization error tolerances recursively as ǫt+1 = ǫt/2. First, we need a uniform convergence result, which is a variant of the one given in (Sridharan et al., 2009):\nLemma 1 For any n0, δ, T , there exists ǫ0 such that\nǫ0 = O(L2B2 log(T/δ) · λ−1), and with probability 1− δ, for allw and all 0 ≤ t < T the following hold:\nĝ0(w0) ≤ ǫ0, (7) ĝt+1(w) ≤ 2 · ĝt(w) + ǫt, (8)\nĝ(w) ≤ 2 · ĝT (w) + ǫT . (9)\nSee Section B.2 for proof. Next, using this lemma, we show the main result.\nProof (of Theorem 4.1) Let κ be the convergence rate of the inner optimizer. Recall, that we set the number of inner iterations of Algorithm 3 to be\nκ̂ , ⌈κ log(6)⌉.\nThis gives us the following bound for the progress that the inner optimizer makes at each stage of the algorithm (for any 0 ≤ t ≤ T ):\nĝt+1(wt+1) ≤ ( 1− 1\nκ\n)κ̂ ĝt+1(wt)\n≤ exp ( − κ̂ κ ) ĝt+1(wt)\n≤ ĝt+1(wt) 6 .\nSuppose that ǫ0 satisfies Lemma 1. We can show by induction that (with probability 1 − δ) for all t ≤ T , model wt is an ǫt-approximate solution for f̂t, i.e. that ĝt(wt) ≤ ǫt. Base case is given by (7). The inductive step follows from:\nĝt+1(wt+1) ≤ ĝt+1(wt) 6 ≤ 2 · ĝt(wt) + ǫt 6\n≤ 2ǫt + ǫt 6 = ǫt 2 = ǫt+1.\nNext, we verify thatwT is an ǫ-approximate solution for f̂ :\nĝ(wT ) ≤ 2 · ĝT (wT ) + ǫT ≤ 3 · ǫT ≤ ǫ.\nFinally, we move on to complexity analysis. The number of iterations in the algorithm is T = O(log(ǫ0/ǫ)) as shown by:\n2T = 2 · ǫ0 ǫT−1 < 6 · ǫ0 ǫ .\nAssuming that one update of the inner optimizer requiresC passes over the data, we obtain the data-access complexity:\nT∑\nt=1\nκ̂ C nt = C κ̂ n0\nT∑\nt=1\n2t\n≤ 2C κ̂ n0 · 2T = O ( n0 ǫ0 · κ\nǫ\n) .\nSee Section B.3 for details regarding the log terms. ✷\nB.2. Proof of Lemma 1\nFirst, we formulate a uniform-convergence bound, which closely resembles Theorem 1 from (Sridharan et al., 2009). The only difference is that they consider PAC setting: sampling an i.i.d. dataset Z from a fixed distribution, and comparing the finite-sample objective f̂ computed using Zwith the true objective f , which is an expectation over the distribution. On the other hand, we consider an increasing sequence of datasets Zt = {zi}nti=1, selected by a uniformly random permutation of the full datasetZ. Note, that we assume the algorithm never observes the full dataset, only loading as much data as needed. Taking the limit of\nN → ∞, the relationship between any subset Zt and the full dataset Z becomes statistically equivalent to i.i.d. sampling from any fixed underlying distribution. Given that our goal is generalization to predicting on new data, that simplification is reasonable, although the analysis does go through in the strict optimization setting, whereN is finite. However, even with this assumption, we still need to describe the relationship between two consecutive subsets in the sequence, which does not fit the i.i.d. sampling model. To that end, we can view Zt as a fraction of elements from Zt+1, selected uniformly at random without replacement. We now describe the relationship between the two consecutive loss estimates in this sequence. Note, that in this section the big-O notation hides only fixed numeric constants.\nLemma 2 With probability 1 − δ, for all w and all 0 ≤ t ≤ T we have\nĝt+1(w) ≤ 2 ĝt(w) +O ( L2B2 log(T/δ)\nλnt\n) . (10)\nProof The proof is very similar to (Sridharan et al., 2009), except we replace standard Rademacher Complexity with Permutational Rademacher Complexity (PRC), proposed in (Tolstikhin et al., 2015). Let us fix t, and consider a specific set of instances Zt+1, from which a random subset Zt is sampled (without replacement). Following (Sridharan et al., 2009), for any r > 0 we define\nHt,r , { ht,rw = ht w\n4kt,r(w) : w ∈ W\n} ,\nwhere\nkt,r(w) , min{k′ ∈ Z+ : f̂t+1(w) ≤ r4k ′}\nand htw(z) , ℓz(w)− ℓz(ŵ∗t+1). Our aim is to analyze the empirical average of the function values fromHt,r evaluated on a given instance set Z:\nh̄t,r Z (w) =\n1 |Z | ∑\nz∈Z\nht,rw (z).\nWe can translate the task of comparing ĝt+1 and ĝt to describe it in terms of the function classHt,r:\nĝt+1(w)− ĝt(w) = ĝt+1(w)− (f̂t(w)− f̂t(ŵ∗t )) ≤ ĝt+1(w)− (f̂t(w)− f̂t(ŵ∗t+1)) = 4kr(w) [ h̄t,r Zt+1 (w)− h̄t,r Zt (w) ] .\nTo compare h̄t,r Zt with h̄t,r Zt+1 we use Theorem 5 (Tolstikhin et al., 2015), which provides transductive risk bounds\nthrough expected PRC of function class Ht,r, conditioned on set Zt+1:\nQ(Ht,r,Zt+1) , E [ Q̂nt,nt/2(Ht,r,Zt) | Zt+1 ] .\nHere, the randomness only comes from selecting Zt as a subset of Zt+1. For any δ > 0, with probability at least 1− δ,\nsup w\n[ h̄t,r Zt+1 (w)− h̄t,r Zt (w) ]\n≤ Q(Ht,r,Zt+1)︸ ︷︷ ︸ Y1 + sup ht,rw ,z |hrw(z)| · O\n  √ log(1/δ)\nnt\n \n︸ ︷︷ ︸ Y2\n.\nNote, that Q(Ht,r,Zt+1) = O(Rnt(Ht,r)) (see (Tolstikhin et al., 2015)), where Rnt is the standard Rademacher Complexity. The remainder of the proof proceeds identically as in (Sridharan et al., 2009) (up to numerical constants), i.e. by bounding both terms Y1 and Y2 by\nO  LB √ r log(1/δ)\nλnt\n  .\nNote, that since the bound is obtained for every possible Zt+1, it will still hold with probability at least 1−δ without conditioning on Zt+1.\nFinally, as shown in (Sridharan et al., 2009), by setting r appropriately we obtain that w.p. 1− δ, for all w\nĝt+1(w) ≤ 2 ĝt(w) +O ( L2B2 log(1/δ)\nλnt\n) .\nApplying union bound to account for all values of t simultaneously, we obtain the desired result. ✷\nWe return to the proof of Lemma 1. Using Lipschitz and boundedness assumptions for the loss ℓ and mapping φ, as well as strong convexity of the regularized objective, we obtain initial tolerance of the loss estimate:\nĝ0(w0) = f̂0(w0)− f̂0(ŵ∗0)\n≤ 1 n0\nn0∑\ni=1\n( ℓzi(w0)− ℓzi(ŵ∗0) )\n≤ LB‖ ŵ∗0‖ ≤ LB √ 2 ĝ0(w0)\nλ ,\nĝ0(w0) ≤ 2L2B2\nλ .\nWe used the fact that w0 is set to zero only for applying inequality ‖w0 ‖ ≤ ‖ ŵ∗0‖ to drop the regularization terms (hence, any initialization satisfying that requirement is acceptable).\nFinally, Condition (9) regards the relationship between approximation error estimate ĝT and full approximation error ĝ. This bound can be obtained by repeating the same argument as in Lemma 2. We can either assume N → ∞ and use standard Rademacher complexity, as in Theorem 1, (Sridharan et al., 2009), or stay with the finite optimization model and apply PRC. Thus, we can clearly set ǫ0 to satisfy the conditions of Lemma 1. ✷\nB.3. Deriving Log Terms in Theorem 4.1\nThe number of iterations, T = O(log(ǫ0/ǫ)), depends on ǫ0. But in Lemma 1 we defined ǫ0 using T . To address this, we have to find ǫ0 satisfying:\nǫ0 ≥ K log ( log(ǫ0/ǫ)\nδ\n) ,\nwith K = O(L2B2/λ). It is easy to show that for small enough ǫ it suffices to set\nǫ0 , 2K log\n( log(1/ǫ)\nδ\n)\n= O ( L2B2\nλ · (log log(1/ǫ) + log(1/δ))\n) .\nThus, setting n0 = 1, we obtain the final complexity bound in Theorem 4.1 as\nO ( κ λǫ · L2B2 · (log log(1/ǫ) + log(1/δ)) ) ."
    } ],
    "references" : [ {
      "title" : "A reliable effective terascale linear learning system",
      "author" : [ "Agarwal", "Alekh", "Chapelle", "Olivier", "Dudı́k", "Miroslav", "Langford", "John" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2014
    }, {
      "title" : "PETSc Web page. http:// www.mcs.anl.gov/petsc, 2016a. URL http:// www.mcs.anl.gov/petsc",
      "author" : [ "Hong", "Zhang" ],
      "venue" : null,
      "citeRegEx" : "Hong et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2016
    }, {
      "title" : "PETSc users manual. Technical Report ANL-95/11 - Revision 3.7, Argonne National Laboratory, 2016b",
      "author" : [ "Hong", "Zhang" ],
      "venue" : "URL http://www.mcs. anl.gov/petsc",
      "citeRegEx" : "Hong et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2016
    }, {
      "title" : "ISSN 1532-4435",
      "author" : [ "July" ],
      "venue" : "URL http://dl.acm.",
      "citeRegEx" : "July,? 2011",
      "shortCiteRegEx" : "July",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Additionally, two-stage approaches have been proposed (Agarwal et al., 2014; Shalev-Shwartz & Zhang, 2013), which employ SGD at the beginning followed by a batch optimizer (e.",
      "startOffset" : 54,
      "endOffset" : 106
    } ],
    "year" : 2017,
    "abstractText" : "We propose Batch-Expansion Training (BET), a framework for running a batch optimizer on a gradually expanding dataset. As opposed to stochastic approaches, batches do not need to be resampled i.i.d. at every iteration, thus making BET more resource efficient in a distributed setting, and when disk-access is constrained. Moreover, BET can be easily paired with most batch optimizers, does not require any parameter-tuning, and compares favorably to existing stochastic and batch methods. We show that when the batch size grows exponentially with the number of outer iterations, BET achieves optimal Õ(1/ǫ) data-access convergence rate for strongly convex objectives.",
    "creator" : "LaTeX with hyperref package"
  }
}