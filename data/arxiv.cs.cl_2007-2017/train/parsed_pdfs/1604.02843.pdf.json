{
  "name" : "1604.02843.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Method of Tibetan Person Knowledge Extraction",
    "authors" : [ "Sun Yuan", "Zhu Zhen" ],
    "emails" : [ "tracy.yuan.sun@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Tibetan, person knowledge extraction, training corpus, template, SVM."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Explosive growth of Web content is making the study of social network on Web from the structure analysis to the content analysis, Knowledge Graph is becoming a hot research of Natural Language Processing in the age of big data [1]. According to the survey results from the National Language Resource Monitoring and Research Center of Minzu University of China in 2013: the number of China ethnic minority language websites is 1,031, which include 655 Uyghur websites, 104 Tibetan websites and 102 Mongolian websites. Knowledge graph with full and complete knowledge system for information retrieval, question answering system, construction of knowledge base and other areas of study provides resources and support. Nodes of Knowledge Graph represent entities and concepts, and connecting edges represent various semantic relations between entities and concepts, and entity knowledge extraction is one of the main research contents. At present, the existing knowledge graph only provides the relevant knowledge of English, Chinese or France, such as Google (more than 5.7 billion entities, 18 billion connections), DBpedia (more than 1,900 million entities. 1 billion relations), Wiki-links (40 million to disambiguate the relations), Wolframalpha (10 trillion), Probase (more than 265 million entities), Baidu, Sogou, etc [2]. Minority language knowledge graph research is just beginning.\nFor example, when we search \" (the Dalai Lama)\", Google has 64,100 results. And when we search \" (Jiawa Rinpoche)\", Google has 586,000 results. In Tibetan, commonly known (the Dalai Lama) as (Jiawa Rinpoche), and the current search engines do not show the relation between them.\n*No.27, Zhongguancun South Street, Haidian District, Beijing, 100081, P.R. China; Tel: 8610-68930880. E-mail: tracy.yuan.sun@gmail.com\nIn addition, all the search results are based on keywords, without the structure of knowledge representation. If we have the semantic link between entity and entity, then we will get more comprehensive information to realize the deep information mining."
    }, {
      "heading" : "2. RELATED WORKS",
      "text" : "The key problem of entity knowledge extraction is the entity relation extraction. Tibetan entity knowledge extraction techniques include the training corpus construction, text representation, entity relation extraction. The present situation is as follows. In training corpus construction, entity knowledge extraction method based on machine learning requires a certain scale of training corpus, and the corpus using artificial mark spends a lot of time and manpower. In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10]. However, most of this corpus aims at English and Chinese. For Tibetan entity relation extraction, there is no ready-made training corpus can be directly obtained. In recent years, some researchers try to use the rich resource language to help the less resource language, and build the training corpus. Kim [11] put forward a cross-language relation extraction method based on label mapping, which applied the training model to the source language in parallel corpus, and mapped the identified high reliability examples to the target language, finally obtained the training corpus to the entity relation of target language. Kim and Lee [12] further used a semisupervised learning algorithm, which mapped more contextual information in the source language into the target language through iterative method, and improved the quantity and quality of training corpus. Yanan Hu [13] used machine translation to realize the relation example transformation from the source language to target language, and helped the less resource language to extract semantic relations.\nIn the aspect of text representation, the most common method of word representation is the One-hot Representation. In this method, word is expressed as a very long vector and the dimension is vocabulary size. The problem of One-hot Representation is dimension disaster. Meanwhile, there is the phenomenon of isolation between words. Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22]. However, Tibetan semantic knowledge base construction is not well developed. At present, there is not a practical Tibetan semantic knowledge base system. In terms of entity relation extraction, the typical methods include feature vector [23,24] and kernel function [25,26]. Feature vector method includes the maximum entropy model [24] and support vector machine (SVM) [28,29]. The main focus of this method is how to obtain all kinds of effective features. Qin Deng [30] introduced the lexical semantic matching technology into Chinese entity relation extraction based on pattern matching technology, and compared the performance between the general pattern matching technology and lexical semantic pattern matching technology. Jifa Jiang [31] proposed a bootstrap acquisition method based on binary relation mode. Weiru Zhang [27] proposed a method based on Wikipedia and mode clustering, which has highly accuracy in Chinese entity relation extraction. Different from the method based on feature vector, the method based on kernel function does not need to construct high dimensional feature vector space. Through calculating the similarity between two discrete objects (such as syntax tree structure), this method uses the parsing structure tree to realize the classification. Zelenko [32] introduced the kernel function method into the relation extraction, which defined the kernel function on the basis of shallow parsing and designed a dynamic programming algorithm to extract entity semantic relations, finally got good results in 200 news texts. Culotta [33] transformed the parse tree into the dependency tree through some transformation rules, and increased features of the part of speech, entity types, phrases, WordNet, used the SVM classifier to relation extraction. Through testing ACE RDC 2003 data, the F value achieved 45.8%. Bunescu [34] further proposed the kernel function based on the shortest path dependence tree, and F value achieved 52.5% on the ACE RDC 2003 data, but the recall rate is lower. Zhang [35] designed a composite convolution kernel function tree for relation extraction. The method combined convolution kernel function with linear kernel function, fully considering the influence of the semantic relations. The F value reached 70.9% and 72.1% respectively on the ACE 2003 and ACE 2004 data. Kebin Liu [36] has realized the automatic extraction system based on the improved semantic sequence kernel function and combined with KNN machine learning algorithm to entity relation. At present, Tibetan display output technology, coding technology, input technology, text processing technology, the homepage manufacture technology obtained the very good solution, however in the study of sentence and chapter level, Tibetan is still in the initial stage. Therefore, some entity relation extraction method in English and Chinese cannot be directly applied to Tibetan.\nIn this condition, this paper proposes a SVM and template based approach to Tibetan person knowledge extraction. The main work of this paper is in the following. (1) Constructing the training corpus. (2) Building the templates based the shallow parsing analysis of Tibetan syntactic, semantic features and verbs. (3) Using the training corpus, we design a hierarchical SVM classifier to realize the entity knowledge extraction. The research of Tibetan person knowledge extraction is the foundation of Tibetan knowledge graph construction. It provides support for Tibetan question answering system, information retrieval, information extraction and other researches, and promotes national unity and social stability."
    }, {
      "heading" : "3. TRAINING CORPUS AND TEMPLATE CONSTRUCTION",
      "text" : "Firstly, we crawl data from some websites, such as Wikipedia, Comba Media Net. Through word segmentation, POS tagging and entity recognition, we use templates and SVM to realize the entity knowledge extraction."
    }, {
      "heading" : "3.1 Training corpus",
      "text" : "We use the \"entity, attribute and value\" of the existing information box in Chinese from Wikipedia to obtain the Chinese sentences with the entity and the attribute. Based on the correspondence between entities in the parallel sentence of Tibetan and Chinese, we construct some training corpus of Tibetan entity knowledge extraction. Meanwhile, based on the shallow parsing analysis of Tibetan syntactic, semantic features and verbs, we get the templates and the main features of Tibetan entity knowledge extraction, shown in Fig. (1)."
    }, {
      "heading" : "3.2 Template building",
      "text" : "Tibetan is a predicate behind language and the verb is the core of the sentence. Meanwhile, Tibetan auxiliary can clearly indicate the semantic structure of sentence, shown in Table 1. Therefore, the feature selection mainly includes Tibetan case makers, verbs and other substantives.\nExample 1: (Zhuo Ma was born in 1988.)\nTemplate 1:<name/nh><time/t>( )/u\n/v[born]\nExample 2: (Zhuo Ga's hometown is Qinghai. )\nTemplate 2: <name/nh>( )/u /n[hometown] /r\n<place name/np>( )/u /v\nExample 3： (Tsewang's birthday is October 1, 1988.)\nTemplate 3:<name/nh>( )/u /n[birthday] /r<time/t> /v\nFig. (1). Tibetan Entity Relation Template Construction."
    }, {
      "heading" : "4. HIERARCHICAL CLASSIFICATION BASED ON SVM",
      "text" : "Although the method based on feature template can obtain higher accuracy in specific test corpus, it requires a lot of manual works and cannot extract the corpus which is not covered. Therefore, we adopted the SVM method based on feature vector, and designed a hierarchical classifier."
    }, {
      "heading" : "4.1 Feature selection",
      "text" : "In this paper, the selected features include keywords, tagging combination feature and words around entities. (1) Keywords Keywords are a noun or verb with high frequency and strong distinction. Most of these features are extracted from the templates. Although keywords are not many, these words have a strong distinction in a certain attribute class. (2) Tagging combination feature The part-of-speech tagging is an important feature. But not every tag can be used as a feature vector, because many tags are not discrimination. Therefore, we use tagging combination features, which get better classification effect. For example, the combination of time tag \"/t\" + auxiliary \"/k\" + \"/ v \" has great help for identifying birth attribute. (3) Words around entities The feature of word around entity includes parts of speech tagging and named entity marking. We consider that\nmarkers which are nearer to the entity are more important. Therefore, we select two words forward entity and one word backward entity."
    }, {
      "heading" : "4.2 Hierarchical classifier construction",
      "text" : "SVM was originally designed to solve the problem of binary classification, but entity attribute extraction is the multi classification problem. For example, person attributes can be divided into birth, birth place, gender and other categories. So the key problem is to design the high performance classifier. At present, there are two types of classifier: (1) One-to-one classifier. If there are k categories, we need to build ( 1) / 2k k − classifiers, and then calculate ( 1) / 2k k − times and get the cumulative weight .The largest cumulative value is the category. (2) One-to-many classifier. This method need to build k classifiers if there are k categories, then we need to calculate / 2k times to forecast each attribute. Comparing the two methods, the first method is better than the second, but the number of classifiers is too much. When the categories are so much, the applicability is poor. Therefore, this paper designs a hierarchical classifier. The method combines the advantages of the two traditional methods, shown in Fig. (2).\nIn the same level, we use one-to-one method. Meanwhile, we use language rules to build a fast track, which has certain advantages in the classification results, speed and the number of classifiers. (1) The filter: Before entering the hierarchical classifier, it is necessary to make a selection in the corpus. The filter will eliminate the sentence which has no entities. To a certain extent, it can reduce the workload of the hierarchical classifier and improve the efficiency. (2) Layer by layer to down: After entering the hierarchical classifier system, the standard classification model is to start from the first layer to the bottom layer. The intermediate classifier will eliminate some of the unrelated data. This step is very important to deal with a large number of negative samples in attribute extraction.\n(3) Multi classification in the same layer: After the hierarchical classification, there are not many categories to deal with using each multi classifier. The number of\nclassifiers is 1\n( 1) / 2 n\nsum i i i N p p = = − , ip is the number of categories in each classifier. It solves the problem of classifications are too small and keep the high accuracy of one-to-one method. (4) Fast track: we design a fast track based on entityattribute tagging characteristics, which can effectively improve the classification effect and speed, because the entity attribute often has obvious distinction in the attribute extraction. For example, when the second entity is time, it only appears the birth date attribute but not a father or a birth place. So it can directly jump to the category of birth date through the fast track.\nFig. (2). Hierarchical Classifier."
    }, {
      "heading" : "5. THE EXPERIMENTAL RESULTS AND ANALYSIS",
      "text" : ""
    }, {
      "heading" : "5.1 Corpus sources",
      "text" : "In this paper, corpus data is from seven Tibetan website, as shown in Table 2. We mainly process the following four entity relations: Name-Birth date, Name- Birth place, NameFather, Name-Mother.\nWe select 2,400 sentences that contain entity-attributes from a large number of web pages. Among them, the 1,975 sentences are contained above four entity-attributes and 425 sentences are for other entity-attributes. We select 1,600 sentences as training corpus, and 800 sentences as test corpus."
    }, {
      "heading" : "5.2 The experimental analysis and evaluation",
      "text" : "Firstly, we test the template method on the open set (800 sentences), which contain 846 attributes. The experimental results are shown in Table 3.The experimental results show that the performance of template method used in open set is not high. The main reason is that this method lacks learning ability and must be constructed through artificial participation. Although performance will gradually increase by generalization and correction templates, too much human intervention becomes the bottleneck of the method. Secondly, we use the SVM hierarchical classifier to extract entity relations, the experimental results are shown in Table 4. Compared with the template method, SVM method improves the recall rate of entity-attribute extraction but accuracy is not improved obviously. The main reason is that the results using SVM can get good performance on the nonobvious classification by diversifying the feature vectors, but there are some mistakes in some clear classification, because of training corpus insufficient. Finally, we use the method based on template and SVM method. The experimental results are shown in Table 5. Table 5 shows the performance of using template and SVM is obviously higher than only using any one method."
    }, {
      "heading" : "6. CONCLUSION",
      "text" : "In this paper, we propose a SVM and template based approach to Tibetan person knowledge extraction. And the experimental results show the method achieves good performance. In the following work, we will increase the training corpus, and improve this method. Then we will use CRF, word embedding method to the Tibetan entity knowledge extraction, and give the comparison results.\nCONFLICT OF INTEREST\nThe author confirms that this article content has no conflict of interest."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work is supported by National Nature Science Foundation (No. 61501529, No. 61331013), National Language Committee Project (No. YB125-139, ZDI125-36), and Minzu University of China Scientific Research Project (No. 2015MDQN11, No. 2015MDTD14C)."
    } ],
    "references" : [ {
      "title" : "Linked data-the story so far",
      "author" : [ "C Bizer", "T Heath", "T Berners-Lee" ],
      "venue" : "International Journal on Semantic Web and Information Systems (IJSWIS),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "The focus of the next search engines: Knowledge Graph",
      "author" : [ "Zhang Jing", "Tang Jie" ],
      "venue" : "CCCF, vol. 9,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Natrual language processing based on naturally annotated web resources",
      "author" : [ "Sun Maosong" ],
      "venue" : "Journal of Chinese Information Processing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "DBpedia: A nucleus for a web of open data",
      "author" : [ "S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z.G. Ives" ],
      "venue" : "Proceedings of the 6th international semantic web conference, pp. 722-735",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "DBpedia - a crystallization point for the web of data",
      "author" : [ "C. Bizer", "J. Lehmann", "G. Kobilarov", "S. Auer", "C. Becker", "R. Cyganiak", "S. Hellmann" ],
      "venue" : "Web Semantics: Science”, Services and Agents on the World Wide Web, vol. 7, no. 3, pp. 154-165",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Freebase: a shared database of structured general human knowledge",
      "author" : [ "K.D. Bollacker", "R.P. Cook", "P. Tufts" ],
      "venue" : "Proceedings of the 22nd national conference on Artificial intelligence, vol. 2, pp. 1962-1963",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "G",
      "author" : [ "F.M. Suchanek" ],
      "venue" : "Kasneci, and G.Weikum, “YAGO: a core of semantic knowledge”, Proceedings of the 16th international conference on World Wide Web, pp. 697-706",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Wikipedia as multilingual source of comparable corpora",
      "author" : [ "Otero", "Pablo Gamallo", "Isaac González López" ],
      "venue" : "Proceedings of the 3rd Workshop on Building and Using Comparable Corpora,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Large-Scale Named Entity Disambiguation",
      "author" : [ "S Cucerzan" ],
      "venue" : "Based on Wikipedia Data”,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "Discovering missing semantic relations between entities in Wikipedia",
      "author" : [ "Mengling Xu", "Zhichun Wang" ],
      "venue" : "The 12th International Semantic Web Conference,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "A crosslingual annotation projection approach for relation detection",
      "author" : [ "Seokhwan Kim", "Minwoo Jeong", "Jonghoon Lee" ],
      "venue" : "Proceedings of COLING,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "A graph-based cross-lingual projection approach for weakly supervised relation extraction",
      "author" : [ "Seokhwan Kim", "Gary Geunbae Lee" ],
      "venue" : "Proceedings of ACL,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Cross Lingual Relation Extraction Via Machine Translation",
      "author" : [ "Hu Yanan" ],
      "venue" : "Journal of Chinese Information Processing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals",
      "author" : [ "Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid O ́. Se ́aghdha", "Sebastian Pado", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz" ],
      "venue" : "In Proceedings of the 5th International Workshop on Semantic Evaluation,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Natural Language Processing (Almost) from Scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Three new graphical models for statistical language modeling",
      "author" : [ "Andriy Mnih", "Geoffrey Hinton" ],
      "venue" : "International Conference on Machine Learning (ICML),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "A scalable hierarchical distributed language model",
      "author" : [ "Andriy Mnih", "Geoffrey Hinton" ],
      "venue" : "The Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2008
    }, {
      "title" : "Statistical Language Models based on Neural Networks",
      "author" : [ "Mikolov Tomáš" ],
      "venue" : "PhD thesis, Brno University of Technology,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Word representations: a simple and general method for semi-supervised learning",
      "author" : [ "Turian Joseph", "Lev Ratinov", "Yoshua Bengio" ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "Improving word representations via global context and multiple word prototypes",
      "author" : [ "Eric Huang", "Richard Socher", "Christopher Manning", "Andrew Ng" ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Mikolov", "Tomas", "Wen-tau Yih", "Geoffrey Zweig" ],
      "venue" : "Proceedings of NAACL-HLT,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Extracting relation information from text documents by exploring various types of knowledge",
      "author" : [ "Guodong Zhou", "Min Zhang" ],
      "venue" : "Information Processing and Management,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2007
    }, {
      "title" : "Combining lexical, syntactic and semantic features with Maximum Entropy models for extracting relations",
      "author" : [ "Nanda Kambhatla" ],
      "venue" : "Proceedings of ACL,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2004
    }, {
      "title" : "Exploiting constituent dependencies for tree kernel-based semantic relation extraction",
      "author" : [ "Longhua Qian", "Gougong Zhou", "Fang Kong" ],
      "venue" : "Proceedings of COLING,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2008
    }, {
      "title" : "Tree kernel-based relation extraction with context-sensitive structured parse tree information",
      "author" : [ "Guodong Zhou", "Min Zhang", "Donghong Ji" ],
      "venue" : "Proceedings of EMNLP/CONLL,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2007
    }, {
      "title" : "A entity relation extraction method based on Wikipedia and pattern clustering",
      "author" : [ "Zhang Weiru", "Sun Le", "Han Xianpei" ],
      "venue" : "Journal of Chinese Information Processing,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Extracting relations with integrated information using kernel methods",
      "author" : [ "B Zhao S", "R Grishman" ],
      "venue" : "Proceedings of ACL,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2005
    }, {
      "title" : "Exploring various knowledge in relation extraction",
      "author" : [ "D Zhou G", "J Su", "J Zhang", "M. Zhang" ],
      "venue" : "Proceedings of ACL,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2005
    }, {
      "title" : "Entity relation extraction method using semantic pattern",
      "author" : [ "Deng Bo", "Fan Xiaozhong", "Yang Ligong" ],
      "venue" : "Computer Engineering,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2007
    }, {
      "title" : "A bootstrapping method for acquisition of bi-relations and bi-relational patterns",
      "author" : [ "Jiang Jifa", "Wang Shuxi" ],
      "venue" : "Journal of Chinese Information Processing,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2005
    }, {
      "title" : "Kernel methods for relation extraction",
      "author" : [ "D Zelenko", "C Aone", "A Richardella" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2003
    }, {
      "title" : "Dependency tree kernels for relation extraction",
      "author" : [ "A Culotta", "J Sorensen" ],
      "venue" : "Proceedings of ACL,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2004
    }, {
      "title" : "A shortest path dependency kernel for relation extraction",
      "author" : [ "C. Bunescu R", "J Mooney R" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2005
    }, {
      "title" : "A compo site kernel to extract relations between entities with both flat and structured features",
      "author" : [ "M Zhang", "J Zhang", "J Su" ],
      "venue" : "Proceedings of ACL,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2006
    }, {
      "title" : "Implementation of a kernel-based Chinese relation extraction system",
      "author" : [ "Liu Kebin", "LiFang", "Liu Lei" ],
      "venue" : "Journal of Computer Research and Development,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Explosive growth of Web content is making the study of social network on Web from the structure analysis to the content analysis, Knowledge Graph is becoming a hot research of Natural Language Processing in the age of big data [1].",
      "startOffset" : 227,
      "endOffset" : 230
    }, {
      "referenceID" : 1,
      "context" : "1 billion relations), Wiki-links (40 million to disambiguate the relations), Wolframalpha (10 trillion), Probase (more than 265 million entities), Baidu, Sogou, etc [2].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "In English and Chinese, training corpus mainly comes from ACE, SemEval and natural annotation corpus [3-10].",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 10,
      "context" : "Kim [11] put forward a cross-language relation extraction method based on label mapping, which applied the training model to the source language in parallel corpus, and mapped the identified high reliability examples to the target language, finally obtained the training corpus to the entity relation of target language.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 11,
      "context" : "Kim and Lee [12] further used a semisupervised learning algorithm, which mapped more contextual information in the source language into the target language through iterative method, and improved the quantity and quality of training corpus.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 12,
      "context" : "Yanan Hu [13] used machine translation to realize the relation example transformation from the source language to target language, and helped the less resource language to extract semantic relations.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 13,
      "context" : "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].",
      "startOffset" : 186,
      "endOffset" : 193
    }, {
      "referenceID" : 14,
      "context" : "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].",
      "startOffset" : 186,
      "endOffset" : 193
    }, {
      "referenceID" : 15,
      "context" : "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].",
      "startOffset" : 186,
      "endOffset" : 193
    }, {
      "referenceID" : 16,
      "context" : "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].",
      "startOffset" : 186,
      "endOffset" : 193
    }, {
      "referenceID" : 17,
      "context" : "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].",
      "startOffset" : 186,
      "endOffset" : 193
    }, {
      "referenceID" : 18,
      "context" : "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].",
      "startOffset" : 186,
      "endOffset" : 193
    }, {
      "referenceID" : 19,
      "context" : "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].",
      "startOffset" : 186,
      "endOffset" : 193
    }, {
      "referenceID" : 20,
      "context" : "Aiming at this problem, some researchers combined semantics knowledge base, such as WordNet, HowNet, or Distributed Representation to improve the accuracy of entity relation description [14-22].",
      "startOffset" : 186,
      "endOffset" : 193
    }, {
      "referenceID" : 21,
      "context" : "In terms of entity relation extraction, the typical methods include feature vector [23,24] and kernel function [25,26].",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 22,
      "context" : "In terms of entity relation extraction, the typical methods include feature vector [23,24] and kernel function [25,26].",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 23,
      "context" : "In terms of entity relation extraction, the typical methods include feature vector [23,24] and kernel function [25,26].",
      "startOffset" : 111,
      "endOffset" : 118
    }, {
      "referenceID" : 24,
      "context" : "In terms of entity relation extraction, the typical methods include feature vector [23,24] and kernel function [25,26].",
      "startOffset" : 111,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : "Feature vector method includes the maximum entropy model [24] and support vector machine (SVM) [28,29].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 26,
      "context" : "Feature vector method includes the maximum entropy model [24] and support vector machine (SVM) [28,29].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 27,
      "context" : "Feature vector method includes the maximum entropy model [24] and support vector machine (SVM) [28,29].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 28,
      "context" : "Qin Deng [30] introduced the lexical semantic matching technology into Chinese entity relation extraction based on pattern matching technology, and compared the performance between the general pattern matching technology and lexical semantic pattern matching technology.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 29,
      "context" : "Jifa Jiang [31] proposed a bootstrap acquisition method based on binary relation mode.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 25,
      "context" : "Weiru Zhang [27] proposed a method based on Wikipedia and mode clustering, which has highly accuracy in Chinese entity relation extraction.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 30,
      "context" : "Zelenko [32] introduced the kernel function method into the relation extraction, which defined the kernel function on the basis of shallow parsing and designed a dynamic programming algorithm to extract entity semantic relations, finally got good results in 200 news texts.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 31,
      "context" : "Culotta [33] transformed the parse tree into the dependency tree through some transformation rules, and increased features of the part of speech, entity types, phrases, WordNet, used the SVM classifier to relation extraction.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 32,
      "context" : "Bunescu [34] further proposed the kernel function based on the shortest path dependence tree, and F value achieved 52.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 33,
      "context" : "Zhang [35] designed a composite convolution kernel function tree for relation extraction.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 34,
      "context" : "Kebin Liu [36] has realized the automatic extraction system based on the improved semantic sequence kernel function and combined with KNN machine learning algorithm to entity relation.",
      "startOffset" : 10,
      "endOffset" : 14
    } ],
    "year" : 2016,
    "abstractText" : "Person knowledge extraction is the foundation of the Tibetan knowledge graph construction, which provides support for Tibetan question answering system, information retrieval, information extraction and other researches, and promotes national unity and social stability. This paper proposes a SVM and template based approach to Tibetan person knowledge extraction. Through constructing the training corpus, we build the templates based the shallow parsing analysis of Tibetan syntactic, semantic features and verbs. Using the training corpus, we design a hierarchical SVM classifier to realize the entity knowledge extraction. Finally, experimental results prove the method has greater improvement in Tibetan person knowledge extraction.",
    "creator" : "PScript5.dll Version 5.2.2"
  }
}