{
  "name" : "1206.6468.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Variational Inference in Non-negative Factorial Hidden Markov Models for Efficient Audio Source Separation",
    "authors" : [ "Gautham J. Mysore", "Maneesh Sahani" ],
    "emails" : [ "gmysore@adobe.com", "maneesh@gatsby.ucl.ac.uk" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Spectrograms reveal a great deal of acoustic structure and are therefore often the representation of choice for modeling sounds. A spectrogram is the magnitude of the short-time Fourier transform (STFT) of a signal and is therefore a non-negative matrix. This has led to the popularity of using non-negative matrix factorization (NMF) (Lee & Seung, 2001) to model audio (Smaragdis & Brown, 2003).\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nConceptually, NMF models each time frame (column) of an audio spectrogram as a linear combination of non-negative dictionary elements. Given the spectrogram of a sound source, we can use NMF to learn a dictionary (Figure 1) that serves as a model for the range of different short-term spectra generated by that source. While potentially a powerful spectral model, NMF provides no generative account of the temporal dynamics linking these short-term spectra, nor of any spectral non-stationaries: two essential ingredients of real audio signals.\nA recent proposal, the non-negative hidden Markov model (N-HMM) (Mysore et al., 2010), deals with this\nfailing of NMF by modeling a sound source with multiple dictionaries (Figure 2) such that each time frame of the spectrogram is modeled by a linear combination of the elements of any one of its many dictionaries, essentially allowing different sub-models for spectra at different time frames. Moreover, transitions between these component dictionaries from one time frame to the next are governed by a Markov chain, thus capturing the temporal dynamics of the source. The model is described more completely in Section 2.1.\nNMF and its probabilistic counterparts have been used extensively for audio source separation (Virtanen, 2007; Smaragdis et al., 2007). The basic idea is to first learn a dictionary for each source from isolated training data. The mixture is then modeled by a dictionary formed from the concatenation the dictionaries of the two1 sources (Figure 3). The goal is then to estimate mixture weights over all dictionary elements at each time frame. Using these mixture weights, we can reconstruct the contribution of each source at each time frame, obtaining the separated spectrogram of each source. The phase of the original mixture STFT is typically then used to obtain time-domain audio signals from these separated spectrograms.\nThe N-HMM has been extended to the non-negative factorial hidden Markov model (N-FHMM) to model sound mixtures and has been used for source separation (Mysore et al., 2010). Each time frame of the spectrogram is modeled by one of the combinatorially many concatenations of dictionaries of the two sources as illustrated in Figure 4. When used for source sep-\n1It is straightforward to extend this and other methods described in the paper to more than two sources.\naration, the goal is to estimate the posterior probabilities of using each pair of dictionaries at each time frame as well as the mixture weights for each of these dictionary pairs. Using these estimates, we can reconstruct the separated spectrograms and perform source separation in the same way as described above.\nThe N-FHMM has been shown to achieve much better source separation accuracy than simple NMF (Mysore et al., 2010). However, the combinatorial nature of the N-FHMM makes the complexity of exact inference exponential in the number of sound sources, which is often intractable. Specifically, if each source has N states and there are S sources, then we must evaluate the posterior probabilities of NS state configurations per time frame.\nIt would therefore be useful to be able to use an approximate inference technique for the N-FHMM. Structured variational inference (Ghahramani & Jordan, 1997) is an attractive approach for factorial hidden Markov models (FHMM) in general. However, the natural extension of this idea to the N-FHMM has certain limitations, which make it a poor approximation (Section 3.1). In this paper, we propose a Bayesian variant of the N-FHMM (Section 2.2) that makes it more amenable to variational inference, and then develop a suitable factored approximation to the posterior distribution deriving the corresponding variational inference algorithm (Section 3.2).\nExperiments (Section 5) show that our algorithm achieves accuracy comparable to that of exact infer-\nence, but is about 30 times faster on the configurations of the N-FHMM that achieve the the highest-quality source separation results."
    }, {
      "heading" : "2. Probabilistic Models",
      "text" : "In this section, we first describe the probabilistic model of the N-HMM (Mysore et al., 2010) for single sources as it forms the foundation for the N-FHMM. We then describe the probabilistic model of the proposed Bayesian variant of the N-FHMM. In these models, each time frame of the spectrogram is viewed as a histogram of “sound quanta” in the same way that a document is viewed as a histogram of words in topic models (Hofmann, 1999; Blei et al., 2003)."
    }, {
      "heading" : "2.1. Non-negative Hidden Markov Model",
      "text" : "The graphical model is shown in Figure 5. The random variables D1...T form a Markov chain, and the spectra in each time frame are independent given these variables. Each possible value of Dt identifies a spectral dictionary. Each dictionary contains a set of dictionary elements (analogous to topics), one of which is selected for each sound quantum by the random variable Zt. Each dictionary element is a normalized vector over frequencies (analogous to a distribution of words). The frequency associated with a particular quantum is selected by Ft.\nThe generative process at time frame t is thus:\n1. Choose state Dt|Dt−1 ∼ Discr [ρ(Dt−1)]\n2. Repeat for each of vt quanta: – Choose dictionary element Zt ∼ Discr [θt(Dt)] – Choose frequency Ft ∼ Discr [β(Dt, Zt)]\nHere, Discr [] represents the discrete distribution; ρ(d) is the column of the Markov transition matrix representing transitions from state d; θt(d) is a vector of normalized mixture weights for dictionary element d in time frame t; and β(d, z) is the normalized dictionary element z of dictionary d. Given the spectrogram\nof a sound source, maximum–likelihood (ML) values of all these parameters may be found by the EM algorithm. The dictionaries and the transition matrix define the model of the sound source, whereas the mixture weights (which depend on t) are nuisance parameters unique to the particular instance of the sound source used for training. A sample of the dictionaries learned from real speech data is shown in Figure 6."
    }, {
      "heading" : "2.2. Non-negative Factorial Hidden Markov Model",
      "text" : "The original N-FHMM introduced an independent Markov chain D(s)1...T for each source s, and timedependent mixing weights that selected elements from a combined state-dependent dictionary θt(d(1), d(2)). Here, we extend this model in two ways. First, θt and Zt will range over all dictionary elements of all sources. Thus the selection of a dictionary based on the state D\n(s) t becomes probabilistic, and elements from more than one dictionary may appear in principle. Second, we treat θt as a Dirichlet-distributed latent variable, rather than as a parameter. Separating the mixture requires estimation of θt: the older N-FHMM formulation used ML estimates; here we use a variational posterior.\nThe generative process (Figure 7) at time frame t is thus:\n1. Choose states for each source: D\n(s) t |D (s) t−1 ∼ Discr\n[ ρ(s)(D\n(s) t−1) ] 2. Choose mixture weights:\nθt ∼ Dirich [ α(D (1) t , D (2) t ) ]\n3. Repeat for each of vt quanta: –Choose dictionary element: Zt ∼ Discr [θt] –Choose frequency: Ft ∼ Discr [β(Zt)]\nThe function α gives the Dirichlet parameters for the mixing weights, and thus specifies the dictionary elements available given each pair of source Markov states. It can most easily be written by introducing indicator variables δ(s)t,n = 1 if D (s) t = n and 0 otherwise; as well as a binary mask array B, with Bsnk = 1 iff dictionary element k is available when the Markov chain associated with source s is in state n. Then the kth generative Dirichlet parameter for θt can be written αk(D (1) t , D (2) t ) = 1 + γ ∑ s ∑ n δ (s) t,nBsnk. Provided the two sources do not share dictionary elements, the sum in this expression evaluates to either 0 or 1. Thus the distribution on θt has parameters of 1 + γ corresponding to the elements selected by the current Markov states, and 1 otherwise. The hyperparameter γ sets the concentration of the Dirichlet; we took γ = 1. Thus, we can write the distribution on mixing weights:\nP (θt|D(1)t , D (2) t ) ∝ ∏ k (∏ s ∏ n θ δ (s) t,nBsnk t,k ) . (1)\nThis formulation does not make exact inference any easier because the number of such distributions that we have to consider is still exponential in the number of sources. However, we will see in Section 3.2 that this is important for our variational inference derivation.\nThe remaining parameters are much as before: β(z) is the zth normalized dictionary element; ρ(1) and ρ(2) are the transition matrices of the two sources. We also need parameters for the initial state probabilities, for which we write π(1) and π(2). The number of quanta at each time frame vt could be modeled as a draw from (say) a Poisson distribution. However, it is independent of the other generative variables and (in our applications) is observed, so we do not model it as a random variable.\nWithout the temporal dynamics, our formulation is similar to that of latent Dirichlet allocation LDA (Blei\net al., 2003). The main difference is that the Dirichlet distribution in a given time frame is a function of the Markov states of the sources rather than being constant for all time frames."
    }, {
      "heading" : "3. Variational Inference",
      "text" : "The parameters describing each source N-HMM are learned from isolated training data of that source. Thus, the goal of inference in the N-FHMM is only to resolve the mixture; specifically, to estimate the marginalized posterior distribution of the mixture weights P (θt|f) at each time frame. Once this distribution is found, we can reconstruct the individual sources and therefore perform source separation. The full posterior distribution is given by P (Z, θ,D(1),D(2)|f), where θ, D(1), and D(2) represent θt, D (1) t , and D (2) t at all time frames and Z represents all draws of Zt at all time frames. f represents the observed values of Ft at all time frames. The computational cost of finding the posterior distribution is exponential in the number of sources due to the coupling of the states of the individual N-HMMs. Exact inference in N-FHMMs is therefore intractable so we resort to approximation.\nVariational inference (Jordan et al., 1999) refers to a class of techniques that are used to approximate an intractable posterior distribution with a simpler (typically factorized) distribution. By minimizing the KL divergence between the two distributions, a lower bound on the log-likelihood is maximized. This is the class of approximations that we employ.\nA natural variational approximation to the N-FHMM would be to decouple from each other the sets latent variables that correspond to each component N-HMM, but to retain the structured posterior over each separate source. This scheme is analogous to the structured variational approximation for FHMMs (Ghahramani & Jordan, 1997). Unfortunately, however, it performs poorly for the N-FHMM of (Mysore et al., 2010). We first briefly sketch the approach and explain why it seems to fail, before moving to the new variant of the N-FHMM to derive a more successful variational inference algorithm."
    }, {
      "heading" : "3.1. Difficulties with Decoupling",
      "text" : "Decoupling the variational posteriors for each sound source requires that it be possible to group latent variables according to the generative source. This is easy for D(s). However, the latent variables Z identify elements from a combined dictionary over both sources. Thus, to proceed we introduce a new latent variable St to indicate the proportions of quanta drawn from each\nsource at time t, and then separately generate Z(1)\nand Z(2), each ranging over the dictionary of a single source. In this parameterization, the posterior is P (Z(1), D(1),Z(2), D(2), S|f), which might be approximated by the decoupled variational distribution:\nq(Z(1), D(1))q(Z(2), D(2)) ∏ t q(St).\nIn this form, the two components q(Z(1), D(1)) and q(Z(2), D(2)) do indeed correspond to structured posteriors within the two N-HMMs describing the sound sources, while the factor q(St) corresponds to the mixing proportions of the two sources at time frame t. The variational iterations then update each individual NHMM posterior using the forward–backward algorithm (Rabiner, 1989) while keeping the contribution of the other N-HMM fixed; and then revise the mixing proportions of the sources. While certainly plausible, this algorithm proves to be very prone to sticking in local optima and in experiments performs more poorly than even basic NMF (implemented in a probabilistic form, see Section 5). Here we provide an intuitive sketch of what we see as the source of the difficulty.\nIn many applications, the sound sources to be separated may have some spectrally similar aspects, so that some or all of the dictionary elements in their individual N-HMMs may have similar forms. This is the case when the sound sources are, for example, speech from different speakers. In such a situation, the dictionary elements of one source may be able to provide a reasonable fit to sounds generated by the other source.\nConsider an example of a single time frame in a speech mixture in which the first source contributed a harmonic spectrum (say a vowel) while the second source produced a noise-like spectrum as might be associated with a fricative. Both source models are likely to contain spectral dictionary elements to account for both vowels and fricatives, although these elements might belong to different dictionaries within each source model. Thus, if at an early stage the harmonic structure is incorrectly assigned to source 2 and the noiselike component to source 1, the inferred Markov states for the two components will be incorrect. We find that it is then very unlikely that further iterations will resolve the error, indeed they seem to make it worse. As the incorrect assignments reinforce each other in the two models, the posterior over Markov states becomes very sharp. Thus, the two sources are confused. This reflects a local optimum: it could very well be that the variational free energy would be larger for the correct assignment, but the hill-climbing form of the iterative algorithm makes it unable to discover that fact.\nIn experiments, we found that this situation appeared with some frequency, despite the fact that the temporal structure of the underlying Markov process biased solutions away from such confusions to an extent."
    }, {
      "heading" : "3.2. Proposed Variational Approximation",
      "text" : "In the proposed variant of the N-FHMM, the link between Markov state and dictionary element is less absolute. Also, we estimate a full posterior over the mixing proportions θt over all dictionary elements, rather than obtaining an ML point estimate—this reduces the risk of zeros (or very small values) in the point estimate, which would have created a similar barrier to exploration. Thus both sources are able to explore the full range of possible dictionary elements and settle on the correct apportionment of the mixture spectrum, while the interaction between Markov state variables and prior on θt strongly favors explanations that concentrate on a single dictionary per source.\nTo develop the variational algorithm for this model, we approximate the posterior distribution P (Z, θ,D(1),D(2)|f) with the following factored form:(∏\nt\nq(θt) )(∏ t ∏ v q(Zt,v) ) q(D(1))q(D(2)) .\nNote that for a given time frame, the index k for θt is over all dictionary elements of all dictionaries of all sources. The mixture weights for a given time frame t are therefore in a single factor q(θt). Moreover, this factor is independent of D(1)t and D (2) t so we do not have the combinatorial problem.\nHowever, the distribution over the states of all time frames of a given sound source q(D(1)) and q(D(2)) are each a single factor. This is because we would still like to make use of the structure of the temporal dynamics in each individual source and exact inference is efficient using the forward–backward algorithm.\nBy minimizing the KL divergence between the true posterior distribution and the factorized distribution, we obtain the following variational inference solution (Jordan et al., 1999) for each of the factors:\nq(θt) ∝ exp 〈 logP (Z, θ,D(1),D(2), f) 〉 q1 , (2)\nq(Zt,v) ∝ exp 〈 logP (Z, θ,D(1),D(2), f) 〉 q2 , (3)\nq(D(1)) ∝ exp 〈 logP (Z, θ,D(1),D(2), f) 〉 q3 , (4)\nq(D(2)) ∝ exp 〈 logP (Z, θ,D(1),D(2), f) 〉 q4 , (5)\nwhere q1 refers to the product of all of the factors except q(θt) (and similarly for the other factors). We\nuse proportionality rather than equality to simply indicate that the quantities are unnormalized. Solving these equations, we find that q(θt) is a Dirichlet distribution, which we parameterize by α̂t,k; q(D(1)) and q(D(2)) are each a set of discrete distributions, with marginal probabilities d̂(1)t,n and d̂ (2) t,n at time frame t. The distribution q(Zt,v) is also discrete, and at any time frame the parameters for all Zt,v whose corresponding observed frequencies ft,v are equal, will be identical. Thus, we write these parameters as ẑt,l,k, where l indexes frequency (in place of v), and k identifies a dictionary element.\nOn simplification of Eq.2, we obtain following update equation for the parameters of q(θt):\nα̂t,k = ∑ v ẑt,ft,v,k + γ ∑ s ∑ n d̂ (s) t,nBsnk + 1 .\nNote that as d̂(s)t,n are defined to be marginal probabilities, they are exactly the expected values under q of δ (s) t,n. The index ft,v is the observed frequency of quantum v at time t. We can group all quanta for which ft,v is equal, to obtain:\nα̂t,k = ∑ l Vltẑt,l,k + γ ∑ s ∑ n d̂ (s) t,nBsnk + 1 , (6)\nwhere Vlt is the value of the spectrogram (i.e., the number of quanta) at frequency l and time frame t.\nOn simplification of Eq.3, we obtain estimates of the parameters of q(Zt,v):\nlog ẑt,l,k = log βl(k)+ψ(α̂t,k)−ψ (∑\nk\nα̂t,k\n) −κ , (7)\nwhere ψ() is the digamma function; κ is a log normalizer; and βl(k) is the value of dictionary element k at frequency l. The digamma terms arise from the normalizing Γ-functions of the Dirichlet distribution (Blei et al., 2003).\nOn simplification of Eq.4 and 5, we first obtain surrogate “likelihood” terms, which we subsequently use in the forward–backward algorithm to obtain the distribution parameters. This likelihood term at time frame t for state n of source s is given by:\nφ̂ (s) t,n = ∑ k Bsnk\n( ψ(α̂t,k)− ψ (∑ k α̂t,k )) . (8)\nThe forward–backward algorithm then finds estimates of the marginals d̂(s)t,n of q(D(s)).\nWe iterate over Eqs.6,7,8, and the forward–backward algorithm for each source. The resulting solution pro-\nvides estimates of the parameters α̂t,k of the distribution q(θt), indicating the distribution of mixture weights."
    }, {
      "heading" : "4. Source Separation",
      "text" : "We reconstruct the spectrograms of the individual sources by taking linear combinations of the dictionary elements of all dictionaries of each individual source according to the estimated mixture weights α̂t,k, at each time frame. This gives us estimates of the separated spectrograms of each source V̂ (s)lt . We can simply go back to the time domain with these estimates using the phase of the original mixture. However, a common source separation practice to first obtain more refined spectrogram estimates by applying the following masking strategy:\nV ∗(s) lt = Vlt V̂ (s) lt∑\ns′ V̂ (s′) lt\n,\nwhere Vlt is the original mixture spectrogram. The final estimated spectrogram for each source is therefore V\n∗(s) lt . We employ this strategy in our experiments."
    }, {
      "heading" : "5. Experimental Results",
      "text" : "To validate our proposed variational inference algorithm, we performed source separation experiments on speech mixtures and compared our results to those of exact inference within the N-FHMM of (Mysore et al., 2010). As the proposed algorithm has much lower computational complexity, our goal was to try to achieve source separation performance that came close to that of exact inference. Additionally, we compared to the performance of the decoupled variational approximation (Section 3.1), and to the performance of probabilistic latent component analysis (PLCA) (Smaragdis et al., 2007), this being the baseline comparison used in (Mysore et al., 2010). PLCA is a probabilistic audio interpretation of NMF (up to a column-wise normalization). It therefore serves as an baseline equivalent to NMF.\nWe performed experiments with 50 different speech mixtures and report the mean of the results. Data were taken from TIMIT, a commonly used corpus for speech processing and speech recognition tasks. It comprises numerous sentences from multiple speakers.\nWe performed each of the 50 experiments as follows. We first randomly chose one male and one female speaker. For each speaker, we assigned nine sentences as training data and one sentence as test data. We then concatenated the training data sentences and ob-\ntained a spectrogram with a window size of 64ms and a hop size of 16ms (the sampling rate was 16KHz). Next, we learned an N-HMM (with an ergodic Markov chain) from the training data of each speaker, yielding a set of dictionaries and a transition matrix for each speaker. The next step was to mix the two test sentences (one from each speaker) at 0dB and obtain the spectrogram using the above window size and hop size. We then combined the dictionaries and transition matrices of the two speakers into either a joint PLCA or an N-FHMM model, performed inference using the various methods, and separated the sources.\nWe used the standard BSS-EVAL suite of metrics (Vincent et al., 2006) to evaluate the source separation performance. This suite consists of three signal to noise ratio (SNR) type metrics (in dB). Source to Interference Ratio (SIR) evaluates the suppression of the unwanted source. Source to Artifacts Ratio (SAR) evaluates the amount of artifacts introduced by the separation process (with larger numbers reflecting less artifacts). Source to Distortion Ratio (SDR) gives us an overall source separation score that takes both the suppression capability as well as the introduced artifacts into account. We computed these metrics on each of our 100 sources separated from 50 mixtures.\nIn order to find the optimal configuration of our model (the number of dictionaries and number of elements per dictionary), we repeated these experiments in 30\ndifferent configurations. The BSS-EVAL metrics for all of these configurations are shown in Figure 8. The optimal configuration (in terms of SDR) was 20 dictionaries with 20 elements each. It is evident, however, that the different configurations yield similar performance scores, except for a noticeable drop when using only 1 element per dictionary. This is encouraging as it implies that the algorithm is not particularly sensitive to the specific configuration for this kind of data.\nTo make comparison to exact inference unbiased, we also searched for the optimal configuration in that case. Here, 20 dictionaries with 30 elements each yielded the best source separation performance (data not shown), although, as with the proposed variational inference algorithm, the metrics did not vary substantially with different configurations. Table 1 shows the results obtained with this configuration as well as the results of the proposed method and decoupled variational inference when using the same configuration.\nAs a baseline, we also experimented with PLCA with various dictionary sizes, and found that 30 dictionary elements yielded the optimal source separation performance. The results of using this configuration of PLCA are are also shown in Table 1.\nWe see that our algorithm performs almost as well as exact inference even when using the same configuration (Table 1). The difference in SDR is less than 0.2 dB. There is however a large difference in computation time. Based on the configuration of each source having 20 dictionaries of 30 elements each, we empirically found each iteration of the proposed method to be about 30 times faster than using exact inference. However, when using the optimal configuration for the proposed method (20 dictionaries of 20 elements each), we observed about a 40x speedup. The proposed method generally took about twenty iterations\nto converge (Figure 9), which is a similar number to that seen with exact inference.\nThe SIR of the proposed method is lower than that of exact inference but the SAR is higher. This can be understood as follows. Exact inference returns a higher SIR because it is more constrained. Only one dictionary from each source may be active in each time frame. This restriction is relaxed in the proposed method, allowing some interference from elements of the other dictionaries. This very property gives the proposed method a higher SAR. In order to reduce artifacts, it can be helpful to recruit some contribution from dictionary elements that correspond to non-active Markov states. This can help explain nuances in the spectral time frame that the active dictionary might not capture completely. This is possible in the proposed method, but not in the rigid exact N-FHMM model. The proposed method and exact inference therefore have a fairly even SIR/SAR trade off leading to approximately the same SDR scores.\nAs shown in Table 1, the proposed method outperforms PLCA in all three metrics and the decoupled variational approximation performs very poorly with a lower SDR and SIR than even PLCA."
    }, {
      "heading" : "6. Conclusions",
      "text" : "We have proposed a Bayesian variant of the N-FHMM and an efficient variational inference algorithm for the model. The computational complexity of the algorithm is linear in the number of sources, and it is about 30 times faster than exact inference on an empirically optimal configuration of the N-FHMM, with comparable source separation accuracy. Although variational inference in the N-FHMM was demonstrated on the task of source separation, it is a general model of sound mixtures and can be used for various other audio tasks such as concurrent speech recognition of multiple speakers and automatic music transcription."
    } ],
    "references" : [ {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D. Blei", "A. Ng", "M. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Factorial hidden Markov models",
      "author" : [ "Z. Ghahramani", "M. Jordan" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Ghahramani and Jordan,? \\Q1997\\E",
      "shortCiteRegEx" : "Ghahramani and Jordan",
      "year" : 1997
    }, {
      "title" : "Probabilistic latent semantic indexing",
      "author" : [ "T. Hofmann" ],
      "venue" : "In Proceedings of the 22nd International Conference on Research and Development in Information Retrieval, Berkeley,",
      "citeRegEx" : "Hofmann,? \\Q1999\\E",
      "shortCiteRegEx" : "Hofmann",
      "year" : 1999
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "M. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Jordan et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Jordan et al\\.",
      "year" : 1999
    }, {
      "title" : "Algorithms for nonnegative matrix factorization",
      "author" : [ "D.D. Lee", "H.S. Seung" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Lee and Seung,? \\Q2001\\E",
      "shortCiteRegEx" : "Lee and Seung",
      "year" : 2001
    }, {
      "title" : "A tutorial on hidden Markov models and selected applications in speech recognition",
      "author" : [ "L.R. Rabiner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Rabiner,? \\Q1989\\E",
      "shortCiteRegEx" : "Rabiner",
      "year" : 1989
    }, {
      "title" : "Non-negative matrix factorization for polyphonic music transcription",
      "author" : [ "P. Smaragdis", "J.C. Brown" ],
      "venue" : "In Proceedings of the IEEE Workshop of Applications of Signal Processing to Audio and Acoustics,",
      "citeRegEx" : "Smaragdis and Brown,? \\Q2003\\E",
      "shortCiteRegEx" : "Smaragdis and Brown",
      "year" : 2003
    }, {
      "title" : "Performance measurement in blind audio source separation",
      "author" : [ "E. Vincent", "C. Fevotte", "R. Gribonval" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2006
    }, {
      "title" : "Monaural sound source separation by nonnegative matrix factorization with temporal continuity and sparseness criteria",
      "author" : [ "T. Virtanen" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing,",
      "citeRegEx" : "Virtanen,? \\Q2007\\E",
      "shortCiteRegEx" : "Virtanen",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "NMF and its probabilistic counterparts have been used extensively for audio source separation (Virtanen, 2007; Smaragdis et al., 2007).",
      "startOffset" : 94,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "In these models, each time frame of the spectrogram is viewed as a histogram of “sound quanta” in the same way that a document is viewed as a histogram of words in topic models (Hofmann, 1999; Blei et al., 2003).",
      "startOffset" : 177,
      "endOffset" : 211
    }, {
      "referenceID" : 0,
      "context" : "In these models, each time frame of the spectrogram is viewed as a histogram of “sound quanta” in the same way that a document is viewed as a histogram of words in topic models (Hofmann, 1999; Blei et al., 2003).",
      "startOffset" : 177,
      "endOffset" : 211
    }, {
      "referenceID" : 0,
      "context" : "Without the temporal dynamics, our formulation is similar to that of latent Dirichlet allocation LDA (Blei et al., 2003).",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "Variational inference (Jordan et al., 1999) refers to a class of techniques that are used to approximate an intractable posterior distribution with a simpler (typically factorized) distribution.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "The variational iterations then update each individual NHMM posterior using the forward–backward algorithm (Rabiner, 1989) while keeping the contribution of the other N-HMM fixed; and then revise the mixing proportions of the sources.",
      "startOffset" : 107,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "By minimizing the KL divergence between the true posterior distribution and the factorized distribution, we obtain the following variational inference solution (Jordan et al., 1999) for each of the factors:",
      "startOffset" : 160,
      "endOffset" : 181
    }, {
      "referenceID" : 0,
      "context" : "The digamma terms arise from the normalizing Γ-functions of the Dirichlet distribution (Blei et al., 2003).",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 7,
      "context" : "We used the standard BSS-EVAL suite of metrics (Vincent et al., 2006) to evaluate the source separation performance.",
      "startOffset" : 47,
      "endOffset" : 69
    } ],
    "year" : 2012,
    "abstractText" : "The past decade has seen substantial work on the use of non-negative matrix factorization and its probabilistic counterparts for audio source separation. Although able to capture audio spectral structure well, these models neglect the non-stationarity and temporal dynamics that are important properties of audio. The recently proposed non-negative factorial hidden Markov model (N-FHMM) introduces a temporal dimension and improves source separation performance. However, the factorial nature of this model makes the complexity of inference exponential in the number of sound sources. Here, we present a Bayesian variant of the N-FHMM suited to an efficient variational inference algorithm, whose complexity is linear in the number of sound sources. Our algorithm performs comparably to exact inference in the original NFHMM but is significantly faster. In typical configurations of the N-FHMM, our method achieves around a 30x increase in speed.",
    "creator" : "LaTeX with hyperref package"
  }
}