{
  "name" : "1506.05790.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Scalable Semi-Supervised Aggregation of Classifiers",
    "authors" : [ "Akshay Balsubramani", "Yoav Freund" ],
    "emails" : [ "abalsubr@cs.ucsd.edu", "yfreund@cs.ucsd.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Ensemble-based learning is a very successful approach to learning classifiers, including well-known methods like boosting [1], bagging [2], and random forests [3]. The power of these methods has been clearly demonstrated in open large-scale learning competitions such as the Netflix Prize [4] and the ImageNet Challenge [5]. In general, these methods train a large number of “base” classifiers and then combine them using a (possibly weighted) majority vote. By aggregating over classifiers, ensemble methods reduce the variance of the predictions, and sometimes also reduce the bias [6].\nThe ensemble methods above rely solely on a labeled training set of data. In this paper we propose an ensemble method that uses a large unlabeled data set in addition to the labeled set. Our work is therefore at the intersection of semi-supervised learning [7, 8] and ensemble learning.\nThis paper is based on recent theoretical results of the authors [9]. Our main contributions here are to extend and apply those results with a new algorithm in the context of random forests [3] and to perform experiments in which we show that, when the number of labeled examples is small, our algorithm’s performance is at least that of random forests, and often significantly better.\nFor the sake of completeness, we provide an intuitive introduction to the analysis given in [9]. How can unlabeled data help in the context of ensemble learning? Consider a simple example with six equiprobable data points. The ensemble consists of six classifiers, partitioned into three “A” rules and three “B” rules. Suppose that the “A” rules each have error 1/3 and the “B” rules each have error 1/6. 1 If given only this information, we might take the majority vote over the six rules, possibly giving lower weights to the “A” rules because they have higher errors.\nSuppose, however, that we are given the unlabeled information in Table 1. The columns of this table correspond to the six classifiers and the rows to the six unlabeled examples. Each entry corresponds to the prediction of the given classifier on the given example. As we see, the main difference between the “A” rules and the “B” rules is that any two “A” rules disagree with probability 1/3, whereas the “B” rules always agree. For this example, it can be seen (e.g. proved by contradiction) that the only possible true labeling of the unlabeled data that is consistent with Table 1 and with the errors of the classifiers is that all the examples are labeled ’+’.\nConsequently, we conclude that the majority vote over the “A” rules has zero error, performing significantly better than any of the base rules. In contrast, giving the “B” rules equal weight would\n1We assume that (bounds on) the errors are, with high probability, true on the actual distribution. Such bounds can be derived using large deviation bounds or bootstrap-type methods.\nar X\niv :1\n50 6.\n05 79\n0v 2\n[ cs\n.L G\n] 1\n1 N\nov 2\n01 5\nresult in an a rule with error 1/6. Crucially, our reasoning to this point has solely used the structure of the unlabeled examples along with the error rates in Table 1 to constrain our search for the true labeling.\nBy such reasoning alone, we have correctly predicted according to a weighted majority vote. This example provides some insight into the ways in which unlabeled data can be useful:\n• When combining classifiers, diversity is important. It can be better to combine less accurate rules that disagree with each other than to combine more accurate rules that tend to agree. • The bounds on the errors of the rules can be seen as a set of constraints on the true labeling. A complementary set of constraints is provided by the unlabeled examples. These sets of constraints can be combined to improve the accuracy of the ensemble classifier.\nThe above setup was recently introduced and analyzed in [9]. That paper characterizes the problem as a zero-sum game between a predictor and an adversary. It then describes the minimax solution of the game, which corresponds to an efficient algorithm for transductive learning.\nIn this paper, we build on the worst-case framework of [9] to devise an efficient and practical semisupervised aggregation algorithm for random forests. To achieve this, we extend the framework to handle specialists – classifiers which only venture to predict on a subset of the data, and abstain from predicting on the rest. Specialists can be very useful in targeting regions of the data on which to precisely suggest a prediction.\nThe high-level idea of our algorithm is to artificially generate new specialists from the ensemble. We incorporate these, and the targeted information they carry, into the worst-case framework of [9]. The resulting aggregated predictor inherits the advantages of the original framework:\n(A) Efficient: Learning reduces to solving a scalable p-dimensional convex optimization, and test-time prediction is as efficient and parallelizable as p-dimensional linear prediction. (B) Versatile/robust: No assumptions about the structure or origin of the predictions or labels. (C) No introduced parameters: The aggregation method is completely data-dependent. (D) Safe: Accuracy guaranteed to be at least that of the best classifier in the ensemble.\nWe develop these ideas in the rest of this paper, reviewing the core worst-case setting of [9] in Section 2, and specifying how to incorporate specialists and the resulting learning algorithm in Section 3.\nThen we perform an exploratory evaluation of the framework on data in Section 4. Though the framework of [9] and our extensions can be applied to any ensemble of arbitrary origin, in this paper we focus on random forests, which have been repeatedly demonstrated to have state-of-theart, robust classification performance in a wide variety of situations [10]. We use a random forest as a base ensemble whose predictions we aggregate. But unlike conventional random forests, we do not simply take a majority vote over tree predictions, instead using a unlabeled-data-dependent aggregation strategy dictated by the worst-case framework we employ."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "A few definitions are required to discuss these issues concretely, following [9]. Write [a]+ = max(0, a) and [n] = {1, 2, . . . , n}. All vector inequalities are componentwise.\nWe first consider an ensemble H = {h1, . . . , hp} and unlabeled data x1, . . . , xn on which we wish to predict. As in [9], the predictions and labels are allowed to be randomized, represented by values in [−1, 1] instead of just the two values {−1, 1}. The ensemble’s predictions on the unlabeled data are denoted by F:\nF = h1(x1) h1(x2) · · · h1(xn)... ... . . . ... hp(x1) hp(x2) · · · hp(xn)  ∈ [−1, 1]p×n (1) We use vector notation for the rows and columns of F: hi = (hi(x1), · · · , hi(xn))> and xj = (h1(xj), · · · , hp(xj))>. The true labels on the test data T are represented by z = (z1; . . . ; zn) ∈ [−1, 1]n. The labels z are hidden from the predictor, but we assume the predictor has knowledge of a correlation vector b ∈ (0, 1]p such that 1n ∑ j hi(xj)zj ≥ bi, i.e. 1 nFz ≥ b. These p constraints on z exactly represent upper bounds on individual classifier error rates, which can be estimated from the training set w.h.p. when all the data are drawn i.i.d., in a standard way also used by empirical risk minimization (ERM) methods that simply predict with the minimum-error classifier [9]."
    }, {
      "heading" : "2.1 The Transductive Binary Classification Game",
      "text" : "The idea of [9] is to formulate the ensemble aggregation problem as a two-player zero-sum game between a predictor and an adversary. In this game, the predictor is the first player, who plays g = (g1; g2; . . . ; gn), a randomized label gi ∈ [−1, 1] for each example {xi}ni=1. The adversary then sets the labels z ∈ [−1, 1]n under the ensemble classifier error constraints defined by b. 2 The predictor’s goal is to minimize the worst-case expected classification error on the test data (w.r.t. the randomized labelings z and g), which is just 12 ( 1− 1nz >g ) . This is equivalently viewed as maximizing worst-case correlation 1nz >g. To summarize concretely, we study the following game:\nV := max g∈[−1,1]n min z∈[−1,1]n,\n1 nFz≥b\n1 n z>g (2)\nThe minimax theorem ([1], p.144) applies to the game (2), and there is an optimal strategy g∗ such\nthat min z∈[−1,1]n,\n1 nFz≥b\n1 n z>g∗ ≥ V , guaranteeing worst-case prediction error 12 (1− V ) on the n unlabeled\ndata. This optimal strategy g∗ is a simple function of a particular weighting over the p hypotheses – a nonnegative p-vector.\nDefinition 1 (Slack Function). Let σ ≥ 0p be a weight vector overH (not necessarily a distribution). The vector of ensemble predictions is F>σ = (x>1 σ, . . . ,x>n σ), whose elements’ magnitudes are the margins. The prediction slack function is\nγ(σ,b) := γ(σ) := −b>σ + 1 n n∑ j=1 [∣∣x>j σ∣∣− 1]+ (3) and this is convex in σ. The optimal weight vector σ∗ is any minimizer σ∗ ∈ argmin\nσ≥0p [γ(σ)].\nThe main result of [9] uses these to describe the minimax equilibrium of the game (2).\nTheorem 2 ([9]). The minimax value of the game (2) is V = −γ(σ∗). The minimax optimal predictions are defined as follows: for all j ∈ [n],\ng∗j := gj(σ ∗) =\n{ x>j σ ∗ ∣∣x>j σ∗∣∣ < 1\nsgn(x>j σ ∗) otherwise\n2Since b is calculated from the training set and deviation bounds, we assume the problem feasible w.h.p."
    }, {
      "heading" : "2.2 Interpretation",
      "text" : "Theorem 2 suggests a statistical learning algorithm for aggregating the p ensemble classifiers’ predictions: estimate b from the training (labeled) set, optimize the convex slack function γ(σ) to find σ∗, and finally predict with gj(σ∗) on each example j in the test set. The resulting predictions are guaranteed to have low error, as measured by V . In particular, it is easy to prove [9] that V is at least maxi bi, the performance of the best classifier.\nThe slack function (3) merits further scrutiny. Its first term depends only on the labeled data and not the unlabeled set, while the second term 1n ∑n j=1 [∣∣x>j σ∣∣− 1]+ incorporates only unlabeled information. These two terms trade off smoothly – as the problem setting becomes fully supervised and unlabeled information is absent, the first term dominates, and σ∗ tends to put all its weight on the best single classifier like ERM.\nIndeed, this viewpoint suggests a (loose) interpretation of the second term as an unsupervised regularizer for the otherwise fully supervised optimization of the “average” error b>σ. It turns out that a change in the regularization factor corresponds to different constraints on the true labels z:\nTheorem 3 ([9]). Let Vα := max g∈[−1,1]n min z∈[−α,α]n,\n1 nFz≥b\n1 n z>g for any α > 0. Then Vα =\nminσ≥0p [ −b>σ + αn ∑n j=1 [∣∣x>j σ∣∣− 1]+]. So the regularized optimization assumes each zi ∈ [−α, α]. For α < 1, this is equivalent to assuming the usual binary labels (α = 1), and then adding uniform random label noise: flipping the label w.p. 12 (1−α) on each of the n examples independently. This encourages “clipping” of the ensemble predictions x>j σ ∗ to the σ∗-weighted majority vote predictions, as specified by g∗."
    }, {
      "heading" : "2.3 Advantages and Disadvantages",
      "text" : "This formulation has several significant merits that would seem to recommend its use in practical situations. It is very efficient – once b is estimated (a scalable task, given the labeled set), the slack function γ is effectively an average over convex functions of i.i.d. unlabeled examples, and consequently is amenable to standard convex optimization techniques [9] like stochastic gradient descent (SGD) and variants. These only operate in p dimensions, independent of n (which is p). The slack function is Lipschitz and well-behaved, resulting in stable approximate learning.\nMoreover, test-time prediction is extremely efficient, because it only requires the p-dimensional weighting σ∗ and can be computed example-by-example on the test set using only a dot product in Rp. The form of g∗ and its dependence on σ∗ facilitates interpretation as well, as it resembles familiar objects: sigmoid link functions for linear classifiers.\nOther advantages of this method also bear mention: it makes no assumptions on the structure of H or F, is provably robust against the worst case, and adds no input parameters that need tuning. These benefits are notable because they will be inherited by our extension of the framework in this paper.\nHowever, this algorithm’s practical performance can still be mediocre on real data, which is often easier to predict than an adversarial setup would have us believe. As a result, we seek to add more information in the form of constraints on the adversary, to narrow the gap between it and reality."
    }, {
      "heading" : "3 Learning with Specialists",
      "text" : "To address this issue, we examine a generalized scenario in which each classifier in the ensemble can abstain on any subset of the examples instead of predicting ±1. It is a specialist that predicts only over a subset of the input, and we think of its abstain/participate decision being randomized in the same way as the randomized label on each example. In this section, we extend the framework of Section 2.1 to arbitrary specialists, and discuss the semi-supervised learning algorithm that results.\nIn our formulation, suppose that for a classifier i ∈ [p] and an example x, the classifier decides to abstain with probability 1 − vi(x). But if the decision is to participate, the classifier predicts\nhi(x) ∈ [−1, 1] as previously. Our only assumption on {vi(x1), . . . , vi(xn)} is the reasonable one that ∑n j=1 vi(xj) > 0, so classifier i is not a worthless specialist that abstains everywhere.\nThe constraint on classifier i is now not on its correlation with z on the entire test set, but on the average correlation with z restricted to occasions on which it participates. So for some [bS ]i ∈ [0, 1],\nn∑ j=1 ( vi(xj)∑n k=1 vi(xk) ) hi(xj)zj ≥ [bS ]i (4)\nDefine ρi(xj) := vi(xj)∑n\nk=1 vi(xk) (a distribution over j ∈ [n]) for convenience. Now redefine our\nunlabeled data matrix as follows:\nS = n ρ1(x1)h1(x1) ρ1(x2)h1(x2) · · · ρ1(xn)h1(xn)... ... . . . ... ρp(x1)hp(x1) ρp(x2)hp(x2) · · · ρp(xn)hp(xn)  (5) Then the constraints (4) can be written as 1nSz ≥ bS , analogous to the initial prediction game (2). To summarize, our specialist ensemble aggregation game is stated as\nVS := min z∈[−1,1]n, 1 nSz≥bS max g∈[−1,1]n\n1 n z>g (6)\nWe can immediately solve this game from Thm. 2, with (S,bS) simply taking the place of (F,b). Theorem 4 (Solution of the Specialist Aggregation Game). The awake ensemble prediction w.r.t.\nweighting σ ≥ 0p on example xi is [ S>σ ] i = n p∑ j=1 ρj(xi)hj(xi)σj . The slack function is now\nγS(σ) := 1\nn n∑ j=1 [∣∣∣[S>σ] j ∣∣∣− 1] + − b>S σ (7)\nThe minimax value of this game is VS = maxσ≥0p [−γS(σ)] = −γS(σ∗S). The minimax optimal predictions are defined as follows: for all i ∈ [n],\n[g∗S ]i . = gS(σ ∗ S) =\n{[ S>σ∗S ] i ∣∣[S>σ∗S]i∣∣ < 1 sgn( [ S>σ∗S ] i ) otherwise\nIn the no-specialists case, the vector ρi is the uniform distribution ( 1n , . . . , 1 n ) for any i ∈ [p], and the problem reduces to the prediction game (2). As in the original prediction game, the minimax equilibrium depends on the data only through the ensemble predictions, but these are now of a different form. Each example is now weighted proportionally to ρj(xi). So on any given example xi, only hypotheses which participate on it will be counted; and those that specialize the most narrowly, and participate on the fewest other examples, will have more influence on the eventual prediction gi, ceteris paribus."
    }, {
      "heading" : "3.1 Creating Specialists for an Algorithm",
      "text" : "We can now present the main ensemble aggregation method of this paper, which creates specialists from the ensemble, adding them as additional constraints (rows of S). The algorithm, HEDGECLIPPER, is given in Fig. 1, and instantiates our specialist learning framework with a random forest [3]. As an initial exploration of the framework here, random forests are an appropriate base ensemble because they are known to exhibit state-of-the-art performance [10]. Their wellknown advantages also include scalability, robustness (to corrupt data and parameter choices), and interpretability; each of these benefits is shared by our aggregation algorithm, which consequently inherits them all.\nFurthermore, decision trees are a natural fit as the ensemble classifiers because they are inherently hierarchical. Intuitively (and indeed formally too [11]), they act like nearest-neighbor (NN) predictors w.r.t. a distance that is “adaptive” to the data. So each tree in a random forest represents a\nsomewhat different, nonparametric partition of the data space into regions in which one of the labels ±1 dominates. Each such region corresponds exactly to a leaf of the tree. The idea of HEDGECLIPPER is simply to consider each leaf in the forest as a specialist, which predicts only on the data falling into it. By the NN intuition above, these specialists can be viewed as predicting on data that is near them, where the supervised training of the tree attempts to define the purest possible partitioning of the space. A pure partitioning results in many specialists with [bS ]i ≈ 1, each of which contributes to the awake ensemble prediction w.r.t. σ∗ over its domain, to influence it towards the correct label (inasmuch as [bS ]i is high).\nThough the idea is complex in concept for a large forest with many arbitrarily overlapping leaves from different trees, it fits the worst-case specialist framework of the previous sections. So the algorithm is still essentially linear learning with convex optimization, as we have described.\nAlgorithm 1 HEDGECLIPPER Input: Labeled set L, unlabeled set U\n1: Using L, grow trees T = {T1, . . . , Tp} (regularized; see Sec. 3.2) 2: Using L, estimate bS on T and its leaves 3: Using U , (approximately) optimize (7)\nto estimate σ∗S Output: The estimated weighting σ∗S , for\nuse at test time\nFigure 1: At left is algorithm HEDGECLIPPER. At right is a schematic of how the forest structure is related to the unlabeled data matrix S, with a given example x highlighted. The two colors in the matrix represent ±1 predictions, and white cells abstentions."
    }, {
      "heading" : "3.2 Discussion",
      "text" : "Trees in random forests have thousands of leaves or more in practice. As we are advocating adding so many extra specialists to the ensemble for the optimization, it is natural to ask whether this erodes some of the advantages we have claimed earlier.\nComputationally, it does not. When ρj(xi) = 0, i.e. classifier j abstains deterministically on xi, then the value of hj(xi) is irrelevant. So storing S in a sparse matrix format is natural in our setup, with the accompanying performance gain in computing S>σ while learning σ∗ and predicting with it. This turns out to be crucial to efficiency – each tree induces a partitioning of the data, so the set of rows corresponding to any tree contains n nonzero entries in total. This is seen in Fig. 1.\nStatistically, the situation is more complex. On one hand, there is no danger of overfitting in the traditional sense, regardless of how many specialists are added. Each additional specialist can only shrink the constraint set that the adversary must follow in the game (6). It only adds information about z, and therefore the performance VS must improve, if the game is solved exactly.\nHowever, for learning we are only concerned with approximately optimizing γS(σ) and solving the game. This presents several statistical challenges. Standard optimization methods do not converge as well in high ambient dimension, even given the structure of our problem. In addition, random forests practically perform best when each tree is grown to overfit. In our case, on any sizable test set, small leaves would cause some entries of S to have large magnitude, 1. This can foil an algorithm like HEDGECLIPPER by causing it to vary wildly during the optimization, particularly since those leaves’ [bS ]i values are only roughly estimated.\nFrom an optimization perspective, some of these issues can be addressed by e.g. (pseudo)-secondorder methods [12], whose effect would be interesting to explore in future work. Our implementation opts for another approach – to grow trees constrained to have a nontrivial minimum weight per leaf. Of course, there are many other ways to handle this, including using the tree structure beyond the leaves; we just aim to conduct an exploratory evaluation here, as several of these areas remain ripe for future research."
    }, {
      "heading" : "4 Experimental Evaluation",
      "text" : "We now turn to evaluating HEDGECLIPPER on publicly available datasets. Our implementation uses minibatch SGD to optimize (6), runs in Python on top of the popular open-source learning package scikit-learn, and runs out-of-core (n-independent memory), taking advantage of the scalability of our formulation. 3 The datasets are drawn from UCI/LibSVM as well as data mining sites like Kaggle, and no further preprocessing was done on the data. We refer to “Base RF” as the forest of constrained trees from which our implementation draws its specialists. We restrict the training data available to the algorithm, using mostly supervised datasets because these far outnumber medium/large-scale public semi-supervised datasets. Unused labeled examples are combined with the test examples (and the extra unlabeled set, if any is provided) to form the set of unlabeled data used by the algorithm. Further information and discussion on the protocol is in the appendix.\nClass-imbalanced and noisy sets are included to demonstrate the aforementioned practical advantages of HEDGECLIPPER. Therefore, AUC is an appropriate measure of performance, and these results are in Table 2. Results are averaged over 10 runs, each drawing a different random subsample of labeled data. The best results according to a paired t-test are in bold.\nWe find that the use of unlabeled data is sufficient to achieve improvements over even traditionally overfitted RFs in many cases. Notably, in most cases there is a significant benefit given by unlabeled data in our formulation, as compared to the base RF used. The boosting-type methods also perform fairly well, as we discuss in the next section.\nThe awake ensemble prediction values x>σ on the unlabeled set are a natural way to visualize and explore the operation of the algorithm on the data, in an analogous way to the margin distribution in boosting [6]. One representative sample is in Fig. 2, on SUSY, a dataset with many (5M) examples, roughly evenly split between ±1. These plots demonstrate that our algorithm produces much more peaked class-conditional ensemble prediction distributions than random forests, suggesting marginbased learning applications. Changing α alters the aggressiveness of the clipping, inducing a more or less peaked distribution. The other datasets without dramatic label imbalance show very similar qualitative behavior in these respects, and these plots help choose α in practice (see appendix).\nToy datasets with extremely low dimension seem to exhibit little to no significant improvement from our method. We believe this is because the distinct feature splits found by the random forest are few in number, and it is the diversity in ensemble predictions that enables HEDGECLIPPER to clip (weighted majority vote) dramatically and achieve its performance gains.\nOn the other hand, given a large quantity of data, our algorithm is able to learn significant structure, the minimax structure appears appreciably close to reality, as evinced by the results on large datasets."
    }, {
      "heading" : "5 Related and Future Work",
      "text" : "This paper’s framework and algorithms are superficially reminiscent of boosting, another paradigm that uses voting behavior to aggregate an ensemble and has a game-theoretic intuition [1, 15]. There is some work on semi-supervised versions of boosting [16], but it departs from this principled structure and has little in common with our approach. Classical boosting algorithms like AdaBoost [17] make no attempt to use unlabeled data. It is an interesting open problem to incorporate boosting ideas into our formulation, particularly since the two boosting-type methods acquit themselves well\n3It is possible to make this footprint independent of d as well by hashing features [13], not done here.\nin our results, and can pack information parsimoniously into many fewer ensemble classifiers than random forests.\nThere is a long-recognized connection between transductive and semi-supervised learning, and our method bridges these two settings. Popular variants on supervised learning such as the transductive SVM [18] and graph-based or nearest-neighbor algorithms, which dominate the semi-supervised literature [8], have shown promise largely in data-poor regimes because they face major scalability challenges. Our focus on ensemble aggregation instead allows us to keep a computationally inexpensive linear formulation and avoid considering the underlying feature space of the data. Largely unsupervised ensemble methods have been explored especially in applications like crowdsourcing, in which the method of [19] gave rise to a plethora of Bayesian methods under various conditional independence generative assumptions on F [20]. Using tree structure to construct new features has been applied successfully, though without guarantees [21].\nLearning with specialists has been studied in an adversarial online setting as in the work of Freund et al. [22]. Though that paper’s setting and focus is different from ours, the optimal algorithms it derives also depend on each specialist’s average error on the examples on which it is awake.\nFinally, we re-emphasize the generality of our formulation, which leaves many interesting questions to be explored. The specialists we form are not restricted to being trees; there are other ways of dividing the data like clustering methods. Indeed, the ensemble can be heterogeneous and even incorporate other semi-supervised methods. Our method is complementary to myriad classification algorithms, and we hope to stimulate inquiry into the many research avenues this opens."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors acknowledge support from the National Science Foundation under grant IIS-1162581."
    }, {
      "heading" : "A Additional Information on Experiments",
      "text" : "A.1 Datasets\nInformation on the datasets used:\nDataset Data sizes (m/)n Dim. d Comments kagg-prot 3751 1776 Kaggle challenge [23] ssl-text 1500 11960 [7] kagg-cred 150000 10 Kaggle challenge [24] a1a 1605 / 30956 123 LibSVM w1a 2477 / 47272 300 LibSVM covtype 581012 54 LibSVM ssl-secstr 83679 (unla-\nbeled:1189472) 315 [7]\nSUSY 5000000 18 UCI HIGGS 11000000 28 UCI epsilon 500000 2000 PASCAL Large Scale Learning Challenge 2008 webspam-uni 350000 254 LibSVM\nAll data from the challenges (e.g. kagg-cred) lacked test labels, so the results reported are averaged over 10 random splits of the training data.\nA.2 Algorithms\nIn all cases, the random forests were grown with default parameters for the feature and data splits (bootstrap data sample of input data size, and∼ √ d features considered per split), and 100 trees was standard. Varying these changes the induced diversity of the trees/partitions and may fundamentally affect the output of our algorithm, but exploration of such aspects is left to future work. All the comparator algorithms were also run with scikit-learn’s default parameters – in many cases like RFs, they are fairly insensitive to parameter choice.\nTo overcome the statistical issues discussed in Sec. 3.2, we found we needed to enforce some regularization on the tree used. We chose to impose a constraint on the minimum number of training examples in any leaf of the tree. This constraint was imposed as a parameter to grow the forest; thereafter, we could use all resulting leaves as specialists. To avoid any leaf specialist weights being too large but still collect as many leaf specialists as possible, we set the minimum number of examples per leaf to 10 with ≥ 1K labeled examples, and to 4 otherwise. We also tried an alternative way of avoiding small specialists: to simply grow an unregularized forest and then filter out leaves, selecting only large enough leaves as specialists. This generally performed comparably or worse, consistent with the intuition that the diversity in unregularized tree predictions often manifests largely on small leaves.\nAs pointed out in the paper, estimating b is an important step in an implementation. Accordingly, we used a bootstrap sample to do so; this performed comparably to holding out a validation set from a constant fraction of the labeled data. We observe throughout that it seems to matter far less how well the ensemble is trained, and more how well b is estimated; so we elected to only keep a modicum of labeled data for actually training the ensemble, and most for estimation.\nA notable issue we encountered is the setting of the “noise” rescaling factor α. We found HEDGECLIPPER to be relatively insensitive to the precise choice of α, so it essentially sufficed for our experiments to try three choices: {0.3, 1.0, 3.0}. The last is > 1.0, and therefore does not have an interpretation in terms of uniform label noise, but it is certainly a valid computational tool.\nWhich of these three αs to choose? Generally, we found that choosing α = 0.3 does not hurt performance, because our performance goals are often met best by separating the class-conditional peaks as much as possible. This is dangerous for more class-unbalanced datasets like kagg-cred, however, for which the default α = 1.0 works best. A useful heuristic which we used to choose α is simply to look at the class-conditional awake ensemble prediction distribution as plotted in Fig.\n2; the distribution can be roughly estimated and plotted on the fly, and we can quickly ascertain sensible choices of parameters like α. These choices appear to matter less at larger scale."
    } ],
    "references" : [ {
      "title" : "Boosting: Foundations and Algorithms",
      "author" : [ "Robert E. Schapire", "Yoav Freund" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "The bellkor solution to the netflix grand prize",
      "author" : [ "Yehuda Koren" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Boosting the margin: A new explanation for the effectiveness of voting methods",
      "author" : [ "Robert E Schapire", "Yoav Freund", "Peter Bartlett", "Wee Sun Lee" ],
      "venue" : "Annals of statistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "Introduction to semi-supervised learning",
      "author" : [ "Xiaojin Zhu", "Andrew B Goldberg" ],
      "venue" : "Synthesis lectures on artificial intelligence and machine learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Optimally combining classifiers using unlabeled data",
      "author" : [ "Akshay Balsubramani", "Yoav Freund" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "An empirical evaluation of supervised learning in high dimensions",
      "author" : [ "Rich Caruana", "Nikos Karampatziakis", "Ainur Yessenalina" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Random forests and adaptive nearest neighbors",
      "author" : [ "Yi Lin", "Yongho Jeon" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Feature hashing for large scale multitask learning",
      "author" : [ "Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Greedy function approximation: a gradient boosting machine",
      "author" : [ "Jerome H Friedman" ],
      "venue" : "Annals of statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2001
    }, {
      "title" : "Game theory, on-line prediction and boosting",
      "author" : [ "Yoav Freund", "Robert E Schapire" ],
      "venue" : "In Proceedings of the ninth annual conference on Computational learning theory,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1996
    }, {
      "title" : "Semiboost: Boosting for semi-supervised learning",
      "author" : [ "P Kumar Mallapragada", "Rong Jin", "Anil K Jain", "Yi Liu" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1997
    }, {
      "title" : "Transductive inference for text classification using support vector machines",
      "author" : [ "Thorsten Joachims" ],
      "venue" : "In Proceedings of the Sixteenth International Conference on Machine Learning,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1999
    }, {
      "title" : "Maximum likelihood estimation of observer error-rates using the em algorithm",
      "author" : [ "Alexander Philip Dawid", "Allan M Skene" ],
      "venue" : "Applied statistics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1979
    }, {
      "title" : "Ranking and combining multiple predictors without labeled data",
      "author" : [ "Fabio Parisi", "Francesco Strino", "Boaz Nadler", "Yuval Kluger" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Fast discriminative visual codebooks using randomized clustering forests",
      "author" : [ "Frank Moosmann", "Bill Triggs", "Frederic Jurie" ],
      "venue" : "In Twentieth Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "Using and combining predictors that specialize",
      "author" : [ "Yoav Freund", "Robert E Schapire", "Yoram Singer", "Manfred K Warmuth" ],
      "venue" : "In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Ensemble-based learning is a very successful approach to learning classifiers, including well-known methods like boosting [1], bagging [2], and random forests [3].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "The power of these methods has been clearly demonstrated in open large-scale learning competitions such as the Netflix Prize [4] and the ImageNet Challenge [5].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "By aggregating over classifiers, ensemble methods reduce the variance of the predictions, and sometimes also reduce the bias [6].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "Our work is therefore at the intersection of semi-supervised learning [7, 8] and ensemble learning.",
      "startOffset" : 70,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "This paper is based on recent theoretical results of the authors [9].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "For the sake of completeness, we provide an intuitive introduction to the analysis given in [9].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "The above setup was recently introduced and analyzed in [9].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "In this paper, we build on the worst-case framework of [9] to devise an efficient and practical semisupervised aggregation algorithm for random forests.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : "We incorporate these, and the targeted information they carry, into the worst-case framework of [9].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 4,
      "context" : "We develop these ideas in the rest of this paper, reviewing the core worst-case setting of [9] in Section 2, and specifying how to incorporate specialists and the resulting learning algorithm in Section 3.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "Though the framework of [9] and our extensions can be applied to any ensemble of arbitrary origin, in this paper we focus on random forests, which have been repeatedly demonstrated to have state-of-theart, robust classification performance in a wide variety of situations [10].",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "Though the framework of [9] and our extensions can be applied to any ensemble of arbitrary origin, in this paper we focus on random forests, which have been repeatedly demonstrated to have state-of-theart, robust classification performance in a wide variety of situations [10].",
      "startOffset" : 272,
      "endOffset" : 276
    }, {
      "referenceID" : 4,
      "context" : "A few definitions are required to discuss these issues concretely, following [9].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "As in [9], the predictions and labels are allowed to be randomized, represented by values in [−1, 1] instead of just the two values {−1, 1}.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 4,
      "context" : ", in a standard way also used by empirical risk minimization (ERM) methods that simply predict with the minimum-error classifier [9].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "The idea of [9] is to formulate the ensemble aggregation problem as a two-player zero-sum game between a predictor and an adversary.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "The minimax theorem ([1], p.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : "The main result of [9] uses these to describe the minimax equilibrium of the game (2).",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "Theorem 2 ([9]).",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "In particular, it is easy to prove [9] that V is at least maxi bi, the performance of the best classifier.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "Theorem 3 ([9]).",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "unlabeled examples, and consequently is amenable to standard convex optimization techniques [9] like stochastic gradient descent (SGD) and variants.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "So for some [bS ]i ∈ [0, 1],",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "As an initial exploration of the framework here, random forests are an appropriate base ensemble because they are known to exhibit state-of-the-art performance [10].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 6,
      "context" : "Intuitively (and indeed formally too [11]), they act like nearest-neighbor (NN) predictors w.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 7,
      "context" : "(pseudo)-secondorder methods [12], whose effect would be interesting to explore in future work.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "The awake ensemble prediction values x>σ on the unlabeled set are a natural way to visualize and explore the operation of the algorithm on the data, in an analogous way to the margin distribution in boosting [6].",
      "startOffset" : 208,
      "endOffset" : 211
    }, {
      "referenceID" : 0,
      "context" : "This paper’s framework and algorithms are superficially reminiscent of boosting, another paradigm that uses voting behavior to aggregate an ensemble and has a game-theoretic intuition [1, 15].",
      "startOffset" : 184,
      "endOffset" : 191
    }, {
      "referenceID" : 10,
      "context" : "This paper’s framework and algorithms are superficially reminiscent of boosting, another paradigm that uses voting behavior to aggregate an ensemble and has a game-theoretic intuition [1, 15].",
      "startOffset" : 184,
      "endOffset" : 191
    }, {
      "referenceID" : 11,
      "context" : "There is some work on semi-supervised versions of boosting [16], but it departs from this principled structure and has little in common with our approach.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "Classical boosting algorithms like AdaBoost [17] make no attempt to use unlabeled data.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "It is possible to make this footprint independent of d as well by hashing features [13], not done here.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "Dataset # training HEDGECLIPPER Random Forest Base RF AdaBoost Trees MART [14] Logistic Regression",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "Popular variants on supervised learning such as the transductive SVM [18] and graph-based or nearest-neighbor algorithms, which dominate the semi-supervised literature [8], have shown promise largely in data-poor regimes because they face major scalability challenges.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "Popular variants on supervised learning such as the transductive SVM [18] and graph-based or nearest-neighbor algorithms, which dominate the semi-supervised literature [8], have shown promise largely in data-poor regimes because they face major scalability challenges.",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 14,
      "context" : "Largely unsupervised ensemble methods have been explored especially in applications like crowdsourcing, in which the method of [19] gave rise to a plethora of Bayesian methods under various conditional independence generative assumptions on F [20].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 15,
      "context" : "Largely unsupervised ensemble methods have been explored especially in applications like crowdsourcing, in which the method of [19] gave rise to a plethora of Bayesian methods under various conditional independence generative assumptions on F [20].",
      "startOffset" : 243,
      "endOffset" : 247
    }, {
      "referenceID" : 16,
      "context" : "Using tree structure to construct new features has been applied successfully, though without guarantees [21].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "[22].",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2015,
    "abstractText" : "We present and empirically evaluate an efficient algorithm that learns to aggregate the predictions of an ensemble of binary classifiers. The algorithm uses the structure of the ensemble predictions on unlabeled data to yield significant performance improvements. It does this without making assumptions on the structure or origin of the ensemble, without parameters, and as scalably as linear learning. We empirically demonstrate these performance gains with random forests.",
    "creator" : "LaTeX with hyperref package"
  }
}