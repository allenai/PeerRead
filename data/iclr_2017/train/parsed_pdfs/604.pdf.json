{
  "name" : "604.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "HERE’S MY POINT: ARGUMENTATION MINING WITH POINTER NETWORKS",
    "authors" : [ "Peter Potash", "Alexey Romanov" ],
    "emails" : [ "ppotash@cs.uml.edu", "aromanov@cs.uml.edu", "arum@cs.uml.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Computational approaches to argument mining/understanding have become very popular (Persing & Ng, 2016; Cano-Basave & He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau & Moens, 2009; Habernal & Gurevych, 2016). One important avenue in this work is to understand the structure in argumentative text (Persing & Ng, 2016; Peldszus & Stede, 2015; Stab & Gurevych, 2016; Nguyen & Litman, 2016). One fundamental assumption when working with argumentative text is the presence of Arguments Components (ACs). The types of ACs are generally characterized as a claim or a premise (Govier, 2013), with premises acting as support (or possibly attack) units for claims. To model more complex structures of arguments, some annotation schemes also include a major claim AC type (Stab & Gurevych, 2016; 2014b).\nGenerally, the task of processing argument structure encapsulates four distinct subtasks: 1) Given a sequence of tokens that represents an entire argumentative text, determine the token subsequences that constitute non-intersecting ACs; 2) Given an AC, determine the type of AC (claim, premise, etc.); 3) Given a set/list of ACs, determine which ACs have a link that determine overall argument structure; 4) Given two linked ACs, determine whether the link is of a supporting or attacking relation. In this work, we focus on subtasks 2 and 3.\nThere are two key assumptions our work makes going forward. First, we assume subtask 1 has been completed, i.e. ACs have already been identified. Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau & Moens, 2009; Cohen, 1987; Peldszus & Stede, 2015; Stab & Gurevych, 2016) Specifically, a given AC can only have a single outgoing link, but can have numerous incoming links. Furthermore, there is a ‘head’ component that has\nFirst, [ :::::: cloning :::: will ::: be ::::::::: beneficial :::: for :::: many :::::: people ::::: who ::: are :: in :::: need ::: of ::::: organ ::::::::: transplants]AC1. In addition, [it shortens the healing process]AC2. Usually, [it is very rare to find an appropriate organ donor]AC3 and [by using cloning in order to raise required organs the waiting time can be shortened tremendously]AC4.\nAC1 Claim\nAC2 Premise\nAC4 Premise AC3 Premise\nFigure 1: An example of argument structure with four ACs. The left side shows raw text that has been annotated for the presence of ACs. Squiggly and straight underlining means an AC is a claim or premise, respectively. The ACs in the text have also been annotated for links to other ACs, which is show in the right figure. ACs 3 and 4 are premises that link to another premise, AC2. Finally, AC2 links to a claim, AC1. AC1 therefore acts as the central argumentative component.\nno outgoing link (the top of the tree). Figure 1 shows an example that we will use throughout the paper to concretely explain how our approach works. First, the left side of the figure presents the raw text of a paragraph in a persuasive essay (Stab & Gurevych, 2016), with the ACs contained in square brackets. Squiggly verse straight underlining differentiates between claims and premises, respectively. The ACs have been annotated as to how the ACs are linked, and the right side of the figure reflects this structure. The argument structure with four ACs forms a tree, where AC2 has two incoming links, and AC1 acts as the head, with no outgoing links. We also specify the type of AC, with the head AC marked as claim and the remaining ACs marked as premise. Lastly, we note that the order of arguments components can be a strong indicator of how components should related. Linking to the first argument component can provide a competitive baseline heuristic (Peldszus & Stede, 2015; Stab & Gurevych, 2016).\nGiven the task at hand, we propose a modification of a Pointer Network (PN) (Vinyals et al., 2015b). A PN is a sequence-to-sequence model that outputs a distribution over the encoding indices at each decoding timestep. The PN is a promising model for link extraction in argumentative text because it inherently possesses three important characteristics: 1) it is able to model the sequential nature of ACs; 2) it constrains ACs to have a single outgoing link, thus partly enforcing the tree structure; 3) the hidden representations learned by the model can be used for jointly predicting multiple subtasks. We also note that since a PN is a type of sequence-to-sequence model (Sutskever et al., 2014), it allows the entire sequence to be seen before making prediction. This is important because if the problem were to be approached as standard sequence modeling (Graves & Schmidhuber, 2009; Robinson, 1994), making predictions at each forward timestep, it would only allow links to ACs that have already been seen. This is equivalent to only allowing backward links. We note that we do test a simplified model that only uses hidden states from an encoding network to make predictions, as opposed to the sequence-to-sequence architecture present in the PN (see Section 5).\nPNs were originally proposed to allow a variable length decoding sequence (Vinyals et al., 2015b). Alternatively, the PN we implement differs from the original model in that we decode for the same number of timesteps as there are input components. We also propose a joint PN for both extracting links between ACs and predicting the type of AC. The model uses the hidden representation of ACs produced during the encoding step (see Section 3.4). Aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the AC types or connectivity, unlike the work of Peldszus (2014). We evaluate our models on the corpora of Stab & Gurevych (2016) and Peldszus (2014), and compare our results with the results of the aformentioned authors."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Recent work in argumentation mining offers data-driven approaches for the task of predicting links between ACs. Stab & Gurevych (2014b) approach the task as a binary classification problem. The\nauthors train an SVM with various semantic and structural features. Peldszus & Stede (2015) have also used classification models for predicting the presence of links. Various authors have also proposed to jointly model link extraction with other subtasks from the argumentation mining pipeline, using either an Integer Linear Programming (ILP) framework (Persing & Ng, 2016; Stab & Gurevych, 2016) or directly feeding previous subtask predictions into another model. The former joint approaches are evaluated on annotated corpora of persuasive essays (Stab & Gurevych, 2014a; 2016), and the latter on a corpus of microtexts (Peldszus, 2014). The ILP framework is effective in enforcing a tree structure between ACs when predictions are made from otherwise naive base classifiers.\nUnrelated to argumentation mining specifically, recurrent neural networks have previously been proposed to model tree/graph structures in a linear manner. Vinyals et al. (2015c) use a sequenceto-sequence model for the task of syntactic parsing. The authors linearize input parse graphs using a depth-first search, allowing it to be consumed as a sequence, achieving state-of-the-art results on several syntactic parsing datasets. Bowman et al. (2015) experiment on an artificial entailment dataset that is specifically engineered to capture recursive logic (Bowman et al., 2014). The text is annotated with brackets, in an original attempt to provide easy input into a recursive neural network. However, standard recurrent neural networks can take in complete sentence sequences, brackets included, and perform competitively with a recursive neural network."
    }, {
      "heading" : "3 POINTER NETWORK FOR LINK EXTRACTION",
      "text" : "In this section we will describe how we use a PN for the problem of extracting links between ACs. We begin by giving a general description of the PN model."
    }, {
      "heading" : "3.1 POINTER NETWORK",
      "text" : "A PN is a sequence-to-sequence model (Sutskever et al., 2014) with attention (Bahdanau et al., 2014) that was proposed to handle decoding sequences over the encoding inputs, and can be extended to arbitrary sets (Vinyals et al., 2015a). The original motivation for a pointer network was to allow networks to learn solutions to algorithmic problems, such as the traveling salesperson and convex hull, where the solution is a sequence over candidate points. The PN model is trained on input/output sequence pairs (E,D), where E is the source and D is the target (our choice of E,D is meant to represent the encoding, decoding steps of the sequence-to-sequence model). Given model parameters Θ, we apply the chain rule to determine the probability of a single training example:\np(D|E; Θ) = m(E)∏ i=1 p(Di|D1, ..., Di−1, E; Θ) (1)\nwhere the functionm signifies that the number of decoding timesteps is a function of each individual training example. We will discuss shortly why we need to modify the original definition of m for our application. By taking the log-likelihood of Equation 1, we arrive at the optimization objective:\nΘ∗ = arg max Θ ∑ E,D log p(D|E; Θ) (2)\nwhich is the sum over all training example pairs.\nThe PN uses Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) for sequential modeling, which produces a hidden layer h at each encoding/decoding timestep. In practice, the PN has two separate LSTMs, one for encoding and one for decoding. Thus, we refer to encoding hidden layers as e, and decoding hidden layers as d.\nThe PN uses a form of content-based attention (Bahdanau et al., 2014) to allow the model to produce a distribution over input elements. This can also be thought of as a distribution over input indices, wherein a decoding step ‘points’ to the input. Formally, given encoding hidden states (e1, ..., en), The model calculates p(Di|D1, ..., Di−1, E) as follows:\nuij = v T tanh(W1ej +W2di) (3)\np(Di|D1, ..., Dj−1, E) = softmax(ui) (4) where matrices W1, W2 and vector v are parameters of the model (along with the LSTM parameters used for encoding and decoding). In Equation 3, prior to taking the dot product with v, the resulting transformation can be thought of as creating a joint, hidden representation of inputs i and j. Vector ui in equation 4 is of length n, and index j corresponds to input element j. Therefore, by taking the softmax of ui, we are able to create a distribution over the input."
    }, {
      "heading" : "3.2 LINK EXTRACTION AS SEQUENCE MODELING",
      "text" : "A given piece of text has a set of ACs, which occur in a specific order in the text, (C1, ..., Cn). Therefore, at encoding timestep i, the model is fed a representation of Ci. Since the representation is large and sparse (see Section 3.3 for details on how we represent ACs), we add a fully-connected layer before the LSTM input. Given a representation Ri for AC Ci the LSTM input Ai becomes:\nAi = σ(WrepRi + brep) (5)\nwhere Wrep, brep in turn become model parameters, and σ is the sigmoid function1. (similarly, the decoding network applies a fully-connected layer with sigmoid activation to its inputs, see Figure 3). At encoding step i, the encoding LSTM produces hidden layer ei, which can be thought of as a hidden representation of AC Ci.\nIn order to make the PN applicable to the problem of link extraction, we explicitly set the number of decoding timesteps to be equal to the number of input components. Using notation from Equation 1, the decoding sequence length for an encoding sequence E is simply m(E) = |{C1, ..., Cn}|, which is trivially equal to n. By constructing the decoding sequence in this manner, we can associate decoding timestep i with AC Ci.\nFrom Equation 4, decoding timestep Di will output a distribution over input indices. The result of this distribution will indicate to which AC component Ci links. Recall there is a possibility that an AC has no outgoing link, such as if it’s the root of the tree. In this case, we state that if AC Ci does not have an outgoing link, decoding step Di will output index i. Conversely, if Di outputs index j, such that j is not equal to i, this implies that Ci has an outgoing link to Cj . For the argument structure in Figure 1, the corresponding decoding sequence is (1, 1, 2, 2). The topology of this decoding sequence is illustrated in Figure 2. Note how C1 points to itself since it has no outgoing link.\nFinally, we note that we modify the PN structure to have a Bidirectional LSTM as the encoder. Thus, ei is the concatenation of forward and backward hidden states −→e i and ←−e n−i+1, produced by two separate LSTMs. The decoder remains a standard forward LSTM."
    }, {
      "heading" : "3.3 REPRESENTING ARGUMENT COMPONENTS",
      "text" : "At each timestep of the decoder, the network takes in the representation of an AC. Each AC is itself a sequence of tokens, similar to the recently proposed Question-Answering dataset (Weston et al., 2015). We follow the work of Stab & Gurevych (2016) and focus on three different types of features\n1We also experimented with relu and elu activations, but found sigmoid to yeild the best performance.\nto represent our ACs: 1) Bag-of-Words of the AC; 2) Embedding representation based on GloVe embeddings (Pennington et al., 2014); 3) Structural features: Whether or not the AC is the first AC in a paragraph, and Whether the AC is in an opening, body, or closing paragraph. See Section 6 for an ablation study of the proposed features."
    }, {
      "heading" : "3.4 JOINT NEURAL MODEL",
      "text" : "Up to this point, we focused on the task of extracting links between ACs. However, recent work has shown that joint models that simultaneously try to complete multiple aspects of the subtask pipeline outperform models that focus on a single subtask (Persing & Ng, 2016; Stab & Gurevych, 2014b; Peldszus & Stede, 2015). Therefore, we will modify the architecture we proposed in Section 3 so that it would allow us to perform AC classification (Kwon et al., 2007; Rooney et al., 2012) together with link prediction. Knowledge of an individual subtask’s predictions can aid in other subtasks. For example, claims do not have an outgoing link, so knowing the type of AC can aid in the link prediction task. This can be seen as a way of regularizing the hidden representations from the encoding component (Che et al., 2015).\nPredicting AC type is a straightforward classification task: given AC Ci, we need to predict whether it is a claim or premise. Some annotation schemes also include the class major claim (Stab & Gurevych, 2014a), which means this can be a multi-class classification task. For encoding timestep i, the model creates hidden representation ei. This can be thought of as a representation of AC Ci. Therefore, our joint model will simply pass this representation through a fully connected layer as follows: zi = Wclsei + bcls (6) where Wcls, bcls become elements of the model parameters, Θ. The dimensionality of Wcls, bcls is determined by the number of classes. Lastly, we use softmax to form a distribution over the possible classes.\nConsequently, the probability of predicting component type at timestep i is defined as:\np(Ci) = p(Ei| −→ E i, ←− E i; Θ) (7)\np(Ei| −→ E i, ←− E i; Θ) = softmax(zi) (8)\nFinally, combining this new prediction task with Equation 2, we arrive at the new training objective:\nΘ∗ = arg max Θ α ∑ E,D log p(D|E; Θ) + (1− α) ∑ E log p(E|Θ) (9)\nwhich simply sums the costs of the individual prediction tasks, and the second summation is the cost for the new task of predicting argument component type. α ∈ [0, 1] is a hyperparameter that\nspecifies how we weight the two prediction tasks in our cost function. The architecture of the joint model, applied to our ongoing example, is illustrated in Figure 3."
    }, {
      "heading" : "4 EXPERIMENTAL DESIGN",
      "text" : "As we have previously mentioned, our work assumes that ACs have already been identified. That is, the token sequence that comprises a given AC is already known. The order of ACs corresponds directly to the order in which the ACs appear in the text. Since ACs are non-overlapping, there is no ambiguity in this ordering. We test the effectiveness of our proposed model on a dataset of persuasive essays (Stab & Gurevych, 2016), as well as a dataset of microtexts (Peldszus, 2014). The feature space for the persuasive essay corpus has roughly 3,000 dimensions, and the microtext corpus feature space has between 2,500 and 3,000 dimensions, depending on the data split (see below).\nThe persuasive essay corpus contains a total of 402 essays, with a frozen set of 80 essays held out for testing. There are three AC types in this corpus: major claim, claim, and premise. We follow the creators of the corpus and only evaluate ACs within a given paragraph. That is, each training/test example is a sequence of ACs from a paragraph. This results in a 1,405/144 training/test split. The microtext corpus contains 112 short texts. Unlike, the persuasive essay corpus, each text in this corpus is itself a complete example. Since the dataset is small, the authors have created 10 sets of 5-fold cross-validation, reporting the the average across all splits for final model evaluation. This corpus contains only two types of ACs (claim and premise) The annotation of argument structure of the microtext corpus varies from the persuasive essay corpus; ACs can be linked to other links, as opposed to ACs. Therefore, if AC Ci is annotated to be linked to link l, we create a link to the source AC of l. On average, this corpus has 5.14 ACs per text. Lastly, we note that predicting the presence of links is directional (ordered): predicting a link between the pair Ci, Cj(i 6= j) is different than Cj , Ci.\nWe implement our models in TensorFlow (Abadi et al., 2015). Our model has the following parameters: hidden input dimension size 512, hidden layer size 256 for the bidirectional LSTMs, hidden layer size 512 for the LSTM decoder, α equal to 0.5, and dropout (Srivastava et al., 2014) of 0.9. We believe the need for such high dropout is due to the small amounts of training data (Zarrella & Marsh, 2016), particularly in the Microtext corpus. All models are trained with Adam optimizer (Kingma & Ba, 2014) with a batch size of 16. For a given training set, we randomly select 10% to become the validation set. Training occurs for 4,000 epochs. Once training is completed, we select the model with the highest validation accuracy (on the link prediction task) and evaluate it on the held-out test set. At test time, we take a greedy approach and select the index of the probability distribution (whether link or type prediction) with the highest value."
    }, {
      "heading" : "5 RESULTS",
      "text" : "The results of our experiments are presented in Tables 1 and 2. For each corpus, we present f1 scores for the AC type classification experiment, with a macro-averaged score of the individual class f1 scores. We also present the f1 scores for predicting the presence/absence of links between ACs, as well as the associated macro-average between these two values.\nWe implement and compare four types of neural models: 1) The previously described PN-based model depicted in Figure 3 (called PN in the tables); 2) The same as 1), but without the fullyconnected input layers; 3) The same as 1), but the model only predicts the link task, and is therefore not optimized for type prediction; 4) A non-sequence-to-sequence model that uses the hidden layers produced by the BLSTM encoder with the same type of attention as the PN (called BLSTM in the table). That is, di in Equation 3 is replaced by ei.\nIn both corpora we compare against the following previously proposed models: Base Classifier (Stab & Gurevych, 2016) is feature-rich, task-specific (AC type or link extraction) SVM classifier. Neither of these classifiers enforce structural or global constraints. Conversely, the ILP Joint Model (Stab & Gurevych, 2016) provides constrains by sharing prediction information between the base classifier. For example, the model attempts to enforce a tree structure among ACs within a given paragraph, as well as using incoming link predictions to better predict the type class claim. For the\nmicrotext corpus only, we have the following comparative models: Simple (Peldszus & Stede, 2015) is a feature-rich logistic regression classifier. Best EG (Peldszus & Stede, 2015) creates an Evidence Graph (EG) from the predictions of a set of base classifier. The EG models the potential argument structure, and offers a global optimization objective that the base classifiers attempt to optimize by adjusting their individual weights. Lastly, MP+p (Peldszus & Stede, 2015) combines predictions from base classifiers with a MSTParser, which applies 1-best MIRA structured learning."
    }, {
      "heading" : "6 DISCUSSION",
      "text" : "First, we point out that the PN model achieves state-of-the-art on 10 of the 13 metrics in Tables 1 and 2, including the highest results in all metrics on the Persuasive Essay corpus, as well as link prediction on the Microtext corpus. The performance on the Microtext corpus is very encouraging for several reasons. First, the fact that the model can perform so well with only a hundred training examples is rather remarkable. Second, although we motivate the use of a PN due to the fact that it partially enforces the tree structure in argumentation, other models explicitly contain further constraints. For example, only premises can have outgoing links, and there can be only one claim in an AC. As for the other neural models, the BLSTM model performs competitively with the ILP Joint Model on the persuasive essay corpus, but trails the performance of the PN model. We believe this is because the PN model is able to create two different representations for each AC, one each in the encoding/decoding state, which benefits performance in the dual tasks, whereas the BLSTM model must encode information relating to type as well as link prediction in a single hidden representation. On one hand, the BLSTM model outperforms the ILP model on link prediction, yet it is not able to match the ILP Joint Model’s performance on type prediction, primarily due to the BLSTM’s poor performance on predicting the major claim class. Another interesting outcome is the importance of the fully-connected layer before the LSTM input. The results show that this extra layer of depth is crucial for good performance on this task. Without it, the PN model is only able to perform competitively with the Base Classifier. The results dictate that even a simple fully-connected layer with sigmoid activation can provide a useful dimensionality reduction for feature representation. Finally, the PN model that only extracts links suffers a large drop in performance, conveying that the joint aspect of the PN model is crucial for high performance in the link prediction task.\nTable 3 shows the results of an ablation study for AC feature representation. Regarding link prediction, BOW features are clearly the most important, as their absence results in the highest drop in performance. Conversely, the presence of structural features provides the smallest boost in performance, as the model is still able to record state-of-the-art results compared to the ILP Joint Model. This shows that, one one hand, the PN model is able to capture structural ques through sequence\nmodeling and semantics (the ILP Joint Model directly integrates these structural features), however the PN model still does benefit from their explicit presence in the feature representation. When considering type prediction, both BOW and structural features are important, and it is the embedding features that provide the least benefit. The Ablation results also provide an interesting insight into the effectiveness of different ‘pooling’ strategies for using individual token embeddings to create a multi-word embedding. The popular method of averaging embeddings (which is used by Stab & Gurevych (2016) in their system) is in fact the worst method, although its performance is still competitive with the previous state-of-the-art. Conversely, max pooling produces results that are on par with the PN results from Table 1.\nTable 4 shows the results on the Persuasive Essay test set with the examples binned by sequence length. First, it is not a surprise to see that the model performs best when the sequences are the shortest. As the sequence length increases, the accuracy on link prediction drops. This is possibly due to the fact that as the length increases, a given AC has more possibilities as to which other AC it can link to, making the task more difficult. Conversely, there is actually a rise in no link prediction accuracy from the second to third row. This is likely due to the fact that since the model predicts at most one outgoing link, it indirectly predicts no link for the remaining ACs in the sequence. Since the chance probability is low for having a link between a given AC in a long sequence, the no link performance is actually better in longer sequences."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "In this paper we have proposed how to use a modified PN (Vinyals et al., 2015b) to extract links between ACs in argumentative text. We evaluate our models on two corpora: a corpus of persuasive essays (Stab & Gurevych, 2016), and a corpus of microtexts (Peldszus, 2014). The PN model records state-of-the-art results on the persuasive essay corpus, as well as achieving state-of-the-art results for link prediction on the microtext corpus, despite only having 90 training examples. The results show that jointly modeling the two prediction tasks is crucial for high performance, as well as the presence of a fully-connected layer prior to the LSTM input. Future work can attempt to learn the AC representations themselves, such as in Kumar et al. (2015). Lastly, future work can integrate subtasks 1 and 4 into the model. The representations produced by Equation 3 could potentially be used to predict the type of link connecting ACs, i.e. supporting or attacking; this is the fourth subtask in the pipeline. In addition, a segmenting technique, such as the one proposed by Weston et al. (2014), can accomplish subtask 1."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "cent Vanhoucke", "Vijay Vasudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "Vanhoucke et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vanhoucke et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Recursive neural networks can learn logical semantics",
      "author" : [ "Samuel R Bowman", "Christopher Potts", "Christopher D Manning" ],
      "venue" : "arXiv preprint arXiv:1406.1827,",
      "citeRegEx" : "Bowman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2014
    }, {
      "title" : "Tree-structured composition in neural networks without tree-structured architectures",
      "author" : [ "Samuel R Bowman", "Christopher D Manning", "Christopher Potts" ],
      "venue" : "arXiv preprint arXiv:1506.04834,",
      "citeRegEx" : "Bowman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "A study of the impact of persuasive argumentation in political debates",
      "author" : [ "Amparo Elizabeth Cano-Basave", "Yulan He" ],
      "venue" : "In Proceedings of NAACL-HLT,",
      "citeRegEx" : "Cano.Basave and He.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cano.Basave and He.",
      "year" : 2016
    }, {
      "title" : "Deep computational phenotyping",
      "author" : [ "Zhengping Che", "David Kale", "Wenzhe Li", "Mohammad Taha Bahadori", "Yan Liu" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Che et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Che et al\\.",
      "year" : 2015
    }, {
      "title" : "Analyzing the structure of argumentative discourse",
      "author" : [ "Robin Cohen" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "Cohen.,? \\Q1987\\E",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1987
    }, {
      "title" : "Coarse-grained argumentation features for scoring persuasive essays",
      "author" : [ "Debanjan Ghosh", "Aquila Khanam", "Yubo Han", "Smaranda Muresan" ],
      "venue" : "In The 54th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Ghosh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ghosh et al\\.",
      "year" : 2016
    }, {
      "title" : "Offline handwriting recognition with multidimensional recurrent neural networks. In Advances in neural information processing",
      "author" : [ "Alex Graves", "Jürgen Schmidhuber" ],
      "venue" : null,
      "citeRegEx" : "Graves and Schmidhuber.,? \\Q2009\\E",
      "shortCiteRegEx" : "Graves and Schmidhuber.",
      "year" : 2009
    }, {
      "title" : "Which argument is more convincing? analyzing and predicting convincingness of web arguments using bidirectional lstm",
      "author" : [ "Ivan Habernal", "Iryna Gurevych" ],
      "venue" : "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Habernal and Gurevych.,? \\Q2016\\E",
      "shortCiteRegEx" : "Habernal and Gurevych.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Ask me anything: Dynamic memory networks for natural language processing",
      "author" : [ "Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher" ],
      "venue" : "arXiv preprint arXiv:1506.07285,",
      "citeRegEx" : "Kumar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2015
    }, {
      "title" : "Identifying and classifying subjective claims",
      "author" : [ "Namhee Kwon", "Liang Zhou", "Eduard Hovy", "Stuart W Shulman" ],
      "venue" : "In Proceedings of the 8th annual international conference on Digital government research: bridging disciplines & domains,",
      "citeRegEx" : "Kwon et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Kwon et al\\.",
      "year" : 2007
    }, {
      "title" : "Context-aware argumentative relation mining",
      "author" : [ "Huy V Nguyen", "Diane J Litman" ],
      "venue" : null,
      "citeRegEx" : "Nguyen and Litman.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nguyen and Litman.",
      "year" : 2016
    }, {
      "title" : "Argumentation mining: the detection, classification and structure of arguments in text",
      "author" : [ "Raquel Mochales Palau", "Marie-Francine Moens" ],
      "venue" : "In Proceedings of the 12th international conference on artificial intelligence and law,",
      "citeRegEx" : "Palau and Moens.,? \\Q2009\\E",
      "shortCiteRegEx" : "Palau and Moens.",
      "year" : 2009
    }, {
      "title" : "Towards segment-based recognition of argumentation structure in short texts",
      "author" : [ "Andreas Peldszus" ],
      "venue" : "ACL",
      "citeRegEx" : "Peldszus.,? \\Q2014\\E",
      "shortCiteRegEx" : "Peldszus.",
      "year" : 2014
    }, {
      "title" : "Joint prediction in mst-style discourse parsing for argumentation mining",
      "author" : [ "Andreas Peldszus", "Manfred Stede" ],
      "venue" : "In Proc. of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Peldszus and Stede.,? \\Q2015\\E",
      "shortCiteRegEx" : "Peldszus and Stede.",
      "year" : 2015
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "End-to-end argumentation mining in student essays",
      "author" : [ "Isaac Persing", "Vincent Ng" ],
      "venue" : "In Proceedings of NAACL-HLT,",
      "citeRegEx" : "Persing and Ng.,? \\Q2016\\E",
      "shortCiteRegEx" : "Persing and Ng.",
      "year" : 2016
    }, {
      "title" : "An application of recurrent nets to phone probability estimation",
      "author" : [ "Anthony J Robinson" ],
      "venue" : "IEEE transactions on Neural Networks,",
      "citeRegEx" : "Robinson.,? \\Q1994\\E",
      "shortCiteRegEx" : "Robinson.",
      "year" : 1994
    }, {
      "title" : "Applying kernel methods to argumentation mining",
      "author" : [ "Niall Rooney", "Hui Wang", "Fiona Browne" ],
      "venue" : "In FLAIRS Conference,",
      "citeRegEx" : "Rooney et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rooney et al\\.",
      "year" : 2012
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q1929\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 1929
    }, {
      "title" : "Annotating argument components and relations in persuasive essays",
      "author" : [ "Christian Stab", "Iryna Gurevych" ],
      "venue" : "In COLING, pp",
      "citeRegEx" : "Stab and Gurevych.,? \\Q2014\\E",
      "shortCiteRegEx" : "Stab and Gurevych.",
      "year" : 2014
    }, {
      "title" : "Identifying argumentative discourse structures in persuasive essays",
      "author" : [ "Christian Stab", "Iryna Gurevych" ],
      "venue" : "In EMNLP, pp",
      "citeRegEx" : "Stab and Gurevych.,? \\Q2014\\E",
      "shortCiteRegEx" : "Stab and Gurevych.",
      "year" : 2014
    }, {
      "title" : "Parsing argumentation structures in persuasive essays",
      "author" : [ "Christian Stab", "Iryna Gurevych" ],
      "venue" : "arXiv preprint arXiv:1604.07370,",
      "citeRegEx" : "Stab and Gurevych.,? \\Q2016\\E",
      "shortCiteRegEx" : "Stab and Gurevych.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Order matters: Sequence to sequence for sets",
      "author" : [ "Oriol Vinyals", "Samy Bengio", "Manjunath Kudlur" ],
      "venue" : "arXiv preprint arXiv:1511.06391,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Łukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Is this post persuasive? ranking argumentative comments in the online forum",
      "author" : [ "Zhongyu Wei", "Yang Liu", "Yi Li" ],
      "venue" : "In The 54th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Wei et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards ai-complete question answering: A set of prerequisite toy tasks",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merriënboer", "Armand Joulin", "Tomas Mikolov" ],
      "venue" : "arXiv preprint arXiv:1502.05698,",
      "citeRegEx" : "Weston et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "Mitre at semeval-2016 task 6: Transfer learning for stance detection",
      "author" : [ "Guido Zarrella", "Amy Marsh" ],
      "venue" : "arXiv preprint arXiv:1606.03784,",
      "citeRegEx" : "Zarrella and Marsh.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zarrella and Marsh.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Computational approaches to argument mining/understanding have become very popular (Persing & Ng, 2016; Cano-Basave & He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau & Moens, 2009; Habernal & Gurevych, 2016).",
      "startOffset" : 83,
      "endOffset" : 213
    }, {
      "referenceID" : 7,
      "context" : "Computational approaches to argument mining/understanding have become very popular (Persing & Ng, 2016; Cano-Basave & He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau & Moens, 2009; Habernal & Gurevych, 2016).",
      "startOffset" : 83,
      "endOffset" : 213
    }, {
      "referenceID" : 6,
      "context" : "Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau & Moens, 2009; Cohen, 1987; Peldszus & Stede, 2015; Stab & Gurevych, 2016) Specifically, a given AC can only have a single outgoing link, but can have numerous incoming links.",
      "startOffset" : 85,
      "endOffset" : 166
    }, {
      "referenceID" : 26,
      "context" : "We also note that since a PN is a type of sequence-to-sequence model (Sutskever et al., 2014), it allows the entire sequence to be seen before making prediction.",
      "startOffset" : 69,
      "endOffset" : 93
    }, {
      "referenceID" : 20,
      "context" : "This is important because if the problem were to be approached as standard sequence modeling (Graves & Schmidhuber, 2009; Robinson, 1994), making predictions at each forward timestep, it would only allow links to ACs that have already been seen.",
      "startOffset" : 93,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : "Aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the AC types or connectivity, unlike the work of Peldszus (2014). We evaluate our models on the corpora of Stab & Gurevych (2016) and Peldszus (2014), and compare our results with the results of the aformentioned authors.",
      "startOffset" : 184,
      "endOffset" : 200
    }, {
      "referenceID" : 16,
      "context" : "Aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the AC types or connectivity, unlike the work of Peldszus (2014). We evaluate our models on the corpora of Stab & Gurevych (2016) and Peldszus (2014), and compare our results with the results of the aformentioned authors.",
      "startOffset" : 184,
      "endOffset" : 265
    }, {
      "referenceID" : 16,
      "context" : "Aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the AC types or connectivity, unlike the work of Peldszus (2014). We evaluate our models on the corpora of Stab & Gurevych (2016) and Peldszus (2014), and compare our results with the results of the aformentioned authors.",
      "startOffset" : 184,
      "endOffset" : 285
    }, {
      "referenceID" : 16,
      "context" : "The former joint approaches are evaluated on annotated corpora of persuasive essays (Stab & Gurevych, 2014a; 2016), and the latter on a corpus of microtexts (Peldszus, 2014).",
      "startOffset" : 157,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : "(2015) experiment on an artificial entailment dataset that is specifically engineered to capture recursive logic (Bowman et al., 2014).",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "Peldszus & Stede (2015) have also used classification models for predicting the presence of links.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 14,
      "context" : "Peldszus & Stede (2015) have also used classification models for predicting the presence of links. Various authors have also proposed to jointly model link extraction with other subtasks from the argumentation mining pipeline, using either an Integer Linear Programming (ILP) framework (Persing & Ng, 2016; Stab & Gurevych, 2016) or directly feeding previous subtask predictions into another model. The former joint approaches are evaluated on annotated corpora of persuasive essays (Stab & Gurevych, 2014a; 2016), and the latter on a corpus of microtexts (Peldszus, 2014). The ILP framework is effective in enforcing a tree structure between ACs when predictions are made from otherwise naive base classifiers. Unrelated to argumentation mining specifically, recurrent neural networks have previously been proposed to model tree/graph structures in a linear manner. Vinyals et al. (2015c) use a sequenceto-sequence model for the task of syntactic parsing.",
      "startOffset" : 0,
      "endOffset" : 890
    }, {
      "referenceID" : 2,
      "context" : "Bowman et al. (2015) experiment on an artificial entailment dataset that is specifically engineered to capture recursive logic (Bowman et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 26,
      "context" : "A PN is a sequence-to-sequence model (Sutskever et al., 2014) with attention (Bahdanau et al.",
      "startOffset" : 37,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : ", 2014) with attention (Bahdanau et al., 2014) that was proposed to handle decoding sequences over the encoding inputs, and can be extended to arbitrary sets (Vinyals et al.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "The PN uses a form of content-based attention (Bahdanau et al., 2014) to allow the model to produce a distribution over input elements.",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 30,
      "context" : "Each AC is itself a sequence of tokens, similar to the recently proposed Question-Answering dataset (Weston et al., 2015).",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 30,
      "context" : "Each AC is itself a sequence of tokens, similar to the recently proposed Question-Answering dataset (Weston et al., 2015). We follow the work of Stab & Gurevych (2016) and focus on three different types of features",
      "startOffset" : 101,
      "endOffset" : 168
    }, {
      "referenceID" : 18,
      "context" : "to represent our ACs: 1) Bag-of-Words of the AC; 2) Embedding representation based on GloVe embeddings (Pennington et al., 2014); 3) Structural features: Whether or not the AC is the first AC in a paragraph, and Whether the AC is in an opening, body, or closing paragraph.",
      "startOffset" : 103,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "Therefore, we will modify the architecture we proposed in Section 3 so that it would allow us to perform AC classification (Kwon et al., 2007; Rooney et al., 2012) together with link prediction.",
      "startOffset" : 123,
      "endOffset" : 163
    }, {
      "referenceID" : 21,
      "context" : "Therefore, we will modify the architecture we proposed in Section 3 so that it would allow us to perform AC classification (Kwon et al., 2007; Rooney et al., 2012) together with link prediction.",
      "startOffset" : 123,
      "endOffset" : 163
    }, {
      "referenceID" : 5,
      "context" : "This can be seen as a way of regularizing the hidden representations from the encoding component (Che et al., 2015).",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 16,
      "context" : "We test the effectiveness of our proposed model on a dataset of persuasive essays (Stab & Gurevych, 2016), as well as a dataset of microtexts (Peldszus, 2014).",
      "startOffset" : 142,
      "endOffset" : 158
    }, {
      "referenceID" : 16,
      "context" : "We evaluate our models on two corpora: a corpus of persuasive essays (Stab & Gurevych, 2016), and a corpus of microtexts (Peldszus, 2014).",
      "startOffset" : 121,
      "endOffset" : 137
    }, {
      "referenceID" : 12,
      "context" : "Future work can attempt to learn the AC representations themselves, such as in Kumar et al. (2015). Lastly, future work can integrate subtasks 1 and 4 into the model.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "Future work can attempt to learn the AC representations themselves, such as in Kumar et al. (2015). Lastly, future work can integrate subtasks 1 and 4 into the model. The representations produced by Equation 3 could potentially be used to predict the type of link connecting ACs, i.e. supporting or attacking; this is the fourth subtask in the pipeline. In addition, a segmenting technique, such as the one proposed by Weston et al. (2014), can accomplish subtask 1.",
      "startOffset" : 79,
      "endOffset" : 440
    } ],
    "year" : 2016,
    "abstractText" : "One of the major goals in automated argumentation mining is to uncover the argument structure present in argumentative text. In order to determine this structure, one must understand how different individual components of the overall argument are linked. General consensus in this field dictates that the argument components form a hierarchy of persuasion, which manifests itself in a tree structure. This work provides the first neural network-based approach to argumentation mining, focusing on extracting links between argument components, with a secondary focus on classifying types of argument components. In order to solve this problem, we propose to use a modification of a Pointer Network architecture. A Pointer Network is appealing for this task for the following reasons: 1) It takes into account the sequential nature of argument components; 2) By construction, it enforces certain properties of the tree structure present in argument relations; 3) The hidden representations can be applied to auxiliary tasks. In order to extend the contribution of the original Pointer Network model, we construct a joint model that simultaneously attempts to learn the type of argument component, as well as continuing to predict links between argument components. The proposed model achieves state-of-the-art results on two separate evaluation corpora. Furthermore, our results show that optimizing for both tasks, as well as adding a fully-connected layer prior to recurrent neural network input, is crucial for high performance.",
    "creator" : "LaTeX with hyperref package"
  }
}