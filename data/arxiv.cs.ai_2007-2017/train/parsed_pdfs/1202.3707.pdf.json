{
  "name" : "1202.3707.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A temporally abstracted Viterbi algorithm",
    "authors" : [ "Shaunak Chatterjee" ],
    "emails" : [ "shaunakc@cs.berkeley.edu", "russell@cs.berkeley.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The Viterbi algorithm (Viterbi, 1967; Forney, 1973) finds the most likely sequence of hidden states, called the “Viterbi path,” conditioned on a sequence of observations in a hidden Markov model (HMM). If the HMM has N states and the sequence is of length T , there are NT possible state sequences, but, because it uses dynamic programming (DP), the Viterbi algorithm’s time complexity is just O(N2T ). It is one of the most important and basic algorithms in the entire field of information technology; its original application was in signal decoding but has since been used in numerous other applications including speech recognition (Rabiner, 1989), language parsing (Klein and Manning, 2003), and bioinformatics (Lytynoja and Milinkovitch, 2003).\nFinding a most-likely state sequence in an HMM is isomorphic to finding a minimum cost path through a state–time trellis graph (see Figure 1) whose link cost is the negative log probability of the corresponding transition–observation pair in the HMM. Thus, the\ncost of finding an optimal path can be reduced further using an admissible (lower bound) heuristic and A* graph search.\nEven with this improvement, the time and space cost can be prohibitive when N and T are very large; for example, with a state space defined by 30 Boolean variables, running Viterbi for a million time steps requires 1024 computations. One possible approach to handle such problems is to use a state abstraction: a mapping φ : S0 7→ S1 from the original state space S0 to a coarser state space S1. For stochastic models (such as an HMM), the parameters of the model in S1 are often chosen to be the maximum of the corresponding constituent parameters in S0. Although these parameters do not define a valid probability measure, they serve as admissible heuristics for an A* search. The same idea can be applied to produce a a hierarchy of abstractions S0, S1, . . . , SL. Coarse-to-fine dynamic programming or CFDP (Raphael, 2001) begins with SL and iteratively finds the shortest path in the current (abstracted) version of the graph and refines along it until the current shortest path is completely refined. Several algorithms—e.g., hierarchical A* (Holte et al., 1996) and HA*LD (Felzenszwalb and McAllester, 2007)—are able to refine only the necessary part of the hierarchy tree and compute heuristics only when needed.\nThe Viterbi algorithm devotes equal effort to every link in the state–time trellis. CFDP and its relatives can determine that an entire set of states need not be explored in detail, based on bounding the cost of paths through that set; but they do so separately for each time step. In this paper, we show how to use temporal abstraction to eliminate large sets of states from consideration over large periods of time.\nTo motivate our algorithm, consider the following problem. We observe Judy’s daily tweets describing what she eats for lunch, and wish to infer the city in which she is staying on each day. The state space S0 is the set of all cities in the world. The abstract space S1 is the set of countries, and S2 is the continents. (Figure 1 shows a small example.) The transition model\nT=1 T=2 T=3 T=4 T=5\nNew York\nChicago\nMontreal\nToronto\nLondon\nManchester\nMunich\nBerlin\nU.S.A.\nCanada\nEngland\nGermany\nEurope\nNorth America\nFigure 1: The state–time trellis for a small version of the tracking problem. The links have weights denoting probabilities of going from a city A to a city B in a day. The abstract state spaces S1 (countries depicted in green) and S2 (continents in yellow) are only shown for T=5 to maintain clarity. The observation links are also omitted for the same reason.\nsuggests that on any given day Judy is unlikely to leave the city she is in, even less likely to leave the country she is in, and very unlikely indeed to travel to another continent. Thus, if Judy had Tandoori chicken on a Thursday but the rest of the week was all hamburgers, then it is most likely that she was in some American city for the entire week. However, if she had Tandoori chicken and/or biryani for an entire week, then it is quite possible that she is in India. Our algorithm, temporally abstracted Viterbi (henceforth TAV), facilitates reasoning over a temporal interval (like a week or month or longer) and localized search within those intervals. Neither of these is possible with Viterbi or state abstraction algorithms like CFDP. The computational savings of TAV on an instance of this problem can be seen in Figure 2.\nTemporal abstractions have been well-studied in the context of planning (Sutton et al., 1999) and inference in dynamic Bayesian networks (Chatterjee and Russell, 2010). An excellent survey of temporal abstraction for dynamical systems can be found in (Pavliotis and Stuart, 2007). To the best of our knowledge, TAV is the first algorithm to use temporal abstraction for general shortest-path problems. TAV is guaranteed to find the Viterbi path, and does so (for certain problem instances) several orders of magnitude faster than the Viterbi algorithm and one to two orders of magnitude faster than CFDP.\nThe rest of the paper is organized as follows. Section 2 reviews the Viterbi algorithm and CFDP and introduces the notations and definitions used in the rest of the paper. Section 3 provides a detailed description of the main algorithm and establishes its correctness.\nSection 4 discusses the computation of temporal abstraction heuristics. Section 5 presents some empirical results to demonstrate the benefits of TAV while section 6 provides some guidance on how to induce abstraction hierarchies."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "Consider a hidden Markov model (HMM) whose latent Markovian state X is in one of N discrete states {1, 2, . . . , N}. Let the actual state at time t be denoted by Xt. The transition matrix A = {aij : i, j= 1, 2, . . . , N} defines the state transition probabilities where aij = p(Xt+1 = j |Xt = i). The Markov chain is assumed to be stationary, so aij is independent of t. Let the discrete observation space be the set {1, 2, . . . ,M}. Let Yt be the observation symbol at time t. The observation matrix B = {bik : i= 1, 2, . . . , N ; k= 1, 2, . . . ,M} defines the emission probabilities where bik = p(Yt = k |Xt = i). (We assume a discrete observation space in this paper, but our methods naturally extend to the continuous case.) The initial state distribution is given by Π = {π1, . . . , πN} where πi = p(X0 = i). The Viterbi path is the maximum likelihood sequence of latent states conditioned on the observation sequence. Following Rabiner (1989), we define\nδt(i) = max X0:t−1\np(X0:t−1, Xt = i, Y1:t |A,B,Π) ,\ni.e., the likelihood score of the optimal (most likely) sequence of hidden states (ending in state i) and the\nfirst t observations. By induction on t, we have:\nδt+1(j) = [max i δt(i)aij ]bjYt+1 .\nThe actual state sequence is retrieved by tracking the transitions that maximize the δ(.) scores for each t and j. This is done via an array of back pointers ψt(j). The complete procedure (Rabiner, 1989) is as follows:\n1. Initialization δ1(i) = πi biY1 , 1 ≤ i ≤ N ψ1(i) = 0 2. Recursion\nδt(j) = max1≤i≤N [δt−1(i)aij ]bjYt , 2 ≤ t ≤ T ψt(j) = arg max1≤i≤N [δt−1(i)aij ], 2 ≤ t ≤ T\n3. Termination\nP ∗ = max 1≤i≤N [δT (i)]\nX∗T = arg max 1≤i≤N [δT (i)]\n4. Path backtracking\nX∗t = ψt+1(X ∗ t+1), t=T − 1, T − 2, . . . , 1\nThe time complexity of this algorithm is O(N2T ) and the space complexity is O(N2 +NT ).\nIn coarse-to-fine (a.k.a. hierarchical) approaches, inference is performed in the coarser models to reduce the amount of computation needed in the finer models. Typically, a set of abstract state spaces S = {S0, S1, . . . , SL} and abstract models M= {M0,M1, . . . ,ML} are defined where S0 (M0) is the original state space (model) and SL (ML) is the coarsest abstract state space (model). Let the parameters of Ml be denoted by the set {Al, Bl,Πl}. A state in this hierarchy is denoted by sli, where l is the abstraction level and i is its index within level l. Nl denotes the number of states in level l. Let φ : Sl 7→ Sl+1 denote the mapping from any level l to its immediate abstract level l + 1. The parameters at level l + 1 are defined by taking the maximum of the component parameters at level l. Thus, Al+1 = {al+1ij }, where\nal+1ij = maxp,q alpq s.t. φ(s l p) = s l+1 i , φ(s l q) = s l+1 j .\nBl+1 and Πl+1 are defined similarly in terms of Bl and Πl. Any transition/emission probability in an abstract model is a tight upper bound on the corresponding probability in its immediate refinement. Hence, the cost (negative log probability) of an abstract trajectory can serve as an admissible heuristic to guide search in a more refined state space.\nCFDP works by starting with only the coarsest states sL1:NL at every time step from 1 to T . The states in t\nand t+1 are connected by transition links whose values are given by AL. BL and ΠL define the other starting parameters. It then iterates between computing the optimal path in the current trellis graph and refining the states (and thereby the associated links) along the current optimal path. The algorithm terminates when the current optimal path contains only completely refined states (i.e., states in S0). A graphical depiction of how CFDP works is shown in Figure 5. The TAV algorithm, which also has an iterative structure, is described in the next section. We will be reusing notation and definitions from this section throughout the rest of the paper."
    }, {
      "heading" : "3 Main algorithm",
      "text" : "The distinguishing feature of TAV is its ability to reason with temporally abstract links. A link in the Viterbi algorithm and CFDP-like approaches describes the transition probability between states over a single time step. A temporally abstract link starting in state s1 at time t1 and ending in state s2 at time t2 > t1 represents all trajectories having those end points and is denoted by a 4-tuple—((s1, t1), (s2, t2)). Links((s, t)) is the set of incoming links to state s at time t. Children(sl) = {s′ : s′ ∈ Sl−1, φ(s′) = sl} is the set of children of state sl in the abstraction hierarchy. We define three different kinds of temporally abstract links:\n1. Direct links: d(s, t1, t2) represents the set of trajectories that start at (s, t1) and end in (s, t2) and stay within s for the entire time interval (t1, t2).\n2. Cross links: c((s1, t1), (s2, t2)) represents the set of trajectories from (s1, t1) to (s2, t2), when s1 6= s2.\n3. Re–entry links: r((s, t1), (s, t2)) represents the set of trajectories that start at (s, t1) and end in (s, t2) but move outside s at least once in the time interval (t1, t2). r((s1, t1), (s1, t2)) = ∅ when t2 − t1 ≤ 1.\nThe direct and cross links are denoted graphically by straight lines, whereas the re-entry links are represented by curved lines as shown in Figure 3. A generic link is denoted by the symbol k. The (heuristic) score of a temporally abstract link has to be an upper bound on the probability of all trajectories in the set of trajectories it represents. Computing admissible and monotone heuristics will be discussed in Section 4.\nOur algorithm’s computational savings over spatial abstraction schemes come from two avenues—first, fewer time points to consider using temporal abstraction; second, fewer states to reason about by considering constrained trajectories using direct links. Although\nthe general flow of the algorithm is similar to CFDP, the refinement constructions are different. The algorithm descriptions provided omit details about observation matrix computations since they are standard. However, the issue is revisited in Section 4 to focus on some subtleties. The correctness of the algorithm depends only on the admissibility of the heuristics."
    }, {
      "heading" : "3.1 Refinement constructions",
      "text" : "A refinement of a temporally abstract link replaces the original link with a set of refined links that represent a partition—a mutually exclusive and exhaustive decomposition—of the set of trajectories represented by the original link. The refinement allows us to reason about subsets of the original set of trajectories separately and thereby potentially narrow down on a single optimal trajectory. There are two different kinds of refinement constructions."
    }, {
      "heading" : "3.1.1 Spatial refinement",
      "text" : "When a direct link, d(sl, t1, t2), lies on the optimal path, the natural thing to do is to refine (partition) the set of trajectories it represents. The original direct link is replaced with all possible cross, direct and re-entry links between Children(sl) at t1 and t2. This is depicted graphically in Figure 3. A link is also refined spatially if its time span (t2 − t1) is 1 time step since temporal refinement is not a possibility. The pseudocode for spatial refinement (see Algorithm 1) provides all the necessary details. It is trivial to show that the new links constitute a partition of the trajectories represented by the original link.\nAlgorithm 1 Spatial Refinement((p1, t1, p2, t2))\nC ← Children(p1);D ← Children(p2) if t2 − t1 > 1 then Links(p1, t2)← Links(p1, t2) \\ d(p1, t1, t2)\nelse 5: Links(p2, t2)← Links(p2, t2) \\ k((p1, t1), (p2, t2))\nend if usedStates(t1)← usedStates(t1) ∪ C usedStates(t2)← usedStates(t2) ∪D for all s ∈ D do\n10: Links(s, t2)← Links(s, t2) ∪ d(s, t1, t2) for all s′ ∈ C do\nif s = s′ then Links(s, t2)← Links(s, t2) ∪ r((s, t1), (s, t2))\nelse 15: Links(s, t2)← Links(s, t2) ∪ c((s′, t1), (s, t2))\nend if end for\nend for"
    }, {
      "heading" : "3.1.2 Temporal refinement",
      "text" : "When a cross link c((s1, t1), (s2, t2)) or a re-entry link r((s1, t1), (s1, t2)) is selected for refinement, we are faced with the task of refining a set of trajectories that do not stay in the same state for the abstraction interval. This is a case where temporal abstraction is not helping (not at the current resolution at least).\nAn example of temporal refinement, which is only invoked when t2 − t1 > 1, is shown in Figure 4. It results in splitting the time interval (t1, t2) into two sub-intervals that together span the original interval. Let us assume that we select the (rounded off) midpoint of the interval. When a link is temporally refined , we temporally split all cross, re-entry and direct links spanning the interval (t1, t2) between states in Children(φ(s1)). We will show later that for any link longer than 1 time step, φ(s1) =φ(s2). It should be noted that re-entry links are only added when the sub–interval length is longer than 1 time step. Also, if t2 − t1 = 1, then a cross link is spatially refined (analogous to CFDP).\nOne possibility is that some of the direct links for states in Children(φ(s1)) between (t1, t2) were already spatially refined. In that case, we apply temporal refinement recursively to the spatially refined links of those direct links. The choice of the splitting point does not affect the correctness of the algorithm as long as the split is replicated in the instantiated portion of the state space tree rooted at φ(s1). The pseudocode (shown in Algorithm 2) provides details of this procedure.\nLemma 3.1 The sets of trajectories represented by links before and after any spatial or temporal refinement are the same. Also, every trajectory is represented by exactly one temporally abstract path.\nLemma 3.2 Any link created by TAV will always be between two states at the same level of abstraction. If the time span of the link is greater than 1 time step, then those two states will also have the same parent at all coarser levels of abstraction.\nProof The original links are all between states of level L. Both refinement constructions add links only between states at the same abstraction level. This proves\nAlgorithm 2 Temporal Refinement((parent, t1, t2))\nnT ← d(t1 + t2)/2e if parent ∈ usedStates(nT ) then\nreturn end if\n5: usedT imes← usedT imes ∪ nT C ← Children(parent) usedStates(nT )← usedStates(nT ) ∪ C for all s ∈ C do\nif d(s, t1, t2) /∈ Links(s, t2) then 10: Temporal Refinement(s, t1, t2)\nLinks(s, t2)← ∅ Links(s, nT )← ∅\nelse Links(s, t2)← {d(s, nT, t2)}\n15: Links(s, nT )← {d(s, t1, nT )} end if for all s′ ∈ C do Links(s, t2)← Links(s, t2) \\ k((s′, t1), (s, t2)) Links(s, t2)← Links(s, t2) ∪ k((s′, nT ), (s, t2)) 20: Links(s, nT )← Links(s, nT ) ∪ k((s′, t1), (s, nT )) end for\nend for\nTemporal Refinement\nT2T1 T2T1 T\ns1 l\ns2 l\ns1 l\ns2 l\ns1 l\ns2 l\ns1 l\ns2 l\ns1 l\ns2 l\nFigure 4: Temporal refinement: When refining a cross or re-entry link, refine all links between nodes that have the same parent as the nodes of the selected link.\nthe first statement. Moreover, upon initialization, there is no coarser level of abstraction—hence the second part of the statement is vacuously true. Temporal refinement always considers links between descendants of the parent node. Spatial refinement also adds links between Children(sl) of a state s\nl except when the time step is 1. This proves the second statement."
    }, {
      "heading" : "3.2 Modified Viterbi algorithm",
      "text" : "It is possible to have links to a state s and to its abstraction φ(s) at the same time step t (see Figure 5(b)). This was not possible in CFDP. Hence, we need a slightly modified scoring and backtracking scheme. δt(s) is the best score of a trajectory ending in state s at time t and ψt(s) contains the temporally abstract link’s information which connects (s, t) to its predecessor. usedT imes is a sorted list of time steps which have links to or from it. usedStates(t) is the set of nodes at time t which have incoming or out-\nAlgorithm 3 BestPath(Links, usedStates, usedT imes)\ncurT ime← 1 while curT ime < T do curT ime← nextUsedT ime(curT ime, usedT imes) for all s ∈ UsedStates(curT ime) do\n5: δt(s)←MaxOverLinks(Links((s, curT ime)), δ) ψt(s)← ArgMaxOverLinks(Links((s, curT ime)), δ)\nend for for level = L− 1 to 0 do\nfor all s ∈ Slevel && s ∈ usedStates(curT ime) do 10: if δt(φ(s)) > δt(s) then\nδt(s)← δt(φ(s)) ψt(s)← ψt(φ(s))\nend if end for\n15: end for for level = 0 to L-1 do\nfor all s ∈ Slevel && s ∈ usedStates(curT ime) do if δt(φ(s)) ≤ δt(s) then δt(φ(s))← δt(s)\n20: ψt(φ(s))← ψt(s) end if\nend for end for\nend while 25: s∗ ← arg maxs∈usedStates(T ) δT (s)\ncurT ime← 1; Path← ∅ while curT ime > 1 do Path← Path ∪ ψcurTime(s∗) (s∗, curT ime)← ψcurTime(s∗)\n30: end while return Path\ngoing links. The score computation algorithm moves forward in time like the normal Viterbi algorithm. The score computation (at each used time step t) is done in 3 phases. The pseudocode is given in Algorithm 3.\n1. δt(s) is computed using the best of its incoming links, Links(s, t) and ψt(s) points to that link.\n2. Starting at level L− 1 and going down to level 0, a state s gets its parent’s (φ(s)) score and backpointer if φ(s) has a higher score.\n3. Starting from level 0 and going up to level L− 1, a state s’s parent φ(s) gets its child’s score and backpointer if s has a higher score\nTheorem 3.3 The score δt(s) computed by the BestPath procedure is a strict upper bound on all trajectories ending in state s at time t given the current abstracted version of the state-time trellis.\nProof Any trajectory ending in a state ŝ, which is neither an ancestor nor a descendant of s, does not include any trajectory to s. Hence, BestPath computes an upper bound on the score of the best trajectory ending in state s at time t.\nFor the bound to be strict, it is sufficient to show that each phase of BestPath only considers scores of such nodes where every incoming link includes at least one trajectory to s. The first phase accounts for all the incoming links to node s itself. Let φ∗(s) denote an ancestor of s. Any incoming link (direct, cross or re– entry) to φ∗(s) includes at least one trajectory to s.\nThis necessitates taking the maximum over δt(φ ∗(s)) (step 2). Finally, any trajectory ending in state s′, where s′ is a descendant of s, is by definition, a trajectory ending in s. Hence, the upper bound is strict.\nThe ordering of the phases is important to perform the desired computation correctly and efficiently."
    }, {
      "heading" : "3.3 Complete algorithm",
      "text" : "The algorithmic structure of TAV and CFDP are quite similar. The complete specification of TAV is presented in Algorithm 4. CFDP has a different initialization and refinement is node–based (TAV is link–based) which introduces links between states at different levels of abstraction. The two initializations are shown in Figure 5(a). CFDP’s initial configuration has no temporally abstract links. The algorithm iterates between two stages: computing the optimal path in the current graph and refining links along the current optimal path. A few steps of execution of the two algorithms are shown on an example in Figure 5.\nAlgorithm 4 TAV(A,B,Π, φ, Y1:T )\nδ1(.)← ScoreInitialization(Π) usedStates(1) = usedStates(T ) = SL usedT imes = {1, T} for all s ∈ SL do\n5: Links(s, T )← d(s, 1, T ) for all s′ ∈ SL do Links(s, T )← Links(s, T ) ∪ k((s′, 1), (s, T ))\nend for end for\n10: V iterbiPathFound← 0 while V iterbiPathFound = 0 do Path = BestPath(Links, usedStates, usedT imes) V iterbiPathFound = 1 for all k ∈ Path do 15: ((s1, t1), (s2, t2))← details(k) if level(s1) > 1 || ¬isDirect(k) then V iterbiPathFound = 0\nend if if isDirect(k) || t2 − t1 = 1 then\n20: Spatial Refinement(k) else Temporal Refinement(k)\nend if end for\n25: end while\nThe correctness of the algorithm follows from the optimality of A* search and Lemma 3.1 and Theorem 3.3."
    }, {
      "heading" : "4 Heuristics for temporal abstraction",
      "text" : "In hierarchical state abstraction schemes, computing heuristics involves taking the maximum of a set of single time step transition probabilities. As mentioned in Section 2, this can be done by hierarchically constructing Al, Bl and Πl. For temporal abstractions however, there are more design choices to be made when it comes to computing heuristic scores of links. There is a more significant tradeoff between cost of computation and quality of heuristic.\nThe heuristic score of a direct link is very easy to compute. We do not have to select between possible state transitions. Thus, the heuristic for a link spanning the interval (t1, t2) can be done in O(t2− t1), since we still need to account for all the observations in that interval. If the score is cached, the score for any subinterval is computable in O(1) time.\nCross links and re–entry links require further consideration. A somewhat expensive option is to compute the Viterbi path in the restricted scope. As Lemma 3.2 shows, a cross or a re–entry link represents trajectories that can switch between sibling states (the ones which map to the same parent via φ). In an abstraction hierarchy, if the cardinality of Children(s) for any state s is restricted to some constant C, then computing this heuristic will require O(C2(t2 − t1)) time. A computationally cheaper but relatively loose heuristic is the following:\nh((si, t1), (sj , t2)) = max k Âik max p,q Ât2−t1−2pq max k\nÂkj∏ t max k B̂kYt\nÂ and B̂ represent the transition matrices for the set Children(φ(si)). This heuristic chooses the best possible transition at every time step other than the two end points and also the best possible observation probability. Its computational complexity is O(C2 + (t2− t1)). Caching values help in both cases. The Viterbi heuristic, being tighter, leads to fewer iterations but needs more computation time. We will compare the two heuristics in our experiments."
    }, {
      "heading" : "5 Experiments",
      "text" : "The simulations we performed were aimed at showing the benefit of TAV over Viterbi and CFDP. The benefits are magnified in systems where variables evolve at widely varying timescales. The timescale of a random variable is the expected number of time steps in which it changes state. A person’s continental location would have a very large timescale, whereas his zip code location would have a much smaller timescale.\nA natural way to generate transition matrices with timescale separation is to use a dynamic Bayesian network (DBN). We consider a DBN with n variables, each of which has a cardinality of k. Hence, the state space size N is kn. We used fully connected DBNs in our simulations. Our observation matrix was multimodal (hence somewhat informative). A DBN with a parameter means that the timescales of successive variables have a ratio of . The fastest variable’s timescale is 1/ and the slowest variable’s (1/ )k.\nThe state space hierarchy at the most abstract level arises from the branching of the slowest variable. In\neach subtree, we branch on the next slowest variable. In the experiments in this section we assume that the abstraction hierarchy is given to us."
    }, {
      "heading" : "5.1 Varying T, N and",
      "text" : "To study the effect of increasing T on the computation time, we generated 2 sequences of length 100000 with = .1 (case 1) and .05 (case 2). N was 256 and the abstraction hierarchy was a binary tree. For each sequence, we found the Viterbi path for the first T timesteps using TAV, CFDP and the Viterbi algorithm. The results are shown in Figure 6(a). TAV’s computational complexity is marginally super-linear. This is because TAV might need to search in the interval [0, T ] even if it had found the Viterbi sequence in that interval as new observations (after time T ) come in. For the values used, TAV is more than 2 orders of magnitude faster than Viterbi and 1 order of magnitude faster than CFDP.\nIt is intuitive that TAV will benefit more from a smaller (i.e. a larger timescale). As Figure 6(a) shows, CFDP also benefits from the timescale artifact. However, TAV’s gains are larger and also grow more quickly with diminishing . This set of experiments had N = 256 and T = 10000.\nFinally, to check the effect of the state space size, we chose = .5 (case 1) and .25 (case 2). We chose fast timescales to show the limitations of TAV (having already demonstrated its benefits for small ). As Figure 6(a) shows, TAV is 3x to 10x faster than CFDP for N < 1024. However, CFDP is about 6x faster than TAV for N = 2048. In this case, TAV performs poorly because of its initialization as a single interval. For fast timescales, the first few refinements in this setting are invariably temporal and these refinements can be computationally very expensive.\n5.2 A priori temporal refinement\nIf T is comparable to the timescale of the slowest variable, then one or more temporal refinements at the very outset of TAV is very likely. Performing these refinements a priori will be beneficial if TAV actually had to perform those refinements. The benefit would be proportional to the cost of deciding whether to refine or not. This decision cost increases with T and N (but does not depend on ).\nFigure 6(b) shows the computation time for cases where we initialized TAV with 20 equal segments. The a priori refinement time was also included in the “PreSegmentation” time. Speedups of 2x to 6x were ob-\ntained for varying values of N and T . When only was varied, the benefit was approximately constant (between 3 to 6 seconds of computation time). This resulted in effective speedups only for the smaller values of , which had small computation times."
    }, {
      "heading" : "5.3 Impact of heuristics",
      "text" : "As discussed in Section 4, there is a trade-off in computing heuristics between accuracy and computation time. Figure 6(b) compares the effect of using the Viterbi heuristic instead of the cheap heuristic described previously. With increasing T , there was a small improvement in computation time, although the speedup was never greater than 2x. The two computation times were virtually the same with increasing . For large state spaces, the Viterbi heuristic produced more than 5x speedup (which made it comparable to CFDP).\nThe main reason for the lack of improvement (in computation time) is the randomness of the data generation process. The Viterbi heuristic can significantly outperform the cheap heuristic only if the most likely state sequences according to the transition model receive very poor support from the observations. In that case, the cheap heuristic will provide very inaccurate bounds and mislead the search. In randomly generated models however, the two heuristics demonstrate\ncomparable performance."
    }, {
      "heading" : "6 Hierarchy induction",
      "text" : "Till this point, we have assumed that the abstraction hierarchy will be an input to the algorithm. However, in many cases, we might have to construct our own hierarchy. Spectral clustering (Ng et al., 2001) is one technique which we have used in our experiments to successfully induce hierarchies. If the underlying structure is a DBN of binary variables with timescale separation between each variable (as discussed in Section 5), there will be a gap in the eigenvalue spectrum after the two largest values. The first two eigenvectors will be analogous to indicator functions for branching on the slowest variable. We can then apply the method recursively within each cluster. The main drawback is the O(N3) computational cost. It can be argued that this is an offline and one-time cost—nonetheless it is quite expensive. It should be noted that spatial hierarchy induction is also a hard problem. Let 28 denote a 8-variable DBN where each variable is binary. The slowest variables are placed at the left— hence 4224 denotes a DBN whose two slowest variables are 4-valued. As we change the underlying model from 28 to 44 to 162, is there a particular abstraction hierarchy which performs well for all the models?\nFor the experiments, we generated 3 different data sets using the following DBNs—28 (left), 44 (middle) and 162 (right)—with N = 256. On each data set, we used the following abstraction hierarchies (28; 2442; 4224; 44; 4182; 8241; 162). The results in Figure 7 show the computation time for TAV and CFDP using different abstraction hierarchies (deepest on the left to shallowest on the right) for two different values of . Both TAV and CFDP perform better with deeper hierarchies, although the improvement is much more pronounced for TAV. The trend across all 3 underlying data models indicates that we could always induce a deep hierarchy. The benefit of lightweight local searches in a deep hierarchy seems to outweigh the cost of the necessary additional refinements."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have presented a temporally abstracted Viterbi algorithm, that can reason about sets of trajectories and uses A* search to provably reach the correct solution. Direct links provide a way to reason about trajectories within a set of states—something that previous DP algorithms did not do. For systems with widely varying timescales, TAV can outperform CFDP handsomely. Our experiments confirm the intuition—the greater the timescale separation, the more the computational benefit.\nAnother smart feature of our algorithm is that it can exploit multiple timescales present in a system by adaptive spatial and temporal refinements. TAV’s limitations arise when the system has frequent state transitions and in such cases, it is better to fall back on the conventional Viterbi algorithm (CFDP is often slower as well in such cases). It might be possible to design an algorithm that uses temporal abstraction and can also switch to conventional Viterbi when the heuristic scores of direct links are low."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to acknowledge NSF for their support (grant no. IIS-0904672). We would also like to thank\nJason Wolfe, Aastha Jain and the anonymous reviewers for their comments and suggestions."
    } ],
    "references" : [ {
      "title" : "Why are DBNs sparse",
      "author" : [ "S. Chatterjee", "S. Russell" ],
      "venue" : "Journal of Machine Learning Research Proceedings Track,",
      "citeRegEx" : "Chatterjee and Russell,? \\Q2010\\E",
      "shortCiteRegEx" : "Chatterjee and Russell",
      "year" : 2010
    }, {
      "title" : "The generalized A* architecture",
      "author" : [ "P.F. Felzenszwalb", "D. McAllester" ],
      "venue" : "J. Artif. Int. Res.,",
      "citeRegEx" : "Felzenszwalb and McAllester,? \\Q2007\\E",
      "shortCiteRegEx" : "Felzenszwalb and McAllester",
      "year" : 2007
    }, {
      "title" : "The Viterbi algorithm",
      "author" : [ "G.D. Forney" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Forney and J.,? \\Q1973\\E",
      "shortCiteRegEx" : "Forney and J.",
      "year" : 1973
    }, {
      "title" : "Hierarchical A*: Searching Abstraction Hierarchies Efficiently",
      "author" : [ "R.C. Holte", "M.B. Perez", "R.M. Zimmer", "A.J. MacDonald" ],
      "venue" : "In AAAI/IAAI,",
      "citeRegEx" : "Holte et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Holte et al\\.",
      "year" : 1996
    }, {
      "title" : "A* Parsing: Fast Exact Viterbi Parse Selection",
      "author" : [ "D. Klein", "C.D. Manning" ],
      "venue" : "In HLT-NAACL",
      "citeRegEx" : "Klein and Manning,? \\Q2003\\E",
      "shortCiteRegEx" : "Klein and Manning",
      "year" : 2003
    }, {
      "title" : "A hidden markov model for progressive multiple",
      "author" : [ "A. Lytynoja", "M.C. Milinkovitch" ],
      "venue" : "alignment. Bioinformatics,",
      "citeRegEx" : "Lytynoja and Milinkovitch,? \\Q2003\\E",
      "shortCiteRegEx" : "Lytynoja and Milinkovitch",
      "year" : 2003
    }, {
      "title" : "On spectral clustering: Analysis and an algorithm",
      "author" : [ "A.Y. Ng", "M.I. Jordan", "Y. Weiss" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ng et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2001
    }, {
      "title" : "Multiscale methods: Averaging and homogenization",
      "author" : [ "G.A. Pavliotis", "A.M. Stuart" ],
      "venue" : null,
      "citeRegEx" : "Pavliotis and Stuart,? \\Q2007\\E",
      "shortCiteRegEx" : "Pavliotis and Stuart",
      "year" : 2007
    }, {
      "title" : "A tutorial on hidden Markov models and selected applications in speech recognition",
      "author" : [ "L. Rabiner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Rabiner,? \\Q1989\\E",
      "shortCiteRegEx" : "Rabiner",
      "year" : 1989
    }, {
      "title" : "Coarse-to-Fine Dynamic Programming",
      "author" : [ "C. Raphael" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Raphael,? \\Q2001\\E",
      "shortCiteRegEx" : "Raphael",
      "year" : 2001
    }, {
      "title" : "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning",
      "author" : [ "R.S. Sutton", "D. Precup", "S.P. Singh" ],
      "venue" : "Artif. Intell.,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm",
      "author" : [ "A. Viterbi" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Viterbi,? \\Q1967\\E",
      "shortCiteRegEx" : "Viterbi",
      "year" : 1967
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "The Viterbi algorithm (Viterbi, 1967; Forney, 1973) finds the most likely sequence of hidden states, called the “Viterbi path,” conditioned on a sequence of observations in a hidden Markov model (HMM).",
      "startOffset" : 22,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "It is one of the most important and basic algorithms in the entire field of information technology; its original application was in signal decoding but has since been used in numerous other applications including speech recognition (Rabiner, 1989), language parsing (Klein and Manning, 2003), and bioinformatics (Lytynoja and Milinkovitch, 2003).",
      "startOffset" : 232,
      "endOffset" : 247
    }, {
      "referenceID" : 4,
      "context" : "It is one of the most important and basic algorithms in the entire field of information technology; its original application was in signal decoding but has since been used in numerous other applications including speech recognition (Rabiner, 1989), language parsing (Klein and Manning, 2003), and bioinformatics (Lytynoja and Milinkovitch, 2003).",
      "startOffset" : 266,
      "endOffset" : 291
    }, {
      "referenceID" : 5,
      "context" : "It is one of the most important and basic algorithms in the entire field of information technology; its original application was in signal decoding but has since been used in numerous other applications including speech recognition (Rabiner, 1989), language parsing (Klein and Manning, 2003), and bioinformatics (Lytynoja and Milinkovitch, 2003).",
      "startOffset" : 312,
      "endOffset" : 345
    }, {
      "referenceID" : 9,
      "context" : "Coarse-to-fine dynamic programming or CFDP (Raphael, 2001) begins with SL and iteratively finds the shortest path in the current (abstracted) version of the graph and refines along it until the current shortest path is completely refined.",
      "startOffset" : 43,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : ", hierarchical A* (Holte et al., 1996) and HA*LD (Felzenszwalb and McAllester, 2007)—are able to refine only the necessary part of the hierarchy tree and compute heuristics only when needed.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : ", 1996) and HA*LD (Felzenszwalb and McAllester, 2007)—are able to refine only the necessary part of the hierarchy tree and compute heuristics only when needed.",
      "startOffset" : 18,
      "endOffset" : 53
    }, {
      "referenceID" : 10,
      "context" : "Temporal abstractions have been well-studied in the context of planning (Sutton et al., 1999) and inference in dynamic Bayesian networks (Chatterjee and Russell, 2010).",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : ", 1999) and inference in dynamic Bayesian networks (Chatterjee and Russell, 2010).",
      "startOffset" : 51,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "An excellent survey of temporal abstraction for dynamical systems can be found in (Pavliotis and Stuart, 2007).",
      "startOffset" : 82,
      "endOffset" : 110
    }, {
      "referenceID" : 8,
      "context" : "Following Rabiner (1989), we define",
      "startOffset" : 10,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : "The complete procedure (Rabiner, 1989) is as follows:",
      "startOffset" : 23,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "Spectral clustering (Ng et al., 2001) is one technique which we have used in our experiments to successfully induce hierarchies.",
      "startOffset" : 20,
      "endOffset" : 37
    } ],
    "year" : 2011,
    "abstractText" : "Hierarchical problem abstraction, when applicable, may offer exponential reductions in computational complexity. Previous work on coarse-to-fine dynamic programming (CFDP) has demonstrated this possibility using state abstraction to speed up the Viterbi algorithm. In this paper, we show how to apply temporal abstraction to the Viterbi problem. Our algorithm uses bounds derived from analysis of coarse timescales to prune large parts of the state trellis at finer timescales. We demonstrate improvements of several orders of magnitude over the standard Viterbi algorithm, as well as significant speedups over CFDP, for problems whose state variables evolve at widely differing rates.",
    "creator" : "TeX"
  }
}