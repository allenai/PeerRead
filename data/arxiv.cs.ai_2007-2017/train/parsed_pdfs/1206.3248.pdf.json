{
  "name" : "1206.3248.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Knowledge Combination in Graphical Multiagent Models",
    "authors" : [ "Quang Duong", "Michael P. Wellman", "Satinder Singh" ],
    "emails" : [ "qduong@umich.edu", "wellman@umich.edu", "baveja@umich.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "A graphical multiagent model (GMM) represents a joint distribution over the behavior of a set of agents. One source of knowledge about agents' behavior may come from gametheoretic analysis, as captured by several graphical game representations developed in recent years. GMMs generalize this approach to express arbitrary distributions, based on game descriptions or other sources of knowledge bearing on beliefs about agent behavior. To illustrate the exibility of GMMs, we exhibit game-derived models that allow probabilistic deviation from equilibrium, as well as models based on heuristic action choice. We investigate three di erent methods of integrating these models into a single model representing the combined knowledge sources. To evaluate the predictive performance of the combined model, we treat as actual outcome the behavior produced by a reinforcement learning process. We nd that combining the two knowledge sources, using any of the methods, provides better predictions than either source alone. Among the combination methods, mixing data outperforms the opinion pool and direct update methods investigated in this empirical trial."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Graphical models provide a compact representation for domains with decomposable structure, with concomitant computational advantages. Multiagent scenarios may be particularly amenable to decomposition, to the extent that interactions among the agents exhibit localized e ects. The idea of exploiting conditional independence among the e ects of agents' decisions was central to the multiagent in uence diagram (MAID)\nframework developed by Koller and Milch (2003). This observation was also a driving motivation for graphical game models, rst introduced by Kearns et al. (2001), and subsequently examined and extended in several research e orts (Kearns, 2007).\nIn the basic graphical game approach, the model is a factored representation of a normal-form game, and special-purpose algorithms operate on this representation to identify approximate or exact Nash equilibria. Daskalakis and Papadimitriou (2006) demonstrated how to map a graphical game to a Markov random eld (MRF), assigning high potential to congurations where an agent plays a best response to its neighbors. They showed that the maximum a posteriori con gurations of the MRF correspond to purestrategy Nash equilibria (PSNE) of the game. This approach enables the exploitation of statistical inference tools for game-theoretic computation, including the full repertoire of graphical model algorithms.\nWe build on these works to introduce graphical multiagent models (GMMs), which are simply graphical models where the joint probability distribution is interpreted as an uncertain belief (e.g., a prediction) about the agents' play. For instance, the Daskalakis and Papadimitriou mapping can be viewed as a GMM where we believe that the agents will play a PSNE if one exists. This is of course just one candidate for belief based on game-theoretic analysis. When reasoning about strategies to play, or designing a mechanism (which induces a game for other agents), we may wish to adopt alternative bases for forming beliefs about the agents' play (Vorobeychik and Wellman, 2006). The GMM framework supports such decision making, and moreover, allows that beliefs may be based on variant solution concepts, models of bounded rationality or equilibrium selection, or for that matter knowledge that has nothing to do with game-theoretic analysis. At this level, our motivation shares the spirit of the network of in uence diagrams formalism of Gal and Pfe er (2008), which extends MAIDs to incorporate non-\nmaximizing models of behavior. It also aligns with the goal of Wolpert's information-theoretic framework for modeling bounded rationality in game play (Wolpert, 2006).\nIn this paper, we illustrate the exibility of GMMs by showing how to construct plausible multiagent models using quite di erent sources of belief about agent play. One example model is based on the game form, and another based on heuristic assumptions of behavior. In both, we assume that the graphical structure representing interactions among players in the game is known. We then introduce and test three approaches to integrate these knowledge sources into a combined model. To evaluate the results, we posit that actual play is generated by agents who start to play heuristically, then update their behavior over repeated interactions through a reinforcement learning (RL) process. Thus, the task we set up is to predict the outcome of a RL regime. More precisely, we seek to compute a reasonable estimation of the joint probability distribution of the agents' play. Intuitively, knowledge about the heuristic starting point is relevant, as is knowledge of strategically stable policies (game-theoretic equilibria), but neither directly captures nor necessarily corresponds to the RL outcome. We nd experimentally that in fact the two knowledge sources are complementary, as the combined model outperforms either alone. Our investigation further provides support for one particular combination approach, based on mixing data.\nWe begin with formal de nitions and speci cations of the GMM framework in Section 2. In Section 3, we present an example multiagent domain, the Internet industry partnership network, and construct two plausible models based on di erent knowledge sources. Section 4 details some alternative combination methods. We follow with an empirical study in Section 5, designed to evaluate the performance of the respective models and their combinations. We conclude the paper with some observations on these results."
    }, {
      "heading" : "2 GRAPHICAL MULTIAGENT MODELS",
      "text" : "Consider a multiagent scenario with n players, where each player i ∈ {1, . . . , n} chooses an action (or strategy) si, from its strategy domain, Si. The outcome is a joint action, or strategy pro le, s, designating the strategy choice of all players. A GMM G for this scenario is a graphical model, G = (V,E, S, π), with vertices V = {v1, . . . , vn} corresponding to the agents (we refer to vi and i interchangeably), and edges (i, j) ∈ E indicating a local interaction between i and j. The graph de nes for each agent a neighborhood, Ni = {j | (i, j) ∈ E}∪{i}, including i and its neighbors\nN−i = Ni\\{i}. Each neighborhood i is associated with a potential function πi(sNi) : Πj∈NiSj → R. Intuitively a local con guration of strategies with a higher potential is more likely to be part of the global outcome than one with lower potential. As in graphical games, the size of the GMM description is exponential only in the size of local neighborhoods rather than in the total number of players. These local potentials de ne the joint probability of a global con guration s,\nPr(s) = Πiπi(sNi)\nZ , (1)\nwhere Z is a normalization term.\nOne source of potential functions is a description of the game played by the n agents. Let G be a game with agents and strategy sets as de ned for the GMM G. Let us further assume that G is a graphical game, such that agent i's payo depends only on sNi : its strategy and those of its neighbors. Formally, i's payo is dened by a utility function, ui : Πj∈NiSj → R. For example, Daskalakis and Papadimitriou (2006) de ned a binary potential function, associating a high value for con gurations where each agent's strategy choice is a best response to its neighbors (i.e., maximizes payo s given sN−i), and a low value for all other con gurations.\nA natural generalization of this approach would smooth out the binary distinction, assigning intermediate potentials based on the degree to which agents deviate from their best response. We may not wish to assume that agents play best responses with certainty, as they may not be perfectly rational, or our attributions of payo functions may be inexact. For a given payo model, let i(sNi) denote i's regret function, representing the maximum gain i can obtain through unilaterally reconsidering its own strategy si given sN−i ,\ni(sNi) = max s′i∈Si\nui(s′i, sN−i)− ui(sNi).\nIntuitively, we expect that high-regret pro les are less likely to be played (all else equal), since as regret increases agents are more apt to recognize and select the better alternatives. We can capture this intuition in a regret potential,\nπi(sNi) = e − i(sNi )/Ti , (2)\nwhere Ti, the temperature parameter, provides a way to calibrate our association between regret and relative likelihood. Greater values of Ti accord more likelihood to agents making less than perfectly rational decisions. For simplicity in notation below, we also de ne λi = 1 Ti , and λ the vector of λi. Let reG denote a GMM employing the regret potential function. In the current study, we consider the\nregret GMM reG as one plausible form of predictive model. We also consider models that are not based directly on payo s in an associated game. In particular, we construct for our particular example a rule-based GMM, hG , encoding heuristic assumptions of agents' behavior."
    }, {
      "heading" : "3 EXAMPLE: INTERNET INDUSTRY PARTNERSHIPS",
      "text" : "We illustrate the GMM framework and motivate the problem of combining knowledge sources through an example multiagent scenario. In the Internet industry partnership domain,1 companies must decide whether to retain (s = 1) or upgrade (s = 2) their current technology. The payo functions in G can be mapped into G's potential functions in several di erent ways. The payo for each strategy depends on the choices of other companies to which they are related through some kind of partnership their neighbors in the interaction graph. For example, the bene ts of upgrading may be larger when one's partners also upgrade, since keeping their technologies synchronized enhances compatibility."
    }, {
      "heading" : "3.1 GAME DEFINITION",
      "text" : "We construct our example scenario using a fragment of the partnership network consisting of 10 representative companies, as depicted in Figure 1. Each node represents a company, which we characterize by three parameters: (1) size class, z; (2) sector, t: either commerce, infrastructure, or content; and (3) change coe cient, ch ∈ [0, 1], representing the intrinsic adaptability of the company's technology. The study by Krebs (2002) provides sector and a rough order of size; the change coe cient is assigned by us in an arbitrary manner. Although somewhat contrived, the scenario speci cation serves our purpose of demonstrating some capabilities of the GMM approach.\nThe payo function, u, de nes the value of retaining or upgrading technology, given the actions of a rm's neighboring companies and the parameters describing these companies. Qualitatively, payo is increased by agreement with neighbors, where larger neighbors from the same sector are relatively more important. Let us rst introduce an intermediate value wij for each connected pair of companies, re ecting the strength of i and j's partnership:\nwij(si, sj) = (zi + zj) ( 1 +\nyij 2It + 41−It\n)Is , (3)\n1The example is inspired by the network model of Krebs (2002), cited by Kearns (2002).\nwhere yij ∼ U [0, 1] is a random variable, and Is and It are indicator functions representing agreement in strategy and technology sector. Is = 1 if si = sj , and 0 otherwise; It = 1 if ti = tj , and 0 otherwise. The intuition for (3) is that the mutual in uence between two rms increases with their size, agreement in action, and sector commonality. We also de ne function φ(chi, si) to compare i's action to its change parameter chi. Speci cally, φ's value is positive if si is upgrade (retain) and chi is greater (smaller) than 0.5. The interpretation is that a value greater (smaller) than 0.5 for chi implies that i is exible (in exible) with respect to technology change.\nφ(chi, si) = { si 2 − chi if si 2 − chi < 0.5\n0.5− si2 + chi otherwise\nFinally, the overall payo combines the pairwise partnership weights, further adjusted by the company's exibility in upgrading its technology.\nui(aNi) = (1 + yiφ(chi, si)) ∑ j wij(si, sj),\nwith yi ∼ U [0, 1]."
    }, {
      "heading" : "3.2 GMM CONSTRUCTIONS",
      "text" : "Given the payo function and the game de nition above, we can generate a regret potential function (2) in a straightforward manner, parametrized by temperature. This potential function in turn de nes a regret GMM, reG .\nOur heuristic rule-based model, hM , in contrast, is speci ed without direct reference to the payo function. In this model, each company independently applies a local heuristic to stochastically choose its action. Speci cally, agent i changes its technology with\nprobability pChange(i), where\npChange(i) = 0.5(1− 10−3)|Ni|(1− 10−3zi).\nThe intuition behind this heuristic is that the more partners (|Ni| − 1) a company has and the greater its size zi, the less likely it is to change. Given the pChange values, it is straightforward to de ne a potential function for the GMM hG such that the outcome distribution is the same as generated by applying the rule independently for each company. As a result, hG 's potential πi is a function of only si instead of sNi .\nThe two GMMs, reG and hG , are based on qualitatively di erent sources of knowledge. If we believe that agents are essentially rational and aware of their environment, we may expect that the regret GMM reG would predict behavior well. If instead we have evidence that agents choose heuristically based on partnership density and size, we might have greater condence in predictions based on hG , or on some other heuristic models that capture our intuition of agents' behavior. In other situations, we may consider that both models capture factors determining behavior, and view the knowledge sources as complementary."
    }, {
      "heading" : "3.3 SIMULATION MODEL",
      "text" : "The role of our simulation model is to generate play data from a plausible agent interaction process. In this study, we treat this data as the actual outcome, and use it to evaluate the GMMs above as well as combined models. The simulation is based on the idea that actual agent behavior is produced via repeated interaction through a reinforcement learning (RL) procedure.\nIn the model, each agent is an independent learner, employing an RL procedure designed for partially observable environments (Jaakkola et al., 1995). The environment for company i comprises i and its partners (i.e., its neighbors N−i), and each con guration of partner strategies sN−i is a possible state. The agent seeks to learn a stochastic play policy: σi(si | sN−i), which denotes the probability of playing action si at state sN−i . However, the agent does not actually observe sN−i before taking its action. Thus, the action is actually selected based on the policy, for a = 1, 2,\nPri(si = a) = ∑ sN−i σi(si = a | sN−i) Pr(sN−i). (4)\nPri(sN−i) is a stationary probability distribution over states, which, for simplicity, we take to be uniform. The scarcity of companies' knowledge about others' strategies and the network e ects motivates our choice of Pri(sN−i) instead of a more complicated model of network dynamics.\nTo learn the policy σi, we apply the RL procedure of Jaakkola et al. (1995).\n1. Initialize σi to pChange(i) (i.e., the heuristic policy from hM ).\n2. Generate a play s using (4). Observe the resulting local state sNi and receive as reward the payo ui(sNi). Update Qi(si, sN−i), the average reward for taking action si in the associated local state.\n3. Choose σ∗i (si | sN−i) to maximize Qi(si, sN−i). Adjust the current policy σi in the direction of σ∗i : σi ← σi(1− γ) + σ∗i γ, where γ is the learning rate.\n4. Repeat steps 2 and 3 until convergence.\nFor our experiments, we used γ = 0.2 and iterated steps 2 and 3 above 40 times, the point after which few changes occurred in the learned RL policy σ. We denote the simulated model at the end of the RL procedure as simM .\nNote that the RL process starts with the local heuristic rule-based policy, but is updated based on payo experience. Thus we expect that both the heuristic rulebased model and the rationalistic regret-based model may o er value for predicting the outcomes of simM ."
    }, {
      "heading" : "4 METHODS FOR COMBINING KNOWLEDGE SOURCES",
      "text" : "Given two complementary sources of knowledge, how can we integrate them into a single GMM? We formulate the problem for the case that one knowledge source is expressed explicitly as a GMM, G1, and the other in the form of some data D = {s1, . . . , sm} of joint plays related to the multiagent scenario.2 Note that D may not re ect the actual distribution of play accurately, for example because it is small in size or because it was observed in a di erent multiagent setting. In this section we answer these questions abstractly and in the next section show how we can do this for our speci c reG model and data D derived from hM ."
    }, {
      "heading" : "4.1 DIRECT UPDATE",
      "text" : "The direct update method combines the two sources of knowledge G1 and D into a new GMM, directG , derived by adjusting the λG1 parameters of G1 to maximize the predictive performance w.r.t. the data D.\n2Since we can generate such a data set from a GMM, or induce a GMM from data, the combination methods can be applied to more general settings where knowledge sources come in either form.\nWe measure predictive performance using the logarithmic scoring rule (Gneiting and Raftery, 2007):\nScore(G | D) = ∑|D|\nk=1 log PrG(s k), which assesses the\nlog-likelihood of the data. We take as our problem to tune the GMM's parameters λ in order to maximize this score (Kappen and Rodríguez, 1997),\nScore(G1 | D) = |D|∑ k=1 log PrG1(s k) = L(D | λ = λG1).\nWe employ the gradient ascent method to maximize data likelihood, which entails computing the gradient of L w.r.t λ:\n∇λ = ∂L(D|λ)∂λ = P|D| k ∂ log e − P i λi i(s k Ni )\n∂λ − |D| ∂ logZ ∂λ\n(5)\nand adjusting λ = λ + α∇λ, where α is the learning rate, until the gradient is below some threshold.\nA major problem in graphical-model parameter learning is the intractability of calculating logZ in (5). It entails iterating over all possible outcomes of the game, which is exponential in the number of players N , rendering exact inference and learning in undirected graphical models intractable. Since our priority is to accurately evaluate knowledge combination methods for GMMs, our current implementation of the inference and learning algorithms does not employ any approximation. Our pilot study of generalized belief propagation approximations in GMMs (Yedidia et al., 2001) has indeed yielded positive results, and will be incorporated in future reports."
    }, {
      "heading" : "4.2 OPINION POOL",
      "text" : "Unlike direct update, which depends on the availability of both play-outcome data and the potential function's parameterized form, the next two methods, opinion pool and mixing data, push the knowledge combination problem towards potentially greater independence from the input knowledge sources' forms.\nThe opinion pool method starts by rst using half the given data D to learn a GMM G2 by adjusting its parameters to maximize the likelihood of the data, as in the direct update method above. The combined model OPG is then an aggregation of G1 and G2 into a single probability distribution:\nPrOPG(s) = f(PrG1(s),PrG2(s)).\nNote that the above equation does not involve de ning a separate pooled potential function for each player in the combined model. The rationale is that potentials are not normalized like the joint probability, and thus,\ntheir absolute values contain little meaning when taken out of the context of their corresponding models.\nWe adopt for our aggregation function the weighted geometric mean, called the logarithmic opinion pool (logOP).\nPrOPG(s) = PrG1(s)\nw PrG2(s) 1−w\nZ .\nThe logOP is the only pool known to preserve independence structure in graphical models (Pennock and Wellman, 2005), which is an important property in our context. The weight parameter w can be set by at, or tuned with data. In our experiments described below, we employ the other half of input game-play data D, denoted D̄, and set w by maximizing the objective function:\nL(D̄ | w) = |D̄|∑ k log PrOPG(sk). (6)\nIn brief, given the two components G1 and G2, the opinion pool method rst initializes w = 0.5. It then repeats computing the gradient ∇w of the loglikelihood w.r.t w by di erentiating (6), and updating w = w + β∇w, where β is the learning rate, until the gradient is acceptably small."
    }, {
      "heading" : "4.3 MIXING DATA",
      "text" : "The mixing data method samples joint plays from the given GMM G1 to generate a new data set D1. We combine D1 and D into one data set mD by sampling from the two sources equally, though one could easily weight one source more than another by adjusting the sampling ratio. We then induce a new GMM for a given parametrized form by tuning the parameter λ, as detailed in the direct update approach (Section 4.1), to maximize the likelihood of the new data mD.\nBelow is the outline of the mixing data method, which produces the combined GMM mixG . Note that we leave out step 4 in our implementation.\n1. Generate a sample of play outcomes D1 from G1.\n2. Build the mixed data setmD fromD1 andD with sampling ratio ω = 0.5.\n3. Initialize mixG with some λmixG . Update λmixG as in direct update using the data mD.\n4. (optional) Tune ω in the direction determined by the gradient-descent method to maximize mixG 's performance on a small held-out part of the testing data. Repeat steps 3 and 4 until the gradient is below some threshold."
    }, {
      "heading" : "5 EMPIRICAL STUDY",
      "text" : "We evaluate our combination approaches experimentally using our simpli ed version of the Internet industry partnership domain. We concentrate mainly on Example 1, the scenario depicted in Figure 1. In the rst experiment, we also examine Example 2, which employs a smaller graph including only the top four companies."
    }, {
      "heading" : "5.1 EXPERIMENT SETTINGS",
      "text" : "In our experiments we use the following components de ned above: (i) the regret GMM reG with the temperature parameters generated uniformly randomly (except in one case explicitly speci ed below), (ii) a data set D of joint plays generated by using the heuristic rule-based model hM , (iii) the heuristic model hM and associated GMM hG , and (iv) a testing data set D∗ derived from the RL-based model simM . We compare our di erent combination approaches based on their ability to predict the test data set as measure by the score function Score(G | D∗). In particular, we compare the performance of a model that combines knowledge sources combinedG relative to the performance of a baseline model baseG using a ratio of scores, R = Score(baseG|D ∗)\nScore(combinedG|D∗) .\nThe inverted order of baseG and combinedG is due to Score's negativity, and thus, any R > 1 indicates combinedG 's improvement over baseG .\nWe experiment with several environment settings. For each setting, we conduct 20 trials, each of which involves a training data D set of 500 plays from hM and a testing data set D∗ of 500 plays from simM ."
    }, {
      "heading" : "5.2 RESULTS AND ANALYSIS",
      "text" : "First, in Figure 2 and Figure 3 we present an overview of our combination methods' e ectiveness. For both gures, reG and D are used to derive the model directG using the direct update combination method, the model OPG using the opinion pool method, and the model mixG using the mixed data method.\nFigure 2 displays predictive performance across the two examples (1 and 2) and the two baseline models (reG and hG). Mixing data is consistently best of the three combination methods, and direct update performs relatively better than opinion pool. All three methods yield better results than individual input models, suggesting that combining two knowledge sources is bene cial regardless of which of the proposed methods is adopted.\nFigure 3 shows the performance of various models com-\npared to a model derived from the same gold-standard source simM as our test data D∗. We sample a separate data set D′ from simM and employ it in learning a GMM simG of the parametrized regret form (2), using maximum likelihood to adjust the temperature parameter. The results reveal that our combined models, especially mixG , closely match simG in terms of predictive performance.\nNext, we study the e ect of varying the quality of the two input sources. Figure 4 shows the e ect of varying the amount of joint-play data available in D. Specifically, we make a fraction ρ|D| of play observations, ρ ∈ [0, 1], available to the three combination methods. From Figure 4, we observe that as long as ρ > 0.1, performance remains fairly stable. In our experiments, this corresponds to a threshold data set size of approximately 500×0.1 = 50. When the amount of data goes\nbelow this threshold, the combined model may very well become inferior to the models reG and hG .\nFigure 5 shows the e ect of varying the quality of the GMM provided as input to the combination methods. To modulate the accuracy of reG , we introduce a parameter δ controlling the relation of its temperature parameters to those of simG . Speci cally, we set λreG to (1 + δ)λsimG . The results of the third experiment are depicted in Figure 5. When compared with the unchanged heuristic model hG , the combination models show a slight decrease in their relative performance with δ, which re ects the e ect of reG 's inaccuracy on the combination methods. When the baseline is reG , in contrast, the degradation of the combined model is dominated by the e ect of compromising the baseline reG . In other words, combining knowledge sources effectively compensates for degrading one of them.\nIn these experiments, OPG 's poor performance relative to that of directG and mixG may be due in part to its reliance on only a single parameter, w, compared to the vector λ available to the other methods. mixG 's overall superiority is likely a result of its directly sampling from reG , which employs information contained in reG more e ectively than directG , where reG only matters at the initialization stage.\nFigure 6 presents the results of an experiment designed to strengthen our claims about the bene ts of integrating knowledge sources in a single model. We examine the combined models' performance in environments where the simulation mode simM is not the product of RL that starts with the input model hM . In particular, we de ne a di erent heuristic model hM ′, such that pChangehM ′(i) = 0.05 for all i. Let E be the input data set generated from hM ′. Based on hM ′,\nwe subsequently build the simulation model simM ′ and its corresponding test data E∗. Given these data sets and models, we can evaluate the combined models across drastically di erent starting points, by comparing their performances when di erent input sources, D and E, are provided, on the same testing data (either D∗ or E∗). First, we compare the performance of the heuristic models employed in generating input data, hG(D) (induced from hM ) and hG(E) (induced from hM ′): hG(D) performs 60% better than hG(E) when tested on D∗, whereas hG(E) outperforms hG(D) by 54% on E∗. This assessment a rms that the two different input data sets, D and E, are indeed di erentiable in terms of the behavior models they represent.\nThe results presented in Figure 6 indicate that better\nperformance is achieved in cases where the test and input environments coincide (the rst and third measures) compared to those in which they are unrelated (second and fourth). This observation con rms that combined models whose input data contains some information about the underlying behavior model outperform those extracting irrelevant information from its inputs. The gaps in mixG 's performance between scenarios where test cases are derived from the same input ( rst and third) and from an unrelated model (second and forth) are relatively smaller than those in the other methods. This phenomenon is likely a result of mixG 's more e ective usage of both knowledge sources, which limits the impact of irrelevant input data, and possibly contributes to the good performance of mixG(D), which is tuned to t D, when tested on E∗."
    }, {
      "heading" : "6 CONCLUSIONS",
      "text" : "GMMs provide a exible representation framework for graphically structured multiagent scenarios, supporting the speci cation of probability distributions based on game-theoretic models as well as heuristic or other qualitatively di erent characterizations of agent behavior. We explored the possibility of exploiting this exibility by employing multiple knowledge sources for prediction, as demonstrated for the task of predicting the outcome of a reinforcement learning process.\nOur basic nding is that combining two knowledge sources in this scenario does improve predictive power over either input source alone. We have also identi ed the most e ective combination method among those tried mixing data and the existence of a threshold for data availability that can help boost e ciency. Furthermore, we have found that our knowledge combination approaches, especially mixing data, can e ectively match the performance of modeling the reinforcement learning process directly.\nThis study is a rst step in what we expect to be an extended e ort to develop the GMM framework for supporting reasoning about strategic situations. One important issue to address is computational feasibility; although the graphical representation facilitates scalability in the number of agents, accurate approximation techniques are nevertheless essential to support practical applications with large models.\nKnowledge about multiagent behavior may come from sources other than play history and regret functions, and so another logical research direction is to develop canonical models for capturing and combining such sources. Learning may be extended to not only the model's parameters, but also the structure of potential functions and the graph topology itself. Such exten-\nsions present more complicated problems for combining models derived from di erent knowledge sources. Finally, we might look to dynamic Bayesian network concepts to extend the GMM framework to add a time dimension, and thus enable modeling sequential and interactive multiagent environments."
    } ],
    "references" : [ {
      "title" : "Computing pure Nash equilibria in graphical games via Markov random elds",
      "author" : [ "C. Daskalakis", "C.H. Papadimitriou" ],
      "venue" : "Seventh ACM conference on Electronic Commerce, pages 91 99, Ann Arbor.",
      "citeRegEx" : "Daskalakis and Papadimitriou,? 2006",
      "shortCiteRegEx" : "Daskalakis and Papadimitriou",
      "year" : 2006
    }, {
      "title" : "Networks of in uence diagrams: A formalism for reasoning about agents' decision-making processes",
      "author" : [ "Y. Gal", "A. Pfe er" ],
      "venue" : "Journal of Arti cial Intelligence Research.",
      "citeRegEx" : "Gal and er,? 2008",
      "shortCiteRegEx" : "Gal and er",
      "year" : 2008
    }, {
      "title" : "Strictly proper scoring rules, prediction and estimation",
      "author" : [ "T. Gneiting", "A.E. Raftery" ],
      "venue" : "Journal of the American Statistical Association, 102(477):359 378.",
      "citeRegEx" : "Gneiting and Raftery,? 2007",
      "shortCiteRegEx" : "Gneiting and Raftery",
      "year" : 2007
    }, {
      "title" : "Reinforcement learning algorithm for partially observable Markov decision problems",
      "author" : [ "T. Jaakkola", "S. Singh", "M.I. Jordan" ],
      "venue" : "Advances in Neural Information Processing Systems 7, pages 345 352.",
      "citeRegEx" : "Jaakkola et al\\.,? 1995",
      "shortCiteRegEx" : "Jaakkola et al\\.",
      "year" : 1995
    }, {
      "title" : "Mean eld approach to learning in Boltzmann machines",
      "author" : [ "H.J. Kappen", "F.B. Rodríguez" ],
      "venue" : "Pattern Recognition Letters, 18:1317 1322.",
      "citeRegEx" : "Kappen and Rodríguez,? 1997",
      "shortCiteRegEx" : "Kappen and Rodríguez",
      "year" : 1997
    }, {
      "title" : "Computational game theory: A tutorial",
      "author" : [ "M. Kearns" ],
      "venue" : "http://www.cis.upenn.edu/~mkearns/ nips02tutorial/nips.pdf. Presented at the Neural Information Processing Systems conference.",
      "citeRegEx" : "Kearns,? 2002",
      "shortCiteRegEx" : "Kearns",
      "year" : 2002
    }, {
      "title" : "Graphical games",
      "author" : [ "M. Kearns" ],
      "venue" : "Nisan, N., Roughgarden, T., Tardos, E., and Vazirani, V. V., editors, Algorithmic Game Theory, pages 159 180. Cambridge University Press.",
      "citeRegEx" : "Kearns,? 2007",
      "shortCiteRegEx" : "Kearns",
      "year" : 2007
    }, {
      "title" : "Graphical models for game theory",
      "author" : [ "M. Kearns", "M.L. Littman", "S. Singh" ],
      "venue" : "Seventeenth Conference on Uncertainty in Arti cial Intelligence, pages 253 260, Seattle.",
      "citeRegEx" : "Kearns et al\\.,? 2001",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 2001
    }, {
      "title" : "Multi-agent in uence diagrams for representing and solving games",
      "author" : [ "D. Koller", "B. Milch" ],
      "venue" : "Games and Economic Behavior, 45:181 221.",
      "citeRegEx" : "Koller and Milch,? 2003",
      "shortCiteRegEx" : "Koller and Milch",
      "year" : 2003
    }, {
      "title" : "Internet industry partnerships",
      "author" : [ "V. Krebs" ],
      "venue" : "http: //www.orgnet.com/netindustry.html.",
      "citeRegEx" : "Krebs,? 2002",
      "shortCiteRegEx" : "Krebs",
      "year" : 2002
    }, {
      "title" : "Graphical models for groups: Belief aggregation and risk sharing",
      "author" : [ "D.M. Pennock", "M.P. Wellman" ],
      "venue" : "Decision Analysis, 2:148 164.",
      "citeRegEx" : "Pennock and Wellman,? 2005",
      "shortCiteRegEx" : "Pennock and Wellman",
      "year" : 2005
    }, {
      "title" : "Mechanism design based on beliefs about responsive play (position paper)",
      "author" : [ "Y. Vorobeychik", "M.P. Wellman" ],
      "venue" : "ACM EC-06 Workshop on Alternative Solution Concepts for Mechanism Design, Ann Arbor, MI.",
      "citeRegEx" : "Vorobeychik and Wellman,? 2006",
      "shortCiteRegEx" : "Vorobeychik and Wellman",
      "year" : 2006
    }, {
      "title" : "Information theory: The bridge connecting bounded rational game theory and statistical physics",
      "author" : [ "D.H. Wolpert" ],
      "venue" : "Complex Engineered Systems. Springer.",
      "citeRegEx" : "Wolpert,? 2006",
      "shortCiteRegEx" : "Wolpert",
      "year" : 2006
    }, {
      "title" : "Generalized belief propagation",
      "author" : [ "J.S. Yedidia", "W.T. Freeman", "Y. Weiss" ],
      "venue" : "Advances in Neural Information Processing Systems, 13:689 695.",
      "citeRegEx" : "Yedidia et al\\.,? 2001",
      "shortCiteRegEx" : "Yedidia et al\\.",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "(2001), and subsequently examined and extended in several research e orts (Kearns, 2007).",
      "startOffset" : 74,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "The idea of exploiting conditional independence among the e ects of agents' decisions was central to the multiagent in uence diagram (MAID) framework developed by Koller and Milch (2003). This observation was also a driving motivation for graphical game models, rst introduced by Kearns et al.",
      "startOffset" : 163,
      "endOffset" : 187
    }, {
      "referenceID" : 5,
      "context" : "This observation was also a driving motivation for graphical game models, rst introduced by Kearns et al. (2001), and subsequently examined and extended in several research e orts (Kearns, 2007).",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 0,
      "context" : "Daskalakis and Papadimitriou (2006) demonstrated how to map a graphical game to a Markov random eld (MRF), assigning high potential to congurations where an agent plays a best response to its neighbors.",
      "startOffset" : 0,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : "When reasoning about strategies to play, or designing a mechanism (which induces a game for other agents), we may wish to adopt alternative bases for forming beliefs about the agents' play (Vorobeychik and Wellman, 2006).",
      "startOffset" : 189,
      "endOffset" : 220
    }, {
      "referenceID" : 12,
      "context" : "It also aligns with the goal of Wolpert's information-theoretic framework for modeling bounded rationality in game play (Wolpert, 2006).",
      "startOffset" : 120,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : "For example, Daskalakis and Papadimitriou (2006) de ned a binary potential function, associating a high value for con gurations where each agent's strategy choice is a best response to its neighbors (i.",
      "startOffset" : 13,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "The study by Krebs (2002) provides sector and a rough order of size;",
      "startOffset" : 13,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "The example is inspired by the network model of Krebs (2002), cited by Kearns (2002).",
      "startOffset" : 48,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "The example is inspired by the network model of Krebs (2002), cited by Kearns (2002). Figure 1: Part of the Internet industry partnership network, from Krebs (2002).",
      "startOffset" : 71,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "The example is inspired by the network model of Krebs (2002), cited by Kearns (2002). Figure 1: Part of the Internet industry partnership network, from Krebs (2002).",
      "startOffset" : 71,
      "endOffset" : 165
    }, {
      "referenceID" : 3,
      "context" : "In the model, each agent is an independent learner, employing an RL procedure designed for partially observable environments (Jaakkola et al., 1995).",
      "startOffset" : 125,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "To learn the policy σi, we apply the RL procedure of Jaakkola et al. (1995).",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "We measure predictive performance using the logarithmic scoring rule (Gneiting and Raftery, 2007): Score(G | D) = ∑|D| k=1 log PrG(s ), which assesses the log-likelihood of the data.",
      "startOffset" : 69,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "We take as our problem to tune the GMM's parameters λ in order to maximize this score (Kappen and Rodríguez, 1997),",
      "startOffset" : 86,
      "endOffset" : 114
    }, {
      "referenceID" : 13,
      "context" : "Our pilot study of generalized belief propagation approximations in GMMs (Yedidia et al., 2001) has indeed yielded positive results, and will be incorporated in future reports.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "The logOP is the only pool known to preserve independence structure in graphical models (Pennock and Wellman, 2005), which is an important property in our context.",
      "startOffset" : 88,
      "endOffset" : 115
    } ],
    "year" : 2008,
    "abstractText" : "A graphical multiagent model (GMM) represents a joint distribution over the behavior of a set of agents. One source of knowledge about agents' behavior may come from gametheoretic analysis, as captured by several graphical game representations developed in recent years. GMMs generalize this approach to express arbitrary distributions, based on game descriptions or other sources of knowledge bearing on beliefs about agent behavior. To illustrate the exibility of GMMs, we exhibit game-derived models that allow probabilistic deviation from equilibrium, as well as models based on heuristic action choice. We investigate three di erent methods of integrating these models into a single model representing the combined knowledge sources. To evaluate the predictive performance of the combined model, we treat as actual outcome the behavior produced by a reinforcement learning process. We nd that combining the two knowledge sources, using any of the methods, provides better predictions than either source alone. Among the combination methods, mixing data outperforms the opinion pool and direct update methods investigated in this empirical trial.",
    "creator" : "TeX"
  }
}