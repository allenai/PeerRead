{
  "name" : "1405.0999.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "KR3: An Architecture for Knowledge Representation and Reasoning in Robotics",
    "authors" : [ "Shiqi Zhang", "Mohan Sridharan", "Michael Gelfond", "Jeremy Wyatt" ],
    "emails" : [ "shiqi.zhang6@gmail.com", "mohan.sridharan@ttu.edu", "michael.gelfond@ttu.edu", "jlw@cs.bham.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Mobile robots deployed in complex domains receive far more raw data from sensors than is possible to process in real-time, and may have incomplete domain knowledge. Furthermore, the descriptions of knowledge and uncertainty obtained from different sources may complement or contradict each other, and may have different degrees of relevance to current or future tasks. Widespread use of robots thus poses fundamental knowledge representation and reasoning challenges—robots need to represent, learn from, and reason with, qualitative and quantitative descriptions of knowledge and uncertainty. Towards this objective, our architecture combines the knowledge representation and non-monotonic logical reasoning capabilities of declarative programming with the uncertainty modeling capabilities of probabilistic graphical models. The architecture consists of two tightly coupled levels and has the following key features:\n1. An action language is used for the HL and LL system descriptions and the definition of recorded history is expanded in the HL to allow prioritized defaults. 2. For any assigned objective, tentative plans are created in the HL using default knowledge and commonsense reasoning, and implemented in the LL using probabilistic algorithms, with the corresponding observations adding suitable statements to the HL history. 3. For each HL action, abstraction and tight coupling between the LL and HL system descriptions enables automatic selection of relevant variables and generation of a suitable action policy in the LL.\nIn this paper, the HL domain representation is translated into an Answer Set Prolog (ASP) program, while the LL domain representation is translated into partially observable Markov decision processes (POMDPs). The novel contributions of the architecture, e.g., allowing histories with prioritized defaults, tight coupling between the two levels, and the resultant automatic selection of the relevant variables in the LL, support reasoning with violation of defaults, noisy observations and unreliable actions in large and complex domains. The architecture is grounded and evaluated in simulation and on physical robots moving objects in indoor domains."
    }, {
      "heading" : "2 Related Work",
      "text" : "Probabilistic graphical models such as POMDPs have been used to represent knowledge and plan sensing, navigation and interaction for robots (Hoey et al. 2010; Rosenthal and Veloso 2012). However, these formulations (by themselves) make it difficult to perform commonsense reasoning, e.g., default reasoning and non-monotonic logical reasoning, especially with information not directly relevant to tasks at hand. In parallel, research in classical planning has provided many algorithms for knowledge representation and logical reasoning (Ghallab, Nau, and Traverso 2004), but these algorithms require substantial prior knowledge about the domain, task and the set of actions. Many of these algorithms also do not support merging of new, unreliable information from sensors and humans with the cur-\nar X\niv :1\n40 5.\n09 99\nv1 [\ncs .A\nI] 5\nM ay\nrent beliefs in a knowledge base. Answer Set Programming (ASP), a non-monotonic logic programming paradigm, is well-suited for representing and reasoning with commonsense knowledge (Gelfond 2008; Baral 2003). An international research community has been built around ASP, with applications such as reasoning in simulated robot housekeepers and for representing knowledge extracted from natural language human-robot interaction (Chen et al. 2012; Erdem, Aker, and Patoglu 2012). However, ASP does not support probabilistic analysis, whereas a lot of information available to robots is represented probabilistically to quantitatively model the uncertainty in sensor input processing and actuation in the real world.\nResearchers have designed cognitive architectures (Laird, Newell, and Rosenbloom 1987; Langley and Choi 2006; Talamadupula et al. 2010), and developed algorithms that combine deterministic and probabilistic algorithms for task and motion planning on robots (Kaelbling and Lozano-Perez 2013; Hanheide et al. 2011). Recent work has also integrated ASP and POMDPs for non-monotonic logical inference and probabilistic planning on robots (Zhang, Sridharan, and Bao 2012). Some examples of principled algorithms developed to combine logical and probabilistic reasoning include probabilistic first-order logic (Halpern 2003), first-order relational POMDPs (Sanner and Kersting 2010), Markov logic network (Richardson and Domingos 2006), Bayesian logic (Milch et al. 2006), and a probabilistic extension to ASP (Baral, Gelfond, and Rushton 2009). However, algorithms based on first-order logic for probabilistically modeling uncertainty do not provide the desired expressiveness for capabilities such as default reasoning, e.g., it is not always possible to express uncertainty and degrees of belief quantitatively. Other algorithms based on logic programming that support probabilistic reasoning do not support one or more of the desired capabilities: reasoning as in causal Bayesian networks; incremental addition of probabilistic information; reasoning with large probabilistic components; and dynamic addition of variables with different ranges (Baral, Gelfond, and Rushton 2009). The architecture described in this paper is a step towards achieving these capabilities. It exploits the complementary strengths of declarative programming and probabilistic graphical models to represent, reason with, and learn from qualitative and quantitative descriptions of knowledge and uncertainty, enabling robots to automatically plan sensing and actuation in larger domains than was possible before."
    }, {
      "heading" : "3 KRR Architecture",
      "text" : "This section describes our architecture’s HL and LL domain representations. The syntax, semantics and representation of the corresponding transition diagrams are described in an action language AL (Gelfond and Kahl 2014). Action languages are formal models of parts of natural language used for describing transition diagrams. AL has a sorted signature containing three sorts: statics, fluents and actions. Statics are domain properties whose truth values cannot be changed by actions, while fluents are properties whose truth values are changed by actions. Actions are defined as a set of elementary actions that can be executed in parallel. A do-\nmain property p or its negation ¬p is a domain literal. AL allows three types of statements:\na causes lin if p0, . . . , pm (Causal law) l if p0, . . . , pm (State constraint) impossible a0, . . . ,ak if p0, . . . , pm\n(Executability condition)\nwhere a is an action, l is a literal, lin is a inertial fluent literal, and p0, . . . , pm are domain literals. The causal law states, for instance, that action a causes inertial fluent literal lin if the literals p0, . . . , pm hold true. A collection of statements of AL forms a system/domain description.\nAs an illustrative example used throughout this paper, we will consider a robot that has to move objects to specific places in an indoor domain. The domain contains four specific places: office, main library, aux library, and kitchen, and a number of specific objects of the sorts: textbook, printer and kitchenware."
    }, {
      "heading" : "3.1 HL domain representation",
      "text" : "The HL domain representation consists of a system description DH and histories with defaults H . DH consists of a sorted signature and axioms used to describe the HL transition diagram τH . The sorted signature: ΣH = 〈O,F ,P〉 is a tuple that defines the names of objects, functions, and predicates available for use in the HL. The sorts in our example are: place, thing, robot, and object; object and robot are subsorts of thing. Robots can move on their own, but objects cannot move on their own. The sort object has subsorts such as textbook, printer and kitchenware. The fluents of the domain are defined in terms of their arguments:\nloc(thing, place) (1) in hand(robot,ob ject)\nThe first predicate states the location of a thing; and the second predicate states that a robot has an object.These two predicates are inertial fluents subject to the law of inertia, which can be changed by an action. The actions in this domain include:\nmove(robot, place) (2) grasp(robot,ob ject) putdown(robot,ob ject)\nThe dynamics of the domain are defined using the following causal laws:\nmove(robot,Pl) causes loc(robot,Pl) (3) grasp(robot,Ob) causes in hand(robot,Ob) putdown(robot,Ob) causes ¬in hand(robot,Ob)\nstate constraints:\nloc(Ob,Pl) if loc(robot,Pl), in hand(robot,Ob) (4) ¬loc(T h,Pl1) if loc(T h,Pl2), Pl1 6= Pl2\nand executability conditions:\nimpossible move(robot,Pl) if loc(robot,Pl) (5) impossible A1, A2, if A1 6= A2. impossible grasp(robot,Ob) if loc(robot,Pl1),\nloc(Ob,Pl2),Pl1 6= Pl2 impossible grasp(robot,Ob) if in hand(robot,Ob) impossible putdown(robot,Ob) if ¬in hand(robot,Ob)\nThe top part of Figure 1 shows some state transitions in the HL; nodes include a subset of fluents (robot’s position) and actions are the arcs between nodes. Although DH does not include the costs of executing actions, these are included in the LL (see Section 3.2).\nHistories with defaults A recorded history of a dynamic domain is usually defined as a collection of records of the form obs( f luent,boolean,step) and hpd(action,step). The former states that a specific fluent was observed to be true or false at a given step of the domain’s trajectory, and the latter states that a specific action happened (or was executed by the robot) at that step. In this paper, we expand on this view by allowing histories to contain (possibly prioritized) defaults describing the values of fluents in their initial states. A default d(X) stating that in the typical initial state elements of class c satisfying property b also have property p is represented as:\nd(X) =  de f ault(d(X)) head(d(X), p(X)) body(d(X),c(X)) body(d(X),b(X))\n(6)\nwhere the literal in the “head” of the default, e.g., p(X) is true if all the literals in the “body” of the default, e.g., b(X) and c(X), hold true; see (Gelfond and Kahl 2014) for formal semantics of defaults. In this paper, we abbreviate obs( f , true,0) and obs( f , f alse,0) as init( f , true) and init( f , f alse) respectively.\nExample 1 [Example of defaults] Consider the following statements about the locations of textbooks in the initial state in our illustrative example. Textbooks are typically in the main library. If a textbook is not there, it is in the auxiliary library. If a textbook is checked out, it can be found in the office. These defaults can be represented as:\nde f ault(d1(X)) head(d1(X), loc(X ,main library)) body(d1(X), textbook(X))\n(7)\nde f ault(d2(X)) head(d2(X), loc(X ,aux library)) body(d2(X), textbook(X)) body(d2(X),¬loc(X ,main library))\n(8)\nde f ault(d3(X)) head(d3(X), loc(X ,o f f ice)) body(d3(X), textbook(X)) body(d3(X),¬loc(X ,main library)) body(d3(X),¬loc(X ,aux library))\n(9)\nA default such as “kitchenware are usually in the kitchen” may be represented in a similar manner. We first present multiple informal examples to illustrate reasoning with these defaults; Definition 3 (below) will formalize this reasoning. For textbook tb1, history H1 containing the above statements should entail: holds(loc(tb1,main library),0). A history H2 obtained from H1 by adding an observation: init(loc(tb1,main library), f alse) renders the first default inapplicable; hence H2 should entail: holds(loc(tb1,aux library),0). A history H3 obtained from H2 by adding an observation: init(loc(tb1,aux library), f alse) entails: holds(loc(tb1,o f f ice),0).\nConsider history H4 obtained by adding observation: obs(loc(tb1,main library), f alse,1) to H1. This observation should defeat the default d1 in Equation 7 because if this default’s conclusion were true in the initial state, it would also be true at step 1 (by inertia), which contradicts our observation. The book tb1 is thus not in the main library initially. The second default will conclude that this book is initially in the auxiliary library—the inertia axiom will propagate this information and H4 will entail: holds(loc(tb1,aux library),1).\nThe definition of entailment relation can now be given with respect to a fixed system description DH . We start with the notion of a state of transition diagram τH of DH compatible with a description I of the initial state of history H . We use the following terminology. We say that a set S of literals is closed under a default d if S contains the head of d whenever it contains all literals from the body of d and does not contain the literal contrary to d’s head. S is closed under a constraint of DH if S contains the constraint’s head whenever it contains all literals from the constraint’s body. Finally, we say that a set U of literals is the closure of S if S ⊆U , U is closed under constraints of DH and defaults of H , and no proper subset of U satisfies these properties.\nDefinition 1 [Compatible initial states] A state σ of τH is compatible with a description I of the initial state of history H if:\n1. σ satisfies all observations of I , 2. σ contains the closure of the union of statics of DH and the set { f : init( f , true) ∈ I } ∪ {¬ f : init( f , f alse) ∈I }.\nLet Ik be the description of the initial state of history Hk. States in Example 1 compatible with I1, I2, I3 must then contain {loc(tb1,main library)}, {loc(tb1,aux library)}, and {loc(tb1,o f f ice)} respectively. There are multiple such states, which differ by the location of robot. Since I1 = I4 they have the same compatible states. Next, we define models of history H , i.e., paths of the transition diagram τH of DH compatible with H .\nDefinition 2 [Models] A path P of τH is a model of history H with description I of its initial state if there is a collection E of init statements such that:\n1. If init( f , true) ∈ E then ¬ f is the head of one of the defaults of I . Similarly, for init( f , f alse).\n2. The initial state of P is compatible with the description: IE = I ∪E.\n3. Path P satisfies all observations in H . 4. There is no collection E0 of init statements which has less elements than E and satisfies the conditions above.\nWe will refer to E as an explanation of H . Models of H1, H2, and H3 are paths consisting of initial states compatible with I1, I2, and I3—the corresponding explanations are empty. However, in the case of H4, the situation is different—the predicted location of tb1 will be different from the observed one. The only explanation of this discrepancy is that tb1 is an exception to the first default. Adding E = {init(loc(tb1,main library), f alse)} to I4 will resolve the problem.\nDefinition 3 [Entailment and consistency] • Let H n be a history of length n, f be a fluent, and\n0≤ i≤ n be a step of H n. We say that H n entails a statement Q = holds( f , i) (¬holds( f , i)) if for every model P of H n, fluent literal f (¬ f ) belongs to the ith state of P. We denote the entailment as H n |= Q. • A history which has a model is said to be consistent.\nIt can be shown that histories from Example 1 are consistent and that our entailment captures the corresponding intuition.\nReasoning with HL domain representation The HL domain representation (DH and H ) is translated into a program in CR-Prolog, which incorporates consistency restoring rules in ASP (Balduccini and Gelfond 2003; Gelfond and Kahl 2014); specifically, we use the knowledge representation language SPARC that expands CR-Prolog and provides explicit constructs to specify objects, relations, and their sorts (Balai, Gelfond, and Zhang 2013). ASP is a declarative language that can represent recursive definitions, defaults, causal relations, special forms of self-reference, and other language constructs that occur frequently in nonmathematical domains, and are difficult to express in classical logic formalisms (Baral 2003). ASP is based on the stable model semantics of logic programs, and builds on research in non-monotonic logics (Gelfond 2008). A CRProlog program is thus a collection of statements describing domain objects and relations between them. The ground literals in an answer set obtained by solving the program represent beliefs of an agent associated with the program1; program consequences are statements that are true in all such belief sets. Algorithms for computing the entailment relation of AL and related tasks such as planning and diagnostics are thus based on reducing these tasks to computing answer sets of programs in CR-Prolog. First, DH and H are translated into an ASP program Π(DH ,H ) consisting of direct translation of causal laws of DH , inertia axioms, closed world assumption for defined fluents, reality checks, records of observations, actions and defaults from H , and special axioms for init:\nholds(F,0)← init(F, true) (10) ¬holds(F,0)← init(F, f alse)\n1SPARC uses DLV (Leone et al. 2006) to generate answer sets.\nIn addition, every default of I is turned into an ASP rule:\nholds(p(X),0)←c(X), holds(b(X),0), (11) not ¬holds(p(X),0)\nand a consistency-restoring rule:\n¬holds(p(X),0) +←c(X), holds(b(X),0) (12)\nwhich states that to restore consistency of the program one may assume that the conclusion of the default is false. For more details about the translation, CR-rules and CR-Prolog, please see (Gelfond and Kahl 2014).\nProposition 1 [Models and Answer Sets] A path P = 〈σ0,a0,σ1, . . . ,σn−1,an〉 of τH is a model of history H n iff there is an answer set S of a program Π(DH ,H ) such that:\n1. A fluent f ∈ σi iff holds( f , i) ∈ S, 2. A fluent literal ¬ f ∈ σi iff ¬holds( f , i) ∈ S, 3. An action e ∈ ai iff occurs(e, i) ∈ S.\nThe proposition reduces computation of models of H to computing answer sets of a CR-Prolog program. This proposition allows us to reduce the task of planning to computing answer sets of a program obtained from Π(DH ,H ) by adding the definition of a goal, a constraint stating that the goal must be achieved, and a rule generating possible future actions of the robot."
    }, {
      "heading" : "3.2 LL domain representation",
      "text" : "The LL system description DL consists of a sorted signature and axioms that describe a transition diagram τL. The sorted signature ΣL of action theory describing τL includes the sorts from signature ΣH of HL with two additional sorts room and cell, which are subsorts of sort place. Their elements satisfy the static relation part of(cell, room). We also introduce the static neighbor(cell, cell) to describe neighborhood relation between cells. Fluents of ΣL include those of ΣH , an additional inertial fluent: searched(cell, object)— robot searched a cell for an object—and two defined fluents: found(object, place)—an object was found in a place—and continue search(room, object)—the search for an object is continued in a room.\nThe actions of ΣL include the HL actions that are viewed as being represented at a higher resolution, e.g., movement is possible to specific cells. The causal law describing the effect of move may be stated as:\nmove(robot,Y ) causes {loc(robot,Z) : neighbor(Z,Y )} (13)\nwhere Y,Z are cells. This causal law states that moving to a cell can cause the robot to be in one of the neighboring cells2. The LL includes an additional action search that enables robots to search for objects in cells; the corresponding\n2This is a special case of a non-deterministic causal law defined in extensions of AL with non-boolean fluents, i.e., functions whose values can be elements of arbitrary finite domains.\ncausal laws and constraints may be written as:\nsearch(cell,ob ject) causes searched(cell,ob ject) (14) f ound(ob ject,cell) if searched(cell,ob ject),\nloc(ob ject,cell) f ound(ob ject,room) if part o f (cell,room),\nf ound(ob ject,cell) continue search(room,ob ject) if ¬ f ound(ob ject,room),\npart o f (cell,room),¬searched(cell,ob ject) We also introduce a defined fluent failure that holds iff the object under consideration is not in the room that the robot is searching—this fluent is defined as:\nf ailure(ob ject,room) if loc(robot,room), (15) ¬continue search(room,ob ject),¬ f ound(ob ject,room)\nThis completes the action theory that describes τL. The states of τL can be viewed as extensions of states of τH by physically possible fluents and statics defined in the language of LL. Moreover, for every HL state-action-state transition 〈σ ,a,σ ′〉 and every LL state s compatible with σ (i.e., σ ⊂ s), there is a path in the LL from s to some state compatible with σ ′.\nUnlike the HL system description in which effects of actions and results of observations are always accurate, the action effects and observations in the LL are only known with some degree of probability. The state transition function T : S×A× S′→ [0,1] defines the probabilities of state transitions in the LL. Due to perceptual limitations of the robot, only a subset of the fluents are observable in the LL; we denote this set of fluents by Z. Observations are elements of Z associated with a probability, and are obtained by processing sensor inputs using probabilistic algorithms. The observation function O : S×Z→ [0,1] defines the probability of observing specific observable fluents in specific states. Functions T and O are computed using prior knowledge, or by observing the effects of specific actions in specific states (see Section 4.1).\nStates are partially observable in the LL, and we introduce (and reason with) belief states, probability distributions over the set of states. Functions T and O describe a probabilistic transition diagram defined over belief states. The initial belief state is represented by B0, and is updated iteratively using Bayesian inference:\nBt+1(st+1) ∝ O(st+1,ot+1)∑ s T (s,at+1,st+1) ·Bt(s) (16)\nThe LL system description includes a reward specification R : S×A×S′→ℜ that encodes the relative cost or value of taking specific actions in specific states. Planning in the LL then involves computing a policy that maximizes the reward over a planning horizon. This policy maps belief states to actions: π : Bt 7→ at+1. We use a point-based approximate algorithm to compute this policy (Ong et al. 2010). In our illustrative example, an LL policy computed for HL action move is guaranteed to succeed, and that the LL policy computed for HL action grasp considers three LL actions: move, search, and grasp. Plan execution in the LL corresponds to using the computed policy to repeatedly choose an action in the current belief state, and updating the belief state after executing that action and receiving an observation. We henceforth refer to this algorithm as “POMDP-1”.\nUnlike the HL, history in the LL representation consists of observations and actions over one time step; the current belief state is assumed to be the result of all information obtained in previous time steps (first-order Markov assumption). In this paper, the LL domain representation is translated automatically into POMDP models, i.e., specific data structures for representing the components of DL (described above) such that existing POMDP solvers can be used to obtain action policies.\nWe observe that the coupling between the LL and the HL has some key consequences. First, for any HL action, the relevant LL variables are identified automatically, improving the computational efficiency of computing the LL policies. Second, if LL actions cause different fluents, these fluents are independent. Finally, although defined fluents are crucial in determining what needs to be communicated between the levels of the architecture, they themselves need not be communicated."
    }, {
      "heading" : "3.3 Control loop",
      "text" : "Algorithm 1 describes the architecture’s control loop3. First, the LL observations obtained in the current location add statements to the HL history, and the HL initial state (sHinit ) is communicated to the LL (line 1). The assigned task determines the HL goal state (sHgoal) for planning (line 2). Planning in the HL provides a sequence of actions with deterministic effects (line 3).\nIn some situations, planning in the HL may provide multiple plans, e.g., when the object that is to be grasped can be in one of multiple locations, tentative plans may be generated for the different hypotheses regarding the object’s location. In such situations, all the HL plans are communicated to the\n3We leave the proof of the correctness of this algorithm as future work.\nAlgorithm 1: Control loop of architecture Input: The HL and LL domain representations, and the\nspecific task for robot to perform.\n1 LL observations reported to HL history; HL initial state (sHinit ) communicated to LL. 2 Assign goal state sHgoal based on task. 3 Generate HL plan(s). 4 if multiple HL plans exist then 5 Send plans to the LL, select plan with lowest\n(expected) action cost and communicate to the HL.\n6 end 7 if HL plan exists then 8 for aHi ∈ HL plan: i ∈ [1,n] do 9 Pass aHi and relevant fluents to LL.\n10 Determine initial belief state over the relevant LL state space. 11 Generate LL action policy. 12 while aHi not completed and aHi achievable do 13 Execute an action based on LL action policy. 14 Make an LL observation and update belief state. 15 end 16 LL observations and action outcomes add statements to HL history. 17 if results unexpected then 18 Perform diagnostics in HL. 19 end 20 if HL plan invalid then 21 Replan in the HL (line 3). 22 end 23 end 24 end\nLL and compared based on their costs, e.g., the expected time to execute the plans. The plan with the least expected cost is communicated to the HL (lines 4-6).\nIf an HL plan exists, actions are communicated one at a time to the LL along with the relevant fluents (line 9). For HL action aHi , the communicated fluents are used to automatically identify the relevant LL variables and set the initial belief state, e.g., a uniform distribution (line 10). An LL action policy is computed (line 11) and used to execute actions and update the belief state until aHi is achieved or inferred to be unachievable (lines 12-15). The outcome of executing the LL policy, and the LL observations, add to the HL history (line 16). For instance, if defined fluent failure is true for object ob1 and room rm1, the robot reports: obs(loc(ob1,rm1), f alse) to the HL history. If the results are unexpected, diagnosis is performed in the HL (lines 17- 19); we assume that the robot is capable of identifying these unexpected outcomes. If the HL plan is invalid, a new plan is generated (lines 20-22); else, the next action in the HL plan is executed."
    }, {
      "heading" : "4 Experimental setup and results",
      "text" : "This section describes the experimental setup and results of evaluating the proposed architecture in indoor domains."
    }, {
      "heading" : "4.1 Experimental setup",
      "text" : "The architecture was evaluated in simulation and on physical robots. To provide realistic observations in the simulator, we included object models that characterize objects using probabilistic functions of features extracted from images captured by a camera on physical robots (Li and Sridharan 2013). The simulator also uses action models that reflect the motion of the robot. Specific instances of objects of different classes were simulated in a set of rooms. The experimental setup also included an initial training phase in which the robot repeatedly executed the different movement actions and applied the visual input processing algorithms on images with known objects. A human participant provided some of the ground truth data, e.g., labels of objects in images. A comparison of the expected and actual outcomes was used to define the functions that describe the probabilistic transition diagram (T , O) in the LL, while the reward specification is defined by also considering the computational time required by different visual processing and navigation algorithms.\nIn each trial of the experimental results summarized below, the robot’s goal is to move specific objects to specific places; the robot’s location, target object, and locations of objects are chosen randomly in each trial. A sequence of actions extracted from an answer set obtained by solving the SPARC program of the HL domain representation provides an HL plan. If a robot (robot1) that is in the office is asked to fetch a textbook (tb1) from the main library, the HL plan consists of the following sequence of actions:\nmove(robot1,main library) grasp(robot1, tb1) move(robot1,o f f ice) putdown(robot1, tb1)\nThe LL action policies for each HL action are generated by solving the appropriate POMDP models using the APPL solver (Ong et al. 2010; Somani et al. 2013). In the LL, the location of an object is considered to be known with certainty if the belief (of the object’s occurrence) in a grid cell exceeds a threshold (0.85).\nWe experimentally compared our architecture, with the control loop described in Algorithm 1, henceforth referred to as “PA”, with two alternatives: (1) POMDP-1 (see Section 3.2); and (2) POMDP-2, which revises POMDP-1 by assigning high probability values to defaults to bias the initial belief states. These comparisons evaluate two hypotheses: (H1) PA enables a robot to achieve the assigned goals more reliably and efficiently than using POMDP-1; (H2) our representation of defaults improves reliability and efficiency in comparison with not using default knowledge or assigning high probability values to defaults."
    }, {
      "heading" : "4.2 Experimental Results",
      "text" : "To evaluate H1, we first compared PA with POMDP-1 in a set of trials in which the robot’s initial position is known but the position of the object to be moved is unknown. The solver used in POMDP-1 is given a fixed amount of time to compute action policies. Figure 2 summarizes the ability to successfully achieve the assigned goal, as a function of the number of cells in the domain. Each point in Figure 2 is the average of 1000 trials, and we set (for ease of interpretation) each room to have four cells. PA significantly improves the robot’s ability to achieve the assigned goal in comparison with POMDP-1. As the number of cells (i.e., size of the domain) increases, it becomes computationally difficult to generate good POMDP action policies which, in conjunction with incorrect observations (e.g., false positive sightings of objects) significantly impacts the ability to successfully complete the trials. PA, on the other hand, focuses the robot’s attention on relevant regions of the domain (e.g., specific rooms and cells). As the size of the domain increases, a large number of plans of similar cost may still be generated which, in conjunction with incorrect observations, may affect the robot’s ability to successfully complete the trials—the impact is, however, much less pronounced.\nNext, we computed the time taken by PA to generate a plan as the size of the domain increases. Domain size is characterized based on the number of rooms and the number of objects in the domain. We conducted three sets of experiments in which the robot reasons with: (1) all available knowledge of domain objects and rooms; (2) only knowledge relevant to the assigned goal—e.g., if the robot knows an object’s default location, it need not reason about other objects and rooms in the domain to locate this object; and (3) relevant knowledge and knowledge of an additional 20% of randomly selected domain objects and rooms. Figure 3 summarizes these results. We observe that PA supports the generation of appropriate plans for domains with a large number of rooms and objects. We also observe that using only the knowledge relevant to the goal significantly reduces the planning time—such knowledge can\nbe automatically selected using the relationships included in the HL system description. Furthermore, if we only use a probabilistic approach (POMDP-1), it soon becomes computationally intractable to generate a plan for domains with many objects and rooms; these results are not shown in Figure 3—see (Sridharan, Wyatt, and Dearden 2010; Zhang, Sridharan, and Washington 2013).\nTo evaluate H2, we first conducted multiple trials in which PA was compared with PA∗, a version that does not include any default knowledge. Figure 4 summarizes the average number of actions executed per trial as a function of the number of rooms in the domain—each sample point is the average of 10000 trials. The goal in each trial is (as before) to move a specific object to a specific place. We observe that the principled use of default knowledge significantly reduces the number of actions (and thus time) required to achieve the assigned goal. Next PA was compared with POMDP-2, which assigns high probability values to default information and suitably revises the initial belief state. We observe that the effect of assigning a probability value to defaults is arbitrary depending on multiple factors: (a) the numerical value chosen; and (b) whether the ground truth matches the default\ninformation. For instance, if a large probability is assigned to the default knowledge that books are typically in the library, but the book the robot has to move is an exception to the default (e.g., a cookbook), it takes a significantly large amount of time for POMDP-2 to revise (and recover from) the initial belief. PA, on the other hand, enables the robot to revise initial defaults and encode exceptions to defaults.\nRobot Experiments: In addition to the trials in simulated domains, we compared PA with POMDP-1 on a wheeled robot over 50 trials conducted on two floors of our department building. This domain includes places in addition to those included in our illustrative example, e.g., Figure 5(a) shows a subset of the domain map of the third floor of our department, and Figure 5(b) shows the wheeled robot platform. Such domain maps are learned by the robot using laser range finder data, and revised incrementally over time. Manipulation by physical robots is not a focus of this work. Therefore, once the robot is next to the desired object, it currently asks for the object to be placed in the extended gripper; future work will include existing probabilistic algorithms for manipulation in the LL.\nFor experimental trials on the third floor, we considered 15 rooms, which includes faculty offices, research labs, common areas and a corridor. To make it feasible to use POMDP-1 in such large domains, we used our prior work on a hierarchical decomposition of POMDPs for visual sensing and information processing that supports automatic belief propagation across the levels of the hierarchy and model generation in each level of the hierarchy (Sridharan, Wyatt, and Dearden 2010; Zhang, Sridharan, and Washington 2013). The experiments included paired trials, e.g., over 15 trials (each), POMDP-1 takes 1.64 as much time as PA (on average) to move specific objects to specific places. For these paired trials, this 39% reduction in execution time provided by PA is statistically significant: p-value = 0.0023 at the 95% significance level.\nConsider a trial in which the robot’s objective is to bring a specific textbook to the place named study corner. The\nrobot uses default knowledge to create an HL plan that causes the robot to move to and search for the textbook in the main library. When the robot does not find this textbook in the main library after searching using a suitable LL policy, replanning in the HL causes the robot to investigate the aux library. The robot finds the desired textbook in the aux library and moves it to the target location. A video of such an experimental trial can be viewed online: http://youtu.be/8zL4R8te6wg"
    }, {
      "heading" : "5 Conclusions",
      "text" : "This paper described a knowledge representation and reasoning architecture for robots that integrates the complementary strengths of declarative programming and probabilistic graphical models. The system descriptions of the tightly coupled high-level (HL) and low-level (LL) domain representations are provided using an action language, and the HL definition of recorded history is expanded to allow prioritized defaults. Tentative plans created in the HL using defaults and commonsense reasoning are implemented in the LL using probabilistic algorithms, generating observations that add suitable statements to the HL history. In the context of robots moving objects to specific places in indoor domains, experimental results indicate that the architecture supports knowledge representation, non-monotonic logical inference and probabilistic planning with qualitative and quantitative descriptions of knowledge and uncertainty, and scales well as the domain becomes more complex. Future work will further explore the relationship between the HL and LL transition diagrams, and investigate a tighter coupling of declarative logic programming and probabilistic reasoning for robots."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors thank Evgenii Balai for making modifications to SPARC to support some of the experiments reported in this paper. This research was supported in part by the U.S. Office of Naval Research (ONR) Science of Autonomy Award\nN00014-13-1-0766. Opinions, findings, and conclusions are those of the authors and do not necessarily reflect the views of the ONR."
    } ],
    "references" : [ {
      "title" : "Towards Answer Set Programming with Sorts",
      "author" : [ "Gelfond Balai", "E. Zhang 2013] Balai", "M. Gelfond", "Y. Zhang" ],
      "venue" : "In International Conference on Logic Programming and Nonmonotonic Reasoning",
      "citeRegEx" : "Balai et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Balai et al\\.",
      "year" : 2013
    }, {
      "title" : "and Gelfond",
      "author" : [ "M. Balduccini" ],
      "venue" : "M.",
      "citeRegEx" : "Balduccini and Gelfond 2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Probabilistic Reasoning with Answer Sets. Theory and Practice of Logic Programming 9(1):57–144",
      "author" : [ "Gelfond Baral", "C. Rushton 2009] Baral", "M. Gelfond", "N. Rushton" ],
      "venue" : null,
      "citeRegEx" : "Baral et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Baral et al\\.",
      "year" : 2009
    }, {
      "title" : "Toward Open Knowledge Enabling for Human-Robot Interaction",
      "author" : [ "Chen" ],
      "venue" : "Journal of Human-Robot Interaction",
      "citeRegEx" : "Chen,? \\Q2012\\E",
      "shortCiteRegEx" : "Chen",
      "year" : 2012
    }, {
      "title" : "Answer Set Programming for Collaborative Housekeeping Robotics: Representation, Reasoning, and Execution",
      "author" : [ "Aker Erdem", "E. Patoglu 2012] Erdem", "E. Aker", "V. Patoglu" ],
      "venue" : "Intelligent Service Robotics",
      "citeRegEx" : "Erdem et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Erdem et al\\.",
      "year" : 2012
    }, {
      "title" : "and Kahl",
      "author" : [ "M. Gelfond" ],
      "venue" : "Y.",
      "citeRegEx" : "Gelfond and Kahl 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Automated Planning: Theory and Practice",
      "author" : [ "Nau Ghallab", "M. Traverso 2004] Ghallab", "D. Nau", "P. Traverso" ],
      "venue" : null,
      "citeRegEx" : "Ghallab et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Ghallab et al\\.",
      "year" : 2004
    }, {
      "title" : "Exploiting Probabilistic Knowledge under Uncertain Sensing for Efficient Robot Behaviour",
      "author" : [ "Hanheide" ],
      "venue" : "In International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Hanheide,? \\Q2011\\E",
      "shortCiteRegEx" : "Hanheide",
      "year" : 2011
    }, {
      "title" : "Automated Handwashing Assistance for Persons with Dementia using Video and a Partially Observable Markov Decision Process. Computer Vision and Image Understanding 114(5):503–519",
      "author" : [ "Hoey" ],
      "venue" : null,
      "citeRegEx" : "Hoey,? \\Q2010\\E",
      "shortCiteRegEx" : "Hoey",
      "year" : 2010
    }, {
      "title" : "and LozanoPerez",
      "author" : [ "L. Kaelbling" ],
      "venue" : "T.",
      "citeRegEx" : "Kaelbling and Lozano.Perez 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "J",
      "author" : [ "Laird" ],
      "venue" : "E.; Newell, A.; and Rosenbloom, P.",
      "citeRegEx" : "Laird. Newell. and Rosenbloom 1987",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "and Choi",
      "author" : [ "P. Langley" ],
      "venue" : "D.",
      "citeRegEx" : "Langley and Choi 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The DLV System for Knowledge Representation and Reasoning",
      "author" : [ "Leone" ],
      "venue" : "ACM Transactions on Computational Logic 7(3):499–562",
      "citeRegEx" : "Leone,? \\Q2006\\E",
      "shortCiteRegEx" : "Leone",
      "year" : 2006
    }, {
      "title" : "and Sridharan",
      "author" : [ "X. Li" ],
      "venue" : "M.",
      "citeRegEx" : "Li and Sridharan 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "D",
      "author" : [ "B. Milch", "B. Marthi", "S. Russell", "D. Sontag", "Ong" ],
      "venue" : "L.; and Kolobov, A.",
      "citeRegEx" : "Milch et al. 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "W",
      "author" : [ "S.C. Ong", "S.W. Png", "D. Hsu", "Lee" ],
      "venue" : "S.",
      "citeRegEx" : "Ong et al. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "and Domingos",
      "author" : [ "M. Richardson" ],
      "venue" : "P.",
      "citeRegEx" : "Richardson and Domingos 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "and Veloso",
      "author" : [ "S. Rosenthal" ],
      "venue" : "M.",
      "citeRegEx" : "Rosenthal and Veloso 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "and Kersting",
      "author" : [ "S. Sanner" ],
      "venue" : "K.",
      "citeRegEx" : "Sanner and Kersting 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "W",
      "author" : [ "A. Somani", "N. Ye", "D. Hsu", "Lee" ],
      "venue" : "S.",
      "citeRegEx" : "Somani et al. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Planning to See: A Hierarchical Aprroach to Planning Visual Actions on a Robot using POMDPs",
      "author" : [ "Wyatt Sridharan", "M. Dearden 2010] Sridharan", "J. Wyatt", "R. Dearden" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "Sridharan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sridharan et al\\.",
      "year" : 2010
    }, {
      "title" : "Planning for Human-Robot Teaming in Open Worlds",
      "author" : [ "Talamadupula" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology",
      "citeRegEx" : "Talamadupula,? \\Q2010\\E",
      "shortCiteRegEx" : "Talamadupula",
      "year" : 2010
    }, {
      "title" : "F",
      "author" : [ "S. Zhang", "M. Sridharan", "Bao" ],
      "venue" : "S.",
      "citeRegEx" : "Zhang. Sridharan. and Bao 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Active Visual Planning for Mobile Robot Teams using Hierarchical POMDPs",
      "author" : [ "Sridharan Zhang", "S. Washington 2013] Zhang", "M. Sridharan", "C. Washington" ],
      "venue" : "IEEE Transactions on Robotics",
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "This paper describes an architecture that combines the complementary strengths of declarative programming and probabilistic graphical models to enable robots to represent, reason with, and learn from, qualitative and quantitative descriptions of uncertainty and knowledge. An action language is used for the low-level (LL) and high-level (HL) system descriptions in the architecture, and the definition of recorded histories in the HL is expanded to allow prioritized defaults. For any given goal, tentative plans created in the HL using default knowledge and commonsense reasoning are implemented in the LL using probabilistic algorithms, with the corresponding observations used to update the HL history. Tight coupling between the two levels enables automatic selection of relevant variables and generation of suitable action policies in the LL for each HL action, and supports reasoning with violation of defaults, noisy observations and unreliable actions in large and complex domains. The architecture is evaluated in simulation and on physical robots transporting objects in indoor domains; the benefit on robots is a reduction in task execution time of 39% compared with a purely probabilistic, but still hierarchical, approach.",
    "creator" : "LaTeX with hyperref package"
  }
}