{
  "name" : "1605.07736.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Multiagent Communication with Backpropagation",
    "authors" : [ "Sainbayar Sukhbaatar", "Arthur Szlam" ],
    "emails" : [ "sainbar@cs.nyu.edu", "aszlam@fb.com", "robfergus@fb.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Introduction Communication is a fundamental aspect of intelligence, enabling agents to behave as a group, rather than a collection of individuals. It is vital for performing complex tasks in real-world environments where each actor has limited capabilities and/or visibility of the world. Practical examples include elevator control [4] and sensor networks [6]; communication is also important for success in robot soccer [26]. In any partially observed environment, the communication between agents is vital to coordinate the behavior of each individual. While the model controlling each agent is typically learned via reinforcement learning [2, 29], the specification and format of the communication is usually pre-determined. For example, in robot soccer, the bots are designed to communicate at each time step their position and proximity to the ball.\nIn this work, we propose a model where cooperating agents learn to communicate amongst themselves before taking actions solely from reward in the environment. Each agent is controlled by a deep feedforward network, which additionally has access to a communication channel carrying a continuous vector at each time step. Through this channel, they receive the summed transmissions of other agents. However, what each agent transmits on the channel is not specified a-priori, being learned instead. Because the communication is continuous, the model can be trained via back-propagation, and thus can be combined with standard single agent RL algorithms. The model is simple, versatile, and scalable, allowing dynamic variation at run time in both the number and type of each agent while avoiding exponential blow-ups in state space and number of agents. This allows it to be applied to a wide range of problems involving partial visibility of the environment, where the agents learn a task-specific communication that aids performance.\n2 Problem Formulation We consider the setting where we have J agents, all cooperating to maximize reward R in some environment. We make the simplifying assumption of full cooperation that each agent receives R, independent of their contribution. In this setting, there is no difference between each agent having its own controller, or viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our controller is a large feed-forward neural network that maps inputs for all agents to their actions, each agent occupying a subset of units. A specific connectivity structure between layers\nar X\niv :1\n60 5.\n07 73\n6v 1\n[ cs\n.L G\n] 2\n5 M\nay 2\n(a) instantiates the broadcast communication channel between agents and (b) propagates the agent state.\nBecause the agents will receive reward, but not necessarily supervision for each action, reinforcement learning is used to maximize expected future reward. We explore two forms of communication within the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and can naturally be handled by reinforcement learning. In the continuous case, the signals passed between agents are no different than hidden states in a neural network; thus credit assignment for the communication can be performed using standard backpropagation (within the outer RL loop). We use policy gradient [36] with a state specific baseline for delivering a gradient to the model. Denote the states in an episode by s(1), ..., s(T ), and the actions taken at each of those states as a(1), ..., a(T ), where T is the length of the episode. The baseline is a scalar function of the states b(s, θ), computed via an extra head on the model producing the action probabilities. Beside maximizing the expected reward with policy gradient, the models are also trained to minimize the distance between the baseline value and actual reward. Thus after finishing an episode, we update the model parameters θ by\n∆θ =\nT∑\nt=1\n ∂ log p(a(t)|s(t), θ)\n∂θ\n( T∑\ni=t\nr(i)− b(s(t), θ) ) − α ∂\n∂θ\n( T∑\ni=t\nr(i)− b(s(t), θ) )2  .\n(1) Here r(t) is reward given at time t, and the hyperparameter α is for balancing the reward and the baseline objectives, which set to 0.03 in all experiments.\n3 Communication Model We now describe the model used to compute p(a(t)|s(t), θ) at a given time t (omitting the time index for brevity). Let sj be the jth agent’s view of the state of the environment. The input to the controller is the concatenation of all state-views s = {s1, ..., sJ}, and the controller Φ is a mapping a = Φ(s), where the output a is a concatenation of discrete actions a = {a1, ..., aJ} for each agent. Note that this single controller Φ encompasses the individual controllers for each agents, as well as the communication between agents.\nOne obvious choice for Φ is a fully-connected multi-layer neural network, which could extract features h from s and use them to predict good actions with our RL framework. This model would allow agents to communicate with each other and share views of the environment. We refer to this type of communication as dense later in the experiments. However, it is inflexible with respect to the composition and number of agents it controls; cannot deal well with agents joining and leaving the group and even the order of the agents must be fixed. On the other hand, if no communication is used then we can write a = {φ(s1), ..., φ(sJ)}, where φ is a per-agent controller applied independently. This communication-free model satisfies the flexibility requirements1, but is not able to coordinate agents’ actions. We refer to this as the none communication model.\n3.1 Controller Structure We now detail our architecture for Φ that has the modularity of the communication-free model but still allows communication. Φ is built from modules f i, which take the form of multilayer neural networks. Here i ∈ {0, ..,K}, where K is the number of communication steps in the network. Each f i takes two input vectors for each agent j: the hidden state hij and the communication c i j , and outputs a vector hi+1j . The main body of the model then takes as input the concatenated vectors h0 = [h01, h 0 2, ..., h 0 J ], and computes:\nhi+1j = f i(hij , c i j) (2)\nci+1j = 1 J − 1 ∑\nj′ 6=j hi+1j′ . (3)\nIn the case that f i is a single linear layer followed by a nonlinearity σ, we have: hi+1j = σ(H ihij + Cicij) and the model can be viewed as a feedforward network with layers h i+1 = σ(T ihi) where hi is the concatenation of all hij and T i takes the block form (where C̄i = Ci/(J − 1)):\n1Assuming sj includes the identity of agent j.\nT i =  Hi C̄i C̄i ... C̄i C̄i Hi C̄i ... C̄i C̄i C̄i Hi ... C̄i\n... ...\n... . . . ... C̄i C̄i C̄i ... Hi\n ,\nConnecting Neural Models\nAnonymous Author(s) Affiliation Address email\nAbstract\nabstract1\n1 Introduction2\nIn this work we make two contributions. First, we simplify and extend the graph neural network3 architecture of ??. Second, we show how this architecture can be used to control groups of cooperating4 agents.5\n2 Model6\nThe simplest form of the model consists of multilayer neural networks f i that take as input vectors7 hi and ci and output a vector hi+1. The model takes as input a set of vectors {h01, h02, ..., h0m}, and8 computes9\nhi+1j = f i(hij , c i j)\n10\nci+1j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j, and i 2 {0, .., K} (we will call K the number of hops in the network).11 If desired, we can take the final hKj and output them directly, so that the model outputs a vector12 corresponding to each input vector, or we can feed them into another network to get a single vector or13 scalar output.14\nIf each f i is a simple linear layer followed by a nonlinearity :15\nhi+1j = (A ihij + B icij),\nthen the model can be viewed as a feedforward network with layers16\nHi+1 = (T iHi),\nwhere T is written in block form17\nT i = 0 BBBB@ Ai Bi Bi ... Bi Bi Ai Bi ... Bi Bi Bi Ai ... Bi\n... ...\n... . . . ... Bi Bi Bi ... Ai\n1 CCCCA .\nThe key idea is that T is dynamically sized, and the matrix can be dynamically sized because the18 blocks are applied by type, rather than by coordinate.19\nSubmitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.\nConnecting Neural Models\nAnonymous Author(s) Affiliation Address email\nAbstract\nabstract1\n1 Introduction2\nIn this work we make two contributions. First, we simplify and extend the graph neural network3 architecture of ??. Second, we show how this architecture can be used to control groups of cooperating4 agents.5\n2 Model6\nThe simplest form of the model consists of multilayer neural networks f i that take as input vectors7 hi and ci and output a vector hi+1. The model takes as input a set of vectors {h01, h02, ..., h0m}, and8 computes9\nhi+1j = f i(hij , c i j)\n10\nci+1j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j, and i 2 {0, .., K} (we will call K the number of hops in the network).11 If desired, we can take the final Kj a d output them directly, so that the model outputs a vector12 corresponding to each input vector, or we can feed them into another network to get a single vector or13 scalar output.14\nIf each i is a simple linear layer follow d by a nonlinearity :15\nhi+1j = (A ihij + B icj),\nthen the model can b viewed as a feedforward network with layers16\nHi+1 = (T iHi),\nwhere T is written in block form17\nT i = 0 BBBB@ Ai Bi Bi ... Bi Ai Bi ... Bi Bi Ai ... B ... ... ...\n. . . Bi Bi Bi ... Ai\n1 CCCCA .\nThe key idea is that T is dynamically sized, and th m trix can be dynamically sized because the18 blocks are applied by type, rather than by coordinate.19\nSubmitted to 29th Conference on Neural I formation Processing System (NIPS 2016). Do not distribute.\nConnecting Neural Models Anonymous Author(s) Affiliation Address email\nAbstract\nabstract1\n1 Introduction2\nIn this work we make two contributions. First, we simplify and extend the graph neural network3 architecture of ??. Second, we show how this architecture can be used to control groups of cooperating4 agents.5\n2 Model6\nThe simplest form of the model consists of multilayer neural networks f i that take as input vectors7 hi and ci and output a vector hi+1. The model takes as input a set of vectors {h01, h02, ..., h0m}, and8 computes9\nhi+1j = f i(hij , c i j)\n10\nci+1j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j, and i 2 {0, .., K} (we will call K the number of hops in the network).11 If desired, we can take the final hKj and output them directly, so that the model outputs a vector12 corresponding to each input vector, or we can feed them into another network to get a single vector or13 scalar output.14\nIf each f i is a simple l near layer followed by a nonlinearity :15\nhi+1j (A ihij + B icij),\nthen the model can be viewed as a feedfor ard etwork wit layers16\nHi+1 = (T iHi),\nwhere T is written in block form17\nT i = 0 BBBB@ Ai Bi Bi ... Bi Bi Ai Bi ... Bi Bi Bi Ai ... Bi\n... ...\n... . . . ... Bi Bi Bi ... Ai\n1 CCCCA .\nThe key idea is that T is dynamically sized, and the matrix can be dynamically sized because the18 blocks are applied by type, rather than by coordinate.19\nSubmitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.\nm an\n2 Problem Formulation33 We consider the setting where we have M agents, all cooperating to maximize reward R in some34 environment. We make the simplifying assumption that each agent receives R, independent of their35 contribution. In this setting, there is no difference between each agent having its own controller, or36 viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our37 controller is a large feed-forward neural network that maps inputs for all agents to their actions, each38 agent occupying a subset of units. A specific connectivity structure between layers (a) instantiates the39 broadcast communication channel between agents and (b) propagates the agent state in the manner of40 an RNN.41 Because the agents will receive reward, but not necessarily supervision for each action, reinforcement42 learning is used to maximize expected future reward. We explore two forms of communication within43 the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and44 will be treated as such by the reinforcement learning. In the continuous case, the signals passed45 between agents are no different than hidden states in a neural network; thus credit assignment for the46 communication can be performed using standard backpropagation (within the outer RL loop).47\nWe use policy gradient [33] with a state specific baseline for delivering a gradient to the model.48 Denote the states in an episode by s(1), ..., s(T ), and the actions taken at each of those states49 as a(1), ..., a(T ), where T is the length of the episode. The baseline is a scalar function of the50 states b(s, ✓), computed via an extra head on the model producing the action probabilities. Beside51 maximizing the expected reward with policy gradient, the models are also trained to minimize the52 distance between the baseline value and actual reward. Thus, after finishing an episode, we update53 the model parameters ✓ by54\n✓ =\nTX\nt=1\n2 4@ log p(a(t)|s(t), ✓)\n@✓\nTX\ni=t\nr(i) b(s(t), ✓) ! ↵ @\n@✓\nTX\ni=t\nr(i) b(s(t), ✓) !23 5 .\nHere r(t) is reward given at time t, and the hyperparameter ↵ is for balancing the reward and the55 baseline objectives, set to 0.03 in all experiments.56\n3 Model57\nWe now describe the m del sed to mpute p(a(t)|s(t), ✓) at a given time t (ommiting the time58 index for brevity). Let sj be the jth agent’s view of the state of the environment. The input to the59 controller is the concatenation of all state-views s = {s1, ..., sJ}, and the controller is a mapping60 a = (s), where the output a is a concatenation of discrete actions a = {a1, ..., aJ} for each agent.61 Note that this single controller encompasses the individual controllers for each agents, as well as62 the communication between agents.63\nOne obvious choice for is a fully-connected multi-layer neural network, which could extract64 features h from s and use them to predict good actions with our RL framework. This model would65 allow agents to communicate with each other and share views of the environment. However, it66 is inflexible with respect to the composition and number of agents it controls; cannot deal well67 with agents joining and leaving the group and even the order of the agents must be fixed. On the68 other hand, if no communication is used then we can write a = { (s1), ..., (sJ)}, where is a69 per-agent controller applied independently. This communication-free model satisfies the flexibility70 requirements1, but is not able to coordinate their actions.71\n3.1 Controller Structure72\nWe now detail the architecture for that has the modularity of the communication-free model but73 still allows communication. is built from modules f i, which take the form of multilayer neural74 networks. Here i 2 {0, .., K}, where K is the number of communication layers in the network.75 Each f i takes two input vectors for each agent j: the hidden state hij and the communication c i j ,76 and outputs a vector hi+1j . The main body of the model then takes as input the concatenated vectors77\n1Assuming sj includes the identity of agent j.\n2\n2 Problem Formulation33 We consider the setting where we have M agents, all cooperating to maximize reward R in some34 environment. We make the simplifying assumption that each agent receives R, independent of their35 contribution. In this setting, there is no difference between each agent having its own controller, or36 viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our37 controller is a large feed-forward neural network that maps inputs for all agents to their actions, each38 agent occupying a subset of units. A specific connectivity structure between layers (a) instantiates the39 broadcast communication channel between agents and (b) propagates the agent state in the manner of40 an RNN.41 Because the agents will receive reward, but not necessarily supervision for each action, reinforcement42 l arning is used to maximize expected future reward. We explore two forms of communication within43 the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and44 will be treated as such by the reinforcement learning. In the continuous case, the signals passed45 between agents are no different than hidden states in a neural network; thus credit assignment for the46 communication can be performed using standard backpropagation (within the outer RL loop).47\nWe use policy gr dient [33] with a state specific baseline for delivering a gradient to the model.48 Denote the states in an episode by s(1), ..., s(T ), and the actions taken at each of those states49 as a(1), ..., a(T ), where T is the length of the episode. The baseline is a scalar function of the50 states b(s, ✓), computed via an extra head on the model producing the action probabilities. Beside51 maximizing the expected reward with policy gradient, the models are also trained to minimize the52 distance between the baseline value and actual reward. Thus, after finishing an episode, we update53 the model parameters ✓ by54\n✓ =\nTX\nt=1\n2 @ log p(a(t)|s(t), ✓)\n@✓\nTX\ni=t\nr(i) b(s(t), ✓) ! ↵ @\n@✓\nTX\ni=t\nr(i) b(s(t), ✓) !23 5 .\nHere r(t) is reward given at time t, and the hyperpar meter ↵ is for balancing the reward nd the55 baseline objectives, set to 0.03 in all experiments.56\n3 Model57\nWe now describe the model used to compute p(a(t)|s( ), ✓) at a given time t (ommiting the time58 in ex for brevity). Let sj be the jth agent’s view of the state of the environment. The input to the59 controller is the concatenation of all state-views s = {s1, ..., sJ}, and the controller is a mapping60 a = (s), where the output a is a concatenation of discrete actions a = {a1, ..., aJ} for each agent.61 Note that this single controller encompasses the individual controllers for each agents, as well as62 the communication between agents.63\nOne obvious choice for is a fully-connected multi-layer neural network, which could extract64 features h from s and use them to predict good actions with our RL framework. This model would65 allow agents to communicate wit each other and share views of the environment. However, it66 is inflexible with respect to the composition and number of agents it controls; cannot deal well67 with agents joining and leaving the group and even the order of the agents must be fixed. On the68 other hand, if no communication is used then we can write a = { (s1), ..., (sJ)}, where is a69 per-agent controller applied independently. This communication-free model satisfies the flexibility70 requirements1, but is not able to coordinate their actions.71\n3.1 Controller Structure72\nW n w d ail the architecture for that has the modula ity of the communication-free model but73 still allows communication. is built from modules f i, which take the form of multilayer neural74 networks. Here i 2 {0, .., K}, where K is the number of communication layers in the network.75 Each f i takes tw input vectors for each agent j: the hid en state hij d the communication c i j ,76 and outputs a vector hi+1j . The main body of the model then takes as input the concatenated vectors77\n1Assuming sj includes the identity of agent j.\n2\nh0 = [h01, h 0 2, ..., h 0 J ], and computes:78\nhi+1j = f i(hij , c i j) (1)\n79\nci+1j = 1 J 1 X\nj0 6=j hi+1j0 . (2)\nIn the case that f i is a single linear layer followed by a nonlinearity , we have: hi+1j = (H ihij +80\nCicij) and the model can be viewed as a feedforward network with layers h i+1 = (T ihi) where hi81 is the concatenation of all hij and T takes the block form:82\nT i = 0 BBBB@ Hi Ci Ci ... Ci Ci Hi Ci ... Ci Ci C Hi ... Ci\n... ...\n... . . . ... Ci Ci Ci ... Hi\n1 CCCCA ,\nConnecting Neural Models\nAnonymous Author(s) Affiliation Address email\nAbstract\nabstract1\n1 Introduction2\nIn this work we make two contributions. First, we simplify and extend the graph neural network3 architecture of ??. Second, we s ow ow this archi ecture can be used to control roup of cooperating4 agents.5\n2 Model6\nThe simplest form of the model consists of multilayer neural networks f i that take as input vectors7 hi and ci and output a vector hi+1. The model tak s as input a s t of vec rs {h01, h02, ..., h0m}, and8 computes9\nh +1j = f i(hij , c i j)\n10 ci+1j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j, and i 2 {0, .., K} (we will call K the num er f hops in the network).11 If desired, we can take the final hKj and output them directly, so that the model outputs a vector12 corresponding to each input vector, or we can feed them into another network to get a single vector or13 scalar output.14\nIf each f i is a simple linear layer followed by a nonlinearity :15\ni+1 j = (A ihij + B icij),\nthen the model can be viewed as a feedforward network with layers16\nHi+1 = (T iHi),\nwhere T is written in block form17\nT i = 0 BBBB@ Ai Bi Bi ... Bi Bi Ai Bi ... Bi Bi Bi Ai ... Bi\n... ...\n... . . . ... Bi Bi Bi ... Ai\n1 CCCCA .\nThe key idea is that T is dynamically sized, and the matrix can be dynamically sized because the18 blocks are applied by type, rather than by coordinate.19\nSubmitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.\nConnecting Neural Models\nAnonymous Author(s) Affiliation Address email\nAbstract\nabstract1\n1 Introduction2\nIn this work we make two contributions. First, we simplify and extend the graph neural network3 architecture of ??. Second, we show how this architecture can be used to control groups of cooperating4 agents.5\n2 Model6\nThe simplest form of the model consists of multilayer neural networks f i that take as input vectors7 hi and ci and output a vector hi+1. The model takes as input a set of vectors {h01, h02, ..., h0m}, and8 computes9\nhi+1j = f i(hij , c i j)\n10 ci+1j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j, and i 2 {0, .., K} (we will call K the number of hops in the network).11 If desired, we can take the final hKj and output them directly, so that the model outputs a vector12 corresponding to each input vector, or we can feed them into another network to get a single vector or13 scalar output.14\nIf each f i is a simple linear layer followed by a nonlinearity :15\nhi+1j = (A ihij + B icij),\nthen the model can be viewed as a feedforward network with layers16\nHi+1 = (T iHi),\nwhere T is written in block form17\nT i = 0 BBBB@ Ai Bi Bi ... Bi Bi Ai Bi ... Bi Bi Bi Ai ... Bi\n... ...\n... . . . ... Bi Bi Bi .. Ai\n1 CCCCA .\nThe key idea is that T is dynamically sized, and the matrix can be dynamically sized because the18 blocks are applied by type, rather than by coordinate.19\nSubmitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.\nConnecti g Neural Models\nAnonymous Auth r( ) Affiliation Address email\nAbstract\nabstract1\n1 Introduction2\nIn this work e make two contributions. First, we s mplify and extend the graph neural network3 architecture of ??. Second, we show how this architecture can be s d to control gr ups of cooperating4 agents.5\n2 Model6\nThe simplest form of the model consists of multilayer neural networks f i that take as input vectors7 hi and ci and output a vector hi+1. T e model takes as input a set of vectors {h01, h02, ..., h0m}, and8 computes9\nhi+1j = f i(hij , c i j)\n10 ci+1j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j, and i 2 {0, .., K} (we will call K th number o hops in the network).11 If desired, we can take the final hKj and output them directly, so that the model outputs a vector12 corresponding to each input vector, or we can feed them into another network to get a single vector or13 scalar output.14\nIf each f i is a simple linear layer followed by a nonlinearity :15\nhi+1j = (A ihij + B icij),\nthen the model can be viewed as a feedforward network with layers16\nHi+1 = (T iHi),\nwhere T is written in block form17\nT i = 0 BBBB@ Ai Bi Bi ... Bi Bi Ai Bi ... Bi B Bi Ai ... Bi\n... ... ... . . ...\nBi Bi Bi ... Ai\n1 CCCCA .\nThe key idea is that T is dynamically sized, and the matrix can be dynamically sized because the18 blocks are applied by type, rather than by coordinate.19\nSubmitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.\nCon ecting Neu al Models\nAnonymous Author(s) Affiliation Addr ss email\nAbstract\nabstract1\n1 Introduction2\nIn this work we make two contributions. First, we simplify nd ext nd the graph neural netw rk3 architecture of ??. Second, we show how this architectur can be u ed to control groups of cooperating4 agents.5\n2 Model6\nThe simplest f rm of the model consists of ultilayer neural etworks f i t at take as input vectors7 hi and ci and output a vector hi+1. The model takes as input a set of vectors {h01, h02, ..., h0m}, and8 computes9\nhi+1j = f i(hij , c i j)\n10\nci+1j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j, and i 2 {0, .., K} (we will call K the number of hops in th network).11 If desired, we can take t e final hKj and output them directly, so that the mo el outputs a vector12 corresponding to each input vector, or we can feed them into another network to get a single vector or13 scalar output.14\nIf each f i is a simple linear layer followed by a nonlinearity :15\nhi+1j = (A ihij + B icij),\nthen the model can be viewed as a feedforward network with layers16\nHi+1 = (T iHi),\nwhere T is written in block form17\nT i = 0 BBBB@ Ai Bi Bi ... Bi Bi A Bi ... Bi Bi Bi Ai ... Bi\n... ...\n... . . . ... Bi Bi Bi ... Ai\n1 CCCCA .\nThe key idea is that T is dynamically sized, and the matrix can be dynamically sized because the18 blocks are applied by type, rather than by coordinate.19\nSubmitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.\nmultilayer NN Avg.\n2 Problem Formulation33\nWe consider the setting where we have M agents, all cooperating to maximize reward R in some34 environment. We make the simplifying assumption that each agent receives R, independent of their35 contribution. In this setting, there is no difference between each agen having its own contr ller, or36 viewing them as pieces of a larger m del con rolling ll agents. Taking the latter perspective, our37 controller is a large feed-forward neural network that maps inputs for all agents to their actions, each38 agent occupying a subset of units. A specific connectivity structure between layers (a) instantiates the39 broadcast communication channel between agents and (b) propagates the agent state in the anner of40 an RNN.41\nBecause the agents will receive reward, but not necessarily supervision for each action, reinforcement42 learning is used to maxi ize expected future rewa d. We xplore two forms of communication within43 the c ntroller: (i) discr te d (ii) continuous. In the former case, communication is an action, and44 will be treated as such by the reinforcement learning. In the continuous case, the signals passed45 between agents are no different than hidden states in a neural network; thus credit assignment for the46 communication can b performed using standard backpropagatio (within the out RL loop).47\nWe use policy gradient [33] with a state specific baseline for delivering a gradient to the model.48 Denote the states in an episode by s(1), ..., s(T ), and the actions taken at each of those states49 as (1), ..., a(T ), where T is the length of th epis de. T baseline is a scalar function of the50 states b(s, ✓), computed via an extra head on the odel producing the action probabilities. Beside51 maximizing the expected reward with policy gradient, the models are also trained to minimize the52 distance between the baseline value and actual reward. Thus, after finishing an episode, we upd te53 the model parameters ✓ by54\n✓ =\nTX\nt=1\n2 4@ log p(a(t)|s(t), ✓)\n@✓\nTX\ni=t\nr(i) b(s(t), ✓) ! ↵ @\n@✓\nTX\ni=t\nr(i) b(s(t), ✓) !23 5 .\nHere r(t) is reward given at time t, and the hyperparameter ↵ is for balancing the reward and the55 baseline objectives, set to 0.03 in all experiments.56\n3 Model57\nWe now describe the model used to compute p(a(t)|s(t), ✓) at a given time t (ommiting the time58 ind x for brevity). Let sj be the jth agent’s view of the state of the environment. The input to the59 controller is the concatenation of all state-views s = {s1, ..., sJ}, and the controller is a mapping60 a = (s), where the output a is a concatenation of discrete actions a = {a1, ..., aJ} for each agent.61 No e that this single controller encompasses the individual controllers for each agents, as well as62 the communication between agents.63\nOn bvious ch ice for is a fully-connected multi-layer neural network, which could extract64 features h from s and use them to predict good actions with our RL framework. This model would65 allow agents to communicate with each other and share views of the environment. However, it66 is inflexible with respect to the composition and number of agents it controls; cannot deal well67 with agents joining and leaving the group and even the order of the agents must be fixed. On the68 other hand, if no communication is used then we can write a = { (s1), ..., (sJ)}, where is a69 per-agent controller applied independently. This communication-free model satisfies the flexibility70 requirements1, but is not able to coordinate their actions.71\n3.1 Controller Structure72\nWe now detail the architecture for that has the modularity of the communication-free model but73 still allows communication. is built from modules f i, which take the form of multilayer neural74 networks. Here i 2 {0, .., K}, where K is the number of communication layers in the network.75 Each f i takes two input vectors for each agent j: the hidden state hij and the communication c i j ,76 and outputs a vector hi+1j . The main body of the model then takes as input the concatenated vectors77\n1Assuming sj includes the identity of agent j.\n2\n2 Problem Formulation33\nWe consider the setting where we have M agents, all cooperating to maximize reward R in some34 environment. We make the simplifying assumption that each agent receives R, independent of th ir35 contribution. In this setting, there is no differen e betw en each agent h ving its own controller, or36 viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our37 controller is a large fee -forward neural network that maps inputs for all agents to their actions, each38 agent occupying a subset of units. A specific connectivity structure between layers (a) instantiates the39 broadcast communication channel between agents and (b) propagates the agent state in the manner of40 an RNN.41\nBecause the agents will receive reward, but not necessarily supervision for each action, reinforcement42 learning i used t maximize expected future reward. We explore wo forms of communication within43 the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and44 will be treated as such by the reinforcement learning. I the continuous case, the signals passed45 betwe n agents are no different than hidden sta es in a n ural netw rk; thus credit assignme t for the46 communi ation can be performed using standard backpropagation (within the outer RL loop).47\nWe use policy gr dient [33] with a state specific baseline for delivering a gradient to the model.48 Denote the states in an episode by s(1), ..., s(T ), and the actions taken at each of those states49 as a(1), ..., a(T ), where T is the length of the episode. The baseline is a scalar function of the50 states b(s, ✓), computed via a extra head on the model pr ucing the action probabilities. Beside51 maximizing the expected reward with policy gr d ent, the m dels are lso rain d t minimize the52 distance between the baseline value and actual reward. Thus, after fini hing an episode, we update53 the model parameters ✓ by54\n✓ =\nTX\nt=1\n2 @ log p( (t)|s(t), ✓)\n@✓\nTX\ni=t\nr(i) b(s(t), ✓) ! ↵ @\n@✓\nTX\ni=t\nr(i) b(s(t), ✓) !23 5 .\nHere r(t) is reward given at time t, and the hyperpar meter ↵ is for balancing the reward nd the55 baseline objectives, set to 0.03 in all experiments.56\n3 Model57\nWe now d cribe the model us d to compute p(a(t)|s(t), ✓) at a given time t (ommiting the time58 index for brevity). Let sj be the jth agent’s view of th state of the environment. The input to th59 controller is the concatenation of all state-views s = {s1, ..., sJ}, and the controller is a mapping60 a = (s), where the output a is a concatenation of discrete actions a = {a1, ..., aJ} for agent.61 Note that this single controller encompasses the individual controllers for each agents, as well as62 the communication between agents.63\nOn obvi us choice f r is a fully-connec ed multi-layer neural network, which could extract64 features h from s and use them to predict good actions with our RL framework. This model would65 allow agents to communicate with each other and share views of the environment. However, it66 is inflexible with respect to the composition and number of agents it controls; cannot deal well67 with agents joining and leaving the group and even the order of the agents must be fixed. On the68 other hand, if no communication is used then we can write a = { (s1), ..., (sJ)}, where is a69 p r-agent controller applied independently. This commu ication-free model satisfies th fl xibility70 requirements1, but is no able to coordinate th ir actions.71\n3.1 Controll r Structure72\nWe now detail the architecture for that has the modularity of the communication-free model but73 still allows communic tio . is built from modules f i, which take the form of multilayer neural74 networks. Here i 2 {0, .., K}, where K is the number of communication layers in the network.75 Each f i takes tw input vectors for each agent j: the hid en state hij d the communication c i j ,76 and outputs a vector hi+1j . The main body of the model then takes as input the concatenated vectors77\n1Assuming sj includes the identity of agent j.\n2\nFigure 1: Bl h.\nThe key idea is that T is dynamically sized. First, the number of agents may vary. This motivates83 the the normalizing factor J 1 in equation (2), which resacles the communication vector by the84 number of communicating agents. Second, the blocks are applied based on category, rather than by85 c ordinate. In this simple form of the model “category” refers to either “self” or “tea mate”; but as86 we ill see below, the communication architecture ca be more complicated than “broadcast t all”,87 and so may require more c t gories. Note also that T i is permuta ion nvariant, t us the order of the88 agents does not matter.89\nAt the fi st layer of the model an enco er function h0j = p(sj) is used. This takes as input state-view90 sjand outputs featu e vector h0j (in Rd0 for some d0). The form of the encoder is problem dependent,91 but for most of our tasks they consist of a lookup-table embedding (or bags of vectors thereof). Unless92 otherwise noted, c0j = 0 for all j.93\nAt the output of the model, a decoder function q(hKj ) is used to output a distribution over the space of94 actions. q(.) takes the form of a single layer network, followed by a softmax. To produce a discrete95 action, we sample from the this distribution.96\nThus the entire model, which we call a Communication Neural Net (CommNN), (i) takes the state-97 view of all agents s, passes it through the e coder h0 = p(s), (ii) iterates h and c in equations (1)98 and (2) to obain hK , (iii) samples actions a for all agents, according to q(hK).99\n3.2 Model Extensions100\nLocal Connectivity: An alternative to the broadcast framework described above is to allow agents101 to communicate to others within a certain range. Let N(j) be the set of agents present within102 communication range of agent j. Then (2) becomes:103\nci+1j = 1 |N(j)| X\nj02N(j) hi+1j0 . (3)\nh0 = [h01, h 0 2, ..., h 0 J ], and computes:78\nhi+1j = f i(hij , c i j) (1)\n79\nci+1j = 1 J 1 X\nj0 6=j hi+1j0 . (2)\nIn the case that f i is a single linear layer followed by a nonlinearity , we have: hi+1j = (H ihij +80\nCicij) and the model can be viewed as a feedf rward network with lay s h i+1 = (T ihi) where hi81 is the concatenation of all hij and T takes the block form:82\nT i = 0 BBBB@ Hi Ci Ci ... Ci Ci Hi Ci ... Ci Ci Ci Hi ... Ci\n... ...\n... . . . ... Ci Ci Ci ... Hi\nCCCCA ,\nConnecting Neural Models\nAn nymou Author(s) Affiliation Address email\nAbstrac\nabstract1\n1 Introduction2\nIn this work we make two contributions. First, we simplify and exte d the graph neural network3 architecture of ??. Second, we show how this architec ure can b used to control groups of cooper ti g4 agents.5\n2 Model6\nThe simplest form of the model consists of multilayer neural networks f i that take as input vectors7 hi and ci and output a ve tor hi+1. The m del takes s inpu a set of vector {h01, h02, ..., h0m}, and8 computes9\nhi+1j = f i(hij , c i j)\n10\nci+1j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j, and i 2 {0, .., K} (we will all K the number f hop in the network).11 If desired, we can take the final hKj and output them directly, so that the mode outputs a vector12 corresponding to each input vector, or we can feed them int another et ork to get a single vector or13 scalar output.14\nIf each f i is a simple linear layer followed by a nonlin ity :15\nhi+1j = (A ihij + B icij),\nthen the model can be viewed as a feedforward network with layers16\nHi+1 = (T iHi),\nwhere T is written in block f rm17\nT i = 0 BBBB@ Ai Bi Bi ... Bi Bi Ai Bi ... Bi Bi Bi Ai ... Bi\n... ...\n... . . . ... Bi Bi Bi ... Ai\n1 CCCCA .\nThe key idea is that T is dynamically sized, and the matrix can be dynamically sized because the18 blocks are applied by typ , rather than by coordinate.19\nSubmitted to 29th Confere ce on Neural I formation rocessing Systems (NIPS 2016). Do not distribute.\nCon ecting Neural Models\nAnonymous Author(s) Affiliation Address email\nAbstract\nab tract1\n1 Introduction2\nIn this work we make two contributions. First, we si plify and extend the gr ph neural network3 architecture of ??. Second, we show h w this architecture can be used to control groups of coop rating4 agents.5\n2 Model6\nThe simplest form of the model consists of multilayer neural net orks f i that take as input vectors7 hi and ci and output a ve tor hi+1. The model takes as input a set of vectors {h01, h02, ..., h0m}, and8 computes9\nhi+1j = f i(hij , c i j)\n10\nci+1j X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j, and i 2 {0, .., K} (we will call K the number of hops in the net ork).11 If desired, we can take the final hKj and output them directl , so that the model utputs a vector12 corresponding to each input vector, or we can f ed them into a other network to get a single vector or13 scalar output.14\nIf each f i is a simple linear layer followed by a nonlinearity :15\nhi+1j = (A ihij + B icij),\nthen the model can be viewed as a feedforward network with layers16\nHi+1 = (T iHi),\nwhere T is written in block for17\nT i = 0 BBB @ Ai Bi Bi ... Bi Bi Ai Bi ... Bi Bi Bi Ai ... Bi\n... ...\n... . . . ... Bi Bi Bi .. Ai\n1 CCCCA .\nThe key idea is that T is dy amically sized, and the matrix can be dynamic lly sized because the18 blocks are applied by type, rather than by coordinate.19\nSubmitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.\nConnecting N ur l Mod ls\nAnonymous Auth r( ) Affiliation Address email\nAbstract\nabstract1\n1 Introduction2\nIn this work e make two contributions. First, we s mplify and extend the graph neural network3 architecture of ??. Second, we show h w this architecture can be s d to control gr ups of cooperating4 agents.5\n2 Model6\nThe simplest form of the model consists of multilayer neural networks f i that take as input vectors7 hi and ci and output a vector hi+1. T e model t kes s input set of vectors {h01, h02, ..., h0m}, and8 computes9\nhi+1j = f i(hij , c i j)\n10 ci+1j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j, and i 2 {0, .., K} (we will call K the number of hops in the network).11 If desired, we can take the final hKj and output them di ectly, s that the model outputs a vector12 corresponding to each input vector, o e can feed them into another network to get a single vector or13 scalar ou put.14\nIf each f i is a simple linear layer followed by a nonlinearity :15\nhi+1j = (A ihij + B icij),\nthen the model can be viewed as a feedforward network with layers16\nHi+1 = (T iHi),\nwhere T is written in block form17\nT i = 0 BBBB@ Ai Bi Bi ... Bi Bi Ai Bi ... Bi B Bi Ai ... Bi\n... ...\n... . . ...\nBi Bi Bi ... Ai\n1 CCCCA .\nThe key idea is that T is dynamically sized, and the matrix can be dynamically sized because the18 blocks are applied by type, rather than by coordinate.19\nSubmitted to 29th Conference on Neural Informatio Processing Systems (NIPS 2016). Do not distribute.\nCon ecting Neu al Mo l\nAnonymous utho (s) Affiliation Address email\nAbstract\nabstract1\n1 Introduction2\nIn this work we make two contributions. First, we simplify nd ext nd the graph neural network3 architecture of ??. Second, we sh w how this architecture can b used to control groups of co perating4 agents.5\n2 Model6\nThe simplest f rm of the model consists of multilayer neural etworks f i t at take as input v ctors7 hi and ci and output a vector hi+1. The model takes as input a set of vectors {h01, h02, ..., h0m}, and8 computes9\nhi+1j = f i(hij , c i j)\n10\nci+1j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j, and i 2 {0, .., K} (we will call K the number of hops in th network).11 If desired, we c n take t e final hKj a d tput them dir ctly, so that th mo el o tputs a vecto12 corresponding to each input vector, or we can feed them into another network to get a si gle vector or13 scalar output.14\nIf each f i is a simple linear layer followed by a nonlinearity :15\nhi+1j = (A ihij + B icij),\nthen the model can be viewed as a feedforward network wi h layers16\nHi+1 = (T iHi),\nwhere T is written in block form17\nT i = 0 BBBB@ Ai i Bi ... Bi Bi A Bi ... Bi Bi Bi Ai ... Bi\n... ...\n... . . . ... Bi Bi Bi ... Ai\n1 CCCCA .\nThe key idea is that T is dynamically sized, and the matrix can be dynamically sized because the18 blocks are applied by type, rat er than by coordinate.19\nSubmitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.\nmultilayer NN Avg.\n2 Proble Formula ion33\nWe consider the setting where we have M agents, all cooperating to maximize reward R in some34 environment. We make the si plifying assumption th t each agent r ceives R, independent of their35 contribution. In this setting, there is no difference between ea h agent having its own controller, or36 viewing them as pieces of a larger model controll ng all age ts. Taking the latter perspective, our37 controller is a large fe d-forward neural ne work that maps i puts for ll agents to their actions, each38 age t ccupying a ubset of u its. A spe ific conne tivity structur between layers (a) instantiates the39 br adcast communic tion channel between agents and (b) propagates the agent state in the manner of40 an RNN.41\nBecause the agents will receive reward, but not n cessarily supervision for each action, reinforcement42 learning is used t maximize expected future reward. We explore two forms of communication within43 the controller: (i) d scr t and (ii) c nt uous. In the former cas , communication is an action, and44 will be treate as such by the reinf rc ment learning. In the co tinuous case, the signals passed45 between age ts re no differe t t an hidden states in n ural n t ork; thus credit assignment for the46 communic tion can be performe using t ndard backpropagation (wi hi the outer RL lo p).47\nWe use policy gradient [33] with a state specific baseline for delivering a gradient to the model.48 Denote the states in an episode by s(1), ..., s(T ), and the actions t ken at each of those states49 as a(1), ..., a(T ), where T is the length of the episode. The baseli e is a scalar function of the50 states b(s ✓), computed via an extra head on the model produc g the action probab lities. Beside51 maxi izing th expected reward with policy gr die t, models are also trained to minimiz the52 distance betwe n th baseline value and actual reward. Thus, after finishing an episode, we update53 the model parameters ✓ by54\n✓ =\nTX\n=1\n2 4@ log p(a(t)|s(t), ✓)\n@✓\nTX\ni=t\nr(i) b(s(t), ✓) ! ↵ @\n@✓\nTX\n=t\nr(i) b(s(t), ✓) !23 5 .\nHere r(t) is reward given at time t, and the hyperparameter ↵ is for balancing the reward and the55 baseline objectives, set to 0.03 in all experim nts.56\n3 Model57\nWe now describe the model used to compute p(a(t)|s(t), ✓) at a given time t (ommiting the time58 index for brevity). Let sj be the jth agent’s view of the state of the environment. The input to the59 co troller is the concatenation f all state-views s = {s1, ..., sJ}, and the controller is a mapping60 a = (s), where the output a is a concatenation of discrete actions a = {a1, ..., aJ} for each agent.61 No e t at this single controller encompasses the individual controllers for each agents, as well as62 the communication between age ts.63\nOne obvious choice for is a fully-connected multi-layer n ural network, which could extract64 features h from s and use them to predict good actions with our RL framework. This model would65 allow agents to communicate with ea h other and share views of the environment. However, it66 is inflexible with respect to the comp sition and number of agents it controls; cannot deal well67 with agents joining and leaving the group and even the order of the agents must be fixed. On the68 other h nd, if no co munication is used then we c n write a = { (s1), ..., (sJ)}, where is a69 per-agent controller applied independently. This communic tion-free model satisfies the flexibility70 requirements1, but is not able to coordinate their actions.71\n3.1 Controller Structure72\nWe now detail the architecture for that has the modularity of the communication-free model but73 still allows communication. is built from modules f i, which take the form of multilayer neural74 networks. Here i 2 {0, .., K}, where K is the number of communication layers in the network.75 Each f i takes two input vectors for each agent j: the hidden state hij and the communication c i j ,76 and outputs a vector hi+1j . Th ma n body of the model then takes as input the concatenated vectors77\n1Assuming sj includes the identity of agent j.\n2\n2 Proble Formulation33\nW consider the setting where we have M agents, all cooperating to aximize eward R in some34 environment. We make the s mplifying assumption tha each agent receives R, ind pendent of heir35 c tribution. In this setting, th re is n differ c between each agent having its own cont oller, or36 vi wing th m as pieces of a larg r model contro i all ag nts. Taking h latter persp ctive, our37 controller is a large feed-forward neural network that maps inputs for all agents to th ir actions, ach38 agent occupying a subset of units. A specific connectivity structure between layers (a) instantiates the39 broadcast communication channel between agents and (b) propagates the agent state in the manner of40 an RNN.41\nBecause the agents will receive reward, but not necessarily supervision for each action, reinforcement42 learning i used t maximize expected future reward. We explore two forms of communication within43 the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and44 will be treated as such by the reinforcement learning. In the continuous case, the signals passed45 between agents are no different than hidden states in a neural network; thus credit assignment for the46 communi ation can be performed using standard ba kpropagation (within the outer RL loop).47\nWe use policy gr dient [33] with a state specific baseline for delivering a gradient to the model.48 Denote the states in an episode by s(1), ..., s(T ), and the actions taken at each of those states49 as a(1), ..., a(T ), where T is the length of the episode. The baseline is a scalar function of the50 states b(s, ✓), computed via an extra head on the model producing the action probabilities. Beside51 maximizing the expected reward with policy gradient, the models are also tr ined to minimize the52 distance between the baseline value and actual reward. Thus, after fi ishing an episode, we update53 the model parameters ✓ by54\n✓ =\nTX\nt=1\n2 @ log p( (t)|s(t), ✓)\n@✓\nTX\ni=t\nr(i) b(s(t), ✓) ! ↵ @\n@✓\nTX\ni=t\nr(i) b(s(t), ✓) !23 5 .\nHere r(t) is reward given at tim t, and the hyperpar meter ↵ is for balancing the reward nd the55 baseline objectives, set to 0.03 in all experiments.56\n3 Mo el57\nWe now d cribe the model us d to compute p(a( )|s( ), ✓) at a given time t ( mmiting the time58 index for brevity). Let sj be the jth agent’s view f the state of the environment. The input to the59 controller is the concatenation of all state-views s = {s1, ..., sJ}, and the controller is a mapping60 a = (s), where the output a is a concatenation of discrete actions a = {a1, ..., aJ} for each agent.61 Note that this singl controller encompasses the individ al controllers for each agents, as w ll as62 the communication between agents.63\nOne obvious choice for is a fully-connected multi-layer neural network, w ich could extract64 features h from s and use them to predict good act ons with our RL fram work. This model would65 allow agents to com unicate with each other and share views of the environment. However, it66 is inflexible with respect to the composition and number of agents it controls; cannot deal well67 with agents joining and leaving the group and even the order of the agents must be fixed. On the68 other hand, if no c mmunicati n is used then we can write a = { (s1), ..., (sJ)}, where is a69 per-agent contro ler applied independently. This c mmunication-free model satisfi s the flexibility70 requirements1, b t s not able to coordinate their act ons.71\n3.1 Controll r Structure72\nWe now detail the architecture for that has the modularity of the communicati n-free model but73 st ll allows commun c tio . is built from modules f i, which take the form of multilayer neural74 networks. Here i 2 {0, .., K}, where K is the number of communication layers in the network.75 Each f i tak s tw input v ct s for each ag nt j: the hid en state hij d the communication c i j ,76 and outpu s a vector hi+1j . The main body of the m del then tak s as input the concatenated vectors77\n1Assuming sj includes the identity of agent j.\n2\nFigure 1: Bl h.\nThe key idea is that T is dynamica ly sized. Fi st, t e number of agents may vary. This motivates83 the the normalizing factor J 1 in equation (2), which resacles the communication vector by the84 number of communic ting agents. Second, the blocks a e applied based on category, rather han by85 c ordinate. In this simple form of he model “category” r fers t either “s lf” r “t a mate”; but as86 we ill see below, the commun cation architecture ca be more complicated than “broadc st t all”,87 and s may require m re c t gories. Note also that T i is p rmuta ion nvariant, t us the order of the88 agents does not matter.89\nAt the first layer of the model an encoder function h0j = p(sj) is used. This takes as input state-view90 sjand outputs featu e vector h0j (in Rd0 for some d0). The form of the encoder is probl m dependent,91 but for most of our tasks they consist f a lookup-table embedding (or bags of vectors thereof). Unless92 otherwise noted, c0j = 0 for all j.93\nAt the output of the model, a decoder function q(hKj ) is used to output a distributi n over the space of94 actions. q(.) takes the form of a single layer network, followed by a softmax. To produce a discrete95 action, we sample from the this distribution.96\nThus th entire model, which we call a Communication Neural Net (CommNN), (i) takes the state-97 view of all agents s, passes it through the e coder h0 = p(s), (ii) iterates h and c in equations (1)98 and (2) to obain hK , (iii) samples actions a for all agents, according to q(hK).99\n3.2 Model Extensions100\nLocal Connectivity: An alternative to the broadcast framework described above is to allow agents101 to communicate to others within a certain range. Let N(j) be the set of agents present within102 communication ra ge of agent j. Then (2) becomes:103\nci+1j = 1 |N(j)| X\nj02N(j) hi+1j0 . (3)\n3\ntanh\nConnecting Neural Models Anonymous Author(s) Affiliation Addres email A stract\nabstract1\n1 I troduction2\nIn this work we make two contributions. Fi st, we simplify and xtend the gra h neur l network3 architecture of ??. S co d, we show how this architectur c n be used to control rou s of coop rating4 agents.5\n2 Model6\nThe i ple t form of he m d l consists of m lti ayer eura ne works f i that take as input vectors7 hi ci and o put a vector hi+1. The mo el ak s as inpu set of vectors {h01, 02, ..., 0m}, and8 co putes9\nhi+1j = f i(hij , c i j)\n10\nci+1j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j, and i 2 {0, . , K} (w will call K the number of h ps i the network).11 If desired, we can take t e final hKj nd outp t them dir ctly, so that the model out uts a v ctor12 c rresponding to e ch inpu v c r, or we fe d them to another network to get a single v c or r13 cala output.14\nIf each f i is simpl linear layer followed by a nonlinearity :15\nhi+1j = (A i i j + B icij),\nhen the model an be vi wed as a feedforward network w th layers16\nHi+1 = (T iHi),\nwhere T is ritten in block form17\nT i = 0 BBBB@ Ai Bi Bi ... Bi Bi Ai Bi ... Bi Bi Bi Ai ... B ... ... ... . . . ...\nBi Bi Bi ... Ai\n1 CCCCA .\nThe key id a is that T i dynami ally sized, and the a rix n be dy amically sized because th18 l cks are applied by type, r ther than by co r i ate.19\nSubmitted to 29th Conference on Neura Info mation Processing Systems (NIPS 2016). Do not distribute.\nCommNN model th communication step Mo ule for agent\nConnecting Neural Models Anonymous uthor(s) Affiliation Addr ss email Abstract\nabstract1\n1 Intr duction2\nIn this work we make two contribution . First, e si plify extend t e graph ural network3 architecture of ??. S cond, w show how this ar hitecture can be used to control g oups of cooper ting4 agents.5\n2 M del6\nThe si plest f rm f the odel c nsists of mul layer neural et orks f i that take as input vectors7 hi and ci nd utput vector i+1. The model tak s as i put a se of vectors {h01, h02, .. , h0m}, and8 computes9\nhi+1j = f i(hij , c i j)\n10 i+1 j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j, and i 2 {0, .., K} (we will call K th number of hops in the etwork).11 If desired, we can tak the final hKj nd output th m irectly, so th th model outpu s vector12 corresp nding to e c input v ct r, o w can feed th m into another etwork to et a singl vector or13 sca r ou put.14\nIf each f i is a simpl linear ayer f llo ed by a onlinearity :15\nhi+j = (A ihi + Bicij),\nthen the m d l can be viewed as a fe forward n twork wi h layers16\nHi+1 = (T iHi),\nwhere T is written in block form17\nT i = 0 BB B@ Ai Bi Bi ... Bi Bi Ai Bi ... Bi Bi Bi Ai . . Bi\n... ...\n... . . . ... Bi Bi Bi ... Ai\n1 CCCCA .\nTh key idea is that T is dyna ically ized, an th mat ix ca be dynamically sized b cause the18 bl cks are pplied by typ , r th than by coor i ate.19\nSubmit ed t 29th Confere ce on Neural Information Proce sing Systems (NIPS 2016). Do not distribute.\nConn cting Neural Models An nymous Author(s) Affiliation Address email Abstract\nabstract1\n1 Intr duction2\nIn this work we mak two con ributions. First, w si plify extend t e g aph neural network3 archit cture of ??. Second, w show how this archit cture can be used to cont ol groups of cooper ting4 agents.5\n2 Model6\nThe simplest fo m of the model c nsists of multilaye ur l networks f i that take as i put vectors7 i and ci and output a vect r i+1. Th model takes as in ut a s of v ct rs {h01, 02, ..., h0m}, and8 com utes9 hi+1j = f i(hij , c i j)\n10\nci+1j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j and i 2 {0, .., K} (we will call K th number of h ps in the network).11 If sired, we can take the final hKj an ou put them dir ctly, so that the model outpu s a vector12 corresp nding to ch input vector, or w can fe d them into a other network to et a single vector or13 scal r output.14\nIf ach f i is a simple lin r ayer followed by a nonlinearity :15\nhi+1j = (A ihi + Bicij),\nthe the mod l can b viewed as a fee forward n tw ith layers16\nHi+1 = (T iHi),\nwhere T is written in block form17\nT i = 0 BB B@ Ai i Bi ... Bi Bi Ai Bi ... Bi Bi Ai ... Bi\n... ...\n... . . . ... Bi Bi Bi ... Ai\n1 CCCCA .\nT ey dea is tha T is dyn mically sized, an th matrix ca be dynamically sized because the18 blocks ar appli d by type, ther than by coordin te.19\nSubmit d t 29th Conf rence on Neu al Information Proce sing Systems (NIPS 2016). Do not distribute.\nConn cting Neural Models An nymous Author(s) Affiliation Address email Abstract\nabstract1\n1 Intr duction2\nIn this work we make wo con ributions. First, w implify a xt nd t e g aph neural network3 rc itectur of ??. Second, w show how this archit cture can be used to ontr l groups of o perating4 ge ts.5\n2 Model6\nTh simplest form of the model c nsists of multilayer neural networks f i that take as input vectors7 hi and c output a vect i+1. Th model tak as inp t se of vectors {h01, h02, ..., h0m}, and8 computes9\nhi+1j = f i hij , c i j)\n10\nci+1j = X\nj0 6=j hi+1j0 ;\nWe set c0j = 0 for all j and i 2 {0, .., K} (we will call K th numbe of h ps in the network).11 If d sir d, we can take he final Kj an u put the d r ctly, s that the mod l utpu s a vector12 corr sp nding o each inpu vect , r we can f ed them int a th r network to et a single vector or13 scalar u put.14\nIf each f i is a simple lin ar ay r followed by a n nlinearity :15\nhi+1j = (A i i + Bicij),\nth the mod l can be viewed as a fe forward n two k with l yers16\nHi+1 = (T iHi),\nhere T is written in block form17\nT i =\n0\nB B@\nAi i Bi ... Bi Bi Ai Bi ... Bi Bi i Ai ... Bi\n... ...\n... . . . ... Bi Bi Bi ... Ai\n1 CCCCA .\nT key ea is that T is dyn ic lly sized, and the atrix ca be dynamically sized because the18 bl cks ar appli d by type, r ther h n y coordin e.19\nSubmit d t 29th Confer nce on Neu al Information Proce sing Systems (NIPS 2016). Do not distribute.\nFigure 1: A ov rview of our communication mod l. Left: view of module f for a single agent j. Note that the parameters are shared across all agents. Middle: a single communication step, where each agen s modul s propagate ir internal stat h, as well as broadcasting a commu icatio vector c on a common channel (shown in red). Rig t: full mo el, showing input states s for each agent, t o communicatio st p and the output actions for e ch agent.\nA key point is that T is dynamically sized since the number of agents may vary. This motivates the the normalizing factor J − 1 in equati n (3), which rescales the communication vector by the number of communicati agents. Note also that T i is permutation invariant thus the order of the agents does not atter.\nAt the fir t layer of he m del n encoder function h0j = r(sj) is us d. This t kes as input state-view sj and outputs feature vector h0j (in Rd0 for some 0). The form of the encoder is problem dependent, but for most of our asks it is a single layer eural network. U less ot rwise noted, c0j = 0 for all j. At th output of he model, a decoder f nction q(hKj ) is sed t output a distribution over th space of actions. q(.) takes th form o single layer network, f llowed y a softmax. To produce a discret ctio , we sample from t this distributi n: j ∼ q( Kj ).\nThus the entire model (shown in Fig. 1), which we call a Communication Neural Net (CommNN), (i) takes the state-view of all agents s, passes it through the encoder h0 = r(s), (ii) iterates h and c in equations (2) and (3) to obtain hK , (iii) samples actions a for all agents, according to q(hK). We refer to this type c mmunic tio as ontinuous type b cause co munication is based on continuous-valued vecto s.\n3.2 M del Ext nsions\nLocal Connectivity: A al ernative to the broadcast framework described above is to allow agents to communicate to others ithin a certai range. Let N(j) be the set of agents present within com unication range f agent j. Then (3) becomes:\nci+1j = 1 |N(j)| ∑\nj′∈N(j) hi+1j′ . (4)\nAs the agents move, enter an exit and the environment, N(j) will change over time. In this setting, our model has a natural interpretation as a dynamic graph, with N(j) being the set of vertices connected to vertex j at the current time. The edges within the graph represent the communication channel tween agents, with (4) being equivalent to belief propagation [22]. Furthermore, the use of multi-layer nets at each vertex makes our model similar to an instantiation of the GGSNN work of Li et al. [14].\nTemporal Recurrence: We also explore having the network be a recurrent neural network (RNN). This is achieved by simply replacing the communication step i in Eqn. (2) and (3) by a time step t,\n3\nand using the same module f t for all t. At every time step, actions will be sampled from q(htj). Note that agents can leave or join the swarm at any time step. If f t is a single layer network, we obtain plain RNNs that communicate with each other. In later experiments, we also use an LSTM as an f t module.\nSkip Connections: For some tasks, it is useful to have the input encoding h0j present as an input for communication steps beyond the first layer. Thus for agent j at step i, we have:\nhi+1j = f i(hij , c i j , h 0 j ). (5)\n3.3 Discrete Communication An alternate way for agents to communicate is via discrete symbols, with the meaning of these symbols being learned during training. Since discrete outputs are not differentiable, reinforcement learning is used to train in this setting. However, unlike actions in the environment, an agent has to output a discrete symbol at every communication step. But if these are viewed as internal time steps of the agent, then the communication output can be treated as an action of the agent at a given (internal) time step and we can direct employ the policy gradient [36].\nAt communication step i, agent j will output the index wij corresponding to a particular symbol, sampled according to: wij ∼ Softmax(Dhij) (6) where matrix D is the model parameter. Let ŵ be a 1-hot binary vector representation of w. In our broadcast framework, at the next step the agent receives a bag of vectors from all the other agents (where ∧ is the element-wise OR operation):\nci+1j = ∧\nj′ 6=j ŵij′ (7)\n4 Related Work Our model combines a deep network with reinforcement learning [9, 20, 13]. Several recent works have applied these methods to multi-agent domains, such as Go [16, 25] and Atari games [30], but they assume full visibility of the environment and lack communication. There is a rich literature on multi-agent reinforcement learning (MARL) [2], particularly in the robotics domain [18, 26, 6, 21, 3]. Amongst fully cooperative algorithms, many approaches [12, 15, 34] avoid the need for communication by making strong assumptions about visibility of other agents and the environment. Others use communication, but with a pre-determined protocol [31, 19, 38, 17].\nA few notable approaches involve learning to communicate between agents under partial visibility: Kasai et al. [10] and Varshavskaya et al. [33], both use distributed tabular-RL approaches for simulated tasks. Giles & Jim [7] use an evolutionary algorithm, rather than reinforcement learning. Guestrin et al. [8] use a single large MDP to control a collection of agents, via a factored message passing framework where the messages are learned. In common with our approach, this avoids an exponential blowup in state and action-space. In contrast to these approaches, our model uses a deep network for both agent control and communication.\nFrom a MARL perspective, the closest approach to ours is the concurrent work of Foerster et al. [5]. This also uses a deep reinforcement learning in multi-agent partially observable tasks, specifically two riddle problems (similar in spirit to our levers task) which necessitate multi-agent communication. Like our approach, the communication is learned rather than being pre-determined. However, the agents communicate in a discrete manner through their actions. This contrasts with our model where multiple continuous communication cycles are used at each time step to decide the actions of all agents. Furthermore, our approach is amenable to dynamic variation in the number of agents.\nThe Neural GPU [39] has similarities to our model but differs in that a 1-D ordering on the input is assumed and it employs convolution, as opposed to the global pooling in our approach (thus permitting unstructured inputs). Our model can be regarded as an instantiation of the GNN construction of Scarselli et al. [24], as expanded on by Li et al. [14]. In particular, in [24], the output of the model is the fixed point of iterating equations (4) and (2) to convergence, using recurrent models. In [14], these recurrence equations are unrolled a fixed number of steps and the model trained via backprop through time. In this work, we do not require the model to be recurrent, neither do we aim to reach steady state. Additionally, we regard Eqn. (4) as a pooling operation, conceptually making our model a single feed-forward network with local connections.\n5 Experiments 5.1 Lever Pulling Task We start with a very simple game that requires the agents to communicate in order to win. This consists of m levers and a pool of N agents. At each round, m agents are drawn at random and they must each choose a lever to pull, simultaneously with the other m− 1 agents, after which the round ends. The goal is for each of them to pull a different lever. Correspondingly, all agents receive reward proportional to the number of distinct levers pulled. Each agent can see its own identity, and nothing else, thus sj = j.\nWe implement the game with m = 5 and N = 500. We use a CommNN with two communication steps (K = 2) and skip connections from (5). The encoder r is a lookup-table with N entries of 128D. Each f i is a two layer neural net with ReLU nonlinearities that takes in the concatenation of (hi, ci, h0), and outputs a 128D vector. The decoder is a linear layer plus softmax, producing a distribution over the m levers, from which we sample to determine the lever to be pulled. We also use a communication free version of the model that has c zeroed during training. The results are shown in Table 1. The metric is the number of distinct levers pulled divided by m = 5, averaged over 500 trials, after seeing 50000 batches of size 64 during training. We explore both reinforcement (see (1)) and direct supervision (where the sorted ordering of agent IDs are used as targets). In both cases, the CommNN with communication performs significantly better than the version without it.\n5.2 Cooperative Games In this section, we consider two multi-agent tasks in the MazeBase environment [27] that use reward as their training signal. The first task is to control cars passing through a traffic junction to maximize the flow while minimizing collisions. The second task is to control multiple agents in combat against enemy bots.\nWe experimented with several module types. With a feedforward MLP, the module f i is a single layer network and K = 2 communication steps are used. For an RNN module, we also used a single layer network for f t, but shared parameters across time steps. Finally, we used an LSTM for f t. In all modules, the hidden layer size is set to 50. All the models use skip-connections. Both tasks are trained for 300 epochs, each epoch being 100 weight updates with RMSProp [32] on mini-batch of 288 game episodes (distributed over multiple CPU cores). In total, the models experience ∼8.6M episodes during training. We repeat all experiments 5 times with different random initializations, and report mean value along with standard deviation. The training time varies from a few hours to a few days depending on task and module type.\n5.2.1 Traffic Junction\nThis consists of a 4-way junction on a 14× 14 grid as shown in Fig. 2(left). At each time step, new cars enter the grid with probability parrive from each of the four directions. However, the total number of cars at any given time is limited to Nmax = 10. Each car occupies a single cell at any given time and is randomly assigned to one of three possible routes (keeping to the right-hand side of the road). At every time step, a car has two possible actions: gas which advances it by one cell on its route or brake to stay at its current location. A car will be removed once it reaches its destination at the edge of the grid.\nTwo cars collide if their locations overlap. A collision incurs a reward rcoll = −10, but does not affect the simulation in any other way. To discourage a traffic jam, each car gets reward of τrtime = −0.01τ at every time step, where τ is the number time steps passed since the car arrived. Therefore, the reward at time t is:\nr(t) = Ctrcoll +\nNt∑\ni=1\nτirtime,\nwhere Ct is the number of collisions occurring at time t, and N t is number of cars present. The simulation is terminated after 40 steps and is classified as a failure if one or more more collisions have occurred. Details of the input representation, training and other game variations can be found in Appendix A.\nIn Table 2, we show the probability of failure of a variety of different module/communication method pairs. Continuous communication between cars significantly reduces the failure rate for all module types. Discrete communication did not give any benefit, except for the easy game. We also tried a dense communication baseline by allowing the matrix T to be arbitrary, resulting in a single large fully-connected network controlling all agents. However, this did not work as well as continuous communication (a video showing this model before and after training can be found at https://youtu.be/onK98y-UNHQ). We also explores how partial visibility within the environment effects the advantage given by communication. As the vision range of each agent decreases, the advantage of communication increases. Impressively, with zero visibility (the cars are driving blind) the continuous communication model is still able to succeed 90% of the time.\n5.2.2 Analysis of Communication We now attempt to understand what the agents communicate when performing the junction task. We start by recording the hidden state hij of each agent and the corresponding communication vectors c̃i+1j = C\ni+1hij (the contribution agent j at step i + 1 makes to the hidden state of other agents). Fig. 3(left) and Fig. 3(right) show the 2D PCA projections of the communication and hidden state vectors respectively. These plots show a diverse range of hidden states but far more clustered communication vectors, many of which are close to zero. This suggests that while the hidden state carries information, the agent often prefers not to communicate it to the others unless necessary. This is a possible consequence of the broadcast channel: if everyone talks at the same time, no-one can understand. See Appendix B for norm of communication vectors and brake locations.\nTo better understand the meaning behind the communication vectors, we ran the simulation with only two cars and recorded their communication vectors and locations whenever one of them braked.\nVectors belonging to the clusters A, B & C in Fig. 3(left) were consistently emitted when one of the cars was in a specific location, shown by the colored circles in Fig. 3(middle) (or pair of locations for cluster C). They also strongly correlated with the other car braking at the locations indicated in red, which happen to be relevant to avoiding collision.\n5.2.3 Combat Task\nWe simulate a simple battle involving two opposing teams in a 15× 15 grid as shown in Fig. 2(right). Each team consists of m = 5 agents and their initial positions are sampled uniformly in a 5 × 5 square around the team center, which is picked uniformly in the grid. At each time step, an agent can perform one of the following actions: move one cell in one of four directions; attack another agent by specifying its ID j (there are m attack actions, each corresponding to one enemy agent); or do nothing. If agent A attacks agent B, then B’s health point will be reduced by 1, but only if B is inside the firing range of A (its surrounding 3× 3 area). Agents need one time step of cooling down after an attack, during which they cannot attack. All agents start with 3 health points, and die when their health reaches 0. A team will win if all agents in the other team die. The simulation ends when one team wins, or neither of teams win within 40 time steps (a draw).\nThe model controls one team during training, and the other team consist of bots that follow a hardcoded policy. The bot policy is to attack the nearest enemy agent if it is within its firing range. If not, it approaches the nearest visible enemy agent within visual range. An agent is visible to all bots, if it is inside the visual range of any individual bot. This shared vision gives an advantage to the bot team. When input to a model, each agent is represented by a set of one-hot binary vectors {i, t, l, h, c} encoding its unique ID, team ID, location, health points and cooldown. A model controlling an agent also sees other agents in its visual range (3× 3 surrounding area). The model gets reward of -1 if the team loses or draws at the end of the game. In addition, it also get reward of −0.1 times the total health points of the enemy team, which encourages it to attack enemy bots.\nTable 3 shows the win rate of different module choices with various types of communication. Among different modules, the LSTM achieved the best performance. Continuous communication improved all module types. With the MLP module, we tried dense and discrete communication types but they degraded performance relative to no communication. We also explored several variations of the task: varying the number of agents in each team by setting m = 3, 10, and increasing visual range of agents to 5 × 5 area. The result on those tasks are shown on the right side of Table 3. Using continuous communication (CommNN model) consistently improves the win rate, even with the greater environment observability of the 5×5 vision case.\n5.3 bAbI tasks\nWe apply our model to the bAbI [35] toy Q & A dataset, which consists of 20 tasks each requiring different kind of reasoning. The goal is to answer a question after reading a short story. We can formulate this as a multi-agent task by giving each sentence of the story its own agent. Communication among agents allows them to exchange useful information necessary to answer the question.\nThe input is {s1, s2, ..., sJ , q}, where sj is j’th sentence of the story, and q is the question sentence. We use the same encoder representation as [28] to convert them to vectors. The f(.) module consists of a two-layer MLP with ReLU non-linearities. After K = 3 communication steps, we sample an output word y from a softmax decoder layer. The model is trained in a supervised fashion using a cross-entropy loss between y and the correct answer y∗. The hidden layer size is set 100 and weights are initialized fromN(0, 0.2). We train the model 100 epochs with learning rate 0.003 and mini-batch size 32 with Adam optimizer [11] (β1 = 0.9, β2 = 0.99, = 10−6). We used 10% of training data as validation set to find optimal hyper-parameters for the model.\nResults on the 10K version of the bAbI task are shown in Table 4, along with other baselines (see Appendix C for a detailed breakdown). Our model is doing better than LSTM baseline, but worse than the MemN2N model [28], which is specifically designed to solve reasoning over long stories. However, it successfully solves most of the tasks, including ones that require information sharing between two or more agents through communication.\n6 Discussion and Future Work\nWe have introduced CommNN, a simple controller for MARL that is able to learn continuous communication between a dynamically changing set of agents. Evaluations on four diverse tasks clearly show the model outperforms models without communication, “dense” baselines , and models using discrete communication. Despite the simplicity of the broadcast channel, examination of the traffic task reveals the model to have learned a sparse communication protocol that conveys meaningful information between agents.\nOne aspect of our model that we did not fully exploit is its ability to handle heterogenous agent types and we hope to explore this in future work. Furthermore, we believe the model will scale gracefully to large numbers of agents, perhaps requiring more sophisticated connectivity structures; we also leave this to future work.\nAcknowledgements\nThe authors wish to thank Daniel Lee and Y-Lan Boureau for their advice and guidance.\nReferences [1] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In ICML, 2009. [2] L. Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of multiagent reinforcement learning.\nSystems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, 38(2):156–172, 2008.\n[3] Y. Cao, W. Yu, W. Ren, and G. Chen. An overview of recent progress in the study of distributed multi-agent coordination. IEEE Transactions on Industrial Informatics, 1(9):427–438, 2013.\n[4] R. H. Crites and A. G. Barto. Elevator group control using multiple reinforcement learning agents. Machine Learning, 33(2):235–262, 1998.\n[5] J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson. Learning to communicate to solve riddles with deep distributed recurrent q-networks. arXiv, abs/1602.02672, 2016.\n[6] D. Fox, W. Burgard, H. Kruppa, and S. Thrun. Probabilistic approach to collaborative multi-robot localization. Autonomous Robots, 8(3):325––344, 2000.\n[7] C. L. Giles and K. C. Jim. Learning communication for multi-agent systems. In Innovative Concepts for Agent Based Systems, pages 377—-390. Springer, 2002.\n[8] C. Guestrin, D. Koller, and R. Parr. Multiagent planning with factored mdps. In NIPS, 2001. [9] X. Guo, S. Singh, H. Lee, R. L. Lewis, and X. Wang. Deep learning for real-time atari game play using\noffline monte-carlo tree search planning. In NIPS, 2014. [10] T. Kasai, H. Tenmoto, and A. Kamiya. Learning of communication codes in multi-agent reinforcement\nlearning problem. IEEE Conference on Soft Computing in Industrial Applications, pages 1–6, 2008. [11] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In The International Conference on\nLearning Representations, 2015. [12] M. Lauer and M. A. Riedmiller. An algorithm for distributed reinforcement learning in cooperative\nmulti-agent systems. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML ’00, 2000.\n[13] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. arXiv:1504.00702, 2015.\n[14] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks, 2015. [15] M. L. Littman. Value-function reinforcement learning in markov games. Cognitive Systems Research,\n2(1):55–66, 2001. [16] C. J. Maddison, A. Huang, I. Sutskever, and D. Silver. Move evaluation in go using deep convolutional\nneural networks. In ICLR, 2015. [17] D. Maravall, J. De Lope, and R. Domnguez. Coordination of communication in robot teams by reinforce-\nment learning. Robotics and Autonomous Systems, 61(7):661–666, 2013. [18] M. Matari. Reinforcement learning in the multi-robot domain. Autonomous Robots, 4(1):73–83, 1997. [19] F. S. Melo, M. Spaan, and S. J. Witwicki. Querypomdp: Pomdp-based communication in multiagent\nsystems. In Multi-Agent Systems, pages 189–204, 2011. [20] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller,\nA. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\n[21] R. Olfati-Saber, J. Fax, and R. Murray. Consensus and cooperation in networked multi-agent systems. Proceedings of the IEEE, 95(1):215–233, 2007.\n[22] J. Pearl. Reverend bayes on inference engines: A distributed hierarchical approach. In AAAI, pages 133–136, 1982.\n[23] B. Peng, Z. Lu, H. Li, and K. Wong. Towards Neural Network-based Reasoning. ArXiv preprint: 1508.05508, 2015.\n[24] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Trans. Neural Networks, 20(1):61–80, 2009.\n[25] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016.\n[26] P. Stone and M. Veloso. Towards collaborative and adversarial learning: A case study in robotic soccer. International Journal of Human Computer Studies, (48), 1998.\n[27] S. Sukhbaatar, A. Szlam, G. Synnaeve, S. Chintala, and R. Fergus. Mazebase: A sandbox for learning from games. CoRR, abs/1511.07401, 2015.\n[28] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus. End-to-end memory networks. NIPS, 2015. [29] R. S. Sutton and A. G. Barto. Introduction to Reinforcement Learning. MIT Press, 1998. [30] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, and R. Vicente. Multiagent cooperation\nand competition with deep reinforcement learning. arXiv:1511.08779, 2015. [31] M. Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In ICML, 1993.\n[32] T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.\n[33] P. Varshavskaya, L. P. Kaelbling, and D. Rus. Distributed Autonomous Robotic Systems 8, chapter Efficient Distributed Reinforcement Learning through Agreement, pages 367–378. Springer Berlin Heidelberg, 2009.\n[34] X. Wang and T. Sandholm. Reinforcement learning to play an optimal nash equilibrium in team markov games. In Advances in neural information processing systems, pages 1571–1578, 2002.\n[35] J. Weston, A. Bordes, S. Chopra, and T. Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. In arXiv preprint: 1502.05698, 2015.\n[36] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Machine Learning, pages 229–256, 1992.\n[37] C. Xiong, S. Merity, and R. Socher. Dynamic memory networks for visual and textual question answering. CoRR, abs/1603.01417, 2016.\n[38] C. Zhang and V. Lesser. Coordinating multi-agent reinforcement learning with limited communication. In Proceedings of the 2013 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS ’13, pages 1101–1108, 2013.\n[39] Łukasz Kaiser and I. Sutskever. Neural gpus learn algorithms, 2015.\nA Traffic Junction\nEach car is represented by one-hot binary vector set {n, l, r}, that encode its unique ID, current location and assigned route number respectively. Each agent controlling a car can only observe other cars in the surrounding 3 × 3 neighborhood (however it can communicate to all cars). The state vector for each agent is thus a concatenation of all these vectors, having dimension 32 × |n| × |l| × |r|. We use curriculum learning [1] to make the training easier. In first 100 epochs of training, we set parrive = 0.05, but linearly increased it to 0.2 during next 100 epochs. Finally, training continues for another 100 epochs. The learning rate is fixed at 0.003 throughout. We also implemented additional easy and hard versions of the game, the latter being shown in Fig.2(middle).\nThe easy version is a junction of two one-way roads on a 7× 7 grid. There are two arrival points, each with two possible routes. During curriculum, we increase Ntotal from 3 to 5, and parrive from 0.1 to 0.3.\nThe harder version consists from four connected junctions of two-way roads in 18×18 as shown in Fig.2(center). There are 8 arrival points and 7 different routes for each arrival point. We set Ntotal = 20, and increased parrive from 0.02 to 0.05 during curriculum.\nB Traffic Junction Analysis\nHere we visualize the average norm of the communication vectors and brake locations over the 14× 14 spatial grid.\nC bAbI tasks\nHere we give further details of the model setup and training, as well as a breakdown of results in Table 4.\nLet the task be {s1, s2, ..., sJ , q, y∗}, where sj is j’th sentence of story, q is the question sentence and y∗ is the correct answer word (when answer is multiple words, we simply concatenate them into single word). Then the input to the model is\nh0j = r(sj , θ0), c 0 j = r(q, θq).\nHere, we use simple position encoding [28] as r to convert sentences into fixed size vectors. Also, the initial communication is used to broadcast the question to all agents. Since the temporal ordering of sentences is relevant in some tasks, we add special temporal word “t = j” to sj for all j.\nFor f module, we use a 2 layer network with skip connection, that is\nhi+1j = σ(Wiσ(A ihij +B icij + h 0 j )),\nwhere σ is ReLU non-linearity (bias terms are omitted for clarity). After K = 3 communication steps, the model outputs an answer word by\ny = Softmax(D J∑ j=1 hKj )\nSince we have the correct answer during training, we will do supervised learning by using cross entropy cost on {y∗, y}. The hidden layer size is set 100 and weights are initialized from N(0, 0.2). We train the model 100 epochs with learning rate 0.003 and mini-batch size 32 with Adam optimizer [11] (β1 = 0.9, β2 = 0.99, = 10−6). We used 10% of training data as validation set to find optimal hyper-parameters for the model."
    } ],
    "references" : [ {
      "title" : "Curriculum learning",
      "author" : [ "Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston" ],
      "venue" : "ICML",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A comprehensive survey of multiagent reinforcement learning",
      "author" : [ "L. Busoniu", "R. Babuska", "B. De Schutter" ],
      "venue" : "Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, 38(2):156–172",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "An overview of recent progress in the study of distributed multi-agent coordination",
      "author" : [ "Y. Cao", "W. Yu", "W. Ren", "G. Chen" ],
      "venue" : "IEEE Transactions on Industrial Informatics, 1(9):427–438",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Elevator group control using multiple reinforcement learning agents",
      "author" : [ "R.H. Crites", "A.G. Barto" ],
      "venue" : "Machine Learning, 33(2):235–262",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "N",
      "author" : [ "J.N. Foerster", "Y.M. Assael" ],
      "venue" : "de Freitas, and S. Whiteson. Learning to communicate to solve riddles with deep distributed recurrent q-networks. arXiv, abs/1602.02672",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Probabilistic approach to collaborative multi-robot localization",
      "author" : [ "D. Fox", "W. Burgard", "H. Kruppa", "S. Thrun" ],
      "venue" : "Autonomous Robots, 8(3):325––344",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Learning communication for multi-agent systems",
      "author" : [ "C.L. Giles", "K.C. Jim" ],
      "venue" : "Innovative Concepts for Agent Based Systems, pages 377—-390. Springer",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Multiagent planning with factored mdps",
      "author" : [ "C. Guestrin", "D. Koller", "R. Parr" ],
      "venue" : "NIPS",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Deep learning for real-time atari game play using offline monte-carlo tree search planning",
      "author" : [ "X. Guo", "S. Singh", "H. Lee", "R.L. Lewis", "X. Wang" ],
      "venue" : "NIPS",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning of communication codes in multi-agent reinforcement learning problem",
      "author" : [ "T. Kasai", "H. Tenmoto", "A. Kamiya" ],
      "venue" : "IEEE Conference on Soft Computing in Industrial Applications, pages 1–6",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "The International Conference on Learning Representations",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An algorithm for distributed reinforcement learning in cooperative multi-agent systems",
      "author" : [ "M. Lauer", "M.A. Riedmiller" ],
      "venue" : "Proceedings of the Seventeenth International Conference on Machine Learning, ICML ’00",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "End-to-end training of deep visuomotor policies",
      "author" : [ "S. Levine", "C. Finn", "T. Darrell", "P. Abbeel" ],
      "venue" : "arXiv:1504.00702",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and R",
      "author" : [ "Y. Li", "D. Tarlow", "M. Brockschmidt" ],
      "venue" : "Zemel. Gated graph sequence neural networks",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Value-function reinforcement learning in markov games",
      "author" : [ "M.L. Littman" ],
      "venue" : "Cognitive Systems Research, 2(1):55–66",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Move evaluation in go using deep convolutional neural networks",
      "author" : [ "C.J. Maddison", "A. Huang", "I. Sutskever", "D. Silver" ],
      "venue" : "ICLR",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Coordination of communication in robot teams by reinforcement learning",
      "author" : [ "D. Maravall", "J. De Lope", "R. Domnguez" ],
      "venue" : "Robotics and Autonomous Systems, 61(7):661–666",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Reinforcement learning in the multi-robot domain",
      "author" : [ "M. Matari" ],
      "venue" : "Autonomous Robots, 4(1):73–83",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Querypomdp: Pomdp-based communication in multiagent systems",
      "author" : [ "F.S. Melo", "M. Spaan", "S.J. Witwicki" ],
      "venue" : "Multi-Agent Systems, pages 189–204",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis" ],
      "venue" : "Nature, 518(7540):529–533",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Consensus and cooperation in networked multi-agent systems",
      "author" : [ "R. Olfati-Saber", "J. Fax", "R. Murray" ],
      "venue" : "Proceedings of the IEEE, 95(1):215–233",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Reverend bayes on inference engines: A distributed hierarchical approach",
      "author" : [ "J. Pearl" ],
      "venue" : "AAAI, pages 133–136",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "Towards Neural Network-based Reasoning",
      "author" : [ "B. Peng", "Z. Lu", "H. Li", "K. Wong" ],
      "venue" : "ArXiv preprint: 1508.05508",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The graph neural network model",
      "author" : [ "F. Scarselli", "M. Gori", "A.C. Tsoi", "M. Hagenbuchner", "G. Monfardini" ],
      "venue" : "IEEE Trans. Neural Networks, 20(1):61–80",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "G",
      "author" : [ "D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre" ],
      "venue" : "Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Towards collaborative and adversarial learning: A case study in robotic soccer",
      "author" : [ "P. Stone", "M. Veloso" ],
      "venue" : "International Journal of Human Computer Studies, (48)",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Mazebase: A sandbox for learning from games",
      "author" : [ "S. Sukhbaatar", "A. Szlam", "G. Synnaeve", "S. Chintala", "R. Fergus" ],
      "venue" : "CoRR, abs/1511.07401",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "End-to-end memory networks",
      "author" : [ "S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus" ],
      "venue" : "NIPS",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Introduction to Reinforcement Learning",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT Press",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Multiagent cooperation and competition with deep reinforcement learning",
      "author" : [ "A. Tampuu", "T. Matiisen", "D. Kodelja", "I. Kuzovkin", "K. Korjus", "J. Aru", "R. Vicente" ],
      "venue" : "arXiv:1511.08779",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multi-agent reinforcement learning: Independent vs",
      "author" : [ "M. Tan" ],
      "venue" : "cooperative agents. In ICML",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "T. Tieleman", "G. Hinton" ],
      "venue" : "COURSERA: Neural Networks for Machine Learning,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2012
    }, {
      "title" : "Distributed Autonomous Robotic Systems 8",
      "author" : [ "P. Varshavskaya", "L.P. Kaelbling", "D. Rus" ],
      "venue" : "chapter Efficient Distributed Reinforcement Learning through Agreement, pages 367–378. Springer Berlin Heidelberg",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Reinforcement learning to play an optimal nash equilibrium in team markov games",
      "author" : [ "X. Wang", "T. Sandholm" ],
      "venue" : "Advances in neural information processing systems, pages 1571–1578",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Towards ai-complete question answering: A set of prerequisite toy tasks",
      "author" : [ "J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov" ],
      "venue" : "arXiv preprint: 1502.05698",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "R.J. Williams" ],
      "venue" : "Machine Learning, pages 229–256",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Dynamic memory networks for visual and textual question answering",
      "author" : [ "C. Xiong", "S. Merity", "R. Socher" ],
      "venue" : "CoRR, abs/1603.01417",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Coordinating multi-agent reinforcement learning with limited communication",
      "author" : [ "C. Zhang", "V. Lesser" ],
      "venue" : "Proceedings of the 2013 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS ’13, pages 1101–1108",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Practical examples include elevator control [4] and sensor networks [6]; communication is also important for success in robot soccer [26].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 5,
      "context" : "Practical examples include elevator control [4] and sensor networks [6]; communication is also important for success in robot soccer [26].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "Practical examples include elevator control [4] and sensor networks [6]; communication is also important for success in robot soccer [26].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : "While the model controlling each agent is typically learned via reinforcement learning [2, 29], the specification and format of the communication is usually pre-determined.",
      "startOffset" : 87,
      "endOffset" : 94
    }, {
      "referenceID" : 28,
      "context" : "While the model controlling each agent is typically learned via reinforcement learning [2, 29], the specification and format of the communication is usually pre-determined.",
      "startOffset" : 87,
      "endOffset" : 94
    }, {
      "referenceID" : 35,
      "context" : "We use policy gradient [36] with a state specific baseline for delivering a gradient to the model.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 32,
      "context" : "47 We use policy gradient [33] with a state specific baseline for delivering a gradient to the model.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 32,
      "context" : "47 We use policy gr dient [33] with a state specific baseline for delivering a gradient to the model.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 32,
      "context" : "47 We use policy gradient [33] with a state specific baseline for delivering a gradient to the model.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 32,
      "context" : "47 We use policy gr dient [33] with a state specific baseline for delivering a gradient to the model.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 32,
      "context" : "47 We use policy gradient [33] with a state specific baseline for delivering a gradient to the model.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 32,
      "context" : "47 We use policy gr dient [33] with a state specific baseline for delivering a gradient to the model.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 21,
      "context" : "The edges within the graph represent the communication channel tween agents, with (4) being equivalent to belief propagation [22].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 13,
      "context" : "[14].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "But if these are viewed as internal time steps of the agent, then the communication output can be treated as an action of the agent at a given (internal) time step and we can direct employ the policy gradient [36].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 8,
      "context" : "4 Related Work Our model combines a deep network with reinforcement learning [9, 20, 13].",
      "startOffset" : 77,
      "endOffset" : 88
    }, {
      "referenceID" : 19,
      "context" : "4 Related Work Our model combines a deep network with reinforcement learning [9, 20, 13].",
      "startOffset" : 77,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "4 Related Work Our model combines a deep network with reinforcement learning [9, 20, 13].",
      "startOffset" : 77,
      "endOffset" : 88
    }, {
      "referenceID" : 15,
      "context" : "Several recent works have applied these methods to multi-agent domains, such as Go [16, 25] and Atari games [30], but they assume full visibility of the environment and lack communication.",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 24,
      "context" : "Several recent works have applied these methods to multi-agent domains, such as Go [16, 25] and Atari games [30], but they assume full visibility of the environment and lack communication.",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 29,
      "context" : "Several recent works have applied these methods to multi-agent domains, such as Go [16, 25] and Atari games [30], but they assume full visibility of the environment and lack communication.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "There is a rich literature on multi-agent reinforcement learning (MARL) [2], particularly in the robotics domain [18, 26, 6, 21, 3].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "There is a rich literature on multi-agent reinforcement learning (MARL) [2], particularly in the robotics domain [18, 26, 6, 21, 3].",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 25,
      "context" : "There is a rich literature on multi-agent reinforcement learning (MARL) [2], particularly in the robotics domain [18, 26, 6, 21, 3].",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "There is a rich literature on multi-agent reinforcement learning (MARL) [2], particularly in the robotics domain [18, 26, 6, 21, 3].",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "There is a rich literature on multi-agent reinforcement learning (MARL) [2], particularly in the robotics domain [18, 26, 6, 21, 3].",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "There is a rich literature on multi-agent reinforcement learning (MARL) [2], particularly in the robotics domain [18, 26, 6, 21, 3].",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 11,
      "context" : "Amongst fully cooperative algorithms, many approaches [12, 15, 34] avoid the need for communication by making strong assumptions about visibility of other agents and the environment.",
      "startOffset" : 54,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : "Amongst fully cooperative algorithms, many approaches [12, 15, 34] avoid the need for communication by making strong assumptions about visibility of other agents and the environment.",
      "startOffset" : 54,
      "endOffset" : 66
    }, {
      "referenceID" : 33,
      "context" : "Amongst fully cooperative algorithms, many approaches [12, 15, 34] avoid the need for communication by making strong assumptions about visibility of other agents and the environment.",
      "startOffset" : 54,
      "endOffset" : 66
    }, {
      "referenceID" : 30,
      "context" : "Others use communication, but with a pre-determined protocol [31, 19, 38, 17].",
      "startOffset" : 61,
      "endOffset" : 77
    }, {
      "referenceID" : 18,
      "context" : "Others use communication, but with a pre-determined protocol [31, 19, 38, 17].",
      "startOffset" : 61,
      "endOffset" : 77
    }, {
      "referenceID" : 37,
      "context" : "Others use communication, but with a pre-determined protocol [31, 19, 38, 17].",
      "startOffset" : 61,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : "Others use communication, but with a pre-determined protocol [31, 19, 38, 17].",
      "startOffset" : 61,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "[10] and Varshavskaya et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[33], both use distributed tabular-RL approaches for simulated tasks.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "Giles & Jim [7] use an evolutionary algorithm, rather than reinforcement learning.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 7,
      "context" : "[8] use a single large MDP to control a collection of agents, via a factored message passing framework where the messages are learned.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 23,
      "context" : "[24], as expanded on by Li et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "In particular, in [24], the output of the model is the fixed point of iterating equations (4) and (2) to convergence, using recurrent models.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "In [14], these recurrence equations are unrolled a fixed number of steps and the model trained via backprop through time.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 26,
      "context" : "2 Cooperative Games In this section, we consider two multi-agent tasks in the MazeBase environment [27] that use reward as their training signal.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 31,
      "context" : "Both tasks are trained for 300 epochs, each epoch being 100 weight updates with RMSProp [32] on mini-batch of 288 game episodes (distributed over multiple CPU cores).",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 34,
      "context" : "3 bAbI tasks We apply our model to the bAbI [35] toy Q & A dataset, which consists of 20 tasks each requiring different kind of reasoning.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 27,
      "context" : "We use the same encoder representation as [28] to convert them to vectors.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "003 and mini-batch size 32 with Adam optimizer [11] (β1 = 0.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 27,
      "context" : "Our model is doing better than LSTM baseline, but worse than the MemN2N model [28], which is specifically designed to solve reasoning over long stories.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : "> 5%) LSTM [28] 36.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 27,
      "context" : "4 16 MemN2N [28] 4.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 36,
      "context" : "2 3 DMN+ [37] 2.",
      "startOffset" : 9,
      "endOffset" : 13
    } ],
    "year" : 2016,
    "abstractText" : "Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNN, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.",
    "creator" : "LaTeX with hyperref package"
  }
}