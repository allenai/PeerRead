{
  "name" : "1511.09058.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Ira Kudryashova" ],
    "emails" : [ "malyshki@ton.ioffe.ru" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n09 05\n8v 2\n[ cs\n.L G\n] 2\nD ec\n2 01\n5"
    }, {
      "heading" : "Multiple–Instance Learning: Radon–Nikodym Approach to",
      "text" : ""
    }, {
      "heading" : "Distribution Regression Problem.",
      "text" : "Vladislav Gennadievich Malyshkin∗\nIoffe Institute, Politekhnicheskaya 26, St Petersburg, 194021, Russia\n(Dated: November, 27, 2015)\n$Id: DistReg1Step.tex,v 1.41 2015/12/02 11:00:50 mal Exp $\nFor distribution regression problem, where a bag of x–observations is mapped to a\nsingle y value, a one–step solution is proposed. The problem of random distribution\nto random value is transformed to random vector to random value by taking distri-\nbution moments of x observations in a bag as random vector. Then Radon–Nikodym\nor least squares theory can be applied, what give y(x) estimator. The probability\ndistribution of y is also obtained, what requires solving generalized eigenvalues prob-\nlem, matrix spectrum (not depending on x) give possible y outcomes and depending\non x probabilities of outcomes can be obtained by projecting the distribution with\nfixed x value (delta–function) to corresponding eigenvector. A library providing nu-\nmerically stable polynomial basis for these calculations is available, what make the\nproposed approach practical.\n∗ malyshki@ton.ioffe.ru\n2 Dedicated to Ira Kudryashova\nI. INTRODUCTION\nMultiple instance learning[1] is an important Machine Learning (ML) concept having numerous applications[2]. In multiple instance learning class label is associated not with a single observation, but with a “bag” of observations. A very close problem is distribution regression problem, where a sample distribution of x is mapped to a single y value. There are numerous heuristics methods developed from both: ML and distribution regression sides, see [3, 4] for review.\nAs in any ML problem the most important part is not so much the learning algorithm, but the way how the learned knowledge is represented. Learned knowledge is often represented as a set of propositional rules, regression function, Neural Network weights, etc. In this paper we consider the case where knowledge is represented as a function of distribution moments. Recent progress in numerical stability of high order moments calculation[5] allow the moments of very high order to be calculated, e.g. in Ref. [6] up to hundreds, thus make this approach practical.\nMost of distribution regression algorithms deploy a two–step type of algorithm[4] to solve the problem. In our previous work [7] a two–step solution with knowledge representation in a form of Christoffel function was developed. However, there is exist a one–step solution to distribution regression problem, a random distribution to random value, that converts each bag’s observations to moments of it, then solving the problem random vector (the moments of random distribution) to random value. Once this transition is made an answer of least squares or Radon–Nikodym type from Ref. [5] can be applied and close form result obtained. The distribution of outcomes, if required, can be obtained by solving generalized eigenvalues problem, then matrix spectrum give possible y outcomes, and the square of projection of localized at given x bag distribution to eigenvector give each outcome probability. This matrix spectrum ideology is similar to the one we used in [7], but is more generic and not reducible to Gauss quadrature.\nThe paper is organized as following: In Section II a general theory of distribution regression is discussed and close form result or least squares and Radon–Nikodym type are presented. Then in Section III an algorithm is described and numerical example of calcula-\n3 tions is presented. In Section IV possible further development is discussed."
    }, {
      "heading" : "II. ONE–STEP SOLUTION",
      "text" : "Consider distribution regression problem where a bag of N observations of x is mapped\nto a single outcome observation y for l = [1..M ].\n(x1, x2, . . . , xj , . . . , xN) (l) → y(l) (1)\nA distribution regression problem can have a goal to estimate y, average of y, distribution of y, etc. given specific value of x\nFor further development we need x basis Qk(x) and some x and y measure. For simplicity, not reducing the generality of the approach, we are going to assume that x measure is a sum over j index ∑N\nj=1, y measure is a ∑M l=1, the basis functions Qk(x) are polynomials\nk = 0..dx − 1, where dx is the number of elements in x basis, typical value for dx is below 10–15.\nLet us convert the problem “random distribution” to “random variable” to the problem “vector of random variables” to “random variable”. The simplest way to obtain “vector of random variables” from x (l) j distributions is to take the moments of it. Now the < Qk > (l) would be this random vector:\n< Qk > (l)=\nN ∑\nj=1\nQk(x (l) j ) (2)\n(\n< Q0 > (l), . . . , < Qdx−1 >\n(l) )\n→ y(l) (3)\nThen the (3) becomes vector to value problem. Introduce\nYq = M ∑\nl=1\ny(l) < Qq > (l) (4)\n(G)qr = M ∑\nl=1\n< Qq > (l)< Qr > (l) (5)\n(yG)qr = M ∑\nl=1\ny(l) < Qq > (l)< Qr > (l) (6)\nThe problem now is to estimate y (or distribution of y) given x distribution, now mapped to a vector of moments < Qk > calculated on this x distribution. Let us denote these input\n4 moments as Mk to avoid confusion with measures on x and y. For the case we study the x value is given, and for a state with exact x the Mk values are:\nMk(x) = NQk(x) (7)\nwhat means that all N observations in a bag give exactly the same x value. The problem now becomes a standard: random vector to random variable. We have solutions of two types for this problem, see [5] Appendix D, Least Squares ALS and Radon–Nikodym ARN . The answers would be:\nALS(x) = dx−1 ∑\nq,r=0\nMq(x) (G) −1 qr Yr (8)\nARN (x) =\ndx−1 ∑\nq,r,s,t=0 Mq(x) (G)\n−1 qr (yG)rs (G) −1 st Mt(x)\ndx−1 ∑ q,r=0 Mq(x) (G) −1 qr Mr(x)\n(9)\nThe (8) is least squares answer to y estimation given x. The (9) is Radon–Nikodym answer to y estimation given x. These are the two y estimators at given x for distribution regression problem 1. These answers can be considered as an extension of least squares and Radon– Nikodym type of interpolation from value to value problem to random distribution to random variable problem. In case N = 1 the ALS and ARN are reduced exactly to value to value problem considered in Ref. [5]. Note, that the ALS(x) answer not necessary preserve y sign, but ARN (x) always preserve y sign, same as in value to value problem.\nIf y distribution at given x need to be estimated this problem can also be solved. With one–step approach of this paper we do not need Qm(y) basis used in two–step approach of Ref. [7] and outcomes of y are estimated from x moments only. Generalized eigenvalues problem[5] give the answer:\ndx−1 ∑\nr=0\n(yG)qr ψ (i) r = y\n(i) dx−1 ∑\nr=0\n(G)qr ψ (i) r (10)\nThe result of (10) is eigenvalues y(i) (possible outcomes) and eigenvectors ψ(i) (can be used to compute the probabilities of outcomes). The problem now becomes: given x value estimate possible y–outcomes and their probabilities. The moments of states with given x value are NQq(x) from (7), so the distribution with (7) moments should be projected to distributions corresponding to ψ(i)q states, the square of this projection give the weight and normalized weight give the probability. This is actually very similar to ideology we used in [7], but the\n5 eigenvalues from (10) no longer have a meaning of Gauss quadrature nodes. The eigenvectors ψ(i)r correspond to distribution with moments < Qq >= ∑dx−1 r=0 (G)qr ψ (i) r , and the distribution with such moments correspond to y(i) value. These distributions can be considered as “natural distribution basis”. This is an important generalizatioh of Refs. [5, 6] approach to random distribution, where natural basis for random value, not random distribution, was considered.\nThe projection of two x distributions with moments M (1) k and M (2) k on each other is\n< M (1)|M (2) >π = dx−1 ∑\nq,r=0\nM (1)q (G) −1 qr M (2) r (11)\nthen the required probabilities, calculated by projecting the (7) distribution to natural basis states, are:\nw(i)(x) =\n\n\ndx−1 ∑\nr=0\nMr(x)ψ (i) r\n\n\n2\n(12)\nP (i)(x) = w(i)(x)/ dx−1 ∑\nr=0\nw(r)(x) (13)\nThe (10) and (13) is one–step answer to distribution regression problem: find the outcomes y(i) and their probabilities P (i)(x). Note, that in this setup possible outcomes y(i) do not depend on x, and only probabilities P (i)(x) of outcomes depend on x. This is different from a two–step solution of [7] where outcomes and their probabilities both depend on x. Also note that ∑dx−1\nr=0 w (r)(x) = ∑dx−1 q,r=0Mq(x) (G) −1 qr Mr(x).\nOne of the major difference between the probabilities (13) and probabilities from Christoffel function approach [7] is that the (13) has a meaning of “true” probability while in two–step solution [7] Christoffel function value is used as a proxy to probability on first step. It is important to note how the knowledge is represented in these models. The model (8) has learned knowledge represented in dx by dx matrix (5) and dx size vector (4). The model (9) as well as distribution answer (13) has learned knowledge represented in two dx by dx matrices (5) and (6)."
    }, {
      "heading" : "III. NUMERICAL ESTIMATION OF ONE–STEP SOLUTION",
      "text" : "Numerical instability similar to the one of two–stage Christoffel function approach [7] also arise for approach in study, but now the situation is much less problematic, because we\n6 do not have y–basis Qm(y), and all the dependence on y enter the answer through matrix (6). In this case the only stable x basis Qk(x) is required.\nThe algorithm for y estimators of (8) or (9) is this: Calculate < Qk > (l) moments from (2, then calculate matrices (5) and (6), if least squares approximation is required also calculate moments (4). In contrast with Christoffel function approach where < QqQr > ; q, r = [0..dx − 1] matrix can be obtained from Qk; k = [0..2dx − 1] moments by application of polynomials multiplication operator, here the (5) and (6) can be hardly obtained this way for N > 1 and should be calculated directly from sample. This is not a big issue, because dx is typically not large. Then inverse matrix (G)qr from (5), this matrix is some kind similar to Gramm matrix, but uses distribution moments, not basis functions. Finally put all these to (8) for least squares y(x) estimation or to (9) for Radon–Nikodym y(x) estimation.\nIf y– distribution is required then solve generalized eigenvalues problem (10), obtain y(i) as possible y–outcomes (they do not depend on x), and calculate x–dependent probabilities (13), these are squared projection coefficient of a state with specific x value, point– distribution (7), or some other x distribution of general form, to ψ(i) eigenvector.\nTo show an application of this approach let us take several simple distribution to apply the theory. Let ǫ be a uniformly distributed [−1; 1] random variable and take N = 1000 and M = 10000. Then consider sample distributions build as following 1) For l = [1..M ] take random x out of [−1; 1] interval. 2) Calculate y = f(x), take this y as y(l). 3) Build a bag of x observations as xj = x + Rǫ; j = [1..N ], where R is a parameter. The following three f(x) functions for building sample distribution are used:\nf(x) = x (14) f(x) = 1\n1 + 25x2 (15)\nf(x) =\n  \n \n0 x ≤ 0 1 x > 0 (16)\nIn Figs. 1, 2, 3, the (8) and (9) answers are presented for f(x) from (14), (15) and (16) respectively for R = {0.1, 0.3} and dx = {10, 20}. The x range is specially taken slightly wider that [−1; 1] interval to see possible divergence outside of measure support. In most cases Radon–Nikodym answer is superior, and in addition to that it preserves the sign of y. Least squares approximation is good for special case f(x) = x and typically diverges at x outside of measure support.\nThe numerical estimation of probability function (the y(i) and P (i)(x)) were also calculated and eigenvalue index i, corresponding to maximal P typically correspond to y(i), for which f(x) is most close. For simplistic case (14) see Fig. 4. See the Ref. [8], file com/polytechnik/ algorithms/ ExampleDistribution1Stage.scala for algorithm implementation."
    }, {
      "heading" : "IV. DISCUSSION",
      "text" : "In this work a one–stage approach is applied to distribution regression problem. The bag’s observations are initially converted to moments, then least squares or Radon–Nikodym theory can be applied and closed form answer to be received. These (8) and (9) estimate y value given x. This answer can be generalized to “what is y estimate given distribution of x”. For this problem obtain moments < Qk >, corresponding to given distribution of x,\n9 first, then use them in (8) or (9) instead of Mk(x), corresponding to localized at x state. Similary, if probabilities of y outcomes are required for given distribution of x, the < Qk > should be used in weights expression (12) instead of Mk(x) (this is a special case of two distribution projection on each other (11)). Computer code implementing the algorithms is available[8].\nAnd in conclusion we want to discuss possible directions of future development.\n• In this work a closed form solution for random distribution to random value problem\n(1) is found. The question arise about problem order increase, replace “random distribution” by “random distribution of random distribution” (or even further “random distribution of random distribution of random distribution”, etc.). In this case each xj in (1) should be treated as a sample distribution itself, and index j then can be treated as 2D index xj1,j2. Working with 2D indexes is actually very similar to working with images, see Ref. [6] where the 2D index was used for image reconstruction by applying Radon–Nikodym or least squares approximation. Similarly, the results of this paper, can be generalized to higher order problems, by considering all indexes as 2D.\n• Obtaining possible y outcomes as matrix spectrum (10) and then calculating their\nprobabilities by projection (11) of given distribution (point distribution (7) is a simplest example of such) to eigenvectors (13) is a powerful approach to estimation of y distribution under given condition. We can expect this approach to show good performance for data drawn from a wide range of probability distributions, especially for distributions that are not normal. The reason is because the (10) is expressed in terms of probability states, what make the role of outliers much less important, compared to methods based on L2 norm, particulary least squares. For example, this approach can be applied to distribtions where only first moment of y is finite, while the L2 norm approaches require second moment of y to be finite, what make them inapplicable to distributions with infinite standard deviation. We expect the (10) approach can be a good foundation for construction of Robust Statistics[9].\n[1] Thomas G Dietterich, Richard H Lathrop, and Tomás Lozano-Pérez, “Solving the multiple\ninstance problem with axis-parallel rectangles,” Artificial intelligence 89, 31–71 (1997).\n10\n[2] Jun Yang, Review of multi-instance learning and its applications , Tech. Rep. (Tech. Rep,\n2005).\n[3] Zhi-Hua Zhou, “Multi-Instance Learning: A Survey,” Department of Computer Science & Technology, Nanjing\n[4] Zoltán Szabó, Arthur Gretton, Barnabás Póczos, and Bharath Sriperumbudur, “Learning\ntheory for distribution regression,” arXiv preprint arXiv:1411.2066 (2014).\n[5] Vladislav Gennadievich Malyshkin and Ray Bakhramov, “Mathematical Foundations of Real-\ntime Equity Trading. Liquidity Deficit and Market Dynamics. Automated Trading Machines.\nhttp://arxiv.org/abs/1510.05510,” ArXiv e-prints (2015), arXiv:1510.05510 [q-fin.CP].\n[6] Vladislav Gennadievich Malyshkin, “Radon–Nikodym approximation in applica-\ntion to image analysis. http://arxiv.org/abs/1511.01887,” ArXiv e-prints (2015),\narXiv:1511.01887 [cs.CV].\n[7] Vladislav Gennadievich Malyshkin, “Multiple–Instance Learning: Christoffel Function Ap-\nproach to Distribution Regression Problem,” ArXiv e-prints (2015), arXiv:1511.07085 [cs.LG].\n[8] Vladislav Gennadievich Malyshkin, (2014), the code for polynomials calculation,\nhttp://www.ioffe.ru/LNEPS/malyshkin/code.html.\n[9] Peter J Huber, Robust statistics (Springer, 2011)."
    } ],
    "references" : [ {
      "title" : "Solving the multiple instance problem with axis-parallel rectangles",
      "author" : [ "Thomas G Dietterich", "Richard H Lathrop", "Tomás Lozano-Pérez" ],
      "venue" : "Artificial intelligence 89, 31–71 (1997).  10",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Review of multi-instance learning and its applications",
      "author" : [ "Jun Yang" ],
      "venue" : "Tech. Rep. (Tech. Rep,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Multi-Instance Learning: A Survey",
      "author" : [ "Zhi-Hua Zhou" ],
      "venue" : "Department of Computer Science & Technology, Nanjing",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Learning theory for distribution regression",
      "author" : [ "Zoltán Szabó", "Arthur Gretton", "Barnabás Póczos", "Bharath Sriperumbudur" ],
      "venue" : "arXiv preprint arXiv:1411.2066 (2014).",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Mathematical Foundations of Realtime Equity Trading. Liquidity Deficit and Market Dynamics. Automated Trading Machines. http://arxiv.org/abs/1510.05510",
      "author" : [ "Vladislav Gennadievich Malyshkin", "Ray Bakhramov" ],
      "venue" : "ArXiv e-prints (2015), arXiv:1510.05510 [q-fin.CP].",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Radon–Nikodym approximation in application to image analysis. http://arxiv.org/abs/1511.01887",
      "author" : [ "Vladislav Gennadievich Malyshkin" ],
      "venue" : "ArXiv e-prints (2015), arXiv:1511.01887 [cs.CV].",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multiple–Instance Learning: Christoffel Function Approach to Distribution Regression Problem",
      "author" : [ "Vladislav Gennadievich Malyshkin" ],
      "venue" : "ArXiv e-prints (2015), arXiv:1511.07085 [cs.LG].",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Multiple instance learning[1] is an important Machine Learning (ML) concept having numerous applications[2].",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "Multiple instance learning[1] is an important Machine Learning (ML) concept having numerous applications[2].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "There are numerous heuristics methods developed from both: ML and distribution regression sides, see [3, 4] for review.",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "There are numerous heuristics methods developed from both: ML and distribution regression sides, see [3, 4] for review.",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "Recent progress in numerical stability of high order moments calculation[5] allow the moments of very high order to be calculated, e.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 5,
      "context" : "[6] up to hundreds, thus make this approach practical.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "Most of distribution regression algorithms deploy a two–step type of algorithm[4] to solve the problem.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : "In our previous work [7] a two–step solution with knowledge representation in a form of Christoffel function was developed.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : "[5] can be applied and close form result obtained.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "This matrix spectrum ideology is similar to the one we used in [7], but is more generic and not reducible to Gauss quadrature.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "We have solutions of two types for this problem, see [5] Appendix D, Least Squares ALS and Radon–Nikodym ARN .",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "[5].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] and outcomes of y are estimated from x moments only.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "Generalized eigenvalues problem[5] give the answer: dx−1 ∑",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "This is actually very similar to ideology we used in [7], but the",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "[5, 6] approach to random distribution, where natural basis for random value, not random distribution, was considered.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "[5, 6] approach to random distribution, where natural basis for random value, not random distribution, was considered.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "This is different from a two–step solution of [7] where outcomes and their probabilities both depend on x.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "One of the major difference between the probabilities (13) and probabilities from Christoffel function approach [7] is that the (13) has a meaning of “true” probability while in two–step solution [7] Christoffel function value is used as a proxy to probability on first step.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : "One of the major difference between the probabilities (13) and probabilities from Christoffel function approach [7] is that the (13) has a meaning of “true” probability while in two–step solution [7] Christoffel function value is used as a proxy to probability on first step.",
      "startOffset" : 196,
      "endOffset" : 199
    }, {
      "referenceID" : 6,
      "context" : "Numerical instability similar to the one of two–stage Christoffel function approach [7] also arise for approach in study, but now the situation is much less problematic, because we",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "[6] where the 2D index was used for image reconstruction by applying Radon–Nikodym or least squares approximation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] Thomas G Dietterich, Richard H Lathrop, and Tomás Lozano-Pérez, “Solving the multiple instance problem with axis-parallel rectangles,” Artificial intelligence 89, 31–71 (1997).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "10 [2] Jun Yang, Review of multi-instance learning and its applications , Tech.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "[3] Zhi-Hua Zhou, “Multi-Instance Learning: A Survey,” Department of Computer Science & Technology, Nanjing [4] Zoltán Szabó, Arthur Gretton, Barnabás Póczos, and Bharath Sriperumbudur, “Learning theory for distribution regression,” arXiv preprint arXiv:1411.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[3] Zhi-Hua Zhou, “Multi-Instance Learning: A Survey,” Department of Computer Science & Technology, Nanjing [4] Zoltán Szabó, Arthur Gretton, Barnabás Póczos, and Bharath Sriperumbudur, “Learning theory for distribution regression,” arXiv preprint arXiv:1411.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "[5] Vladislav Gennadievich Malyshkin and Ray Bakhramov, “Mathematical Foundations of Realtime Equity Trading.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Vladislav Gennadievich Malyshkin, “Radon–Nikodym approximation in application to image analysis.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Vladislav Gennadievich Malyshkin, “Multiple–Instance Learning: Christoffel Function Approach to Distribution Regression Problem,” ArXiv e-prints (2015), arXiv:1511.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2015,
    "abstractText" : "For distribution regression problem, where a bag of x–observations is mapped to a single y value, a one–step solution is proposed. The problem of random distribution to random value is transformed to random vector to random value by taking distribution moments of x observations in a bag as random vector. Then Radon–Nikodym or least squares theory can be applied, what give y(x) estimator. The probability distribution of y is also obtained, what requires solving generalized eigenvalues problem, matrix spectrum (not depending on x) give possible y outcomes and depending on x probabilities of outcomes can be obtained by projecting the distribution with fixed x value (delta–function) to corresponding eigenvector. A library providing numerically stable polynomial basis for these calculations is available, what make the proposed approach practical.",
    "creator" : "gnuplot 4.4 patchlevel 4"
  }
}