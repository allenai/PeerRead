{
  "name" : "1511.04143.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "ACTION SPACE", "Matthew Hausknecht" ],
    "emails" : [ "mhauskn@cs.utexas.edu", "pstone@cs.utexas.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "This paper extends the Deep Deterministic Policy Gradients (DDPG) algorithm (Lillicrap et al., 2015) into a parameterized action space. We document two modifications to the published version of the DDPG algorithm: Q-Diffs and Action-Space bounding. Both of these modifications were necessary for stable learning in this domain and will likely be valuable for future practitioners attempting to learn in continuous and parameterized action spaces.\nUtilizing these extensions, we demonstrate reliable learning, from scratch, of RoboCup soccer policies capable of goal scoring. These policies operate on a low-level continuous state space and a parameterized-continuous action space. Using a single reward function, the agents learn to locate and approach the ball, dribble to the goal, and score on an empty goal. The best learned agent proves more reliable at scoring goals, though slower, than the hand-coded 2012 RoboCup champion.\nRoboCup 2D Half-Field-Offense is a research platform for exploring single agent learning, multiagent learning, and adhoc teamwork. While the domain features several choices of state and action representation, we focus a low-level continuous state space and low-level parameterized action space. More specifically, the parameterized action space requires the agent to first select the type of action it wishes to perform from a discrete list of high level actions and then specify the continuous parameters to accompany that action. This parameterization introduces structure not found in a purely continuous action space.\nThe rest of this paper is organized as follows: the RoboCup 2D simulated soccer domain is presented in Section 2. Section 3 presents background on deep continuous reinforcement learning including detailed actor and critic updates. Section 4 presents our extensions to DDPG. Section 6 covers experiments and results. Finally, related work is presented in Section 7 followed by conclusions."
    }, {
      "heading" : "2 DOMAIN",
      "text" : "Simulated RoboCup 2D soccer is a task in which two teams of simulated autonomous agents compete to score goals. Each agent receives its own state sensations and must independently select its own actions. However, in this study we examine the challenge of learning to score a goal on a single robot. RoboCup 2D soccer is naturally characterized as an episodic multi-agent POMDP because of the sequential partial observations and actions on the part of the agents and the well-defined\nar X\niv :1\n51 1.\n04 14\n3v 1\n[ cs\n.A I]\n1 3\nN ov\n2 01\n5\nepisodes which culminate in either a goal being scored or the ball leaving the play area. To begin each episode, the agent and ball are positioned randomly on the offensive half of the field. The episode ends when a goal is scored, the ball leaves the field, or 500 timesteps pass. The following subsections introduce the low-level state and action space used by agents in this domain."
    }, {
      "heading" : "2.1 STATE SPACE",
      "text" : "The agent uses a low-level, egocentric viewpoint encoded using 58 continuously-valued features. These features are derived through Helios-Agent2D’s (Akiyama, 2010) world model and provide angles and distances to various on-field objects of importance such as the ball, the goal, and the other players. Figure 1 depicts the perceptions of the agent. The most relevant features include: Agent’s position, velocity, and orientation, and stamina; Indicator if the agent is able to kick; Angles and distances to the following objects: Ball, Goal, Field-Corners, Penalty-Box-Corners, Teammates, and Opponents. A full list of state features may be found at https://github.com/mhauskn/ HFO/blob/master/doc/manual.pdf."
    }, {
      "heading" : "2.2 ACTION SPACE",
      "text" : "RoboCup 2D features a low-level, parameterized action space. There are four mutually-exclusive discrete actions: Dash, Turn, Tackle, and Kick. At each timestep the agent must select one of these four to execute. Each action has 1-2 continuously-valued parameters which must also be specified. An agent must select both the discrete action it wishes to execute as well as the continuously valued parameters required by that action. The full set of parameterized actions is:\nDash(power, direction): Moves in the indicated direction with a scalar power in [0, 100]. Movement is faster forward than sideways or backwards. Turn(direction): Turns to indicated direction. Tackle(direction): Contests the ball by moving in the indicated direction. This action is only useful when playing against an opponent. Kick(power, direction): Kicks the ball in the indicated direction with a scalar power in [0, 100]. All directions are parameterized in the range of [−180, 180] degrees."
    }, {
      "heading" : "2.3 REWARD SIGNAL",
      "text" : "True rewards in the RoboCup 2D domain come from winning full games. However, such a reward signal is far too sparse for learning agents to gain traction. Instead we introduce a hand-crafted reward signal with four components: Move To Ball Reward provides a scalar reward proportional\nto the change in distance between the agent and the ball d(a, b). An additional reward Ikick of 1 is given the first time each episode the agent is close enough to kick the ball. Kick To Goal Reward is proportional to the change in distance between the ball and the center of the goal d(b, g). An additional reward is given for scoring a goal Igoal. A weighted sum of these components results in a single reward that first guides the agent close enough to kick the ball, then rewards for kicking towards goal, and finally for scoring. It was necessary to provide a higher gain for the kick-to-goal component of the reward because immediately following each kick, the move-to-ball component produces negative rewards as the ball moves away from the agent. The overall reward is as follows:\nrt = dt−1(a, b)− dt(a, b) + Ikickt + 3 ( dt−1(b, g)− dt(b, g) ) + 5Igoalt (1)\nIt is disappointing that reward engineering is necessary. However, the exploration task proves far too difficult to ever gain traction on a reward that consists only of scoring goals, because acting randomly is exceedingly unlikely to yield even a single goal in any reasonable amount of time. An interesting direction for future work is to find better ways of exploring large state spaces. One recent approach in this direction, Stadie et al. (2015) assigned exploration bonuses based on a model of system dynamics."
    }, {
      "heading" : "3 BACKGROUND: DEEP REINFORCEMENT LEARNING",
      "text" : "Deep neural networks are adept general purpose function approximators that have been most widely used in supervised learning tasks. Recently, however they have been applied to reinforcement learning problems, giving rise to the field of deep reinforcement learning. This field seeks to combine the advances in deep neural networks with reinforcement learning algorithms to create agents capable of acting intelligently in complex environments. This section presents background in deep reinforcement learning in continuous action spaces. The notation closely follows that of Lillicrap et al. (2015).\nDeep, model-free RL in discrete action spaces can be performed using the Deep Q-Learning method introduced by Mnih et al. (2015) which employs a single deep network to estimate the value function of each discrete action and, when acting, selects the maximally valued output for a given state input. Several variants of DQN have been explored. Narasimhan et al. (2015) used decaying traces, Hausknecht & Stone (2015) investigated LSTM recurrency, and van Hasselt et al. (2015) explored double Q-Learning. These networks work well in continuous state spaces but do not function in continuous action spaces because the output nodes of the network, while continuous, are trained to output Q-Value estimates rather than continuous actions.\nAn Actor/Critic architecture (Sutton & Barto, 1998) provides one solution to this problem by decoupling the value learning and the action selection. Represented using two deep neural networks, the actor network outputs continuous actions while the critic estimates the value function. The actor network µ, parameterized by θµ, takes as input a state s and outputs a continuous action a. The critic network Q, parameterized by θQ, takes as input a state s and action a and outputs a scalar Q-Value Q(s, a). Figure 2 shows Critic and Actor networks.\nUpdates to the critic network are largely unchanged from the standard temporal difference update used originally in Q-Learning (Watkins & Dayan, 1992) and later by DQN:\nQ(s, a) = Q(s, a) + α ( r + γmax\na′ Q(s′, a′)−Q(s, a)\n) (2)\nAdapting this equation to the neural network setting described above results in minimizing a loss function defined as follows:\nLQ(s, a|θQ) = ( Q(s, a|θQ)− ( r + γmax\na′ Q(s′, a′|θQ)\n))2 (3)\nHowever, in continuous action spaces, this equation is no longer tractable as it involves maximizing over next-state actions a′. Instead we ask the actor network to provide a next-state action a′ = µ(s′|θµ). This yields a critic loss with the following form:\nLQ(s, a|θQ) = ( Q(s, a|θQ)− ( r + γQ(s′, µ(s′|θµ)′|θQ) ))2 (4)\nThe value function of the critic can be learned by gradient descent on this loss function with respect to θQ. However, the accuracy of this value function is highly influenced by the quality of the actor’s policy, since the actor determines the next-state action a′ in the update target.\nThe critic’s knowledge of action values is then harnessed to learn a better policy for the actor. Given a sample state, the goal of the actor is to minimize the difference between its current output a and the optimal action in that state a∗.\nLµ(s|θµ) = ( a− a∗ )2 = ( µ(s|θQ)− a∗ )2 (5)\nThe critic may be used to provide estimates of the quality of different actions but naively estimating a∗ would involve maximizing the critic’s output over all possible actions: a∗ ≈ argmaxaQ(s, a|θQ). Instead of seeking a global maximum, the critic network can provide gradients which indicate directions of change, in action space, that lead to higher estimated Q-Values: ∇aQ(s, a|θQ). To obtain these gradients requires a single backward pass over the critic network, much faster than solving an optimization problem in continuous action space. Note that these gradients are not the common gradients with respect to parameters. Instead these are gradients with respect to inputs, first used in this way by NFCQA (Hafner & Riedmiller, 2011). To update the actor network, these gradients are placed at the actor’s output layer (in lieu of targets) and then backpropagated through the network. For a given state, the actor is run forward to produce an action that the critic evaluates, and the resulting gradients may be used to update the actor:\n∇θµµ(s) = ∇aQ(s, a|θQ)∇θµµ(s|θµ) (6)\nAlternatively one may think about these updates as simply interlinking the actor and critic networks: On the forward pass, the actor’s output is passed forward into the critic and evaluated. Next, a Q-Diff1 is set for the critic network, instructing it to produce gradients ∇aQ that increase the QValue. On the backwards pass, the gradients flow from the critic through the actor. An update is then performed only over the actor’s parameters. Figure 2 shows an example of this update."
    }, {
      "heading" : "3.1 STABLE UPDATES",
      "text" : "Updates to the critic rely on the assumption that the actor’s policy is a good proxy for the optimal policy. Updates to the actor rest on the assumption that the critic’s gradients, or suggested directions for policy improvement, are valid when tested in the environment. It should come as no surprise that several techniques are necessary to make this learning process stable and convergent.\nBecause the critic’s policy Q(s, a|θQ) influences both the actor and critic updates, errors in the critic’s policy can create destructive feedback resulting in divergence of the actor, critic, or both. To resolve this problem Mnih et al. (2015) introduce a Target-Q-Network Q′, a replica of the critic network that changes on a slower time scale than the critic. This target network is used to generate next state targets for the critic update (Equation 4). Similarly a Target-Actor-Network µ′ combats quick changes in the actor’s policy.\nThe second stabilizing influence is a replay memoryD, a FIFO queue consisting of the agent’s latest experiences (typically one million). Updating from mini-batches of experience sampled uniformly from this memory reduces bias compared to updating exclusively from the most recent experiences.\n1A Q-Diff takes the place of an update target and indicates a direction of change for the critic’s gradients. Section 4.1 provides more detail.\nEmploying these two techniques the critic loss in Equation 4 and actor update in Equation 5 can be stably re-expressed as follows:\nLQ(θ Q) = E(st,at,rt,st+1)∼D [( Q(st, at)− ( rt + γQ ′(st+1, µ ′(st+1)) ))2] (7)\n∇θµµ = Est∼D [ ∇aQ(st, a|θQ)∇θµµ(st)|a=µ(st) ] (8)\nFinally, these updates are applied to the respective networks, where α is a per-parameter step size determined by the gradient descent algorithm. Additionally, the target-actor and target-critic networks are updated to smoothly track the actor and critic using a factor τ 1:\nθQ = θQ + α∇θQLQ(θQ) θµ = θµ + α∇θµµ θQ ′ = τθQ + (1− τ)θQ ′\nθµ ′ = τθµ + (1− τ)θµ ′\n(9)\nOne final component is an adaptive learning rate method such as ADADELTA (Zeiler, 2012), RMSPROP (Tieleman & Hinton, 2012), or ADAM (Kingma & Ba, 2014). Lillicrap et al. (2015) user several other techniques for the DDPG algorithm including batch normalization and OrnsteinUhlenbeck exploration. We find simple epsilon-greedy exploration and no batch normalization is sufficient for this domain."
    }, {
      "heading" : "4 EXTENSIONS TO DDPG",
      "text" : "This section describes three extensions to DDPG that were found necessary for stable learning. The first two are required in any continuous domain; the last is specific to parameterized action domains. In our experiments, the algorithm doesn’t work at all if these extensions are not present. Though\nthere is is no mention of these extensions in DDPG (Lillicrap et al., 2015), we hypothesize that something similar may have been present. One contribution of this paper is making these extensions explicit."
    }, {
      "heading" : "4.1 Q-DIFF FOR ACTOR UPDATE",
      "text" : "In order to update the actor it is necessary to provide a diff to the critic before backprop. This diff replaces a target and instead tells the critic which direction the Q-Values should change. For example, a Q-Diff of −1 tells the critic to generate gradients necessary to increase the Q-Value by one, equivalent to providing a target equal to the Q-Value plus one. The experiments in this paper use a Q-Diff of −10−2. Learning is markedly less stable with Q-Diffs approaching −1, demonstrating that the critic’s gradients∇aQ(st, a|θQ) are sensitive to the setting of this parameter."
    }, {
      "heading" : "4.2 BOUNDING ACTION SPACE GRADIENTS",
      "text" : "Despite the added stability measures, when applying the updates in Section 3.1 we quickly observed continuous action values growing very large, and upwards divergence of the critic’s estimated QValues. Upward divergence indicates that the actor’s policy was indeed moving in a direction that increased the critic’s valuation. However, if unbounded, these updates would result in infinite QValues within a few hundred iterations. This instability stems from the fact that the critic’s manifold contains directions for improvement in action space which lead to unbounded Q-Values in the limit. We address the issue by bounding the continuous action space – if the critic’s gradient ∇aQ(st, a|θQ) suggests increasing/decreasing an action whose activation has reached or exceeded the maximum/minimum of the range, we multiply the gradient for that action by−1. These changes prevent divergence in action space which translates into stability in the Q-Value space. The resulting optimization problem is well posed. The bounds for the discrete components of the action-space are [−1, 1]. The action parameters are already bounded by the RoboCup 2D simulator: [−180, 180] degrees for directions and [0, 100] for power. We enforced these boundaries as they are."
    }, {
      "heading" : "4.3 PARAMETERIZED ACTION SPACE ARCHITECTURE",
      "text" : "Following notation in (Masson & Konidaris, 2015), a Parameterized Action Space Markov Decision Process (PAMDP) is defined by a set of discrete actions Ad = {a1, a2, . . . , ak}. Each discrete action a ∈ Ad features ma continuous parameters {pa1 , . . . , pama} ∈ R\nma . Actions are represented by tuples (a, pa1 , . . . , p a ma). Thus the overall action space A = ∪a∈Ad(a, p a 1 , . . . , p a ma).\nThe parameterized action space induces latent structure. Namely, only one discrete action may be selected at each timestep and only the parameters for that action are used. The parameters associated with all other discrete actions are ignored. For example, if the actor chooses discrete action a, all other discrete actions a′ 6= a and their parameters pa′ 6=a are ignored. This structure is not explicitly revealed to the neural network or learning algorithm. While it would be possible to actively enforce this structure, perhaps by nullifying gradients to parameters associated with unused actions (e.g. pa\n′ 6=a), we instead rely on the ability of the critic to learn the latent action structure and provide sensible gradients. The experiments in Section 6 show that despite the latent structure, the critic still provides valid directions of improvement to the actor. Having presented the learning algorithm, we now describe the architecture and hyperparameters."
    }, {
      "heading" : "5 NETWORK ARCHITECTURE AND EXPERIMENTAL DETAILS",
      "text" : "Both the actor and critic share the same architecture: The 58 state inputs are processed by four fully connected layers consisting of 1024-512-256-128 units respectively. Each fully connected layer is followed by a rectified linear (ReLU) activation function with negative slope 10−2. Weights of the fully connected layers use Gaussian initialization with a standard deviation of 10−2. Connected to the final inner product layer are two output layers: one for the four discrete actions and another for the six parameters accompanying these actions. In addition to the 58 state features, the critic also takes as input the four discrete actions and six action parameters. It outputs a single scalar Q-value. We use the ADAM solver with both actor and critic learning rate set to 10−3. Target\nnetworks track the actor and critic using a τ = 10−4. Complete source code for our agent is available at https://github.com/mhauskn/dqn-hfo and for the RoboCup 2D domain at https://github.com/mhauskn/HFO/."
    }, {
      "heading" : "6 RESULTS",
      "text" : "We evaluate ability of the agent to learn in the parameterized RoboCup 2D domain. Performance is compared over 100 trials to a baseline agent using a handcoded policy derived from Helios, the 2012 RoboCup 2D champion team (Akiyama, 2010). This policy is designed to coordinate with a full set of teammates and play against a full set of opponents. Thus, it is more than sophisticated enough for the task we examine as may be seen by the sample trajectory in Figure 1.\nTo show that the learning process is reliable, we independently train three agents, DDPG1−3 for 3 million iterations, approximately 20,000 episodes of play. Training each agent took three days on a NVidia Titan-X GPU. Each of these agents is then evaluated for 100 episodes. Figure 3 shows the superimposed results from all three agents. Three distinct phases of learning may be seen: the agents first get small rewards for approaching the ball (episode 1500), then learn to kick the ball towards the goal (episodes 2,000 - 8,000), and start scoring goals around episode 10,000.\nRemarkably, the best of the DDPG agents scores more reliably than the handcoded champion, Agent2D. Failures result from noise in the action space, which occasionally causes missed kicks. DDPG takes a longer time to score each goal (average of 32 extra steps), but becomes more accurate as a result. This extra time is reasonable considering DDPG is rewarded only for scoring and experiences no real pressure to score more quickly."
    }, {
      "heading" : "7 RELATED WORK",
      "text" : "RoboCup 2D soccer has a rich history of learning. In one of the earliest examples, Andre & Teller (1999) used Genetic Programming to evolve policies for RoboCup 2D Soccer. By using a sequence of reward functions, they first encourage the players to approach the ball, kick the ball, score a goal, and finally to win the game. Similarly, our work features players whose policies are entirely trained and have no hand-coded components. Our work differs by using a gradient-based learning method paired with using reinforcement learning rather than evolution.\nMasson & Konidaris (2015) present a parameterized-action MDP formulation and approaches for model-free reinforcement learning in such environments. Applied to a simplified abstraction of simulated RoboCup soccer, the resulting agents operate over a parameterized action space and can score on a fixed-policy goalie. There are three main differences from our work: first, Masson and\nKonidaris start each episode by co-locating the agent and ball. In our paper, trials start by randomly positioning both the agent and the ball. Thus our agent’s policy must be able to locate and approach the ball, as in a real game of soccer. Second, Masson and Konidaris use a higher-level action space consisting only of parameterized kick, shoot-left-of-goalie, and shoot-right-of-goalie actions. Their agent automatically moves towards the ball and only needs to learn where to kick. In contrast, our agent must learn to follow the ball while dribbling and must decide how and where to shoot on goal without the benefit of actions to shoot left or right of the goalie. Finally, we use a higher-dimensional state space consisting of 58 continuous features as opposed to the 14 used by Masson and Konidaris.\nCompetitive RoboCup agents are primarily handcoded but may feature components that are learned or optimized. MacAlpine et al. (2015) employed the layered-learning framework to incrementally learn a series of interdependent behaviors. Perhaps the best example of comprehensively integrating learning is the Brainstormers who, in competition, use a neural network to make a large portion of decisions spanning low level skills through high level strategy (Riedmiller et al., 2009; Riedmiller & Gabel, 2007). However their work was done prior to the advent of deep reinforcement learning, and thus required more constrained, focused training environments for each of their skills. In contrast, our study learns to approach the ball, kick towards the goal, and score, all within the context of a single, monolithic policy.\nDeep learning methods have proven useful in various control domains. As previously mentioned DQN (Mnih et al., 2015) and DDPG (Lillicrap et al., 2015) provide great starting points for learning in discrete and continuous action spaces. Additionally, Levine et al. (2015) demonstrates the ability of deep learning paired with guided policy search to learn manipulation policies on a physical robot. The high requirement for data (in the form of experience) is a hurdle for applying deep reinforcement learning directly onto robotic platforms. Our work differs by examining an action space with latent structure and parameterized-continuous actions."
    }, {
      "heading" : "8 FUTURE WORK",
      "text" : "The harder task of scoring on a goalie is left for future work. Additionally, the RoboCup domain presents many opportunities for multi-agent collaboration both in an adhoc-teamwork setting (in which a single learning agent must collaborate with unknown teammates) and true multi-agent settings (in which multiple learning agents must collaborate). Challenges in multi-agent learning in the RoboCup domain have been examined by prior work (Kalyanakrishnan et al., 2007) and solutions may translate into the deep reinforcement learning settings as well. Progress in this direction could eventually result in a team of deep reinforcement learning soccer players.\nAnother interesting possibility is utilizing the critic’s gradients with respect to state inputs ∇sQ(s, a|θQ). These gradients indicate directions of improvement in state space. An agent with a forward model may be able to exploit these gradients to transition into states which the critic finds more favorable. Recent developments in model-based deep reinforcement learning (Oh et al., 2015) show that detailed next state models are possible."
    }, {
      "heading" : "9 CONCLUSION",
      "text" : "This paper has presented an agent trained exclusively with deep reinforcement learning which learns from scratch how to approach the ball, kick the ball to goal, and score. The best learned agent scores goals more reliably than a handcoded expert policy. Our work does not address more challenging tasks such as scoring on a goalie or cooperating with a team, but still represents a step towards fully learning complex RoboCup agents. More generally we have demonstrated the capability of deep reinforcement learning in parameterized action space.\nTo make this possible, we extended the DDPG algorithm (Lillicrap et al., 2015), a recent development in deep reinforcement learning. We described two modifications necessary for stable learning: the use of Actor Update Q-Diffs and Bounded Action Space Gradients. These extensions are not specific to the RoboCup 2D domain and will likely prove useful for any continuous (or parameterized) action space."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The authors wish to thank Yilun Chen. This work has taken place in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin. LARG research is supported in part by grants from the National Science Foundation (CNS-1330072, CNS1305287), ONR (21C184-01), AFRL (FA8750-14-1-0070), AFOSR (FA9550-14-1-0087), and Yujin Robot. Additional support from the Texas Advanced Computing Center, and Nvidia Corporation."
    } ],
    "references" : [ {
      "title" : "Evolving Team Darwin United",
      "author" : [ "Andre", "David", "Teller", "Astro" ],
      "venue" : "Lecture Notes in Computer Science,",
      "citeRegEx" : "Andre et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Andre et al\\.",
      "year" : 1999
    }, {
      "title" : "Reinforcement learning in feedback control",
      "author" : [ "Hafner", "Roland", "Riedmiller", "Martin" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Hafner et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hafner et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep recurrent q-learning for partially observable mdps",
      "author" : [ "Hausknecht", "Matthew J", "Stone", "Peter" ],
      "venue" : "CoRR, abs/1507.06527,",
      "citeRegEx" : "Hausknecht et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hausknecht et al\\.",
      "year" : 2015
    }, {
      "title" : "Half field offense in RoboCup soccer: A multiagent reinforcement learning case study",
      "author" : [ "Kalyanakrishnan", "Shivaram", "Liu", "Yaxin", "Stone", "Peter" ],
      "venue" : "Tomoichi (eds.), RoboCup-2006: Robot Soccer World Cup X,",
      "citeRegEx" : "Kalyanakrishnan et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Kalyanakrishnan et al\\.",
      "year" : 2007
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik P", "Ba", "Jimmy" ],
      "venue" : "CoRR, abs/1412.6980,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "End-to-end training of deep visuomotor policies",
      "author" : [ "Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter" ],
      "venue" : "CoRR, abs/1504.00702,",
      "citeRegEx" : "Levine et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2015
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra" ],
      "venue" : null,
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2015
    }, {
      "title" : "UT Austin Villa 2014: RoboCup 3D simulation league champion via overlapping layered learning",
      "author" : [ "MacAlpine", "Patrick", "Depinet", "Mike", "Stone", "Peter" ],
      "venue" : "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "MacAlpine et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "MacAlpine et al\\.",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning with parameterized actions",
      "author" : [ "Masson", "Warwick", "Konidaris", "George" ],
      "venue" : "CoRR, abs/1509.01644,",
      "citeRegEx" : "Masson et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Masson et al\\.",
      "year" : 2015
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg", "Petersen", "Stig", "Beattie", "Charles", "Sadik", "Amir", "Antonoglou", "Ioannis", "King", "Helen", "Kumaran", "Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis" ],
      "venue" : null,
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Language understanding for textbased games using deep reinforcement learning",
      "author" : [ "Narasimhan", "Karthik", "Kulkarni", "Tejas", "Barzilay", "Regina" ],
      "venue" : "CoRR, abs/1506.08941,",
      "citeRegEx" : "Narasimhan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2015
    }, {
      "title" : "Actionconditional video prediction using deep networks in atari",
      "author" : [ "Oh", "Junhyuk", "Guo", "Xiaoxiao", "Lee", "Honglak", "Lewis", "Richard L", "Singh", "Satinder P" ],
      "venue" : "games. CoRR,",
      "citeRegEx" : "Oh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning for robot soccer",
      "author" : [ "Riedmiller", "Martin", "Gabel", "Thomas", "Hafner", "Roland", "Lange", "Sascha" ],
      "venue" : "Autonomous Robots,",
      "citeRegEx" : "Riedmiller et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Riedmiller et al\\.",
      "year" : 2009
    }, {
      "title" : "On experiences in a complex and competitive gaming domain: Reinforcement learning meets robocup",
      "author" : [ "Riedmiller", "Martin A", "Gabel", "Thomas" ],
      "venue" : "In CIG, pp. 17–23",
      "citeRegEx" : "Riedmiller et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Riedmiller et al\\.",
      "year" : 2007
    }, {
      "title" : "Incentivizing exploration in reinforcement learning with deep predictive models",
      "author" : [ "Stadie", "Bradly C", "Levine", "Sergey", "Abbeel", "Pieter" ],
      "venue" : "CoRR, abs/1507.00814,",
      "citeRegEx" : "Stadie et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Stadie et al\\.",
      "year" : 2015
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "Sutton", "Richard S", "Barto", "Andrew G" ],
      "venue" : "URL http://www.cs.ualberta.ca/%7Esutton/book/ ebook/the-book.html",
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "T. Tieleman", "G. Hinton" ],
      "venue" : "COURSERA: Neural Networks for Machine Learning,",
      "citeRegEx" : "Tieleman and Hinton,? \\Q2012\\E",
      "shortCiteRegEx" : "Tieleman and Hinton",
      "year" : 2012
    }, {
      "title" : "Deep reinforcement learning with double qlearning",
      "author" : [ "van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David" ],
      "venue" : "CoRR, abs/1509.06461,",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2015
    }, {
      "title" : "ADADELTA: An adaptive learning rate method",
      "author" : [ "Zeiler", "Matthew D" ],
      "venue" : "CoRR, abs/1212.5701,",
      "citeRegEx" : "Zeiler and D.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler and D.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "This paper extends the Deep Deterministic Policy Gradients (DDPG) algorithm (Lillicrap et al., 2015) into a parameterized action space.",
      "startOffset" : 76,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "One recent approach in this direction, Stadie et al. (2015) assigned exploration bonuses based on a model of system dynamics.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "The notation closely follows that of Lillicrap et al. (2015). Deep, model-free RL in discrete action spaces can be performed using the Deep Q-Learning method introduced by Mnih et al.",
      "startOffset" : 37,
      "endOffset" : 61
    }, {
      "referenceID" : 6,
      "context" : "The notation closely follows that of Lillicrap et al. (2015). Deep, model-free RL in discrete action spaces can be performed using the Deep Q-Learning method introduced by Mnih et al. (2015) which employs a single deep network to estimate the value function of each discrete action and, when acting, selects the maximally valued output for a given state input.",
      "startOffset" : 37,
      "endOffset" : 191
    }, {
      "referenceID" : 6,
      "context" : "The notation closely follows that of Lillicrap et al. (2015). Deep, model-free RL in discrete action spaces can be performed using the Deep Q-Learning method introduced by Mnih et al. (2015) which employs a single deep network to estimate the value function of each discrete action and, when acting, selects the maximally valued output for a given state input. Several variants of DQN have been explored. Narasimhan et al. (2015) used decaying traces, Hausknecht & Stone (2015) investigated LSTM recurrency, and van Hasselt et al.",
      "startOffset" : 37,
      "endOffset" : 430
    }, {
      "referenceID" : 6,
      "context" : "The notation closely follows that of Lillicrap et al. (2015). Deep, model-free RL in discrete action spaces can be performed using the Deep Q-Learning method introduced by Mnih et al. (2015) which employs a single deep network to estimate the value function of each discrete action and, when acting, selects the maximally valued output for a given state input. Several variants of DQN have been explored. Narasimhan et al. (2015) used decaying traces, Hausknecht & Stone (2015) investigated LSTM recurrency, and van Hasselt et al.",
      "startOffset" : 37,
      "endOffset" : 478
    }, {
      "referenceID" : 6,
      "context" : "The notation closely follows that of Lillicrap et al. (2015). Deep, model-free RL in discrete action spaces can be performed using the Deep Q-Learning method introduced by Mnih et al. (2015) which employs a single deep network to estimate the value function of each discrete action and, when acting, selects the maximally valued output for a given state input. Several variants of DQN have been explored. Narasimhan et al. (2015) used decaying traces, Hausknecht & Stone (2015) investigated LSTM recurrency, and van Hasselt et al. (2015) explored double Q-Learning.",
      "startOffset" : 37,
      "endOffset" : 538
    }, {
      "referenceID" : 9,
      "context" : "To resolve this problem Mnih et al. (2015) introduce a Target-Q-Network Q′, a replica of the critic network that changes on a slower time scale than the critic.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "Lillicrap et al. (2015) user several other techniques for the DDPG algorithm including batch normalization and OrnsteinUhlenbeck exploration.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "there is is no mention of these extensions in DDPG (Lillicrap et al., 2015), we hypothesize that something similar may have been present.",
      "startOffset" : 51,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "Perhaps the best example of comprehensively integrating learning is the Brainstormers who, in competition, use a neural network to make a large portion of decisions spanning low level skills through high level strategy (Riedmiller et al., 2009; Riedmiller & Gabel, 2007).",
      "startOffset" : 219,
      "endOffset" : 270
    }, {
      "referenceID" : 9,
      "context" : "As previously mentioned DQN (Mnih et al., 2015) and DDPG (Lillicrap et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : ", 2015) and DDPG (Lillicrap et al., 2015) provide great starting points for learning in discrete and continuous action spaces.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "MacAlpine et al. (2015) employed the layered-learning framework to incrementally learn a series of interdependent behaviors.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "Additionally, Levine et al. (2015) demonstrates the ability of deep learning paired with guided policy search to learn manipulation policies on a physical robot.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "Challenges in multi-agent learning in the RoboCup domain have been examined by prior work (Kalyanakrishnan et al., 2007) and solutions may translate into the deep reinforcement learning settings as well.",
      "startOffset" : 90,
      "endOffset" : 120
    }, {
      "referenceID" : 11,
      "context" : "Recent developments in model-based deep reinforcement learning (Oh et al., 2015) show that detailed next state models are possible.",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "To make this possible, we extended the DDPG algorithm (Lillicrap et al., 2015), a recent development in deep reinforcement learning.",
      "startOffset" : 54,
      "endOffset" : 78
    } ],
    "year" : 2017,
    "abstractText" : "Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables. The best learned agent can score goals more reliably than the 2012 RoboCup champion agent. As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.",
    "creator" : "LaTeX with hyperref package"
  }
}