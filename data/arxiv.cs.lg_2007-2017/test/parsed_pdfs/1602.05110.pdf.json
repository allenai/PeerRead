{
  "name" : "1602.05110.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Generating images with recurrent adversarial networks",
    "authors" : [ "Daniel Jiwoong Im", "Chris Dongjoo Kim", "Hui Jiang", "Roland Memisevic" ],
    "emails" : [ "IMDANIEL@IRO.UMONTREAL.CA", "KIMDON20@GMAIL.COM", "HJ@CSE.YORKU.CA", "MEMISEVR@IRO.UMONTREAL.CA" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Generating realistically-looking images has been a longstanding goal in machine learning. The early motivation for generating images was mainly as a diagnostic tool, based on the belief that a good generative model can count as evidence for the degree of “understanding” that a model has of the visual world (see, example, (Hinton et al., 2006), (Hyvrinen et al., 2009), or (Ranzato et al., 2013) and references in these). More recently, due to immense quality improvements over the last two years (for example, (Gregor et al., 2015; Denton et al., 2015; Radford et al., 2015; Gatys et al., 2015)), and the successes of discriminative modeling overall, image generation has become a goal on its own, with industrial applications within close reach.\nThe currently most common image generation models can be roughly categorized into two classes: The first are based on probabilistic generative models, such as the variational autoencoder (Kingma & Welling, 2014) and a variety of equivalent models introduced at the same time. The idea in\nthese models is to train an autoencoder whose latent representation satisfies certain distributional properties, which make it easy to sample from the hidden variables, as well as from the data distribution (by plugging samples into the decoder).\nThe second are models based on adversarial sampling (Goodfellow et al., 2014). This approach forgoes the need to encourage a particular latent distribution, and in fact an encoder altogether, by training a simple feed-forward neural network to generate “data-like” examples. “Datalikeness” is judged by a simultaneously trained, but otherwise separate, discriminator neural network.\nFor both types of approach, sequential variants were introduced recently, which were shown to work much better in terms of visual quality: The DRAW network (Gregor et al., 2015), for example, is a sequential version of the variational autoencoder, where images are generated by accumulating updates into a canvas using a recurrent network. This also allows the use of an attention mechanism which was shown to be crucial to obtain good samples. As the sequential variant of an adversarial network, the LAPGAN model (Denton et al., 2015) generates images in coarseto-fine fashion, by generating and upsampling in multiple steps.\nMotivated by the successes of sequential generation, in this paper, we propose a new image generation model based on a recurrent network. Like (Denton et al., 2015) our model generates an image in a sequence of structurally identical steps, but in contrast to that work we do not impose a coarse-to-fine (or any other) structure on the generation procedure. Instead we let the recurrent network learn the optimal procedure by itself. In contrast to (Gregor et al., 2015), we optain very good samples without resorting to an attention mechanism and without variational training criteria (such as a KL-penalty on the hiddens).\nOur is mainly inspired by a third type of image generation method proposed recently by (Gatys et al., 2015). In this work, the goal is to change the texture (or “style”) of a given reference image by generating a new image that\nar X\niv :1\n60 2.\n05 11\n0v 1\n[ cs\n.L G\n] 1\n6 Fe\nb 20\n16\nmatches image features and texture features within the layers of a pretrained convolutional network. As shown by (Gatys et al., 2015), ignoring the style- cost in this approach and only matching image features, it is possible to render images which are similar to the reference image. As we shall show, unrolling the gradient descent based optimization that generates the target image yields a recurrent computation, in which an “encoder” convolutional network extracts images of a current “canvas”. The resulting code and the code for the refence image get fed to a “decoder” which decides on an update to the “canvas”.\nThis view, along with the successes of trained sequential generation networks, suggests that an “iteratively stepped” convolutional network trained to accumulate updates onto a visual canvas, should be good at generating images in general, not just those shown as reference images. We show in this paper that this is indeed the case.\nTo evaluate and compare the relative performance of adversarial generative models quantitatively, we also introduce a new evaluation scheme based on a “cross-over” battle between the discriminators and generators of the two models."
    }, {
      "heading" : "2. Background",
      "text" : "Generative Adversarial Networks (GAN) are built upon the concept of game theory, where two models play a noncooperative game (Nash, 1951). The game is between a generative and a discriminative model, G and D, respectively, such that the generative model generates samples that are hard for the discriminator D to distinguish from real data, and the discriminator tries to avoid getting fooled by the generative model G.\nFormally, the discriminative model is a classifier D : RM → {0, 1} that tries to differentiate whether a given point x ∈ RM came from the data or not. The generative model G : RK → RM generates samples x ∈ RM that are indistinguishable from the data by mapping a sample z ∈ RK drawn randomly from some prior distribution p(z) to the data space. These models can be trained by playing a minmax game as follows:\nmin θG max θD V (D,G) = min G max D\n[ Ex∼pD [ logD(x) ] + Ez∼pG [ log ( 1−D(G(z)) )]] . (1)\nwhere θG and θD are the parameters of discriminator and generator, rspectively.\nIn practice, the second term in Equation 1 is troublesome due to the saturation of log ( 1 − D(G(z)) ) . This makes insufficient gradient flow through the generative model G as the magnitude of gradients get smaller and prevent them from learning. To remedy the vanising gradient problem,\nthe objective function in Equation 1 is reformulated into two separate objectives:\nmin θG max θD\nEx∼pD [ logD(x) ]\n+ max θG\nEz∼pG [ logD ( G(z) )] . (2)\nAlthough Equation 2 is not the same as Equation 1, the underlying intuition is the same.\nThe generating and discriminating procedure are simple. Let us consider a prior distribution to be a Gaussian distribution with zero-mean and unit variance. Then, the process of generating an output is simply to pass a sample z ∼ N (µ = 0,σ = 1) to the generative model to obtain the sample x ∼ G(z; θG). Note that the generative model G can be a deterministic or a probabilistic model. However, only deterministic models have been deployed in the past, which makes x = G(z; θG). Subsequently, the sample can be passed on to the discriminator to predict D(x; θD).\nAfter computing the cost in Equation 2, the model parameters can be updated through backpropagation. Due to the two different min-max operators in Equation 2, the update rule is set as follows:\n{θ′D, θ′G} ←  Update θD if D(x) predict wrong Update θD if D(G(z)) predict wrong Update θG if D(G(z)) predict correct\nIdeally, we would like our generative model to learn a distribution such that pG = pD. This requires the generative model to be capable of transforming a simple prior distribution p(z) to more complex distributions. In general, deep neural networks are good candidates as they are capable of modeling complicated functions and they were shown to be effective in previous works (Goodfellow et al., 2014; Mirza & Osindero, 2014; Gauthier, 2014; Denton et al., 2015). However, note that this min-max continuous game does not neccessarily have a Nash equilibrium solution because there are uncountably many possible neural networks.\nRecently, (Radford et al., 2015) showed excellent samples of realistic images using fully convolutional neural network as the discriminative model and fully decovolutional neural network (Zeiler et al., 2011) as the generative model. Then, the lth convolutional layer in the discriminative network takes the form\nhk (l)\nj = f ∑ j∈Mk hl−1j ∗W k(l) + bk (l) j  , (3) and the lth convolutional transpose layer1 in the generative\n1It is more proper to say “convolutional transpose operation” rather than “deconvolutional” operation. Hence, we will be using the term “convolutional transpose” from now.\nnetwork takes the form\ngc (l)\nj = f ∑ j∈Mc gl−1j ? W c(l) + bc (l) j  . (4) In these equations, ∗ is the convolution operator, ? is the convolutional transpose operator, Mj is the selection of inputs from the previous layer (“input maps”), f is an activation function, and {W k(l) , bk(l)j } and {W c (l) , bc (l)\nj } are the parameters of the discriminator and generator at layer l. The detailed explanation of convolutional transpose is explained in the supplementary materials.\nThe quality of samples can depend on several tricks (Radford et al., 2015), including: 1.Removing fully connected hidden layers and replacing pooling layers with strided convolutions on the discriminator (Springenberg et al., 2014) and fractional-strided convolutions (upsampling) on the generator. 2. Using batchnormalization (Ioffe & Szegedy, 2015) on both generative and discriminative models. 3. Using ReLU activations in every layer of the generative model except the last layer, and using LeakyReLU activations (Maas et al., 2013) in all layers of the discriminative model. Overall, these architectural tricks make it easier to generate smooth and realistic samples. In the following section, we rely on some of these tricks and incorporate them in our proposed model as well."
    }, {
      "heading" : "3. Model",
      "text" : "Here, we propose sequential modeling using GANs on images. Before introducing our proposed methods, we discuss some of the motivations for our approach. One interesting aspect of models such as the Deep Recurrent Attentive Writer (DRAW) (Gregor et al., 2015) and the Laplacian Generative Adversarial Networks (LAPGAN) (Denton et al., 2015) are that they generate image samples in a sequential process, rather than generating them in one\nshot. Both were shown to outperform their ancestor models, which are the variational auto-encoder (Kingma & Welling, 2014) and GAN, respectively. The obvious advantage of such sequential models is that repeatly generating outputs conditioned on previous states simplifies the problem of modeling complicated data distributions by mapping them to a sequence of simpler problems.\nThere is a close relationship between sequential generation and Backpropgating to the Input (BI). BI is a well-known technique where the goal is to obtain a neural network input that minimizes a given objective function derived from the network. For example, (Gatys et al., 2015) recently introduced a model for stylistic rendering by optimizing the input image to simultaneously match higher-layers features of a reference content image and a non-linear, texturesensitive function of the same features of a reference style image. They also showed that in the absence of the stylecost this optimization yields a rendering of the content image (in a quality that depends on the chosen feature layer).\nInterestingly, rendering by feature matching in this way is itself closely related to DRAW (Gregor et al., 2015): optimizing matching cost with respect to the input pixels with backprop amounts to first extracting the current image features fx at the chosen layer using a forward path through the network (up to that layer). Computing the gradient of the feature reconstruction error then amounts to back-propogating the difference fx − fI back to the pixels. This is equivalent to traversing a “decoder” network, defined as the linearized, inverse network that computes the backward pass. The negative of this derivative is then added into the current version, x, of the generated image. We can thus think of the image x as a buffer or “canvas” onto which updates are accumulated sequentially (see the left of Figure 1). Like in the DRAW model, the updates are computed using a (forward) pass through an encoder network, followed by a (backward) pass through a decoder\nnetwork. This approach is almost identical to the DRAW network, except for three subtle differences (see, (Gregor et al., 2015)): (i) in DRAW, the difference between the current image and the image to be rendered is used in the forward pass, whereas here this difference is computed in the feature space (after encoding); (ii) in DRAW the decoder is nonlinear; (iii) DRAW uses a learned, attention-based decoder and encoder rather than (fixed) convolutional network. (see the right of Figure 1) We thoroughly elaborate the relationship between two methods in the supplementary materials.\nIn this work, we explore a generative recurrent adversarial network as an intermediate between DRAW and gradientbased optimization based on a generative adversarial objective function"
    }, {
      "heading" : "3.1. Generative Recurrent Adversarial Networks",
      "text" : "The underlying structure of a Generative Recurrent Adversarial Networks (GRAN) is similar to other GANs. The main difference between GRAN versus other generative adversarial models is that the generator G consists of a recurrent feedback loop that takes a sequence of noise samples drawn from the prior distribution z ∼ p(z) and draws the ouput at different time steps C1, C2, · · · , CT . Figure 3 delineates the high-level abstraction of GRAN.\nAt each time step t, a sample z from the prior distribution is passed onto a function f(·) with the hidden state hc,t where hc,t represents the current encoded status of the previous drawing Ct−1. Ct is what is drawn to the canvas at time t and it contains the output of the function f(·). More specifically, hc,t is a hidden state that is encoded by func-\ntion g(·) from the previous drawing Ct−1. Henceforth, the function g(·) can be seen as a way to mimic the inverse of the function f(·). Accumulating the updates at each time step yields the final sample drawn to the canvas C.\nUltimately, the function f(·) acts as a decoder that receives the input from the previous hidden state hc,t and noise sample z, and the function g(·) acts as an encoder that provides a hidden representation of the output Ct−1 for time step t. One interesting aspect of GRAN is that the procedure of GRAN starts with a decoder instead of an encoder, whereas most of auto-encoding type of models such as VAE or DRAW start by encoding an image as shown in\nFigure 3.\nIn the following, we describe the details of the GRAN procedure more precisely. We have an initial hidden state hc,0 where hc,0 is set to the zero vector at the beginning and z ∼ p(Z). Then, we compute the following for each time step from 1 · · ·T :\nhz,t = tanh(Wz + b) (5) hc,t = g(Ct−1) (6) Ct = f([hz,t,hc,t]). (7)\nNote that [hz,t,hc,t] denotes the concatenation of two vector hz,t and hc,t. At the end, we sum all the drawings and then apply logistic function to the sum in order to scale the final output to be in between (0, 1) such that\nC = σ( T∑ t=1 Ct). (8)\nThe reason for applying the affine transformation along with tanh(·) function in Equation 5 is to rescale z to (−1, 1) so that it lies in the same (bounded) domain as hc,t; g(·) applies tanh(·) at the end, which makes hc,t to be in the range of (−1, 1).\nIn general, the function f(·) and g(·) can be any type of model. In our experiments, we used a variant of DCGAN (Radford et al., 2015). Figure 2 demonstrates the architecture of GRAN at a time step t. The function f(·) starts with one fully connected layer at the bottom and decovolutional layers with fractional-stride convolution at rest of the upper layers. This makes the images grow gradually as the layer gets higher. Conversely, the function g(·) starts from convolutional layers and the full connected layer at the top. The architecture between two functions, f(·) and g(·) is symmetric as shown in Figure 3."
    }, {
      "heading" : "4. Model Evaluation: Battle between GANs",
      "text" : "A problem with generative adversarial models is that there is not a clear way to evaluate them quantitatively. In the past, (Goodfellow et al., 2014) evaluated GANs by looking at the single nearest-neighbour data from the generated samples. LAPGAN was evaluated in the same way, as well as using human inspections (Denton et al., 2015). For human inspections, volunteers were asked to judge whether given images are drawn from the dataset or generated by LAPGAN. In that case, the discriminator can be viewed as a human, while the generator is a trained GAN. The problems with this approach are that human inspectors may have high variance, which makes it necessary to average over a large number of human inspectors, and the experimental setup is both expensive and cumbersome. A third evaluation scheme, used recently by (Radford et al., 2015)\nis classification performance. However, this approach is rather indirect and relies heavily on the choice of classifier. For example, in mentioned work they used the nearest neighbour classifier, which suffers from the problem that Euclidean distance is not a good dissimilarity measure for images.\nHere, we propose an alternative way to evaluate generative adversarial models. Our approach is to directly compare two generative adversarial models by having them engage in a “battle” against each other. The naive intuition is that because every generative adversarial models consists of a discriminator and a generator in pairs, we can exchange the pairs and have them play the generative adversarial game againts each other. Figure 5 illustrate this approach2.\nThe training and test stage are as follows. Consider two generative adversarial models, M1 and M2. Each model consists of a generator and a discriminator,\nM1 = {(G1, D1)} and M2 = {(G2, D2)}. (9)\nDuring the training stage, both models are being trained to prepare them for the battle with one another. Thus, in the training phase, G1 competes with D1 in order to be equipped for the battle in the test phase. Likewise for G2 and D2. In the test phase, model M1 plays against model M2 by having G1 try to fool D2 and vice-versa.\nAccordingly, we end up with the combinations shown in Table 1. Each entry in the table contains two scores, one from discriminating training or test data points and the other from discriminating generated samples. At test time, we can look at the following ratios between the discriminative scores of the two models:\nrtest def = ( D1(xtest) ) ( D2(xtest)\n) and (10) rsamples def = ( D1(G1(z)) ) ( D2(G2(z))\n) , (11) where (·) outputs the classification error rate. These ratios allow us to compare the model performance.\nThe test ratio, rtest, tells us which model generalizes better since it is based on discriminating the test data. Note that when the discriminator is overfit to the training data,\n2Larger figure shown in the supplementary materials.\nD1 G1\nTraining Phase M1\nD2 G2 M2\nFigure 4. Training Phase of Generative Adversarial Networks.\nD1 G2\nTest Phase (a.k.a Battle Phase) M1 versus M2\nD2 G1\nFigure 5. Training Phase and Test Phase of Generative Adversarial Networks.\nTable 2. Model Evaluation on various data sets.\nthe generator will also be affected by this. This would increase the chance of producing biased samples towards the training data, for example.\nThe sample ratio, rsample, tells us which model can fool the other model more easily, since the discriminators are classifying over the samples generated by their opponents. Strictly speaking, as our goal is to generate good samples, the sample ratio determines which model is better at generating good (“data like”) samples. We suggest using the sample ratio to determine the winning model, and to use the test ratio to determine the validity of the outcome as outlined below.\nThe reason for using the latter is that we cannot decide which model is better solely based on the sample ratio. Consider as a counter example the case where the discriminator of M1 only outputs false and the generator of M1 is trained against the discriminator of M1. On the other hand, M2 is a model that is trained based on generative adversarial objective in Equation 2. Then, the error rate for D1 on samples generated by M2 will be zero. So, M1 wins since the error rate of M1 is lower than error rate of M2. However, M1 should lose to M2 since M2 is obviously not a good model. This problem arises because we have not\naccounted for the test ratio. To remedy this, our proposed evaluation metric qualifies the sample ratio using the test ratio by defining the winning model as follows:\nwinner =  M1 if rsample < 1 and rtest ' 1 M2 if rsample > 1 and rtest ' 1 Tie otherwise (12)\nWe call this evaluation Generative Adversarial Metric (GAM). GAM is not only able to compare generative adversarial models againts each other, but also to partially compare other models, such as the VAE by observing the sample ratio rsample as a evaluation criterion."
    }, {
      "heading" : "5. Experiments",
      "text" : "In order to evaluate whether the extension of sequential generation enhances the perfomance, we assessed both quantitatively and qualitatively under three different image datasets. We conducted several empirical studies on GRAN under the model selection metrics discussed in Section 4. Additionally, we observed generated samples and analyzed them to evaluate the GRAN qualitatively.\nThe MNIST dataset contains 60,000 images for training and 10,000 images for testing and each of the images is 28×28 pixels for handwritten digits from 0 to 9 (LeCun et al., 1998). Out of the 60,000 training examples, we used 10,000 examples as validation set to tune the hyperparameters of our model. The CIFAR10 dataset consists of 60000 32×32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The LSUN dataset consists of high resolution natural scene images with 10 different classes (Yu et al., 2015).\nWe considered training on outdoor church images, which contains 126,227 training, 300 validaiton, and 1000 test images. These images were downsampled to 64×64 pixels.\nAll datasets were normalized such that each pixel value ranges in between [0, 1]. For all of our results, we optimized all our models with ADAM (Kingma & Ba, 2014). The batch size was set to 100, and the learning rate was selected from a discrete range chosen based on the validation set. Importantly, we used different learning rates for the discriminative network and generative network. Throughout the experiments, we found that having different learning rates is useful to obtain succesfully trained generative adversarial models. As our proposed model is a sequential generator, we must select the number of steps, T , to run the model for generation. We compared models with numbers of steps in {1, 3, 5}. Note that GRAN is equivalent to DCGAN when T = 1 up to one extra fully connected layer. We denote this as GRAN1.\nThroughout the experiments, we used a similar architecture for the generative and discriminative network as shown in Figure 2 and Figure 3. The convolutional layers of the networks for the discriminator and generator are shared. The number of convolutional layers and the number of hidden units at each layer were varied based on the dataset. Table 4 shows the number of convolution kernels and the size of the filters at each convolutional layer. The numbers in the array from left to right corresponds to the bottom to the top layer of the convolutional neural network. One can tie the weights of convolutional and convolutional transpose in the encoder and decoder of GRAN to have the same number of\nparameters for both DCGAN and GRAN.\nIn the following, we analyze the results by answering a set of questions on our experiments."
    }, {
      "heading" : "Q: How does GRAN perform?",
      "text" : "The performance of GRAN is presented in Table 2. We focused on comparing GRANs with 1, 3 and 5 time steps, which were denoted as GRAN1, GRAN3 and GRAN5. For all three datasets, GRAN3 and GRAN5 outperformed GRAN1 as shown in Table 2."
    }, {
      "heading" : "Q: How do GRAN and other GAN type of models perform compared to non generative adversarial models?",
      "text" : "Although this may not be the best way to assess the two models since generator of GRAN does get assessed explicitly, we tested comparing our model to other generative models such as denoising VAE (DVAE) (Im et al., 2015) and DRAW on the MNIST dataset. Table 3 presents the results of applying GAM. The error rates were all below 50%, and especially low for DVAE’s samples. Surprisingly, even though samples from DRAW look very nice, the error rate on their samples were also quite low with GRAN3. This il-\nlustrate that discriminator of generative adversarial models are good at discriminating the samples generated by DVAE and DRAW. Our hypothesis is that the samples look nicer due to the smoothing effect of having a mean-square error in their objective, but they do not capture all relevant aspects of the statics of real handwritten images."
    }, {
      "heading" : "Q: How do GRAN’s samples look?",
      "text" : "We present samples from GRAN for MNIST, cifar10 and LSUN in Figure 6, Figure 7 and Figure 8. Most of the MNIST and cifar10 samples shown in Figure 6 and Figure 7 appear to be discernible and reasonably classifiable by humans. Similarly, the LSUN samples from Figure 8 seem to cover the variety of church buildings and contain fine detailed textures. The “image statics” of two real image datasets are embedded into both types of sample.\nSince it is infeasible to look across the training data and determine whether a given sample looks like a training case, it is common (albeit somewhat questionable) to look at knearest neighbours to do basic sanity check. As shown in Figure 9, Figure 12, and Figure 11, one does not find any\nreplicates of training data cases\nEmpirically speaking, we did notice that GRAN tends to generate samples by interpolating between the training data. For example, Figure 11 in the supplementary material illustrates that the church buildings consist of similar structure of the entrance but the overall structure of the church is in a different shape. Based on such examples, we hypothesize that the overfitting for GRAN in the worst case may imply that the model learns to interpolate sensibly between training examples. This is not the typical way of the term overfitting is used for generative models, which usually refers to memorizing the data. In fact, in adversarial training in general, the objective function is not based on mean square error on the pixels which makes it not obvious how to memorize the training samples. However, this could means that they will have a hard time generativing images that are interpolated from test data."
    }, {
      "heading" : "Q: How do the samples look like during the intermediate time steps t?",
      "text" : "Figure 10, Figure 29, and Figure 35 present the intermediate samples when the total number of steps is 3. From the figures, we can observe the gradual development of the samples over time. The common observation from the intermediate samples is that images become more finegrained and introduce details missing from the previous time step image. Intermediate samples for models with a total number of time steps of 5 can be found in the supplementary materials. This behaviour is somewhat similar to (Denton et al., 2015), as one might expect (although fillingin of color details suggest that there is more going on than just a coarse-to-fine behaviour). In general, this behaviour is not enforced in our case since we use an identical architecture at every time step.\nQ: What happens when we use different noise for each step?\nNote that we sampled a noise vector z ∼ p(Z) and used the same noise for every time step. This is because z acts as a reference frame in Artify as shown in Figure 1. On the other hand, DRAW injects different noise at each step, z1, z2, · · · zT ,∼ p(Z) due to variational auto-encoding properties. We also experimented with both sampling z once in the beginning versus sampling zi at each time step. Here we describe the advantages and disadvantages to these two approaches.\nFor example, the samples of cifar10 and lsun generated by injecting different noises at each time step are shown in Figure 15 and Figure 16 Similar to when injecting same noises as shown in Figure 7 and Figure 8, the samples appear to be discernible and reasonably classifiable by humans as well. However, we observe few samples that look very close to each other. During the experiments, we found that when using different noises, it is more time consuming to find a set of hyper-parameters that produce good samples. Furthermore,the samples tend to collapse when training for a long time. Hence, we had to carefully select the total number of iterations. This illustrates that the training became much more difficult and it provokes GRAN to cheat by putting a lot of probability mass on samples that the discriminator cannot classify, which produce samples that looks very similar to each other.\nOn the other hand, when we look at the intermmediate time steps of samples generated using multiple noises, we find that there are much more dynamics within each time step as demontrated in Figure 17 and Figure 18. For examples, the colour of the train suddenly changed in Figure 17 and only the partial church is drawn in Figure 18. This illustrates that adding different noise increases the model capability on generating much dynamical images.\nFigure 15. Cifar10 samples generated by GRAN with injecting different noises at every time step\nFigure 16. Samples of LSUN images generated by GRAN with injecting different noises at every time step"
    }, {
      "heading" : "6. Discussion",
      "text" : "We proposed a new generative model based on adversarial training of a recurrent neural network inspired by (Gatys et al., 2015). We showed conditions under which the model performs well and showed that it can produce higher quality visual samples than an equivalent single-step model. We also introduced a new metric for comparing adversarial networks quantitatively and showed that the recurrent generative model yields superior performance over existing stateof-the-art generative models under this metric."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank the members of the LISA Lab at Montreal, in particular Mohammed Pezeshki and Donghyun Lee, for helpful discussions"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional network with respect reference image features is a way to render images of high visual quality. We show that unrolling this gradient-based optimization yields a recurrent computation that creates images by incrementally adding onto a visual “canvas”. We propose a recurrent generative model inspired by this view, and show that it can be trained using adversarial training to generate very good image samples. We also propose a way to quantitatively compare adversarial networks by having the generators and discriminators of these networks compete against each other.",
    "creator" : "LaTeX with hyperref package"
  }
}