{
  "name" : "1303.5743.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Handling Uncertainty during Plan Recognition in Task-Oriented Consultation Systems",
    "authors" : [ "Bhavani Raskutti", "Ingrid Zukerman" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 INTRODUCTION\nDuring task-oriented consultations, a consultant needs to infer a user's requirements from his/her statements in order to provide assistance. To this effect, the con sultant needs to interpret a user's statements correctly. However, this task is hindered by the fact that people\noften provide partial and/or inaccurate information. This requires the consultant to fill in the missing in formation by using different information sources, such as knowledge of discourse coherence, domain knowl edge and knowledge about the user. However, these information sources are not fully reliable, requiring the consultant to draw inferences which are inherently un certain. As a result, the statements issued by the user may be interpreted in more than one way. Hence, the consultant needs to evaluate the possible interpreta tions and select the most probable one. In this pa per, we present a mechanism for handling the uncer tainty arising from the lack of reliability of the various information sources used by the consultant, and for discriminating between multiple interpretations of a user's statements.\nAn interpretation of a user's statements consists of a sequence of plans that the user proposes to carry out, and a plan consists of an action with a number of parameters defining the action. For instance, in the travel domain, the proposal to fly from Melbourne to Sydney on December 1st, 1990, is a plan, where flying is the action, and the parameters origin, destination and departure date are instantiated.\nA number of researchers have used plan recognition as a means to response generation during consultation (Grosz 1977, Allen and Perrault 1980, Sidner and Is rael 1981, Carberry 1983, Litman and Allen 1987, Pol lack 1990). However, the models of plan recognition developed by these researchers cope only with a single interpretation of a user's actions or utterances. Car berry (1990) addresses the problem of multiple inter pretations by using default inferences, and by applying Dempster-Shafer theory of evidence (Section 9.1, Pearl 1988) to compute plausibility factors of alternate hy potheses. However, in domains such as travel, where the default assumptions are weak, this approach alone does not cope with the problem of multiple interpreta tions. Kautz and Allen (1986) use circumscription to generate all possible interpretations during story un derstanding. However, since all possibilities are con structed, and the search is limited only by the struc-\nture of the plan hierarchy, this approach can be very expensive.\nOur mechanism applies Bayesian theory of probability in order to address the problem of multiple interpreta tions which result from uncertain information sources. Probability theory in the form of Bayes Belief Net works (Pearl 1988) was applied by Goldman and Char niak (1989) to model the difference between the effect of objects on plan recognition in stories and in 'real life'. In an earlier research, Goldman and Charniak (1988) combined probability theory with Assumption based TMS (de Kleer 1986) for plan inference during story understanding. Our mechanism expands on this earlier research with respect to plan inference during task-oriented consultations. We have observed that in these consultations, the user generally provides suf ficient information to enable the consultant to help him/her achieve his/her goals. Hence, the better de fined the plans in an interpretation, the more likely it is that this is the interpretation intended by the user. Based on this observation, we augment probabilistic reasoning by means of information content consider ations, in order to narrow down the number of inter pretations produced by the system from a user's state ments. The information content of an interpretation is a measure of how well defined an interpretation is in terms of the actions to be performed on the basis of the interpretation. It depends on the specificity and the strength of the inferences in it, where the strength of an inference depends on the reliability of the infor mation source on which it is based.\nThe use of information content considerations is valid only in intended plan recognition, such as the one oc curring in cooperative interactions. In keyhole recog nition of plans, where the observer infers an agent's plans and goals by unobtrusively observing the agent (Schmidt, Sridharan and Goodson 1978), we cannot assume the same strong desire for communication and hence cannot use the information c\n\"antent of an inter pretation to assess its probability.\nThe subsequent sections describe the inference mech anism with particular reference to the means used for handling the uncertainties arising during the inference process. The actual procedures which draw the infer ences are discussed in [Raskutti and Zukerman 1991). We make use of the following dialogue excerpt to illus trate our approach:\nTraveler: \"I want to go to Sydney the day after to morrow. I am going to Hawaii on the 11:00 am flight. By the way, I'll be leaving from Adelaide.\""
    }, {
      "heading" : "2 THE INFERENCE MECHANISM",
      "text" : "The inference mechanism operates on input provided by a Natural Language Interface (NLI), which con sists of predicates, such as FLY and LEAVE, and meta\npredicates that indicate the modality of the state ments, such as CAN and MUST. Based on this input, it generates the intended plans of the user. The infer ence mechanism consists of three processes: ( 1) Direct inference, (2) Indirect Inference, and (3) Evaluation of interpretations. In these processes, the uncertainties arising due to partial and possibly unreliable informa tion are handled by using Bayesian theory of probabil ity combined with information content considerations.\nThe direct inference process generates a set of possible interpretations from the input provided by the NLI, using definitions of domain actions and coherence con siderations. During this stage, there are uncertainties arising due to the many possible interpretations of a statement as well as due to the many possible rela tions between the interpretation of a new statement and the previous discourse. For instance, in the above dialogue excerpt, there is uncertainty as to which trip the departure location 'Adelaide' refers to. This un certainty is handled by computing the probability of an interpretation of a piece of discourse in terms of the probability of an interpretation of each new statement (Section 3.1), the probability of an interpretation of the previous discourse, and the probability of a rela tion between the two (Section 3.2). The calculated probabilities are used to prune the set of interpreta tions (Section 3.3).\nThe interpretations generated by the direct inference process are usually incomplete. The unspecified de tails are filled in by indirect inferences based on other information sources, such as domain knowledge and world knowledge. All these sources are not equally re liable and hence the strength of these inferences are not the same. For instance, the desired mode of trans port between Sydney and Hawaii may be inferred by taking into consideration typical assumptions about the domain. This type of inference is stronger than an inference based on general 'world knowledge', but weaker than a direct inference. During the indirect in ference process, the strength of an inference is used to prevent a weaker inference from refuting the results of a previous stronger inference (Section 4).\nA measure of information content is used both after the direct inference process and during the indirect inference process to determine whether the inference process should be continued. This measure is also used after both the direct and the indirect inference process to increase the probability of the interpretations with a high information content and to decrease the proba bility of those with a low information content (Section 5). The updated probabilities are used to prune the set of interpretations by dropping the interpretations with a low probability (Section 3.3). Thus, the eval uation process prefers those interpretations that are well-defined, i.e., those that were completely specified by the user or those in which the gaps left by the user could be filled in by using the system's knowledge.\n3 THE PROBABILITY OF AN INTERPRETATION OF THE\nDISCOURSE\nThe probability of an interpretation of a set of state ments is a measure of how likely it is that the speaker intended this interpretation when s/he uttered the statements in question. Assume that our discourse s' is composed of the previous discourse S and a new statement s, where s stands for the input returned by the NLI and consists of a predicate and a possibly nil meta predicate. Further, assume that S has a set of possible interpretations { Ii}, and s has a set of pos sible interpretations { ik}. In addition, assume that {Rm} is the set of possible relations between an inter pretation Ij of S and an interpretation ik of s. Each relation Rm is one of the possible discourse relations between a statement and previous discourse, namely, Elaboration, Introduction and Correction. In addi tion, the relations Elaboration and Correction must also refer to a topic or plan that is being elaborated or corrected. Finally, let D be the set of apriori domain knowledge of the listener including the plan-base and the rule-base that are used to generate the set of in terpretations. Thus, the probability that the speaker meant an interpretation Ijkm, consisting of Ij and ik and the relation Rm between Ij and ik, when sfhe said s' is P(IjkmiS', D). However, since we are deal ing with a closed system where the domain knowl edge remains constant throughout the interaction, we can omit D from our calculations, and focus only on P(IjkmiS'). This probability is calculated as follows:\nP(IikmiS') = P(Ij,ik, RmiS,s)\nAccording to Bayes Rule for conditional probability:\nP(I· IS')= P(Ij,ik,Rm,S,s)\nJkm P(S,s)\nUsing Bayes Rule for conditional probability, the nu merator can be rewritten as follows:\nP(Ij,ik,Rm,S,s) = P(Sis,it,Ij,Rm) * P(siik,Ij,Rm) * P(Rmlik,Ii) * P{Ijiik) * P(ik)\n• P(Sis,ik,Ij,Rm) = P(SIIj), since S is con ditionally independent of s, ik and Rm, given Ij; P(SIIi) indicates the probability that a user would utter the statements in discourse S when s/he wanted to mean Ij.\n• P(slik, Ij, Rm) = P(slik), since s is conditionally independent of Ij and Rm, given ik; P(siik) indi cates the probability that a user would utter the statement s when s/he wanted to mean ik.\n• P(Iilik) = P(Ij), since Ij is the interpretation of statements before s and is independent of ik.\nUsing the above results, the expression for P(IjkmiS') may be rewritten as follows:\nP(IikmiS') =\nP(SIIi) P(slik) P(Rm lik, Ii) P(Ii) P( ik) P(S,s)\nThe application of Bayes Rule to the first two terms of the numerator yields:\nThe terms P(s), P(S) and P(S, s) represent how often the statement s, the discourse s and the discourse s' consisting of S and s are ever uttered, regardless of the purpose for which they are uttered. These probabili ties, in general, can be very difficult to estimate. How ever, since they are independent of the probabilities of the interpretations, and since we are interested in com parisons of probabilities and not in absolute values, we can define a normalizing constant a, a = P( S) x P(') P(S,s) Hence,\nP(Ii IS) is the probability of the interpretation Ij af ter processing the discourse S. It is the result of it eratively applying the process described here with re spect to the statements in discourse S. P( ik is) is the probability of an interpretation ik of a user's state ments and it is computed as described in Section 3.1 . P(Rm lik, Ii) links up the new statement's interpreta tion to the interpretation of the earlier discourse by means of relation Rm. It is determined as described in Section 3.2. The probabilities of the new interpre tations, P(IjkmiS'), for all the combinations of {Ij }, {ik} and {Rm}, are used to prune the set of interpre tations.\n3.1 PROBABILITY OF A N INTERPRETATION OF A STATEMENT\nThe input processed by the inference mechanism is a parsed version of the original statements issued by the user, and consists of a predicate and a possibly nil meta predicate for each statement. During the direct inference process, interpretations of an input predicate are determined by using an operator library (Fikes and Nilsson 1971) and plan inference rules (Allen and Per rault 1980). The operator library defines the basic actions in the domain. Each operator definition in the library consists of the preconditions, effects and body of an action, where the body defines the composition\nof the action. The plan inference rules state that all those operators that have the input predicate in their precondition, effect or body must be chosen as possible interpretations of the predicate.\nThe probability of an interpretation ik is determined as follows: all the interpretations generated by the same rule, e.g., a body rule, are assigned the same probabil ity; and as more possibilities are generated, each indi vidual one is considered less probable. However, the total probability mass allocated to all the interpreta tions resulting from the application of a rule depends on two factors: ( 1) the meta predicate returned by the NLI, and (2) whether other rules gave rise to the inference of any interpretations.\nThe presence of a meta predicate, such as WANT, en ables us to resolve ambiguity when the input predicate appears in both the precondition and the effect of an operator. For instance, if a user's request about BEing at a place is expressed as \"I want to be ... \" or \"I can be ... ,\" but the NLI returns it as BE ( ... ), this predicate can refer either to the precondition or the effect of an operator with equal probability. By taking into consideration the presence of a meta predicate, we in crease the bias towards the appropriate inference rule, e.g., if the user had said \"I can be ... ,\" then the prob ability mass of the precondition rule would have been increased. This scheme is implemented by determin ing in advance the manner in which each of the meta predicates that may be returned by the NLI affects the probability mass assigned to each rule.\nA rule which does not match an input predicate fails to infer any interpretations. If one or more rules fail to infer interpretations, the probability mass allocated to the rules that were successful is modified by allocating the apriori mass of the unsuccessful rule proportion ally between the successful ones. For instance, if only the effect and precondition rules yield interpretations, then the probability mass allocated io the body rule is 0. This results in the a priori allocation of the body rule being split up proportionally between the effect and precondition rules.\nThese considerations for determining the probability of an interpretation are domain independent, since the prior probabilities of all the interpretations of a pred icate are considered equal, and they are modified us ing only meta predicates. At this juncture, if domain knowledge is available, it can be used to modify the apriori probabilities of the interpretations of a predi cate.\n3.2 PROBABILITY OF A RELATION\nThe possible discourse relations considered by the in ference mechanism are as follows: Elaboration, Cor rection, Digression and Introduction. Digression is a special case of Elaboration, where the probability of elaborating on the last topic is considerably reduced.\nHence, the relation Rm can only be one of the other three. In addition, in the case of Elaboration and Cor rection, Rm also includes the topic that is being re ferred to. The possible relations and the number of topics that can be referred to lead to a large number of elements in the set { Rm}. This set is constrained by assigning probabilities to the inferred relations so that normal patterns of discourse are preferred. The fol lowing considerations are used while determining the probability of an inferred relation:\n1. When a new statement can be interpreted as elab orating on two or more topics discussed earlier, the elaboration of the last referenced topic is pre ferred to the elaboration of earlier topics. Further, the probability of an earlier topic being referred to falls exponentially as its distance from the current statement increases.\n2. A new statement can always be interpreted as in troducing a new topic of discussion. However, if the new statement can also be interpreted as elaborating on an earlier topic, the probability of introduction is considerably reduced. This is achieved by assigning a constant and low prob ability to the introduction relation. If a highly probable elaboration relation is possible, then the relative magnitude of the probability of introduc tion is reduced.\n3. If there are cue words that indicate a particular re lation or that point to a particular previous topic, the probability of this relation or this topic in creases. For instance, in the statement \"By the way, I'll be leaving from Adelaide,\" from the dia logue excerpt presented in Section 1, the italicized cue phrase indicates a digression. Hence, the pre ferred interpretation is that 'Adelaide' refers to the trip that was mentioned earlier in the dis course, namely the Sydney trip.\n4. Correction and digression are never inferred un less the NLI provides evidence to this effect by means of cue phrases, such as \"on second thought\" and \"by the way,\" respectively.\nThe above considerations are domain independent, since the probability of a specific relation Rm is de termined by the cue words returned by the NLI and by the contents of the interpretations ik and Ij. Do main knowledge of distances is not taken into consid eration while calculating the probability of the relation Rm. However, the probability of going to Hawaii on the way between Sydney and Melbourne is definitely lower than the probability of going to Sydney on the way between Melbourne and Hawaii. If domain knowl edge is available, it should be incorporated into the system to update the probabilities obtained using the domain independent considerations discussed above.\n3.3 PRUNING THE SET OF INTERPRETATIONS USING THEIR PROBABILITIES\nAfter determining the probabilities of a set of interpre tations, the probabilities are normalized. The normal ized probabilities are used to prune the set of inter pretations by dropping all those interpretations whose probabilities fall below a relative rejection threshold. In principle, the improbable interpretations could be maintained and revisited, if necessary. But, empiri cally, this has not been necessary since the intended interpretation has been found among the retained in terpretations. All the interpretations I that are re tained have a probability that satisfies the following condition:\n-..!.!___ > Threshold Pmax -\nwhere Pmax is the maximum of all the normalized probabilities, PI is the probability of a retained inter pretation, and Threshold is a number in [0,1) range. This calculation ensures that interpretations with a low probability relative to the most probable interpre tation are dropped. For example, with 0.5 as the value for Threshold, an interpretation with probability 0.3 is dropped if there is another interpretation with prob ability 0.7. At the same time, if there is a situation where there are three interpretations with probability 0.4, 0.3 and 0.3, then all three are retained.\nBy judicious choice of a value for Threshold, the sys tem can be tailored to consider more or less possibil ities. For instance, currently we have two thresholds. The first one used during the direct inference process is 0.5 and was chosen in line with the probabilities assigned to the discourse relations, so that plausible interpretations are not discarded. The second thresh old, which is used to prune the interpretations after the indirect inference process, is 0.7, and was chosen so that fewer possibilities are considered as additional information is brought to bear.\n4 STRENGTH OF INFERENCES\nWe postulate that the strength of an inference is di rectly proportional to the reliability of the informa tion source that is used as the basis for this inference. Hence, we categorize different information sources that are used to draw inferences and list them below in de creasing order of reliability.\n1. User's Statements- Direct inferences from what is explicitly stated. While these inferences can be presumed correct, there is still a degree of uncer tainty in relating a new statement to the earlier ones due to the different discourse relations pos sible.\n2. Domain Knowledge- Indirect inferences that are derived by using the system's beliefs about the\nuser's domain knowledge. A typical example is the inference of the arrival time at the destina tion once the departure time is known. Such an inference is useful when there are multiple legs in a proposed journey, requiring that departure times at subsequent locations be inferred.\n3. Domain Assumptions - Indirect inferences that are derived by assuming what is normal in the domain. For example, when no details about the mode of travel are specified, it is possible to derive this information from the usual mode of transport between two places.\n4. User Model- Indirect inferences that arc made on the basis of the system's model of the user. The user model may be a default model describ ing a typical user, or it may be more specific. In the context of a travel agency, we have adopted a default model based on the assumption that typi cally, in a travel agency, the information provider cannot form an extensive user model.\n5. Common-Sense- Indirect inferences that are de rived by assuming normal behavior or common notions outside the domain of interest. Typically, such notions are used when we postulate return journeys based on the assumption that people usually do not move from their residence.\nThe inference types are assigned a strength in the (0,1] range, and this strength is used to calculate the infor mation content of a parameter and also to determine whether a particular parameter should be revised by a new inference during the indirect inference process. The inferences derived from the user's statements have a strength of 1 and all other inference types have a progressively decreasing strength according to the re liability of their source of information. Undefined pa rameters are assigned a minimum strength. This as signment enables us to distinguish between parameters that are defined inexactly by the user and parame ters that are left undefined, and assign less information content to undefined parameters.\nIn the process of deriving indirect inferences, we em ulate one aspect of human behavior whereby once a conclusion is accepted with a particular degree of con fidence, people consider it to be certain when drawing subsequent conclusions (Gettys, Kelly and Peterson 1982). Thus, like Carberry (1990), we do not com pound the uncertainty in chains of inferences. This approach is different from that used for the compu tation of confidence factors (CFs) in MYCIN, where the CF of a parameter P is computed by taking into account the CFs of the parameters P1, P2, ... , Pn that are used for calculating P (Shortliffe and Buchanan 1975). Our approach is implemented by tagging each parameter, P, in the plans in each interpretation with the type of inference that gave rise to the value of P, without taking into consideration the inference types of P1, P2, ... , Pn. Since the type of inference indicates\nHandling Uncertainty During Plan Recognition in Thsk-Oriented Consultation Systems 313\nthe strength of the inference rule, it represents the con ditional probability of P, given P1, Pz, ... , Pn.\nDuring the indirect inference process, the strength of the inference in a parameter's tag is compared with the strength of a new inference, before updating the pa rameter with the new inference. If the strength of the new inference is the same or higher than the strength of the inference type in the tag, the new inference re places the old one. Otherwise, the old inference is re tained. In this manner, a weaker inference is prevented from refuting the results obtained from a stronger in ference.\nThus, during the indirect inference process, the gaps left in the interpretations generated by the direct infer ence process are filled in using the most reliable indi rect inference, and unreliable inferences are not main tained. In principle, it is necessary to generate a new interpretation for each new value of a parameter, and process all the generated interpretations. However, this can lead to an exponential explosion of possibili ties, and our decision to consider only the strongest inference during the indirect inference process is a trade-off between the benefits of exploring all possibil ities and the resource limitations for doing so (Horvitz 1989)."
    }, {
      "heading" : "5 INFORMATION CONTENT AND ITS USE",
      "text" : "The information content of an interpretation is a mea sure of the extent of its definition. After both direct and indirect inferences, this measure is used both to determine if the processing is to be continued as well as to modify the probabilities of the interpretations so that the probabilities of interpretations with a higher information content are increased. The modified prob abilities are used to prune the set of interpretations as described in Section 3.3.\n5.1 THE INFORMATION CONTENT OF AN INTERPRETATION\nWe define the information content of an interpretation as the sum of the information content of all the plans in the interpretation, and the information content of a plan as the sum of the information content of all the parameters that are necessary for the definition of the plan. The information content of a parameter, in turn, depends on two factors: (1) its specificity, which is de fined as the reciprocal of the number of possible values assigned to this parameter, and (2) its strength, which depends on the source of information from which this parameter was obtained. That is, both a parameter with multiple values assigned to it and a parameter derived from an unreliable source of information are deemed to have a low information content.\nWe borrow from Information Theory (Shannon 1948), to define the information content of a parameter p, IC(p), as follows:\nS(p) IC(p) = log2 N(p)\nwhere N(p) is the number of possible values assigned to p, and S(p) is the strength associated with p. The strength of the parameter is the strength of the in ference type that was used to derive the value of the parameter (Section 4.2).\nAccording to this formula, undefined parameters have the least information content, since they can take on all the possible values in the domain, and parameters inferred exactly from a reliable source, such as a direct inference from a user's statement, have a ma.ximum in formation content. For instance, if we have directly in ferred that the departure date for a trip is between the 9th and the 15th of May, 1991, then the information content of this parameter is log2 1/7. This measure is additive over multiple plans and it ranges over the negative values, with a maximum information content of 0 when all the parameters which are necessary for the definition of a plan are exactly defined. Thus, the information content of an interpretation I, IC(I), is:\nI C(I) = '\\' '\\' I S(p;) D D og z N( ) {plans Pj in I} {parameters p; in Pj} Pi\n5.2 CHECKING COMPLETION\nAfter the direct inference process and during the in direct inference process, the information content mea sure is used to determine if the processing should be continued. Processing is stopped if at least one com plete interpretation, i.e., an interpretation with zero information content is determined. Processing of an interpretation is also stopped if no new inferences can be drawn on the basis of the existing evidence, i.e., the information content of an interpretation cannot be increased further. Thus, the information content mea sure is used as an informative stopping rule (Berger and Berry 1988) to determine if the processing should be stopped.\n5.3 UPDATING PROBABILITIES\nThe information content measure, which ranges over the negative values, is mapped to the [0,1] range and then used to update the probability of an interpreta tion, P(I). This is performed by means of the follow ing formula: P(I) <- P(I) ( 1 - IC(I) ) ICNORM ICNORM is currently defined to be the minimum possible information content in the domain. In our\n314 Raskutti and Zukerman\nrestricted domain, where the number of possible des tinations and origins is low, this choice of 1CNORM has a large impact on the probability. However, in a realistic domain, where there is a greater degree of freedom in terms of the values that can be assigned to the parameters, another definition of 1CNORM may be preferred. Currently, we are experimenting with 1CNORM as the sum of the information content of all the interpretations, and use this definition to up date the probabilities when there are multiple inter pretations. The update of probabilities using informa tion content is valid only in cooperative information seeking interactions, such as those occurring at a travel agency, where the user wants his/her intentions to be understood by the listener and hence, interpretations with higher information content are more probable.\n6 EXAMPLES\nOur system has been implemented to understand dis courses in travel domain. The language used for the implementation is Franz Lisp. Our system has a rule base containing twelve rules and a plan-base contain ing seven plan operators. The input to the system is in the form of predicates, and the system produces out put in the form of possible interpretations consisting of one or more plans that the user proposes to carry out. To illustrate the inference process, we consider two plausible dialogue excerpts at a Melbourne travel agency.\nEXAMPLE 1\nTraveler: \"I want to go to Sydney the day after to morrow. I am going to Hawaii on the 11:00 am flight. By the way, I'll be leaving from Adelaide.\"\nThis chunk of statements issued by a traveler is re turned by the NLI as the followiJ?g four predicates, where the last two predicates are due to the third sen tence:\n(1) Go (departure._date = day after tomorrow, destination = Sydney) (2) FLY (departure._ time = 11:00 am, destination = Hawaii) (3) DIGRESS (4) LEAVE (origin= Adelaide)\nThe first two domain predicates give rise to an inter pretation composed of two plans: (a) to go to Sydney and (b) to fly to Hawaii at 11:00 am. The DIGRESS discourse relation in predicate (3) indicates that a plan which precedes the plan currently in focus is likely to be the topic of discussion for the forthcoming predi cate. Thus, with the fourth predicate, we have two possible interpretations: h -it elaborates plan (a), or h -it elaborates plan (b). h has a higher prob ability due to the presence of the DIGRESS discourse relation.\nThe information content measure rates both these in terpretations equally. Hence, both interpretations are retained after the direct inference process. During the indirect inference process, we consider the case where the temporal order of plans is assumed to be the same as the order of presentation during the discourse. Other possible temporal orders are considered and pro cessed in [Raskutti and Zukerman 1991]. The use of indirect inference rules coupled with the assumption for temporal order of plans gives rise to two scenarios:\nh Adelaide -+ Sydney -> Hawaii\nh Melbourne ->Sydney, Adelaide ->Hawaii\nh has less information content, since its parameters cannot be inferred due to the need to postulate an ad ditional intervening plan to take the user from Sydney to Adelaide. Hence its probability is correspondingly decreased and 11 is chosen as the best interpretation.\nEXAMPLE 2\nTraveler: \"I want to go to Sydney the day after to morrow. From Sydney I'll be going to Hawaii on the 11:00 am flight. I'll be leaving from Adelaide.\"\nThis chunk of statements issued by a traveler is re turned by the NLI as the following three predicates.\n(1) Go (departure._date = day after tomorrow, destination = Sydney) (2) FLY (departure._time = 11:00 am, origin = Sydney destination = Hawaii) (3) LEAVE (origin= Adelaide)\nThe first two domain predicates give rise to an inter pretation composed of two plans: (a) to go to Sydney and (b) to fly to Hawaii from Sydney at 11:00 am. With the third predicate, we have two possible inter pretations: h -it elaborates plan (a), or h - it introduces a new plan (c) to go from Adelaide. The probability of 11 is higher since elaboration is preferred to introduction. However, since the elaboration is that of a plan discussed before, 12 is retained as a possibil ity. Thus after the direct inference process, we have two possibilities:\nh Adelaide -+Sydney, Sydney -> Hawaii\nh ?x-> Sydney, Sydney -> Hawaii, Adelaide -> ?y\nh has higher information content since the origins and destinations of the two proposed trips are known. This coupled with the previous lower probability of 12 en sures that h is the only possibility carried over to the indirect inference process. Thus, during the indirect inference process, !1 is completed to yield the same interpretation as the one in the first example.\n7 CONCLUSIONS\nIn this paper, we have described a means for handling uncertainty during plan recognition in task-oriented consultation systems by using Bayesian probability theory augmented by an information content measure. We have used our system on five simple discourse sam ples similar to the one discussed in Section 6. In each case, the system chooses the same interpretation that people choose, indicating that our tenet of link ing specificity and strength of inference to the proba bility of an interpretation can be valuable in handling real conversations in cooperative interactions. Finally, by modifying the definition of the information content measure to suit different domains, our method may be used for interpreting user's statements in general, as well as in the area of multi-media document retrieval.\nAcknowledgments\nThis research was supported in part by grant Y90/03/22 from the Australian Telecommunications and Electronics Research Board. We thank Prof. J. Roach and D. Sanford from the Virginia Polytechnic Institute and State University for allowing us to use their transcripts of telephone conversations at travel agencies. We also thank Wilson Wen from Telecom Research Laboratories for his advice on probability theory.\nReferences\nAllen, J.F. & Perrault, C.R. (1980), Analyzing In tention in Utterances. In Artificial Intelligence 15, pp. 143-178.\nBerger, J .0. & Berry, D.A. (1988), The Relevance of Stopping Rules in Statistical Inferences. In Gupta, S.S. and Berger, J .0. (Eds.), Statistical Decision Theory and Related T�pics IV, Vol. 1, Springer Verlag.\nCarberry, S. (1983), Tracking User Goals in an Information-seeking Environment. In Proceedings of the National Conference on Artificial Intelli gence, Washington DC, pp. 59-63.\nCarberry, S. (1990), Incorporating Default Infer ences into Plan Recognition. In Proceedings of the National Conference on Artificial Intelligence, Boston, pp. 471-478.\nde Kleer, J. (1986), An Assumption-based TMS. In Artificial Intelligence 28, pp. 163-224.\nFikes, R.E. & Nilsson, N.J. (1971), STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving. In Artificial Intelligence 2, pp. 189-208.\nGettys, C.F., Kelly III, C. & Peterson, C.R. (1982), The Best-guess Hypothesis in Multistage Infer ence. In Kahneman, D., Slovic, P., and Tversky,\nA. (Eds.) , Judgment under Uncertainty: Heuris tics and Biases, Cambridge University Press, pp. 370-377.\nGoldman, R. & Charniak, E. (1989), Plan Recogni tion in Stories and in Life. In Proceedings of the IJCAI89 Workshop on Uncertainty in Artificial Intelligence, Windsor, Canada, pp. 54-59.\nGoldman, R. & Charniak, E. (1988), A Probabilistic ATMS for Plan Recognition. In Proceedings of the AAAI-88 Workshop on Plan Recognition, St. Paul, Minnesota, August 1988.\nGrosz, B.J. (1977), The Representation and Use of Focus in Dialogue Understanding. Tech. Note 151, SRI International, Menlo Park, CA.\nHorvitz, E.J. (1989), Beliefs and Actions Under Com putational Resource Constraints. In Kana!, L.N ., Levitt, T.S. and Lemmer, J .F. (Eds.), Uncertainty in Artificial Intelligence 3, North Holland.\nKautz, H. & Allen, J .(1986), Generalized Plan Recog nition. Proceedings of the Fifth National Con ference on Artificial Intelligence, Philadelphia, Pennsylvania.\nLitman, D. & Allen, J.F. (1987), A Plan Recogni tion Model for Subdialogues in Conversation. In Cognitive Science 11, pp. 163-200.\nPearl, J. (1988), Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Mor gan Kaufmann Publishers, San Mateo, California.\nPollack, M. (1990), Plans as Complex Mental Atti tudes. In Cohen, P. , Morgan, J. and Pollack, M. (Eds.) , Intentions in Communication, MIT Press.\nRaskutti, B. & Zukerman, I. (1991), Generation and Selection of Likely Interpretations during Plan Recognition in Task-oriented Consultation Sys tems. In User Modeling and User Adapted In teraction, an International Journal.\nSchmidt, C.F ., Sridhar an, N .S. & Goodson, J .1. (1978), The Plan Recognition Problem: An Inter section of Artificial Intelligence and Psychology. In Artificial Intelligence 10, pp. 45-83.\nShannon, C.E. (1948), A Mathematical Theory of Communications. In Bell System Technical Jour nals, October 1948.\nShortliffe, E.H. & Buchanan, B.G. (1975), A Model of Inexact Reasoning in Medicine, Mathematical Biosciences 23, pp. 351-379.\nSidner, C.L. & Israel, D.J. (1981), Recognizing Intended Meaning and Speakers' Plans. In Proceedings of the Seventh International Joint Conference on Artificial Intelligence, Vancouver, Canada, pp. 203-208."
    } ],
    "references" : [ {
      "title" : "Analyzing In­ tention in Utterances",
      "author" : [ "J.F. Allen", "C.R. Perrault" ],
      "venue" : "In Artificial Intelligence",
      "citeRegEx" : "Allen and Perrault,? \\Q1980\\E",
      "shortCiteRegEx" : "Allen and Perrault",
      "year" : 1980
    }, {
      "title" : "The Relevance of Stopping Rules in Statistical Inferences",
      "author" : [ "J Berger" ],
      "venue" : "J .0. (Eds.), Statistical Decision Theory and Related T�pics IV,",
      "citeRegEx" : "Berger,? \\Q1988\\E",
      "shortCiteRegEx" : "Berger",
      "year" : 1988
    }, {
      "title" : "Tracking User Goals in an Information-seeking Environment",
      "author" : [ "S. Carberry" ],
      "venue" : "In Proceedings of the National Conference on Artificial Intelli­ gence, Washington DC,",
      "citeRegEx" : "Carberry,? \\Q1983\\E",
      "shortCiteRegEx" : "Carberry",
      "year" : 1983
    }, {
      "title" : "Incorporating Default Infer­ ences into Plan Recognition",
      "author" : [ "S. Carberry" ],
      "venue" : "In Proceedings of the National Conference on Artificial Intelligence,",
      "citeRegEx" : "Carberry,? \\Q1990\\E",
      "shortCiteRegEx" : "Carberry",
      "year" : 1990
    }, {
      "title" : "An Assumption-based TMS",
      "author" : [ "J. de Kleer" ],
      "venue" : "In Artificial Intelligence",
      "citeRegEx" : "Kleer,? \\Q1986\\E",
      "shortCiteRegEx" : "Kleer",
      "year" : 1986
    }, {
      "title" : "STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving",
      "author" : [ "R.E. Fikes", "N.J. Nilsson" ],
      "venue" : "In Artificial Intelligence",
      "citeRegEx" : "Fikes and Nilsson,? \\Q1971\\E",
      "shortCiteRegEx" : "Fikes and Nilsson",
      "year" : 1971
    }, {
      "title" : "The Best-guess Hypothesis in Multistage Infer­ ence",
      "author" : [ "C.F. Gettys", "C. Kelly III", "C.R. Peterson" ],
      "venue" : null,
      "citeRegEx" : "Gettys et al\\.,? \\Q1982\\E",
      "shortCiteRegEx" : "Gettys et al\\.",
      "year" : 1982
    }, {
      "title" : "Plan Recogni­ tion in Stories and in Life",
      "author" : [ "R. Goldman", "E. Charniak" ],
      "venue" : "In Proceedings of the IJCAI89 Workshop on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Goldman and Charniak,? \\Q1989\\E",
      "shortCiteRegEx" : "Goldman and Charniak",
      "year" : 1989
    }, {
      "title" : "A Probabilistic ATMS for Plan Recognition",
      "author" : [ "R. Goldman", "E. Charniak" ],
      "venue" : "In Proceedings of the AAAI-88 Workshop on Plan Recognition,",
      "citeRegEx" : "Goldman and Charniak,? \\Q1988\\E",
      "shortCiteRegEx" : "Goldman and Charniak",
      "year" : 1988
    }, {
      "title" : "The Representation and Use of Focus in Dialogue Understanding",
      "author" : [ "B.J. Grosz" ],
      "venue" : "Tech. Note 151, SRI International,",
      "citeRegEx" : "Grosz,? \\Q1977\\E",
      "shortCiteRegEx" : "Grosz",
      "year" : 1977
    }, {
      "title" : "Beliefs and Actions Under Com­ putational Resource Constraints",
      "author" : [ "E.J. Horvitz" ],
      "venue" : "J .F. (Eds.), Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Horvitz,? \\Q1989\\E",
      "shortCiteRegEx" : "Horvitz",
      "year" : 1989
    }, {
      "title" : "Generalized Plan Recog­ nition",
      "author" : [ "H. Kautz", "J Allen" ],
      "venue" : "Proceedings of the Fifth National Con­ ference on Artificial Intelligence,",
      "citeRegEx" : "Kautz and Allen,? \\Q1986\\E",
      "shortCiteRegEx" : "Kautz and Allen",
      "year" : 1986
    }, {
      "title" : "A Plan Recogni­ tion Model for Subdialogues in Conversation",
      "author" : [ "D. Litman", "J.F. Allen" ],
      "venue" : "In Cognitive Science",
      "citeRegEx" : "Litman and Allen,? \\Q1987\\E",
      "shortCiteRegEx" : "Litman and Allen",
      "year" : 1987
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Mor­ gan",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "Plans as Complex Mental Atti­ tudes",
      "author" : [ "M. Pollack" ],
      "venue" : "Intentions in Communication,",
      "citeRegEx" : "Pollack,? \\Q1990\\E",
      "shortCiteRegEx" : "Pollack",
      "year" : 1990
    }, {
      "title" : "Generation and Selection of Likely Interpretations during Plan Recognition in Task-oriented Consultation Sys­ tems",
      "author" : [ "B. Raskutti", "I. Zukerman" ],
      "venue" : "In User Modeling and User Adapted In­ teraction,",
      "citeRegEx" : "Raskutti and Zukerman,? \\Q1991\\E",
      "shortCiteRegEx" : "Raskutti and Zukerman",
      "year" : 1991
    }, {
      "title" : "The Plan Recognition Problem: An Inter­ section of Artificial Intelligence and Psychology",
      "author" : [ "an", "N .S", "J Goodson" ],
      "venue" : "In Artificial Intelligence",
      "citeRegEx" : "an et al\\.,? \\Q1978\\E",
      "shortCiteRegEx" : "an et al\\.",
      "year" : 1978
    }, {
      "title" : "A Mathematical Theory of Communications",
      "author" : [ "C.E. Shannon" ],
      "venue" : "In Bell System Technical Jour­ nals,",
      "citeRegEx" : "Shannon,? \\Q1948\\E",
      "shortCiteRegEx" : "Shannon",
      "year" : 1948
    }, {
      "title" : "A Model of Inexact Reasoning in Medicine",
      "author" : [ "E.H. Shortliffe", "B.G. Buchanan" ],
      "venue" : "Mathematical Biosciences",
      "citeRegEx" : "Shortliffe and Buchanan,? \\Q1975\\E",
      "shortCiteRegEx" : "Shortliffe and Buchanan",
      "year" : 1975
    }, {
      "title" : "Recognizing Intended Meaning and Speakers' Plans",
      "author" : [ "C.L. Sidner" ],
      "venue" : "In Proceedings of the Seventh International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Sidner,? \\Q1981\\E",
      "shortCiteRegEx" : "Sidner",
      "year" : 1981
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "(Grosz 1977, Allen and Perrault 1980, Sidner and Is­ rael 1981, Carberry 1983, Litman and Allen 1987, Pol­ lack 1990). However, the models of plan recognition developed by these researchers cope only with a single interpretation of a user's actions or utterances. Car­ berry (1990) addresses the problem of multiple inter­ pretations by using default inferences, and by applying Dempster-Shafer theory of evidence (Section 9.",
      "startOffset" : 13,
      "endOffset" : 282
    }, {
      "referenceID" : 0,
      "context" : "(Grosz 1977, Allen and Perrault 1980, Sidner and Is­ rael 1981, Carberry 1983, Litman and Allen 1987, Pol­ lack 1990). However, the models of plan recognition developed by these researchers cope only with a single interpretation of a user's actions or utterances. Car­ berry (1990) addresses the problem of multiple inter­ pretations by using default inferences, and by applying Dempster-Shafer theory of evidence (Section 9.1, Pearl 1988) to compute plausibility factors of alternate hy­ potheses. However, in domains such as travel, where the default assumptions are weak, this approach alone does not cope with the problem of multiple interpreta­ tions. Kautz and Allen (1986) use circumscription to generate all possible interpretations during story un­ derstanding.",
      "startOffset" : 13,
      "endOffset" : 680
    }, {
      "referenceID" : 13,
      "context" : "works (Pearl 1988) was applied by Goldman and Char­ niak (1989) to model the difference between the effect of objects on plan recognition in stories and in 'real life'.",
      "startOffset" : 6,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "works (Pearl 1988) was applied by Goldman and Char­ niak (1989) to model the difference between the effect of objects on plan recognition in stories and in 'real life'.",
      "startOffset" : 7,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "In an earlier research, Goldman and Charniak (1988) combined probability theory with Assumption­ based TMS (de Kleer 1986) for plan inference during story understanding.",
      "startOffset" : 24,
      "endOffset" : 52
    }, {
      "referenceID" : 15,
      "context" : "The actual procedures which draw the infer­ ences are discussed in [Raskutti and Zukerman 1991).",
      "startOffset" : 67,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "During the direct inference process, interpretations of an input predicate are determined by using an operator library (Fikes and Nilsson 1971) and plan inference rules (Allen and Per­ rault 1980).",
      "startOffset" : 119,
      "endOffset" : 143
    }, {
      "referenceID" : 18,
      "context" : ", Pn that are used for calculating P (Shortliffe and Buchanan 1975).",
      "startOffset" : 37,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "Thus, like Carberry (1990), we do not com­ pound the uncertainty in chains of inferences.",
      "startOffset" : 11,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "However, this can lead to an exponential explosion of possibili­ ties, and our decision to consider only the strongest inference during the indirect inference process is a trade-off between the benefits of exploring all possibil­ ities and the resource limitations for doing so (Horvitz 1989).",
      "startOffset" : 278,
      "endOffset" : 292
    }, {
      "referenceID" : 17,
      "context" : "We borrow from Information Theory (Shannon 1948), to define the information content of a parameter p, IC(p), as follows:",
      "startOffset" : 34,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : "Other possible temporal orders are considered and pro­ cessed in [Raskutti and Zukerman 1991].",
      "startOffset" : 65,
      "endOffset" : 93
    } ],
    "year" : 2011,
    "abstractText" : "During interactions with human consultants, people are used to providing partial and/ or inaccurate information, and still be under­ stood and assisted. We attempt to em­ ulate this capability of human consultants in computer consultation systems. In this paper, we present a mechanism for han­ dling uncertainty in plan recognition dur­ ing task-oriented consultations. The uncer­ tainty arises while choosing an appropriate interpretation of a user's statements among many possible interpretations. Our mecha­ nism handles this uncertainty by using proba­ bility theory to assess the probabilities of the interpretations, and complements this assess­ ment by taking into account the information content of the interpretations. The informa­ tion content of an interpretation is a mea­ sure of how well defined an interpretation is in terms of the actions to be performed on the basis of the interpretation. This mea­ sure is used to guide the inference process to­ wards interpretations with a higher informa­ tion content. The information content of an interpretation depends on the specificity and the strength ofthe inferences in it, where the strength of an inference depends on the reli­ ability of the information on which the infer­ ence is based. Our mechanism has been de­ veloped for use in task-oriented consultation systems. The domain that we have chosen for exploration is that of a travel agency.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}