{
  "name" : "1610.08222.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A self-tuning Firefly algorithm to tune the parameters of Ant Colony System (ACSFA)",
    "authors" : [ "M. K. A. Ariyaratne", "S. Weerakoon", "John Kotelawala" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The collective behavior of natural insects - ants, bees, fireflies and termites mimic the problem solving capabilities of the swarms [10, 13, 4]. These capabilities were adopted in various heuristics and meta-heuristics to solve difficult optimization problems. Each meta-heuristic has a real world inspiration of optimization. As such, the main inspiration for the ant colony system algorithm is the natural food finding strategy of ants. Ants are capable of finding the shortest path from the food source to their nests. A chemical inside them call pheromone is the reason for this optimized behavior. Following this real world strategy, the ant colony system algorithm was developed by Marco Dorigo et al.[6] to suit for path optimization problems. Initially the algorithm was developed to solve the TSP. This original implementation supports the hypothesis that the ant colony algorithm is successful in finding the shortest path for the TSP.\nSimilar to any meta-heuristic, the ant colony system also has algorithm specific parameters. The initial implementation used the trial and error method to find the best collection of parameters. The values of the parameters depend on the problem at hand and hence at each instance, the most suitable parameter set for the problem should be evaluated. The\nar X\niv :1\n61 0.\n08 22\n2v 1\n[ cs\n.A I]\n2 6\nO ct\n2 01\ntask of parameter tuning again is an optimization where the optimal parameter set will give the optimum performance.\nThough the initial study relied on trial and error, Dorigo had stated some important features of parameters such as pheromone behavior, the number of ants and how they affect the performance of the algorithm [6]. The original paper discusses the ranges for different parameters so that an idea about the distribution of each parameter is given. The results were in an encouraging position ascertaining that the algorithm is successful in solving the TSP. Instances from TSPLIB and randomly generated TSPs’ were used to evaluate the algorithm.\nSince the original ACS works well with pre-tested parameter values, finding approaches to set better parameters without trial and error can improve the performance of the algorithm. The best way is to consider setting of parameters as another optimization problem. The same matter of tuning parameters of an ACS is considered by many other researchers. Yet none was successful in maintaining fully parameterless environment. An Adaptive Parameter Control Strategy for Ant Colony Systems by Zhi-Feng Hao et al. is a study carried out to enhance the performance of the ant system solving the TSP [8]. Although it has been mentioned as the ant colony system, they have used the ant system for the study. The particle swarm optimization (PSO) has been used to optimize the parameters of the ant system (PSOACS). The parameters of the ant system were considered as a whole where one particle represents approximation for the set of parameters. The number of particles is the same as the number of ants, and once ants complete a tour the PSO will update the parameters. The conclusions are based on the performance of the new algorithm and the existing ACS. The results support the conclusions, but the following matters were not addressed. Although the study focused on a parameter tuning technique, the effects of the method towards the parameters were not mentioned. Basically the study was focused on improving the TSP results of the existing approach. For both PSOACS and original ACS, the results obtained conflict with the optimal results given in the TSPLIB. The reason may be the differences in the implementations. Also in PSOACS, the equations used are not clear enough to get an idea about the new algorithm.\nIn Evolving Ant Colony Optimization, another research by Hozefa M. Botee and Eric Bonabeauy, they have made the use of genetic algorithm (GA) to evolve the best set of parameters [3]. Here also, one parameter set represents an individual of the genetic algorithm. However the implementation was tested only over two TSP instances. The results were encouraging, but the parameters of the ACS depend on the selection of the parameters of GA such as crossover and mutation probability.\nApart from meta-heuristics, machine learning techniques have also been used to tune parameters of ACS. For example Ayse Hande Erol et al. in their research, have used artificial neural networks (ANN) to find the best parameter set for ACS solving a given TSP [7]. The research focuses on only two parameters, α: pheromone decay parameter and β: the relative importance of the pheromone vs. distance. Initially the ACS-ANN hybrid algorithm runs with different α and β values for 50 times. These parameters work as the inputs to the ANN. Then ANN predicts the best parameter values for a given TSP instance. The hybrid algorithm was tested with several TSP instances from the TSPLIB [21]. The results support the hypothesis that the hybrid algorithm performs well in finding better parameter values. However the study focused on tuning only two parameters. The information about the ANN\nsuch as the training methods, weights and their effects are not mentioned in the research. As such there are other researches which have been conducted to find better parameter sets for the ACS in solving TSP [9, 20]. But as a whole, all these methods rely on another algorithm or technique, where it again contains its own parameters. Therefore the performance of the ACS again depends on the parameters of the used algorithm. To overcome this issue we have designed an algorithm with the help of the firefly algorithm and the selftuning framework for optimization algorithms proposed by Xin-She Yang, to optimize the parameters of the ACS algorithm solving symmetric TSP instances. Xin-She Yang et al., in the implementation of the self-tuning framework have used the firefly algorithm to apply the framework to tune FA’s parameters [19]. In their framework, the problem solved by the optimization algorithm and the parameter set of the algorithm both were considered as a single problem. The framework was initially tested for the firefly algorithm and proved its capability of tuning parameters of itself (FA). In a research done by M.K.A. Ariyaratne et al., use this self tuning framework combined with the firefly algorithm to solve nonlinear equitations [2]. They have solved univariate nonlinear equations having complex roots with the help of a modified firefly algorithm. To find the best parameters values, the self tuning framework was implemented on the firefly algorithm. In their research, a firefly carries an approximation for a root as well as approximations to the parameter values. Finally as the output, they receive best approximations for the roots in a given range as well as best approximations for the parameter values. The research again confirms the powerfulness of the self tuning framework in optimizing parameters.\nThe significance of this research lies in the potential of the developed ant colony optimization algorithm for the TSP with the self tuning framework to optimize the parameters. Despite the recent advancements in the field of route optimization and parameter optimization, the following issues have not been addressed by other researchers (See Table ?? for details of previously applied approaches) where our research has accomplished.\n• None of the existing systems are capable of providing virtually parameter-free environments for the ant colony systems to solve TSPs.\n• In most of the researches, parameter optimization is done using another meta-heuristic algorithm whose parameters should be manually selected.\n• None of the approaches have used a designed framework for parameter optimization.\nGA-Genetic Algorithms, PSO-Particle Swarm Optimization, ACS-Ant Colony Systems, ACO-Ant Colony Optimization\nIn this paper, Yang’s self-tuning framework is studied, and a parameter selection strategy based on the firefly algorithm is developed. To deliver a better idea about the present work, the remainder of this paper is structured as follows. Section 2 provides some preliminaries related to the Ant colony system and the firefly algorithm. In section 3, we briefly describe the self-tuning framework for the firefly algorithm and how we adopt it to tune the parameters of the ACS when solving symmetric TSPs. There, we also present the hybrid ACS-FA. Section 4 points out the TSP examples, the parameters used and the results obtained from the new approach. To emphasize the goodness of the new algorithm, we compare the results with the original ACS and with some other parameter tuning approaches. Finally, we draw conclusions briefly in Section 5."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Traveling Salesman Problem (TSP)",
      "text" : "The Traveling Salesman Problem is one of the most intensively studied problems in computational mathematics which is simple to state but very difficult to solve [1]. The problem is NP hard, making it not computable in polynomial time [15]. The problem is about finding the shortest possible tour through a set of n cities/nodes so that each city/node is visited exactly once. A weighted graph G(N,E) can represent a TSP, where N represents the cities and E represents the set of edges connecting cities. There is a specific distance d for each (i, j) ∈ E. If d(i, j) = d(j, i), it is known as a symmetric TSP where in an asymmetric TSP, d(i, j) 6= d(j, i) can occur. In our study we consider only the symmetric situation."
    }, {
      "heading" : "2.2 Ant Colony Systems (ACS)",
      "text" : "Ants, the popular social insects normally live in colonies represent a highly structured distributed system. There are many ant species, some of which are blind. All ant species are known to be deaf [11]. Despite of these incapabilities, ants are inbred with a strong indirect communication using a chemical produced within them. While foraging, ants lay the chemical; pheromone on the ground and follow the pheromone placed by other ants. The pheromones tend to decay over time and hence the ants in the colony will choose the path with high pheromone density at the moment. This pheromone communication allows the ants to find the shortest path from the food source to their nest. The optimized behaviors of real ants are based on implementing artificial ant colonies.\nThe ant colony systems is an example of an ant colony optimization method from the field of swarm intelligence, meta-heuristics and computational intelligence. Around 1990’s Marko Dorigo introduced the idea of the ant system and later Dorigo and Gambardella introduce the ant colony system. In the original implementation, ACS was applied to solve the TSP [6]. Initially, ants in the artificial colony are positioned on random nodes/ cities. They travel from one node to another keeping the travel history in a data structure. The likelihood of selecting a node is based on the pheromone density of the cities laid by other ants, which in the algorithm known as the state transition rule. Once visiting a city, an ant lays an amount of pheromone using the local pheromone updating. Upon completing the tours by all ants, the cities belong to the globally best path again get updated with pheromones using global pheromone updating rule."
    }, {
      "heading" : "2.2.1 State Transition Rule, Local and Global updating",
      "text" : "State transition rule is responsible for an ant to find its next visiting city. Assume the ant is in the node r. It’s next city s is determined by the equation 1.\ns = { arg maxu∈Jk(r){[τ(r, u)θ].[η(r, u)]β} q ≤ q0 S Otherwise\n(1)\nwhere τ(r, u) is the pheromone density of an edge (r, u), η(r, u) is [1/distance(r, u)] for TSP. Jk(r) is the set of cities that remain to be visited by ant k positioned on city r. The relative\nimportance of the pheromone trail and the heuristic information are represented by the parameters θ and β (θ, β ≥ 0). q is a random number uniformly distributed in [0, 1], q0 is a parameter (0 ≤ q0 ≤ 1), and S is a random variable from the probability distribution given by the equation (2).\nPk(r, s) =  [τ(r, u)]θ.[η(r, u)]β∑ u∈Jk(r)[τ(r, u)]θ.[η(r, u)]β ifs ∈ Jk(r)\n0 Otherwise (2)\nACS local and global updating happens according to the equation (3) and equation (4) respectively.\nτ(r, s)← (1− ρ).τ(r, s) + ρ.∆τ(r, s) (3)\nwhere 0 < ρ < 1 is a parameter.\nτ(r, s)← (1− α).τ(r, s) + α.∆τ(r, s) (4) where\n∆τ(r, s) = {\n(Lgb)−1 if(r, s) ∈ global best tour 0 Otherwise (5)\n0 < α < 1 is the pheromone decay parameter and Lgb is the length of the globally best tour. In the original implementation, Dorigo et al. have given the set of parameter values obtained from the trial and error approach to suit the selected TSP instances.\nThe ACS grasped the attention of the world of optimization and hence many researches have been carried out to improve the algorithm as well as to check its ability over solving other optimization problems."
    }, {
      "heading" : "2.3 Firefly Algorithm (FA)",
      "text" : "Firefly is a winged beetle commonly known as the lightning bug due to the charming light it emits. The light is used to attract mates or preys. Biological studies reveal many factors about fireflies’ life style that are interesting [14]. Focusing on their flashing behavior, the firefly algorithm was developed by Xin-She-Yang in 2009 [18]. The algorithm basically assumes the following.\n• Fireflies’ attraction to each other is gender independent.\n• Attractiveness is proportional to the brightness of the fireflies, for any two fireflies, the less brighter one is attracted by (and thus moves toward) the brighter one; however, the brightness can decrease as the distance increases; If there is no brighter one than a particular firefly, it moves randomly.\n• The brightness of a firefly is determined by the value of the problem specific objective function.\nAs many meta-heuristics, the initial population for the particular problem is generated randomly. In FA also, the parameter set should be specified properly. After these initial steps, the fireflies in the population start moving towards brighter fireflies according to the following equation.\nxi = xi + β(xj − xi) + α(rand− 0.5) (6)\nwhere β = β0.e−γr 2 (7)\nβ0 is the attraction at r = 0. The three terms in equation (6) represent the contribution from the current firefly, attraction between two fireflies and a randomization term respectively. The equation supports both exploitation and exploration. α plays an important role in the randomization process, which is from Uniform or Gaussian distribution. To control the randomness, Yang has used δ, the randomness reduction factor which reduces α according to the equation (8).\nα = α0.δ where δ ∈ [0, 1] (8)\nFA, as a newcomer in the world of meta-heuristics marked its remarkable capability of handling optimization problems. Yang et al. in 2013 introduced a framework for selftuning algorithms and it was implemented with the firefly algorithm successfully [19]. The framework allows a meta-heuristic algorithm to solve a problem while optimizing it’s own algorithm specific parameters."
    }, {
      "heading" : "3 Self-tuning firefly algorithm optimizing Ant colony",
      "text" : "system’s parameters (ACSFA)\nIn this research, the main aim is to tune the parameters of the ant colony system algorithm solving symmetric TSP problems while obtaining the shortest path for the selected TSP. FA as an outstanding performer, has used here in tuning the parameters of the ACS. Another reason is that the self-tuning framework can be easily implemented with firefly algorithm rather than directly on ACS, since the ACS solves a discrete problem (TSP) and the parameters are continuous in nature.\nRegarding the ACS, β, θ, ρ and q0 parameters have to be tuned. The FA also has several parameters such as α, γ, β and the randomness reduction factor δ. The main aim of the self-tuning concept is to find the best parameter settings that minimize the computational cost. When applying the self-tuning framework to the firefly algorithm solving a given optimization problem, both the problem domain and the parameter domain are considered as a single domain in solving the problem. The objective could be the objective of the problem.\nIn our case the FA is used to tune the parameters of ACS and the ACS solves a given TSP. The objective of the algorithms is to find the optimal solution of a given TSP instance. The problem of this study contains both parameters of ACS and FA. The pseudo code of the proposed ACSFA algorithm is presented in algorithm 1.\nAfter initializing parameters and parameter ranges, inverse of a tour distance is set as the objective of the TSP. The problem is set as a minimization problem. The algorithm will build tours until a predetermined end condition is completed. As in the original ACS, ants are positioned on random starting nodes where each ant completes a tour using the state transition rule stated in equation 1. While completing the tour ants will lay pheromones on the visited cities according to the equation 3. Upon completing tours by all ants, a globally best tour will be identified and the cities belong to the globally best tour will be awarded with extra pheromone values according to the equation 4. Apart from building tours, each ant also carries approximations of the parameters of both ACS and FA.\nAfter completing a tour, all ants will work as fireflies. They now represent approximations of the parameters of FA and ACS. The parameters of ACS and FA will get updated using the self tuning firefly algorithm. The fireflies will move in the direction of better parameters/fireflies according to the equation 6. At the end of each tour, the best parameter set will be detected.\nAlgorithm 1 : Pseudo code of the ACSFA 1: Begin; 2: Initialize parameter values and ranges of parameter values to be tuned 3: Assign approximations of the parameters to be tuned to each ant (Firefly). 4: Define the objective function (I − Inverse of the distance) 5: while End condition do 6: Begin Tour 7: Position ants on starting nodes 8: Build the tour while local pheromone update 9: End Tour 10: Do global pheromone update 11: for i = 1 : n (all n ants) do 12: for j = 2 : n (n ants) do 13: if Ij < Ii then 14: Move firefly i towards firefly j by using equation (6); 15: end if 16: Attractiveness varies with distance r via e−γr2 using equation (7); 17: Evaluate new solutions and update parameter values; 18: end for 19: end for 20: Rank the fireflies and find the current best (best parameter values); 21: end while 22: Post process results and visualization; 23: End"
    }, {
      "heading" : "4 Experimentation",
      "text" : "The goal of this experiment is to analyze the performance of the ACSFA algorithm solving symmetric TSPs’ while selecting the most suitable parameter values for ACS and FA. We aim to formulate the new algorithm with minimum number of user input parameters where all other parameters of the two algorithms are to be tuned while optimizing the TSP instances."
    }, {
      "heading" : "4.1 Parameter values and problem instances",
      "text" : "The new algorithm is implemented to solve symmetric Traveling Salesman problems. 12 symmetric TSPs are selected for testing [21]. The parameters of ACS include: β, α, ρ, q0,m, τ0 and τ . τ0 is based on the nearest neighbor heuristic. Pheromone density τ depends on α and ρ parameters. Since α works only with the globally best ant, we initialize α to be 0.1. Here, we consider β, ρ and q0 to be tuned by the self-tuning firefly algorithm. . The range for these parameters are β ∈ [0 8], ρ ∈ [0.5 1] and q0 ∈ [0.5 1] which are large enough to select the best parameter values for many symmetric TSP instances. Apart from that, we tune the parameters of the FA. The parameters of firefly algorithm include α, β, γ, δ and number of fireflies. In equation 7, the brightness β depends on three main factors; β0 which we initiate to be 1, γ; the light absorption coefficient and r; the Cartesian distance between two fireflies which should be calculated during iterations. in equation 8, the value of α get reduced with α0 and δ. α0 is initialized to be 2.3. Therefore the value of α depends on δ. Hence the only factors to be tuned in FA are γ and δ. For convenience, we will use FF_α0, FF_β0, FF_γ and FF_δ to indicate the parameters of FA. These parameters varies in the ranges FF_γ ∈ [0 10] and FF_δ ∈ [0.8 1]. For further clarification a detailed list of the parameters used for the algorithms are presented in Table (2).\nThe new algorithm is implemented using MATLAB [12] and the experiment is conducted on a laptop with an Intel (R) Core (TM) i5-5200U CPU @ 2.20 GHz processor and 8GB memory. Since the speed relies on the programming language, structure and the type of the machine, the comparing algorithms were also implemented using the same environment. The\nnew algorithm is compared with the original ant colony algorithm (ACS) [6] and an adaptive parameter control strategy for ACO implemented using the PSO algorithm (PSOACS) [8]. The TSP instances and their best known solutions are indicated in Table (3)."
    }, {
      "heading" : "5 Results",
      "text" : "To accomplish the experimental comparison, we considered randomly selected 12 TSP instances from the TSPLIB, that have been presented in the Table (3) [21]. Table (4) presents the results. Each algorithm executed 10 times with each TSP instance to get the results. Results are formatted as the best TSP distance, the average, the worst and the average time taken by each algorithm. The obtained results illustrate the strength of the ACSFA over other two algorithms. The interesting factor is that, in ACSFA, results are better and most of the parameters are handled by the self tuning firefly algorithm.\nHowever, to prove the results in a convenient way, a statistical analysis is also conducted over the obtained results."
    }, {
      "heading" : "5.1 Statistical Analysis",
      "text" : "A convenient statistical analysis was conducted to prove the validity of the results.The guidelines provided by Derrac et al. [5] were followed to perform the statistical analysis. However, here we have used parametric methods to conduct the statistical comparison. To test the hypothesis’ related to the study, we have used ANOVA (Analysis of variance) technique [17]. ANOVA is useful when we need to do an experiment to conduct a comparison over more than 2 samples. Therefore, to compare the 3 Algorithms in terms of error, ANOVA with RCBD (Randomized Complete Block Designs) has been applied. The hypothesis tested here is: H0: There is no any significant difference between 3 algorithms H1: At least one algorithm is different from others The results of the performed statistical test is as follows:\nAnalysis for error\nP value of the algorithm field (0.07) is less than 0.1. Based on that, we reject H0 and the conclusion can be made as that there exist at least one algorithm which is significantly different from others at a 0.1 significance level. To find which algorithms are different from which, we have conducted a pairwise comparison over the algorithms. To conduct a pairwise comparison over the error, Tukey’s method was applied [16]. The results were as follows.\nTukey Pairwise Comparisons: Response = error, Term = Algorithm\nGrouping Information Using the Tukey Method and 90% Confidence\nAlgorithm N Mean Grouping ACS 12 1016.75 A PSOACS 12 366.67 A B ACSFA 12 257.58 B\nMeans that do not share a letter are significantly different.\nThe hypothesis used this time is as follows.\nH0: There is no any difference between 2 algorithms H1: There is a difference between 2 algorithms\nThe pairwise comparison on the error concluded that there is no any difference between ACSFA and PSOACS at 90% confidence level and there is a difference between ACSFA and ACS at 90% confidence level.\nThe same procedure was conducted considering the average and the best results of the algorithms for 12 TSP instances. Having P- values as 0.072 and 0.070, Analysis of variance in both cases supported to reject H0 concluding that at least one algorithm is significantly different from others at a 0.1 significance level. Pairwise comparisons were also conducted in both cases. The results are as follows.\nTukey Pairwise Comparisons: Response = Average, Term = Algorithm\nGrouping Information Using the Tukey Method and 90% Confidence\nAlgorithm N Mean Grouping ACS 12 19399.8 A PSOACS 12 18525.5 A B ACSFA 12 18163.5 B\nMeans that do not share a letter are significantly different.\nTukey Pairwise Comparisons: Response = Best, Term = Algorithm\nGrouping Information Using the Tukey Method and 90% Confidence\nAlgorithm N Mean Grouping ACS 12 19137.3 A PSOACS 12 18487.2 A B ACSFA 12 18378.2 B\nMeans that do not share a letter are significantly different.\nBoth states support the conclusion that there is no any difference between ACSFA and PSOACS at 90% confidence level. Considering the best case, ACSFA and ACS appeared to be different at 90% confidence level.\nFinally the analysis support the facts that the performance of ACSFA and PSOACS is equally strong where the performance of ACS is not as strong as ACSFA and PSOACS. But the results considering the best case, demonstrate that ACSFA outperforms other two algorithms. However although there is no significant difference between ACSFA and PSOACS from the statistical viewpoint, there exists a strong advantage of ACSFA over PSOACS: the ability of performing well without considering the selection of suitable parameter values for both ACS and FA."
    }, {
      "heading" : "5.2 Parameter Optimization",
      "text" : "The statistical study emphasizes that there is no significant difference between ACSFA and PSOACS. But still ACSFA is better since it does provide a parameter-free environment to the user. The firefly algorithm with the self-tuning framework tunes the necessary parameters of ACS as well as FA. The figure 4 represents the evolution of parameter values of ACS over the iterations for the eil51 TSP instance. Here the mean value of each parameter for each iteration is calculated.\nIt is necessary to see the behavior of the parameters of the FA as well. Figure 5 shows the evolution of the parameters of FA during iterations for the eil51 TSP instance. Here also, the mean value of each parameter for each iteration is calculated.\nFigures 4 and 5 point out that the parameter values varies up to some number of iterations and then stabilize over an optimum value."
    }, {
      "heading" : "6 Concluding Remarks",
      "text" : "The study has focused on implementing an ant colony algorithm to solve symmetric TSP problems whose parameters are handled by a self tuning firefly algorithm. The algorithm was successfully implemented and tested with standard TSP problems. According to the results obtained, some key conclusions can be drawn. In terms of optimization, the results show that the ACSFA performs well in finding the shortest path for a given TSP instance. The comparisons done with ACS and PSOACS shows ACSFA works well. Although the statistical analysis concludes that both ACSFA and PSOACS have same performance, ACSFA outperforms PSOACS by providing a parameter free environment. The self tuning framework worked fine with the firefly algorithm in tuning both parameters of ACS and FA. The graphical representations of the evolution of parameters of both ACS and FA clearly demonstrates the ability of the self tuning firefly algorithm. With these we can consider the new ACSFA as a better performer to solve TSPs using ACS. For further development, this research encourages us to study the performance of the self tuning framework with other nature inspired algorithms such as particle swarm optimization, bees algorithm etc. Also since the increasing number of cities drops the performance of the algorithm, more experimentation should be done on the population size and the initialization of parameter ranges as well."
    }, {
      "heading" : "Acknowledgment",
      "text" : "The authors would like to thank Dr. Xin-She Yang for his valuable suggestions and explanations on implementing the self tuning framework and Ms. W.J. Polegoda, lecturer at the Faculty of Animal Science & Export Agriculture, Uva Wellassa University, Sri Lanka for the guidance given on the statistical analysis."
    } ],
    "references" : [ {
      "title" : "The Traveling Salesman Problem: A Computational Study (Princeton Series in Applied Mathematics)",
      "author" : [ "David L. Applegate", "Robert E. Bixby", "Vasek Chvatal", "William J. Cook" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "A self-tuning modified firefly algorithm to solve univariate nonlinear equations with complex roots",
      "author" : [ "M.K.A. Ariyaratne", "T.G.I. Fernando", "S. Weerakoon" ],
      "venue" : "IEEE Congress on Evolutionary Computation (CEC),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Evolving ant colony optimization",
      "author" : [ "Hozefa M. Botee", "Eric Bonabeau" ],
      "venue" : "Advances in Complex Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1998
    }, {
      "title" : "Sexual communication by pheromones in a firefly, phosphaenus hemipterus (coleoptera: Lampyridae)",
      "author" : [ "R. De Cock", "E. Matthysen" ],
      "venue" : "Animal Behaviour,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms",
      "author" : [ "JoaquÃŋn Derrac", "Salvador GarcÃŋa", "Daniel Molina", "Francisco Herrera" ],
      "venue" : "Swarm and Evolutionary Computation,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Ant colony system: A cooperative learning approach to the traveling salesman problem",
      "author" : [ "M. Dorigo", "L.M. Gambardella" ],
      "venue" : "Trans. Evol. Comp,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1997
    }, {
      "title" : "Optimizing the ant colony optimization algorithm using neural network for the traveling salesman problem",
      "author" : [ "Ayşe Hande Erol", "Merve Er", "Serol Bulkan" ],
      "venue" : "In Actas de la Conferencia Internacional de,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "An adaptive parameter control strategy for aco",
      "author" : [ "Z. f. Hao", "R. c. Cai", "H. Huang" ],
      "venue" : "In 2006 International Conference on Machine Learning and Cybernetics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2006
    }, {
      "title" : "Fine-tuning the ant colony system algorithm through particle swarm optimization",
      "author" : [ "D Gomez-Cabrero", "DN Ranasinghe" ],
      "venue" : "In Proceedings of the International Conference on Information and Automation,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2005
    }, {
      "title" : "Collective Wisdom of Ants",
      "author" : [ "D.M. Gordon" ],
      "venue" : "Scientific American,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "The Ants",
      "author" : [ "B. Hölldobler", "E.O. Wilson" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1990
    }, {
      "title" : "Swarm orientation in honeybees",
      "author" : [ "Roger A. Morse" ],
      "venue" : "Science, 141(3578):357–358,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1963
    }, {
      "title" : "Correlated evolution of female neoteny and flightlessness with male spermatophore production in fireflies (coleoptera: Lampyridae)",
      "author" : [ "Adam South", "Kathrin Stanger-Hall", "Ming-Luen Jeng", "Sara M. Lewis" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Statistical Principles in Experimental Design",
      "author" : [ "B.J. Winer" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1991
    }, {
      "title" : "Firefly algorithms for multimodal optimization",
      "author" : [ "Xin-She Yang" ],
      "venue" : "In Proceedings of the 5th International Conference on Stochastic Algorithms: Foundations and Applications,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "A framework for self-tuning optimization algorithm",
      "author" : [ "Xin-She Yang", "Suash Deb", "Martin Loomes", "Mehmet Karamanoglu" ],
      "venue" : "Neural Computing and Applications,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Optimizing the ant colony optimization using standard genetic algorithm",
      "author" : [ "Raed Abu Zitar", "Hussein Hiyassat" ],
      "venue" : "In Artificial Intelligence and Applications,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "The collective behavior of natural insects - ants, bees, fireflies and termites mimic the problem solving capabilities of the swarms [10, 13, 4].",
      "startOffset" : 133,
      "endOffset" : 144
    }, {
      "referenceID" : 11,
      "context" : "The collective behavior of natural insects - ants, bees, fireflies and termites mimic the problem solving capabilities of the swarms [10, 13, 4].",
      "startOffset" : 133,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "The collective behavior of natural insects - ants, bees, fireflies and termites mimic the problem solving capabilities of the swarms [10, 13, 4].",
      "startOffset" : 133,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "[6] to suit for path optimization problems.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "Though the initial study relied on trial and error, Dorigo had stated some important features of parameters such as pheromone behavior, the number of ants and how they affect the performance of the algorithm [6].",
      "startOffset" : 208,
      "endOffset" : 211
    }, {
      "referenceID" : 7,
      "context" : "is a study carried out to enhance the performance of the ant system solving the TSP [8].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "Botee and Eric Bonabeauy, they have made the use of genetic algorithm (GA) to evolve the best set of parameters [3].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : "in their research, have used artificial neural networks (ANN) to find the best parameter set for ACS solving a given TSP [7].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "As such there are other researches which have been conducted to find better parameter sets for the ACS in solving TSP [9, 20].",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "As such there are other researches which have been conducted to find better parameter sets for the ACS in solving TSP [9, 20].",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 15,
      "context" : ", in the implementation of the self-tuning framework have used the firefly algorithm to apply the framework to tune FA’s parameters [19].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : ", use this self tuning framework combined with the firefly algorithm to solve nonlinear equitations [2].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "1 Traveling Salesman Problem (TSP) The Traveling Salesman Problem is one of the most intensively studied problems in computational mathematics which is simple to state but very difficult to solve [1].",
      "startOffset" : 196,
      "endOffset" : 199
    }, {
      "referenceID" : 10,
      "context" : "All ant species are known to be deaf [11].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "In the original implementation, ACS was applied to solve the TSP [6].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "q is a random number uniformly distributed in [0, 1], q0 is a parameter (0 ≤ q0 ≤ 1), and S is a random variable from the probability distribution given by the equation (2).",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "Biological studies reveal many factors about fireflies’ life style that are interesting [14].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "Focusing on their flashing behavior, the firefly algorithm was developed by Xin-She-Yang in 2009 [18].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "δ where δ ∈ [0, 1] (8)",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 15,
      "context" : "in 2013 introduced a framework for selftuning algorithms and it was implemented with the firefly algorithm successfully [19].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : "The range for these parameters are β ∈ [0 8], ρ ∈ [0.",
      "startOffset" : 39,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "These parameters varies in the ranges FF_γ ∈ [0 10] and FF_δ ∈ [0.",
      "startOffset" : 45,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "1 β [0 8] β [0 8] β 2 ρ [0.",
      "startOffset" : 4,
      "endOffset" : 9
    }, {
      "referenceID" : 7,
      "context" : "1 β [0 8] β [0 8] β 2 ρ [0.",
      "startOffset" : 12,
      "endOffset" : 17
    }, {
      "referenceID" : 9,
      "context" : "3 PSO_Q2 2 FF_β0 1 FF_γ [0 10] FF_δ [0.",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "new algorithm is compared with the original ant colony algorithm (ACS) [6] and an adaptive parameter control strategy for ACO implemented using the PSO algorithm (PSOACS) [8].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "new algorithm is compared with the original ant colony algorithm (ACS) [6] and an adaptive parameter control strategy for ACO implemented using the PSO algorithm (PSOACS) [8].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 4,
      "context" : "[5] were followed to perform the statistical analysis.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "To test the hypothesis’ related to the study, we have used ANOVA (Analysis of variance) technique [17].",
      "startOffset" : 98,
      "endOffset" : 102
    } ],
    "year" : 2016,
    "abstractText" : "Ant colony system (ACS) is a promising approach which has been widely used in problems such as Travelling Salesman Problems (TSP), Job shop scheduling problems (JSP) and Quadratic Assignment problems (QAP). In its original implementation, parameters of the algorithm were selected by trial and error approach. Over the last few years, novel approaches have been proposed on adapting the parameters of ACS in improving its performance. The aim of this paper is to use a framework introduced for self-tuning optimization algorithms combined with the firefly algorithm (FA) to tune the parameters of the ACS solving symmetric TSP problems. The FA optimizes the problem specific parameters of ACS while the parameters of the FA are tuned by the selected framework itself. With this approach, the user neither has to work with the parameters of ACS nor the parameters of FA. Using common symmetric TSP problems we demonstrate that the framework fits well for the ACS. A detailed statistical analysis further verifies the goodness of the new ACS over the existing ACS and also of the other techniques used to tune the parameters of ACS.",
    "creator" : "LaTeX with hyperref package"
  }
}