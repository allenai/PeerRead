{
  "name" : "1303.4183.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "GENERATING EXTREMA APPROXIMATION OF ANALYTICALLY INCOMPUTABLE FUNCTIONS THROUGH USAGE OF PARALLEL COMPUTER AIDED GENETIC ALGORITHMS",
    "authors" : [ "Lukasz Swierczewski" ],
    "emails" : [ "luk.swierczewski@gmail.com" ],
    "sections" : [ {
      "heading" : "GENERATING EXTREMA APPROXIMATION OF ANALYTICALLY INCOMPUTABLE FUNCTIONS THROUGH USAGE OF PARALLEL COMPUTER AIDED GENETIC ALGORITHMS",
      "text" : "Lukasz Swierczewski\nluk.swierczewski@gmail.com Computer Science and Automation Institute, College of Computer Science and Business\nAdministration in Lomza, Poland\nThis paper presents capabilities of using genetic algorithms to find approximations of function extrema, which cannot be found using analytic ways. To enhance effectiveness of calculations, algorithm has been parallelized using OpenMP library. We gained much increase in speed on platforms using multithreaded processors with shared memory free access. During analysis we used different modifications of genetic operator, using them we obtained varied evolution process of potential solutions. Results allow to choose best methods among many applied in genetic algorithms and observation of acceleration on Yorkfield, Bloomfield, Westmere-EX and most recent Sandy Bridge cores.\nKeywords: artificial intelligence, genetic algorithms, parallel algorithms, functions extremes"
    }, {
      "heading" : "Introduction",
      "text" : "Genetic algorithm (GA) is a type of algorithm inspired by the evolution of living organisms in the nature. It belongs to evolution algorithms whose idea was started by John Henry Holland, the American engineer and scientist. GA in a specific way searches in the area of solutions of a problem to find the best solution. The algorithm defines environment in which a specific population of specimens being possible solutions of the problem exists. Next, similarly to organisms in the nature, the specimens are cross-bred, mutated and selection of the best solutions based on the value of adaptation function occurs.\nIdeas of genetic algorithm were presented in Fig. 1.\nGenetic algorithms have distinctive features which distinguish them from other methods of calculation:\n• genetic operators are used which are adapted to the form of solutions, • processing of solution population allows to at the same time search in area of solutions from\ndifferent starting points in order to direct the searching process, • the quality of current solutions is the sufficient information, • random elements are introduced on purpose\nGenetic algorithms have enormous range of applications in the modern science and engineering. Due to their advantages, they are used when a method of solving problem is not precisely defined, but a method of evaluation of a solution in comparison with others is well known. The classic example is travelling salesman dilemma with whose solution standard algorithms manage relatively poor. GA are also widely used when designing electrical circuits. In such case, evaluation of each specimen is based on the amount of various elements and on their electrical features which can be easily calculated. Using GA, John R. Koza formulated new versions of PID [1] (proportionalintegral-derivative controller) which is frequently applied in automatics. Due to the development of FPGA, systems enabling programming structure of electrical circuit contained in them, an experimental project named Golem [2] was designed. It uses genetic algorithms for construction of robots without any support from man. Unfortunately, the project shown that the modern engineering cannot manage such issues and with the use of modern equipment too much time is required for the visible evolution to occur.\nConsiderably simpler, yet even more popular application of GA is finding extremes of functions. For this purpose also various numerical methods can be used, however, as it turns out, GA often provides good solutions in much shorter amount of time. In some cases, good effects may be obtained by using parallel programming and potential of the modern processors with several arithmetic and logic units. The aspect of genetic algorithms in connection with advanced parallel platforms was brought up in this work."
    }, {
      "heading" : "Algorithm",
      "text" : "The implementation of GA described and presented in the work is a fragment of Olib library. Its sources written in C/C++ language are available under [3] address. In genetic algorithm the following crossing methods were implemented:\n• One-point crossover, • Two-point crossover, • Three-point crossover, • Uniform crossover (Mixing Ratio = 0.5), • Uniform crossover (Random Mixing Ratio), • Half Uniform crossover, • Arithmetic crossover (functions: AND, OR, NOR, NAND, XOR, Random)\nOne-point crossover method was implemented in two ways. In the first implementation in the event of crossing the result is always one child. In the second implementation with 2/3 probability one child will be born, with 2/9 probability two children and with 1/9 probability three children. Pseudocode representing extension of the method was presented in Listing 1.1.\nThe most frequently encountered in literature and supported by the algorithm selection methods include:\n• Roulette, • Tournament, • Linear Ranking\nIn case of performance testing on various hardware configurations, the condition of stop of algorithm was defined with the number of generations. During the analysis of various genetic methods whose results were presented in Tab. 1, Tab. 2 and Tab. 3 the algorithm ended its operation if the condition was fulfilled:\nwhere: F(indiv_i) specifies value of adaptation function for i-specimen, n defines the number of specimens in population, F(best(indiv)) defines value of adaptation function of the best specimen in population.\nGenerally, it can be said that this condition determines stopping of the program execution when all specimens obtain the same, best value.\nListing 1.1. Pseudocode of the modified version of One-point crossover operator.\nvoid one_point_crossover(individuals_table[], individuals, lvl) {\nif(lvl < 3) {\nrandom_value = rand(); if(random_value % 3 == 0) {\nlvl++; one_point_crossover(individuals_table, individuals, lvl);\n}\n// One child is // created at this point.\n} }\none_point_crossover(individuals_table, individuals, 0);"
    }, {
      "heading" : "Realization calculations",
      "text" : "During the calculations the capabilities of parallel programming were used. Genetic operators were made parallel in such a way that they were executed according to the needs parallel on many specimens. The effect of making the executed instructions parallel was obtained through the use of OpenMP library which in a simple way allows to create new threads. This type of software fully\nutilises capabilities of the modern multi-core processors as well as multiple core platforms with common memory. All calculations were carried out in Linux OS environment and for compilation of programs Intel ICC 12.x version was used.\nGraphs in the range [2; 130] were presented in Fig. 2 and Fig. 3, respectively. Mainly in this range the focus was put on looking for maximum and minimum of the function. The algorithm was tested on four different calculating platforms:\n• Yorkfield: processor Intel Core 2 Quad Q8400 • Bloomfield: processor Intel i7 920 • Westmere-EX: processor Intel Xeon E7-4860 • Sandy Bridge: processor Intel i5-2400\nAdditionally, during calculations the impact of setting process affinity on the full time of processing data was checked. On Intel i7\\ 920 and Xeon E7-4860 processors the Hyper-Threading technology is available which enables increasing performance in result of duplication of some fragments of a processor (mainly registers). The actual impact on the calculations of HT technology was tested. Furthermore, some processors with a code name Sandy Bridge support Turbo Boost 2.0 technology. As it turned out, Intel Core i5-2400 processor using this technology had in some cases problems with achieving full performance.\nThe results concerning convergence of algorithm due to the application of various genetic algorithms.\nThe produced outcomes regarding capabilities of the implemented genetic algorithms and speed of their convergence for the function (1) were presented in Tab. 1 Analyses for the function (2) were contained in Tab. 2 and Tab. 3.\nIn case of the first function the genetic algorithm always, regardless of the applied methods, found the appropriate solution. In this instance, convergent outcomes could be very quickly obtained with Half Uniform crossover method, and the algorithm had to generate only 7 generations to fulfil the stop condition. Relatively good results were also obtained as a result of Two-point crossover method. Only 7 generations in case of looking for minimum and 33 in case of maximum is an interesting outcome as well. It is worth to consider results of two different implementations of Onepoint crossover method. In theory, the version recurrently performed at most three times and generating one to three children should provide better outcomes than the implementation which always creates only one child. However, in case of looking for the maximum it turns out that the situation during the carrying out of the tests looks completely differently and the simpler version of crossing is significantly more quickly convergent. Such result may signify a large impact of random factors on the operation of the entire program.\nIn Tab. 2 and Tab. 3 considerably more detailed outcomes of the function analysis were presented (2). In Fig. 2 it is possible to observe that the function has many distinctive local maximums and minimums. In this case, genetic algorithm considerably more easily may fall into local minimum or maximum which is not the best result. In conducted tests concerning various selection types the best outcomes were obtained with tournament selection when the size of the group was 10 specimens. In case of smaller tournament groups of 2 specimens, the algorithm was the least frequently finding the correct solution (only 4 times out of 24 trials). Linear Ranking and Roulette methods are\nsolutions marked with indirect effectiveness. Precise results are presented in the respective tables.\nIn case of function (2) it can also be noticed that different implementations of One-point crossover genetic operator are convergent, in accordance with the predictions, and the version generating according to distribution of probability maximally three children usually causes quicker end of the program operation.\nThe results concerning effectiveness of implementation of various genetic operators\nImplementations of various genetic operators are characterised by various calculation complexity. Intuitively, calculation complexity of Two-point crossover operator will be higher than One-point crossover due to division of chromosome in the larger number of points. On the other hand, complexity of all Arithmetic crossover operators will be comparable. Only Arithmetic crossover operating in Random mode (the arithmetic function applied in a case is always random) it may work noticeably slower due to the necessity of formulating a function generating a pseudorandom number.\nhe time of executing only various functions responsible for crossing in genetic algorithm was presented in Fig 4. A similar comparison for selection and comparison methods was presented in Fig. 5 and Fig. 6, respectively. In Fig. 7 it is possible to see execution of what functions constitutes the operation time of the whole program. According to the measurements, the most time of processor is devoted to selection: about 91%. Other operators of crossing and mutation consume only 3.6% and 1.2%, respectively. It proves that optimisation of genetic operator of selection can yield better gain of total performance.\nThe results achieved with the use of various hardware platforms\nOther, significantly more interesting aspect discussed in this work is the use of various hardware platforms for calculations based on genetic algorithms. In case of ordinary processors used at home, an actual acceleration was observed. The outcomes for Intel Core 2 Quad Q8400, i7\\ 920 and i52400 processors were presented in Fig. 8, Fig. 9 and Fig 10, respectively. The list includes Real Time – the time of program execution from the perspective of user (time of execution of the longest thread) as well as total time calculated for all threads – System Time. On all platforms the tests included execution of algorithm with 1 to 8 threads. Only in respect to Intel i7\\ 920 processor an increase of performance in comparison between four and eight threads was observed. This processor has physically only four cores but thanks to Hyper Threading technology it is recognised by the system as a configuration with eight cores. HT technology in case of this processor allows to reduce time by additional 740 seconds, that is about 29.5% in comparison with the ordinary time of use of four cores. The typical fall of performance when comparing times for four and five threads is most probably caused by platform’s problems with assigning the excessive, fifth thread, to one of the four physical processors and its migration during operation of the program through various cores. The problem disappears in case of six and higher number of threads where the potential of Hyper-Threading technology is much better utilised. In case of Core 3 Quad Q8400 and i5-2400 processors, the time of execution falls only when four or less threads are used. Further threads created on these platforms are still executed by four processors. They have to be divided into smaller time quanta and properly assigned to processors which causes fall of performance. It is for this reason why usually it is presumed that the number of calculation threads in a program should be equal to the number of available processors in the system. In Fig. 10 an extreme fall of performance in the moment of moving from four to five threads in the case of i5-2400 processor can be seen. When the number of threads does not exceed the physical number of cores the processor works with\nfull clock rate amounting to 3100 MHz. When the number of threads was larger, Turbo Boost 2.0 technology set clock rate to 1600 MHz which caused a considerable fall of performance.\nA very advanced platform constructed with four ten-core Intel Xeon 7-4860 processors was used in the tests. These processors support Hyper-Threading technology which makes one unit to be seen by the system as 20 cores. The system is built from four such processors, therefore, in theory it enables running a program with 80 threads. For purely practical reasons performance measurements were carried out using only 1 to 40 threads. The results were presented in Fig. 11. The shortest execution time was obtained when the calculating capabilities of only 19 threads were used. The problem with performance scaling on computers with common memory is frequently that, while the number of processors may increase linearly, speed of operating memory does not increase linearly and it is memory which at some point becomes a bottleneck of the entire configuration.\nThe results obtained through changes in process affinity settings\nThe process affinity settings were also tested. Process affinity consists of locking processes to specific processors so the process does not have to lose time on migration between processors, during the execution. System planner typically deals with this task. As it turns out, improvement of performance achieved in result of process affinity is slight. For Intel i7 920 processors comparison of times of program execution with enabled and disabled affinity may be seen in Fig. 13. The largest gain was observed when 2 threads were used and it amounted to 106 seconds which is barely 2.9%.\nOn the other hand, a considerable gain was brought in case of 10 threads and a platform consisting of four Intel Xeon E7-4860 processors. In case of dividing threads to processors by system planner, the time of execution of the program amounted to 1995 seconds. When user manually made all threads to be executed by 10 cores organised within one processor, the time decreased to only 1303. The difference is 34.6% which is a perceptible value. The acceleration was shown in Fig. 12.\nTable. 1. A summary list of speed and correctness of convergence of different crossing methods for function (1).\nParameters of the program: Function (2); Range: [2; 1048578]; Searching Maximum; Population size: 16384; Probability of mutation = 100%; Crossover: Two-point crossover (probability = 50%); Selection: Roulette; Linear\nscaling; 10000 generations.\nParameters of the program: Function (2); Range: [2; 1048578]; Searching Maximum; Population size: 16384;\nMutation: Bit inversion (probability = 1%); Crossover: Two-point crossover (probability = 50%); Selection: Roulette; Linear scaling; 10000 generations.\nSystem time measured for eight threads processor Intel Core i7 920."
    }, {
      "heading" : "Conclusion",
      "text" : "Genetic operators analysed in the article can be divided into more and less demanding methods in respect to the executed operations. As regards crossing, Uniform crossover implementation proved to be the most complex, and in relation to selection this tournament method whose complexity raises with the increase of size of tournament group.\nGenetic algorithms relatively well undergo process of paralleling. The acceleration is perfectly noticeable on the modern multi-core processors. In case of more advanced platforms built from\nmany multi-core processors problems with appropriate performance scaling may occur.\nThe application of programmable graphic accelerators which in the recent years have become very popular may turn out to be a very interesting prospect. Properly utilised, they will certainly allow to even more quickly process data."
    }, {
      "heading" : "Acknowledgment",
      "text" : "The work has been prepared using the supercomputer resources provided by the Faculty of Mathematics, Physics and Computer Science of the Maria Curie-Skłodowska University in Lublin and Computer Science and Automation Institute of the College of Computer Science and Business Administration in Lomza."
    }, {
      "heading" : "GENERACJA PRZYBLIŻEŃ EKSTREMÓW FUNKCJI NIEOBLICZALNYCH ANALITYCZNIE DZIĘKI ZASTOSOWANIU ALGORYTMÓW GENETYCZNYCH ZE WSPARCIEM KOMPUTERÓW RÓWNOLEGŁYCH",
      "text" : "Praca prezentuje możliwości zastosowania algorytmów genetycznych do odnajdywania przybliżeń ekstremów funkcji, których nie można obliczyć w sposób analityczny. Aby zwiększyć efektywność prowadzonych obliczeń algorytm poddano równoleglizacji z wykorzystaniem biblioteki OpenMP. Uzyskano dzięki temu zauważalne przyśpieszenie na platformach o swobodnym dostępie do pamięci wspólnej wykorzystujących procesory wielordzeniowe. Podczas analiz wykorzystano różne modyfikacje operatorów genetycznych, dzięki którym uzyskano zróżnicowane procesy ewolucji osobników, będących potencjalnymi rozwiązaniami. Wyniki umożliwiają wybór najlepszych metod spośród wielu stosowanych w algorytmach genetycznych oraz obserwację akceleracji na układach Yorkfield, Bloomfield, Westmere-EX oraz najnowocześniejszych Sandy Bridge.\nSłowa kluczowe: sztuczna inteligencja, algorytmy genetyczne, algorytmy równoległe, ekstrema funkcji\nPraca została przygotowana z wykorzystaniem zasobów superkomputerowych udostępnionych przez Wydział Matematyki, Fizyki i Informatyki Uniwersytetu Marii Curie-Skłodowskiej w Lublinie oraz Instytutu Informatyki i Automatyki Państwowej Wyższej Szkoły Informatyki i Przedsiębiorczości w Łomży."
    } ],
    "references" : [ {
      "title" : "Genetic Algorithms + Data Structures = Evolution Programs,SpringerVerlag",
      "author" : [ "Z. Michalewicz" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1999
    }, {
      "title" : "Parallel Genetic Algorithm Taxonomy",
      "author" : [ "R. Poli", "M. Nowostawski" ],
      "venue" : "Wydawnictwa Naukowo-Techniczne,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "Genetic Programming – An Introduction",
      "author" : [ "W. Banzhaf", "P. Nordin", "R. Keller", "F. Francone" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "An Introduction to Genetic Algorithms",
      "author" : [ "M. Mitchell" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1996
    }, {
      "title" : "Theory of Genetic Algorithms",
      "author" : [ "L. Schmitt" ],
      "venue" : "Theoretical Computer Science",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2001
    }, {
      "title" : "Theory of Genetic Algorithms II: models for genetic operators over the stringtensor representation of populations and convergence to global optima for arbitrary fitness function under scaling",
      "author" : [ "L. Schmitt" ],
      "venue" : "Theoretical Computer Science",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2004
    } ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "This paper presents capabilities of using genetic algorithms to find approximations of function extrema, which cannot be found using analytic ways. To enhance effectiveness of calculations, algorithm has been parallelized using OpenMP library. We gained much increase in speed on platforms using multithreaded processors with shared memory free access. During analysis we used different modifications of genetic operator, using them we obtained varied evolution process of potential solutions. Results allow to choose best methods among many applied in genetic algorithms and observation of acceleration on Yorkfield, Bloomfield, Westmere-EX and most recent Sandy Bridge cores.",
    "creator" : "Writer"
  }
}