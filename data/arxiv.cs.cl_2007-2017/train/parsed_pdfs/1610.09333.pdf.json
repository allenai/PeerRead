{
  "name" : "1610.09333.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Word Embeddings for the Construction Domain",
    "authors" : [ "Antoine J.-P. Tixier", "Michalis Vazirgiannis", "Matthew R. Hallowell" ],
    "emails" : [ "ANTOINE.TIXIER-1@COLORADO.EDU", "MVAZIRG@LIX.POLYTECHNIQUE.FR", "MATTHEW.HALLOWELL@COLORADO.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In construction like in many other industries, larger and larger amounts of digital natural text are becoming available. The needs to efficiently process that text are pressing. However, the recent text mining approaches introduced in the construction field do not capitalize on the latest advances in Natural Language Processing (NLP). For instance, the feature extraction system of (Tixier et al., 2016a) relies on manually written rules and human-\nbuilt dictionaries of keywords, while (Chokor et al., 2016; Williams and Gong, 2014; Yu and Hsu, 2013; Caldas and Soibelman, 2003) are all based on the traditional vector space model and Term Frequency - Inverse Document Frequency (TF-IDF) weighting scheme (Salton and Buckley, 1988).\nIn this paper, we apply word embeddings to the construction domain for the first time. Word embeddings are lowdimensional continuous representations of words that have recently rose to fame following the introduction of the acclaimed word2vec model (Mikolov et al., 2013a). Provided a large enough corpus, this model was shown to be able to generate embeddings of unmatched quality at minimum cost. These high quality embeddings were in turn shown to boost many NLP tasks like syntactic and semantic word similarity (Mikolov et al., 2013a), machine translation (Mikolov et al., 2013b), or document classification (Kusner et al., 2015).\nThe contributions of this paper are fourfold :\n• we release an 11M-word corpus of constructionrelated text that we created from scratch by leveraging only publicly available resources. We believe this corpus to be one of the largest publicly available collections of raw construction-specific text to date,\n• we show that our corpus can be used to learn word embeddings that both encode meaningful constructionspecific concepts,\n• we introduce a novel data set for injury report classification (5,845 cases, 11 dependent variables). In addition to document categorization performance, this data set can be used to evaluate keyword extraction performance,\n• using an injury report classification task as a case study, we show that our custom (i.e., local) embed-\nar X\niv :1\n61 0.\n09 33\n3v 1\n[ cs\n.C L\n] 2\n8 O\nct 2\ndings outperform in many cases global word vectors learned on a general 100B-word corpus (Google News).\nThe rest of this paper is structured as follows. We first provide an introduction to the concept of word embeddings. Then, we discuss the creation of our construction-specific word vectors and explore the embedding space. Finally, we apply our vectors to the task of injury report classification and quantitatively compare their performance to that of the Google News ones."
    }, {
      "heading" : "2. Word Embeddings",
      "text" : ""
    }, {
      "heading" : "2.1. Limitations of the vector space model",
      "text" : "Traditionally, text has been represented with the vector space model (Salton and Buckley, 1988). Within this framework, each unique term (i.e., unigram, bigram, etc., up to a certain order) in the universe of documents is considered as an independent dimension of the space, and is encoded as a so-called “one-hot” vector. In that discrete space, documents are represented as sparse vectors where the entries are usually binary (1 when the word is present in the document, 0 else), occurrence counts (“bag-of-words” approach), or TF-IDF weights.\nThis approach is limited because it considers terms as independent units. Therefore, semantic (meaning) and syntactic (grammar) term-term dependence are completely overlooked. For instance, the word hammer may be represented as the vector [0, 0, 1, 0, ..., 0, 0, 0, 0] and the word tool as the vector [0, 0, 0, 0, ..., 0, 0, 1, 0]. It follows that −−−−−−→ hammer · −−→ tool = 0. Put differently, according to the vector space model, hammer and tool are orthogonal. They are as dissimilar as hammer and truck.\nThe vector space model also suffers from the curse of dimensionality. For instance, if the goal is to model the probability distribution of a sequence of n = 6 terms in a corpus of vocabulary V of size |V | = 105 unique words, the space that needs to be filled is a 6-dimensional hypercube where each dimension has 105 slots. One would need astronomic amounts of data to perform this task (called ngram language modeling). This is why traditionally, only bigrams and trigrams models have been used in practice (Katz, 1987). Of course, using such short contexts significantly reduce the amount of term-term relationship information that can be captured."
    }, {
      "heading" : "2.2. Distributed word representations",
      "text" : "The two aforementioned limitations have motivated the use of distributed representations of words, also known as word embeddings or word vectors (Bengio et al., 2003). As shown in Figure 1, word embeddings map each word in\nthe vocabulary to a real-valued vector in a dense continuous space of dimension m |V |. The m features encode concepts shared by all words. Typically, while |V | lies in the [105, 106] interval, m takes values within [100, 500]. Word embeddi gs: distributed repr sentation of words\n- smoothing (discrete to continuous)\n- densification (sparse to dense)\n• Fighting the curse of\ndimensionality with:\n• Similar words end up close to each other in the feature space\n• Each unique word is mapped to a point in a rea continuous m-dimen ional space • Typically, V > 106, 100 < m < 500\n5\nRecently, (Mikolov et al., 2013a) showed that high quality word embeddings could be obtained in a very fast way as a side effect of feeding very large amounts of text to a shallow neural network. The underlying assumption is that a model simple enough to be fed very large amounts of text generates better embeddings than a more complex model that can only afford to be trained on less data.\nMoreover, the embeddings generated after training on large corpora were shown to encode amazingly good syntactic and semantic regularities as simple vector operations (Mikolov et al., 2013c). Constant linear translations were shown to capture many concepts like pluralization, gender, country-capital, or genius-field. Classical examples include −−→ cats − −→cat = −−−→mice − −−−−→mouse, −−→ king − −−→man + −−−−−→woman = −−−→queen, −−−−→ france − −−−→paris =\n−−→ italy − −−−→rome, and−−−−−→\neinstein−−−−−−−→scientist+−−−−−→painter = −−−−−→picasso."
    }, {
      "heading" : "2.3. word2vec",
      "text" : "The model of (Mikolov et al., 2013a), also referred to as word2vec, is based on the Distributional Hypothesis (Harris, 1954), which can roughly be summarized as “we shall know a word by the company it keeps”, and is simply illustrated in Figure 2.\n• Key idea of word2vec: achieve better performance not by using a more complex model\n(i.e., with more layers), but by allowing a simpler (shallower) model to be trained on much larger amounts of data\n• Two algorithms for learning words vectors:\n- CBOW: from context predict target (focus of what follows) - Skip-gram: from target predict context\n• Compared to Bengio et al.’s (2003) NNLM:\n- no hidden layer (leads to 1000X speedup) - projection layer is shared (not just the weight matrix) - context: words from both history & future:\nGoogle’s word2vec (Mikolov et al. 2013a)\nMore precisely, word2vec first builds (context, target) pairs by linearly parsing the input text from start to finish, where target is a given word and its context of size 2n is made of the n preceding and n following words. Reasonable values of n are around 5, which captures much\nricher word-word relationship information than 2-gram or 3-gram models. word2vec then iteratively passes the (context, target) pairs to a shallow neural network (featuring an input, projection, and output layer only), with the task of predicting either the target word given its context (CBOW architecture) or the context of a given target word (skip-gram architecture), as described in Figure 3.\nFormally, the objective of the skip-gram model is to maximize:\n1\nT T∑ t=1 ∑ −n≤j≤n log p(wt+j |wt) (1)\nWhere 2n is the context size, and the training corpus is a sequence of words w1, w2, ..., wT . Note that to obtain good results we must have T |V |.\nAt each iteration, the error gets backpropagated via stochastic gradient descent and accordingly updates the weights of the projection-output and input-projection matrices. Those weight matrices precisely contain the desired word vectors. They are initialized randomly (i.e., words are initially dispersed at random within the space), and then, as training takes place, words get closer to each other (or more distant from each other) as a reaction to the error, in an attraction-repulsion spring-like fashion (Rong, 2014). The movements are coarse at first and get finer and finer as the neural network gets better and better and the error diminishes. Upon several epochs of training with a decreasing learning rate, the final vectors are ready.\nWithin the resulting embedding space, words sharing similar meanings occupy neighboring positions. While both the input and the output weight matrices can theoretically be used or combined, the |V | by m input matrix is generally used as the word vectors (Mikolov et al., 2013c).\nSeveral facts are interesting to note:\n• the word vectors are obtained as a side effect of training. The neural network is actually never used for the prediction task it was trained to perform,\n• thanks to its simplicity (no hidden layer, only a linear projection layer), word2vec is extremely fast to train. Using the original C code with multiprocessing has been reported to offer training rates of several billions of words per hour (Mikolov et al., 2013b),\n• because the process is stochastic, running word2vec twice on the same corpus will not return the same word vectors. However, the similarities (i.e., distances) between words and the organization of concepts remain the same,\n• word2vec was released as an open source project1 by Google in 2013 and has since then been ported to other languages. In this study, we used the popular gensim Python implementation2 (Řehůřek and Sojka, 2010)."
    }, {
      "heading" : "3. Word Vectors Creation",
      "text" : "The main objective at this stage was to gather as much construction-related text as possible. Indeed, word2vec requires large quantities of text to yield good embeddings (the more the better, with diminishing returns). However, although the size of our input text was important, we needed that text to be related to construction as much as possible. In what follows, we present, sorted by decreasing size, the various publicly available resources we leveraged to create our construction-specific corpus."
    }, {
      "heading" : "3.1. Wikipedia",
      "text" : "A first and obvious large source of text was the English Wikipedia. We selected all pages related to the Construction category, and all their children and grandchildren, resulting in a final list of 12,256 pages3. We then used Wikipedia’s Special:Export tool4 to download all these pages as XML files. All the text corresponding to the content of these files was then extracted, yielding a corpus of 6,383,953 words (403,763 unique words).\n1https://code.google.com/archive/p/word2vec/ 2https://radimrehurek.com/gensim/models/word2vec.\nhtml 3https://github.com/Tixierae/WECD/blob/master/ list_wikipedia_pages.txt 4https://en.wikipedia.org/wiki/Special:Export"
    }, {
      "heading" : "3.2. ELCOSH",
      "text" : "The Electronic Library of Construction Occupational Safety and Health5 (ELCOSH) also turned out to be a valuable source of construction-related text. We scraped the pages related to Handouts (245 documents), Toolbox Talks (179), Research Reports (166), and Training Materials (102). Whenever text was not directly available on the web page, we parsed the linked PDF document(s). We thus obtained a corpus of 2,074,769 words (56,070 unique words)."
    }, {
      "heading" : "3.3. OSHA",
      "text" : "We also extracted text from the Occupational Safety and Health Organization (OSHA) website, as follows: IMIS. We queried the Integrated Management Information System (IMIS) accident search tool6 for reports pertaining to the NAICS codes 236, 237, and 238. These codes respectively correspond to the Construction of Buildings, Heavy and Civil Engineering, and Specialty Trade Contractors categories, and were associated at the time of the study with 2,691, 2,430, and 9,780 injury reports respectively. Altogether, the 14,901 reports returned a corpus of 1,497,056 words (25,382 unique words). SLTC. We also leveraged the list of Safety and Health Topics7. 165 pages were parsed in total, giving a 37,629-word corpus (4,851 unique words)."
    }, {
      "heading" : "3.4. CPWR",
      "text" : "Publications. The Center for Construction Research and Training, also known as CPWR, offers a list of research findings and articles on the publications page of its website8. More precisely, we parsed the research reports found in the Design for Safety, Accident Data Analysis, Health Hazards, Safety Hazards, and Hispanic Workers pages. We also parsed the PDF documents linked in the Key Findings from Research webpage. This made for a list of 162 documents in all, yielding a corpus of 416,150 words (26,095 unique words). Workbook. We also parsed the Day Laborers’s Health and Safety Workbook9. This 453-page document returned a 68,776-word (6,679 unique words) corpus.\n5http://www.elcosh.org/index.php 6https://www.osha.gov/pls/imis/accidentsearch. html 7https://www.osha.gov/SLTC/text_index.html 8http://www.cpwr.com/publications/publications 9http://www.cpwr.com/sites/ default/files/publications/ DayLaborersTrainingGuide-UIC-edition-English.pdf"
    }, {
      "heading" : "3.5. NIOSH FACE",
      "text" : "Another source of text specific to the construction domain was found in the form of accident reports from the National Institute for Occupational Safety and Health (NIOSH) Fatality Assessment and Control Evaluation (FACE) program10. The text from the 249 reports belonging to the Construction category (at the time of the study) was gathered. Whenever the webpages included links to PDF documents, those documents were also parsed. This eventually gave us a corpus of 381,969 words (13,770 unique words)."
    }, {
      "heading" : "3.6. USACE",
      "text" : "Finally, we parsed the US Army Corps of Engineers (USACE) manual which prescribes the Safety and Health requirements for all Corps of Engineers operations. This 977- page document returned a corpus of 185,449 words (11,621 unique words)."
    }, {
      "heading" : "3.7. Aggregation and learning",
      "text" : "After final cleaning, we obtained an overall corpus of 11,043,511 words (456,402 unique words, 70MB in size), which was split into 55,495 200-word chunks before being passed to gensim. To enable future studies to build on our work, we make our corpus and the different sub-corpora previously presented freely available for download11.\nTo generate our word embeddings, we used the Skip-gram architecture, as it was reported to give better performance on small corpora (Mikolov et al., 2013b), with standard parameter values: context of size 10, m = 300, a downsampling threhsold of 10−5 for high-frequency words, a negative sampling value of 3, and 10 training epochs. Moreover, words that occurred less than five times in the corpus, standard English stopwords, and custom stopwords12 were not embedded. Using multiprocessing, training took only a few minutes on a standard laptop with four virtual cores. The final embeddings had dimensions 32,689 by 300, which respectively correspond to the vocabulary size and to the number of features (m)."
    }, {
      "heading" : "4. Exploration of the Embedding Space",
      "text" : "We qualitatively evaluated the quality of our word vectors by assessing the extent to which they encoded meaningful relationships between construction-specific concepts. To do so, we assigned our model different problems, which are next presented (along with the results) by increasing\n10https://www.cdc.gov/niosh/face/inhouse.html 11https://github.com/Tixierae/WECD/blob/master/ corpora.zip 12https://github.com/Tixierae/WECD/blob/master/ custom_stpwds.txt\norder of complexity.\nClustering task. We first verified that words similar in meaning were close from one another in our embedding space. To be able to visualize the groupings, we reduced dimensionality with PCA using R (R Core Team, 2015). Figure 4 shows the projections of some selected words onto the first two principal directions. Even if the picture is partial and compressed, we can see that semantically close words, such as “worker”, “employee”, and “crew”, or “sand” and “dirt’, indeed occupy neighboring positions. More generally, words that are often found in similar contexts are also close together (e.g., “bolts”, “beam”, and “steel”, “dust” and “welder”).\nWord similarity task. Another way to assess the extent to which our embeddings learned meaningful construction semantics was to ask our model to return the 10 words closest to a given query (in terms of cosine similarity).\nThe cosine of the angle between two vectors ~v1 and ~v2 is a widely-used similarity metric in vector spaces. It is defined as follows:\ncos(~v1, ~v2) = ~v1 · ~v2\n‖~v1‖ × ‖~v2‖ (2)\nwhere ‖ · ‖ is the euclidean distance. A null cosine indicates orthogonality, while values approaching the unit signify that the two vectors are highly similar.\nAs shown in Tables 1 to 4, the responses to the queries make a lot of sense. Despite the fact that our model was fed raw text in a fully unsupervised fashion (that is, it was not passed any annotated lists of construction-specific words), it managed to learn about different kinds of acids, construction materials and personnel, as well as Computer Aided Design (CAD)/Building Information Modeling (BIM) tools (among other things).\nWithout having been taught technical construction jargon or word pronunciation, the model knows for instance that the abbreviation “2x4”, which refers to standard dimensions of rough planks, can also be written “two-by-four”.\nIt also knows that a foreman is a supervisor, and that Microstation and ZWCAD are competitors of Autocad (Autodesk’s flagship software) in the CAD/BIM market. This approach could be used to automatically build dictionaries of synonyms that would improve the flexibility of tools such as that of (Tixier et al., 2016a).\nWord mismatch task. We also tested whether our word vectors could discriminate between different constructionspecific concepts. To do so, we used the “which words does not match?” task. This problem consists in identifying the word that should be removed from a given list because it is semantically unrelated to the other words. To solve it, the system computes the cosine similarity in the embedding space between each word in the list and all the others, and averages the results. The word that does not match is the one that is associated with the lowest score.\nAs shown in Table 5, our model can successfully differentiate between the construction material, trades, software, equipment, tools, and injury concepts (among others).\nWord analogy task. While the previous problem was a simple mismatch detection task, finding word analogies is slightly more complex. This problem can be stated as finding the answer to “a” is to “b” as “c” is to “unknown”. The solution can be found via the vector offset method (Mikolov et al., 2013c). We simply compute~b−~a+~c and retrieve the word whose vector is the closest (in terms of cosine similarity) from the output. The results for this job are shown in Table 6, and are very convincing. Again, the implication is that following a phase of fully unsupervised training on raw unannotated construction-related text, word2vec was able to learn construction-specific concepts and organize them meaningfully within the embedding space.\nVisualizing analogies. As already explained, concepts are implicitly mapped in a way that allows their relationship\nto be accessed via simple vector operations, such as linear translations. We decided to visualize some of these translations in the PCA space (see Figures 5 to 8). Even though plotting the vectors on the first two principal directions gives a compressed and incomplete representation of what actually happens in the full embedding space, we still can observe that our word vectors encode meaningful regularities. For instance, roughly constant mappings exist between body parts and the corresponding injuries or Personal Protective Equipment (PPE), and between materials and the associated tools or trade."
    }, {
      "heading" : "5. Application to Injury Report Classification",
      "text" : "We also wanted to quantitatively evaluate the quality of our word vectors in a real-life application. To proceed, we designed an injury report classification task. In what follows,\nwe describe the creation of our data set, the experimental set-up, and the results."
    }, {
      "heading" : "5.1. Data set creation",
      "text" : "To create our data set, we iterated through the OSHA IMIS injury reports (for the NAICS codes 236, 237, and 238, see subsection 3.3) and retained the reports that were associated with a “complete” table, that is, a table featuring the following seventeen fields (regardless of whether the fields were blank or not): (1) identification number, (2) keywords, (3) narrative, (4) end use, (5) project type, (6) project cost, (7) number of stories, (8) building height, (9) fatality (yes/no answer), (10) number of employees involved, (11) inspection number, (12) age, (13) gender, (14) injury severity (called degree on the OSHA website), (15)\ninjury type (nature), (16) trade (occupation), and (17) additional information. Other than making easier the systematic and automated creation of the data set, we proceeded in such a manner because we assumed that the reports associated with well-formed tables would be of better quality.\nWe were thus able to obtain a structured data set of 5,845 rows (reports) by 18 columns (NAICS code plus all the seventeen aforementioned field names). Because the age and gender columns had blank fields only, we removed them, making for a final count of 16 columns, including 11 potential dependent variables. For reproducibility, and also so that future studies can use it for benchmarking, we release our data set as publicly available13. It can be used to evaluate injury report classification performance (for various categories), but also keyword extraction performance.\nAll of the 5,845 reports had their major fields (narrative, id number...) filled out. However, we decided to classify reports for injury severity, injury type, and trade. We thus had to retain only the reports for which the fields corresponding to these categories had not been left blank. This yielded a final subset of 1,688 reports (0-based index provided here14) that was still big enough to suit our experiments.\n5.2. k-nearest neighbor classifier\nk-nearest neighbor (Cover and Hart, 1967) is a basic machine learning algorithm that is not supervised nor unsupervised. Indeed, while it does require a set of “training” data, the k-nearest neighbor classifier does not derive rules from them like Support Vector Machines or Random Forest. It simply computes the distance (in the feature space) between a new observation and each observation in the training set, and aggregates the target values of the k neighbors closest to the new observation to generate a prediction for the target value of the new observation. In the case of a continuous dependent variable, the mean or median of the neighbors’ target values is returned, whereas in the categorical case, the most frequent class is used.\nBecause it waits until a prediction is required to actually process the training data, the k-nearest neighbor algorithm is said to be a lazy learning technique. Determining k is subject to the classical bias-variance trade-off: considering only a few neighbors tends to overfit the data (high variance-low bias), whereas taking too many neighbors into account underfits them (low variance, but high bias). Tuning this parameter is therefore important.\n13https://github.com/Tixierae/WECD/blob/master/ classification_data_set.csv 14https://github.com/Tixierae/WECD/blob/master/ index_overlap.txt"
    }, {
      "heading" : "5.3. Word Mover’s Distance",
      "text" : "As was previously explained, the k-nearest neighbor classifier requires a distance to compute similarity between observations in the feature space. For this purpose we used the recently introduced Word Mover’s Distance (Kusner et al., 2015), or WMD. It provides an intuitive way to convert word embeddings to document distances.\nThe WMD is a simple adaptation of the Earth Mover’s Distance (Rubner et al., 2000), a well-known metric in the Computer Vision field, to NLP. It measures the distance between two pieces of text ~p1 and ~p1 as the minimum weighted cumulative cost needed for all words of ~p1 to “travel” to ~p2. It is formally defined as a transportation problem:\nWMD(~p1, ~p2) = min |V |∑\ni,j=1\nTi,jci,j (3)\nwhere |V | is the size of the vocabulary, Ti,j is the (i, j)th entry of a non-negative transfer matrix T ∈ R|V |2 that represents the amount of the ith word in ~p1 ( ~wi) that travels to the jth word in ~p2 ( ~wj), and ci,j is the cost of traveling from ~wi to ~wj . This problem is subject to the constraint that ~p1 must be entirely transported to ~p2.\nFurthermore, (Kusner et al., 2015) represent a document ~pk ∈ R|V | as a normalized vector of word counts in the vector space model (normalized bag-of-words representation), with elements:\npki = count(wi ∈ pk)∑|V | j=1 count(wj ∈ pk) ,∀i ∈ {1, ..., |V |} (4)\nWhat actually makes the connection with word embeddings, is that the travel cost ci,j between ~wi and ~wj is defined as the euclidean distance between them in the word embedding space:\nci,j = ‖ ~wi − ~wj‖ (5)\nThanks to this meaningful travel cost, the WMD provides a natural transition from word embeddings to document distances. An illustrative example is provided in Table 7, where the top 5 injury reports closest to a given report are shown. We can see that the neighbors retrieved by the WMD are very relevant to the query in several aspects. First, all neighbors share the same outcome as the query (hospitalized injury), second, they all deal (except the closest one), with fall from height injuries (like the query). Using in that case the 5-nearest neighbors would therefore yield accurate forecasts for injury severity and injury type.\nApart from plain overlap, some words (shown in italic in Table 7) like “initiated” and “initial”, “plant” and “facility”, “knee” and “leg”, etc. are very close to each other in the embedding space (due to their synonymy) which even more reduces the WMD between the responses and the queries. It is also interesting to note that the responses and the query are very similar in size. Having documents of roughly equal size thus must ease the transportation problem and reduce the WMD distance between them."
    }, {
      "heading" : "5.4. Experimental set-up",
      "text" : "Benchmarking. We compared the quality of our custom construction-specific word vectors against that of general embeddings15 trained on a 100B-word Google News corpus (Mikolov et al., 2013d). These word vectors were generated using the same parameters as we used in our study (m = 300, subsampling threshold of 10−5, negative sampling of 3...), but with the continuous bag-of-words (CBOW) architecture. Recall that in our case, we used the Skip-gram architecture as it is held to perform better on small corpora. While Google News vocabulary size is 3 million, we only selected the vectors of the 32,689 words we had custom embeddings for.\n4-fold cross validation. We used the data set of 1,688 reports described in subsection 5.1 with three prediction tasks: injury severity, injury type, and trade. The number of categories into which observations could fold were 3, 9, and 4, respectively for each task. The words for which no embedding was available were removed from the reports beforehand, as well as punctuation marks. The processed reports can be found here16 (one report per line). Evaluation was conducted under a 4-fold cross validation setting: the full set of reports was split into four folds of equal size (422 reports). Then, at each of four steps, three folds were used as the training instances and the last fold was used as the testing set. The k-nearest neighbor classifier described in subsection 5.2 then generated predictions for each element in the testing set with k varying from 5 to 25, by steps of 5, using both our custom embeddings (custom in what follows) and the Google News embeddings (google in what follows).\nBag-of-words baseline. For comparison purposes, we also used as a baseline the euclidean distance between bag-of-words representations of injury reports (bow in what follows). It was implemented using the euclidean_distances and CountVectorizer functions of the scikit-learn Python library (Pedregosa et al., 2011).\n15https://drive.google.com/file/d/\n0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit 16https://github.com/Tixierae/WECD/blob/master/ reports_processed.txt\nKeyword-based compression. The k-nearest neighbor algorithm has to compute the distance between every element of the test set and every element of the training set to sort the observations and select the nearest ones to generate predictions. This represents a significant number of distances to compute within each fold, which takes time because the WMD is an expensive metric.\nTo speed-up the process, we experimented with a keywordbased compression of injury reports. That is, rather than using the full text of the reports, we tried to represent them with their keywords. The distances (WMD or euclidean) were then computed for these compressed representations rather than based on the full text.\nTo perform keyword extraction, we used the fully unsupervised CoreRank technique of (Tixier et al., 2016b). In short, and as illustrated in Figure 9, this approach builds a graph-of-words from a document, decomposes the graph into its k-cores, and assigns a score to each node that corresponds to the sum of the core numbers of its neighbors. Finally, the top p% nodes are retained as keywords. Here, we used p = 30%.\nAs illustrated in Figure 9, a graph-of-words represents a piece of text as a graph where nodes are unique nouns and adjectives in the document, and where there is an edge between two nodes if the terms they represent co-occur within a sliding window of predetermined size W that is moved along the entire document from start to finish.\nGraph-of-words have many graph building and mining parameters (Tixier et al., 2016c). Here, we weighted edges based on co-occurrence counts, and disregarded their direction (undirected graph). Finally, we ran a window of size W = 8 over the processed versions of the reports. Note that we only performed keyword extraction for the reports featuring at least 15 words. For smaller reports, the full text was kept. The keywords for each report can be\nfound here17. The keyword extraction led to a significant compression illustrated in Figure 10, and to a significant speed-up too, as will be discussed in the next section."
    }, {
      "heading" : "5.5. Results",
      "text" : "Classification performance using the full text of the reports is shown for each class in Tables 8, 9, and 10, respectively for the injury severity, injury type, and construction trade prediction tasks. These results were obtained with the classification_report function of the scikit-learn Python library. The metric used is the standard F1-score, computed (at the class level) for each observation and then averaged (i.e., macro-averaging). A F1-score of 1 indicates perfect classification, while null values mean no skill. The support is the number of observations (and not of predictions) in each class. For each prediction task, we compare approaches for the number of neighbors that yielded the best absolute performance.\nInjury severity. For injury severity, all methods can discriminate well hospitalized injuries from the rest of the outcomes, and to a lesser extent, fatalities. However, perfor-\n17https://github.com/Tixierae/WECD/blob/master/\ncompressed_reports.txt\nmance is very bad for non-hospitalized injuries.\nFor the two categories associated with good skill, the rankings are invariant: google reaches best performance, followed by our custom word embeddings (0.73 and 4.69 absolute improvements, respectively for hospitalized injuries and fatalities). This can be explained by the fact that the vocabulary that discriminates between the different severity levels is not specific to the construction industry at all. Therefore, our custom embeddings cannot compensate for the small size of their corpus of origin (11M-word) with better construction knowledge of the domain, and the google word vectors (trained on a huge general 100B-word corpus) take the lead.\nInjury type. The results are very different for injury type. For this prediction task, our construction-specific word embeddings reach best performance for 4 categories out of 9 (with absolute improvements over google ranging between\n3.45 and 0.21), and are as good as the Google embeddings on a fifth class.\nIt is interesting to note that our embeddings tend to outperform Google ones when the absolute performance is high: of the 4 classes for which custom reaches best performance, 3 are associated with F1-scores over 60%, while 2 out of the 4 categories for which google wins correspond to F1-scores below 43%. Finally, it can be observed that the bag-of-words representation coupled with the euclidean distance leads to very poor results, which shows well the value added by the joint use of distributed representations of words and the WMD.\nConstruction trade. For this prediction task, our custom embeddings outperform Google ones for 2 classes out of 4.\nFor painters, the margin is even quite wide: using custom leads to an absolute improvement of 7.57 in F1-score over google. Conversely, for the categories for which google is better, the absolute improvements do not exceed 1.\nIt is also interesting to note that the 2 classes on which our word vectors reach best performance (namely roofers and painters) are small categories (less than 134 observations in each case, see the “support” row). This could mean that for those small domains, knowledge about specific construction vocabulary, brought by our local embeddings but not by the more global Google ones, is necessary to do well. For this prediction task again, bow performs poorly (although it is surprisingly competitive for the roofers class).\nImpact of keyword-based compression. As can be observed from Table 11, using the keywords extracted from the injury reports rather than their full text leads to a significant speed-up (8 times faster). More precisely, compute time in seconds dropped from 9.7K to 1.1K, as measured during 4-fold cross-validation on an 8-core Intel Xeon 2.4GHz machine (within-fold multiprocessing). The associated relative decrease in performance is around 10%. It was measured in terms of overall F1-score computed over observations (not over categories).\nDiscussion. Overall, it should be noted that the performance of the Google News word vectors is remarkable. Even though they were not trained on a constructionspecific corpus, they still manage to reach best performance in many cases. This tends to corroborate (Kusner et al., 2015; Mikolov et al., 2013a) who observed that using more data is superior than using relevant data when training embeddings. However, our custom word vectors do outperform Google ones in roughly half of the cases (sometimes with a wide margin), suggesting that in some applications,\nlocal embeddings are indeed better than global ones. These results are in accordance with (Diaz et al., 2016)."
    }, {
      "heading" : "6. Conclusion and Next steps",
      "text" : "We presented one of the earliest applications of word embeddings to the construction domain. In addition to releasing one of the largest publicly available collections of raw construction-related text to date (11M words, 450K unique words), and a novel data set for injury report classification, we showed through multiple examples and a case study that the use of word embeddings in the construction industry is very promising and has many potential applications. By allowing more flexible, semi-supervised classification of injury reports into categories, they could be used to better predict and prevent injuries (Tixier et al., 2016d) or for more accurate safety risk modeling and simulation (Tixier et al., 2016e).\nWe could improve our embeddings in several ways. First, our 11M-word corpus can obviously be augmented. Even though we tried to be comprehensive, there are surely many large freely accessible sources of construction text that we did not leverage. This is one of the reasons why we decided to release our corpus as publicly available, so that others can add to it and let the entire construction research community benefit from these additions. Second, we did not tune any of the parameters of word2vec (architecture, window size, downsampling threshold, negative sampling value...), mainly because we were more interested in providing an initial motivation for the use of word embeddings in the construction domain rather than reaching best possible performance. We could verify that out-of-the-box, embeddings learned with standard parameter values do well qualitatively and quantitatively, but fine-tuning could certainly yield better performance.\nHowever, we did quickly check that embedding phrases (in our case, only bigrams) instead of simple unigrams was not giving better results. But once again, this may depend on the parameter values used. We also evaluated embeddings trained solely on the injury reports sub-corpora rather than on the full corpus, but performance was not better. Using pre-trained word vectors (such as Google News) as a starting point and continuing training on our corpus could be worth investigating.\nFinally, it seems that keyword-based compression can significantly increase distance computation speed with only a limited drop in accuracy. We used the CoreRank technique out-of-the-box for exploration purposes, but did not perform any kind of parameter tuning. In the end, the WMD is mainly determined by a few highly discriminative words in each document. Therefore, if these words could happen to be the extracted keywords (through tun-\ning), the decrease in performance would be only marginal. Future work should also investigate the trade-off between compression ratio (number of keywords retained) and classification performance."
    } ],
    "references" : [ {
      "title" : "A neural probabilistic language model",
      "author" : [ "Bengio", "Yoshua", "Ducharme", "Réjean", "Vincent", "Pascal", "Jauvin", "Christian" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Automating hierarchical document classification for construction management information systems",
      "author" : [ "Caldas", "Carlos H", "Soibelman", "Lucio" ],
      "venue" : "Automation in Construction,",
      "citeRegEx" : "Caldas et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Caldas et al\\.",
      "year" : 2003
    }, {
      "title" : "Analyzing Arizona OSHA Injury Reports Using Unsupervised Machine Learning",
      "author" : [ "Chokor", "Abbas", "Naganathan", "Hariharan", "Chong", "Wai K", "El Asmar", "Mounir" ],
      "venue" : "Procedia Engineering,",
      "citeRegEx" : "Chokor et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chokor et al\\.",
      "year" : 2016
    }, {
      "title" : "Nearest neighbor pattern classification",
      "author" : [ "Cover", "Thomas", "Hart", "Peter" ],
      "venue" : "IEEE transactions on information theory,",
      "citeRegEx" : "Cover et al\\.,? \\Q1967\\E",
      "shortCiteRegEx" : "Cover et al\\.",
      "year" : 1967
    }, {
      "title" : "Query Expansion with Locally-Trained Word Embeddings",
      "author" : [ "Diaz", "Fernando", "Mitra", "Bhaskar", "Craswell", "Nick" ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Diaz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Diaz et al\\.",
      "year" : 2016
    }, {
      "title" : "Estimation of probabilities from sparse data for the language model component of a speech recognizer",
      "author" : [ "Katz", "Slava" ],
      "venue" : "IEEE transactions on acoustics, speech, and signal processing,",
      "citeRegEx" : "Katz and Slava.,? \\Q1987\\E",
      "shortCiteRegEx" : "Katz and Slava.",
      "year" : 1987
    }, {
      "title" : "From word embeddings to document distances",
      "author" : [ "Kusner", "Matt J", "Sun", "Yu", "Kolkin", "Nicholas", "Weinberger", "Kilian Q" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)",
      "citeRegEx" : "Kusner et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kusner et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey" ],
      "venue" : "arXiv preprint arXiv:1301.3781,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Exploiting similarities among languages for machine translation",
      "author" : [ "Mikolov", "Tomas", "Le", "Quoc V", "Sutskever", "Ilya" ],
      "venue" : "arXiv preprint arXiv:1309.4168,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic Regularities in Continuous Space Word Representations",
      "author" : [ "Mikolov", "Tomas", "Yih", "Wen-tau", "Zweig", "Geoffrey" ],
      "venue" : "In Proceedings of HLT-NAACL",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "A Language and Environment for Statistical Computing",
      "author" : [ "R R Core Team" ],
      "venue" : "R Foundation for Statistical Computing,",
      "citeRegEx" : "Team.,? \\Q2015\\E",
      "shortCiteRegEx" : "Team.",
      "year" : 2015
    }, {
      "title" : "Software Framework for Topic Modelling with Large Corpora",
      "author" : [ "Radim Řehůřek", "Petr Sojka" ],
      "venue" : "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks ,",
      "citeRegEx" : "Řehůřek and Sojka.,? \\Q2010\\E",
      "shortCiteRegEx" : "Řehůřek and Sojka.",
      "year" : 2010
    }, {
      "title" : "word2vec parameter learning explained",
      "author" : [ "Rong", "Xin" ],
      "venue" : "arXiv preprint arXiv:1411.2738,",
      "citeRegEx" : "Rong and Xin.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rong and Xin.",
      "year" : 2014
    }, {
      "title" : "The earth mover’s distance as a metric for image retrieval. International journal of computer vision",
      "author" : [ "Rubner", "Yossi", "Tomasi", "Carlo", "Guibas", "Leonidas J" ],
      "venue" : null,
      "citeRegEx" : "Rubner et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Rubner et al\\.",
      "year" : 2000
    }, {
      "title" : "Term-weighting approaches in automatic text retrieval",
      "author" : [ "Salton", "Gerard", "Buckley", "Christopher" ],
      "venue" : "Information processing & management,",
      "citeRegEx" : "Salton et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Salton et al\\.",
      "year" : 1988
    }, {
      "title" : "A Graph Degeneracy-based Approach to Keyword Extraction",
      "author" : [ "Tixier", "Antoine J.-P", "Malliaros", "Fragkiskos. D", "Vazirgiannis", "Michalis" ],
      "venue" : "In Proceedings of EMNLP,",
      "citeRegEx" : "Tixier et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tixier et al\\.",
      "year" : 2016
    }, {
      "title" : "GoWvis: a web application for Graph-ofWords-based text visualization and summarization",
      "author" : [ "Tixier", "Antoine J.-P", "Skianis", "Konstantinos", "Vazirgiannis", "Michalis" ],
      "venue" : "In Proceedings of ACL ,",
      "citeRegEx" : "Tixier et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tixier et al\\.",
      "year" : 2016
    }, {
      "title" : "Application of machine learning to construction injury prediction",
      "author" : [ "Tixier", "Antoine J.-P", "Hallowell", "Matthew R", "Rajagopalan", "Balaji", "Bowman", "Dean" ],
      "venue" : "Automation in Construction,",
      "citeRegEx" : "Tixier et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tixier et al\\.",
      "year" : 2016
    }, {
      "title" : "Construction Safety Risk Modeling and Simulation",
      "author" : [ "Tixier", "Antoine J.-P", "Hallowell", "Matthew R", "Rajagopalan", "Balaji" ],
      "venue" : "arXiv preprint arXiv:1609.07912,",
      "citeRegEx" : "Tixier et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tixier et al\\.",
      "year" : 2016
    }, {
      "title" : "Predicting construction cost overruns using text mining, numerical data and ensemble classifiers",
      "author" : [ "Williams", "Trefor P", "Gong", "Jie" ],
      "venue" : "Automation in Construction,",
      "citeRegEx" : "Williams et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2014
    }, {
      "title" : "Content-based text mining technique for retrieval of CAD documents",
      "author" : [ "Yu", "Wen-der", "Hsu", "Jia-yang" ],
      "venue" : "Automation in Construction,",
      "citeRegEx" : "Yu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : ", 2016a) relies on manually written rules and humanbuilt dictionaries of keywords, while (Chokor et al., 2016; Williams and Gong, 2014; Yu and Hsu, 2013; Caldas and Soibelman, 2003) are all based on the traditional vector space model and Term Frequency - Inverse Document Frequency (TF-IDF) weighting scheme (Salton and Buckley, 1988).",
      "startOffset" : 89,
      "endOffset" : 181
    }, {
      "referenceID" : 6,
      "context" : ", 2013b), or document classification (Kusner et al., 2015).",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "The two aforementioned limitations have motivated the use of distributed representations of words, also known as word embeddings or word vectors (Bengio et al., 2003).",
      "startOffset" : 145,
      "endOffset" : 166
    }, {
      "referenceID" : 0,
      "context" : "• Compared to Bengio et al.’s (2003) NNLM: - no hidden layer (leads to 1000X speedup) - projection layer is shared (not just the weight matrix) - context: words from both history & future: “You shall know a word by the company it keeps” (John R.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "In this study, we used the popular gensim Python implementation2 (Řehůřek and Sojka, 2010).",
      "startOffset" : 65,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "For this purpose we used the recently introduced Word Mover’s Distance (Kusner et al., 2015), or WMD.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "The WMD is a simple adaptation of the Earth Mover’s Distance (Rubner et al., 2000), a well-known metric in the Computer Vision field, to NLP.",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : "Furthermore, (Kusner et al., 2015) represent a document ~ pk ∈ R|V | as a normalized vector of word counts in the vector space model (normalized bag-of-words representation), with elements:",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "This tends to corroborate (Kusner et al., 2015; Mikolov et al., 2013a) who observed that using more data is superior than using relevant data when training embeddings.",
      "startOffset" : 26,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "These results are in accordance with (Diaz et al., 2016).",
      "startOffset" : 37,
      "endOffset" : 56
    } ],
    "year" : 2016,
    "abstractText" : "We introduce word vectors for the construction domain. Our vectors were obtained by running word2vec on an 11M-word corpus that we created from scratch by leveraging freely-accessible online sources of construction-related text. We first explore the embedding space and show that our vectors capture meaningful constructionspecific concepts. We then evaluate the performance of our vectors against that of ones trained on a 100B-word corpus (Google News) within the framework of an injury report classification task. Without any parameter tuning, our embeddings give competitive results, and outperform the Google News vectors in many cases. Using a keyword-based compression of the reports also leads to a significant speed-up with only a limited loss in performance. We release our corpus and the data set we created for the classification task as publicly available, in the hope that they will be used by future studies for benchmarking and building on our work.",
    "creator" : "LaTeX with hyperref package"
  }
}