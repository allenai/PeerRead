{"conference": "ICLR 2017 conference submission", "title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.  We evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well.\n\nThe contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field.\n\nThe contribution is not extremely novel: the change with respect to weight normalization is minor. The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better. Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Paper proposes a modification of batch normalization. After the revisions the paper is a much better read. However it still needs more diverse experiments to show the success of the method.\n \n Pros:\n - interesting idea with interesting analysis of the gradient norms\n - claims to need less computation\n \n Cons:\n - Experiments are not very convincing and only focus on only a small set of lm tasks.\n - The argument for computation gain is not convincing and no real experimental evidence is presented. The case is made that in speech domain, with long sequences this should help, but it is not supported.\n \n With more experimental evidence the paper should be a nice contribution.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Sloppy writing, unsufficient experimental validation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors show how the hidden states of an LSTM can be normalised in order to preserve means and variances. The method\u2019s gradient behaviour is analysed. Experimental results seem to indicate that the method compares well with similar approaches.\n\nPoints\n\n1) The writing is sloppy in parts. See at the end of the review for a non-exhaustive list.\n\n2) The experimental results show marginal improvements, of which the the statistical significance is impossible to asses. (Not completely the author\u2019s fault for PTB, as they partially rely on results published by others.) Weight normalisation seems to be a viable alternative in the: the performance and runtime are similar. The implementation complexity of weight norm is, however, arguably much lower. More effort could have been put in by the authors to clear that up. In the current state, practitioners as well as researchers will have to put in more effort to judge whether the proposed method is really worth it for them to replicate.\n\n3) Section 4 is nice, and I applaud the authors for doing such an analysis.\n\n\nList of typos etc.\n\n- maintain -> maintain\n- requisits -> requisites\n- a LSTM -> an LSTM\n- \"The gradients of ot and ft are equivalent to equation 25.\u201d Gradients cannot be equivalent to an equation.\n- \u201cbeacause\"-> because\n- One of the \u03b3x > \u03b3h at the end of page 5 is wrong.\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "incremental", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I think this build upon previous works, in the attempt of doing something similar to batch norm specific for RNNs. To me the experiments are not yet very convincing, I think is not clear this works better than e.g. Layer Norm or not significantly so. I'm not convinced on how significant the speed up is either, I can appreciate is faster, but it doesn't feel like order of magnitude faster. The theoretical analysis also doesn't provide any new insights. \n\nAll in all I think is good incremental work, but maybe is not yet significant enough for ICLR.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well.\n\nThe contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field.\n\nThe contribution is not extremely novel: the change with respect to weight normalization is minor. The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better. Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "16 Dec 2016", "TITLE": "is gamma optimized during training or kept fixed?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "10 Dec 2016", "TITLE": "Significance of performance", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "06 Dec 2016", "TITLE": "Pre-review question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"IS_META_REVIEW": true, "comments": "The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well.\n\nThe contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field.\n\nThe contribution is not extremely novel: the change with respect to weight normalization is minor. The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better. Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "Paper proposes a modification of batch normalization. After the revisions the paper is a much better read. However it still needs more diverse experiments to show the success of the method.\n \n Pros:\n - interesting idea with interesting analysis of the gradient norms\n - claims to need less computation\n \n Cons:\n - Experiments are not very convincing and only focus on only a small set of lm tasks.\n - The argument for computation gain is not convincing and no real experimental evidence is presented. The case is made that in speech domain, with long sequences this should help, but it is not supported.\n \n With more experimental evidence the paper should be a nice contribution.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "Sloppy writing, unsufficient experimental validation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "The authors show how the hidden states of an LSTM can be normalised in order to preserve means and variances. The method\u2019s gradient behaviour is analysed. Experimental results seem to indicate that the method compares well with similar approaches.\n\nPoints\n\n1) The writing is sloppy in parts. See at the end of the review for a non-exhaustive list.\n\n2) The experimental results show marginal improvements, of which the the statistical significance is impossible to asses. (Not completely the author\u2019s fault for PTB, as they partially rely on results published by others.) Weight normalisation seems to be a viable alternative in the: the performance and runtime are similar. The implementation complexity of weight norm is, however, arguably much lower. More effort could have been put in by the authors to clear that up. In the current state, practitioners as well as researchers will have to put in more effort to judge whether the proposed method is really worth it for them to replicate.\n\n3) Section 4 is nice, and I applaud the authors for doing such an analysis.\n\n\nList of typos etc.\n\n- maintain -> maintain\n- requisits -> requisites\n- a LSTM -> an LSTM\n- \"The gradients of ot and ft are equivalent to equation 25.\u201d Gradients cannot be equivalent to an equation.\n- \u201cbeacause\"-> because\n- One of the \u03b3x > \u03b3h at the end of page 5 is wrong.\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "incremental", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "I think this build upon previous works, in the attempt of doing something similar to batch norm specific for RNNs. To me the experiments are not yet very convincing, I think is not clear this works better than e.g. Layer Norm or not significantly so. I'm not convinced on how significant the speed up is either, I can appreciate is faster, but it doesn't feel like order of magnitude faster. The theoretical analysis also doesn't provide any new insights. \n\nAll in all I think is good incremental work, but maybe is not yet significant enough for ICLR.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "No Title", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well.\n\nThe contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field.\n\nThe contribution is not extremely novel: the change with respect to weight normalization is minor. The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better. Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"DATE": "16 Dec 2016", "TITLE": "is gamma optimized during training or kept fixed?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "10 Dec 2016", "TITLE": "Significance of performance", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"DATE": "06 Dec 2016", "TITLE": "Pre-review question", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}], "authors": "C\u00e9sar Laurent, Nicolas Ballas, Pascal Vincent", "accepted": false, "id": "519"}