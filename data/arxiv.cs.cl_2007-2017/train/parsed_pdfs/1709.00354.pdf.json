{
  "name" : "1709.00354.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "QUERY-BY-EXAMPLE SPOKEN TERM DETECTION USING ATTENTION-BASED MULTI-HOP NETWORKS",
    "authors" : [ "Chia-Wei Ao", "Hung-yi Lee" ],
    "emails" : [ "r04942094@ntu.edu.tw,", "hungyilee@ntu.edu.tw" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— Query-by-example, Attention-based Multi-hop Network"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Entering queries in spoken form is attractive because this is the most natural user interface. Hand-held or wearable devices make spoken queries an even more natural choice. Retrieving spoken content with spoken queries is also known as query-by-example spoken term detection (STD). Because both the content and the queries are in the form of speech, it is possible to match the signals directly on the acoustic level, without first transcribing them into phonemes or words. For low-resourced languages with scarce annotated data, or languages without written forms, recognition is difficult, so it makes sense to bypass the need for speech recognition, which usually involves learning from large quantities of annotated audio data.\nThe most intuitive way to search over spoken content for a spoken query is to directly match the audio signals to find\nThis work was sponsored by Ministry of Science and Technology, R.O.C.\nthose audio snippets that sound like the spoken query. Since the audio events in speech signals can be produced at different speeds with different durations, the spoken content and the spoken query are generally not aligned at the same pace. Dynamic time warping (DTW) [1] is widely used in this case. Despite DTW’s wide use, it has several drawbacks. As typical DTW does not have trainable parameters, even in an online system that collects the training data from user feedback, the data cannot be directly used to improve the algorithm. In addition, the time complexity of DTW is usually proportional to the product of the lengths of the spoken queries and audio segments, which for real applications is usually excessive.\nIn this paper, we propose an end-to-end query-by-example STD model. The model is an attention-based multi-hop network, the input of which is a query and an audio segment (containing several utterances), and the output a confidence score representing whether the audio segment includes the query term. In the network, the input spoken query is represented as a vector by an LSTM encoder. We use the attention mechanism to locate the time span of the query term in the audio segment. Similar to query expansion, multiple hops are used update the spoken queries via information extracted from the audio segment. Then a key term detector determines whether the query term exists in the input audio segment. These network components are all learned end-to-end, and the model can be learned in a supervised or unsupervised setting. In a supervised setting, the model is learned from a set of labeled data, which can be collected by user feedback in real applications. In an unsupervised setting, the neural network mimics the behavior of DTW, and it performs as well as DTW but with a lower run-time complexity."
    }, {
      "heading" : "2. RELATED WORK",
      "text" : "There are tons of research related to STD. Here we are not going to cover all aspects of STD. The reader is referred to several tutorial chapters and papers [2–6]. Here we focus on query-by-example STD. This direction exactly matches the target of the Spoken Web Search (SWS) task [7–11] 1, a part\n1It was renamed as “Query by Example Search on Speech Task” (QUESST) since 2014.\nar X\niv :1\n70 9.\n00 35\n4v 1\n[ cs\n.C L\n] 1\nS ep\n2 01\nof the MediaEval campaigns [12]. A complete overview of the approaches developed in SWS in 2011 and 2012 is available [13].\nThe target of this paper is to develop an end-to-end deep learning model for query-by-example STD. On STD with text query, end-to-end approaches have been explored, in which a function which can map the acoustic features of an utterance and a text query to a confidence score is developed. Along this direction, encouraging results have been obtained based on structured support vector machine (SVM) [14–18], in which the time spans corresponding to the queries in utterances are considered as hidden information. However, learning structured SVM is computationally intensive, so this approach is hard to scale. An end-to-end deep learning based system for text query STD has been proposed [19]. In this model, a RNN auto-encoder encodes the utterances in the database, and a character-level RNN encodes the input text query. Finally, a feed-forward neural network predicts whether the query occurs in the utterance. Attention mechanism and multiple hops are not used in the model, which is different from the proposed approach in this paper.\nQuery-by-example STD by representing each word segment as a vector [20–23] is much more efficient than the conventional Dynamic Time Warping (DTW) based approaches, because only the similarities between two single vectors are needed, in additional to the significantly better retrieval performance obtained [23]. Audio segment representation is still an open problem. Several approaches have been successfully used in STD [21, 24–26], but these approaches were developed primarily in more heuristic ways, rather than deep learning. By learning RNN with an audio segment as the input and the corresponding word as the target, the outputs of the hidden layer at the last few time steps can be taken as the representation of the input segment [27]. Audio segment embedding can also be jointly learned with their corresponding character sequences by multi-view approach [28]. Sequence-to-sequence Autoencoder is used to represent variable-length audio segments by vectors with fixed dimensionality, which is referred to as Audio Word2Vec [23]. This previous approach assumes that speech segments to be retrieved have been pre-segmented at word boundaries, which is not realistic. However, it was shown that neural embeddings learned from pre-segmented audio can be applied for embedding arbitrary segments [28]. In this paper, we use attention mechanism to locate the time span of the input query in the utterances to be retrieved, so word boundary segmentation is not needed at both the training and testing stages, and attention is shown to improve the query-by-example performance.\nOn the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31–35]. Some new attention mechanisms [36, 37] recently proposed are shown to be help-\nful on reading comprehension dataset where the answer to every question is a segment of text. In vidual question answering (VQA), attention-based configurable convolutional neural network (ABC-CNN) can learn question-guided attention [38], and it is shown that multiple hops yielded improved results compared to a single step [39]. Attention-based multihop networks were also applied on machine comprehension of spoken content [40], and hierarchical attention model (HAM) [41] further constructed tree-structured sentence representations for sentences from their parsing trees and estimate attention weights on different nodes of the hierarchies."
    }, {
      "heading" : "3. FRAMEWORK AND TRAINING SCENARIO",
      "text" : "Shown in the upper half of Fig 1 is the framework of query-by-example STD using an end-to-end network. The network architecture is described in the next section. The network input is the spoken query and an audio segment in the database to be retrieved. Both the spoken query and the audio segment are represented by acoustic features such as MFCCs. In this paper, the audio segments cover several utterances, and as such are much longer than the spoken queries. The output of the network is a scalar. The scalar represents the confidence that the term in the spoken query exists in the audio segment. Given a spoken query, the system ranks the audio segments in the database according to the confidence scores, and yields the search results. It may seem that the proposed approach only extracts the target audio segments as opposed to locating the time spans of the query terms in the segments; in truth, the time spans are determined based on the attention mechanism in the network. This is shown in the experimental results.\nAs shown in the lower half of Fig. 1, the network is trained in two different scenarios. In the first scenario (left lower corner), labeled examples are collected, each pair of which is composed of a spoken query and an audio segment, including a label indicating whether the segment contains the term in\nthe spoken query. In this case, network training is cast as a binary classification problem. Since labeled data is used in the first scenario, the network achieves better performance than query-by-example approaches such as DTW that use no labeled data. In the second scenario (right lower corner), given a set of query-segment pairs, an existing query-by-example STD approach referred to here as the teacher approach (any method with good performance could be used here) assigns a score to each example pair. This score represents the confidence that the segment includes the query. The network thus learns to predict the confidence scores of the teacher approach given the same example pairs; this is hence a regression task. In the second scenario, as no extra labeled data is needed, it is unsupervised. Since the network is learned from an existing approach, it cannot outperform its teacher. However, if the network performance is equivalent to that of the teacher approach, and if the network is faster than the teacher, it may be reasonable to use the network at testing time instead of the teacher approach."
    }, {
      "heading" : "4. ATTENTION-BASED MULTI-HOP NETWORK",
      "text" : "In this section we describe the model architecture of the attention-based multi-hop network."
    }, {
      "heading" : "4.1. Query Representation",
      "text" : "Fig. 2 (A) illustrates the encoding of the input spoken query into a vector representation VQ. The input query is a sequence of T vectors, x1, x2, ..., xT , each vector xi of which is an acoustic feature vector such as MFCC. In Fig. 2 (A), a bidirectional long short-term memory (LSTM) network [42] takes one frame from the input spoken query sequentially at a time. In this example we use bidirectional LSTM, but it is trivial to replace bidirectional LSTM with unidirectional LSTM. In Fig. 2 (A), the hidden layer output of the forward LSTM (blue rectangle) at time index t is denoted by yft ; that of the backward LSTM (pink rectangle) is ybt . After going through all the frames in the query, the query vector representation VQ,\nor VQ = [y f T ‖yb1]2 is formed as the concatenation of the hidden layer output of the forward LSTM network at the last time index yfT and that of the backward GRU network at the first time index yb1."
    }, {
      "heading" : "4.2. Audio Segment Representation with Attention",
      "text" : "Fig. 2 (B) shows an audio segment (containing several utterances) in the database to be retrieved; although this is a lengthy acoustic feature sequence, we show only eight features for simplicity. The bidirectional LSTM in Fig 2 (B) goes through the whole document and encodes each frame3. The vector representation of the t-th frame St is the concatenation of the hidden layer outputs of the forward and backward LSTM networks; that is, St = [y f t ‖ybt ]. This process can be completed off-line, before the spoken query is submitted. Then the attention value αt for each time index t is the cosine similarity between the query vector VQ (obtained in Fig. 2 (A)) and the vector representation St of each frame, αt = St VQ4. We normalize attention values αt as α′t. Then vectors St from the bidirectional LSTM network for every frame in the audio segment are weighted with this normalized attention value α′t and summed to yield the segment representation vector VS = ∑ t α ′ tSt, which is used to determine the confidence score for spoken query VQ. To ensure a time complexity linear to the length of the input audio segment, we do not use more sophisticated attention models [43]; thus the approach is faster than DTW.\nAs suggested by Chorowski et al. in [44], there are two ways to normalize the scores:\nSharpening: The score list is normalized using the softmax activation function:\nα′i = exp(αi)∑T i=1 exp(αi)\n(1)\nThis has been widely used in many existing neural attention frameworks [45–48], and works well with noisy data.\nSmoothing: The sharpening normalization method puts more focus on a single feature vector Si, and can hence negatively affect the model’s performance. We thus propose a way for the model to aggregate selections from multiple topscored frames, thus bringing more diversity to the model by taking into account more input locations. We replace the exponential function in equation (1) with logistic sigmoid function σ:\nα′i = σ(αi)∑T i=1 σ(αi)\n(2)"
    }, {
      "heading" : "4.3. Hopping",
      "text" : "Fig. 3 illustrates hopping: the input spoken query is first converted into a vector VQ1 by the module in Fig. 2 (A), after\n2Here the symbol [·‖·] denotes concatenation of two vectors. 3The bidirectional LSTMs used in Figs 2 (A) and (B) are the same. 4Symbol denotes cosine similarity between two vectors.\nwhich the module in (B) uses this VQ1 to compute the attention values αt to obtain the story vector VS1 . Then VQ1 and VS1 are summed to form the new question vector VQ2 . This process is the first hop (hop 1). The output of the first hop VQ2 can be used to compute the new attention values to obtain a new story vector VS2 : this can be seen as query expansion, where the machine goes over the audio segment again, extracting information to expand the query to form a new query vector. Again, VQ1 and VS1 are summed to form VQ2 (hop 2). After n hops (n is predefined), the output of the last hop VSn is used to calculate the confidence score."
    }, {
      "heading" : "4.4. Keyword Detection",
      "text" : "Finally, as shown in the upper half of Fig 3, a detector determines the confidence score based on query vector representation VQ and utterance vector representation VSn . Here we use three ways to calculate this score: (1) Use the cosine similarity between VQ and VSn as the score; (2) Use the detector – a connected feedforward neuron network – taking VQ and VSn as input, and output a scalar as the confidence score; (3) Combine approaches (1) and (2): a neural network takes as input the query vector VQ, utterance vector VSn , and cosine similarity, and outputs a score."
    }, {
      "heading" : "5. EXPERIMENTAL SETUP",
      "text" : "We used the LibriSpeech corpus [49] as the data for the experiments. To train the attention-based multi-hop network, some query-segment pairs were needed as training examples. 70,000 training examples were used in the experiments, including 500 different spoken queries; all audio segments were from the LibriSpeech train-clean-360 set. In the supervised scenario, the label for each example (a query-segment pair) specified whether the audio segment contains the spoken query5. In the unsupervised scenario, each example is labeled using the score from the DTW algorithm [50]. There are three testing sets. In all testing sets, the audio segments were\n5This is easily determined using the manual transcriptions of the audio segments available from the LibriSpeech corpus.\nfrom the train-other-500 set in LibriSpeech; thus the audio segments in the training and testing sets did not overlap. As described below, the spoken queries were different.\n• Testing Set 1: There were 1,500 query-segment pairs, including 30 different spoken queries. The spoken queries were all from the training set.\n• Testing Set 2: As with testing set 1, this set also had 1,500 query-segment pairs with 30 different spoken queries. The spoken queries in this set had the same text form as testing set 1, but did not come from the training set.\n• Testing Set 3: This set had 10,000 query-segment pairs with 100 different spoken queries. In this set, the spoken queries were not from the training set, and the text form of the spoken queries never appeared in the training queries.\n39-dimension MFCCs were used as the acoustic features. Mean average precision (MAP) was used as the evaluation measure for query-by-example STD.\nThe network structure and hyper-parameters were set as below without further tuning if not specified:\n• The RNN encoder consisted of two hidden layers with 128 LSTM units.\n• The networks were trained for 100 epochs using ADAM [51] without momentum, with a fixed learning rate of 0.01.\n• Unless otherwise specified, the keyword detector was a network with four hidden layers with 128, 64, 32 and 2 neurons respectively. The keyword detectors were endto-end learned with other part of the network. In the supervised scenario, the attention-based multi-hop network was a binary classifier trained using cross-entropy loss; in the unsupervised scenario, mean square error was the loss function."
    }, {
      "heading" : "6. EXPERIMENTAL RESULTS",
      "text" : "In Sections 6.1, 6.2 and 6.3, we consider the supervised scenario. The results of the unsupervised scenario are presented in Section 6.4."
    }, {
      "heading" : "6.1. Attention-based Model",
      "text" : "Table 1 shows the results of the attention-based model with a single hop. Rows (A) and (B) are two baselines: Row (A) is the MAP of the search results ranked according to DTW similarities on the three testing sets, and row (B) is the model without attention mechanism. We used an unidirectional LSTM to encode both the spoken query and audio segment as a vector representation by taking the hidden layer output of the LSTM\nnetwork at the last time index. Next, we input to the neural network key term detector the query vector and audio segment representations. The detector then outputs a score representing the confidence that the query appears in the audio segment. We find that without the attention mechanism, even though the networks are learned from labeled training data, they are outperformed by DTW, which needs no training data (rows (B) v.s. (A)).\nWe observe that the attention results outperform those without attention and DTW (part (C) v.s. rows (A), (B)). This shows that the attention mechanism is helpful. From Table 1, we note that compared to DTW, the attention model yields larger improvements on test sets 1 and 2. This shows that even though the training and testing queries are from different speakers, the attention-based model still learns the keyword acoustic patterns, which are speaker-independent. However, the attention-based network yields little improvement on test set 3; it is more difficult for the network to transfer what it has learned to keywords it has never seen before.\nIn comparing normalization methods, we find that smooth attention normalization also outperforms the baselines (part (D) v.s. rows (A), (B)), except for the result in row (D-2). However, the sharp attention normalization yields better performance than the smooth version regardless of the keyword detector (parts (C) v.s. (B)). This is reasonable, because the audio segment usually contains only one query term: sharp attention thus yields better performance, as it focuses the network on a narrow span in the audio segment. We also com-\npare results based on unidirectional and bidirectional LSTM encoders, and find that the bidirectional LSTM encoder is not very helpful (parts (D) v.s. (B)) probably because the bidirectional LSTM model has more parameters, and thus tends to overfit the training data. Part (F) shows the integration of the DTW output in row (A) and the attention-based model in part (C), for which the integration weight is 0.4 for DTW and 0.6 for the attention-based model. We observe improvements for test sets 1 and 3. This shows that DTW and the attentionbased model are complementary.\nFor each model in parts (C) to (F), we evaluated the three detectors mentioned in Section 4.4, denoted in Table 1 as NN, Cos, and NN+Cos. Regardless of the model, using the network for keyterm detection always works better than simply computing cosine similarity (NN v.s Cos). Taking cosine similarity as another input to the keyterm detection network (NN+Cos) does not improve performance on test set 1, but does improve the performance in some cases on test sets 2 and 3."
    }, {
      "heading" : "6.2. Attention analysis",
      "text" : "In spoken term detection, we seek to determine not only whether the query term exists in the audio segments, but sometimes also – if they exist – the time spans of these query terms within the segments. We find that the attention weights reveal the time spans of the query terms: Fig. 4 shows an example. An audio segment spectrogram is shown in Fig. 46. The blue curve is the normalized attention weights (sharpening). The time span of the query term in the audio segment is in red7. We find that larger attention weights usually appear at the end of the words (even though we do not provide the model with information on word boundaries). In this exam-\n6The transcription of the audio segment is “was withal one of the most beautiful girls ever seen as people naturally love their own likeness this mother even donated on her eldest daughter and at the same time had a horrible aversion for the youngest she made her eat in the kitchen and work continually”.\n7The query term is “aversion”.\nple, the highest peak of the attention weights is close to the end of the query term.\nIn Fig. 5 we present a quantitative analysis of test set 1. For all query terms and for the audio segments containing the query terms, we compute the time difference between the position with the highest attention weight and the end of the query. The horizontal axis in the figure shows the time difference, while the vertical axis is the percentage among all the audio segments considered. From the figure, we find that distances under one second accounted for 25% of the cases; that is, in these cases the attention mechanism located the query term with less than a one-second error. Further analysis in the enclosed subfigure shows the time duration from 0 to 1 seconds. We find that most of the time differences fall between 0.1 and 0.5; this shows that the highest peak of the attention weights are quite close to the query word. This suggests that attention yields a precise focus on the end location of the query."
    }, {
      "heading" : "6.3. Multiple Hopping",
      "text" : "Table 2 shows the results when using multiple hops to generate audio segment representations. The results in row\n(B) are the results without multiple hops, also shown in row (C-1) of Table 1; the results with 2 to 5 hops are those in rows (C) to (F). Multiple hops outperform single hops (rows (C) to (F) v.s. (B)), except for 1 and 3 hops on test set 1. This shows that hopping improves model generality because in test sets 2 and 3 the training and testing data are mismatched. In row (G) we also integrated the DTW and 3-hop results, yielding further improvements to the MAP score on test set 1."
    }, {
      "heading" : "6.4. Unsupervised Scenario",
      "text" : "Here the network is learned from a teacher approach, so no extra label data is needed. We use DTW as the teacher approach, and normalize the DTW similarity scores between 0 and 1 as the target of regression. The results are shown in Table 3. From the table, we find that the performance of the attention-based network without multiple hops is comparable to DTW (rows (B) v.s. (A)), and that the 3-hop network outperforms DTW on test sets 2 and 3 (rows (C) v.s. (A)). Here we emphasize that the time complexity of the network during testing is far less than DTW: given a document length of M and a query length of N , the time complexity of DTW is O(M × N), while the time complexity of the network is O(M × n), where n is the number of hops8. Therefore, it is reasonable to replace DTW with a network learned from it."
    }, {
      "heading" : "7. CONCLUSION",
      "text" : "In this paper, we propose an end-to-end query-by-example STD model based on an attention-based multi-hop network which has been successfully used in QA. The model can be trained in either an supervised or unsupervised fashion. In the supervised scenario, we show that attention and multiple hops are both very helpful, and that the attention weights of the proposed model reveal the time span of the input keyterm. In the unsupervised setting, the neural network mimics DTW behavior, and achieves performance comparable to DTW with shorter runtimes. In the future, we will explore more new attention-based models, and investigate new models which directly output time spans instead of a confidence score. We will also apply the attention-based multi-hop model on STD with text query.\n8We implemented DTW in C++ and the network using Tensorflow. On average, without using a GPU, the proposed approach was 7 times faster than DTW."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] Yaodong Zhang and James R Glass, “Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams,” in Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 398–403.\n[2] Gokhan Tur and Renato DeMori, Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, chapter 15, pp. 417–446, John Wiley & Sons Inc, 2011.\n[3] C. Chelba, T.J. Hazen, and M. Saraclar, “Retrieval and browsing of spoken content,” Signal Processing Magazine, IEEE, vol. 25, no. 3, pp. 39 –49, may 2008.\n[4] Martha Larson and Gareth J. F. Jones, “Spoken content retrieval: A survey of techniques and technologies,” Found. Trends Inf. Retr., vol. 5, pp. 235–422, 2012.\n[5] Anupam Mandal, K.R. Prasanna Kumar, and Pabitra Mitra, “Recent developments in spoken term detection: a survey,” International Journal of Speech Technology, pp. 1–16, 2013.\n[6] Lin-shan Lee, James Glass, Hung-yi Lee, and Chunan Chan, “Spoken content retrievalbeyond cascading speech recognition with text retrieval,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 9, pp. 1389–1420, 2015.\n[7] F. Metze, N. Rajput, X. Anguera, M. Davel, G. Gravier, C. van Heerden, G.V. Mantena, A. Muscariello, K. Prahallad, I. Szoke, and J. Tejedor, “The spoken web search task at mediaeval 2011,” in ICASSP, 2012.\n[8] F. Metze, E. Barnard, M. Davel, C. Van Heerden, X. Anguera, G. Gravier, and N. Rajput., “The spoken web search task,” in MediaEval 2012 Workshop, 2012.\n[9] Xavier Anguera, Florian Metze, Andi Buzo, Igor Szoke, and Luis Javier Rodriguez-Fuentes, “The spoken web search task,” in MediaEval 2013 Workshop, 2013.\n[10] Xavier Anguera, Luis-Javier Rodriguez Fuentes, Igor Szke, Andi Buzo, and Florian Metze, “Query by example search on speech at mediaeval 2014,” in MediaEval 2014 Workshop, 2014.\n[11] Andi Buzo, Xavier Anguera, Florian Metze, Jorge Proenca, Martin Lojka, and Xiao Xiong, “Query by example search on speech at mediaeval 2015.,” .\n[12] http://www.multimediaeval.org/.\n[13] Florian Metze, Xavier Anguera, Etienne Barnard, Marelie Davel, and Guillaume Gravier, “Language independent search in mediaeval’s spoken web search task,”\nComputer Speech & Language, vol. 28, no. 5, pp. 1066 – 1082, 2014.\n[14] Rohit Prabhavalkar, Karen Livescu, Eric Fosler-Lussier, and Joseph Keshet, “Discriminative articulatory models for spoken term detection in low-resource conversational settings,” in ICASSP, 2013.\n[15] Rohit Prabhavalkar, Joseph Keshet, Karen Livescu, and Eric Fosler-Lussier, “Discriminative spoken term detection with limited data,” in MLSLP, 2012.\n[16] M. Wollmer, F. Eyben, J. Keshet, A. Graves, B. Schuller, and G. Rigoll, “Robust discriminative keyword spotting for emotionally colored spontaneous speech using bidirectional LSTM networks,” in ICASSP, 2009.\n[17] Joseph Keshet, David Grangier, and Samy Bengio, “Discriminative keyword spotting,” Speech Communication, vol. 51, pp. 317 – 329, 2009.\n[18] Joseph Keshet, David Grangier, and Samy Bengio, “Discriminative keyword spotting,” in NOLISP, 2007.\n[19] Kartik Audhkhasi, Andrew Rosenberg, Abhinav Sethy, Bhuvana Ramabhadran, and Brian Kingsbury, “Endto-end asr-free keyword search from speech,” arXiv preprint arXiv:1701.04313, 2017.\n[20] Keith Levin, Katharine Henry, Aren Jansen, and Karen Livescu, “Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings,” in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 410– 415.\n[21] Keith Levin, Aren Jansen, and Benjamin Van Durme, “Segmental acoustic indexing for zero resource keyword search,” in ICASSP, 2015.\n[22] Herman Kamper, Weiran Wang, and Karen Livescu, “Deep convolutional acoustic word embeddings using word-pair side information,” in ICASSP, 2016.\n[23] Yu-An Chung, Chao-Chung Wu, Chia-Hao Shen, HungYi Lee, and Lin-Shan Lee, “Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder,” arXiv preprint arXiv:1603.00982, 2016.\n[24] Hung-Yi Lee and Lin-Shan Lee, “Enhanced spoken term detection using support vector machines and weighted pseudo examples,” Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 6, pp. 1272–1284, 2013.\n[25] I.-F. Chen and C.-H. Lee, “A hybrid HMM/DNN approach to keyword spotting of short words,” in INTERSPEECH, 2013.\n[26] A. Norouzian, A. Jansen, R. Rose, and S. Thomas, “Exploiting discriminative point process models for spoken term detection,” in INTERSPEECH, 2012.\n[27] Guoguo Chen, Carolina Parada, and Tara N. Sainath, “Query-by-example keyword spotting using long shortterm memory networks,” in ICASSP, 2015.\n[28] Shane Settle, Keith Levin, Herman Kamper, and Karen Livescu, “Query-by-example search with discriminative neural acoustic word embeddings,” arXiv preprint arXiv:1706.03818, 2017.\n[29] J. Weston, S. Chopra, and A. Bordes, “Memory networks,” CoRR, vol. abs/1410.3916, 2014.\n[30] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, “Weakly supervised memory networks,” CoRR, vol. abs/1503.08895, 2015.\n[31] A. Kumar, O. Irsoy, J. Su, J. Bradbury, R. English, B. Pierce, P. Ondruska, I. Gulrajani, and R. Socher, “Ask me anything: Dynamic memory networks for natural language processing,” CoRR, vol. abs/1506.07285, 2015.\n[32] A. Bordes, N. Usunier, S. Chopra, and J. Weston, “Large-scale simple question answering with memory networks,” CoRR, vol. abs/1506.02075, 2015.\n[33] C. Xiong, S. Merity, and R. Socher, “Dynamic memory networks for visual and textual question answering,” CoRR, vol. abs/1603.01417, 2016.\n[34] K. M. Hermann, T. Kociský, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom, “Teaching machines to read and comprehend,” CoRR, vol. abs/1506.03340, 2015.\n[35] Kevin J. Shih, Saurabh Singh, and Derek Hoiem, “Where to look: Focus regions for visual question answering,” CoRR, vol. abs/1511.07394, 2015.\n[36] Caiming Xiong, Victor Zhong, and Richard Socher, “Dynamic coattention networks for question answering,” CoRR, vol. abs/1611.01604, 2016.\n[37] Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi, “Bidirectional attention flow for machine comprehension,” CoRR, vol. abs/1611.01603, 2016.\n[38] Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, and Ram Nevatia, “ABC-CNN: An attention based convolutional neural network for visual question answering,” in CVPR, 2016.\n[39] Huijuan Xu and Kate Saenko, “Ask, attend and answer: Exploring question-guided spatial attention for visual question answering,” in CVPR, 2016.\n[40] B.-H. Tseng, S.-S. Shen, H.-Y. Lee, and L.-S. Lee, “Towards machine learning comprehension of spoken content: Initial toefl listening comprehension test by machine,” in INTERSPEECH, 2016.\n[41] Wei Fang, Jui-Yang Hsu, Hung yi Lee, and Lin-Shan Lee, “Hierarchical attention model for improved machine comprehension of spoken content,” in SLT, 2016.\n[42] Sepp Hochreiter and Jürgen Schmidhuber, “Long shortterm memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[43] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi, “Bidirectional attention flow for machine comprehension,” arXiv preprint arXiv:1611.01603, 2016.\n[44] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-based models for speech recognition,” in Advances in Neural Information Processing Systems, 2015, pp. 577–585.\n[45] Huijuan Xu and Kate Saenko, “Ask, attend and answer: Exploring question-guided spatial attention for visual question answering,” arXiv preprint arXiv:1511.05234, 2015.\n[46] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio, “Show, attend and tell: Neural image caption generation with visual attention,” arXiv preprint arXiv:1502.03044, 2015.\n[47] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, “Neural machine translation by jointly learning to align and translate,” arXiv preprint arXiv:1409.0473, 2014.\n[48] Jason Weston, Sumit Chopra, and Antoine Bordes, “Memory networks,” arXiv preprint arXiv:1410.3916, 2014.\n[49] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, “Librispeech: an ASR corpus based on public domain audio books,” in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 5206–5210.\n[50] “dynamic time warping approach,” github, https:// github.com/chunan/libdtw.\n[51] Diederik Kingma and Jimmy Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014."
    } ],
    "references" : [ {
      "title" : "Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams",
      "author" : [ "Yaodong Zhang", "James R Glass" ],
      "venue" : "Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 398–403.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "DeMori, Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, chapter 15, pp. 417–446",
      "author" : [ "Gokhan Tur", "Renato" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Retrieval and browsing of spoken content",
      "author" : [ "C. Chelba", "T.J. Hazen", "M. Saraclar" ],
      "venue" : "Signal Processing Magazine, IEEE, vol. 25, no. 3, pp. 39 –49, may 2008.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Spoken content retrieval: A survey of techniques and technologies",
      "author" : [ "Martha Larson", "Gareth J.F. Jones" ],
      "venue" : "Found. Trends Inf. Retr., vol. 5, pp. 235–422, 2012.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Recent developments in spoken term detection: a survey",
      "author" : [ "Anupam Mandal", "K.R. Prasanna Kumar", "Pabitra Mitra" ],
      "venue" : "International Journal of Speech Technology, pp. 1–16, 2013.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Spoken content retrievalbeyond cascading speech recognition with text retrieval",
      "author" : [ "Lin-shan Lee", "James Glass", "Hung-yi Lee", "Chunan Chan" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 9, pp. 1389–1420, 2015.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The spoken web search task at mediaeval 2011",
      "author" : [ "F. Metze", "N. Rajput", "X. Anguera", "M. Davel", "G. Gravier", "C. van Heerden", "G.V. Mantena", "A. Muscariello", "K. Prahallad", "I. Szoke", "J. Tejedor" ],
      "venue" : "ICASSP, 2012.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The spoken web search task",
      "author" : [ "F. Metze", "E. Barnard", "M. Davel", "C. Van Heerden", "X. Anguera", "G. Gravier", "N. Rajput." ],
      "venue" : "MediaEval 2012 Workshop, 2012.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The spoken web search task",
      "author" : [ "Xavier Anguera", "Florian Metze", "Andi Buzo", "Igor Szoke", "Luis Javier Rodriguez-Fuentes" ],
      "venue" : "MediaEval 2013 Workshop, 2013.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Query by example search on speech at mediaeval 2014",
      "author" : [ "Xavier Anguera", "Luis-Javier Rodriguez Fuentes", "Igor Szke", "Andi Buzo", "Florian Metze" ],
      "venue" : "MediaEval 2014 Workshop, 2014.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Query by example search on speech at mediaeval 2015",
      "author" : [ "Andi Buzo", "Xavier Anguera", "Florian Metze", "Jorge Proenca", "Martin Lojka", "Xiao Xiong" ],
      "venue" : ".",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Language independent search in mediaeval’s spoken web search task",
      "author" : [ "Florian Metze", "Xavier Anguera", "Etienne Barnard", "Marelie Davel", "Guillaume Gravier" ],
      "venue" : " Computer Speech & Language, vol. 28, no. 5, pp. 1066 – 1082, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Discriminative articulatory models for spoken term detection in low-resource conversational settings",
      "author" : [ "Rohit Prabhavalkar", "Karen Livescu", "Eric Fosler-Lussier", "Joseph Keshet" ],
      "venue" : "ICASSP, 2013.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Discriminative spoken term detection with limited data",
      "author" : [ "Rohit Prabhavalkar", "Joseph Keshet", "Karen Livescu", "Eric Fosler-Lussier" ],
      "venue" : "MLSLP, 2012.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Robust discriminative keyword spotting for emotionally colored spontaneous speech using bidirectional LSTM networks",
      "author" : [ "M. Wollmer", "F. Eyben", "J. Keshet", "A. Graves", "B. Schuller", "G. Rigoll" ],
      "venue" : "ICASSP, 2009.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Discriminative keyword spotting",
      "author" : [ "Joseph Keshet", "David Grangier", "Samy Bengio" ],
      "venue" : "Speech Communication, vol. 51, pp. 317 – 329, 2009.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Discriminative keyword spotting",
      "author" : [ "Joseph Keshet", "David Grangier", "Samy Bengio" ],
      "venue" : "NOLISP, 2007.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Endto-end asr-free keyword search from speech",
      "author" : [ "Kartik Audhkhasi", "Andrew Rosenberg", "Abhinav Sethy", "Bhuvana Ramabhadran", "Brian Kingsbury" ],
      "venue" : "arXiv preprint arXiv:1701.04313, 2017.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings",
      "author" : [ "Keith Levin", "Katharine Henry", "Aren Jansen", "Karen Livescu" ],
      "venue" : "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 410– 415.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Segmental acoustic indexing for zero resource keyword search",
      "author" : [ "Keith Levin", "Aren Jansen", "Benjamin Van Durme" ],
      "venue" : "ICASSP, 2015.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep convolutional acoustic word embeddings using word-pair side information",
      "author" : [ "Herman Kamper", "Weiran Wang", "Karen Livescu" ],
      "venue" : "ICASSP, 2016.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder",
      "author" : [ "Yu-An Chung", "Chao-Chung Wu", "Chia-Hao Shen", "Hung- Yi Lee", "Lin-Shan Lee" ],
      "venue" : "arXiv preprint arXiv:1603.00982, 2016.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Enhanced spoken term detection using support vector machines and weighted pseudo examples",
      "author" : [ "Hung-Yi Lee", "Lin-Shan Lee" ],
      "venue" : "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 6, pp. 1272–1284, 2013.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A hybrid HMM/DNN approach to keyword spotting of short words",
      "author" : [ "I.-F. Chen", "C.-H. Lee" ],
      "venue" : "INTER- SPEECH, 2013.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Exploiting discriminative point process models for spoken term detection",
      "author" : [ "A. Norouzian", "A. Jansen", "R. Rose", "S. Thomas" ],
      "venue" : "INTERSPEECH, 2012.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Query-by-example keyword spotting using long shortterm memory networks",
      "author" : [ "Guoguo Chen", "Carolina Parada", "Tara N. Sainath" ],
      "venue" : "ICASSP, 2015.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Query-by-example search with discriminative neural acoustic word embeddings",
      "author" : [ "Shane Settle", "Keith Levin", "Herman Kamper", "Karen Livescu" ],
      "venue" : "arXiv preprint arXiv:1706.03818, 2017.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Memory networks",
      "author" : [ "J. Weston", "S. Chopra", "A. Bordes" ],
      "venue" : "CoRR, vol. abs/1410.3916, 2014.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Weakly supervised memory networks",
      "author" : [ "S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus" ],
      "venue" : "CoRR, vol. abs/1503.08895, 2015.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Ask me anything: Dynamic memory networks for natural language processing",
      "author" : [ "A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher" ],
      "venue" : "CoRR, vol. abs/1506.07285, 2015.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Large-scale simple question answering with memory networks",
      "author" : [ "A. Bordes", "N. Usunier", "S. Chopra", "J. Weston" ],
      "venue" : "CoRR, vol. abs/1506.02075, 2015.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Dynamic memory networks for visual and textual question answering",
      "author" : [ "C. Xiong", "S. Merity", "R. Socher" ],
      "venue" : "CoRR, vol. abs/1603.01417, 2016.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "K.M. Hermann", "T. Kociský", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom" ],
      "venue" : "CoRR, vol. abs/1506.03340, 2015.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Where to look: Focus regions for visual question answering",
      "author" : [ "Kevin J. Shih", "Saurabh Singh", "Derek Hoiem" ],
      "venue" : "CoRR, vol. abs/1511.07394, 2015.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Dynamic coattention networks for question answering",
      "author" : [ "Caiming Xiong", "Victor Zhong", "Richard Socher" ],
      "venue" : "CoRR, vol. abs/1611.01604, 2016.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Bidirectional attention flow for machine comprehension",
      "author" : [ "Min Joon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi" ],
      "venue" : "CoRR, vol. abs/1611.01603, 2016.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "ABC-CNN: An attention based convolutional neural network for visual question answering",
      "author" : [ "Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia" ],
      "venue" : "CVPR, 2016.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering",
      "author" : [ "Huijuan Xu", "Kate Saenko" ],
      "venue" : "CVPR, 2016.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Towards machine learning comprehension of spoken content: Initial toefl listening comprehension test by machine",
      "author" : [ "B.-H. Tseng", "S.-S. Shen", "H.-Y. Lee", "L.-S. Lee" ],
      "venue" : "INTERSPEECH, 2016.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Hierarchical attention model for improved machine comprehension of spoken content",
      "author" : [ "Wei Fang", "Jui-Yang Hsu", "Hung yi Lee", "Lin-Shan Lee" ],
      "venue" : "SLT, 2016.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Long shortterm memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Bidirectional attention flow for machine comprehension",
      "author" : [ "Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi" ],
      "venue" : "arXiv preprint arXiv:1611.01603, 2016.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Attention-based models for speech recognition",
      "author" : [ "Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems, 2015, pp. 577–585.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering",
      "author" : [ "Huijuan Xu", "Kate Saenko" ],
      "venue" : "arXiv preprint arXiv:1511.05234, 2015.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.03044, 2015.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473, 2014.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Memory networks",
      "author" : [ "Jason Weston", "Sumit Chopra", "Antoine Bordes" ],
      "venue" : "arXiv preprint arXiv:1410.3916, 2014.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Librispeech: an ASR corpus based on public domain audio books",
      "author" : [ "Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 5206–5210.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980, 2014.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Dynamic time warping (DTW) [1] is widely used in this case.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "The reader is referred to several tutorial chapters and papers [2–6].",
      "startOffset" : 63,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "The reader is referred to several tutorial chapters and papers [2–6].",
      "startOffset" : 63,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "The reader is referred to several tutorial chapters and papers [2–6].",
      "startOffset" : 63,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "The reader is referred to several tutorial chapters and papers [2–6].",
      "startOffset" : 63,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "The reader is referred to several tutorial chapters and papers [2–6].",
      "startOffset" : 63,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "This direction exactly matches the target of the Spoken Web Search (SWS) task [7–11] 1, a part",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "This direction exactly matches the target of the Spoken Web Search (SWS) task [7–11] 1, a part",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "This direction exactly matches the target of the Spoken Web Search (SWS) task [7–11] 1, a part",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "This direction exactly matches the target of the Spoken Web Search (SWS) task [7–11] 1, a part",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "This direction exactly matches the target of the Spoken Web Search (SWS) task [7–11] 1, a part",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "A complete overview of the approaches developed in SWS in 2011 and 2012 is available [13].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "Along this direction, encouraging results have been obtained based on structured support vector machine (SVM) [14–18], in which the time spans corresponding to the queries in utterances are considered as hidden information.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "Along this direction, encouraging results have been obtained based on structured support vector machine (SVM) [14–18], in which the time spans corresponding to the queries in utterances are considered as hidden information.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 14,
      "context" : "Along this direction, encouraging results have been obtained based on structured support vector machine (SVM) [14–18], in which the time spans corresponding to the queries in utterances are considered as hidden information.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "Along this direction, encouraging results have been obtained based on structured support vector machine (SVM) [14–18], in which the time spans corresponding to the queries in utterances are considered as hidden information.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : "Along this direction, encouraging results have been obtained based on structured support vector machine (SVM) [14–18], in which the time spans corresponding to the queries in utterances are considered as hidden information.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 17,
      "context" : "An end-to-end deep learning based system for text query STD has been proposed [19].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 18,
      "context" : "Query-by-example STD by representing each word segment as a vector [20–23] is much more efficient than the conventional Dynamic Time Warping (DTW) based approaches, because only the similarities between two single vectors are needed, in additional to the significantly better retrieval performance obtained [23].",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 19,
      "context" : "Query-by-example STD by representing each word segment as a vector [20–23] is much more efficient than the conventional Dynamic Time Warping (DTW) based approaches, because only the similarities between two single vectors are needed, in additional to the significantly better retrieval performance obtained [23].",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 20,
      "context" : "Query-by-example STD by representing each word segment as a vector [20–23] is much more efficient than the conventional Dynamic Time Warping (DTW) based approaches, because only the similarities between two single vectors are needed, in additional to the significantly better retrieval performance obtained [23].",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "Query-by-example STD by representing each word segment as a vector [20–23] is much more efficient than the conventional Dynamic Time Warping (DTW) based approaches, because only the similarities between two single vectors are needed, in additional to the significantly better retrieval performance obtained [23].",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "Query-by-example STD by representing each word segment as a vector [20–23] is much more efficient than the conventional Dynamic Time Warping (DTW) based approaches, because only the similarities between two single vectors are needed, in additional to the significantly better retrieval performance obtained [23].",
      "startOffset" : 307,
      "endOffset" : 311
    }, {
      "referenceID" : 19,
      "context" : "Several approaches have been successfully used in STD [21, 24–26], but these approaches were developed primarily in more heuristic ways, rather than deep learning.",
      "startOffset" : 54,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : "Several approaches have been successfully used in STD [21, 24–26], but these approaches were developed primarily in more heuristic ways, rather than deep learning.",
      "startOffset" : 54,
      "endOffset" : 65
    }, {
      "referenceID" : 23,
      "context" : "Several approaches have been successfully used in STD [21, 24–26], but these approaches were developed primarily in more heuristic ways, rather than deep learning.",
      "startOffset" : 54,
      "endOffset" : 65
    }, {
      "referenceID" : 24,
      "context" : "Several approaches have been successfully used in STD [21, 24–26], but these approaches were developed primarily in more heuristic ways, rather than deep learning.",
      "startOffset" : 54,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : "By learning RNN with an audio segment as the input and the corresponding word as the target, the outputs of the hidden layer at the last few time steps can be taken as the representation of the input segment [27].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 26,
      "context" : "Audio segment embedding can also be jointly learned with their corresponding character sequences by multi-view approach [28].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 21,
      "context" : "Sequence-to-sequence Autoencoder is used to represent variable-length audio segments by vectors with fixed dimensionality, which is referred to as Audio Word2Vec [23].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 26,
      "context" : "However, it was shown that neural embeddings learned from pre-segmented audio can be applied for embedding arbitrary segments [28].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 27,
      "context" : "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31–35].",
      "startOffset" : 118,
      "endOffset" : 126
    }, {
      "referenceID" : 28,
      "context" : "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31–35].",
      "startOffset" : 118,
      "endOffset" : 126
    }, {
      "referenceID" : 29,
      "context" : "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31–35].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 29,
      "context" : "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31–35].",
      "startOffset" : 235,
      "endOffset" : 242
    }, {
      "referenceID" : 30,
      "context" : "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31–35].",
      "startOffset" : 235,
      "endOffset" : 242
    }, {
      "referenceID" : 31,
      "context" : "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31–35].",
      "startOffset" : 235,
      "endOffset" : 242
    }, {
      "referenceID" : 32,
      "context" : "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31–35].",
      "startOffset" : 235,
      "endOffset" : 242
    }, {
      "referenceID" : 33,
      "context" : "On the other hand, reasoning systems incorporating memory and attention mechanisms such as the memory network (MemNN) [29, 30] and dynamic memory network (DMN) [31] were shown to be very successful in end-toend question answering (QA) [31–35].",
      "startOffset" : 235,
      "endOffset" : 242
    }, {
      "referenceID" : 34,
      "context" : "Some new attention mechanisms [36, 37] recently proposed are shown to be helpful on reading comprehension dataset where the answer to every question is a segment of text.",
      "startOffset" : 30,
      "endOffset" : 38
    }, {
      "referenceID" : 35,
      "context" : "Some new attention mechanisms [36, 37] recently proposed are shown to be helpful on reading comprehension dataset where the answer to every question is a segment of text.",
      "startOffset" : 30,
      "endOffset" : 38
    }, {
      "referenceID" : 36,
      "context" : "In vidual question answering (VQA), attention-based configurable convolutional neural network (ABC-CNN) can learn question-guided attention [38], and it is shown that multiple hops yielded improved results compared to a single step [39].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 37,
      "context" : "In vidual question answering (VQA), attention-based configurable convolutional neural network (ABC-CNN) can learn question-guided attention [38], and it is shown that multiple hops yielded improved results compared to a single step [39].",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 38,
      "context" : "Attention-based multihop networks were also applied on machine comprehension of spoken content [40], and hierarchical attention model (HAM) [41] further constructed tree-structured sentence representations for sentences from their parsing trees and estimate attention weights on different nodes of the hierarchies.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 39,
      "context" : "Attention-based multihop networks were also applied on machine comprehension of spoken content [40], and hierarchical attention model (HAM) [41] further constructed tree-structured sentence representations for sentences from their parsing trees and estimate attention weights on different nodes of the hierarchies.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 40,
      "context" : "2 (A), a bidirectional long short-term memory (LSTM) network [42] takes one frame from the input spoken query sequentially at a time.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 41,
      "context" : "To ensure a time complexity linear to the length of the input audio segment, we do not use more sophisticated attention models [43]; thus the approach is faster than DTW.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 42,
      "context" : "in [44], there are two ways to normalize the scores: Sharpening: The score list is normalized using the softmax activation function:",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 43,
      "context" : "This has been widely used in many existing neural attention frameworks [45–48], and works well with noisy data.",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 44,
      "context" : "This has been widely used in many existing neural attention frameworks [45–48], and works well with noisy data.",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 45,
      "context" : "This has been widely used in many existing neural attention frameworks [45–48], and works well with noisy data.",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 46,
      "context" : "This has been widely used in many existing neural attention frameworks [45–48], and works well with noisy data.",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 47,
      "context" : "We used the LibriSpeech corpus [49] as the data for the experiments.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 48,
      "context" : "• The networks were trained for 100 epochs using ADAM [51] without momentum, with a fixed learning rate of 0.",
      "startOffset" : 54,
      "endOffset" : 58
    } ],
    "year" : 2017,
    "abstractText" : "Retrieving spoken content with spoken queries, or query-byexample spoken term detection (STD), is attractive because it makes possible the matching of signals directly on the acoustic level without transcribing them into text. Here, we propose an end-to-end query-by-example STD model based on an attention-based multi-hop network, whose input is a spoken query and an audio segment containing several utterances; the output states whether the audio segment includes the query. The model can be trained in either a supervised scenario using labeled data, or in an unsupervised fashion. In the supervised scenario, we find that the attention mechanism and multiple hops improve performance, and that the attention weights indicate the time span of the detected terms. In the unsupervised setting, the model mimics the behavior of the existing query-by-example STD system, yielding performance comparable to the existing system but with a lower search time complexity.",
    "creator" : "LaTeX with hyperref package"
  }
}