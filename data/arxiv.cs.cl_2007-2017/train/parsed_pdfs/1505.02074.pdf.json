{
  "name" : "1505.02074.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Image Question Answering: A Visual Semantic Embedding Model and a New Dataset",
    "authors" : [ "Mengye Ren", "Ryan Kiros", "Richard Zemel" ],
    "emails" : [ "MREN@CS.TORONTO.EDU", "RKIROS@CS.TORONTO.EDU", "ZEMEL@CS.TORONTO.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Combining image understanding and natural language interaction is one of the grand dreams of artificial intelligence. We are interested in the problem of jointly learning image and text through a question-answering task. Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus. Image QA involves an extra layer of interation between human and computers. Here the inputs are combined image and text and the model needs to output an answer that is targeted to the question asked. The model needs to pay attention to details of the image instead of describing it in a vague sense. The problem also combines many com-\nPreliminary work. Under review by the Deep Learning Workshop at the International Conference on Machine Learning (ICML). Do not distribute.\nputer vision subproblems such as image labelling and object detection. Unlike traditional computer vision tasks, it is not obvious what kind of task the model needs to perform unless it understands the question.\nIn this paper we present our contribution to the problem: a generic end-to-end QA model using visual semantic embeddings to connect a CNN and a recurrent neural net (RNN); an automatic question generation algorithm that converts description sentences into questions; and a new QA dataset (COCO-QA) which is generated using the algorithm. We will make the our COCO-QA dataset publicly available upon the release of the paper.\nar X\niv :1\n50 5.\n02 07\n4v 1\n[ cs\n.L G\n] 8\nM ay\n2 01"
    }, {
      "heading" : "2. Problem Formulation",
      "text" : "The inputs of the problem are an image and a question, and the output is an answer. In this work we assume that the answers consist of only a single word, which allows us to treat the problem as a classification problem. This also makes the evaluation of the models easier and more robust."
    }, {
      "heading" : "3. Related Work",
      "text" : "Malinowski & Fritz (2014a) released a dataset with images and question-answer pairs, the DAtaset for QUestion Answering on Real-world images (DAQUAR). All images are from the NYU depth v2 dataset (Silberman et al., 2012), and are taken from indoor scenes. Human segmentations, image depth values, and object labellings are available in the dataset. The QA data has two sets of configurations, which differ by the number of object classes appearing in the questions (37-class and 894-class). There are mainly three types of questions in this dataset: object type, object color, and number of objects. Some questions are easy but many questions are very hard to answer even for humans. Figure 2 shows some examples of easy and hard questions. Since DAQUAR is the only publicly available image-based QA dataset, it is one of our benchmarks to evaluate our models.\nTogether with the release of the DAQUAR dataset, Malinowski & Fritz (2014b) presented an approach which combines semantic parsing and image segmentation. In the natural language part of their work, they used a semantic parser (Liang et al., 2013) to convert sentences into latent logical forms. They obtained multiple segmentations of the image by sampling the uncertainty of the segmentation algorithm. Their model is based on a Bayesian formulation that every logical form and image segmentation has certain probability. They also converted each image segmentation to a bag of predicates. To make their algorithm scalable, they chose to sample from the nearest neighbors in the training set according to the similarity of the predicates.\nThis approach is notable as one of the first attempts at image QA, but it has a number of limitations. First, although they are handling a number of spatial relations, a human-defined possible set of predicates are very datasetspecific. To obtain the predicates, their algorithm also depends on a good image segmentation algorithm and image depth information. Second, before asking any of the questions, their model needs to compute all possible spatial relations in the training images, so even for a small dataset like 1500 images there could be 4 million predicates in the worst case (Malinowski & Fritz, 2014b). Even though the model searches from the nearest neighbors of the test images, it could still be an expensive operation in larger datasets. Lastly the accuracy of their model is not\nvery strong. We will show later that some simple baselines will perform better in terms of plain accuracy."
    }, {
      "heading" : "4. Proposed Methodology",
      "text" : "The methodology presented here is two-fold. On the model side we applied recurrent neural networks and visualsemantic embeddings on this task, and on the dataset side we proposed new ways of synthesizing QA pairs from currently available image description dataset."
    }, {
      "heading" : "4.1. Models",
      "text" : "In recent years, recurrent neural networks (RNNs) have enjoyed some successes in the field of natural language processing (NLP). Long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) is a form of RNN which is easier to train than standard RNNs because of its linear error propagation and multiplicative gatings. There has been increasing interest in using LSTM as encoders and decoders on sentence level. Our model builds directly on top of the LSTM sentence model and is called the “VIS+LSTM” model. It treats the image as one word of the question. We borrowed this idea of treating the image as a word from caption generation work done by Vinyals et al. (2014). The difference with caption generation is that here we only output the answer at the last timestep.\n1. We used the last hidden layer of the Oxford Conv Net (Simonyan & Zisserman, 2014) trained on ImageNet (Russakovsky et al., 2014) as our visual embeddings. The conv-net (CNN) part of our model is kept frozen during training.\n2. We experimented with several different word embedding models: randomly initialized embedding, dataset-specific skip-gram embedding and generalpurpose skip-gram embedding model (Mikolov et al., 2013). The word embeddings can either be frozen or dynamic.\n3. We then treated the image as if it is the first word of the sentence. Similar to (Frome et al., 2013), we used a linear or affine transformation to map 4096 dimension image feature vectors to a 300 or 500 dimensional vector that matches the dimension of the word embeddings.\n4. We can optionally treat the image as the last word of the question as well through a different weight matrix.\n5. We can optionally add a reverse LSTM, which gets the same content but operates in a backward sequential fashion.\n6. The LSTM(s) outputs are fed into a softmax layer at the last timestep to generate answers."
    }, {
      "heading" : "4.2. Question-Answer Generation",
      "text" : "The currently available DAQUAR dataset contains approximately 1500 images, with 7000 images on 37 common object classes, which might be not enough for training large complex models. Another problem with the current dataset is that simply guessing the mode can yield very good accuracy. We aim to create another dataset, to produce a much larger number of QA pairs and a more even distribution of answers. While collecting human generated QA pairs is one possible approach, and another is to generate questions based on image labellings, we instead propose to automatically convert descriptions into QA form.\nAs a starting point we used the Microsoft-COCO dataset (Lin et al., 2014), but the same method can be applied to any other image description dataset, such as Flickr (Hodosh et al., 2013), SBU (Ordonez et al., 2011), or even the internet. Another advantage of using image descriptions is that they are generated by humans in the first place. Hence most objects mentioned in the descriptions are easier to notice than the ones in DAQUAR’s human generated questions,\nand synthetic QAs from ground truth labellings. This allows the model to rely more on common sense and rough image understanding without any logic reasoning. Lastly the conversion process preserves the language variability in the original description and will result in more human-like questions than questions generated from image labellings.\nQuestion generation is still an open-ended topic. We are not trying to solve a linguistic problem but instead aim to create a usable image QA dataset. We adopt a conservative approach to generating questions: because we have the luxury of large publicly-available image description datasets, we can afford to reject many possible questions in an attempt to create high-quality questions."
    }, {
      "heading" : "4.2.1. COMMON STRATEGIES",
      "text" : "1. Compound sentences to simple sentences Here we only consider a simple case, where two sentences are joined together with a conjunctive words. We split the orginial sentences into two independent sentences. For example, “There is a cat and the cat is running.” will be split as “There is a cat.” and “The cat is running.”\n2. Indefinite determiners to definite determiners. Asking questions on a specific instance of the subject requires changing the determiner into definite form “the”. For example, “A boy is playing baseball.” will have “the” instead of “a” in its question form: “What is the boy playing?”.\n3. Wh-movement constraints In English, questions tend to start with interrogative words such as “what”. The algorithm needs to move the verb as well as the “wh-” constituent to the front of the sentence. In this work we consider the following two simple constraints:\n(a) A-over-A principle The A-over-A principle restricts the movement of a wh-word inside an NP (Chomsky, 1973). For example, “I am talking to John and Bill” cannot be transformed into “*Who am I talking to John and” because “Bill” is an noun phrase (NP) that is under another NP “John and Bill”.\n(b) Clauses Our algorithm does not move any wh-word that is contained in a clause constituent. This rule can be refined further in future work."
    }, {
      "heading" : "4.2.2. PRE-PROCESSING",
      "text" : "We used the Stanford parser (Klein & Manning, 2003) to obtain the syntatic structure of the original image description.\n4.2.3. Object QUESTIONS\nFirst, we consider asking about an object using “what”. This involves replacing the actual object with a “what” in the sentence, and then transforming the sentence structure so that the “what” appears in the front of the sentence. The entire algorithm has the following stages:\n1. Split long sentences into simple sentences\n2. Change indefinite determiners to definite determiners.\n3. Traverse the sentence and identify potential answers and replace with “what”. During the traversal of object-type question generation, we currently ignore all the prepositional phrase (PP) consituents.\n4. Perform wh-movement\nFigure 4 illustrates an example with tree diagrams. In order to identify a possible answer word, we used WordNet (Fellbaum, 1998) and NLTK software package (Bird, 2006) to get noun categories.\n4.2.4. Number QUESTIONS\nTo generate “how many” type questions, we follow a similar procedure as the previous algorithm, except for a different way to identify potential answers. This time, we need to extract numbers from original sentences. Splitting compound sentences, changing determiners, and wh-movement parts remain the same.\n4.2.5. Color QUESTIONS\nColor questions are much easier to generate. This only requires locating the color adjective and the noun which the adjective attaches to. Then it simply forms a sentence “What is the color of the object” with the “object” replaced by the actual noun.\n4.2.6. Location QUESTIONS\nThese are similar to generating object questions, except that now the answer traversal will only search within PP contituents that start with the preposition “in”. We also added rules to filter out clothing so that the answers will mostly be locations, scenes, or large objects that contain smaller objects."
    }, {
      "heading" : "4.2.7. POST-PROCESSING",
      "text" : "We will show in our experiment results that mode-guessing actually works unexpectedly well in the DAQUAR dataset. One of our design requirements of the new dataset is to avoid too common answers. To achieve this goal, we applied a heuristic to reject the answers that appear too rare or too often in our generated dataset. First, answers that appear less than a frequency threshold are discarded. In COCO-QA the threshold is around 20 in training set and 10 in test set. Then, all QA pairs are shuffled to mitigate the dependence between neighboring pairs. We formulate the rejection process as a Bernoulli random process. The probability of enrolling the next QA pair (q, a) is:\np(q, a) =\n{ 1 if count(a) ≤ K\nexp ( − count(a)−KK2 ) otherwise\n(1) where count(a) denotes the current number of enrolled QA pairs that have a as the ground truth answer, and K, K2 are some constants with K ≤ K2. In the COCO-QA generation we chose K = 100 and K2 = 200. After this\nQA rejection process we obtain a more uniform distribution across the possible answers. Post-processing reduces the mode composition from 24.80% down to 6.65% in the test set of COCO-QA."
    }, {
      "heading" : "5. Experimental Results",
      "text" : ""
    }, {
      "heading" : "5.1. Datasets",
      "text" : "Table 1 summarizes the difference between DAQUAR and COCO-QA. It should be noted that since we applied the QA pair rejection process, mode-guessing performs very poorly on COCO-QA; however, COCO-QA questions are actually easier to answer from a human point of view. This encourages the model to exploit salient object relations instead of exhaustively search all possible relations.\nTable 2 gives a breakdown of all four types of questions present in the COCO-QA dataset, with train/test split information."
    }, {
      "heading" : "5.2. Model Details",
      "text" : "We trained two versions of the architectures that we found to work the best. The first model is the CNN and LSTM with a weight matrix in the middle; we call this “VIS+LSTM” in our tables and figures. The second model has two image feature input at the start and the end of the sentence with different learned linear transformations, and also has LSTMs going in both the forward and backward direction. Both LSTMs output to the softmax layer at the last timestep. We call the second model “VIS+LSTM-2”."
    }, {
      "heading" : "5.3. Baselines",
      "text" : "To evaluate the effectiveness of our models, we designed a few baselines."
    }, {
      "heading" : "5.3.1. GUESS MODEL",
      "text" : "One very simple baseline is to predict the mode based on question type. For example, if the question contains “how many” then the model will output “two.” In DAQUAR, the modes are “table”, “two”, and “white” and in COCOQA, the modes are “cat”, “two”, “white”, and “room”. This baseline actually works unexpectedly well in the DAQUAR dataset."
    }, {
      "heading" : "5.3.2. BOW MODEL",
      "text" : "To avoid over-interpretation of the results, we designed “blind” models which are given only the questions without the images. One of the simplest blind models is to sum all the word vectors of the question into a bag-of-word (BOW) vector. We then use logistic regression to classify answers with an optional tanh hidden layer."
    }, {
      "heading" : "5.3.3. LSTM MODEL",
      "text" : "Another blind model we experimented with simply inputs the question words into the LSTM alone."
    }, {
      "heading" : "5.3.4. VIS+BOW MODEL",
      "text" : "This is the same as the blind BOW model but replaced the feature layer with bag-of-word sentence features concatenated with image features from the Oxford Net last hidden layer (4096 dimension) after a linear transformation (to 300 or 500 dimensions)."
    }, {
      "heading" : "5.3.5. VIS MODEL",
      "text" : "For each type of question, we train a separate CNN last hidden layer (with all lower layers frozen during training). Note that this model knows the type of question in order to make its performance somewhat comparable to the models that can take into account the words to narrow down the answer space. However, the model does not know anything about the question except the type."
    }, {
      "heading" : "5.4. Performance Metrics",
      "text" : "To evaluate model performance, we used the plain answer accuracy as well as the Wu-Palmer similarity (WUPS) measure (Wu & Palmer, 1994; Malinowski & Fritz, 2014b). The WUPS calculates the similarity between two words based on their longest common subsequence in the taxonomy tree. The similarity function takes in a threshold parameter. If the similarity between two words is less than the threshold then a score of zero will be given to the candidate\nanswer. It reduces to plain accuracy when the threshold equals to 1.0. Following (Malinowski & Fritz, 2014b), we measure all the models in terms of accuracy, WUPS 0.9, and WUPS 0.0.\nTable 3 summarizes the learning results on DAQUAR. Here we compare our results with (Malinowski & Fritz, 2014b). It should be noted that their result is for the entire dataset whereas our result is for the 98.3% of the original dataset with single-word answers."
    }, {
      "heading" : "6. Discussion",
      "text" : "From the above results we show that our model outperforms the baselines and the existing approach in terms of answer accuracy and WUPS. It is interesting to see that the blind model is in fact very strong on the DAQUAR dataset, comparable to the VIS+LSTM model. We speculate that it is likely that the ImageNet images are very different from the indoor scene images which are mostly composed of furniture. Hence the VIS+LSTM cannot really make use of the image features unless the question is asking about the largest object, i.e., differentiating between sofas, beds, and tables. However, the VIS+LSTM model outperforms the blind model by a large margin on the COCO-QA dataset. There are three possible reasons behind. First, the objects in MS-COCO resemble the ones in ImageNet more; second, MS-COCO images have fewer objects whereas the indoor scenes have considerable clutter; and third, COCOQA has more data to train complex models.\nThere are many interesting examples but due to space limitations we can only show a few in Figure 5, 6, 7. Full results are available to view at http://www.cs.toronto. edu/˜mren/imageqa/results. For some of the examples, we specifically tested extra questions (the ones have an “a” in the question ID) to avoid over-interpretation of the questions that the VIS+LSTM accidentally got correct. The parentheses in the figures represent the confidence score given by the softmax layer of the models. “DQ” denotes questions from DAQUAR and “CQ” denotes questions from COCO-QA."
    }, {
      "heading" : "6.1. Model Selection",
      "text" : "We did not find that using different word embedding has a significant impact on the final classification results. The best model for DAQUAR uses a randomly initialized embedding of 300 dimensions whereas the best model trained for COCO-QA uses a problem-specific skip-gram embedding to initialize. We observed that fine-tuning the word embedding and normalizing the CNN hidden image features help achieve better results. The bidirectional LSTM model can further boost the result by a little."
    }, {
      "heading" : "6.2. Object Questions",
      "text" : "As the original CNN was trained for the ImageNet challenge, the VIS+LSTM benefited largely from its single object recognition ability. For some simple pictures in COCO-QA, the VIS+LSTM and VIS+BOW can easily get the correct answer just from the image features. However, the challenging part is to consider spatial relations between multiple objects and to focus on details of the image. Some qualitative results in Figure 5 show that the VIS+LSTM only does a moderately acceptable job on it. Sometimes it\nfails to make a correct decision but outputs the most salient object, while sometimes the blind model can equally guess the most probable objects based on the question alone (e.g., chairs should be around the dining table)."
    }, {
      "heading" : "6.3. Counting",
      "text" : "In DAQUAR, we could not observe any advantage in the counting ability of the VIS+LSTM model compared to other blind baselines. In COCO-QA there is some observable counting ability in very clean images with single object type. The model can sometimes count up to five or six. However, as shown in Figure 6, the ability is fairly weak as it does not count correctly when different object types are present. As the VIS+LSTM model only beats the blind one by 3% on counting, there is a lot of room for improvement in the counting task, and in fact this could be a separate computer vision problem on its own."
    }, {
      "heading" : "6.4. Color",
      "text" : "In COCO-QA there is a significant win for the VIS+LSTM model against the LSTM model on color-type questions. We further discovered that the model is not only able to recognize the dominant color of the image but sometimes associate different colors to different objects, as shown in Figure 7. However, the model is still not very robust and fails on a number of easy examples."
    }, {
      "heading" : "6.5. Limitations and Future Work",
      "text" : "Image question answering is a fairly new research topic, and the approach we present here has a number of limitations. First the model is just an answer classifier. Ideally we would like to permit longer answers which will involve some sophisticated text generation model or structured output. Secondly, our question generation algorithm also assumes that all answers are single-word, and the implementation of the algorithm is heavily dependent on the question type. Furthermore, the algorithm is only applicable to the English language at this time. Lastly, it is hard to interpret why the model outputs a certain answer. By comparing the model to some baselines we can roughly infer whether the model understood the image but humans are prone to overinterpretation of the results. Visual attention is another future direction, which could both improve the results (based on recent successes in image captioning (Xu et al., 2015)) as well as help explain the model output by examining the attention output at every timestep."
    }, {
      "heading" : "7. Conclusion",
      "text" : "In this paper, we consider the image QA problem and present our VIS+LSTM model, which combines CNN and LSTM(s) with visual-semantic embeddings. As the cur-\nrently available dataset is not large enough, we designed an algorithm that helps us collect large scale image QA dataset from image descriptions. Our model shows a reasonable understanding of the question and some coarse image understanding, but it is still very naive in many situations. Our question generation algorithm is extensible to many image description datasets and can be automated without requiring extensive human effort. We hope that the release of the new dataset will encourage more data-driven approaches to this problem in the future."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Nitish Srivastava for the support of Toronto Conv Net, from which we extracted the CNN image features. We would also like to thank Zhi Hao Luo for editing suggestions."
    } ],
    "references" : [ {
      "title" : "NLTK: the natural language toolkit",
      "author" : [ "Bird", "Steven" ],
      "venue" : "ACL",
      "citeRegEx" : "Bird and Steven.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bird and Steven.",
      "year" : 2006
    }, {
      "title" : "Learning a recurrent visual representation for image caption generation",
      "author" : [ "Chen", "Xinlei", "Zitnick", "C. Lawrence" ],
      "venue" : "CoRR, abs/1411.5654,",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Conditions on Transformations",
      "author" : [ "Chomsky", "Noam" ],
      "venue" : null,
      "citeRegEx" : "Chomsky and Noam.,? \\Q1973\\E",
      "shortCiteRegEx" : "Chomsky and Noam.",
      "year" : 1973
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "Donahue", "Jeff", "Hendricks", "Lisa Anne", "Guadarrama", "Sergio", "Rohrbach", "Marcus", "Venugopalan", "Subhashini", "Saenko", "Kate", "Darrell", "Trevor" ],
      "venue" : "CoRR, abs/1411.4389,",
      "citeRegEx" : "Donahue et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2014
    }, {
      "title" : "ed.). WordNet An Electronic Lexical Database",
      "author" : [ "Fellbaum", "Christiane" ],
      "venue" : null,
      "citeRegEx" : "Fellbaum and Christiane,? \\Q1998\\E",
      "shortCiteRegEx" : "Fellbaum and Christiane",
      "year" : 1998
    }, {
      "title" : "Long shortterm memory",
      "author" : [ "Hochreiter", "Sepp", "Schmidhuber", "Jürgen" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Framing image description as a ranking task: Data, models and evaluation",
      "author" : [ "Hodosh", "Micah", "Young", "Peter", "Hockenmaier", "Julia" ],
      "venue" : "metrics. J. Artif. Intell. Res. (JAIR),",
      "citeRegEx" : "Hodosh et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hodosh et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep fragment embeddings for bidirectional image sentence mapping",
      "author" : [ "Karpathy", "Andrej", "Joulin", "Armand", "Fei-Fei", "Li" ],
      "venue" : "CoRR, abs/1406.5679,",
      "citeRegEx" : "Karpathy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2014
    }, {
      "title" : "Unifying visual-semantic embeddings with multimodal neural language models",
      "author" : [ "Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Richard S" ],
      "venue" : "CoRR, abs/1411.2539,",
      "citeRegEx" : "Kiros et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2014
    }, {
      "title" : "Accurate unlexicalized parsing",
      "author" : [ "Klein", "Dan", "Manning", "Christopher D" ],
      "venue" : "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Klein et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2003
    }, {
      "title" : "Phrase-based image captioning",
      "author" : [ "Lebret", "Rémi", "Pinheiro", "Pedro O", "Collobert", "Ronan" ],
      "venue" : "CoRR, abs/1502.03671,",
      "citeRegEx" : "Lebret et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lebret et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning dependency-based compositional semantics",
      "author" : [ "Liang", "Percy", "Jordan", "Michael I", "Klein", "Dan" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Liang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2013
    }, {
      "title" : "Microsoft COCO: common objects in context",
      "author" : [ "Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Dollár", "Piotr", "Zitnick", "C. Lawrence" ],
      "venue" : "In Computer Vision - ECCV 2014 - 13th European Conference,",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards a visual turing challenge",
      "author" : [ "Malinowski", "Mateusz", "Fritz", "Mario" ],
      "venue" : "CoRR, abs/1410.8027,",
      "citeRegEx" : "Malinowski et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Malinowski et al\\.",
      "year" : 2014
    }, {
      "title" : "Explain images with multimodal recurrent neural networks",
      "author" : [ "Mao", "Junhua", "Xu", "Wei", "Yang", "Yi", "Wang", "Jiang", "Yuille", "Alan L" ],
      "venue" : "CoRR, abs/1410.1090,",
      "citeRegEx" : "Mao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey" ],
      "venue" : "CoRR, abs/1301.3781,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Im2text: Describing images using 1 million captioned photographs",
      "author" : [ "Ordonez", "Vicente", "Kulkarni", "Girish", "Berg", "Tamara L" ],
      "venue" : "In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Ordonez et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ordonez et al\\.",
      "year" : 2011
    }, {
      "title" : "Indoor segmentation and support inference from RGBD images",
      "author" : [ "Silberman", "Nathan", "Hoiem", "Derek", "Kohli", "Pushmeet", "Fergus", "Rob" ],
      "venue" : "In Computer Vision - ECCV 2012 - 12th European Conference on Computer",
      "citeRegEx" : "Silberman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Silberman et al\\.",
      "year" : 2012
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Simonyan", "Karen", "Zisserman", "Andrew" ],
      "venue" : "CoRR, abs/1409.1556,",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2014
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru" ],
      "venue" : "CoRR, abs/1411.4555,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2014
    }, {
      "title" : "Verb semantics and lexical selection",
      "author" : [ "Wu", "Zhibiao", "Palmer", "Martha" ],
      "venue" : "Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Wu et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 1994
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua" ],
      "venue" : "CoRR, abs/1502.03044,",
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.",
      "startOffset" : 35,
      "endOffset" : 219
    }, {
      "referenceID" : 8,
      "context" : "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.",
      "startOffset" : 35,
      "endOffset" : 219
    }, {
      "referenceID" : 7,
      "context" : "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.",
      "startOffset" : 35,
      "endOffset" : 219
    }, {
      "referenceID" : 14,
      "context" : "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.",
      "startOffset" : 35,
      "endOffset" : 219
    }, {
      "referenceID" : 3,
      "context" : "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.",
      "startOffset" : 35,
      "endOffset" : 219
    }, {
      "referenceID" : 21,
      "context" : "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.",
      "startOffset" : 35,
      "endOffset" : 219
    }, {
      "referenceID" : 10,
      "context" : "Recently, image caption generation (Vinyals et al., 2014; Kiros et al., 2014; Karpathy et al., 2014; Mao et al., 2014; Donahue et al., 2014; Chen & Zitnick, 2014; Fang et al., 2014; Xu et al., 2015; Lebret et al., 2015) has shown us feasible ways of jointly learning image and text to form higher level representations from models such as convolutional neural networks (CNNs) trained on object recognition and word embeddings trained on large scale text corpus.",
      "startOffset" : 35,
      "endOffset" : 219
    }, {
      "referenceID" : 17,
      "context" : "All images are from the NYU depth v2 dataset (Silberman et al., 2012), and are taken from indoor scenes.",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "In the natural language part of their work, they used a semantic parser (Liang et al., 2013) to convert sentences into latent logical forms.",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : "We borrowed this idea of treating the image as a word from caption generation work done by Vinyals et al. (2014). The difference with caption generation is that here we only output the answer at the last timestep.",
      "startOffset" : 91,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "We experimented with several different word embedding models: randomly initialized embedding, dataset-specific skip-gram embedding and generalpurpose skip-gram embedding model (Mikolov et al., 2013).",
      "startOffset" : 176,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "As a starting point we used the Microsoft-COCO dataset (Lin et al., 2014), but the same method can be applied to any other image description dataset, such as Flickr (Hodosh et al.",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : ", 2014), but the same method can be applied to any other image description dataset, such as Flickr (Hodosh et al., 2013), SBU (Ordonez et al.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 16,
      "context" : ", 2013), SBU (Ordonez et al., 2011), or even the internet.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 21,
      "context" : "Visual attention is another future direction, which could both improve the results (based on recent successes in image captioning (Xu et al., 2015)) as well as help explain the model output by examining the attention output at every timestep.",
      "startOffset" : 130,
      "endOffset" : 147
    } ],
    "year" : 2015,
    "abstractText" : "This work aims to address the problem of imagebased question-answering (QA) with new models and datasets. In our work, we propose to use recurrent neural networks and visual semantic embeddings without intermediate stages such as object detection and image segmentation. Our model performs 1.8 times better than the recently published results on the same dataset. Another main contribution is an automatic question generation algorithm that converts the currently available image description dataset into QA form, resulting in a 10 times bigger dataset with more evenly distributed answers.",
    "creator" : "LaTeX with hyperref package"
  }
}