{
  "name" : "1301.7360.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Hybrid Algorithm to Compute Marginal and Joint Beliefs in Bayesian Networks and Its Complexity",
    "authors" : [ "Mark Bloemeke", "Marco Valtorta" ],
    "emails" : [ "bloemeke@cs.sc.edu", "mgv@cs.sc.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "There exist two general forms of exact algo rithms for updating probabilities in Bayesian Networks. The first approach involves using a structure, usually a clique tree, and performing local message based calculation to extract the be lief in each variable. The second general class of algorithm involves the use of non-serial dynamic programming techniques to extract the belief in some desired group of variables. In this paper we present a hybrid algorithm based on the lat ter approach yet possessing the ability to retrieve the belief in all single variables. The technique is advantageous in that it saves a NP-hard computa tion step over using one algorithm of each type. Furthermore, this technique re-enforces a conjec ture of Jensen and Jensen [JJ94] in that it still requires a single NP-hard step to set up the struc ture on which inference is performed, as we show by confirming Li and D'Ambrosio's [LD94] con jectured NP-hardness of OFP.\n1 Overview\nBayesian Networks(BN) provide a standard way to repre sent a probability distribution on a series of discrete propo sitional variables. By taking advantage of independence information between the variables, BN's can reduce the amount of space necessary to specify the distribution, but\nthey then require special algorithms to recover meaningful distributions. One such algorithm to recover the marginals\nof all the variables is known as the tree of cliques approach [LS88] [Pea88] [Nea90] [Jen96].\nAnother approach to the calculation of a marginal proba bility distribution on a set of target variables, called Sym bolic Probabilistic Inference (SPI) is discussed in [LD94]. It involves solving the Optimal Factoring Problem (OFP defined in Section 4) for the target set of variables whose\ndistribution you are interested in. The solution to the OFP is then used to combine the conditional probability tables that describe the Bayesian Network and extract the desired marginal distribution.\nUnknown, however, was the time complexity of the OFP. In [LD94] it was suggested that the OFP was NP-hard, but this was never shown. In sections 4 to 6 of this paper we will confirm Li and D'Ambrosio's conjecture that the OFP is indeed NP-hard by reduction from the secondary problem of non-serial dynamic programming.\nIn section 7 through 10 a new method, based on Li and D'Ambrosio's, is given that uses an OFP solution to build a data structure (called a factor tree, which is similar to the expression tree of [LD93]) from which not only the target joint belief can be extracted, but also all the marginal be liefs. This is obtained by using a method that is similar in outline to the tree of cliques approach. This similarity extends even to the complexity of the algorithm in such a way as to further confirm Jensen's hypothesis that all algo rithms as efficient as the tree of cliques that recover single marginals must include an NP-hard step.\n2 Symbolic Probabilistic Inference\nAssuming that we have a Bayesian Network with DAG G = (V, E) and conditional probability tables P(v;!II(vi)), where II(vi) are the parents of vi in G, we can, if only very inefficiently, recover the total joint proba bility using the chain rule for Bayesian Networks:\nP (V) = IT P (viiii(vi)) (1) v;EV\nand from this we can use marginalization to retrieve our belief in any subset of variables V' as:\nP (V') = L P (V). (2) V-V'\nThe SPI algorithm is based on direct use of equations 1 and\n2 to retrieve any desired joint. In order to avoid the expo-\nnential size of the resulting tables the fact that multiplica tion distributes over addition is employed to push some of the summations down into the products. This allows some control to be maintained over the size and time complexity of the resulting calculation by allowing variable elimina tion from the joint at the earliest possible time. The true cost of this method in fact hinges upon which ordering of terms is selected for equation 1.\nFigure 1: Simple Example Network.\nFor example consider the network shown in Figure 1. We can calculate the joint probability of the variables A and C directly from equations 1 and 2 using the equation\nP (A,C) = ]:B,D,E P (E/C) * P (D/B, C)* P (C/A) * P (B/A) * P (A).\n(3)\nAssuming that each variable A, B, C, D, E has two states, this will need a table with 25 entries to be calculated that will requires at least 22 + 23 + 24 + 25 multiplications to construct and 28 additions to marginalize onto A and C. Thus using just equations 1 and 2 to get P (A, C) will re quire a total of 92 significant operations.\nHowever, with a slight re-ordering of the terms combined by equation 1 followed by the distribution of the summa tions from 2, we get\nP (A, C) = P (A) * [P(C/A) * EE [P(E/C)* []:B P (B/A) * l:n P (D/B, C)]]] (4)\nwhich requires only 24 multiplications and 12 additions for a total of 36 significant operations.\nSince we can only push the summation of a variable down as far as its earliest occurrence in the combination order ing, the ordering determines the amount of time and space we can save. An appropriate combinatorial optimization approach is defined in [LD94] that treats each conditional probability table as a set of variables and defines a combi nation function for two sets and a cost function based on combination. Then the optimal set combination ordering with respect to cost function minimality can be derived for\nany set of target variables whose joint density is required. From that ordering the calculation of the joint occurs in ac cordance with equations 1 and 2 utilizing the distribution described above.\n3 Non-Serial Dynamic Programming\nNon-Serial Dynamic Programming (NSDP) as defined in [BB72] involves performing a global operation, usually maximization or minimization, over a series of functions defined on a common domain of discrete variables. To solve a NSDP instance one combines the functions, accord ing to the combination operator, and then performs the de sired operation on the resulting much larger function. This process is very expensive; in fact it requires a space equiv alent to the cross-product of the variables in the domain.\nFortunately, we can take advantage of Bellman's principle of optimality to reduce the cost of computing the global operation. Bellman's principle states that once all the func tions involving a single variable have been combined, we can reduce the size of the resulting interim function by per forming the global operation on the interim function. We then carry forth just the values of the variable being re moved that produce the best results relative the global op eration for each combination of the remaining variables in the function.\nFor example, suppose that we have a domain of three vari abies, V = {A, B, C}, each of which can take on two states (e.g. a and -.a) and upon which three functions !1 : A -+ z+, h : A, B -+ z+, and h : B, c -+ z+ are defined. The functions are defined by the tables in Figure 2, and we will assume that we wish to maximize (global operation) the sum (combination operator) of these func tions. In this particular case the functions are called the components and their sum is called the objective function [BB72].\nIf we start by combining fr and h then we would get a\nSo when we combine hffiz with h we get only a table based on two variables, B and C, with only a note about which value of A maximized hffi2 carried over. It is easy to see that for a larger example the order of combination becomes very important. That is why the secondary prob lem of NSDP (2-NSDP), that of computing the combina tion elimination ordering, becomes so important.\nIn fact the process of computing a solution to 2-NSDP such that the minimum table size is assure9 is NP-hard [ ACP87], with the following variant being known to be NP Complete.\nDefinition 1 (2-NSDP(d)) Does there exist a combina tion - elimination ordering for a set of n function F = {!I, ... , fn} defined over a domain of discrete variables V onto the positive integers s.t. no interim table, before application of Bellman 's Principle of Optimality, is formed whose domain contains more than d variables?\n4 Optimal Factoring Problem\nThe optimal factoring problem takes on the same role as 2- NSDP did for NSDP in that it gives us the minimum com bination (multiplication)-elimination(marginalization) or dering for the extraction of a joint marginal on a set of tar get variables, T, from a BN. The machinery of the prob lem is very simple. We start by building a set of sets S = { S1, ... , Sm} , henceforth to be called the factoring, s.t. each set, Si, is a subset of the variables, V, on which the BN is defined.\nThese sets correspond to the variables in the conditional probability tables for the BN. For example the BN in Figure 1 yields the set representation:\nS ={{A} ,{A,B} ,{A,C} ,{B,C,D}, {C,E}}\nThe combination of two sets Si and Sj into a new set S iffii is defined as:\nwith the cost of the combination JLs,ffii being:\nwhere b is the maximum number of states any single vari able in V may take on and fL is zero for any of the original sets. After creation of the new set Siffii the two original sets, Si and Si are removed from Sand Siffii is inserted.\nIn this way the process continues until all the sets have been combined and we are left with just one set equivalent toT.\nDefinition 2 (Optimal Factoring Problem) Given a setS of sets defined over a group of variables V that have no more than b possible states, calculate the combination or dering that for a target set of variables T minimizes the total cost as defined by fL.\nGiven a solution to the OFP we can clearly solve a decision problem version:\nDefinition 3 (OFP(c)) Given a factoring, S, defined over a group of variables V, a value b to serve as the base of the cost function JL, a target set ofvariables T, and a total cost c, does their exist a combination ordering s.t. the cost of deriving T is less than c?\nTheorem 1 (NP-completeness of OFP(c)) OFP(c) is NP complete.\nSince a solution to the general OFP allows the immediate solution of the decision problem OFP(c), proof that OFP(c) is NP-complete shows that the general optimal factoring problem is NP-hard.\n5 Reduction\nWe reduce 2-NSDP(d) to OFP(c) in the following way:\n• Define the variables for OFP(c) as the variables for 2- NSDP(d).\n• For each function fi E F(l ::; i ::; n) create one set Si E S s.t. every variable in the domain of /i is in the set si.\n• Set b, the base of JL, ton.\n• SetT = ¢.\n• Set c = b'l+1\n6 Proof of Theorem 1\nDefinition 4 (Function - Set Correspondence) We say that a function fi corresponds to a set Si iff the variables in the domain of fi are equivalent to the members of the set Si.\nDefinition 5 (Function Set- Factoring Equivalence) We say that a jUnction set F is equivalent to a factoring S iff for all Si E S there exists one and only one corresponding function fi E F and there are no unmatched functions in F.\nLemma 1 (Combination Set-Function Equivalence) Let function set F be equivalent to factoring S. If we combine two sets Si E17 Sj in factoring S to get the new fac toring S' while combining their corresponding functions fi El7 fj in F to get a new function set F' then F' and S' are function set - factoring equivalent.\nIn order to prove Lemma 1 we simply observe that all the sets in S are in a one to one correspondence with domains of all the functions in F. Then if we combine any two sets in S and combine their corresponding functions in F, be fore elimination, they are defined on the same variables. That is Si u Sj is defined on the same variables as the do main of !i El7 h. Furthermore, a variable will be eliminated from Si E!7 Sj if and only if it is also eliminated from fi E!7 h. Since Bell man's principle of optimality only allows variable elimina tion if the variable exists only in fi E!7 h, the combination of fi with fi removes the same variables as the removal portion of the set combination rule for the factoring when T is empty.\n0\nLemma 2 (Number of Combinations) In either represen tation there will be exactly n - 1 combinations in the solu tion of the problem.\nClearly since each combination replaces two sets (func tions) with just one there can be no more than n - 1 com binations, where n is the number of sets (functions), until there is only one set (function) left.\n0\nLemma 3 (Elimination Equivalence) For any sequence of factorings 8°, 81, ... , sn-1 formed during the solu tion of the OFP and their equivalent, with respect to which sets (functions) are combined, sequence of function sets F0, F1, ... , pn-1 formed during a solution to 2-NSDP, the size of the interim tables formed at each combination is equivalent to the exponent in the cost function ofOFP for that combination.\nNote that at the start of the problem we have function set - factoring equivalence, and only corresponding functions and sets are combined in the transition from pi to pi+1 and Si to gi+l for 0 � i � n - 2. Then, by combination function set- factoring equivalence pi+ 1 is function set factoring equivalent to si+l after reduction. Furthermore,\nEfficient Hybrid Belief Computation 19\nthe number of variables in the set Si u Sj is equivalent to the number of variables in the domain of fi El7 h, before the respective reductions. Thus the dimension of the interim functions is equivalent to the exponent of the cost function.\n0\nLemma 4 (Exhaustive Combination Ordering Equiva lence) The possible combination orderings for solving the OFP are in one to one correspondence with the possible combination orderings for solving 2-NSDP.\nThis follows from the observation that all possible combi nation sequences for the set of functions have an equivalent factoring elimination ordering and since all elimination se quences for factorings have an equivalent set of function elimination ordering.\n0\nFrom Lemma 4 the dimensions of the resulting functions are equivalent to the exponents of the cost functions for any given elimination ordering. Now, note that, if an order ing of sets exists such that the exponent of the cost func tion d1, ... , dn-1 is always below d, then the correspond ing OFP cost is:\nn-1 L bd' � (n- 1) * nd � nd+1 - nd < nd+1 i=1\nIn other words OFP(c) answers yes only if there exists a solution for 2-NSDP(d).\nConversely if every possible combination ordering for 2- NSDP(d) involves at least one interim table with a dimen sion of at least d + 1, then by exhaustive combination order ing equivalence every possible cost for OFP( c) must exceed nd+l (i.e. at least one term in the summation is greater than or equal to nd+l ). Thus if there exists no yes solution for 2-NSDP( d) then there can exist no yes solution for OFP(c ).\nThis concludes the proof of the reduction portion of Theo rem 1. All that remains to establish is that the problem is NP-complete is to show that it is in NP. This is an obvious result since we can check to see if a solution requires fewer that c multiplications in non-deterministic linear time.\nWe note that the base of the cost function can be reduced to an arbitrary integer k � 2 by simply replacing each vari able in the set of sets with flogk n l copies of itself (i.e. A becomes A1, . . . , Apogk nl)· Since all these variables will exist in the same sets, they will be eliminated at the same time as the variable in the original set representation would be. Thus we can view the cost at any time for a combina tion as kflog; nl*di which is the same as (kflogk nl)d; that for the sake of the above proof is equivalent to nd•. It fol lows, by setting k = 2, that OFP (c) is NP-complete even when restricted to instances for which every variable has only two possible values (i.e. b = 2).\n20 Bloemeke and Valtorta\n7 Factor Trees\nBuilding upon SPI [LD94] we now present a two stage method for deriving not only the desired joint but also all the single beliefs. The first stage corresponds to the Op timal Factor calculation phase of the Li and D'Ambrosio algorithm and results in the creation of a calculation struc ture called a factor tree. The second phase involves running a two-stage message passing algorithm on the factor tree to retrieve not only the joint but also all the single beliefs.\nThe following algorithm constructs a factor tree in four phases.\n1. Start by calculating the optimal factoring order for the network given the target set of variables whose joint is desired.\n2. From this ordering construct a binary tree showing the combination ordering of the initial probability tables and the conformal tables. A conformal table is de fined as any table formed by the combination of two probability tables or the combination of a conformal table with a probability table.\n3. Label edges between tables along which variables are marginalized with the variables(s) marginalized be fore the combination.\n4. Add an additional head that has an empty label above the current root, a conformal table labeled with the target set of variables, that has no variables. The edge between the old root and the new is then labeled with the variables in the old root.\nUtilizing the above algorithm on the graph shown in Figure 1 factored according to the order seen in equation 4, a factor tree is built that looks like the one in Figure 4.\n8 Propagation Phase\nOnce the labeled factor tree described in section 2 is con structed, the algorithm takes on a propagation framework similar to Pearl's method [Pea88] for singly connected net works. We begin at the leaf nodes and propagate up the edges along the direction marked. Messages are tables that are combined using pointwise multiplication [Jen96, Sec tion 4.1].\nOnce the top of the factor tree is reached we send a new message down the edges in the reverse direction. For the sake of notational similarity we will call the messages that travel up the graph A messages and those that travel down the graph 1r messages. This similarity in naming does not strictly correspond to a similarity in purpose, as we shall soon see.\nI T\nC(A,C)\nPA .� C(A,B , C)\nP(�B,C) C ( , C)\nP(� l�\n4. Combine the rr message with the A message sent by the right child.\n5. Send that as the rr messages to the left child.\nThe following is the procedure performed by a labeled edge whenever a message is sent along it.\nLabeled Edge\nA message- Store the lambda message in the edge.\nrr message-Combine the rr message with the stored A message; then marginalize the result onto the variable for which the edge is labeled, obtaining the probability distribution for that variable. In the case of the edge entering into the root it will contain the desired joint.\nIn the case where variables have been instantiated, marginalization simply passes through the values from the interim table that correspond to the instantiation. In this case P(¢) will be zero whenever an impossible combina tion of instantiated variables is given, otherwise it will be the joint marginal probability of the instantiated variables, which is customarily called the probability of the evidence [Jen96, Section 4.2].\n9 Correctness\nWithout loss of generality we will prove that the belief in one variable v contained at the edge labeled with v is valid. This edge connects Vi to Vj and we start our proof by re moving it from the graph. We then add a new node labeled v' in its place. Two new edges are then added: one from vi to v' and the other from Vj to v'. We then re-orient all other edges in the graph so that v' becomes the root of a new factor tree. Above this node we place a new P(v) node, and we add an edge from v' to the new node P(v) labeled with all the variables contained in v' except v. Clearly this is a legal factor tree and represents a legal combination or dering with respect to equations 1 and 2 with respect to distribution.\nFor example consider the task of retrieving P (B) from the factor tree in Figure 4. Using the above method we modifY the tree so that we arrive at the tree shown in Figure 5 which does indeed correspond to the following legal combination ordering\nP(B) = L:A,c [[P(BIA) * L:D P(DIB, C)]* [[P(¢) * P(A)] * [P(CjA) * L:E P(EIC)]]].\n(5)\nIn general, consider any labeled edge in the original graph, G, and apply a similar transformation to it, obtaining a new graph G'. Clearly, the rr message sent down the edge in the original graph, G, is equivalent to one of the A message\nEfficient Hybrid Belief Computation 21\nsent to the node n' in the new graph, G', while the other A message received by v' is the same in both graphs. Thus the edge labeled with v has access to the same messages in the original graph that the node v' has access to in the new graph. Therefore the labeled edge in G can compute the same legal belief in v that G' calculates in the node v'. In other words the two messages combined in the labeled edge in the original graph are in actuality the two A mes sages it would receive in the modified graph, and the belief calculated at the labeled edge is the same as that computed by a factor tree built for the variable in the label.\n10 Time Complexity\nDefine:\nn- the number of variables in the network.\nb - the number of states of the largest variable in the network.\nk - the number of variables in the largest table in the factor tree.\nmultiplications:\n1. Each internal node (of which there are n-1) combines 3 tables using no more than bk multiplications.\n(a) Combines left and right child's A messages into its A message.\n(b) Combines its left child's A message with its par ent's rr message.\n(c) Combines its right child's A message with its par ent's rr message.\n2. Each labeled edge (of which there are n) combines a A message with a rr message using no more that bk multiplications.\nadditions:\n1. Each labeled edge marginalizes twice.\n(a) Once whenever a A message passes it using no more than bk additions.\n(b) Once to remove the final distribution from its stored combination of A and 7l\" messages using no more than bk additions.\n2. Each internal table where a rr message is received may need to marginalize the message onto its local label using no more than bk additions.\nThis means that the factor tree method to recover the prob ability of all the variables requires at most 4nbk multiplica tions and at most 3nbk additions giving the algorithm a to tal complexity of at most 7nbk significant operations. This\n22 Bloemeke and Valtorta\n$ v'(A,B,C)\nA P(BJA) P(DJB, C) C(A,C) P(!.\\IC)\nFigure 5: The factor tree for extracting P(B).\ntime complexity is comparable with the complexity of the tree of cliques approach which runs in at most 5mb1 where m is the number of cliques, b is the same, and l is the num ber of variables in the largest clique [Nea90]. In fact, since the merging of variables into cliques reduces a linear fac tor, n, at the possible expense of an exponential factor, bk, it seems likely that graphs exist for which this algorithm is more efficient (although none have yet been found).\nTwo further improvements can be made to this approach. First, in the case where one wishes to calculate the joint and single beliefs multiple times, one can merge all sub-trees that don't contain a labeled edges into a single node. This merged node then takes the place of the conformal node that was at the root of the merged sub-tree in the factor tree and will save one the amount of calculation that was necessary to build the conformal node that the merged node replaces.\nSecond, it is interesting to note that the optimal factoring problem can be run with no target set of variables. In this case the set reached just before finishing, or the node just below the root of the factor tree, will contain the most effi ciently calculable probability, joint or single, for the net work. This fact can be easily established by contradic tion: since summing away has no cost for OFP, the joint or marginal immediately beneath the root must be the most efficiently calculable or else a new factoring yielding the more efficiently calculable distribution would yield an OFP solution of less cost.\nThis would violate the definition of OFP and therefore it is certain that the solution with zero factor set is the most effi-\ncient calculation possible. This leads to the interesting ob servation that the algorithm is calculating the beliefs in all of the single variables in multiplicative constant (approxi mately 4) time with respect to the most efficient calculation possible for a given network.\n1 1 Conclusions\nWe proved that OFP is NP-hard, confirming Li and D'Ambrosio's [LD94] conjecture. We extended SPI to compute all single-variable marginal beliefs as well as an arbitrary joint belief. The new algorithm contains one NP-hard step, namely the solution of an instance of OFP, thereby reinforcing Jensen and Jensen's [JJ94] conjecture that any scheme for belief updating has an NP-hard opti mality step or is less efficient than the junction tree scheme. Three situations are possible:\n1. In some cases, the junction tree method is more effi cient than the factor tree method described in this pa per, and in some cases the factor tree method is more efficient;\n2. one method strictly dominates the other;\n3. the two methods are of the same complexity.\nDetermining which of the three conditions hold is a prob lem for further work. It seems clear, however, that the new algorithm allows for more efficient use of Bayesian networks in systems that require both the joint probabil ity table for some set of variables as well as for all single\nvariables. This is true, because the new approach saves an NP-hard step over using an algorithm from both classes (junction tree and SPI) simultaneously, which is the only other way to derive a target joint as well as the belief in all the single variables without an unpredictable increase in computational cost as required by the variable propagation approach [Jen96, p. 99].\n12 Acknowledgments\nThis work was partially supported by the Office of Naval Research project \"Dynamic Decision Support for Com mand, Control, and Communication in the Context of Tac tical Defense\" (BA97-006). An earlier version of the factor tree method appeared in [Blo98]. Thanks also to Bruce D'Ambrosio for his review and suggestions on an early draft of this paper.\nReferences\n[ACP87] Stefan Arnborg, Derek G. Corneil, and Andrzej Proskurowski. Complexity of finding embed dings in a k - tree. SIAM Journal of Algorithms and Discrete Methods, 8(2):277-284, 1987.\n[BB72] Umberto Bertele and Francesco Brioschi. Nons e rial Dynamic Programming, volume 91 of Math ematics in Science and Engineering. Academic Press, New York and London, 1972.\n[Blo98] Mark Bloemeke. An algorithm for the recov ery of both target joint beliefs and full belief from bayesian networks. In Proceedings 36th Annual Southeast ACM Conference, New York, NY, April 1998. The Association for Computing Machinery, Inc.\n[Jen96] Finn V. Jensen. An Introduction to Bayesian Net works. Springer, 1996.\n[JJ94] Finn V. Jensen and Frank Jensen. Optimal junc tion trees. In Uncertainty in Artificial Intelli gance: Proceedings of the Tenth Conference, pages 360--366, San Mateo, CA, July 1994. Mor gan Kaufman.\n[LD93] Zhaoyu Li and Bruce D'Ambrosio. An efficient approach for finding the MPE in belief networks. In Uncertainty in Artificial Intelligence: Pro ceedings of the Ninth Conference, pages 342- 349, San Mateo, CA, July 1993. Morgan Kauf man.\n[LD94) Zhaoyu Li and Bruce D'Ambrosio. Efficient In ference in Bayes Networks as a Combinatorial Optimization Problem. International Journal of Approximate Reasoning, 11:55--81, 1994.\nEfficient Hybrid Belief Computation 23\n[LS88) S.L. Lauritzen and D.J. Spiegelhalter. Local computation with probabilites in graphical struc tures and their applications to expert systems. Journal of the Royal Statistical Society B, 50(2), 1988.\n[Nea90] Richard E. Neapolitan. Probabilistic Reasoning in Expert Systems: Theory and Algorithms. John Wiley and Sons, New York, NY, 1990.\n[Pea88] Judea Pearl. Probabilistic Reasoning in Intelli gent Systems: Networks of Plausible Inference. Morgan Kaufinan, San Mateo, CA, 1988."
    } ],
    "references" : [ {
      "title" : "Complexity of finding embed­ dings in a k - tree",
      "author" : [ "Stefan Arnborg", "Derek G. Corneil", "Andrzej Proskurowski" ],
      "venue" : "SIAM Journal of Algorithms and Discrete Methods,",
      "citeRegEx" : "Arnborg et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Arnborg et al\\.",
      "year" : 1987
    }, {
      "title" : "Nons e­ rial Dynamic Programming, volume 91 of Math­ ematics in Science and Engineering",
      "author" : [ "Umberto Bertele", "Francesco Brioschi" ],
      "venue" : null,
      "citeRegEx" : "Bertele and Brioschi.,? \\Q1972\\E",
      "shortCiteRegEx" : "Bertele and Brioschi.",
      "year" : 1972
    }, {
      "title" : "An algorithm for the recov­ ery of both target joint beliefs and full belief from bayesian networks",
      "author" : [ "Mark Bloemeke" ],
      "venue" : "In Proceedings 36th Annual Southeast ACM Conference,",
      "citeRegEx" : "Bloemeke.,? \\Q1998\\E",
      "shortCiteRegEx" : "Bloemeke.",
      "year" : 1998
    }, {
      "title" : "An Introduction to Bayesian Net­",
      "author" : [ "Finn V. Jensen" ],
      "venue" : null,
      "citeRegEx" : "Jensen.,? \\Q1996\\E",
      "shortCiteRegEx" : "Jensen.",
      "year" : 1996
    }, {
      "title" : "Optimal junc­ tion trees",
      "author" : [ "Finn V. Jensen", "Frank Jensen" ],
      "venue" : "In Uncertainty in Artificial Intelli­ gance: Proceedings of the Tenth Conference,",
      "citeRegEx" : "Jensen and Jensen.,? \\Q1994\\E",
      "shortCiteRegEx" : "Jensen and Jensen.",
      "year" : 1994
    }, {
      "title" : "An efficient approach for finding the MPE in belief networks",
      "author" : [ "Zhaoyu Li", "Bruce D'Ambrosio" ],
      "venue" : "In Uncertainty in Artificial Intelligence: Pro­ ceedings of the Ninth Conference,",
      "citeRegEx" : "Li and D.Ambrosio.,? \\Q1993\\E",
      "shortCiteRegEx" : "Li and D.Ambrosio.",
      "year" : 1993
    }, {
      "title" : "Efficient In­ ference in Bayes Networks as a Combinatorial Optimization Problem",
      "author" : [ "Zhaoyu Li", "Bruce D'Ambrosio" ],
      "venue" : "International Journal of Approximate Reasoning,",
      "citeRegEx" : "Li and D.Ambrosio.,? \\Q1994\\E",
      "shortCiteRegEx" : "Li and D.Ambrosio.",
      "year" : 1994
    }, {
      "title" : "Local computation with probabilites in graphical struc­ tures and their applications to expert systems",
      "author" : [ "S.L. Lauritzen", "D.J. Spiegelhalter" ],
      "venue" : "Journal of the Royal Statistical Society B,",
      "citeRegEx" : "Lauritzen and Spiegelhalter.,? \\Q1988\\E",
      "shortCiteRegEx" : "Lauritzen and Spiegelhalter.",
      "year" : 1988
    }, {
      "title" : "Probabilistic Reasoning in Expert Systems: Theory and Algorithms",
      "author" : [ "Richard E. Neapolitan" ],
      "venue" : null,
      "citeRegEx" : "Neapolitan.,? \\Q1990\\E",
      "shortCiteRegEx" : "Neapolitan.",
      "year" : 1990
    }, {
      "title" : "Probabilistic Reasoning in Intelli­ gent Systems: Networks of Plausible Inference",
      "author" : [ "Judea Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl.,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl.",
      "year" : 1988
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "There exist two general forms of exact algo­ rithms for updating probabilities in Bayesian Networks. The first approach involves using a structure, usually a clique tree, and performing local message based calculation to extract the be­ lief in each variable. The second general class of algorithm involves the use of non-serial dynamic programming techniques to extract the belief in some desired group of variables. In this paper we present a hybrid algorithm based on the lat­ ter approach yet possessing the ability to retrieve the belief in all single variables. The technique is advantageous in that it saves a NP-hard computa­ tion step over using one algorithm of each type. Furthermore, this technique re-enforces a conjec­ ture of Jensen and Jensen [JJ94] in that it still requires a single NP-hard step to set up the struc­ ture on which inference is performed, as we show by confirming Li and D'Ambrosio's [LD94] con­ jectured NP-hardness of OFP.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}