{
  "name" : "1702.01208.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Theoretical Analysis of First Heuristics of Crowdsourced Entity Resolution",
    "authors" : [ "Arya Mazumdar", "Barna Saha" ],
    "emails" : [ "arya@cs.umass.edu", "barna@cs.umass.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Entity resolution (ER, record linkage, deduplication, etc.) seeks to identify which records in a data set refer to the same underlying real-world entity (Fellegi and Sunter 1969; Elmagarmid, Ipeirotis, and Verykios 2007; Getoor and Machanavajjhala 2012; Larsen and Rubin 2001; Christen 2012). Our ability to represent information about real-world entities in very diverse ways makes this a complicated problem. For example, collecting profiles of people and businesses, or specifications of products and services from websites and social media sites can result in billions of records that need to be resolved. These entities are identified in a wide variety of ways, complicated further by language ambiguity, poor data entry, missing values, changing attributes and formatting issues. ER is a fundamental task in data processing with wide-array of applications. There is a huge literature on ER techniques; many include machine learning algorithms, such as decision trees, SVMs, ensembles of classifiers, conditional random fields, unsupervised learning etc. (see (Getoor and Machanavajjhala 2012) for a recent survey). Yet, ER remains a demanding task for any automated strategy yielding low accuracy.\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nER can be cast as a clustering problem. Consider a set of n elements V that must be clustered into k disjoint parts Vi, i = 1, 2, . . . , k. The true underlying clusters Vi ∈ [n], i ∈ [1, k] are unknown to us, and so is k. Each of these Vis represents an entity. Each element v ∈ V has a set of attributes. A similarity function is used to estimate the similarity of the attribute sets of two nodes u and v. If u and v represent the same entity, then an ideal similarity function will return 1, and if they are different, then it will return 0. However, in practice, it is impossible to find an ideal similarity function, or even a function close to it. Often, some attribute values may be missing or incorrect, and that leads to similarity values that are noisy representation of the ideal similarity function. Any automated process that uses such similarity function is thus prone to make errors. To overcome this difficulty, a relatively recent line of works propose to use human knowledge via crowdsourcing to boost accuracy of ER (Davidson et al. 2014; Firmani, Saha, and Srivastava 2016; Verroios and GarciaMolina 2015; Gruenheid et al. 2015; Wang et al. 2012; Wang et al. 2013; Vesdapunt, Bellare, and Dalvi 2014; Yi et al. 2012; Whang, Lofgren, and Garcia-Molina 2013). Human based on domain knowledge can match and distinguish entities with complex representations, where automated strategies fail. Motivating example. Consider the following illustrative example shown in Figure 1. The Walt Disney, commonly known as Disney, is an American multinational media and entertainment company that owns and licenses 14 theme parks around the world. 1 Given the six places (r1) Disney World, (r2) Walt Disney World Resort, (r3) Walt Disney Theme Park, Orlando, (r4) Disneyland, (r5) Disneyland Park, humans can determine using domain knowledge that these correspond to two entities: r1, r2, r3 refer to one entity, and r4, r5 refer to a second entity.\nAnswering queries by crowd could be time-consuming and costly. Therefore, a crowd based ER strategy must attempt to minimize the number of queries to the oracle while resolving the clusters exactly. Having access to ideal crowd answers, a good ordering of comparing record pairs is (r1, r2), (r2, r3), (r4, r5), (r1, r5). After the first three pairs have been compared, we can safely infer as “matching” the remaining pair\n1 https://en.wikipedia.org/wiki/The_Walt_ Disney_Company\nar X\niv :1\n70 2.\n01 20\n8v 1\n[ cs\n.D B\n] 3\nF eb\n2 01\n7\n(r1, r3) leveraging transitive relations. After the last pair in the ordering has been compared, we can safely infer as “nonmatching” all the remaining pairs (r1, r4), (r2, r4), (r2, r5), (r3, r4), (r3, r5) in the database.\nThe work by Wang et al. (Wang et al. 2013) was among the first few (Wang et al. 2012; Demartini, Difallah, and CudréMauroux 2012; Whang, Lofgren, and Garcia-Molina 2013) to propose the notion of hybrid human-machine approach for entity resolution. Moreover, it is the first paper to leverage the transitive relationship among the entities to minimize the number of queries which has since become a staple in every follow-up work on this topic (Firmani, Saha, and Srivastava 2016; Verroios and Garcia-Molina 2015; Gruenheid et al. 2015; Vesdapunt, Bellare, and Dalvi 2014). Assuming there is an oracle, an abstraction of a crowd-sourcing platform that can correctly answer questions of the form “Do records u and v refer to the same entity?”, they presented a new algorithm for crowd-sourced ER. To minimize the number of queries to the crowd oracle, Wang et al. utilizes the transitive relation in which known match and non-match labels on some record pairs can be used to automatically infer match or non-match labels on other record pairs. In short, the heuristic algorithm by Wang et al. does the following: it orders the tuples (record pairs/edges) in nonincreasing order of similarity, and query any edge according to that order whenever the right value of that edge cannot be transitively deduced from the already queried/inferred edges so far.\nWhile the crowd-sourcing algorithm of Wang et al. works reasonably well on real datasets, theoretical guarantees for it was not provided. However, in (Vesdapunt, Bellare, and Dalvi 2014), Vesdapunt et al. showed that in some instances this algorithm can only give an Θ(n) approximation, that is when an optimum algorithm may require c queries, Wang et al.’s algorithm can require Θ(cn) queries.\nVesdapunt et al. proposed an algorithm that proceeds in the following iterative manner. In each round, an element to be clustered is compared with one representative of all the existing clusters. The order of these comparisons is defined by a descending order of the similarity measures. As soon as a positive query result is found the element is assigned to the corresponding cluster and the algorithm moves to the next round with a new element. It is easy to see that in the worst case the number of queries made by the algorithm is\nnk, where n is the number of elements and k is the number of clusters. It also follows that this is at least an O(k) approximation.\nNote that (Wang et al. 2013; Vesdapunt, Bellare, and Dalvi 2014) consider the answers of queries are correct as an ideal crowd abstraction - this can often be guaranteed via majority voting. But it is unclear that how the quality of the similarity measurements affects the total number of queries. Indeed, in typical datasets, the performances of the algorithms of Wang et al. and Vesdapunt et al. are quite similar, and they are much better than their worst case guarantees that do not take into account the existence of any meaningful similarity measures. This means the presence of the similarity measures helps reduce the query complexity significantly. Is there a way to theoretically establish that and come up with guarantees that match the experimental observations?\nIt is of paramount interest to characterize the query complexity (number of questions asked to the crowd) of these popular heuristics and come up with algorithms that minimize such complexity. The query complexity is directly proportional to the overall cost of a crowd-based algorithm, due to the fact that crowd questions are time-consuming and in many times involve compensations. Designing a strategy that would minimize the query complexity can directly be seen as alternatives to active learning problem with minimum labeling requirements (Sarawagi and Bhamidipaty 2002; Bellare et al. 2012). From the perspective of lower bounding the query complexity, ER can be seen as a reinforcement learning problem. Indeed, in each step of assigning a record to one of the underlying entities, a query must be made wisely so that under any adversarial configurations, the total number of queries remain small. Contributions. In this paper we assume the following model for the similarity measurements. LetW = {wu,v}(u,v)∈V×V denote the matrix obtained by pair-wise similarity computation, where wu,v is a random variable drawn from a probability distribution fg if u and v belong to the same cluster and drawn from a probability distribution fr otherwise. The subscripts of fr and fg are chosen to respectively signify a “red edge” (or absence of a link) and a “green edge” (or presence of a link). Note that, this model of similarity matrix is by no means the only possible; however it captures the essential flavor of the problem.\nOur main contribution in this paper is to provide a theoretical analysis of query complexities of the two aforementioned heuristics from (Wang et al. 2013; Vesdapunt, Bellare, and Dalvi 2014). Our analysis quantifies the effect of the presence of similarity measures in these algorithms, establishes the superiority between these algorithms under different criteria, and derives the exact expression of query complexity under some fundamental probability models.\nNext, to establish the near-optimality or sub-optimality of the above heuristics, we compare our results with an information theoretic lower bound recently proposed by us (Mazumdar and Saha 2016). As a corollary to the results of (Mazumdar and Saha 2016), it can be seen that the information theoretic lower bound depends on the Hellinger divergence between fg and fr. More interestingly, the quality of the similarity matrix can be characterized by the Hellinger divergence between fg and fr as well.\nFinally, we show that the experimental observations of (Wang et al. 2013; Vesdapunt, Bellare, and Dalvi 2014) agree with our theoretical analysis of their algorithms. Moreover, we conduct a thorough experiment on the bibliographical cora (McCallum 2004) dataset for ER and several synthetic datasets to validate the theoretical findings further."
    }, {
      "heading" : "2 System model and techniques",
      "text" : "2.1 Crowdsourced Entity Resolution Crowd-ER Consider a set of elements V ≡ [n] which is a disjoint union of k clusters Vi, i = 1, . . . , k, where k and the subsets Vi ⊆ [n] are unknown. The crowd (or the oracle) is presented with an element-pair (u, v) ∈ V × V for a query, that results in a binary answer denoting the event u, v belonging to the same cluster. Note that, this perfect oracle model had been used in the prominent previous works by Wang et al. and Vesdapunt et al.(2013; 2014).\nWe can assume that with probability 0 < pi < 1, the crowd gives a wrong answer to the ith query. However, with resampling the ith query Ω(log n) times, that is by asking the same ith query to Ω(log n) different users and by taking the majority vote, we can drive the probability pi to nearly 0 and return to the model of perfect oracle. Note that we have assumed independence among the resampled queries over the index j, which can be justified since we are sampling a growing (Ω(log n)) number of samples. Furthermore, repetition of the same query to the crowd may not not lead to reduction in the error probability, i.e., a persistent error. Even in this scenario an element can be queried with multiple elements from a same cluster to infer with certainty whether the element belong to the cluster or not. These situations have been covered in detail in our recent work (Mazumdar and Saha 2016). Henceforth, in this paper, we only consider the perfect oracle model. All our results hold for the faulty oracle model described above with only an O(log n) blow-up in the query complexity.\nConsider W , an n× n similarity matrix, with the (u, v)th entry wu,v a nonnegative random variable in [0, 1] drawn from a probability density or mass function fg when u, v belong to the same cluster, and drawn from a probability density or mass function fr otherwise. fg and fr are unknown.\nThe problem of Crowd-ER is to design a set of queries in V × V , given V and W , such that from the answers to the queries, it is possible to recover Vi, i = 1, 2, ..., k."
    }, {
      "heading" : "2.2 The two heuristic algorithms",
      "text" : "The Edge ordering algorithm (Wang et al. 2013). In this algorithm, we arrange the set V × V in non-increasing order of similarity values wi,js. We then query sequentially according to this order. Whenever possible we apply transitive relation to infer edges. For example, if the queries (i, j) and (j, l) both get positive answers then there must be an edge (i, l), and we do not have to make the query (i, l). We stop when all the edges are either queried, or inferred.\nThe Node ordering algorithm (Vesdapunt, Bellare, and Dalvi 2014). In this algorithm, the empirical expected size of the cluster containing element i, 1 ≤ i ≤ n, is first computed as ∑ j wi,j . Then all the elements are ordered nonincreasingly according to the empirical expected sizes of the clusters containing them. At any point in the execution, the algorithm maintains at most k clusters. The algorithm selects the next element and issues queries involving that element and elements which are already clustered in non-increasing order of their similarity, and apply transitivity for inference. Therefore, the algorithm issues at most one query involving the current node and an existing cluster. Trivially, this gives an O(k)-approximation.\n3 Information theoretic lower bound Note that, in the absence of similarity matrix W , any optimal (possibly randomized) algorithm must make Ω(nk) queries to solve Crowd-ER. This is true because an input can always be generated that makes Ω(n) vertices to be involved in Ω(k) queries before they can be correctly assigned. However, when we are allowed to use the similarity matrix, this bound can be significantly reduced. Indeed, the following lower bound follows as a corollary of the results of our previous work (Mazumdar and Saha 2016).\nTheorem 1. Given the number of clusters k and fg, fr, any randomized algorithm that does not perform at least Ω ( min { k2 H2(fg,fr) , nk })\nqueries, will be unable to return the correct clustering with high probability, where H2(fg, fr) ≡ 12 ∫∞ −∞( √ fg(x)− √ fr(x))\n2dx is the squared Hellinger divergence between the probability measures fg and fr.\nThe main idea of proving this lower bound already appears in our recent work (Mazumdar and Saha 2016), and we give a brief sketch of the proof below for the interested readers. Strikingly, Hellinger divergence between fg and fr appears to be the right distinguishing measure even for analyzing the heuristic algorithms.\nTo show the lower bound we consider an input where one of the clusters are fully formed and given to us. The remaining k − 1 clusters each has size a = ⌊ 1\n8H2(fg,fr)\n⌋ . We prove\nthe result through contradiction. Assume there exists a randomized algorithm ALG that makes a total of o ( k2\nH(fg,fr)2\n)\nqueries and assigns all the remaining vertices to correct clusters with high probability. However, that implies that the average number of queries ALG makes to assign each of the remaining elements to a cluster must be o(k).\nSince there are k clusters, this actually guarantees the existence of an element that is not queried with the correct cluster Vi it is from, and that completely relies on the W matrix for the correct assignment. Now the probability distribution (which is a product measure) of W , PW , can be one of two different distributions, P ′W and P ′′ W depending on whether this vertex belong to Vi or not. Therefore these two distributions must be far apart in terms of total variation distance for correct assignment.\nHowever, the total variation distance between P ′W and P ′′ W\n‖P ′W−P ′′W ‖TV ≤ √\n2H(P ′W , P ′′W ). But as bothP ′W , P ′′W are product measures that can differ in at most 2a random variables (recall the clusters are all of size a), we must have, using the properties of the Hellinger divergence, H(P ′W , P ′′W ) ≤√\n2aH(fg, fr)2 ≤ 12 . This means, ‖P ′ W − P ′′W ‖TV ≤ 1√2 , i.e., the two distributions are close enough to be confused with a positive probability - which leads to a contradiction. Note that, in stead of recovery with positive probability, if we want to ensure exact recovery of the clusters (i.e., with probability 1) we must query each element at least once. This leads to the following corollary. Corollary 1. Any (possibly randomized) algorithm with the knowledge of fg, fr, and the number of clusters k, must\nperform at least Ω ( n+ k 2\nH2(fg,fr)\n) queries,H(fg, fr) > 0,\nto return the correct clustering exactly."
    }, {
      "heading" : "4 Main results: Analysis of the heuristics",
      "text" : "We provide expressions for query complexities for both the edge ordering and the node ordering algorithms. It turns out that the following quantity plays a crucial role in the analysis of both:\nLg,r(t) ≡ ∫ 1 0 (∫ r 0 fg(y)dy )t fr(x)dx.\nTheorem 2 (The Edge ordering). The query complexity for Crowd-ER with the edge ordering algorithm is at most,\nn+ min 1≤s≤n [(k 2 ) s2 + n k∑ i=1 |Vi|∑ `=s `Lg,r ((` 2 ))] .\nThe proof of this theorem is provided in Section 5. Theorem 3 (The Node ordering). The query complexity for Crowd-ER with the node ordering algorithm is at most,\nn+ k∑ i=1 |Vi|∑ s=1 min{k, (n− |Vi|)Lg,r(s)}.\nThe proof of this theorem is provided in Section 6."
    }, {
      "heading" : "4.1 Illustration: -biased Uniform Noise Model",
      "text" : "We consider two distributions for fr and fg which are only far in terms of total variation distance from the uniform\ndistribution. However, if we consider Hellinger distance, then Dist-1 is closer to uniform distribution than Dist-2. These two distributions will be used as representative distributions to illustrate the potentials of the edge ordering and node ordering algorithms. In both cases, substituting with 0, we get uniform distribution which contains no information regarding the similarities of the entries.\nDist-1. Consider the following probability density functions for fr and fg , where x ∈ [0, 1], and 0 < < 1/2,\nfr(x) = { (1 + ) if x < 12 (1− ) if x ≥ 12 fg(x) = { (1− ) if x < 12 (1 + ) if x ≥ 12 .\nNote that ∫ 1 0 fr(x) dx = ∫ 1/2 0 (1 + ) dx+ ∫ 1 1/2\n(1− ) dx = 1. Similarly, ∫ 1 0 fg(x) dx = 1, that is they represent valid probability density functions. We have, H2(fg, fr) = 1 −∫ 1 0 √ 1− 2dx = 1− √ 1− 2 ≈ 2/2. Dist-2. Now consider the following probability density functions for fr and fg with 0 < < 1/2.\nfr(x) = 1\n1− , 0 ≤ x ≤ 1− , fg(x) =\n1\n1− , ≤ x ≤ 1.\nAgain, ∫ 1 0 fr(x) dx = ∫ 1− 0\n1 (1− ) dx = 1. Similarly,∫ 1\n0 fg(x) dx =\n∫ 1 fg(x) dx = 1, that is they represent\nvalid probability density functions. We have,H2(fg, fr) = 1−\n∫ 1−\n1 1− dx = 1− ≈ .\nWe have the following results for these two distributions. Proposition 1 (Lower bound). Any (possibly randomized) algorithm for Crowd-ER, must make Ω(n + k 2\n2 ) queries for Dist-1 and Ω(n+ k 2\n) queries for Dist-2, to recover the clusters exactly (with probability 1).\nThe proof of this theorem follows from Theorem 1, Corollary 1, and by plugging in the Hellinger distances between fg, fr in both cases.\nThe following set of results are corollaries of Theorem 2. Proposition 2 (Uniform noise (no similarity information)). Under the uniform noise model where fg, fr ∼ Unif [0, 1], the edge ordering algorithm has query complexity O(nk log nk ) for Crowd-ER.\nProof. Since fg = fr, the similarity matrix W amounts to no information at all. We know that in this situation, one must make O(nk) queries for the correct solution of Crowd-ER.\nIn this situation, a straight-forward calculation shows that, Lg,r(t) = 1t+1 . This means, ignoring the first n term, from Theorem 2, the edge ordering algorithm makes at most min1≤s≤n [( k 2 ) s2 + n ∑k i=1 ∑|Vi| `=s ` 2 `(`−1)+2 ] ≤\nmin1≤s≤n\n[ k2s2 2 + 2n ∑k i=1 ∑|Vi| `=s 1 `−1 ] number of queries.\nBy bounding the harmonic series and using the concavity of log, we have the number of queries made by the edge ordering algorithm is at most min1≤s≤n [ k2s2 2 + 2n ∑k i=1 ln |Vi|−1 s−2 ] ≤ min1≤s≤n [ k2s2 2 +2nk ln n−k k(s−2) ] = O(nk log nk ),where\nwe have substituted s = √ n/k.\nProposition 3 (Dist-1). When fg, fr ∼ Dist-1, the edge ordering algorithm has query complexity O(nk(1− 2 ) log nk ) for Crowd-ER.\nProof. The proof is identical to the above. For small , we have Lg,r(t) ≈ 1− (1+ )(t+1) ≈ 1−2 t+1 (see, Section 8). The algorithm queries at most O(nk(1− 2 ) log nk ) edges.\nProposition 4 (Dist-2). When fg, fr ∼ Dist-2, the edge ordering algorithm has query complexity O ( n+ k 2 logn ) for\nCrowd-ER.\nProof. For this case, we have Lg,r(t) ≤ e − (t+1)\nt+1 (see, Section 8). Choose s = √ 4 logn + 1. Then using Theorem\n2, n ∑k i=1 ∑|Vi| `=s `Lg,r (( ` 2 )) ≤ n ∑k i=1 ∑|Vi| `=s 2e − (`2) `−1 < 1.\nTherefore, the number of queries is O ( n+ k 2 logn ) , match-\ning the lower bound within a log n factor.\nFor the Node-ordering algorithm, we have the following result as a corollary of Theorem 3. Proposition 5 (Node-Ordering). When fg, fr ∼Dist-1, the node ordering algorithm has query complexityO(nk(1− 2)) for Crowd-ER. When fg, fr ∼ Dist-2, node ordering has query complexity O ( n+ k 2 logn ) for Crowd-ER.\nProof. For Dist-1, Lg,r(s) ≈ 1−2 s+1 . Therefore, when s ≥ n k (1 − ), min {k, (n− |Vi|)Lg,r(s)} ≤ (1 − )k. Thus, the total number of queries is O(nk(1 − ) + nk(1 − )) = O(nk(1 − 2)). For Dist-2, Lg,r(s) = exp(− s)s+1 . Therefore, when s ≥ 2 logn , min {k, (n− |Vi|)Lg,r(s)} ≤\n1 n(s+1) . Thus the total number of expected queries is O(n+∑k i=1 k logn + |Vi| log |Vi| n ) = O(n+ k2 logn ).\nNote that, there is no difference in the upper bounds given between the Edge and Node ordering algorithms for Dist-2. But Edge-ordering uses order log(n/k) factor more queries than the optimal (O(nk)) for Dist-1. Dist-1 is closer to uniform distribution by the Hellinger measure than Dist-2, which shows that Hellinger distance is the right choice for distance here. Assuming k = o(n), we get a drastic reduction in query complexity by moving from Dist-1 to Dist-2."
    }, {
      "heading" : "5 Analysis of the Edge ordering algorithm: proof of Theorem 2",
      "text" : "Let R be a random variable with distribution fr and G1, . . . , Gt be identical random variables with distribution fg . Let R,G1, G2, . . . , Gt be all independent. Note that,\nPr(R ≥ max{G1, . . . , Gt})\n= ∫ 1 0 (∫ r 0 fg(y)dy )t fr(x)dx = Lg,r(t). (1)\nIn the interest of clarity, let us call a pair (u, v) ∈ V ×V a green edge iff u, v ∈ Vi for some i = 1, . . . , k, and otherwise call the pair a red edge.\nIn the current graph, let there exist ` nodes, called U ⊂ V , which all belong to the same cluster but no edge from the induced graph on these ` vertices have been queried yet. Then there are ( ` 2 ) green edges within U , yet to be queried. On the other hand, there are at most n` red edges with one end point incident on the vertices in U . We now count the number of red edges incident on U that the algorithm will query before querying a green edge within U . We can account for all the red edges queried by the algorithm by considering each cluster at a time, and summing over the queried red edges incident on it. In fact, by doing this, we double count every red edge. Since the probability of querying a red edge incident on U before querying any of the ( ` 2 ) green edges\nincident on U is Lg,r( ( ` 2 ) , the expected number of queried red edges incident on U before querying a green edge in U is at most n`Lg,r( ( ` 2 ) ).\nLet s be a positive integer. Consider a cluster Vi : |Vi| ≥ s. Suppose at some point of time, there are ` components of Vi remaining to be connected. Then, again there are at least( ` 2 ) green edges, querying any of which will decrease the number of components by 1. Thus, the expected number of red edges that are queried incident on nodes in Vi before there remain at most s components of Vi is at most n ∑|Vi| `=s `Lg,r( ( ` 2 ) ). Therefore, the expected number of red edges that are queried until only s components are left for every cluster is n ∑k i=1 ∑|Vi| `=s `Lg,r (( ` 2 )) .\nNow the number of red edges across the clusters having size at most s is at most ( k 2 ) s2. Therefore, even if we query all those edges, we get the total number of queried red edges to be at most ( k 2 ) s2 + n ∑k i=1 ∑|Vi| `=s `Lg,r (( ` 2 )) .\nThe algorithm queries a total of n− k green edges, exactly spanning every cluster. Thus the total number of queries is at most n+ ( k 2 ) s2 + n ∑k i=1 ∑|Vi| `=s `Lg,r (( ` 2 )) ."
    }, {
      "heading" : "6 Analysis of the Node ordering algorithm: proof of Theorem 3",
      "text" : "The computed expected cluster size for each node can be a highly biased estimator, and may not provide any useful information. For example, the expected cluster size of a node in Vi is c |Vi| + ( 1 2 − 2c )n where c = 2 for Dist 1 and c = 1 for Dist 2. Therefore, the node ordering considered by (Vesdapunt, Bellare, and Dalvi 2014) can be arbitrary. Hence, for the purpose of our analysis, we ignore this ordering based on the expected size.\nConsider the state of the algorithm where it needs to insert a node v which truly belongs to cluster Vi. Suppose the current size of Vi is s, that is Vi already contains s nodes when v is considered. Consider another cluster Vj , j 6= i, and let its current size be s′. Let Ci and Cj denote the current subclusters of Vi and Vj that have been formed.Then, P (wv,u ≥ maxx∈Vi wv,x) where u ∈ Cj is at most Lg,r(s). Hence, P (∃u ∈ Cj , wv,u ≥ maxx∈Vi wv,x) ≤ min {1, s′Lg,r(s)}. Thus the expected number of queried red edges before v is correctly inserted in Vi is at most min {k, Lg,r(s) ∑ j∈[1,k],j 6=i |Vj |} ≤\nSimilarity  \nFr ac\n,o n   of   E dg\nes   \n(a) similarity value distribution (b) #queries vs recall\nFigure 2: cora\nmin{k, (n − |Vi|)Lg,r(s)}. Hence the expected total number of queried red edges to grow the ith cluster is at most∑|Vi| s=1 min{k, (n−|Vi|)Lg,r(s)}, and thus the expected total number of queries, including green and red edges is bounded by n+ ∑k i=1 ∑|Vi| s=1 min{k, (n− |Vi|)Lg,r(s)}."
    }, {
      "heading" : "7 Experimental Observations",
      "text" : "A detailed comparison of the node ordering and edge ordering methods on multiple real datasets has been shown in (Vesdapunt, Bellare, and Dalvi 2014, Figures 12,14). The number of queries issued by the two methods are very close on complete resolution.To validate further, we did the following experiments. Datasets. (i) We created multiple synthetic datasets each containing 1200 nodes and 14 clusters with the following size distribution: two clusters of size 200, four clusters of size 100, eight clusters of size 50, two clusters each of size 30 and 20 and the rest of the clusters of size 10. The datasets differed in the way similarity values are generated by varying and sampling the values either from Dist-1 or Dist-2. The similarity values are further discretized to take values from the set {0, 0.1, 0.2, ..., 0.9, 1}.\n(ii) We used the widely used cora (McCallum 2004) dataset for ER. cora is a bibliography dataset, where each record contains title, author, venue, date, and pages attributes. There are 1878 nodes in total with 191 clusters, among which 124 are non-singletons. The largest cluster size is 236, and the total number of pairs is 17, 64, 381. We used the similarity function as in (Whang, Lofgren, and Garcia-Molina 2013; Wang et al. 2013; Vesdapunt, Bellare, and Dalvi 2014; Firmani, Saha, and Srivastava 2016). Observation. The number of queries for the node-ordering and edge-ordering algorithms are reported in Table 1 for the synthetic datasets. Clearly, the number of queries asked for Dist-2 is significantly less than that for Dist-1 at the same value of . This confirms with our theoretical findings. Interestingly, we observe that the number of queries asked by the edge-ordering algorithm is consistently higher than the node-ordering algorithm under Dist-1. This is also expected from Propositions 3 and 5 due to a gap of log nk in the number of queries of the two algorithms. In a similar vein, we see the edge-ordering algorithm is more effective than the node-\nNode-Ordering Edge-Ordering Distribution 4475 4460 Dist-1 = 12 5207 6003 Dist-1 = 13 5883 7145 Dist-1 = 14 6121 7231 Dist-1 = 15 6879 8545 Dist-1 = 110 7398 9296 Dist-1 = 120 1506 1277 Dist-2 = 15 1986 1296 Dist-2 = 110 2760 1626 Dist-2 = 120\nTable 1: Number of Queries for Dist-1 and Dist-2\nordering for Dist-2, possibly because of hidden constants in the asymptotic analysis.\nFigure 2(a) shows the similarity value distribution for cora which is closer to Dist-2 than Dist-1. Figure 2(b) shows the recall vs number of queries issued by the two methods. The line marked with ‘+’ sign is the curve for the ideal algorithm that will ask only the required “green” edges first to grow all the clusters and then ask just one “red” edge across every pair of clusters. Upon completion, the number of queries issued by the edge ordering and node ordering methods are respectively 21,099 and 23,243 which are very close to optimal. Interestingly, this confirms with our observation on the However, they achieve above 0.996 recall in less than 4, 000 queries. This can also be explained by our analysis. The remaining large number of queries are mainly spent on growing small clusters, e.g. when cluster sizes are o(log n)– they do not give much benefit on recall, but consume many queries.\n8 Appendix: Lg,r(t) for Dist-1, Dist-2 Proposition 6. For fg, fr ∼ Dist-1 and small , we have Lg,r(t) ≈ (1− )(1+ )(t+1) .\nProof. We have,\nLg,r(t) = ∫ 1 r=0 (∫ r x=0 fG(x) dx )t fR(r) dr\n= ∫ 1/2 r=0 (∫ r x=0 fG(x) dx )t (1 + ) dr\n+ ∫ 1 r=1/2 (∫ r x=0 fG(x) dx )t (1− ) dr\n= ∫ 1/2 r=0 (∫ r x=0 (1− ) dx )t (1 + ) dr + ∫ 1 r=1/2(∫ 1/2\nx=0 (1− ) dx+ ∫ r x=1/2 (1 + ) dx )t (1− ) dr\n= (1 + )(1− )t\n2t+1(t+ 1) + (1− ) ∫ 1 r=1/2 (r(1 + )− )t dr\nSet z = r(1 + )− , then dz = (1 + )dr. We have (1− ) ∫ 1 r=1/2 (r(1 + )− )t dr = (1− ) (1 + ) ∫ 1 z=\n(1− ) 2\nzt dz\n= (1− )\n(1 + )(t+ 1)\n( 1− (1− ) t+1\n2t+1\n) .\nTherefore, Lg,r(t)\n= (1 + )(1− )t\n2t+1(t+ 1) + (1− ) (1 + )(t+ 1)\n( 1− (1− ) t+1\n2t+1 ) =\n(1− ) (1 + )(t+ 1)\n( 1 + ( 1−\n2\n)t−1) .\nProposition 7. For fg, fr ∼ Dist-2 we have Lg,r(t) ≤ e− (t+1)\nt+1 .\nProof. We have,\nLg,r(t) = ∫ 1− r= (∫ r x= fG(x) dx )t 1 1− dr\n= ∫ 1− r= (∫ r x= 1 1− dx )t 1 1− dr\n= 1\n(1− )t+1 ∫ 1− r= (r − )t dr = 1 t+ 1 ( 1− 2 1− )t+1 = 1\nt+ 1\n( 1−\n1−\n)t+1 ≤ (1− ) t+1\nt+ 1 ≤ e\n− (t+1)\nt+ 1 .\nAcknowledgements: This research is supported in part by NSF CCF Awards 1464310, 1642658, 1642550 and a Google Research Award. The authors would like to thank Sainyam Galhotra for his many help with the simulation results."
    } ],
    "references" : [ {
      "title" : "A",
      "author" : [ "K. Bellare", "S. Iyengar", "Parameswaran" ],
      "venue" : "G.; and Rastogi, V.",
      "citeRegEx" : "Bellare et al. 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "S",
      "author" : [ "Davidson" ],
      "venue" : "B.; Khanna, S.; Milo, T.; and Roy, S.",
      "citeRegEx" : "Davidson et al. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "D",
      "author" : [ "Demartini, G.", "Difallah" ],
      "venue" : "E.; and Cudré-Mauroux, P.",
      "citeRegEx" : "Demartini. Difallah. and Cudré.Mauroux 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "V",
      "author" : [ "A.K. Elmagarmid", "P.G. Ipeirotis", "Verykios" ],
      "venue" : "S.",
      "citeRegEx" : "Elmagarmid. Ipeirotis. and Verykios 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A",
      "author" : [ "I.P. Fellegi", "Sunter" ],
      "venue" : "B.",
      "citeRegEx" : "Fellegi and Sunter 1969",
      "shortCiteRegEx" : null,
      "year" : 1969
    }, {
      "title" : "Online entity resolution using an oracle",
      "author" : [ "Saha Firmani", "D. Srivastava 2016] Firmani", "B. Saha", "D. Srivastava" ],
      "venue" : null,
      "citeRegEx" : "Firmani et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Firmani et al\\.",
      "year" : 2016
    }, {
      "title" : "and Machanavajjhala",
      "author" : [ "L. Getoor" ],
      "venue" : "A.",
      "citeRegEx" : "Getoor and Machanavajjhala 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Fault-tolerant entity resolution with the crowd",
      "author" : [ "Gruenheid" ],
      "venue" : "CoRR abs/1512.00537",
      "citeRegEx" : "Gruenheid,? \\Q2015\\E",
      "shortCiteRegEx" : "Gruenheid",
      "year" : 2015
    }, {
      "title" : "D",
      "author" : [ "M.D. Larsen", "Rubin" ],
      "venue" : "B.",
      "citeRegEx" : "Larsen and Rubin 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "and Saha",
      "author" : [ "A. Mazumdar" ],
      "venue" : "B.",
      "citeRegEx" : "Mazumdar and Saha 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "and Bhamidipaty",
      "author" : [ "S. Sarawagi" ],
      "venue" : "A.",
      "citeRegEx" : "Sarawagi and Bhamidipaty 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "and GarciaMolina",
      "author" : [ "V. Verroios" ],
      "venue" : "H.",
      "citeRegEx" : "Verroios and Garcia.Molina 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Crowdsourcing algorithms for entity resolution",
      "author" : [ "Bellare Vesdapunt", "N. Dalvi 2014] Vesdapunt", "K. Bellare", "N. Dalvi" ],
      "venue" : null,
      "citeRegEx" : "Vesdapunt et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Vesdapunt et al\\.",
      "year" : 2014
    }, {
      "title" : "M",
      "author" : [ "J. Wang", "T. Kraska", "Franklin" ],
      "venue" : "J.; and Feng, J.",
      "citeRegEx" : "Wang et al. 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "M",
      "author" : [ "J. Wang", "G. Li", "T. Kraska", "Franklin" ],
      "venue" : "J.; and Feng, J.",
      "citeRegEx" : "Wang et al. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "S",
      "author" : [ "Whang" ],
      "venue" : "E.; Lofgren, P.; and Garcia-Molina, H.",
      "citeRegEx" : "Whang. Lofgren. and Garcia.Molina 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "T",
      "author" : [ "J. Yi", "R. Jin", "A.K. Jain", "S. Jain", "Yang" ],
      "venue" : "2012. Semi-crowdsourced clustering: Generalizing crowd labeling by robust distance metric learning. In NIPS",
      "citeRegEx" : "Yi et al. 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Entity resolution (ER) is the task of identifying all records in a database that refer to the same underlying entity, and are therefore duplicates of each other. Due to inherent ambiguity of data representation and poor data quality, ER is a challenging task for any automated process. As a remedy, human-powered ER via crowdsourcing has become popular in recent years. Using crowd to answer queries is costly and time consuming. Furthermore, crowd-answers can often be faulty. Therefore, crowd-based ER methods aim to minimize human participation without sacrificing the quality and use a computer generated similarity matrix actively. While, some of these methods perform well in practice, no theoretical analysis exists for them, and further their worst case performances do not reflect the experimental findings. This creates a disparity in the understanding of the popular heuristics for this problem. In this paper, we make the first attempt to close this gap. We provide a thorough analysis of the prominent heuristic algorithms for crowd-based ER. We justify experimental observations with our analysis and information theoretic lower bounds.",
    "creator" : "LaTeX with hyperref package"
  }
}