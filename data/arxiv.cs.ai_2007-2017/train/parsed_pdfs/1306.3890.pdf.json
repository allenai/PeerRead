{
  "name" : "1306.3890.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Big Data and the SP Theory of Intelligence",
    "authors" : [ "J Gerard Wolff" ],
    "emails" : [ "jgw@cognitionresearch.org." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Artificial intelligence, big data, cognitive science, computational efficiency, data compression, data-centric computing, energy efficiency, pattern recognition, uncertainty, unsupervised learning.\nI. INTRODUCTION Big data—the large volumes of data that are now produced in many fields—can present problems in storage, transmission, and processing, but their analysis may yield useful information and useful insights.\nThis article is about how the SP theory of intelligence and its realisation in the SP machine (Section II) may, with advantage, be applied to big data. Naturally, in an area like that, problems will not be solved in one step. The ideas described in this article provide a foundation and framework for further research (Section XII).\nProblems associated with big data are reviewed quite fully in Frontiers in Massive Data Analysis [1] from the US National Research Council, and there is another useful perspective, from IBM, in Smart Machines: IBM’s Watson and the Era of Cognitive Computing [2]. These and other sources are referenced at appropriate points throughout the article.\nIn broad terms, the potential benefits of the SP system, as applied to big data, are in these areas:\n• Overcoming the problem of variety in big data. Harmonising diverse kinds of knowledge, diverse formats for\nGerry Wolff is founder and director of CognitionResearch.org, Menai Bridge, UK. e-mail: jgw@cognitionresearch.org.\nManuscript received [Month] [day], [year]; revised [Month] [day], [year].\nknowledge, and their diverse modes of processing, via a universal framework for the representation and processing of knowledge. • Learning and discovery. The unsupervised learning or discovery of ‘natural’ structures in data. • Interpretation of data. The SP system has strengths in areas such as pattern recognition, information retrieval, parsing and production of natural language, translation from one representation to another, several kinds of reasoning, planning and problem solving. • Velocity: analysis of streaming data. The SP system lends itself to an incremental style, assimilating information as it is received, much as people do. • Volume: making big data smaller. Reducing the size of big data via lossless compression can yield direct benefits in the storage, management, and transmission of data, and indirect benefits in several of the other areas discussed in this article. • Additional economies in the transmission of data. There is potential for additional economies in the transmission of data, potentially very substantial, by judicious separation of ‘encoding’ and ‘grammar’. • Energy, speed, and bulk. There is potential for big cuts in the use of energy in computing, for greater speed of processing with a given computational resource, and for corresponding reductions in the size and weight of computers. • Veracity: managing errors and uncertainties in data. The SP system can identify possible errors or uncertainties in data, suggest possible corrections or interpolations, and calculate associated probabilities. • Visualisation. Knowledge structures created by the system, and inferential processes in the system, are all transparent and open to inspection. They lend themselves to display with static and moving images.\nThese topics will be discussed, each in its own section, below. But first, the SP theory and the SP machine will be introduced.\nII. INTRODUCTION TO THE SP THEORY AND SP MACHINE\nThe SP theory, which has been under development for several years, aims to simplify and integrate concepts across artificial intelligence, mainstream computing and human perception and cognition, with information compression as a unifying theme.\nThe theory is conceived as an abstract brain-like system that, in an ‘input’ perspective, may receive New information via its senses, and compress some or all of it to create Old information, as illustrated schematically in Fig. 1. In the theory, information compression is the mechanism both for\nar X\niv :1\n30 6.\n38 90\nv4 [\ncs .A\nI] 3\n1 M\nar 2\n01 4\nthe learning and organisation of knowledge and for pattern recognition, reasoning, problem solving, and more.\nIn the SP system, all kinds of knowledge are represented with patterns: arrays of atomic symbols in one or two dimensions.\nAt the heart of the system are processes for compressing information by finding good full and partial matches between patterns and merging or ‘unifying’ parts that are the same. More specifically, all processing is done via the creation of multiple alignments, like the one shown in Fig. 2.1\nThe close association between information compression and concepts of prediction and probability [6] means that the SP system is intrinsically probabilistic. Each SP pattern has an associated frequency of occurrence, and for each multiple alignment, the system may calculate associated probabilities [4, Section 3.7] (reproduced in [3, Section 4.4]). Although the SP system is fundamentally probabilistic, it can, if required, be constrained to operate in the clockwork style of a conventional computer, delivering all-or-nothing results [4, Chapter 10].\nAn important idea in the SP programme is the DONSVIC principle [3, Section 5.2]: the conjecture, supported by evidence, that information compression, properly applied, is the key to the discovery of ‘natural’ structures, meaning the kinds of things that people naturally recognise, such as words, objects, and classes of objects. Evidence to date suggests that the SP system does indeed conform to that principle.\nThe SP theory is realised in a computer model, SP70, which may be regarded as a first version of the SP machine. It is envisaged that the SP computer model will provide the basis for the development of a high-parallel, open-source version of the SP machine, as described in Section XII.\nThe theory has things to say about several aspects of computing and cognition, including unsupervised learning, concepts of computing, aspects of mathematics and logic, the representation of knowledge, natural language processing, pattern recognition, several kinds of reasoning, information\n1The concept of multiple alignment in the SP system [3, Section 4], [4, Section 3.4] is borrowed from that concept in bioinformatics, but with important differences.\nstorage and retrieval, planning and problem solving, and aspects of neuroscience and of human perception and cognition.\nThere is a relatively full account of the SP system in [4], an extended overview in [3], an account of its existing and expected benefits and applications in [5], a description of its foundations in [7], and an introduction to the system in [8]. More information may be found via www.cognitionresearch.org/sp.htm."
    }, {
      "heading" : "III. OVERCOMING THE PROBLEM OF VARIETY IN BIG DATA",
      "text" : "“The manipulation and integration of heterogeneous data from different sources into a meaningful common representation is a major challenge.” [1, p. 76]. “Over the past decade or so, computer scientists and mathematicians have become quite proficient at handling specific types of data by using specialized tools that do one thing very well. ... But that approach doesn’t work for complex operational challenges such as managing cities, global supply chains, or power grids, where many interdependencies exist and many different kinds of data have to be taken into consideration.” [2, p. 48].\nThe many different kinds of data include: the world’s many languages, spoken or written; static and moving images; music as sound and music in its written form; numbers and mathematical notations; tables; charts; graphs; networks; trees; grammars; computer programs; and more. With many of these kinds of data, there are several different computer-based formats, such as, with static images: JPEG, TIFF, WMF, BMP, GIF, EPS, PDF, PNG, PBM, and more. And, normally, each kind of data, and each different format, needs to be processed in its own special way.\nSome of this diversity is necessary and useful. For example: • The cultural life of a community is often intimately\nconnected with the language of that community. • Notwithstanding the dictum that “A picture is worth a\nthousand words”, natural languages, collectively, have special strengths. • Ancient texts are of interest for historical, cultural and other reasons. • With techniques and technologies as they have developed to date, it often makes sense to use different formalisms or formats for different purposes. • Over-zealous standardisation may stifle creativity. Nevertheless, there are several reasons, described in the next subsection, for trying to develop a universal framework for the representation and processing of diverse kinds of knowledge (UFK). Such a system may help to reduce unnecessary diversity in formalisms and formats for knowledge\nand in their modes of processing. But it is likely that many existing systems would continue in use for the kinds of reasons mentioned above, perhaps with translations into UFK form, if or when that proves necessary."
    }, {
      "heading" : "A. Reasons for Developing a Universal Framework for the Representation and Processing of Knowledge",
      "text" : "Of the reasons described here for developing a UFK, some relate fairly directly to issues with big data (Sections III-A1, III-A2, and III-A4), while the rest draw on other aspects of computing, engineering, and biology.\n1) The Discovery of Structure in Data: If we are trying to discover patterns of association or other structures in big data (Section IV), a diversity of formalisms and formats is a handicap. Let us imagine for example how an artificial learning system might discover the association between lightning and thunder. Detecting that association is likely to be difficult if:\n• Lightning appears in big data as a static image in one of several formats, like those mentioned above; or in a moving image in one of several formats; or it is described, in spoken or written form, as any one of such things as “firebolt”, “fulmination”, “la foudre”, “der Blitz”, “lluched”, “a big flash in the sky”, or indeed “lightning”. • Thunder is represented in one of several different audio formats; or it is described, in spoken or written form, as “thunder”, “gök gürültüsü”, “le tonnerre”, “a great rumble”, and so on.\nThe association between lightning and thunder will be most easily detected via the underlying meanings of the forms that have been mentioned. We may suppose that, at some level, knowledge about lightning has an associated code or identifier, something like ‘LTNG’, and that knowledge about thunder has a code or identifier such as ‘THDR’. Encodings like those would cut through much of the complexity of surface forms and allow underlying associations, such as ‘LTNG THDR’, to show through.\nIt seems likely that at least part of the reason that people find it relatively easy to recognise, without being told, that there is an association between lightning and thunder is that, in our brains, there is some uniformity in the way different kinds of knowledge are represented and processed, without awkward inconsistencies (Section III-A7).\n2) The Interpretation of Data: If we are trying to recognise objects in images, do scene analysis, or otherwise interpret what the images mean, it would make things simpler if we did not have to deal with the diversity of formats for images mentioned earlier. Likewise for other kinds of data.\n3) Data Fusion: In many fields, there is often a need to combine diverse sources of information to create a coherent whole. For example, in a study of the migration of whales, we may have, for each animal, a stream of information about the temperature of the water at each point along its route, another stream of information about the depths at which the animal is swimming, information about the weather at the surface at each point, information about dates and times, and so on.\nIf we are to weld those streams of information together, it would not be helpful if the geographical coordinates for different streams of information were to be expressed in different ways, perhaps using the Greenwich meridian for temperatures, the Paris meridian for depths, the Universal Transverse Mercator (UTM) system for weather, and some other scheme for the dates and times.\nIn short, there is a clear need to adopt a uniform system for representing the data—geographical coordinates in this example—that are needed to fuse separate but related streams of information to create a coherent view.\n4) The Understanding and Translation of Natural Languages: In our everyday use of natural languages we recognise that meanings are different from the words that express them and that, very often, two or more distinct sequences of words may mean the same thing or have the same referent: “the capital of the United States” means the same as “Washington, D. C.”; “Ursus maritimus” means the same as “polar bear”; and so on. These intuitions corroborate the need for a UFK, or something like it, which is independent of the words in any natural language.\nAgain, it is widely recognised that, if machine translation of natural languages is ever to reach the standard of good human translators, it will be necessary to provide some kind of interlingua—an abstract language-independent representation—to express the meaning of the source language and to serve as a bridge between the source language and the target language.2 Any such interlingua is likely to be similar to or the same as a UFK.\n5) The “Semantic Web”, the “Internet of Things”, and the “Web of Entities”: The need for standardisation in the representation of knowledge is recognised in writings about the semantic web (eg, [9]), the internet of things (eg, [10]), and in the Okkam project, aiming to create unique identifiers for a global web of entities.3\n6) The Long-Term Preservation of Data: The continual creation of new formalisms and new formats for information and their subsequent obsolescence can mean that old data, which may include data of great value, may become unreadable or\n2See, for example, “Interlingual machine translation”, Wikipedia, hrefhttp://bit.ly/1mCDTs3bit.ly/1mCDTs3, retrieved 2014-01-24.\n3See “Okkam: Enabling the Web of Entities. A scalable and sustainable solution for systematic and global identifier reuse in decentralized information environments”, project reference: 215032, completed: 2010-06-30, URL: bit.ly/OSjc1b, information retrieved 2014-03-24.\notherwise unusable. A UFK would help to reduce or eliminate this problem.\n7) Knowledge and Brains: In keeping with the long tradition in engineering of borrowing ideas from biology, the structure and functioning of brains provide reasons for trying to develop a UFK:\n• Since brains are composed largely of neural tissue, it appears that neurons and their inter-connections, with glial cells, provide a universal framework for the representation and processing of all kinds of sensory data and all other kinds of knowledge. • In support of that view is evidence that one part of the brain can take over the functions of another part (see, for example, [11], [12]). This implies that there are some general principles operating across several parts of the brain, perhaps all of them. • Most concepts are an amalgam of several different kinds of data or knowledge. For example, the concept of a “picnic” combines the sights, sounds, tactile and gustatory sensations, and the social and logistical knowledge associated with such things as a light meal in pleasant rural surroundings. To achieve that kind of seamless integration of different kinds of knowledge, it seems necessary for the human brain to be or to contain a UFK."
    }, {
      "heading" : "B. The Potential of the SP System as a Universal Framework for the Representation and Processing of Knowledge",
      "text" : "In the SP programme, the aim has been to create a system that, in accordance with Occam’s Razor, combines conceptual simplicity with descriptive or explanatory power [4, Section 1.3], [5, Section 2]. Although the SP computer model is relatively simple—its “exec” file requires less than 500 KB of storage space—and despite the great simplicity of SP patterns as a vehicle for knowledge (Section II), the SP system, without additional programming, may serve in the representation and processing of several different kinds of knowledge:\n• Syntax and semantics of natural languages. The system provides for the representation of syntactic rules, including discontinuous dependencies in syntax, and for the parsing and production of language [4, Chapter 5], [3, Section 8]. It has potential to represent non-syntactic ‘meanings’ via such things as class hierarchies and partwhole hierarchies (next), and it has potential in the understanding of natural language and in the production of sentences from meanings [4, Section 5.7]. • Class hierarchies and part-whole hierarchies. The system lends itself to the representation of class hierarchies (eg, species, genus, family, etc), heterarchies (class hierarchies with cross-classification), and part-whole hierarchies (eg, [[head [eyes, nose, mouth, ...]], [body ...], [legs ...]]) and their processing in pattern recognition, reasoning, and more [4, Sections 6.4.1 and 6.4.2], [3, Section 9.1]. • Networks and trees. The SP system supports the representation and processing of such things as hierarchical and network models for databases [13, Section 5], and probabilistic decision networks and decision trees [4, Section 7.5]. And it has advantages as an alternative to\nBayesian networks [4, Section 7.8] (reproduced in [3, Sections 10.2, 10.3, and 10.4]). • Relational knowledge. The system supports the representation of knowledge with relational tuples, and retrieval of information in the manner of query-by-example [13, Section 3], and it has some apparent advantages compared with the relational model [13, Section 4.2.3]. • Rules and reasoning. The system supports several kinds of reasoning, with the representation of associated knowledge. Examples include one-step ‘deductive’ reasoning, abductive reasoning, chains of reasoning, reasoning with rules, nonmonotonic reasoning, and causal diagnosis [4, Chapter 7]. • Patterns and pattern recognition. The SP system has strengths in the representation and processing of onedimensional patterns [4, Chapter 6], [5, Section 9], and it may be applied to medical diagnosis, viewed as a pattern recognition problem [14]. • Images. Although the SP computer model has not yet been generalised to work with patterns in two dimensions, there is clear potential for the SP system to be applied to the representation and processing of images and other kinds of information with a 2D form. This is discussed in [4, Section 13.2.1] and also in [15]. • Structures in three dimensions. It appears that the multiple alignment framework may be applied to the representation and processing of 3D structures via the stitching together of overlapping 2D views [15, Section 7.1], in much the same way that 3D models may be created from overlapping 2D photos,4 or a panoramic photo may be created from overlapping shots. • Procedural knowledge. The SP system can represent simple procedures (actions that need to be performed in a particular sequence); it can model such things as ‘variables’, ‘values’, ‘types’, ‘function with parameters’, repetition of operations, and more [5, Section 6.6.1]; and it has potential to represent sets of procedures that may be performed in parallel [5, Section 6.6.3]. These representations may serve to control real-world operations in sequence and in parallel.\nAs a candidate for the role of UFK, the SP system has other strengths:\n• Because of the generality of the concept of information compression via the matching and unification of patterns, there is reason to believe that the system may be applied to the representation and processing of all kinds of knowledge, not just those listed above. • Because all kinds of knowledge are represented in one simple format (arrays of atomic symbols in one or two dimensions), and because all kinds of knowledge are processed in the same way (via the creation of multiple alignments), the system provides for the seamless integration of diverse kinds of knowledge, in any combination [5, Section 7]. • Because of the system’s existing and potential capabilities\n4See, for example, “Big Object Base” (bit.ly/1gwuIfa), “Camera 3D” (bit.ly/1iSEqZu), or “PhotoModeler” (bit.ly/MDj70X.)\nin learning and discovery (Section IV), it has potential for the automatic structuring of knowledge, reducing or eliminating the need for hand crafting, with corresponding benefits in terms of speed, cost, and reducing errors. • For reasons given in Section XI), the SP system may facilitate the visualisation of structures and processes via static and moving images.\nIn summary, the relative simplicity of the SP system, its versatility in the representation and processing of diverse kinds of knowledge, its provision for seamless integration of different kinds of knowledge in any combination, the system’s potential for automatic structuring of knowledge, and for the visualisation of structures and processes, makes it a good candidate for development into a UFK."
    }, {
      "heading" : "C. Standardisation and Translation",
      "text" : "The SP system, or any other UFK, may be used in two distinct ways:\n• Standardisation in the representation of knowledge. There is potential, on relatively long timescales, to standardise the representation and processing of many kinds of knowledge, cutting out much of the current jumble of formalisms and formats. But for the kinds of reasons mentioned in Section III, it is likely that some of those formalisms or formats will never be replaced or will coexist with representation and processing via the UFK. • Translation into the universal framework. Where a body of information is expressed in one or more non-standard forms but is needed in the standard form, it may be translated. This may be done via the SP system, as outlined in Section V. Or it may be done using conventional technologies, in much the same way that the source code for a computer program may, using a compiler, be translated into object code. The translation of natural languages is likely to prove more challenging than the translation of artificial formalisms and formats.\nEither way, any body of big data may be expressed in a standard form that facilitates the unsupervised learning or discovery of structures and associations within those data (Section IV), and facilitates forms of interpretation as outlined in Section V."
    }, {
      "heading" : "IV. LEARNING AND DISCOVERY",
      "text" : "“While traditional computers must be programmed by humans to perform specific tasks, cognitive systems will learn from their interactions with data and humans and be able to, in a sense, program themselves to perform new tasks.” [2, p. 7].\nIn broad terms, unsupervised learning in the SP system means lossless compression of a body of information, I, via the matching and unification of patterns (Section IV-A).\nThe SP computer model (SP70, [4, Chapter 9], [3, Section 5]), has already demonstrated an ability to discover generative grammars from unsegmented samples of English-like artificial languages, including segmental structures, classes of structure, and abstract patterns. As it is now, it has shortcomings,\noutlined in [3, Section 3.3]. But I believe these problems are soluble, and that their solution will clear the path to the unsupervised learning of other kinds of structures, such as class hierarchies, part-whole hierarchies, and discontinuous dependencies in data. In what follows, we shall assume that these and other problems have been solved and that the system is relatively robust and mature.\nA strength of the SP system is that it can discover structures in data, not merely statistical associations between preestablished structures.\nAs noted in Section II, evidence to date suggests that the system conforms to the DONSVIC principle [3, Section 5.2]."
    }, {
      "heading" : "A. The Product of Learning",
      "text" : "The product of learning from a body of data, I, may be seen to comprise a grammar (G) and an encoding (E) of I in terms of the grammar. Here, the term ‘grammar’ has a broad meaning that includes grammars for natural languages, grammars for static and moving images, grammars for business procedures, and so on.\nAs with all other kinds of data in the SP system, G and E are both represented using SP patterns.\nIn accordance with the principles of minimum length encoding [16], the SP system aims to minimise the overall size of G and E.5 Together, G and E achieve lossless compression of I.\nG is largely about redundancies within I, while E is mainly a record of the non-redundant aspects of I. Here, any symbol or sequence of symbols represents redundancy within I if it repeats more often than one would expect by chance. To reach that threshold, small patterns need to occur more frequently than large patterns.6\nG may be regarded as a distillation of the ‘essence’ of I. Normally, G would be more interesting than E, and more useful in the kinds of applications described in Sections V and X-B.\nWith data that is received or processed as a stream rather than a static body of data of fixed size (Section VI, below), G may be grown incrementally. And, quite often, there is likely to be a case for merging Gs from different sources, with unification of patterns that are the same. In principle, there could be a single ‘super’ G, expressing the essentials of the world’s knowledge in a compressed form. Similar remarks apply to Es—if they are needed.\n5The similarity with research on grammatical inference is not accidental: the SP programme of research has grown out of earlier research developing computer models of language learning (see [17] and other publications that may be downloaded via bit.ly/JCd6jm). But in developing the SP system, a radical reorganisation has been needed to meet the goal of simplifying and integrating concepts across artificial intelligence, mainstream computing, and human perception and cognition. Unlike the earlier models and other research on grammatical inference, multiple alignment is central in the workings of the SP computer model, including unsupervised learning. A bonus of the new structure is potential for the unsupervised learning of such things as class hierarchies, part-whole hierarchies, and discontinuous dependencies in data.\n6G may contain some patterns that do not, in themselves, represent redundancy but are included in G because of their supporting role [4, Section 3.6.2]."
    }, {
      "heading" : "B. Unsupervised Learning and the Problem of Variety in Big Data",
      "text" : "Systems for unsupervised learning may be applied most simply and directly when the data for learning come in a uniform style as, for example, in DNA data: simple sequences of the letters A, T, G, and C. But as outlined in Section III-A1, it may be difficult to discover recurrent associations or structures when there is a variety of formalisms and formats.\nThe discussion here focuses on the relatively challenging area of natural languages, because the variety of natural languages is a significant part of the problem of variety in big data, because the SP system has strengths in that area, and because it seems likely that solutions with natural languages will generalise relatively easily to other areas.\nWith natural languages, learning processes in a mature version of the SP system may be applied in four distinct but inter-related ways, discussed in the following subsections.\n1) Learning the Surface Forms of Language: If the data for learning are text in a natural language, then the product of learning (Section IV-A) will be about words and parts of words, about phrases, clauses and sentences, and about grammatical categories at all levels. Likewise with speech.\nEven with human-like capabilities in learning, a G that is derived without the benefit of meanings is likely to differ in some respects from a grammar created by a linguist who can understand what the text means. This is because there are subtle interdependencies between syntax and semantics [5, Section 6.2] which cannot be captured from text on its own, without information about meanings.\n2) Learning Non-Syntactic Knowledge: The SP system may be applied to learning about the non-syntactic world: objects and their interactions, scenery, music, games, and so on. These have an intrinsic interest that is independent of natural language, but they are also the things that people talk and write about: the non-syntactic meanings or semantics of natural language. Some aspects of this area of learning are discussed in [4, Section 13.2.1] and [15].\n3) Connecting Syntax with Semantics: Of course, for any natural language to be effective, syntax must connect with semantics. Examples that show how syntax and semantics may work together in the multiple alignment framework, in both the analysis and production of language, are presented in [4, Section 5.7]. As noted in Section III-B, seamless integration of different kinds of knowledge is facilitated by the use of one simple format for all kinds of knowledge and a single framework—multiple alignment—for processing diverse kinds of knowledge.\nIn broad terms, making the connection between syntax and semantics means associational learning, no different in principle from learning the association between lightning and thunder (Section III-A1), between smoke and fire, between a savoury aroma and a delicious meal, and so on.\nFor the SP system to learn the connections between syntax and semantics, it will need speech or text to be presented alongside the non-syntactic information that it relates to, in much the same way that, normally, children have many opportunities to hear people talking and see what they are talking about at the same time.\n4) Learning Via the Interpretation of Surface Forms: Since speech and text in natural languages are an important part of big data, it is clear that if the SP system, or any other learning system, is to get full value from big data, it will need to learn from the meanings of speech or text as well as from their surface forms.\nFor any given body of text or speech, I, the first step, of course, will be to derive its meanings. This can be done via processes of interpretation, as described in Section V.\nThe set of SP patterns that represent the meanings of I may then be processed as if it was New information, searching for redundancies in the data, unifying patterns that match each other, and creating a compressed representation of the data. Then it should be possible to discover such things as the association between lightning and thunder (Section III-A1), regardless of how the data was originally presented.\nV. INTERPRETATION OF DATA\nBy contrast with unsupervised learning, which compresses a body of information (I) to create G and E, the concept of interpretation in this article means processing I in conjunction with a pre-established grammar (G) to create a relatively compact encoding (E) of I.\nDepending on the nature of I and G, the process of interpretation may be seen to achieve such things as pattern recognition, information retrieval, parsing or production of natural language, translation from one representation to another, several kinds of reasoning, planning, and problem solving. Some of these were touched on briefly in Section III-B. Here is a little more detail:\n• Pattern recognition. With the SP system, pattern recognition may be achieved: at multiple levels of abstraction; with “family resemblance” or polythetic categories; in the face of errors of omission, commission or substitution in data; with the calculation of a probability for any given identification, classification or associated inference; with sensitivity to context in recognition; and with the seamless integration of pattern recognition with other aspects of intelligence—reasoning, learning, problem solving, and so on [3, Section 9], [4, Chapter 6]. As previously mentioned, the system may be applied in computer vision [15] and in medical diagnosis [14], viewed as pattern recognition. • Information retrieval. The SP system lends itself to information retrieval in the manner of query-by-example and, with the provision of SP patterns representing relevant rules, there is potential to create the facilities of a query language like SQL [13]. • Parsing and production of natural language. As can be seen in Fig. 2, the creation of a multiple alignment in the SP system may achieve the effect of parsing a sentence in a natural language (see also [4, Section 3.4.3 and Chapter 5]). It may also function in the production of sentences [4, Section 3.8]. • Translation from one representation to another. There is potential with the SP system for the integration of syntax and semantics in both the understanding and production\nof natural language [4, Section 5.7], with corresponding potential for the translation of any one language into an SP-style interlingua and further translation into any other natural language [5, Section 6.2.1]. Probably less challenging, as mentioned earlier, would be the translation of artificial formalisms and formats—JPEG, MP3, and so on—into an SP-style representation. • Several kinds of reasoning. The SP system can perform several kinds of reasoning, including one-step ‘deductive’ reasoning, abductive reasoning, reasoning with probabilistic networks and trees, reasoning with ‘rules’, nonmonotonic reasoning, Bayesian reasoning and “explaining away”, causal diagnosis, and reasoning that is not supported by evidence [4, Chapter 7]. • Planning. With SP patterns representing direct flights between cities, the SP system can normally work out one or more routes between any two cities that are not connected directly, if such a route exists [4, Section 8.2]. • Problem solving. The system can also solve textual versions of geometric analogy problems, like those found in puzzle books and IQ tests [4, Section 8.3].\nVI. VELOCITY: ANALYSIS OF STREAMING DATA\n“Most of today’s computing tasks involve data that have been gathered and stored in databases. The data make a stationary target. But, increasingly, vitally important insights can be gained from analyzing information that’s on the move. ... This approach is called streams analytics. Rather than placing data in a database first, the computer analyses it as it comes from a variety of sources, continually refining its understanding of the data as conditions change. This is the way humans process information.” [2, pp. 49–50].\nAlthough, in its unsupervised learning, the SP system may process information in batches, it lends itself most naturally to an incremental style. In the spirit of the quotation above, the SP system is designed to assimilate New information to a steadily-growing body of relatively-compressed Old information, as shown schematically in Fig. 1.\nLikewise, in interpretive processes such as pattern recognition, processing of natural language, and reasoning, the SP system may be applied to streams of data as well as the processing of data in batches.\nVII. VOLUME: MAKING BIG DATA SMALLER\n“Very-large-scale data sets introduce many data management challenges.” [1, p. 41]. “In addition to reducing computation time, proper data representations can also reduce the amount of required storage (which translates into reduced communication if the data are transmitted over a network).” [1, p. 68].\nBecause information compression is central in how the SP system works, it has potential to reduce problems of volume in big data by making it smaller. Although comparative studies have not yet been attempted, the SP system has potential to\nachieve relatively high levels of lossless compression for two main reasons: it is designed so that, if required, it can perform a relatively thorough search for redundancies in data; and there is potential to tap into discontinuous dependencies in data, an aspect of redundancy that appears to be outside the scope of other systems for compression of information [5, Section 6.7].\nIn brief, information compression in the SP system can yield direct benefits in the storage, management, and transmission of data, and indirect benefits as described elsewhere in this article: unsupervised learning (Section IV), processes of interpretation such as pattern recognition and reasoning (Section V), economies in the transmission of data via the separation of grammar and encoding (Section VIII), gains in computational efficiency (Section IX), and assistance in the management of errors and uncertainties in data (Section X)."
    }, {
      "heading" : "VIII. ADDITIONAL ECONOMIES IN THE TRANSMISSION OF DATA",
      "text" : "“One roadblock to using cloud services for massive data analysis is the problem of transferring the large data sets. Maintaining a high-capacity and widescale communications network is very expensive and only marginally profitable.” [1, p. 55]. “To control costs, designers of the [DOME] computing system have to figure out how to minimize the amount of energy used for processing data. At the same time, since so much of the energy in computing is required to move data around, they have to discover ways to move the data as little as possible.” [2, p. 65].\nAlthough the second of these two quotes may refer in part to movements of data such as those between the CPU and the memory of a computer, the discussion here is about transmission of data over longer distances such as, for example, via the internet.\nAs we have seen (Section VII), the SP system may promote the efficient transmission of data by making it smaller. But there is potential with the SP system for additional economies in the transmission of data and these may be very substantial [5, Section 6.7.1].\nAny body of data, I, may be compressed by encoding it in terms a ‘grammar’ (G), provided that G contains the kinds of structures that are found in I (Section IV-A). Then I may be sent from A to B by sending only the ‘encoding’ (E). Provided that B has a copy of G, I may be recreated with complete fidelity by means of the SP system [3, Section 4.5], [4, Section 3.8]. Since E would normally be very much smaller than the I from which it was derived, it seems likely that there would be a net gain in efficiency, allowing for the computational costs of encoding and decoding.\nSince a copy of G must be transmitted to B, any savings will be relatively small if it is used only for the decoding of a single instance of E. But significant savings are likely if, as would normally be the case, one copy of G may be used for the decoding of many different instances of E, representing many different Is.\nIn this kind of application, it would probably make sense for there to be a division of labour between creating a\ngrammar and using it in the encoding and decoding of data. For example, the computational heavy lifting required to build a grammar for video images may be done by a highperformance computer. But that grammar, once constructed, may serve in relatively low-powered devices—smartphones, tablet computers, and the like—for the much less demanding processes of encoding and decoding video transmissions."
    }, {
      "heading" : "IX. ENERGY, SPEED, AND BULK",
      "text" : "“... we’re reaching the limits of our ability to make [gains in the capabilities of CPUs] at a time when we need even more computing power to deal with complexity and big data. And that’s putting unbearable demands on today’s computing technologies— mainly because today’s computers require so much energy to perform their work.” [2, p. 9].\n“The human brain is a marvel. A mere 20 watts of energy are required to power the 22 billion neurons in a brain that’s roughly the size of a grapefruit. To field a conventional computer with comparable cognitive capacity would require gigawatts of electricity and a machine the size of a football field. So, clearly, something has to change fundamentally in computing for sensing machines to help us make use of the millions of hours of video, billions of photographs, and countless sensory signals that surround us. ... Unless we can make computers many orders of magnitude more energy efficient, we’re not going to be able to use them extensively as our intelligent assistants.” [2, p. 75, p. 88].\n“Supercomputers are notorious for consuming a significant amount of electricity, but a less-known fact is that supercomputers are also extremely ‘thirsty’ and consume a huge amount of water to cool down servers through cooling towers ....”7\nIt is now clear that, if we are to do meaningful analyses of more than a small fraction of present and future floods of big data, substantial gains will be needed in the computational efficiency of computers, with associated benefits:\n• Cutting demands for energy, with corresponding cuts in the need for cooling of computers. • Speeding up processing with a given computational resource. • Consequent reductions in the size and weight of computers.\nWith the possible exception of the need for cooling, these things are particularly relevant to mobile devices, including autonomous robots.\nThe following subsections describe how the SP system may contribute to computational efficiency, via information compression and probabilities, and via a synergy with ‘datacentric’ computing.\n7From “How can supercomputers survive a drought?”, HPC Wire, 2014-01-26, bit.ly/LruEPS."
    }, {
      "heading" : "A. Computational Efficiency via Information Compression and Probabilities",
      "text" : "In the light of evidence that the SP system is Turingequivalent [4, Chapter 4], and since information processing in the SP system means compression of information via the matching and unification of patterns (Section II), anything that increases the efficiency of searching for good full and partial matches between patterns will also increase the efficiency of information processing.\nIt appears that information compression and associated probabilities can themselves be a means of increasing the efficiency of searching, as described in the next two subsections.\n1) Reducing the Sizes of Data to be Searched and of Search Terms: As described in [5, Section 6.7.2], if we wish to search a body of information, I, for instances of a pattern like “Treaty on the Functioning of the European Union,” the efficiency of searching may be increased:\n• By reducing the size of I so that there is less to be searched. The size of I may be reduced by replacing all but one of the instances of “Treaty on the Functioning of the European Union” with a relatively short code or identifier like “TFEU”, and likewise with other recurrent patterns. More generally, the size of I may be reduced via unsupervised learning in the SP system. It is true that the compression of I is a computational cost, but this investment is likely to pay off in later processing. • By searching with a short code like “TFEU” instead of a relatively large pattern like “Treaty on the Functioning of the European Union”. Other things being equal, a smaller search pattern means faster searching.\nWith regard to the second point, there is potential to cut out some searching altogether by creating direct connections between each instance of a code (“TFEU” in this example) and the thing that it represents (“Treaty on the Functioning of the European Union”). In SP-neural (Section IX-B1), there are connections of that kind between “pattern assemblies”, as shown schematically in Fig. 3.\n2) Concentrating Search Where Good Results Are Most Likely to be Found: If we want to find some strawberry jam, our search is more likely to be successful in a supermarket than it would be in a hardware shop or a car-sales showroom. This may seem too simple and obvious to deserve comment but it illustrates the extraordinary knowledge that most people have of an informal ‘statistics’ of the world that we inhabit, and how that knowledge may help us to minimise effort.8\nWhere does that statistical knowledge come from? In the SP theory, it flows directly from the central role of information compression in our perceptions, learning and thinking, and from the intimate relationship between information compression and concepts of prediction and probability [6].\nAlthough the SP computer model may calculate probabilities associated with multiple alignments (Section II), it actually uses levels of information compression as a guide to search. Those levels are used, with heuristic search methods (including\n8See also G. K. Zipf’s Human Behaviour and the Principle of Least Effort [18].\nescape from ‘local peaks’), to ensure that searching is concentrated in areas where it is most likely to be fruitful [4, Sections 3.9, 3.10, and 9.2]. This not only speeds up processing but yields Big-O values for computational complexity that are within acceptable limits [4, Sections 3.10.6, 9.3.1, and A.4].\n3) Potential Gains in Computational Efficiency: No attempt has yet been made to quantify potential gains in computational efficiency from the compression of information, as described in Sections IX-A1, IX-A2, and VIII, but they could be very substantial:\n• Since information compression is fundamental in the workings of the SP system, there is potential for corresponding savings in all parts and levels in the system. • The entire structure of knowledge that the system creates for itself is intrinsically statistical, with potential on many fronts for corresponding savings in computational costs and associated demands for energy.\nIt may be argued that, since object-oriented programming already provides for compression of information via class hierarchies and inheritance of attributes, the benefits of information compression are already available in conventional computing systems. In response, it may be said that, while there are undoubted benefits from object-oriented programming, existing object-oriented systems run on conventional computers and suffer from the associated inefficiencies.\nRealising the full potential of information compression as a means of improving computational efficiency will probably mean new thinking about computer architectures, probably in conjunction with the development of data-centric computing (next)."
    }, {
      "heading" : "B. A Potential Synergy with Data-Centric Computing",
      "text" : "“What’s needed is a new architecture for computing, one that takes more inspiration from the human brain. Data processing should be distributed throughout the computing system rather than concentrated in a CPU. The processing and the memory should be closely integrated to reduce the shuttling of data and instructions back and forth.” [2, p. 9]. “Unless we can make computers many orders of magnitude more energy efficient, we’re not going to be able to use them extensively as our intelligent assistants. Computing intelligence will be too costly to be practical. Scientists at IBM Research believe that to make computing sustainable in the era of big data, we will need a different kind of machine— the data-centric computer. ... Machines will perform computations faster, make sense of large amounts of data, and be more energy efficient.” [2, p. 88].\nThe SP concepts may help to integrate processing and memory, as described in the next two subsections.\n1) SP-Neural: Although the main emphasis in the SP programme has been on developing an abstract framework for the representation and processing of knowledge, the theory includes proposals—called SP-neural—for how those abstract concepts may be realised with neurons [4, Chapter 11].\nFig. 3 shows in outline how an SP-style conceptual structure would appear in SP-neural. It is envisaged that SP patterns would be realised with pattern assemblies—groupings of neurons like those shown in the figure within broken-line envelopes.\nThe whole scheme is quite different from ‘artificial neural networks’ as they are commonly conceived in computer science.9 It may be seen as a development of Donald Hebb’s [19] concept of a ‘cell assembly’, with more precision about how structures may be shared, and other differences.10\nIn SP-neural, what is essentially a statistical model of the world is reflected directly in groupings of neurons and their interconnections, as shown in Fig. 3. It is envisaged that such things as pattern recognition would be achieved via the transmission of impulses between pattern assemblies, and via the transmission of impulses between neurons within each pattern assembly. In keeping with what is known about the workings of brains and nervous systems, it is likely that there would be important roles for both excitatory and inhibitory signals.\nIn short, neurons in SP-neural serve for both the representation and processing of knowledge, with close integration of the two—in accordance with the concept of data-centric computing. One architecture may promote computational efficiency by combining the benefits of information compression and probabilistic knowledge with the benefits of data-centric computing.\n2) Computing with Light or Chemicals: The SP concepts appear to lend themselves to computing with light or chemicals, perhaps by-passing such things as transistors or logic gates that have been prominent in the development of electronic computers [5, Section 6.10.6].11\nAt the heart of the SP system is a process of finding good full and partial matches between patterns. This may be done with light, with the potential advantage that light beams may cross each other without interference. Another potential advantage is that, with collimated light, there may be relatively small losses over distance—although distances should probably be minimised to save on transmission times and to minimise the sizes of computing devices. There appears to be potential to create an optical or optical/electronic version of SP-neural.\nFinding good full and partial matches between patterns may\n9See, for example, “Artificial neural network”, Wikipedia, en.wikipedia.org/wiki/Artificial neural network, retrieved 2013-12-23.\n10In particular, unsupervised learning in the SP system [3, Section 5], [4, Chapter 9] is radically different from the “Hebbian” concept of learning (see, for example, “Hebbian theory”, Wikipedia, http://en.wikipedia.org/wiki/Hebbian learning, retrieved 2013-12-23), described by Hebb [19] and adopted as the mechanism for learning in most artificial neural networks. By contrast with Hebbian learning, the SP system, like a person, may learn from a single exposure to some situation or event. And, by contrast with Hebbian learning, it takes time to learn a language in the SP system because of the complexity of the search space, not because of any kind of gradual strengthening or “weighting” of links between neurons [4, Section 11.4.4].\n11“The most promising means of moving data faster is by harnessing photonics, the generation, transmission, and processing of light waves.” [2, p. 93].\nalso, potentially, be done with chemicals such as DNA,12 with potential for high levels of parallelism, and with the attraction that DNA can be a means of storing information in a very compact form, and for very long periods [20].\nWith both light and chemicals, the SP system may help realise data-centric integration of knowledge and processing. As before, there is potential for gains in computational efficiency via one architecture that combines the benefits of information compression and probabilistic knowledge with the benefits of data-centric computing.\n12See, for example, “DNA computing”, Wikipedia, bit.ly/1gfEP4p, retrieved 2013-12-30.\nX. VERACITY: MANAGING ERRORS AND UNCERTAINTIES IN DATA\n“In building a statistical model from any data source, one must often deal with the fact that data are imperfect. Real-world data are corrupted with noise. Such noise can be either systematic (i.e., having a bias) or random (stochastic). Measurement processes are inherently noisy, data can be recorded with error, and parts of the data may be missing.” [1, p. 99]. “Organizations face huge challenges as they attempt to get their arms around the complex interactions between natural and human-made systems. The enemy is uncertainty. In the past, since computing systems didn’t handle uncertainty well, the tendency was to pretend that it didn’t exist. Today, it is clear that that approach won’t work anymore. So rather than trying to eliminate uncertainty, people have to embrace it.” [2, pp. 50–51].\nThe SP system has potential in the management of errors and uncertainties in data as described in the following subsections."
    }, {
      "heading" : "A. Parsing or Pattern Recognition That Is Robust in the Face of Errors",
      "text" : "As mentioned in Section II, the SP system is inherently probabilistic. Every SP pattern has an associated frequency of occurrence, and probabilities may be derived for each multiple alignment [3, Section 4.4], [4, Section 3.7 and Chapter 7].\nThe probabilistic nature of the system means that, in operations such as parsing natural language or pattern recognition, it is robust in the face of errors of omission, of commission, or of substitution [3, Section 4.2.2], [4, Section 6.2.1]. In the same way that we can recognise things visually despite disturbances such as falling leaves or snow (and likewise for other senses), the SP system can, within limits, produce what we intuitively judge to be ‘correct’ analyses of inputs that are not entirely accurate.\nFig. 4 shows how the SP system may achieve a ‘correct’ parsing of the same sentence as in Fig. 2 but with errors: the addition of ‘x’ within ‘t h e’, the omission of ‘l’ from ‘a p p l e s’, and the substitution of ‘k’ for ‘w’ in ‘s w e e t’. In effect, the parsing identifies errors in the sentence and suggests corrections for them: ‘t h x e’ should be ‘t h e’, ‘a p p e s’ should be ‘a p p l e s’, and ‘s k\ne e t’ should be ‘s w e e t’. The system’s ability to fill in gaps—such as the missing ‘l’ in ‘a p p l e s’—is closely related to the system’s ability to make probabilistic inferences—going beyond the information given—discussed in some detail in [4, Chapter 7] and more briefly in [3, Section 10]."
    }, {
      "heading" : "B. Unsupervised Learning with Errors and Uncertainties in Data",
      "text" : "Insights that have been achieved in research on language learning and grammatical inference [3, Section 5.3], [17], [4, Sections 2.2.12 and 12.6] may help to illuminate the problem of managing errors and uncertainties in big data.\nThe way we learn a first language has some key features:\n• We learn from a finite sample of the language, normally quite large.13 This is represented by the smallest of the envelopes shown in Fig. 5. • It is clear that mature knowledge of a given language, L, includes an ability to interpret and, normally, to produce an infinite number of utterances in L.14 It also includes an ability to distinguish sharply between utterances that belong in L—represented by the middle-sized envelope in Fig. 5—and those that don’t—represented by the area between the middle-sized envelope and the outer-most envelope in the figure. • The finite sample of language from which we learn includes many utterances which are not correct, meaning that they do not belong in L. These include false starts, incomplete sentences, garbled words, and so on. These utterances are marked dirty data in the figure.\n13An alternative view, promoted most notably by Noam Chomsky, is that we are born with a knowledge of ‘universal grammar’—structures that appear in all the world’s languages. But despite decades of research, there is still no satisfactory account of what that universal grammar may be or how it may function in the learning of a first language. Notice that the concept of a universal grammar is different from that of a UFK because the former means linguistic structures hypothesised to exist in all the world’s languages, while the latter means a framework for the representation and processing of diverse kinds of knowledge.\n14Exceptions in the latter case are people who can understand language but, because of physical handicap or other reason, may not be able to produce language (more below).\nFrom these key features, two main questions arise, described here with putative answers provided by unsupervised learning via information compression:\n• Learning with dirty data. How is it that we can develop a keen sense of what does or does not belong in our native language or languages, despite the fact that much of the speech that we hear as children contains the kinds of haphazard errors mentioned above, and in the face of evidence that language learning may be achieved without the benefit of error correction by a teacher, or anything equivalent.15\nIt appears that the principle of minimum length encoding (Section IV-A) provides an answer. In a learning system that seeks to minimise the overall size of the ‘grammar’ (G) and the ‘encoding’ (E), most of the haphazard errors that people make in speaking—rare individually but collectively quite common—would be recorded largely\n15In brief, the evidence is that people with a physical handicap that prevents them producing intelligible speech can still learn to understand their native language [21], [22]. If such a child is saying nothing that is intelligible, there is nothing for adults to correct. Christy Brown [22] went on to become a successful author, using his left foot for typing, and drawing on the knowledge of language that he learned by listening.\nin E, leaving G as a relatively clean expression of the language. Anything that is comparatively rare but exceeds the threshold for redundancy (Section IV-A) may appear in G, perhaps seen as a linguistic irregularity—such as ‘bought’ (not ‘buyed’) as the past tense of ‘buy’—or as a dialect form. • Generalisation without over-generalisation. How is it that, in learning a first language, L, we can generalise from the finite sample of language which is the basis for learning to the infinite set of utterances that belongs in L, without overgeneralising into the region between the middle-sized envelope and the outer-most envelope in Fig. 5. As before, there is evidence, discussed in the sources referenced above, that language learning does not depend on error-correction by a teacher or anything equivalent. As with learning with dirty data, it appears that generalisation without over-generalisation may be understood in terms of the principle of minimum length encoding. It appears that a learning process that seeks to minimise the overall size of G and E normally results in a grammar that generalises beyond the data in I but does not over-generalise. Both under-generalisation and overgeneralisation results in a greater overall size for G and E.\nThese principles apply to any kind of data, not just linguistic data. With unsupervised learning from a body of big data, I, the SP system provides two broad options:\n• Users may focus on both G and E, taking advantage of the system’s capabilities in lossless information compression, and ignoring the system’s potential with dirty data and the formation of generalisations without overgeneralisation. This would be the best option in areas of application where the precise form of the data is important, including any ‘errors’. • By focussing on G and ignoring E, users may see the redundant features in I and exclude everything else. As a rough generalisation, redundant features are likely to be ‘important’. They are likely to exclude most of the haphazard errors in I such as typos, misprints and other rarities that users may wish to ignore (but see Section X-C). And G is likely to generalise beyond what is in I—filling in apparent gaps in the data—and to do so with generalisations that are sensitive to the statistical structure of I, and excluding over-generalisations without that statistical support.\nThese two options are not mutually exclusive. Both would be available at all times, and users may adopt either or both of them according to need."
    }, {
      "heading" : "C. Rarity, Probabilities, and Errors",
      "text" : "Some issues relating to what has been said in Sections X-A and X-B are considered briefly here.\n1) Rarity and Interest: It may seem odd to suggest that we might choose to ignore things that are rare, since antiques that are rare may attract great interest and command high prices,\nand conservationists often have a keen interest in animals or plants that are rare.\nThe key point here is that there is an important difference between a body of information to be mined for its recurrent structures and things like antiques, animals, or plants. The latter may be seen as information objects that are themselves the products of learning processes designed to extract redundancy from sensory data. Like other real-world objects, an antique chair is a persistent, recurrent feature of the world, and it is the recurrence of such an entity in different contexts that allows us to identify it as an object.\n2) The Flip Side of Probabilities: As we have seen (Sections X-A and X-B), a probabilistic machine can help to identify probable errors in big data. But contradictory as it may seem, a consequence of working with probabilities—for both people and machines—is that mistakes may be made. We may bet on “Desert King” but find that “Midnight Lady” is the winner. And in the same way that people can be misled by a frequently-repeated lie, probabilistic machines are likely to be vulnerable to systematic distortions in data.\nThese observations may suggest that we should stick with computers in their traditional form, delivering precise, all-ornothing answers. But:\n• There are reasons to believe that computing and mathematics are fundamentally probabilistic: “I have recently been able to take a further step along the path laid out by Gödel and Turing. By translating a particular computer program into an algebraic equation of a type that was familiar even to the ancient Greeks, I have shown that there is randomness in the branch of pure mathematics known as number theory. My work indicates that—to borrow Einstein’s metaphor—God sometimes plays dice with whole numbers.” [23, p. 80]. • As noted in Section II, the SP system can be constrained to deliver all-or-nothing results in the manner of conventional computers. But “constraint” is the key word here: it appears that the comforting certainties of conventional computers come at the cost of restrictions in how they work, restrictions that may have been motivated originally by the low power of early computers [4, p. 28].\nXI. VISUALISATION\n“... methods for visualization and exploration of complex and vast data constitute a crucial component of an analytics infrastructure.” [1, p. 133]. “[An area] that requires attention is the integration of visualization with statistical methods and other analytic techniques in order to support discovery and analysis.” [1, p. 142].\nIn the analysis of big data, it is likely to be helpful if the results of analysis, and analytic processes, can be displayed with static or moving images.\nIn this connection, the SP system has three main strengths: • Transparency in the representation of knowledge. By\ncontrast with sub-symbolic approaches to artificial intelligence, there is transparency in the representation of knowledge with SP patterns and their assembly into\nmultiple alignments. Both SP patterns and multiple alignments may be displayed as they are or, where appropriate, translated into other graphical forms such as tree structures, networks, tables, plans, or chains of inference. • Transparency in processing. In building multiple alignments and deriving grammars and encodings, the SP system creates audit trails. These allow the processes to be inspected and could, with advantage, be displayed with moving images to show how knowledge structures are created. • The DONSVIC principle. As previously noted, the SP system aims to realise the DONSVIC principle [3, Section 5.2] and is proving successful in that regard. This means that structures created or discovered by the system— entities, classes of entity, and so on—should be ones that people regard as natural. Those kinds of structures are also likely to be ones that are well suited to representation with static or moving images."
    }, {
      "heading" : "XII. A ROAD MAP",
      "text" : "As mentioned in Section II, it is envisaged that the SP computer model will provide the basis for the development of a new version of the SP machine. How things may develop is shown schematically in Fig. 6. It is envisaged that this new version will be realised as a software virtual machine, hosted on an existing high-performance computer, that it will employ high levels of parallelism, that it will be accessible via a userfriendly interface from anywhere in the world, that all software will be open source, and that users will be able to create new versions of the system. This high-parallel, open source version of the SP machine will be a means for researchers everywhere to explore what can be done with the system and to create new versions of it.\nAs argued persuasively in [2, Chapters 5 and 6], and echoed in this article in Sections IX-A3 and IX-B, getting a proper grip on the problem of big data will probably require the development of new architectures for computers.\nBut there is plenty that can be done with existing computers. Most of the developments proposed in this article may be pursued without waiting for the development of new kinds of computer. Likewise, many of the potential benefits and\napplications of the SP system, described in [5] and including such things as intelligent databases [13] and new approaches to medical diagnosis [14], may be realised with existing kinds of computer."
    }, {
      "heading" : "XIII. CONCLUSION",
      "text" : "The SP system, designed to simplify and integrate concepts across artificial intelligence, mainstream computing, and human perception and cognition, has potential in the management and analysis of big data.\nThe SP system has potential as a universal framework for the representation and processing of diverse kinds of knowledge (UFK), helping to reduce the problem of variety in big data: the great diversity of formalisms and formats for knowledge, and how they are processed. The system may discover ‘natural’ structures in big data, and it has strengths in the interpretation of data, including such things as pattern recognition, natural language processing, several kinds of reasoning, and more. It lends itself to the analysis of streaming data, helping to overcome the problem of velocity in big data.\nApart from several indirect benefits described in this article, information compression in the SP system is likely to yield direct benefits in the storage, management, and transmission of big data by making it smaller. The system has potential for substantial additional economies in the transmission of data (via the separation of encoding from grammar), and for substantial gains in computational efficiency (via information compression and probabilities, and via a synergy with datacentric computing), with consequent benefits in energy efficiency, greater speed of processing with a given computational resource, and reductions in the size and weight of computers. The system provides a handle on the problem of veracity in big data, with potential to assist in the management of errors and uncertainties in data. It may help, via static and moving images, in the visualisation of knowledge structures created by the system and in the visualisation of processes of discovery and interpretation.\nThe creation of a high-parallel, open-source version of the SP machine, as outlined in Section XII, would be a means for researchers everywhere to explore what can be done with the system and to create new versions of it."
    }, {
      "heading" : "XIV. ACKNOWLEDGEMENTS",
      "text" : "I am grateful to Daniel J. Wolff for drawing my attention to big data as an area where the SP system may make a contribution."
    } ],
    "references" : [ {
      "title" : "Smart machines: IBM’s Watson and the era of cognitive computing",
      "author" : [ "J.E. Kelly", "S. Hamm" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "The SP theory of intelligence: an overview",
      "author" : [ "J.G. Wolff" ],
      "venue" : "Information, vol. 4, no. 3, pp. 283–341, 2013, see bit.ly/19MmbLd.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The SP theory of intelligence: benefits and applications",
      "author" : [ "——" ],
      "venue" : "Information, vol. 5, no. 1, pp. 1–27, 2014, see bit.ly/1lcquWF.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Information compression, intelligence, computing, and mathematics",
      "author" : [ "J.G. Wolff" ],
      "venue" : "2013, in preparation. See bit.ly/1jEoECH.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The sp theory of intelligence: an introduction",
      "author" : [ "——" ],
      "venue" : "2013, unpublished document. See bit.ly/1cFYTfw.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The semantic web",
      "author" : [ "T. Berners-Lee", "J. Hendler", "O. Lassila" ],
      "venue" : "Scientific American, pp. 35–43, May 2001.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The internet of things",
      "author" : [ "N. Gershenfeld", "R. Krikorian", "D. Cohen" ],
      "venue" : "Scientific American, vol. 291, no. 4, pp. 76–81, 2004.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Theoretical basis for brain plasticity after a TBI",
      "author" : [ "P. Bach-y-Rita" ],
      "venue" : "Brain Injury, vol. 17, no. 8, pp. 643–651, 2003.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Sensory substitution and the humanmachine interface",
      "author" : [ "P. Bach-y-Rita", "S.W. Kercel" ],
      "venue" : "Trends in Cognitive Science, vol. 7, no. 12, pp. 541– 546, 2003.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Towards an intelligent database system founded on the SP theory of computing and cognition",
      "author" : [ "J.G. Wolff" ],
      "venue" : "Data & Knowledge Engineering, vol. 60, pp. 596–624, 2007, see bit.ly/Yg2onp.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Medical diagnosis as pattern recognition in a framework of information compression by multiple alignment, unification and search",
      "author" : [ "——" ],
      "venue" : "Decision Support Systems, vol. 42, pp. 608–625, 2006, see bit.ly/XE7pRG.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Application of the SP theory of intelligence to the understanding of natural vision and the development of computer vision",
      "author" : [ "——" ],
      "venue" : "2013, in preparation. See bit.ly/Xj3nDY (PDF).",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A formal theory of inductive inference. Parts I and II",
      "author" : [ "R.J. Solomonoff" ],
      "venue" : "Information and Control, vol. 7, pp. 1–22 and 224–254, 1964.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1964
    }, {
      "title" : "Learning syntax and meanings through optimization and distributional analysis",
      "author" : [ "J.G. Wolff" ],
      "venue" : "Categories and Processes in Language Acquisition, Y. Levy, I. M. Schlesinger, and M. D. S. Braine, Eds. Hillsdale, NJ: Lawrence Erlbaum, 1988, pp. 179–215, see bit.ly/ZIGjyc.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Human Behaviour and the Principle of Least Effort. New York: Hafner, 1949, republished by Martino Publishing",
      "author" : [ "G.K. Zipf" ],
      "venue" : "Mansfield Centre,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "The Organization of Behaviour",
      "author" : [ "D.O. Hebb" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1949
    }, {
      "title" : "Towards practical, high-capacity, lowmaintenance information storage in synthesized DNA",
      "author" : [ "N. Goldman", "P. Bertone", "S. Chen", "C. Dessimoz", "E.M. LeProust", "B. Sipos", "E. Birney" ],
      "venue" : "Nature, vol. 494, no. 7435, pp. 77–80, 2013.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Understanding language without the ability to speak",
      "author" : [ "E.H. Lenneberg" ],
      "venue" : "Journal of Abnormal and Social Psychology, vol. 65, pp. 419–425, 1962.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "Randomness in arithmetic",
      "author" : [ "G.J. Chaitin" ],
      "venue" : "Scientific American, vol. 259, no. 1, pp. 80–85, 1988.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "of Cognitive Computing [2].",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "1 in [3], with permission.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "There is a relatively full account of the SP system in [4], an extended overview in [3], an account of its existing and expected benefits and applications in [5], a description of its foundations in [7], and an introduction to",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "There is a relatively full account of the SP system in [4], an extended overview in [3], an account of its existing and expected benefits and applications in [5], a description of its foundations in [7], and an introduction to",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 3,
      "context" : "There is a relatively full account of the SP system in [4], an extended overview in [3], an account of its existing and expected benefits and applications in [5], a description of its foundations in [7], and an introduction to",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 4,
      "context" : "the system in [8].",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "1 in [5], with permission.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "the “Web of Entities”: The need for standardisation in the representation of knowledge is recognised in writings about the semantic web (eg, [9]), the internet of things (eg, [10]), and in the Okkam project, aiming to create unique identifiers for a global web of entities.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : "the “Web of Entities”: The need for standardisation in the representation of knowledge is recognised in writings about the semantic web (eg, [9]), the internet of things (eg, [10]), and in the Okkam project, aiming to create unique identifiers for a global web of entities.",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 7,
      "context" : "• In support of that view is evidence that one part of the brain can take over the functions of another part (see, for example, [11], [12]).",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "• In support of that view is evidence that one part of the brain can take over the functions of another part (see, for example, [11], [12]).",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 10,
      "context" : "The SP system has strengths in the representation and processing of onedimensional patterns [4, Chapter 6], [5, Section 9], and it may be applied to medical diagnosis, viewed as a pattern recognition problem [14].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 11,
      "context" : "1] and also in [15].",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "In accordance with the principles of minimum length encoding [16], the SP system aims to minimise the overall size of G and E.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "5The similarity with research on grammatical inference is not accidental: the SP programme of research has grown out of earlier research developing computer models of language learning (see [17] and other publications that may be downloaded via bit.",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 11,
      "context" : "1] and [15].",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "[15] and in medical diagnosis [14], viewed as pattern recognition.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[15] and in medical diagnosis [14], viewed as pattern recognition.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : "rules, there is potential to create the facilities of a query language like SQL [13].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 14,
      "context" : "Zipf’s Human Behaviour and the Principle of Least Effort [18].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 15,
      "context" : "9 It may be seen as a development of Donald Hebb’s [19] concept of a ‘cell assembly’, with more precision about how structures may be shared, and other differences.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "org/wiki/Hebbian learning, retrieved 2013-12-23), described by Hebb [19] and adopted as the mechanism for learning in most artificial neural networks.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 16,
      "context" : "potential for high levels of parallelism, and with the attraction that DNA can be a means of storing information in a very compact form, and for very long periods [20].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 13,
      "context" : "3], [17], [4, Sections 2.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 13,
      "context" : "1 in [17], with permission.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 17,
      "context" : "15In brief, the evidence is that people with a physical handicap that prevents them producing intelligible speech can still learn to understand their native language [21], [22].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 1,
      "context" : "2 in [3], with permission.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 2,
      "context" : "Likewise, many of the potential benefits and applications of the SP system, described in [5] and including",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "such things as intelligent databases [13] and new approaches to medical diagnosis [14], may be realised with existing kinds of computer.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : "such things as intelligent databases [13] and new approaches to medical diagnosis [14], may be realised with existing kinds of computer.",
      "startOffset" : 82,
      "endOffset" : 86
    } ],
    "year" : 2014,
    "abstractText" : "This article is about how the SP theory of intelligence and its realisation in the SP machine may, with advantage, be applied to the management and analysis of big data. The SP system—introduced in the article and fully described elsewhere— may help to overcome the problem of variety in big data: it has potential as a universal framework for the representation and processing of diverse kinds of knowledge (UFK), helping to reduce the diversity of formalisms and formats for knowledge and the different ways in which they are processed. It has strengths in the unsupervised learning or discovery of structure in data, in pattern recognition, in the parsing and production of natural language, in several kinds of reasoning, and more. It lends itself to the analysis of streaming data, helping to overcome the problem of velocity in big data. Central in the workings of the system is lossless compression of information: making big data smaller and reducing problems of storage and management. There is potential for substantial economies in the transmission of data, for big cuts in the use of energy in computing, for faster processing, and for smaller and lighter computers. The system provides a handle on the problem of veracity in big data, with potential to assist in the management of errors and uncertainties in data. It lends itself to the visualisation of knowledge structures and inferential processes. A high-parallel, open-source version of the SP machine would provide a means for researchers everywhere to explore what can be done with the system and to create new versions of it.",
    "creator" : "LaTeX with hyperref package"
  }
}