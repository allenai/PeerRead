{
  "name" : "1610.02055.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Places: An Image Database for Deep Scene Understanding",
    "authors" : [ "Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Antonio Torralba", "Aude Oliva" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Scene understanding, scene classification, visual recognition, deep learning, deep feature, image dataset.\nF"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "What does it take to reach human-level performance with a machine-learning algorithm? In the case of supervised learning, the problem is two-fold. First, the algorithm must be suitable for the task, such as pattern classification in the case of object recognition [1], [2], pattern localization for object detection [3] or the necessity of temporal connections between different memory units for natural language processing [4], [5]. Second, it must have access to a training dataset of appropriate coverage (quasi-exhaustive representation of classes and variety of examplars) and density (enough samples to cover the diversity of each class). The optimal space for these datasets is often taskdependent, but the rise of multi-million-item sets has enabled unprecedented performance in many domains of artificial intelligence.\nThe successes of Deep Blue in chess, Watson in “Jeopardy!”, and AlphaGo in Go against their expert human opponents may thus be seen as not just advances in algorithms, but the increasing availability of very large datasets: 700,000, 8.6 million, and 30 million items, respectively [6]– [8]. Convolutional Neural Networks [1], [9] have likewise achieved near human-level visual recognition, trained on 1.2 million object [10]–[12] and 2.5 million scene images [2]. Expansive coverage of the space of classes and samples allows getting closer to the right ecosystem of data that a natural system, like a human, would experience.\nHere we describe the Places Database, a quasi-exhaustive repository of 10 million scene photographs, labeled with 476 scene semantic categories and attributes, comprising the types of visual environments encountered in the world. Image samples are shown in Fig. 1. In the context of Places,\n• B. Zhou, A. Khosla, A.Torralba, A.Oliva are with the Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, USA. • A. Lapedriza is with Universitat Oberta de Catalunya, Spain.\nwe explain the steps to create high-quality datasets enabling the remarkable feats of machine-learning algorithms."
    }, {
      "heading" : "2 PLACES DATABASE",
      "text" : ""
    }, {
      "heading" : "2.1 Coverage of the categorical space",
      "text" : "The primary asset of a high-quality dataset is an expansive coverage of the categorical space we want to learn. The strategy of Places is to provide an exhaustive list of the categories of environments encountered in the world, bounded by spaces where a human body would fit (e.g. closet, shower). The SUN (Scene UNderstanding) dataset [13] provided that initial list of semantic categories. The SUN dataset was built around a quasi-exhaustive list of scene categories with different functionalities, namely categories with unique identities in discourse. Through the use of WordNet [14], the SUN database team selected 70,000 words and concrete terms that described scenes, places and environments that can be used to complete the phrase “I am in a place”, or “let’s go to the/a place”. Most of the words referred to basic and entry-level names ( [15]), resulting in a corpus of 900 different scene categories after bundling together synonyms, and separating classes described by the same word but referring to different environments (e.g. inside and outside views of churches). Details about the building of that initial corpus can be found in [13]. Places Database has inherited the same list of scene categories from the SUN dataset."
    }, {
      "heading" : "2.2 Construction of the database",
      "text" : ""
    }, {
      "heading" : "2.2.1 Step 1: Downloading images using scene category and adjectives",
      "text" : "From online image search engines (Google Images, Bing Images, and Flickr), candidate images were downloaded using a query word from the list of scene classes provided by the SUN database [13]. In order to increase the diversity of visual appearances in the Places dataset (see Fig. 2),\nar X\niv :1\n61 0.\n02 05\n5v 1\n[ cs\n.C V\n] 6\nO ct\n2 01\n6\n2\neach scene class query was combined with 696 common English adjectives (e.g., messy, spare, sunny, desolate, etc.). About 60 million images (color images of at least 200×200 pixels size) with unique URLs were identified. Importantly, the Places and SUN datasets are complementary: PCAbased duplicate removal was conducted within each scene category in both databases so that they do not contain the same images."
    }, {
      "heading" : "2.2.2 Step 2: Labeling images with ground truth category",
      "text" : "Image ground truth label verification was done by crowdsourcing the task to Amazon Mechanical Turk (AMT). Fig.3 illustrates the experimental paradigm used: AMT workers were each given instructions relating to a particular image category at a time (e.g. cliff), with a definition and samples of true and false images. Workers then performed a go/no-go categorical task (Fig.3). The experimental interface displayed a central image, flanked by smaller version of images the worker had just responded to, on the left, and will respond to next, on the right. Information gleaned from construction of the SUN dataset suggests that the first iteration of labeling will show that more than 50% of the the downloaded images are not true exemplars of the category. As illustrated in Fig.3, the default answer is set to No (see images with bold red contours), so the worker can more easily press the space bar to move the majority of No images forward. Whenever a true category exemplar appears in the center, the worker can press a specific key to mark it as a positive exemplar (responding yes to the question: “is this a place term”). Reaction time from the moment the image is centrally placed to the space bar or key press is recorded. The interface also allows moving backwards to revise previous annotations. Each AMT HIT\n3\n(Human Intelligence Task, one assignment for one worker), consisted of 750 images for manual annotation. A control set of 30 positive samples and 30 negative samples with ground-truth category labels from the SUN database were intermixed in the HIT as well. Only worker HITs with an accuracy of 90% or higher on these control images were kept.\nThe positive images resulting from the first cleaning iteration were sent for a second iteration of cleaning. We used the same task interface but with the default answer set to Yes. In this second iteration, 25.4% of the images were relabeled as No. We tested a third iteration on a few exemplars but did not pursue it further as the percentage of images relabeled as No was not significant.\nAfter the two iterations of annotation, we collected one scene label for 7,076,580 images pertaining to 476 scene categories. As expected, the number of images per scene category vary greatly (i.e. there are many more images of bedroom than cave on the web). There were 413 scene categories that ended up with at least 1000 exemplars, and 98 scene categories with more than 20,000 exemplars."
    }, {
      "heading" : "2.2.3 Step 3: Scaling up the dataset using a classifier",
      "text" : "As a result of the previous round of image annotation, there were 53 million remaining downloaded images not assigned to any of the 476 scene categories (e.g. a bedroom picture could have been downloaded when querying images for living-room category, but marked as negative by the AMT worker). Therefore, a third annotation task was designed to re-classify then re-annotate those images, using a semiautomatic bootstrapping approach.\nA deep learning-based scene classifier, AlexNet [1], was trained to classify the remaining 53 million images: We first randomly selected 1,000 images per scene category as training set and 50 images as validation set (for the 413 categories which had more than 1000 samples). AlexNet achieved 32% scene classification accuracy on the validation set after training and was then used to classify the 53\nmillion images. We used the predicted class score by the AlexNet to rank the images within one scene category as follow: for a given category with too few exemplars, the top ranked images with predicted class confidence higher than 0.8 were sent to AMT for a third round of manual annotation using the same interface shown in Fig.3. The default answer was set to No.\nAfter completing the third round of AMT annotation, the distribution of the number of images per category flattened out: 401 scene categories had more than 5,000 images per category and 240 scene categories had more than 20,000 images. Totally there are about 3 million images added into the dataset."
    }, {
      "heading" : "2.2.4 Step 4: Improving the separation of similar classes",
      "text" : "Despite the initial effort to bundle synonyms from WordNet, the scene list from the SUN database still contained categories with very close synonyms (e.g. ‘ski lodge’ and ‘ski resort’, or ‘garbage dump’ and ‘landfill’). We identified 46 synonym pairs like these and merged their images into a single category.\n4\nAdditionally, some scene categories are easily confused with blurry categorical boundaries, as illustrated in Fig. 5. This means that answering the question “Does image I belong to class A?” might be difficult. It is easier to answer the question “Does image I belong to class A or B?” In that case, the decision boundary becomes clearer for a human observer and it also gets closer to the final task that a computer system will be trained to solve.\nIndeed, in the previous three steps of the AMT annotation, it became apparent that workers were confused with some pairs of scene categories, for instance, putting images of ‘canyon’ and ‘butte’ into ‘mountain’, or putting ‘jacuzzi’ into ‘swimming pool indoor’, mixing images of ‘pond’ and ’lake’, ‘volcano’ and ‘mountain’, ‘runway’ and ‘landing deck’, ‘highway and road’, ‘operating room’ and ‘hospital room’, etc. In the whole set of categories, we identified 53 such ambiguous pairs.\nTo further differentiate the images from the categories with shared content, we designed a new interface (Fig. 4) for a fourth step of annotation. We combined exemplar images from the two categories with shared content (such as art school and art studio), and asked the AMT workers to classify images into either of the categories or neither of them.\nAfter the four steps of annotations, the Places database was finalized with over 10 millions labeled exemplars (10,624,928 images) from 434 place categories."
    }, {
      "heading" : "2.3 Scene-Centric Datasets",
      "text" : "Scene-centric datasets correspond to images labeled with a scene, or place name, as opposed to an object name. Fig. 6 illustrates the differences among the number of images found in Places, ImageNet and SUN for a set of scene categories common to all three datasets. Places Database is the largest scene-centric image dataset so far."
    }, {
      "heading" : "2.3.1 Defining the Benchmarks of the Places",
      "text" : "Here we describe four subsets of Places as benchmarks. Places205 and Places88 are from [2]. Two new benchmarks were added: from the 434 categories, we selected 365 categories with more than 4000 images each to create Places365-Standard and Places365-Challenge.\nPlaces365-Standard has 1,803,460 training images with the image number per class varying from 3,068 to 5,000. The validation set has 50 images per class and the test set has 900 images per class. Note that the experiments in this paper are reported on Places365-Standard.\nPlaces365-Challenge contains the same categories as Places365-Standard, but the training set is significantly larger with a total of 8 million training images. The validation set and testing set are the same as the Places365Standard. This subset was released for the Places Challenge 20161 held in conjunction with the European Conference on Computer Vision (ECCV) 2016, as part of the ILSVRC Challenge.\nPlaces205. Places205, described in [2], has 2.5 million images from 205 scene categories. The image number per class varies from 5,000 to 15,000. The training set has 2,448,873 total images, with 100 images per category for the validation set and 200 images per category for the test set.\nPlaces88. Places88 contains the 88 common scene categories among the ImageNet [12], SUN [13] and Places205 databases. Note that Places88 contains only the images obtained in round 2 of annotations, from the first version of Places used in [2]. We call the corresponding subsets Places88, ImageNet88 and SUN88. These subsets are used to compare performances across different scene-centric databases, as the three datasets contain different exemplars per category. Note that finding correspondences between the classes defined in ImageNet and Places brings some challenges. ImageNet follows the WordNet definitions, but\n1. http://places2.csail.mit.edu/challenge.html\nsome WordNet definitions are not always appropriate for describing places. For instance, the class ’elevator’ in ImageNet refers to an object. In Places, ’elevator’ takes different meanings depending on the location of the observer: elevator door, elevator interior, or elevator lobby. Many categories in ImageNet do not differentiate between indoor and outdoor (e.g., ice-skating rink) while in Places, indoor and outdoor versions are separated as they do not necessarily afford the same function."
    }, {
      "heading" : "2.3.2 Dataset Diversity",
      "text" : "Given the types of images found on the internet, some categories will be more biased than others in terms of viewpoints, types of objects, or even image style [16]. However, bias can be compensated with a high diversity of images (with many appearances represented in the dataset). In the next section, we describe a measure of dataset diversity to compare how diverse images from three scenecentric datasets (Places88, SUN88 and ImageNet88) are.\nComparing datasets is an open problem. Even datasets covering the same visual classes have notable differences providing different generalization performances when used to train a classifier [16]. Beyond the number of images and categories, there are aspects that are important but difficult to quantify, like the variability in camera poses, in decoration styles or in the type of objects that appear in the scene.\nAlthough the quality of a database is often task dependent, it is reasonable to assume that a good database should be dense (with a high degree of data concentration), and diverse (it should include a high variability of appearances and viewpoints). Imagine, for instance, a dataset composed of 100,000 images all taken within the same bedroom. This dataset would have a very high density but a very low diversity as all the images will look very similar. An ideal dataset, expected to generalize well, should have high diversity as well. While one can achieve high density by collecting a large number of images, diversity is not an obvious quantity to estimate in image sets, as it assumes some notion of similarity between images. One way to estimate similarity is to ask the question are these two images\nsimilar? However, similarity in the wild is a subjective and loose concept, as two images can be viewed as similar if they contain similar objects, and/or have similar spatial configurations, and/or have similar decoration styles and so on. A way to circumvent this problem is to define relative measures of similarity for comparing datasets.\nSeveral measures of diversity have been proposed, particularly in biology for characterizing the richness of an ecosystem (see [17] for a review). Here, we propose to use a measure inspired by the Simpson index of diversity [18]. The Simpson index measures the probability that two random individuals from an ecosystem belong to the same species. It is a measure of how well distributed the individuals across different species are in an ecosystem, and it is related to the entropy of the distribution. Extending this measure for evaluating the diversity of images within a category is non-trivial if there are no annotations of subcategories. For this reason, we propose to measure the relative diversity of image datasets A and B based on the following idea: if set A is more diverse than set B, then two random images from set B are more likely to be visually similar than two random samples from A. Then, the diversity of A with respect to B can be defined as DivB(A) = 1−p(d(a1, a2) < d(b1, b2)), where a1, a2 ∈ A and b1, b2 ∈ B are randomly selected. With this definition of relative diversity we have that A is more diverse than B if, and only if, DivB(A) > DivA(B). For an arbitrary number of datasets, A1, ..., AN :\nDivA2,...,AN (A1) = 1−p(d(a11, a12) < min i=2:N d(ai1, ai2))\n(1)\nwhere ai1, ai2 ∈ Ai are randomly selected. We measured the relative diversities between SUN, ImageNet and Places using AMT. Workers were presented with different pairs of images and they had to select the pair that contained the most similar images. The pairs were randomly sampled from each database. Each trial was composed of 4 pairs from each database, giving a total of 12 pairs to choose from. We used 4 pairs per\n6 a) b)\nDiversity 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nN u\nm b\ne r\no f c a\nte g\no ri e\ns\n0\n5\n10\n15\n20\n25\n30\n35\n40\nc)\nFig. 7. Examples of pairs for the diversity experiment for a) playground and b) bedroom. Which pair shows the most similar images? The bottom pairs were chosen in these examples. c) Histogram of relative diversity per each category (88 categories) and dataset. Places (in blue line) contains the most diverse set of images, then ImageNet (in red line) and the lowest diversity is in the SUN database (in yellow line) as most images are prototypical of their class.\ndatabase to increase the chances of finding a similar pair and avoiding users having to skip trials. AMT workers had to select the most similar pair on each trial. We ran 40 trials per category and two observers per trial, for the 88 categories in common between ImageNet, SUN and Places databases. Fig. 7.a-b shows some examples of pairs from the diversity experiments for the scene categories playground (a) and bedroom (b). In the figure only one pair from each database is shown. We observed that different annotators were consistent in deciding whether a pair of images was more similar than another pair of images.\nFig. 7.c shows the histograms of relative diversity for all the 88 scene categories common to the three databases. If the three datasets were identical in terms of diversity, the average diversity should be 2/3 for the three datasets. Note that this measure of diversity is a relative measure between the three datasets. In the experiment, users selected pairs from the SUN database to be the closest to each other 50% of the time, while the pairs from the Places database were judged to be the most similar only on 17% of the trials. ImageNet pairs were selected 33% of the time.\nThe results show that there is a large variation in terms of diversity among the three datasets, showing Places to be the most diverse of the three datasets. The average relative diversity on each dataset is 0.83 for Places, 0.67 for ImageNet and 0.50 for SUN. To illustrate, the categories with the largest variation in diversity across the three datasets were playground, veranda and waiting room."
    }, {
      "heading" : "3 CONVOLUTIONAL NEURAL NETWORKS FOR SCENE CLASSIFICATION",
      "text" : "Given the impressive performance of the deep Convolutional Neural Networks (CNNs), particularly on the ImageNet benchmark [1], [12], we choose three popular CNN architectures, AlexNet [1], GoogLeNet [19], and VGG 16 convolutional-layer CNN [20], then train them on Places205 and Places365-Standard respectively to create baseline CNN models. The trained CNNs are named as PlacesSubset-CNN, i.e., Places205-AlexNet or Places365VGG.\nAll the Places-CNNs presented here were trained using the Caffe package [21] on Nvidia GPUs Tesla K40 and Titan X2. Additionally, given the recent breakthrough performances of the Residual Network (ResNet) on ImageNet classification [22], we further fine-tuned ResNet152 on the Places365-Standard (termed as Places365-ResNet) and compared it with the other trained-from-scratch PlacesCNNs for scene classification."
    }, {
      "heading" : "3.1 Results on Places205 and Places365",
      "text" : "After training the various Places-CNNs, we used the final output layer of each network to classify the test set images of Places205 and SUN205 (see [2]). The classification results for Top-1 accuracy and Top-5 accuracy are listed in Table 1. As a baseline comparison, we show the results of a linear SVM trained on ImageNet-CNN features of 5000 images per category in Places205 and 50 images per category in SUN205 respectively.\nPlaces-CNNs perform much better than the ImageNet feature+SVM baseline while, as expected, Places205GoogLeNet and Places205-VGG outperformed Places205AlexNet with a large margin due to their deeper structures. To date (Oct 2, 2016) the top ranked results on the test set of Places205 leaderboard3 is 64.10% on Top-1 accuracy and 90.65% on Top-5 accuracy. Note that for the test set of SUN205, we didn’t fine-tune the Places-CNNs on the training set of SUN205, as we directly evaluated them on the test set of SUN.\nWe further evaluated the baseline Places365-CNNs on the validation set and test set of Places365 shown in Fig.2. Places365-VGG and Places365-ResNet have similar top performances compared with the other two CNNs4. Even if\n2. All the Places-CNNs are available at https://github.com/metalbubble/ places365\n3. http://places.csail.mit.edu/user/leaderboard.php 4. The performance of the ResNet might result from fine-tuning or\nunder-training, as the ResNet is not trained from scratch.\n7\nTABLE 1 Classification accuracy on the test set of Places205 and the test set of SUN205. We use the class score averaged over 10-crops of each test image to classify the image. ∗ shows the top 2 ranked results from the Places205 leaderboard.\nTest set of Places205 Test set of SUN205 Top-1 acc. Top-5 acc. Top-1 acc. Top-5 acc.\nImageNet-AlexNet feature+SVM 40.80% 70.20% 49.60% 80.10% Places205-AlexNet 50.04% 81.10% 67.52% 92.61% Places205-GoogLeNet 55.50% 85.66% 71.6% 95.01% Places205-VGG 58.90% 87.70% 74.6% 95.92% SamExynos∗ 64.10% 90.65% - - SIAT MMLAB∗ 62.34% 89.66% - -\nPlaces365 has 160 more categories than Places205, the Top5 accuracy of the Places205-CNNs (trained on the previous version of Places [2]) on the test set only drops by 2.5%.\nFig.8 shows the responses to examples correctly predicted by the Places365-VGG. Most of the Top-5 responses are very relevant to the scene description. Some failure or ambiguous cases are shown in Fig.9: Broadly, we can identify two kinds of misclassification given the current label attribution of Places: 1) less-typical activities happening in a scene, such as taking group photo in a construction site and camping in a junkyard; 2) images composed of multiple scene parts, which make one ground-truth scene label not sufficient to describe the whole environment. These illustrate the need to have multi-ground truth labels for describing environments.\nIt is important to emphasize that for many scene categories the Top-1 accuracy might be an ill-defined measure: environments are inherently multi-labels in terms of their semantic description. Different observers will use different terms to refer to the same place, or different parts of the same environment, and all the labels might fit well the description of the scene. This is obvious in the examples of Fig.9. Future development of the Places database, and the Places Challenge, will explore to assign multiple ground truth labels or free-form sentences to images to better capture the richness of visual descriptions inherent to environments."
    }, {
      "heading" : "3.2 Web-demo for Scene Recognition",
      "text" : "Based on the Places-CNN we trained, we created a webdemo for scene recognition5, accessible through a computer browser or mobile phone. People can upload photos to the web-demo to predict the type of environment, with the 5 most likely semantic categories, and relevant scene attributes. Two screenshots of the prediction result on the mobile phone are shown in Fig.10. Note that people can submit feedback about the result. The top-5 recognition accuracy of our recognition web-demo in the wild is about 72% (from the 9,925 anonymous feedbacks dated from Oct.19, 2014 to May 5, 2016), which is impressive given that people uploaded all kinds of photos from real-life\n5. http://places.csail.mit.edu/demo.html\nGT: construction site top-1: martial arts gym (0.157) top-2: stable (0.156) top-3: boxing ring (0.091) top-4: locker room (0.090) top-5: basketball court (0.056)\nGT: aquarium top-1: restaurant (0.213) top-2: ice cream parlor (0.139) top-3: coffee shop (0.138) top-4: pizzeria (0.085) top-5: cafeteria (0.078)\nGT: junkyard top-1: campsite (0.306) top-2: sandbox (0.276) top-3: beer garden (0.052) top-4: market outdoor (0.035) top-5: flea market indoor (0.033)\nGT: lagoon top-1: balcony interior (0.136) top-2: beach house (0.134) top-3: boardwalk (0.123) top-4: roof garden (0.103) top-5: restaurant patio (0.068)\nFig. 9. Examples of predictions rated as incorrect in the validation set by the Places365-VGG. GT states for ground truth label. Note that some of the top5 responses are often not wrong per se, predicting semantic categories near by the GT category. See the text for details.\nand not necessarily places-like photos (these results are for Places205-AlexNet as the back-end prediction model in the demo)."
    }, {
      "heading" : "3.3 Generic Visual Features from ImageNet-CNNs and Places-CNNs",
      "text" : "We further used the activation from the trained PlacesCNNs as generic features for visual recognition tasks using different image classification benchmarks. Activations from the higher-level layers of a CNN, also termed deep features, have proven to be effective generic features with state-ofthe-art performance on various image datasets [23], [24].\n8\nTABLE 2 Classification accuracy on the validation set and test set of Places365. We use the class score averaged over 10-crops of each testing image to classify the image.\nValidation Set of Places365 Test Set of Places365 Top-1 acc. Top-5 acc. Top-1 acc. Top-5 acc.\nPlaces365-AlexNet 53.17% 82.89% 53.31% 82.75% Places365-GoogLeNet 53.63% 83.88% 53.59% 84.01% Places365-VGG 55.24% 84.91% 55.19% 85.01% Places365-ResNet 54.74% 85.08% 54.65% 85.07%\nGT: cafeteria top-1: cafeteria (0.179) top-2: restaurant (0.167) top-3: dining hall (0.091) top-4: coffee shop (0.086) top-5: restaurant patio (0.080)\nGT: natural canal top-1: swamp (0.529) top-2: marsh (0.232) top-3: natural canal (0.063) top-4: lagoon (0.047) top-5: rainforest (0.029)\nGT: chalet top-1: ski resort (0.141) top-2: ice floe (0.129) top-3: igloo (0.114) top-4: balcony exterior (0.103) top-5: courtyard (0.083)\nGT: classroom top-1: locker room (0.585) top-2: lecture room (0.135) top-3: conference center (0.061) top-4: classroom (0.033) top-5: elevator door (0.025)\nGT: creek top-1: forest broadleaf (0.307) top-2: forest path (0.208) top-3: creek (0.086) top-4: rainforest (0.076) top-5: cemetery (0.049)\nGT: crosswalk top-1: crosswalk (0.720) top-2: plaza (0.060) top-3: street (0.055) top-4: shopping mall indoor (0.039) top-5: bazaar outdoor (0.021)\nGT: drugstore top-1: supermarket (0.286) top-2: hardware store (0.248) top-3: drugstore (0.120) top-4: department store (0.087) top-5: pharmacy (0.052)\nGT: greenhouse indoor top-1: greenhouse indoor (0.479) top-2: greenhouse outdoor (0.055) top-3: botanical garden (0.044) top-4: assembly line (0.025) top-5: vegetable garden (0.022)\nGT: market outdoor top-1: promenade (0.569) top-2: bazaar outdoor (0.137) top-3: boardwalk (0.118) top-4: market outdoor (0.074) top-5: flea market indoor (0.029)\nFig. 8. The predictions given by the Places365-VGG for the images from the validation set. The ground-truth label (GT) and the top 5 predictions are shown. The number beside each label indicates the prediction confidence.\nBut most of the deep features are from the CNNs trained on ImageNet, which is mostly an object-centric dataset.\nHere we evaluated the classification performances of the deep features from object-centric CNNs and scenecentric CNNs in a systematic way. The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].\nAll of the experiments follow the standards in those papers. In the SUN397 experiment [13], the training set size is 50 images per category. Experiments were run on 5 splits of the training set and test set given in the dataset. In the MIT Indoor67 experiment [25], the training set size is 100 images per category. The experiment is run on the split of the training set and test set given in the dataset. In the Scene15 experiment [26], the training set size is 50 images per category. Experiments are run on 10 random splits of the training set and test set. In the SUN Attribute experiment [27], the training set size is 150 images per attribute. The reported result is the average precision. The splits of the training set and test set are given in the paper. In Caltech101 and Caltech256 experiment [28], [29], the training set size is 30 images per category. The experiments are run on 10 random splits of the training set and test set. In the Stanford Action40 experiment [30], the training set size is 100 images per category. Experiments are run\n9 on 10 random splits of the training set and test set. The reported result is the classification accuracy. In the UIUC Event8 experiment [31], the training set size is 70 images per category and the test set size is 60 images per category. The experiments are run on 10 random splits of the training set and test set.\nPlaces-CNNs and ImageNet-CNNs have the same network architectures for AlexNet, GoogLeNet, and VGG, but they are trained on scene-centric data and object-centric data respectively. For AlexNet and VGG, we used the 4096-dimensional feature vector from the activation of the Fully Connected Layer (fc7) of the CNN. For GoogLeNet, we used the 1024-dimensional feature vector from the response of the global average pooling layer before softmax producing the class predictions. The classifier in all of the experiments is a linear SVM with the default parameter for all of the features.\nTable 3 summarizes the classification accuracy on various datasets for the deep features of Places-CNNs and the deep features of the ImageNet-CNNs. Fig.11 plots the classification accuracy for different visual features on the SUN397 database over different numbers of training samples per category. The classifier is a linear SVM with the same default parameters for the two deep feature layers (C=1) [32]. The Places-CNN features show impressive performance on scene-related datasets, outperforming the ImageNet-CNN features. On the other hand, the ImageNet-CNN features show better performance on object-related image datasets. Importantly, our comparison shows that Places-CNN and ImageNet-CNN have complementary strengths on scenecentric tasks and object-centric tasks, as expected from the type of the datasets used to train these networks. On the other hand, the deep features from the Places365VGG achieve the best performance (63.24%) on the most challenging scene classification dataset SUN397, while the deep features of Places205-VGG performs the best on the MIT Indoor67 dataset. As far as we know, they are the state-of-the-art scores achieved by a single feature + linear SVM on those two datasets. Furthermore, we merge the 1000 classes from the ImageNet and the 365 classes from the Places365-Standard to train a VGG (Hybrid1365-VGG). The deep feature from the Hybrid1365-VGG achieves the best score averaged over all the eight image datasets."
    }, {
      "heading" : "3.4 Visualization of the Internal Units and the CNNs",
      "text" : "Through the visualization of the units responses for various levels of network layers, we can have a better understanding of what has been learned inside CNNs and what are the differences between the object-centric CNN trained on ImageNet and the scene-centric CNN trained on Places given that they share the same architecture (here we use AlexNet). Following the methodology in [33] we estimated the receptive fields of the units in the Places-CNN and ImageNet-CNN. Then we segmented the images with high unit activation using the estimated receptive fields. The image segmentation results by the receptive fields of units\n1 5 10 20 50 0\n10\n20\n30\n40\n50\n60\n70\nNumber of training samples per category\nC la\nss ifi\nca tio\nn ac\ncu ra\ncy\nCombined kernel [37.5] HoG2x2 [26.3] DenseSIFT [23.5] Ssim [22.7] Geo texton [22.1] Texton [21.6] Gist [16.3] LBP [14.7] ImageNet−AlexNet [42.6] Places205−AlexNet [54.3] Places365−VGG [63.24]\nFig. 11. Classification accuracy on the SUN397 Dataset. We compare the deep features of Places365VGG, Places205-AlexNet (result reported in [2]), and ImageNet-AlexNet, to those hand-designed features. The deep features of Places365-VGG outperforms other deep features and hand-designed features in large margins. Results of other hand-designed features/kernels are fetched from [13].\nfrom different layers are shown in Fig.12. We can see that from pool1 to pool5, the units detect visual concepts from low-level edge/texture to high-level object/scene parts. Furthermore, in the object-centric ImageNet-CNN there are more units detecting object parts such as dog and people’s heads in the pool5 layer, while in the scene centric PlacesCNN there are more units detecting scene parts such as bed, chair, or buildings in the pool5 layer.\nThus the specialty of the units in the object-centric CNN and scene-centric CNN yield very different performances of generic visual features on a variety of recognition benchmarks (object-centric datasets vs scene-centric datasets) in Table 3.\nWe further synthesized preferred input images for the Places-CNN by using the image synthesis technique proposed in [34]. This method uses a learned prior deep generator network to generate images which maximize the final class activation or the intermediate unit activation of the Places-CNN. The synthetic images for 50 scene categories are shown in Fig.13. These abstract image contents reveal the knowledge of the specific scene learned and memorized by the Places-CNN: examples include the buses within a road environment in the bus station, and the tents surrounded by forest-types of features for the campsite. Here we used Places365-AlexNet (other Places365-CNNs generated similar results). We further used the synthesis technique to generate the images preferred by the units in the pool5 layer of Places365-AlexNet. As shown in Fig.14, the synthesized images are very similar to the segmented image regions by the estimated receptive field of the units.\n10\n11\n12"
    }, {
      "heading" : "4 CONCLUSION",
      "text" : "From the Tiny Image dataset [35], to ImageNet [11] and Places [2], the rise of multi-million-item dataset initiatives and other densely labeled datasets [36]–[39] have enabled data-hungry machine learning algorithms to reach nearhuman semantic classification of visual patterns, like objects and scenes. With its high-coverage and high-diversity of exemplars, Places offers an ecosystem of visual context to guide progress on currently intractable visual recognition problems. Such problems could include determining the actions happening in a given environment, spotting inconsistent objects or human behaviors for a particular place, and predicting future events or the cause of events given a scene."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The authors would like to thank Santani Teng, Zoya Bylinskii, Mathew Monfort and Caitlin Mullin for comments on the paper. Over the years, the Places project was supported by the National Science Foundation under Grants No. 1016862 to A.O and No. 1524817 to A.T; ONR N000141613116 to A.O; as well as MIT Big Data Initiative at CSAIL, Toyota, Google, Xerox and Amazon Awards, and a hardware donation from NVIDIA Corporation, to A.O and A.T. B.Z is supported by a Facebook Fellowship."
    } ],
    "references" : [ {
      "title" : "Imagenet classification with deep convolutional neural networks.",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Learning deep features for scene recognition using places database",
      "author" : [ "B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva" ],
      "venue" : "In Advances in Neural Information Processing Systems, 2014.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "2014.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, 1997.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Deep blue",
      "author" : [ "M. Campbell", "A.J. Hoane", "F.-h. Hsu" ],
      "venue" : "Artificial intelligence, 2002.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Watson: beyond jeopardy!",
      "author" : [ "D. Ferrucci", "A. Levas", "S. Bagchi", "D. Gondek", "E.T. Mueller" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree search",
      "author" : [ "D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot" ],
      "venue" : "Nature, 2016.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, 1998.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "Proc. CVPR, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei" ],
      "venue" : "Proc. CVPR, 2009.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein" ],
      "venue" : "International Journal of Computer Vision, 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sun database: Large-scale scene recognition from abbey to zoo",
      "author" : [ "J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba" ],
      "venue" : "Proc. CVPR, 2010.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "G.A. Miller" ],
      "venue" : "Communications of the ACM, vol. 38, no. 11, pp. 39–41, 1995.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Pictures and names: Making the connection",
      "author" : [ "P. Jolicoeur", "M.A. Gluck", "S.M. Kosslyn" ],
      "venue" : "Cognitive psychology, 1984.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Unbiased look at dataset bias",
      "author" : [ "A. Torralba", "A.A. Efros" ],
      "venue" : "Proc. CVPR, 2011.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Indices of diversity and evenness",
      "author" : [ "C. Heip", "P. Herman", "K. Soetaert" ],
      "venue" : "Oceanis, 1998.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Measurement of diversity.",
      "author" : [ "E.H. Simpson" ],
      "venue" : "Nature,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1949
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "Proc. CVPR, 2015.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556, 2014.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Caffe: An open source convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia" ],
      "venue" : "http://caffe.berkeleyvision.org/, 2013.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "Proc. CVPR, 2016.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Decaf: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "2014.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Cnn features off-the-shelf: an astounding baseline for recognition",
      "author" : [ "A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson" ],
      "venue" : "arXiv preprint arXiv:1403.6382, 2014.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Recognizing indoor scenes",
      "author" : [ "A. Quattoni", "A. Torralba" ],
      "venue" : "Proc. CVPR, 2009.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories",
      "author" : [ "S. Lazebnik", "C. Schmid", "J. Ponce" ],
      "venue" : "Proc. CVPR, 2006.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Sun attribute database: Discovering, annotating, and recognizing scene attributes",
      "author" : [ "G. Patterson", "J. Hays" ],
      "venue" : "Proc. CVPR, 2012.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories",
      "author" : [ "L. Fei-Fei", "R. Fergus", "P. Perona" ],
      "venue" : "Computer Vision and Image Understanding, 2007.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Caltech-256 object category dataset",
      "author" : [ "G. Griffin", "A. Holub", "P. Perona" ],
      "venue" : "2007.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Human action recognition by learning bases of action attributes and parts",
      "author" : [ "B. Yao", "X. Jiang", "A. Khosla", "A.L. Lin", "L. Guibas", "L. Fei-Fei" ],
      "venue" : "Proc. ICCV, 2011.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "What, where and who? classifying events by scene and object recognition",
      "author" : [ "L.-J. Li", "L. Fei-Fei" ],
      "venue" : "Proc. ICCV, 2007.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "LIBLINEAR: A library for large linear classification",
      "author" : [ "R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin" ],
      "venue" : "2008.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Object detectors emerge in deep scene cnns",
      "author" : [ "B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba" ],
      "venue" : "International Conference on Learning Representations, 2015.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks",
      "author" : [ "A. Nguyen", "A. Dosovitskiy", "T. Yosinski", "Jason band Brox", "J. Clune" ],
      "venue" : "arXiv preprint arXiv:1605.09304, 2016.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "80 million tiny images: A large data set for nonparametric object and scene recognition",
      "author" : [ "A. Torralba", "R. Fergus", "W.T. Freeman" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence, 2008.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollár", "C.L. Zitnick" ],
      "venue" : "European Conference on Computer Vision. Springer, 2014, pp. 740–755.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Semantic understanding of scenes through the ade20k dataset",
      "author" : [ "B. Zhou", "H. Zhao", "X. Puig", "S. Fidler", "A. Barriuso", "A. Torralba" ],
      "venue" : "arXiv preprint arXiv:1608.05442, 2016.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The pascal visual object classes (voc) challenge",
      "author" : [ "M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : "Int’l Journal of Computer Vision, 2010.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The cityscapes dataset for semantic urban scene understanding",
      "author" : [ "M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele" ],
      "venue" : "arXiv preprint arXiv:1604.01685, 2016.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "First, the algorithm must be suitable for the task, such as pattern classification in the case of object recognition [1], [2], pattern localization for object detection [3] or the necessity of temporal connections between different memory units for natural language processing [4], [5].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "First, the algorithm must be suitable for the task, such as pattern classification in the case of object recognition [1], [2], pattern localization for object detection [3] or the necessity of temporal connections between different memory units for natural language processing [4], [5].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 2,
      "context" : "First, the algorithm must be suitable for the task, such as pattern classification in the case of object recognition [1], [2], pattern localization for object detection [3] or the necessity of temporal connections between different memory units for natural language processing [4], [5].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 3,
      "context" : "First, the algorithm must be suitable for the task, such as pattern classification in the case of object recognition [1], [2], pattern localization for object detection [3] or the necessity of temporal connections between different memory units for natural language processing [4], [5].",
      "startOffset" : 277,
      "endOffset" : 280
    }, {
      "referenceID" : 4,
      "context" : "6 million, and 30 million items, respectively [6]– [8].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "6 million, and 30 million items, respectively [6]– [8].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "Convolutional Neural Networks [1], [9] have likewise achieved near human-level visual recognition, trained on 1.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "Convolutional Neural Networks [1], [9] have likewise achieved near human-level visual recognition, trained on 1.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "2 million object [10]–[12] and 2.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "2 million object [10]–[12] and 2.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "5 million scene images [2].",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "The SUN (Scene UNderstanding) dataset [13] provided that initial list of semantic categories.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : "Through the use of WordNet [14], the SUN database team selected 70,000 words and concrete terms that described scenes, places and environments that can be used to complete the phrase “I am in a place”, or “let’s go to the/a place”.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "Most of the words referred to basic and entry-level names ( [15]), resulting in a corpus of 900 different scene categories after bundling together synonyms, and separating classes described by the same word but referring to different environments (e.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "Details about the building of that initial corpus can be found in [13].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : "1 Step 1: Downloading images using scene category and adjectives From online image search engines (Google Images, Bing Images, and Flickr), candidate images were downloaded using a query word from the list of scene classes provided by the SUN database [13].",
      "startOffset" : 252,
      "endOffset" : 256
    }, {
      "referenceID" : 0,
      "context" : "A deep learning-based scene classifier, AlexNet [1], was trained to classify the remaining 53 million images: We first randomly selected 1,000 images per scene category as training set and 50 images as validation set (for the 413 categories which had more than 1000 samples).",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "Places205 and Places88 are from [2].",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "Places205, described in [2], has 2.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "Places88 contains the 88 common scene categories among the ImageNet [12], SUN [13] and Places205 databases.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "Places88 contains the 88 common scene categories among the ImageNet [12], SUN [13] and Places205 databases.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "Note that Places88 contains only the images obtained in round 2 of annotations, from the first version of Places used in [2].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : "2 Dataset Diversity Given the types of images found on the internet, some categories will be more biased than others in terms of viewpoints, types of objects, or even image style [16].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 14,
      "context" : "Even datasets covering the same visual classes have notable differences providing different generalization performances when used to train a classifier [16].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 15,
      "context" : "Several measures of diversity have been proposed, particularly in biology for characterizing the richness of an ecosystem (see [17] for a review).",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "Here, we propose to use a measure inspired by the Simpson index of diversity [18].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "3 CONVOLUTIONAL NEURAL NETWORKS FOR SCENE CLASSIFICATION Given the impressive performance of the deep Convolutional Neural Networks (CNNs), particularly on the ImageNet benchmark [1], [12], we choose three popular CNN architectures, AlexNet [1], GoogLeNet [19], and VGG 16 convolutional-layer CNN [20], then train them on Places205 and Places365-Standard respectively to create baseline CNN models.",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 10,
      "context" : "3 CONVOLUTIONAL NEURAL NETWORKS FOR SCENE CLASSIFICATION Given the impressive performance of the deep Convolutional Neural Networks (CNNs), particularly on the ImageNet benchmark [1], [12], we choose three popular CNN architectures, AlexNet [1], GoogLeNet [19], and VGG 16 convolutional-layer CNN [20], then train them on Places205 and Places365-Standard respectively to create baseline CNN models.",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 0,
      "context" : "3 CONVOLUTIONAL NEURAL NETWORKS FOR SCENE CLASSIFICATION Given the impressive performance of the deep Convolutional Neural Networks (CNNs), particularly on the ImageNet benchmark [1], [12], we choose three popular CNN architectures, AlexNet [1], GoogLeNet [19], and VGG 16 convolutional-layer CNN [20], then train them on Places205 and Places365-Standard respectively to create baseline CNN models.",
      "startOffset" : 241,
      "endOffset" : 244
    }, {
      "referenceID" : 17,
      "context" : "3 CONVOLUTIONAL NEURAL NETWORKS FOR SCENE CLASSIFICATION Given the impressive performance of the deep Convolutional Neural Networks (CNNs), particularly on the ImageNet benchmark [1], [12], we choose three popular CNN architectures, AlexNet [1], GoogLeNet [19], and VGG 16 convolutional-layer CNN [20], then train them on Places205 and Places365-Standard respectively to create baseline CNN models.",
      "startOffset" : 256,
      "endOffset" : 260
    }, {
      "referenceID" : 18,
      "context" : "3 CONVOLUTIONAL NEURAL NETWORKS FOR SCENE CLASSIFICATION Given the impressive performance of the deep Convolutional Neural Networks (CNNs), particularly on the ImageNet benchmark [1], [12], we choose three popular CNN architectures, AlexNet [1], GoogLeNet [19], and VGG 16 convolutional-layer CNN [20], then train them on Places205 and Places365-Standard respectively to create baseline CNN models.",
      "startOffset" : 297,
      "endOffset" : 301
    }, {
      "referenceID" : 19,
      "context" : "All the Places-CNNs presented here were trained using the Caffe package [21] on Nvidia GPUs Tesla K40 and Titan X2.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 20,
      "context" : "Additionally, given the recent breakthrough performances of the Residual Network (ResNet) on ImageNet classification [22], we further fine-tuned ResNet152 on the Places365-Standard (termed as Places365-ResNet) and compared it with the other trained-from-scratch PlacesCNNs for scene classification.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "1 Results on Places205 and Places365 After training the various Places-CNNs, we used the final output layer of each network to classify the test set images of Places205 and SUN205 (see [2]).",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : "Places365 has 160 more categories than Places205, the Top5 accuracy of the Places205-CNNs (trained on the previous version of Places [2]) on the test set only drops by 2.",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 21,
      "context" : "Activations from the higher-level layers of a CNN, also termed deep features, have proven to be effective generic features with state-ofthe-art performance on various image datasets [23], [24].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 22,
      "context" : "Activations from the higher-level layers of a CNN, also termed deep features, have proven to be effective generic features with state-ofthe-art performance on various image datasets [23], [24].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 11,
      "context" : "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 23,
      "context" : "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 24,
      "context" : "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 25,
      "context" : "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 26,
      "context" : "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 27,
      "context" : "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 28,
      "context" : "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].",
      "startOffset" : 236,
      "endOffset" : 240
    }, {
      "referenceID" : 29,
      "context" : "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].",
      "startOffset" : 258,
      "endOffset" : 262
    }, {
      "referenceID" : 11,
      "context" : "In the SUN397 experiment [13], the training set size is 50 images per category.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 23,
      "context" : "In the MIT Indoor67 experiment [25], the training set size is 100 images per category.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 24,
      "context" : "In the Scene15 experiment [26], the training set size is 50 images per category.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 25,
      "context" : "In the SUN Attribute experiment [27], the training set size is 150 images per attribute.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 26,
      "context" : "In Caltech101 and Caltech256 experiment [28], [29], the training set size is 30 images per category.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 27,
      "context" : "In Caltech101 and Caltech256 experiment [28], [29], the training set size is 30 images per category.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 28,
      "context" : "In the Stanford Action40 experiment [30], the training set size is 100 images per category.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 29,
      "context" : "In the UIUC Event8 experiment [31], the training set size is 70 images per category and the test set size is 60 images per category.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 30,
      "context" : "The classifier is a linear SVM with the same default parameters for the two deep feature layers (C=1) [32].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 31,
      "context" : "Following the methodology in [33] we estimated the receptive fields of the units in the Places-CNN and ImageNet-CNN.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "We compare the deep features of Places365VGG, Places205-AlexNet (result reported in [2]), and ImageNet-AlexNet, to those hand-designed features.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : "Results of other hand-designed features/kernels are fetched from [13].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 32,
      "context" : "We further synthesized preferred input images for the Places-CNN by using the image synthesis technique proposed in [34].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 31,
      "context" : "See the detailed visualization methodology in [33].",
      "startOffset" : 46,
      "endOffset" : 50
    } ],
    "year" : 2016,
    "abstractText" : "The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach nearhuman semantic classification at tasks such as object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories and attributes, comprising a quasi-exhaustive list of the types of environments encountered in the world. Using state of the art Convolutional Neural Networks, we provide impressive baseline performances at scene classification. With its high-coverage and high-diversity of exemplars, the Places Database offers an ecosystem to guide future progress on currently intractable visual recognition problems.",
    "creator" : "LaTeX with hyperref package"
  }
}