{
  "name" : "1512.07807.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "VISUALIZATIONS RELEVANT TO THE USER BY MULTI-VIEW LATENT VARIABLE FACTORIZATION",
    "authors" : [ "Seppo Virtanen", "Homayun Afrabandpey", "Samuel Kaski" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— Data visualization, latent factor models, manifold embedding, multi-view learning\n1. INTRODUCTION\nIn the machine learning community there has been a strong trend in developing methods for non-linear dimensionality reduction and manifold embedding, that is, finding lowerdimensional manifolds within high-dimensional data spaces. Examples of these methods include Laplacian eigenmap [1], Isomap [2], locally linear embedding [3] and stochastic neighbor embedding [4]. For a comparison of several methods see [5]. Different methods aim at preserving different geometric properties, but common to all is that they produce a lower-dimensional output where similar data points are located closer together than dissimilar data points.\nThe low-dimesional projections can be used to construct scatterplots of the data, to fulfill the constantly increasing need for data visualizations across a wide range of applications. If the dimensionality of the data manifold is two, the methods work well for visualization. If it is higher, a simple\nWe thank the Academy of Finland for funding (Finnish Centre of Excellence in Computational Inference Research COIN)\nsolution is to choose the output dimensionality to be two, essentially compressing the data manifold. It has turned out that most of the methods are not able to do that well, but by formulating the cost functions in terms of misses and false positives, a desired tradeoff between the two can be optimized for the visualization [6].\nAssuming all dimensions of the data manifold are not equally relevant to the user, a better solution is to focus on visualizing the relevant ones. In this paper we assume data are available about the user preferences, from which the visualization can be learned, but the data about preferences may be indirect. A simple example is explicit feedback about groups of data being similar, which can be expressed as cluster memberships or data classes. These types of auxiliary data can contain noise that is structured in the sense of containing classes that are not visible in the primary data. Given the primary data, and auxiliary data about user preferences, the task of finding the relevant aspects of the data is then essentially a two-view learning problem: identify what is statistically shared in the two data views. We additionally will want the shared aspects to be used for the visualization, requiring that the rest of the signals, “structured noise,” is explained away. This is what our model is capable of handling.\nWe build on the popular stochastic neighbor embedding (SNE) method to infer the visualizations. SNE has earlier been formulated for multiple views, as multi-view stochastic neighbor embedding (mSNE) [7]. That work integrated the features into a unified representation but did not yet consider visualization. The model assumed a single set of lowdimensional latent variables which explains all views, and hence cannot directly separate the source-specific “structured noise” from signal. A related model called multiple relational embedding [8] introduced view-specific mappings that can switch latent variables off from the views and hence could implement view-specific “explaining away.” They did not aim at separating shared variation from view-specific, however, and did not consider visualization yet either.\nIn summary, the main contribution of this paper is that we formulate a generative probabilistic model to solve the problem of learning a visualization relevant to the user, operating on two-view data. The first view consists of the primary data, and the second view of auxiliary data collected from the\nar X\niv :1\n51 2.\n07 80\n7v 2\n[ cs\n.L G\n] 2\n5 Ja\nn 20\n16\nuser. The main difference between our model and the existing models is that our model has a set of latent variables, some of which are coordinates of the visualization, and the rest explain away the parts of data not relevant to the user.\n2. MODEL\nWe propose a generative model for two relational count data sets, denoted as D and F , that represent similarities between pairs of N items. For example, the items can correspond to scientific articles or other documents. We denote the count between items i and j as di,j for D and fi,j for F , respectively. We assume the counts are symmetric, that is, di,j = dj,i for all i, j ∈ {1, . . . , N}. High count indicates strong similarity between two items, whereas low count indicates the two items are less similar. However, we do not assume that counts should be similar between the data sets.\nWe model the data D with a distribution p and the user data F with a distribution q, both defined over pairs of data items. Assuming that user data are available for only a subset O of data pairs, and the data D for all pairs, the joint generative distribution of the parameters and data is\np(D,F ,Θ) ∝ N∏\ni=1,j>i\np di,j i,j ∏ i′,j′∈O q fi′,j′ i′,j′ , (1)\nwhere we have collected all parameters to Θ. In the following, for brevity, we limit our notation to the data set D and variables related to it, noting that similar constructions apply for the F , due to symmetry. We then normalize the observed counts to a distribution over the items denoted by d̃ and f̃ , and use the mean-normalized log-likelihood."
    }, {
      "heading" : "2.1. Data visualization and two-view learning",
      "text" : "For learning the visualization, we introduce three vectorvalued (factorised) latent variables for each item. The variables z are shared by the two views, and the z(D) and z(F) are specific to their respective view. With these latent variables, we construct the latent (mean data) distributions as\npi,j ∝ exp(−||zi − zj ||2 − ||z(D)i − z (D) j || 2), (2) qi,j ∝ exp(−||zi − zj ||2 − ||z(F)i − z (F) j || 2),\nwhere ||·|| denotes the Euclidean distance. The idea is that the shared latent variables capture dependencies (common joint variation) between the two data sets, whereas the data setspecific latent variables capture non-shared variation for each data set. Figure 1 shows a graphical representation of our proposed model where, for simplicity, we have collected the latent variable vectors into matrices to more clearly show the main dependencies.\nIn this parameterization, distances between the latent variables for any two items are proportional to pair-wise dissimilarity between them. Assuming a large pi,j , the model\nprefers keeping the distance between latent points i and j small. In contrast, a small pi,j does not affect the cost function as strongly. This insight underlies stochastic neighbor embedding and related further developments [4, 6, 9].\nIn this work, we assume the shared latent variables are 2D and use them as visualisation coordinates. Assuming the view-specific latent variables have sufficient modelling flexibility to explain view-specific variation, the shared latent variables will capture the most significant commonalities between the data sets."
    }, {
      "heading" : "2.2. Group-sparse formulation",
      "text" : "The model can be written more compactly by concatenating the latent variables together,\nyi = [zi, z (D) i , z (F) i ] ,\nand introducing data set-specific binary indicator variables b (D) k and b (F) k , for k = 1, . . . ,K, that represent the latent variables either as active (one) or non-active (zero) in the corresponding data set. We note that the explicit factorization of the latent space in Equation 2 is useful for understanding the structure of the multi-view model and the role the different latent variables play. However, learning with a model that has fixed factorized latent variables induces severe identifiability problems for any local learning algorithm. An approach to alleviate this problem is to assume a common latent space formulation and relax the binary indicator variables to continuous variables W(D) and W(F), respectively. Assuming sparsity for the indicator variables, during learning some of them will approach zero, shutting off the corresponding view.\nThus, we re-parameterize\npi,j ∝ exp(−(yi − yj)TW(D)W(D) T (yi − yj)), (3)\nwhere yi for i = 1, . . . , N are the (concatenated) Kdimensional latent variables and W(D) is a K ×K diagonal\nmatrix (we make a similar modification for q with W(F)). We set the first two elements of W(D) (and W(F)) to unity capturing the shared latent variables, whereas the remaining variables on the diagonal are unconstrained. In the experiments, we show empirically on multiple data sets that the construction correctly captures the dominant shared variation between the two views via the shared latent variables.\nFor estimating the unobserved variables (locations on the display), we used unconstrained gradient-based optimization to find the maximum likelihood estimate."
    }, {
      "heading" : "2.3. Data setup and visualization",
      "text" : "So far, we have not specified what the D and F correspond to. In this work, the D denotes the data the user wants to visualize. They may come directly as observations of similarity between item pairs, forming count data di,j , or they may come as feature vectors xi, from which the similarities are computed as d̃i,j = exp(−||xi − xj ||2/σ2). The F denotes data provided by the user or measured from the user (more details below); they may come directly as count data fi,j , or computed from feature vectors fi as for the data xi.\nThe first option for the types of data the user may provide, is data about pairwise similarities of the data items. The user data can be collected in an interactive data-analysis session, whereby fi,j would be the number of times the items i and j were considered similar, or derived from categorizations or classifications: fi,j then is the number of classes in which both i and j belong. A particularly handy interactive visualization scenario is where the user indicates a set of data items being similar, which reduces to the (multiple) classification setting. The second option for user data types is that a feature vector is measured from the user during interaction, or the user provides an auxiliary feature vector fi for some of the i. They can then be converted to f̃i,j = exp(−||fi − fj ||2/σ2f ).\nIn both options, the user data are regarded as indirect evidence of what is relevant to the user in the primary data. The key assumption is that aspects of the primary data that have a statistical relationship with the user data are more relevant. The rest of the user input is not relevant to the primary data, and the rest of the primary data is not supported by the user input, and hence is likely to be less relevant to the user.\n3. EXPERIMENTAL EVALUATION\nWe compare our approach to the closest available alternatives; none of them have been designed for precisely the proposed task, but they can still be applied for the task. We compare to SNE that uses only one data set (not the user data), mSNE that uses both views and assumes a single set of common latent variables, and neighbourhood component analysis (NCA) that is a supervised method which assumes classes instead of another full-blown data view. We leave out comparison to MRE because code is not available, and the method would\nneed some further development to be applicable for information visualisation; it is not obvious which latent variables to visualise.\nWe used three different data sets for comparison: scientific articles from [10], Reuters Corpus Volume 1 (RCV1), and Heart Disease data set [11] from the UCI repository [12]. We show numerical data for all but only have space to show the visualizations on one (RCV1). We use available class information as ground truth for user interest, and simulate additional structured noise to the user data (alternative classes and unstructured noise). For NCA we gave the advantage of using ground truth.\nPerformance is evaluated numerically by measuring the separability of the ground truth classes on the 2D visualization. The logic is that since we know the ground truth classes to be relevant in the sense that they inhabit the shared data space, and random errors are more likely to mix up the classes than to separate them, a better visualization separates the class distributions better. As a measure of separation we use the (leave-one-out) performance of a k-nearest neighbor classifier on the visualization."
    }, {
      "heading" : "3.1. Reuters Corpus Volume 1",
      "text" : "We used a subset of RCV1-v2 corpus, first used by [13]. The subset is a document-term matrix containing N = 9, 625 documents which are divided into four categories, “C15”, “ECAT”, “GCAT”, and “MCAT”. For each document, feature vectors are generated by the standard TF-IDF weighting scheme. For details about the RCV1 corpus see [14].\nFigure 2 shows our method finds the relevant structure clearly by inferring well-separated class-specific clusters. In this figure, categories are shown by colors of the dots: red, green, blue, and cyan represent “C15”, “ECAT”, “GCAT”, and “MCAT”, respectively. Some of the relevant structure is visible in the user-data-specific latent variables (Fig. 2e), and also in the irrelevant data (Fig. 2f), indicating that the dimensionality of the relevant structure is higher than twodimensional and hence a higher-dimensional display would be needed to display all of the ground truth. For the alternative methods, based on the figure, we see that mSNE fails for the task, SNE infers two (noisy) clusters with incorrect classes and NCA captures a single cluster. For the other two data sets the behaviour of our proposed method was analogous and it separated the clusters well using the shared latent variables. Due to the lack of space we only show quantitative indicators on those two data sets, in Table 1, instead of visualizations.\nThe classes have become separated quantitatively as well, as shown with 5-nearest neighbor classification results in the second column of Table 1. We can see that the proposed multi-view latent variable model outperforms other techniques with a clear margin. It may seem striking that even the supervised NCA is clearly worse; the reason is that the data contain also irrelevant classes, and NCA is not better in\ndistinguishing the relevant and irrelevant ones. The table also contains the results for the two other data set where again our method is the best."
    }, {
      "heading" : "3.2. Multi-labelling",
      "text" : "To demonstrate that different aspects of the same data set can be visualized for different users, according to what is relevant to them, we simulate two users who are interested in different aspects and give different feedbacks (labellings). We used the Abalone data set from the UCI repository. This is a classification data set aimed at predicting the age of abalones from physical measurements. The number of rings inside the shell is a significant factor for determining the age of an abalone. We thresholded the numbers into three categories where the first group contains 3 to 9 rings, the second group contains 10 to 16 rings and finally the last group contains 17 to 23 rings.\nWe left out ring numbers having less than 5 samples.This resulted in only 9 discarded records, which is negligible in the total number of 4177.\nThe first user is interested in the age and assigns similarities according to ring numbers. The second user is interested in sex and gives feedback according to the three categories M, F, and I. Figure 3 shows the visualization of the shared latent variables of our method for the two different labellings. In 3a the ages become separated to an extent, and in 3b the sexes. The visualizations are different, as they should be for two users interested in different aspects, both having support in the data.\n4. CONCLUSION\nWe have introduced a statistical principle to identify and visualize aspects of data relevant to the user, by exploiting statistical relations found between the primary data, and userprovided auxiliary data. Unlike manifold embedding-based dimensionality reduction methods which have not been designed for compressing dimensionalities to two for visualization on display, our proposed method is able to visualize well in two dimensions, by explaining away the irrelevant data with additional latent variables.\nIn this paper, we successfully demonstrated and compared the proposed method on multiple static data sets, where the ground truth came from categorizations of the data. A main future goal is to use similar techniques in interactive visualization, where user interaction data will be measured all the time, and the visualization needs to react faster.\nWe did not consider model order selection in this paper. Our expectation is that standard probabilistic methods, in particular automatic relevance determination (ARD), could be used for determining the total number of latent variables. It may turn out to be more difficult to choose how many latent variables are relevant for the user, in case the relevant subspace cannot be adequately presented in 2D. Our current hypothesis is that group sparsity combined with ARD would be sufficient.\n5. REFERENCES\n[1] Mikhail Belkin and Partha Niyogi, “Laplacian eigenmaps for dimensionality reduction and data representation,” Neural computation, vol. 15, no. 6, pp. 1373– 1396, 2003.\n[2] Joshua B Tenenbaum, Vin De Silva, and John C Langford, “A global geometric framework for nonlinear dimensionality reduction,” Science, vol. 290, no. 5500, pp. 2319–2323, 2000.\n[3] Sam T Roweis and Lawrence K Saul, “Nonlinear dimensionality reduction by locally linear embedding,” Science, vol. 290, no. 5500, pp. 2323–2326, 2000.\n[4] Geoffrey E Hinton and Sam T Roweis, “Stochastic neighbor embedding,” in Advances in Neural Information Processing Systems, 2002, pp. 833–840.\n[5] L.J.P. van der Maaten, E.O. Postma, and H.J. van den Herik, “Dimensionality reduction: A comparative review.,” Tech. Rep. TiCC-TR 2009-005, Tilburg University, 2009.\n[6] Jarkko Venna, Jaakko Peltonen, Kristian Nybo, Helena Aidos, and Samuel Kaski, “Information retrieval perspective to nonlinear dimensionality reduction for data visualization,” The Journal of Machine Learning Research, vol. 11, pp. 451–490, 2010.\n[7] Bo Xie, Yang Mu, Dacheng Tao, and Kaiqi Huang, “msne: Multiview stochastic neighbor embedding,” IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 41, no. 4, pp. 1088–1096, 2011.\n[8] Roland Memisevic and Geoffrey E Hinton, “Multiple relational embedding,” in Advances in neural information processing systems, 2004, pp. 913–920.\n[9] Jaakko Peltonen and Samuel Kaski, “Generative modeling for maximizing precision and recall in information visualization,” in International Conference on Artificial Intelligence and Statistics, 2011, pp. 579–587.\n[10] Jaakko Peltonen, Max Sandholm, and Samuel Kaski, “Information retrieval perspective to interactive data visualization,” in EuroVis-Short Papers, 2013, pp. 49–53.\n[11] Robert Detrano, Andras Janosi, Walter Steinbrunn, Matthias Pfisterer, Johann-Jakob Schmid, Sarbjit Sandhu, Kern H Guppy, Stella Lee, and Victor Froelicher, “International application of a new probability algorithm for the diagnosis of coronary artery disease,” The American journal of cardiology, vol. 64, no. 5, pp. 304–310, 1989.\n[12] M. Lichman, “UCI machine learning repository,” 2013.\n[13] Deng Cai and Xiaofei He, “Manifold adaptive experimental design for text categorization,” Knowledge and Data Engineering, IEEE Transactions on, vol. 24, no. 4, pp. 707–719, 2012.\n[14] David D Lewis, Yiming Yang, Tony G Rose, and Fan Li, “Rcv1: A new benchmark collection for text categorization research,” The Journal of Machine Learning Research, vol. 5, pp. 361–397, 2004."
    } ],
    "references" : [ {
      "title" : "Laplacian eigenmaps for dimensionality reduction and data representation",
      "author" : [ "Mikhail Belkin", "Partha Niyogi" ],
      "venue" : "Neural computation, vol. 15, no. 6, pp. 1373– 1396, 2003.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A global geometric framework for nonlinear dimensionality reduction",
      "author" : [ "Joshua B Tenenbaum", "Vin De Silva", "John C Langford" ],
      "venue" : "Science, vol. 290, no. 5500, pp. 2319–2323, 2000.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Nonlinear dimensionality reduction by locally linear embedding",
      "author" : [ "Sam T Roweis", "Lawrence K Saul" ],
      "venue" : "Science, vol. 290, no. 5500, pp. 2323–2326, 2000.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Stochastic neighbor embedding",
      "author" : [ "Geoffrey E Hinton", "Sam T Roweis" ],
      "venue" : "Advances in Neural Information Processing Systems, 2002, pp. 833–840.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Dimensionality reduction: A comparative review",
      "author" : [ "L.J.P. van der Maaten", "E.O. Postma", "H.J. van den Herik" ],
      "venue" : "Tech. Rep. TiCC-TR 2009-005, Tilburg University, 2009.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Information retrieval perspective to nonlinear dimensionality reduction for data visualization",
      "author" : [ "Jarkko Venna", "Jaakko Peltonen", "Kristian Nybo", "Helena Aidos", "Samuel Kaski" ],
      "venue" : "The Journal of Machine Learning Research, vol. 11, pp. 451–490, 2010.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "msne: Multiview stochastic neighbor embedding",
      "author" : [ "Bo Xie", "Yang Mu", "Dacheng Tao", "Kaiqi Huang" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 41, no. 4, pp. 1088–1096, 2011.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multiple relational embedding",
      "author" : [ "Roland Memisevic", "Geoffrey E Hinton" ],
      "venue" : "Advances in neural information processing systems, 2004, pp. 913–920.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Generative modeling for maximizing precision and recall in information visualization",
      "author" : [ "Jaakko Peltonen", "Samuel Kaski" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics, 2011, pp. 579–587.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Information retrieval perspective to interactive data visualization",
      "author" : [ "Jaakko Peltonen", "Max Sandholm", "Samuel Kaski" ],
      "venue" : "EuroVis-Short Papers, 2013, pp. 49–53.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "International application of a new probability algorithm for the diagnosis of coronary artery disease",
      "author" : [ "Robert Detrano", "Andras Janosi", "Walter Steinbrunn", "Matthias Pfisterer", "Johann-Jakob Schmid", "Sarbjit Sandhu", "Kern H Guppy", "Stella Lee", "Victor Froelicher" ],
      "venue" : "The American journal of cardiology, vol. 64, no. 5, pp. 304–310, 1989.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "UCI machine learning repository",
      "author" : [ "M. Lichman" ],
      "venue" : "2013.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Manifold adaptive experimental design for text categorization",
      "author" : [ "Deng Cai", "Xiaofei He" ],
      "venue" : "Knowledge and Data Engineering, IEEE Transactions on, vol. 24, no. 4, pp. 707–719, 2012.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Rcv1: A new benchmark collection for text categorization research",
      "author" : [ "David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li" ],
      "venue" : "The Journal of Machine Learning Research, vol. 5, pp. 361–397, 2004.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Examples of these methods include Laplacian eigenmap [1], Isomap [2], locally linear embedding [3] and stochastic neighbor embedding [4].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "Examples of these methods include Laplacian eigenmap [1], Isomap [2], locally linear embedding [3] and stochastic neighbor embedding [4].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "Examples of these methods include Laplacian eigenmap [1], Isomap [2], locally linear embedding [3] and stochastic neighbor embedding [4].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "Examples of these methods include Laplacian eigenmap [1], Isomap [2], locally linear embedding [3] and stochastic neighbor embedding [4].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "For a comparison of several methods see [5].",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "It has turned out that most of the methods are not able to do that well, but by formulating the cost functions in terms of misses and false positives, a desired tradeoff between the two can be optimized for the visualization [6].",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 6,
      "context" : "SNE has earlier been formulated for multiple views, as multi-view stochastic neighbor embedding (mSNE) [7].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 7,
      "context" : "A related model called multiple relational embedding [8] introduced view-specific mappings that can switch latent variables off from the views and hence could implement view-specific “explaining away.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "This insight underlies stochastic neighbor embedding and related further developments [4, 6, 9].",
      "startOffset" : 86,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "This insight underlies stochastic neighbor embedding and related further developments [4, 6, 9].",
      "startOffset" : 86,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "This insight underlies stochastic neighbor embedding and related further developments [4, 6, 9].",
      "startOffset" : 86,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "We used three different data sets for comparison: scientific articles from [10], Reuters Corpus Volume 1 (RCV1), and Heart Disease data set [11] from the UCI repository [12].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "We used three different data sets for comparison: scientific articles from [10], Reuters Corpus Volume 1 (RCV1), and Heart Disease data set [11] from the UCI repository [12].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 11,
      "context" : "We used three different data sets for comparison: scientific articles from [10], Reuters Corpus Volume 1 (RCV1), and Heart Disease data set [11] from the UCI repository [12].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 12,
      "context" : "We used a subset of RCV1-v2 corpus, first used by [13].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : "For details about the RCV1 corpus see [14].",
      "startOffset" : 38,
      "endOffset" : 42
    } ],
    "year" : 2016,
    "abstractText" : "A main goal of data visualization is to find, from among all the available alternatives, mappings to the 2D/3D display which are relevant to the user. Assuming user interaction data, or other auxiliary data about the items or their relationships, the goal is to identify which aspects in the primary data support the user’s input and, equally importantly, which aspects of the user’s potentially noisy input have support in the primary data. For solving the problem, we introduce a multi-view embedding in which a latent factorization identifies which aspects in the two data views (primary data and user data) are related and which are specific to only one of them. The factorization is a generative model in which the display is parameterized as a part of the factorization and the other factors explain away the aspects not expressible in a two-dimensional display. Functioning of the model is demonstrated on several data sets.",
    "creator" : "LaTeX with hyperref package"
  }
}