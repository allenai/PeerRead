{
  "name" : "1107.0018.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Ahmed Al-Ani" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "Journal of Arti ial Intelligen e Resear h 17 (2002) 333-361 Submitted 2/2002; published 11/2002",
      "text" : "A New Te hnique for Combining Multiple Classi ers usingThe Dempster-Shafer Theory of Eviden eAhmed Al-Ani a.alani qut.edu.auMohamed Deri he m.deri he qut.edu.auSignal Pro essing Resear h CentreQueensland University of Te hnologyGPO Box 2434, Brisbane, Q 4001, AustraliaAbstra tThis paper presents a new lassi er ombination te hnique based on the Dempster-Shafer theory of eviden e. The Dempster-Shafer theory of eviden e is a powerful methodfor ombining measures of eviden e from di erent lassi ers. However, sin e ea h of theavailable methods that estimates the eviden e of lassi ers has its own limitations, wepropose here a new implementation whi h adapts to training data so that the overall meansquare error is minimized. The proposed te hnique is shown to outperform most available lassi er ombination methods when tested on three di erent lassi ation problems.1. Introdu tionIn the eld of pattern re ognition, the main obje tive is to a hieve the highest possible las-si ation a ura y. To attain this obje tive, resear hers, throughout the past few de ades,have developed numerous systems working with di erent features depending upon the ap-pli ation of interest. These features are extra ted from data and an be of di erent typeslike ontinuous variables, binary values, et . As su h, a lassi ation algorithm used with aspe i set of features may not be appropriate with a di erent set of features. In addition, lassi ation algorithms are di erent in their theories, and hen e a hieve di erent degreesof su ess for di erent appli ations. Even though, a spe i feature set used with a spe i lassi er might a hieve better results than those obtained using another feature set and/or lassi ation s heme, we an not on lude that this set and this lassi ation s heme a hievethe best possible lassi ation results (Kittler, Hatef, Duin, & Matas, 1998). As di erent lassi ers may o er omplementary information about the patterns to be lassi ed, ombin-ing lassi ers, in an eÆ ient way, an a hieve better lassi ation results than any single lassi er (even the best one).As explained by Xu et al. (1992), the problem of ombining multiple lassi ers onsistsof two parts. The rst part, losely dependent on spe i appli ations, in ludes the problemsof \\How many and what type of lassi ers should be used for a spe i appli ation?, andfor ea h lassi er what type of features should we use?\", as well as other problems that arerelated to the onstru tion of those individual and omplementary lassi ers. The se ondpart, whi h is ommon to various appli ations, in ludes the problems related to the question\\How to ombine the results from di erent existing lassi ers so that a better result an beobtained?\". In our work, we will be on entrating on problems related to the se ond issue. 2002 AI A ess Foundation and Morgan Kaufmann Publishers. All rights reserved.\nAl-Ani & Deri heThe output information from various lassi ation algorithms an be ategorized intothree levels: the abstra t, the rank, and the measurement levels. In the abstra t level,a lassi er only outputs a unique label, as in the ase of synta ti lassi ers. For therank level, a lassi er ranks all labels or a subset of the labels in a queue with the labelat the top being the rst hoi e. This type was dis ussed by Ho et al. (1994). For themeasurement level, a lassi er attributes to ea h lass a measurement value that re e ts thedegree of on den e that a spe i input belongs to a given lass. Among the three levels,the measurement level ontains the highest amount of information while the abstra t level ontains the lowest. For this reason, we adopted, in this work, the measurement level.Kittler et al. (1998) di erentiated between two lassi er ombination s enarios. In the rst s enario, all the lassi ers use the same representation of the input pattern. On theother hand, ea h lassi er uses its own representation of the input pattern in the se onds enario. They illustrated that in the rst ase, ea h lassi er an be onsidered to pro-du e an estimate of the same a posteriori lass probability. However, in the se ond aseit is no longer possible to onsider the omputed a posteriori probabilities to be estimatesof the same fun tional value, as the lassi ation systems operate in di erent measure-ment systems. Kittler et al. (1998) fo used on the se ond s enario, and they ondu teda omparative study of the performan e of several ombination s hemes namely; produ t,sum, min, max, and median. By assuming the joint probability distributions to be on-ditionally independent, they found that the sum rule gave the best results. A well knownapproa h that has been used in ombining the results of di erent lassi ers is the weightedsum, where the weights are determined through a Bayesian de ision rule (Lam & Suen,1995). An alternative method was presented by Hashem & S hmeiser (1995), where a ostfun tion was used to minimize the mean square error (MSE) in order to al ulate a linear ombination of the orresponding outputs from a number of trained arti ial neural net-works (ANNs). The expe tation maximization algorithm was used by Chen & Chi (1998)to perform the linear ombination. The fuzzy integral has been used by Cho & Kim (1995a,1995b) to ombine multiple ANNs, while (Rogova, 1994; Mandler & S hurmann, 1988) haveused the Dempster-Shafer theory of eviden e to ombine the result of several ANNs. Manyother ombination methods have also been used to ombine lassi ers, su h as bagging andboosting (Dietteri h, 1999), whi h are powerful methods for diversifying and ombining lassi ation results obtained using a single lassi ation algorithm and a spe i featureset. In bagging, we get a family of lassi ers by training on di erent portions of the trainingset. The method works as follows. We rst reate N training bags. A single training bagis obtained by taking a training set of size S and sampling this training set S times withrepla ement. Some training instan es will o ur multiple times in a bag, while others maynot appear at all. Next, ea h bag is used to train a lassi er. These lassi ers are then ombined. Boosting, on the other hand, is based on multiple learning iterations. At ea hiteration, instan es that are in orre tly lassi ed are given a greater weight in the next it-eration. By doing so, in ea h iteration, the lassi er is for ed to on entrate on instan es itwas unable to orre tly lassify in earlier iterations. In the end, all of the trained lassi ersare ombined.In this paper, we will fo us on ombining lassi ation results obtained using N di erentfeature sets, f1; ; fN . Ea h feature set will be used to train a lassi er, and hen e therewill be N di erent lassi ers, 1; ; N . For a spe i input x, ea h lassi er n produ es334\nA New Te hnique for Combining Multiple Classifiers Feature\nExtraction 1\nClassifier\nc1\nClassifier Nc\nx\nExtraction N\nFeature\nf\nCombination z\ny\ny\nf N\n1 1\nNFigure 1: A multi- lassi er re ognition systema real ve tor yn = [yn(1); yn(k); yn(K)℄T , where K is the number of lass labels andyn(k) orresponds to the degree that n onsiders x has the label k. This degree ould bea probability, as in the Bayesian lassi er, or any other s oring system. Fig. 1 shows theblo k diagram of a multi- lassi er re ognition system.Unlike statisti al-based ombination te hniques, the Dempster-Shafer theory of eviden ehas the ability to represent un ertainties and la k of knowledge. This is quite important forthe problem of lassi er ombination, be ause there is usually a ertain level of un ertaintyasso iated with the performan e of ea h of the lassi ers. Sin e available lassi er ombi-nation methods based on this theory do not a urately estimate the eviden e of lassi ers,this paper attempts to solve this issue by proposing a new te hnique based on the gradientdes ent learning algorithm, whi h aims at minimizing the MSE between the ombined out-put and the target output of a given training set. Aha (1995) gave the following de nitionfor learning:Learning denotes hanges in the system that are adaptive in the sense that theyenable the system to do the same task or tasks drawn from the same populationmore e e tively the next time.Based on the above, we show that instead of attempting to nd an analyti al formula whi ha urately measures eviden e, one an obtain a very good estimate of eviden e by just usingappropriate learning pro edures, as will be dis ussed later.Some basi on epts of the Dempster-Shafer theory of eviden e are presented in thenext se tion. Se tion three dis usses the existing methods for omputing eviden e. Theproposed ombination te hnique is presented in se tion four. Se tion ve ompares theproposed algorithm to other onventional methods used by Kittler et al. (1998), the fuzzyintegral, and a previous implementation of the Dempster-Shafer theory. Se tion six providesa on lusion to the paper.2. The Dempster-Shafer Theory of Eviden eThe Dempster-Shafer (D-S) theory of eviden e (Shafer, 1976) is a powerful tool for rep-resenting un ertain knowledge. This theory has inspired many resear hers to investigate335\nAl-Ani & Deri hedi erent aspe ts related to un ertainty and la k of knowledge and their appli ations to reallife problem. Today, the D-S theory overs several di erent models, su h as the theory ofhints (Kohlas & Monney, 1995) and the transferable belief model (TBM) (Smets, 1998).The latter will be adopted in this paper as it represents a powerful tool for ombiningmeasures of eviden e.Let = f 1; :::::; Kg be a nite set of possible hypotheses. This set is referred to as theframe of dis ernment, and its powerset denoted by 2 . Following are the basi on epts ofthe theory:Basi belief assignment (BBA). A basi belief assignment m is a fun tion that assignsa value in [0; 1℄ to every subset A of and satis es the following:m(;) = 0; andXA m(A) = 1 (1)It is worth mentioning that m(;) ould be positive when onsidering unnormalized ombi-nation rule as will be explained later. While in probability theory a measure of probability isassigned to atomi hypotheses i, m(A) is the part of belief that supports A, but does notsupport anything more spe i , i.e., stri t subsets of A. For A 6= i, m(A) re e ts some ig-noran e be ause it is a belief that we annot subdivide into ner subsets. m(A) is a measureof support we are willing to assign to a omposite hypothesis A at the expense of supportm( i) of atomi hypotheses i. A subset A for whi h m(A) > 0 is alled a fo al element.The partial ignorant asso iated with A leads to the following inequality: m(A)+m(A) 1,where A is the ompliment of A. In other words, the D-S theory of eviden e allows usto represent only our a tual knowledge without being for ed to over ommit when we areignorant.Belief fun tion. The belief fun tion, bel(:), asso iated with the BBA m(:) is a fun tionthat assigns a value in [0; 1℄ to every nonempty subset B of . It is alled \\degree of beliefin B\" and is de ned by bel(B) = XA Bm(A) (2)We an onsider a basi belief assignment as a generalization of a probability density fun -tion whereas a belief fun tion is a generalization of a probability fun tion.Combination rule. Consider two BBAs m1(:) and m2(:) for belief fun tions bel1(:) andbel2(:) respe tively. Let Aj and Bk be fo al elements of bel1 and bel2 respe tively. Thenm1(:) and m2(:) an be ombined to obtain the belief mass ommitted to C a ordingto the following ombination or orthogonal sum formula (Shafer, 1976),m(C) = m1 m2(C) = Xj;k;Aj\\Bk=Cm1(Aj)m2(Bk)1 Xj;k;Aj\\Bk=;m1(Aj)m2(Bk) ; C 6= ; (3)336\nA New Te hnique for Combining Multiple ClassifiersThe denominator is a normalizing fa tor, whi h intuitively measures how mu h m1(:) andm2(:) are on i ting. Smets (1990) proposed the unnormalized ombination rule:m1 \\ m2(C) = XAj\\Bk=Cm1(Aj)m2(Bk); 8C (4)This rule implies that m(;) ould be positive, and in su h ase re e ts some kind of on-tradi tion in the belief state. In this work we will onsider that m(;) = 0 and use thenormalized ombination rule. A omparison between normalized and unnormalized ombi-nation rules for the problem of ombining lassi ers will be onsidered in the future.Combining several belief fun tions. The ombination rule an be easily extended toseveral belief fun tions by repeating the rule for new belief fun tions. Thus the pairwiseorthogonal sum of n belief fun tions bel1; bel2; ; beln, an be formed as((bel1 bel2) bel3) beln = nMi=1 beli (5)Notation. A ording to Smets (2000), the full notation for bel and its related fun tions is:bel <Y;t [ECY;t℄(w0 2 A) = xwhere Y represents the agent, t the time, the frame of dis ernment, < a boolean algebraof subsets of , w0 the a tual world, A a subset of , and ECY;t all what agent Y knowsat t. Thus, the above expression denotes that the degree of belief held by Y at t that w0belongs to the set A of worlds is equal to x. The belief is based on the evidential orpusECY;t held be Y at t.In pra ti e, many indi es an be omitted for simpli ity sake. Usually < is the power setof , whi h is 2 . When bel is de ned on 2 , < is not expli itly stated. 'w0 2 A' is denotedas 'A'. Y and/or t are omitted when the values of the missing elements are learly de nedfrom the ontext. Furthermore, EC is usually just a onditioning event. So, bel(A) is oneof the most often used notations (Smets, 2000). In the proposed method, we will adopt thefollowing notation: beln( k), where the agent is the lassi er, and the subsets of on ernare the lass labels.It is important to mention that the ombination rule given by Eq. 3 assumes that thebelief fun tions to be ombined are independent. Consider that we have ertain informationand would like to measure its belief, then we an think of this pro ess as a mapping fromthe \\original information level\" to the \\belief level\". Liu & Bundy (1992) explained thatindependen e in the original information level would lead to independen e in the belief level.But, if two independent belief fun tions are rooted to the original information level, thentheir original information may or may not be independent. For the problem of ombiningmultiple lassi ers, the original information level onsists of outputs of the lassi ers to be ombined, while the belief level onsists of the eviden e of these lassi ers (or their BBAs).The assumption that these BBAs are independent, whether obtained from independent ordependent original information, an hen e justify the use of D-S theory. In fa t, many337\nAl-Ani & Deri heexisting lassi er ombination methods assume the lassi ation results of di erent lassi- ers to be independent (Mandler & S hurmann, 1988; Hansen & Salamon, 1990; Xu et al.,1992). Sin e the lassi ers' eviden e plays a ru ial role in the ombination performan e,there is an in reased interest in the proper estimation of su h eviden e. In the next se tion,we dis uss how a number of existing lassi er ombination methods estimate eviden e of lassi ers, and in se tion 4 we present our proposed method.3. Existing Methods for Computing Eviden eMandler & S hurmann (1988) proposed a method that transforms distan e measures of thedi erent lassi ers into eviden e. This was a hieved by rst al ulating a distan e betweenlearning data sets and a number of referen e points in order to estimate statisti al distri-butions of intra- and inter lass distan es. For both, the a posteriori probability fun tionwas estimated, indi ating to whi h degree an input pattern belongs to a ertain referen epoint. Then, for ea h lass label, the lass onditional probabilities were ombined intoeviden e value ranging between 0 and 1, whi h was onsidered as the BBA of that lass.Finally, Dempster's ombination rule was used to ombine the BBAs of the di erent lassi- ers to give the nal result. As explained by Rogova (1994), this method brought forwardquestions about the hoi e of referen e ve tors and the distan e measure. Moreover, ap-proximations asso iated with estimation of parameters of statisti al models for intra- andinter lass distan es an lead to ina urate measure of the eviden e.Xu et al. (1992) used K + 1 lasses to perform the lassi ation task, where for the(K + 1)th lass denotes that the lassi er has no idea about whi h lass the input omesfrom. For ea h lassi er n, n = 1::N , re ognition, substitution, and reje tion rates ( nr , ns ,and 1 nr ns ) were used as a measure of BBA, mn, on as follows:1. If the maximum output of a spe i lassi er belongs to K + 1, then mn has only afo al element with mn( ) = 1.2. When the maximum output belongs to one of theK lasses, mn has two fo al elements k and k with mn( k) = nr , mn( k) = ns . As the lassi er says nothing about anyother propositions, mn( ) = 1 mn( k) mn( k).The drawba k of this method is again the way eviden e is measured. There are two problemsasso iated with this method. Firstly, many lassi ers do not produ e binary outputs, butrather probability like outputs. So, in the rst ase, it is ina urate to assign 0 to bothmn( k) andmn( k). Se ondly, this way of measuring eviden e ignores the fa t that lassi ersnormally do not have the same performan e with di erent lasses. This had a lear impa ton the performan e of this ombination method when ompared with other onventionalmethods espe ially the Bayesian (Xu et al., 1992).Rogova (1994) used several proximity measures between a referen e ve tor and a las-si er's output ve tor. The proximity measure that gives the highest lassi ation a ura ywas later transformed into eviden es. The referen e ve tor used was the mean ve tor, nk , ofthe output set of ea h lassi er n and ea h lass label k. A number of proximity measures,dnk , for nk and yn were onsidered. For ea h lassi er, the proximity measure of ea h lass338\nA New Te hnique for Combining Multiple Classifiersis transformed into the following BBAs:mk( k) = dnk ; mk( ) = 1 dnkmk( k) = 1 Yl 6=k(1 dnl ); mk( ) =Yl 6=k(1 dnl )The eviden e of lassi er n and lass k is obtained by ombining the knowledge about k, thus mk mk. Finally, Dempster's ombination rule was used to ombine eviden esfor all lassi ers to obtain a measure of on den e for ea h lass label. Note that the rst ombination was performed with respe t to the lass label (Rogova used the notations k andk), while in the se ond one the agent was n. This idea was a promising one. However, themajor drawba k is the way the referen e ve tors are al ulated, where the mean of outputve tors may not be the best hoi e. Also, trying several proximity measures and hoosingthe one that gives the highest lassi ation a ura y is itself questionable.4. The Proposed Combination Te hniqueIn this se tion we will estimate the value of mn( k), whi h represents the belief in lass labelk that is produ ed by lassi er n. In addition, we will also estimate mn( ), whi h re e tsthe ignoran e asso iated with lassi er n. Sin e the ultimate obje tive is to minimize theMSE between the ombined lassi ation results and the target output, mn( k) and mn( )will be estimated using an iterative pro edure that aims at attaining this obje tive. We will rst ompare yn, whi h is the output lassi ation ve tor produ ed by lassi er n, to areferen e ve tor, wnk , and the obtained distan e will be used to estimate the BBAs. TheseBBAs will then be ombined to obtain a new output ve tor, z, that represents the ombined on den e in ea h lass label. wnk will be measured su h that the MSE between z and thetarget ve tor, t, of a training dataset is minimized. Note that there are two indi es for wnk .Thus, for lass label k, we don't only onsider the value assigned to it by lassi er n, butrather the whole output ve tor (values assigned to ea h lass label).Let the frame of dis ernment = f 1; k; ; Kg, where k is the hypothesis thatthe input x is of lass k. Considering a BBA, mn, su h that mn( k) 0, mn( ) =1 PKk=1mn( k), and mn is 0 elsewhere. Let dn( k) be a distan e measure and gn theunnormalized ignoran e of lassi er n, thenmn( k) andmn( ) will be estimated a ordingto the following formulas: dn( k) = exp( kwnk ynk2) (6)mn( k) = dn( k)KXk=1 dn( k) + gn (7)mn( ) = gnKXk=1 dn( k) + gn (8)where mn( k) and mn( ) are the normalized values of dn( k) and gn respe tively. Similarto wnk , the minimized MSE will be used to estimate gn.339\nAl-Ani & Deri heEviden es of all lassi ers are ombined a ording to the normalized ombination ruleto obtain a measure of on den e of ea h lass label. The kth element of the new ombinedve tor is given by: z(k) = m( k) = m1( k) mN ( k) =Mn2Nmn( k) (9)For a given lassi er n, let I = f1 Ng n fng, mI =Li2I mi, then Eq. 9 an be writtenas: z(k) = mI( k) mn( k) (10)where a ording to Eq. 3, the ombination of two BBAs is:mj( k) ml( k) = mj( k)ml( k) +mj( k)ml( ) +mj( )ml( k)1 Xp Xqq 6=p mj( p)ml( q) (11)wnk and gn will be initialized randomly, then their values will be adjusted a ording to atraining dataset so that the MSE of z is minimized.Err = kz tk2 (12)The values of wnk and gn are adjusted a ording to the formulas:wnk [new℄ = wnk [old℄ Err wnk [old℄ (13)gn[new℄ = gn[old℄ Err gn[old℄ (14)where and are the learning rates. The terms Err= wnk and Err= gn are derived asfollows: Err wnk = Err z(k) z(k) mn( k) mn( k) wnk (15) Errgn = Err z(k) z(k) mn( k) mn( k) gn (16)\n340\nA New Te hnique for Combining Multiple Classifierswhere, Err z(k) = 2[z(k) t(k)℄ (17) z(k) mn( k) = (\"1 Xp Xqq 6=p mn( p)mI( q)#[mI( k) +mI( )℄ + [mn( k)mI( k) +mn( k)mI( ) +mn( )mI( k)℄\"Xpp6=k mI( p)#),\"1 Xp Xqq 6=p mn( p)mI( q)#2 (18) mn( k) wnk = 2 exp( kwnk ynk2)[wnk yn℄[Xpp6=k dn( p) + gn℄[Xp dn( p) + gn℄2 (19) mn( k) gn = dn( k)[Xp dn( p) + gn℄2 (20)Fig. 2 shows a ow hart of these learning pro edures. It has been found that adjustingthe values of gn an be a hieved during the rst few iterations. By ontinuing the trainingto ne-tune the values of wnk until there is no further improvement on the training set, orwe rea h a pre-de ned maximum number of epo hs1, the result ould be further enhan ed.Note that the weight values are adjusted by ea h pattern (not bat h training). We x thevalue of = 10 6, while is rst initialized to 5 10 4, and is then hanged a ording tothe value of MSE, as des ribed in the ow hart.Although the omputational ost involved in implementing our te hnique is higher thanthat of other ombination methods2, we only need to perform training on e, whi h an bedone o -line. Then, with the optimal values of wnk and gn, we an perform the on-line ombination, whi h is omparable to other ombination methods.On the other hand, as indi ated in the beginning of this se tion, we onsider a referen eve tor, wnk , for ea h lass. This leads to an in rease in training time as the number of lassesand/or lassi ers in reases. An alternative is to onsider only using a referen e value forea h lass, wnk . This will save more than 50% of training time for the ase of several lassi ers and lasses. Note that the same learning formulas are appli able by repla ing wnkwith wnk and yn with ynk . We will refer to these two alternative approa hes as DS1 and DS2,respe tively. In the following se tion, we will ompare DS1 and DS2 with other well-known ombination methods.1. The maximum number of epo hs is set to 50 in all experiments des ribed in this paper2. Training time of most of the experiments ondu ted in se tion 5 required less than 3 minutes on a onventional PC 341\nAl-Ani & Deri he Start\nα = α∗0.7\nα = α∗1.03\nEnd\nFor each pattern\nErr = Errold new\nIt_no=It_no+1\nk (θ ) and (Θ)m mn n Compute\nn kw g nand Adjust\n-6β=10 α=5∗10 ,-4\nInitialize learning rates\nErr -Err >Thnewold\nIF\nn kw g nand\nn k=1: N, =1: K\nz and Errnew Compute\nIt_no<It_nomax\nα > 10 and -4\nIF\nErrnew Errold It_no It_nomax= max. no. of iterations\nTh\n= current no. of iterations = previous MSE\n= error threshold = current MSE\nNo\nYes\nYes\nNo\nRandomly initialize\nFigure 2: Training pro edure of the proposed te hniqueIt is worth mentioning that although the training pro edures of both the proposedmethod and the ba kpropagation algorithm of ANN are based on minimizing the MSE usingiterative approa hes, the proposed method and ANN are not similar. The ba kpropagationtraining operates by passing the weighted sum of its input through an a tivation fun tion,usually in a multi-layer ar hite ture known as multi-layer per eptron (MLP). Extra tingrules from a trained MLP is a very hallenging problem. On the other hand, the training ofthe proposed method operates by measuring a distan e between a lassi ation ve tor anda referen e ve tor. This distan e would later be used to measure the belief of ea h lasslabel for all lassi ers. The nal on den e of ea h lass label is obtained by ombiningthe beliefs of all lassi ers. Unlike MLP, the belief of a given lass label for ea h lassi erindi ates its ontribution towards the nal on den e. The reader may refer to (Denoeux,2000) for a des ription of an ANN lassi er based on the D-S theory. 342\nA New Te hnique for Combining Multiple Classifiers5. Performan e Analysis of Di erent Combination MethodsThe following three lassi ation problems have been onsidered: texture lassi ation, lassi ation of spee h segments a ording to their manner of arti ulation, and speakeridenti ation. ANNs are used to perform lassi ation for the three problems. For ea h ase, lassi ers will be sorted a ording to their performan e, su h that the best lassi eris referred to as 1, the 2nd best as 2, and the worst one as N .For ea h problem, we will onsider di erent number of lasses, and ombine the results ofdi erent number of lassi ers, where ombining results of the best, the worst and mixturesof best and worst lassi ers will be investigated. For example, if we have ve lassi ersand would like to ombine two of these, then we will onsider ombining the best two,f 1; 2g, best one and worst one, f 1; 5g, and worst two lassi ers, f 4; 5g. The following ombination methods were tested: the weighted sum (WS)3, average (Av), median (Md),maximum (Mx), majority voting (MV), fuzzy integral (FI) (Cho & Kim, 1995a) 4, Rogova'sD-S method (DS0) (Rogova, 1994), and our proposed method with its two alternatives (DS1& DS2). The training set used to train the ANNs will be used to estimate the onfusionmatrix for WS and FI, as well as to estimate the eviden e of DS0, DS1, and DS2.Two measures will be used to ompare the performan e of the di erent ombinationmethods, namely: overall performan e and error redu tion rate (ERR). The overall perfor-man e is the mean of lassi ation a ura y obtained by ombining all onsidered subsetsof 2; ; N lassi ers. ERR is the per entage of error redu tion obtained by ombining lassi ers with referen e to the best single lassi er:ERR = ERBSC ERCCERBSC 100 (21)where ERBSC is the error rate of the best single lassi er and ERCC is the error rate ob-tained by ombining the onsidered lassi ers. Unlike lassi ation a ura y, ERR learlyshows how the performan e of the ombined lassi ers improves or deteriorates omparedto the best single lassi er. In other words, it shows the merit of performing the ombi-nation. We will spe i ally on entrate on the maximum ERR obtained by ombining allthe onsidered subsets of 2; ; N lassi ers. In addition, we will also investigate how thevalue of ERR gets a e ted by in reasing the number of ombined lassi ers.5.1 Texture Classi ationSeveral experiments have been arried out for the lassi ation of texture images. Thetextures onsidered here are: bark, bri k, bubbles, leather, raÆa, water, weave, wood andwool (USC, 1981). In order to obtain a better omparison between the di erent ombinationmethods, we onsidered lassifying the rst two textures, then the rst three, the rst veand nally all the nine textures. Additive Gaussian noise, with di erent signal-to-noise ratio,has been added to (1024 1024) pixels image of ea h texture lass to form the training andtesting sets. 961 patterns were obtained from ea h image using (64 64) windows with anoverlap of 32 pixels.3. The weights of ea h lassi er are determined a ording to the lassi ation a ura y of ea h lass labelusing the training dataset4. The reader may refer to Appendix A for a brief des ription of this method343\nAl-Ani & Deri heNo. of lasses SDH1 SDH2 SDH3 SDH4 En2 86.96 85.73 84.44 85.45 91.143 84.58 84.52 83.91 86.24 89.725 85.10 84.62 84.34 83.46 88.849 80.97 77.44 77.51 75.72 83.65Table 1: Texture lassi ation a ura y of the ve original lassi ers for di erent numberof lass labelsFour nine-feature ve tors were al ulated using statisti s of sum and di eren e histogram(SDH) of the o-o urren e matrix with di erent dire tions, verti al (SDH1), horizontal(SDH2), and the two diagonals (SDH3 and SDH4) . For ea h dire tion, the features usedwere: mean, varian e, energy, orrelation, entropy, ontrast, homogeneity, luster shade,and luster prominen e. The fra tal dimension (FD) has also been used to form the tenthfeature of ea h ve tor. The energy ontents of texture images (En) has been used to formanother feature ve tor using 9 di erent masks. Again the tenth feature was FD.Ea h of these ve feature ve tors has been used as input to an ANN. The numbers oftraining and testing patterns depend upon number of lasses onsidered, i.e. for the aseof two lasses, 15376 patterns were used to train the networks and 5766 to test them. Theresults obtained are shown in Table 1. Note that as the number of lasses in reases theoverall a ura y de reases. In addition, the performan e of the En lassi ers is found to bebetter than that of the other four.No. of lasses WS Av Md Mx MV FI DS0 DS1 DS22 89.16 89.04 87.66 90.12 88.09 90.08 88.70 90.66 90.723 88.52 88.39 87.41 88.86 87.30 88.71 88.40 90.21 90.085 89.60 89.41 87.99 89.23 87.83 90.28 89.52 92.69 91.509 84.96 84.55 83.37 82.90 83.23 86.76 84.87 89.83 86.79Table 2: Overall performan e of the various ombination methods for di erent number of lass labels (texture lassi ation)The overall performan e of the tested ombination methods for di erent number of lasslabels are shown in Table 2. For the ase of 2 lasses, it is lear that the overall performan esof DS1 and DS2 are better than that of the other ombination methods. When mixtures ofgood and bad lassi ers are onsidered, the performan e of ombination methods, ex eptfor DS1 and DS2, is loser to or worse than that of the best single lassi er. This is shownin Table 3 for the ombination of f 1; 3; 4; 5g, f 1; 4; 5g, f 1; 5g, et 5. When 3 and 5 lasses are onsidered, DS1 performs slightly better than DS2, and both outperform theother methods. The gap between DS1 and other methods gets wider when all 9 lasses are onsidered. The superiority of DS1 re e ts the advantage of using the whole output ve torin measuring eviden es of lassi ers.5. The reader may refer to Appendix B for detailed results of other ases344\nA New Te hnique for Combining Multiple ClassifiersClassi ers WS Av Md Mx MV FI DS0 DS1 DS2 1, 2 92.56 92.59 92.59 92.51 92.51 92.61 92.40 92.46 92.46 1, 5 91.16 91.12 91.12 91.09 91.09 91.00 91.33 91.62 91.61 4, 5 85.07 85.07 85.07 85.22 85.22 85.07 85.15 85.10 85.12 1, 2, 3 91.21 91.21 88.92 91.62 88.92 91.69 90.81 92.40 92.53 1, 2, 5 91.03 90.81 88.68 91.48 88.71 91.48 90.43 92.47 92.39 1, 4, 5 89.80 89.59 86.21 91.21 86.21 91.24 88.88 91.68 91.78 3, 4, 5 85.38 85.38 85.47 85.40 85.48 85.33 85.22 85.43 85.40 1, 2, 3, 4 89.94 89.70 87.84 91.47 89.13 91.59 89.13 92.21 92.33 1, 2, 3, 5 89.70 89.42 87.53 91.42 89.04 91.29 88.92 92.32 92.42 1, 2, 4, 5 89.72 89.49 87.37 91.48 88.94 91.40 89.00 92.25 92.26 1, 3, 4, 5 88.57 88.45 86.30 91.09 87.03 90.98 87.81 91.78 91.87 2, 3, 4, 5 86.07 86.11 85.87 86.25 86.26 86.13 85.93 86.66 86.80 1, 2, 3, 4, 5 88.81 88.54 86.63 91.33 86.59 91.21 88.10 92.21 92.33Table 3: Classi ation a ura y of texture images using di erent ombination methods (2textures)The best ERR values of WS, FI, DS0, DS1 and DS2 are determined a ording to Eq.21. Sin e WS has been widely used in the literature, and it outperforms other onventionalmethods (Av, Md, Mx, and MV), as observed in Table 2, then we will use it as a represen-tative of the onventional methods when performing the omparison with FI, DS0, DS1 andDS2. Figure 3a shows the ERR values when 2 lasses are onsidered. It is lear that themaximum ERR values of these ve ombination methods are very lose, ranging between14% to 16%. They are obtained by ombining the best two lassi ers for WS, FI and DS0,while DS1 and DS2 use three lassi ers to obtain their maximum ERR. As mentionedearlier, The performan e of the rst four individual lassi ers is weaker than that of theEn. Noti e that, for both DS1 and DS2, there is no signi ant degradation in ERR as thenumber of ombined lassi ers in reases.For the ase of 3 lasses, both DS1 and DS2 outperform other ombination methodsin terms of the maximum ERR. They a hieve values of 17:3% and 19:6% respe tively, ompared to 11:4% or less for other methods as shown in Figure 3b. In addition, ERR ofDS1 and DS2 are not a e ted as the number of ombined lassi ers in reases.For the ase of 5 lasses, the maximum ERR values sorted in a des ending order are:DS1 50:7%, DS2 40:2%, FI 31:6%, WS 28:1%, and DS0 23:8%, as shown in Figure 3 . Inaddition, ERR values of DS1 improve as the number of ombined lassi ers in reases, DS2is the se ond best, while ERR values of other methods degrade as the number of ombined lassi ers in reases. For the ase of 9 lasses, the superiority of DS1 be omes learer, whereas shown in Figure 3d, the maximum ERR value of DS1 is 54% ompared to 37:5% or lessfor other methods. It is worth mentioning that even though the maximum ERR values ofother methods degrade, they still perform better than the best single lassi er. This leadsus to on lude that as the number of lasses in reases, the performan e of most lassi er ombination methods gets better overall. 345\nAl-Ani & Deri he\n−30\n−20\n−10\n0\n10\n20\nWS FI DS0 DS1 DS2 −20\n−10\n0\n10\n20\n30\n2 2.5 3 3.5 4 4.5 5 0\n10\n20\n30\n40\n50\nE R\nR\nNo. of combined classifiers 2 2.5 3 3.5 4 4.5 5\n10\n20\n30\n40\n50\n(a) 2 classes (b) 3 classes\n(c) 5 classes (d) 9 classesFigure 3: ERR of di erent lassi er ombination methods obtained by onsidering di erentnumber of lassi ers for the ases: (a) 2 lasses, (b) 3 lasses, ( ) 5 lasses, and(d) 9 lassesTaking all these fa ts into onsideration, we an sort the methods in a des ending orderas follows: DS1, DS2, FI, WS, DS0, and the other onventional methods. Thus, in summary,for the problem of texture lassi ation, our proposed te hnique with its two alternatives(DS1 and DS2) learly outperforms other standard ombination methods with an in reasein lassi ation a ura y of about 2 7%. For the ases of 2 and 3 lasses, there is a littledi eren e in performan e between DS1 and DS2. This is be ause using referen e ve tors ofsmall size, 2 1 and 3 1, does not make a big impa t upon the estimation of eviden e ompared to that obtained using a single referen e value. As the size of the referen e ve torin reases, 5 1 and 9 1 for the other two ases, its impa t on estimating the eviden ebe omes learer, whi h leads to better results, but at the ost of in reasing omputationalload.5.2 Spee h Segment Classi ationSix di erent input feature sets have been used to lassify spee h segments a ording to theirmanner of arti ulation, these were: 13 mel-frequen y epstral oeÆ ients (MFC), 16 logmel- lter bank (MFB), 12 linear predi tive epstral oeÆ ients (LPC), 12 linear predi tive346\nA New Te hnique for Combining Multiple ClassifiersNo. of lasses MFC MFB LPC LPR WVT ARP3 88.21 90.98 81.64 80.69 90.64 70.876 83.16 85.50 74.77 74.06 84.33 62.909 78.48 83.24 71.64 70.03 81.33 56.66Table 4: Spee h segment lassi ation a ura y of the six original lassi ers for di erentnumber of lass labelsre e tion oeÆ ients (LPR), 10 wavelet energy bands (WVT), and 12 autoregressive modelparameters (ARP). For this experiment, spee h was obtained from the TIMIT database(MIT, SRI, & TI, 1990). Segments of 152 speakers (56456 segments) were used to trainthe ANNs, and 52 speakers (19228 segments) to test them. Three ases were onsidered:3 lasses (vowel, onsonant, and silen e), 6 lasses (vowel, nasal, fri ative, stop, glide, andsilen e), and nally 9 lasses (vowel, semi-vowel, nasal, fri ative, stop, losure, lateral,rhoti , and silen e). The lassi ation results for these three ases are summarized in Table4. No. of lasses WS Av Md Mx MV FI DS0 DS1 DS23 90.80 90.41 90.20 86.15 89.51 90.65 90.90 91.57 91.316 85.54 84.91 84.62 81.16 84.03 85.29 85.18 87.18 86.379 83.05 82.31 81.93 75.63 81.00 82.73 82.86 85.20 84.22Table 5: Overall performan e of the various ombination methods for di erent number of lass labels (spee h segment lassi ation)The two best individual lassi ers are MFB and WVT in all three ases, followed byMFC then other methods. Unlike texture lassi ers that had one good lassi er and four,relatively, weak lassi ers, we have here three good lassi ers (MFB, MFC and WVT) andthree weak lassi ers (LPC, LPR and ARP).The overall performan e values of the various ombination methods are displayed inTable 5. For the ase of the 3 lasses, it an be seen that the overall performan e of DS1 isbetter than that of DS2 and they both outperform the other methods. This be omes even learer as the number of lasses in reases (with more than 2% in rease in a ura y).The ERR values for the ase of 3 lasses are shown in Figure 4a. The maximum ERRvalue of DS1 is 23:4%, whi h is a hieved by ombining all six lassi ers, ompared to 20:3%for DS2 and 19:6% or less for the other methods. The gap between DS1 and the othermethods gets wider when we onsider 6 and 9 lasses as shown in Figures 4b and 4 .Be ause there are more good lassi ers in this experiment ompared to that of the textureexperiment, the variations of the ERR values when the number of lassi ers in reases arefound to be smaller. In addition, we an see that as the number lasses in reases DS1 keepsits steady and superior performan e in terms of ERR with more than 10% in rease.As a summary, DS1 outperforms other methods in terms of overall performan e andERR measurements. It is followed by DS2, WS, and the rest of the methods.347\nAl-Ani & Deri he\n2 3 4 5 6\n5\n10\n15\n20\n25\n2 3 4 5 6\n10\n15\n20\n25\nWS FI DS0 DS1 DS2\n2 3 4 5 6\n5\n10\n15\n20\n25\nNo. of combined classifiers\nE R\nR (a) 3 classes (b) 6 classes\n(c) 9 classesFigure 4: ERR of di erent lassi er ombination methods obtained by onsidering di erentnumber of lassi ers for the ases: (a) 3 lasses, (b) 6 lasses, and ( ) 9 lasses5.3 Speaker Identi ationThree limited-s ope experiments were arried out to perform speaker identi ation using 2,3, and 4 speakers. Spee h data from the TIMIT database was also used (MIT et al., 1990).The number of training patterns were 3232, 4481 and 5931 respe tively, and the number oftesting patterns were 1358, 1921 and 2542 respe tively. The same features used to lassifyspee h segments a ording to their manner of arti ulation were used to identify speakers.Classi ation results of the six lassi ers are shown in Table 6. The performan e of theindividual lassi ers are not quite similar to the spee h segment problem, where the threegood lassi ers are: MFB, MFC and LPC and the three weak lassi ers are: LPR, WVTand ARP.The overall performan e of the various ombination methods are shown in Table 7. Forthe ase of 2 lasses, it is lear that the overall performan e of most ombination methodsis very omparable. The superiority of DS1, and to a lesser degree DS2, be omes lear asthe number of lasses in reases (more patterns were in luded to estimate eviden e).Note that, be ause of the high performan e of individual lassi ers for the ase of 2 lasses, a small di eren e in the performan e of ombination methods will have great impa ton ERR, whi h explains the graphs' u tuations, as shown in Figure 5a. It an be seen348\nA New Te hnique for Combining Multiple ClassifiersNo. of lasses MFC MFB LPC LPR WVT ARP2 94.58 96.17 92.49 89.60 87.80 84.553 85.84 87.25 82.20 81.00 74.39 73.034 85.01 85.96 80.84 77.97 70.93 64.59Table 6: Speaker identi ation a ura y of the six original lassi ers with di erent numberof speakersNo. of lasses WS Av Md Mx MV FI DS0 DS1 DS22 95.53 95.50 95.26 95.36 95.21 95.25 95.46 95.48 95.453 90.80 90.41 90.20 86.15 89.51 90.65 90.90 91.57 91.314 83.05 82.31 81.93 75.63 81.00 82.73 82.86 85.20 84.22Table 7: Overall performan e of the various ombination methods for di erent number of lass labels (speaker identi ation)that both maximum ERR and overall performan e of most ombination methods are lose.These results do not favor DS1 nor DS2, be ause they have an additional omputational ost. Let's now onsider the ase of 3 lasses, Figure 5b shows that the maximum ERRof DS2 is the highest followed by DS1, and they both outperform the other methods. Forthe ase of 4 lasses, the maximum ERR of DS1 is 30%, ompared to 27% or less for othermethods, as shown in Figure 5 . The gure also shows that ERR values of DS2 and WSare lose. However, as the overall performan e of DS2 is better than that of WS, DS2 anbe onsidered as the se ond best method followed by WS, DS0 and nally FI.The above results learly show how the performan e of DS1 and DS2 get a e ted by thenumber of training patterns, whi h is ru ial in a hieving good estimation of the eviden e ofea h lassi er. This is very lear for the ase of 2 speakers. Their performan e, however, getbetter as the number of speakers and training patterns in rease. In other words, DS1 andDS2 require a larger number of patterns to work properly. Failing to provide su h numberof patterns, other onventional methods, su h as WS, an a hieve similar performan e.The experiments of textures, spee h segments and speaker lassi ation show that ourproposed te hnique learly outperforms the other methods in terms of overall performan eand ERR, providing that a suÆ ient number of patterns to estimate eviden e of lassi ersexists. Also, among the di erent ombination methods, DS1 and DS2 are the least e e tedby the in lusion of weak lassi ers. The experiments also show that the BBAs ould bebetter estimated using referen e ve tors rather than referen e values, espe ially for largenumber of lasses.It is worth mentioning that ea h one of the ombination methods has its own merit.For example, the MV is very useful ombination method when dealing with lassi ers thatprodu e results of the abstra t level. When working in the measurement level, other om-bination methods ould have better performan e.The Mx method an provide good results when the performan e of the ombined lassi- ers are lose. In su h ase, the lassi er with higher on den e an provide better results349\nAl-Ani & Deri he\n2 3 4 5 6\n−5\n0\n5\n10\n15\n20\n2 3 4 5 6\n15\n20\n25\n30\n35\n40\nWS FI DS0 DS1 DS2\n2 3 4 5 6\n5\n10\n15\n20\n25\n30\nNo. of combined classifiers\nE R\nR (a) 2 classes (b) 3 classes\n(c) 4 classes Figure 5: ERR of di erent lassi er ombination methods obtained by onsidering di erentnumber of lassi ers for the ases: (a) 2 lasses, (b) 3 lasses, and ( ) 4 lassesthan any individual lassi er. This is shown in Tables 11-13 (refer to Appendix B), wheregood results are a hieved when ombining the best two or three lassi ers of the spee hsegment experiment ompared to the best individual lassi er. However, if there is a leardi eren e in the performan e of lassi ers, as in the ase when onsidering mixtures of goodand bad lassi ers, then using Mx to ombine the lassi ation results will not be a good hoi e. In ase we don't have any information about the performan e of the lassi ers, i.e.,there is no training dataset, the Av and Md methods ould provide an attra tive hoi e.Similar to the ndings of (Kittler et al., 1998; Alkoot & Kittler, 1999), the performan e ofthese two methods are found to be lose with slight favor of the Av method. If the las-si ation a ura y of the di erent lassi ers are available, then the WS method representsa good hoi e, where it outperforms Av in almost all the ondu ted experiments. This isexpe ted, as asso iating ea h lassi er with a weight that re e ts its performan e, wouldmake the better lassi er ontributes more towards the nal de ision. If the performan eof the ombined lassi ers are very lose, then ombining their results using both the Avand WS methods would lead to very similar performan e, as shown in Tables 11-13 for the ases of ombining the best two and three spee h segment lassi ers.The FI and DS0 represent two non-linear ombination methods. A ording to (Cho &Kim, 1995a), the performan e of FI was slightly better than the WS when tested using an350\nA New Te hnique for Combining Multiple Classifiersopti al hara ter re ognition database, whi h is similar to the results we obtained for thetexture experiments. However, for spee h segment lassi ation and speaker identi ationexperiments, the performan e of FI was not as good as that of WS. On the other hand, theexperiments ondu ted here show that WS slightly outperforms DS0. Note that Rogova(1994) only ompared DS0 to the original lassi ers. The main problem with both FIand DS0 is the appropriate estimation of their parameters. For example, the desired sumof fuzzy densities a e ts the ombination results of FI, while the hoi e of the proximitymeasure and referen e ve tor plays an important role in the performan e of DS0.DS1 and DS2 di er from DS0 by the appropriate measure of the referen e ve tors,and hen e the a urate estimation of the eviden e of ea h lassi er. This will exploitthe omplementary information provided by the di erent lassi ers. In other words, thea urate estimation of eviden e of ea h lassi er will lead to minimizing the MSE of the ombined results, and hen e resolving the on i ts between lassi ers.6. Con lusionWe have developed in this work a new powerful lassi er ombination te hnique basedon the D-S theory of eviden e. The te hnique, based on adjusting the eviden e of di erent lassi ers by minimizing the MSE of training data, gave very good results in terms of overallperforman e and error redu tion rate. To test the algorithm, three experiments were arriedout: texture lassi ation, spee h segments lassi ation, and speaker identi ation. Allof the experiments showed the superiority of the proposed te hnique when ompared to onventional methods, fuzzy integral, and another D-S implementation that uses a di erentmeasure of eviden e. We have shown that a urate estimation of the eviden e from di erent lassi ers based on the whole output ve tors (DS1) gives the best performan e, espe iallyfor higher number of lass labels. The only drawba k of the algorithm is that training an be omputationally expensive (this is used to a urately estimate the eviden e of ea h lassi er). However, this an be exe uted o -line, and as su h, has no major e e t on theperforman e of the algorithm. We have also shown that the proposed algorithm an easilya hieve an in rease in lassi ation a ura y of the order of 2% to 7% ompared to other ombination methods. We believe that with more work on enhan ing the te hnique, thes heme an form a new framework for pattern lassi ation in the future.A knowledgmentThe authors wish to thank Dr. J. Chebil and Dr. M. Mesbah for their valuable ommentson the paper. The authors also a knowledge the support of Queensland University ofTe hnology for the work presented in this paper. Dr. Deri he a knowledges the support ofKing Fahd University, Saudi Arabia, where he is urrently on leave.Appendix A. Classi er Combination Based on the Fuzzy IntegralFuzzy integral is a non-linear ombination method de ned with respe t to a fuzzy measure.Detailed explanation of lassi er ombination based on the g fuzzy measure an be foundin the work of Cho & Kim (1995a, 1995b). 351\nAl-Ani & Deri heFor a nite set of elements, Z, the g fuzzy measure (Sugeno, 1977) is de ned as the setfun tion g: 2Z ! [0; 1℄ that satis es the following onditions:1. g(;) = 0; g(Z) = 1,2. g(A) g(B) if A B,3. if fAig1i=1 is an in reasing sequen e of measurable sets, then limi!1 g(Ai) = g(limi!1Ai),4. g(A [B) = g(A) + g(B) + g(A)g(B)for all A;B Z and A \\ B = ;, and for some > 1. Let h : Z ! [0; 1℄ be a fuzzysubset of Z. The fuzzy integral over Z of the fun tion h with respe t to a fuzzy measure gis de ned by h(z) Æ g(:) = maxE Z min minz2E h(z):g(E) = max 2[0;1℄[min( ; g(F ))℄; whereF = fzjh(z) gLet Z = fz1; zng, and suppose that h(z1) h(z2) h(zn), (if not, Z is rearrangedso that this relation holds). Then a fuzzy integral e, with respe t to a fuzzy measure g overZ an be omputed bye = nmaxi=1 [min(h(zi); g(Ai))℄; whereAi = fz1; zigg(A1) = g(fz1g) = g1g(Ai) = gi + g(Ai 1) + gig(Ai 1); for 1 < i n is given by solving: + 1 = Qni=1(1 + gi), where 2 ( 1;1) and 6= 0. This an be al ulated by solving an (n 1)st degree polynomial and nding the unique root greaterthan 1.For the problem of ombining lassi ers, Z represents the set of lassi ers, A the obje tunder onsideration for lassi ation, and hk(zi) is the partial evaluation of the obje t A for lass !k. Corresponding to ea h lassi er zi, the degree of importan e, gi, that re e ts howgood is zi in the lassi ation of lass !k must be given. These densities an be indu edfrom a training dataset.\n352\nA New Te hnique for Combining Multiple ClassifiersAppendix B. Tables of Classi ation A ura y for Di erent CombinationMethodsClassi ers WS Av Md Mx MV FI DS0 DS1 DS2 1, 2 90.89 90.79 90.79 90.83 90.17 90.22 90.81 91.09 90.81 1, 5 89.69 89.57 89.57 89.68 89.10 88.99 89.92 90.45 90.29 4, 5 85.05 85.05 85.05 84.67 84.81 85.13 85.51 85.49 85.44 1, 2, 3 89.92 89.83 88.04 90.48 87.77 89.88 89.59 91.26 91.73 1, 2, 5 89.55 89.39 87.54 90.23 87.09 89.56 89.18 91.12 91.10 1, 4, 5 89.05 88.80 86.95 89.62 86.62 89.26 88.80 90.77 90.73 3, 4, 5 85.76 85.73 85.49 84.81 85.49 85.67 85.91 87.27 86.88 1, 2, 3, 4 89.29 89.07 87.98 90.17 87.91 89.85 88.82 91.44 91.51 1, 2, 3, 5 89.07 88.83 87.24 90.01 87.50 89.58 88.81 91.48 91.53 1, 2, 4, 5 88.87 88.78 87.34 90.02 87.61 89.64 88.66 91.16 91.26 1, 3, 4, 5 88.60 88.38 87.05 89.39 87.52 89.44 88.39 91.31 91.00 2, 3, 4, 5 86.45 86.44 86.18 85.52 86.31 86.36 86.46 88.37 87.20 1, 2, 3, 4, 5 88.54 88.47 87.06 89.73 87.03 89.65 88.29 91.50 91.61Table 8: Classi ation a ura y of texture images using di erent ombination methods (3textures)Classi ers WS Av Md Mx MV FI DS0 DS1 DS2 1, 2 91.98 91.87 91.87 90.28 89.89 92.21 91.45 93.40 92.81 1, 5 91.95 91.74 91.74 90.72 89.58 91.30 91.55 93.27 92.72 4, 5 85.14 85.19 85.19 84.77 84.50 84.94 85.82 86.28 85.36 1, 2, 3 91.49 91.29 88.78 90.39 88.60 92.35 91.08 94.37 93.19 1, 2, 5 90.77 90.50 87.92 90.34 87.74 91.54 90.32 93.66 92.96 1, 4, 5 90.61 90.35 87.67 90.29 87.43 91.41 90.25 93.69 92.90 3, 4, 5 86.29 86.22 85.40 85.99 85.64 85.91 86.98 89.16 87.29 1, 2, 3, 4 90.26 90.03 88.37 90.12 88.90 91.86 89.93 94.48 93.19 1, 2, 3, 5 90.14 89.91 88.17 90.25 89.00 91.83 90.09 94.41 93.26 1, 2, 4, 5 89.73 89.43 87.62 90.19 88.45 91.11 89.41 93.82 92.56 1, 3, 4, 5 90.32 90.06 87.91 90.50 88.51 91.83 90.20 94.50 93.33 2, 3, 5, 6 86.41 86.38 85.99 85.98 85.79 86.15 87.21 89.47 86.97 1, 2, 3, 4, 5 89.65 89.39 87.27 90.11 87.81 91.22 89.51 94.46 93.01Table 9: Classi ation a ura y of texture images using di erent ombination methods (5textures) 353\nAl-Ani & Deri heClassi ers WS Av Md Mx MV FI DS0 DS1 DS2 1, 2 88.45 88.13 88.13 85.44 85.91 89.78 89.26 91.42 89.20 1, 5 86.39 85.97 85.97 82.27 83.00 88.18 87.10 90.07 88.08 4, 5 79.55 79.40 79.40 78.74 78.53 79.37 79.28 81.22 79.91 1, 2, 3 87.10 86.53 84.20 84.65 84.60 89.51 87.06 92.00 88.59 1, 2, 5 86.48 85.85 83.66 84.08 84.17 88.72 86.92 92.13 88.53 1, 4, 5 85.92 85.32 83.22 82.97 83.67 88.25 85.98 92.01 88.30 3, 4, 5 80.27 80.21 79.63 79.54 79.38 79.97 79.80 82.48 80.89 1, 2, 3, 4 86.42 85.95 84.56 84.33 85.16 89.18 85.96 92.45 88.70 1, 2, 3, 5 85.70 85.11 83.37 83.91 84.26 88.52 85.41 92.02 88.30 1, 2, 4, 5 85.79 85.39 84.29 83.83 84.91 88.52 85.94 92.48 88.66 1, 3, 4, 5 85.42 84.92 83.06 83.47 84.08 88.34 84.81 92.35 88.35 2, 3, 4, 5 81.60 81.44 81.15 80.75 81.03 81.54 81.02 84.70 82.17 1, 2, 3, 4, 5 85.33 84.88 83.19 83.74 83.35 88.02 84.83 92.43 88.65Table 10: Classi ation a ura y of texture images using di erent ombination methods (9textures)Classi ers WS Av Md Mx MV FI DS0 DS1 DS2 1, 2 92.34 92.34 92.34 92.34 92.22 91.84 92.38 92.45 92.29 1, 6 89.99 88.11 88.11 83.95 82.64 91.06 90.92 91.48 91.42 5, 6 81.09 80.65 80.65 76.19 76.10 82.03 81.54 82.29 82.01 1, 2, 3 92.63 92.61 92.37 92.34 92.27 92.36 92.63 92.79 92.67 1, 2, 6 92.17 91.79 91.83 85.97 91.60 92.03 92.27 92.53 92.23 1, 5, 6 89.85 88.79 88.20 84.17 87.50 89.66 90.34 91.92 91.79 4, 5, 6 84.98 84.96 84.76 79.36 84.08 84.89 84.62 85.65 85.36 1, 2, 3, 4 92.59 92.57 92.42 91.64 92.04 92.38 92.61 92.77 92.64 1, 2, 3, 6 92.62 92.47 92.50 86.94 92.02 92.49 92.73 92.78 92.54 1, 2, 5, 6 92.25 91.93 91.82 86.01 91.96 92.14 92.37 92.77 92.49 1, 4, 5, 6 90.15 89.51 89.34 84.64 89.84 89.48 90.12 91.92 91.84 3, 4, 5, 6 88.79 88.42 88.30 83.25 88.46 88.46 88.76 90.51 89.94 1, 2, 3, 4, 5 92.75 92.64 92.23 91.42 92.13 92.49 92.74 93.07 92.81 1, 2, 3, 4, 6 92.57 92.48 92.30 86.96 91.92 92.25 92.56 92.77 92.56 1, 2, 3, 5, 6 92.73 92.50 92.20 86.93 91.91 92.53 92.76 93.03 92.72 1, 2, 4, 5, 6 92.14 91.70 91.07 86.19 90.97 91.71 92.23 92.78 92.46 1, 3, 4, 5, 6 91.41 91.04 90.63 85.79 90.41 90.98 91.46 92.48 92.14 2, 3, 4, 5, 6 91.51 90.98 90.52 85.73 90.60 91.15 91.50 92.67 92.40 1, 2, 3, 4, 5, 6 92.62 92.36 92.24 86.95 91.97 92.38 92.61 93.09 92.64Table 11: Classi ation a ura y of spee h segments using di erent ombination methods(3 lasses) 354\nA New Te hnique for Combining Multiple Classifiers\nClassi ers WS Av Md Mx MV FI DS0 DS1 DS2 1, 2 86.97 86.95 86.95 86.46 86.30 86.67 86.64 88.12 87.59 1, 6 84.30 81.98 81.98 77.97 76.66 85.01 84.52 86.71 86.39 5, 6 74.60 73.91 73.91 70.38 70.20 75.00 74.20 76.47 75.68 1, 2, 3 88.09 88.08 87.77 87.11 87.76 87.51 87.49 88.73 88.10 1, 2, 6 86.48 85.76 86.01 80.38 85.31 86.49 86.45 88.19 87.50 1, 5, 6 84.33 82.80 81.88 78.64 81.24 83.88 84.56 87.16 86.45 4, 5, 6 79.27 78.73 77.84 74.07 77.26 78.46 78.51 80.17 79.39 1, 2, 3, 4 88.03 87.91 87.81 86.58 87.29 87.67 87.48 88.98 88.09 1, 2, 3, 6 87.64 87.17 87.26 82.59 86.95 87.37 87.25 88.61 88.08 1, 2, 5, 6 86.51 85.84 85.90 80.54 86.10 86.69 86.59 88.47 87.57 1, 4, 5, 6 84.61 83.64 83.28 79.47 83.85 83.97 84.29 87.38 86.68 3, 4, 5, 6 83.84 83.12 82.84 79.28 83.05 83.22 83.56 86.00 84.95 1, 2, 3, 4, 5 87.90 87.87 87.44 86.01 87.35 87.72 87.34 89.15 88.15 1, 2, 3, 4, 6 87.57 87.24 86.98 82.76 86.85 87.36 87.08 88.95 88.11 1, 2, 3, 5, 6 87.69 87.17 86.91 82.64 86.96 87.52 87.32 88.93 88.12 1, 2, 4, 5, 6 86.71 86.00 85.54 81.21 85.63 86.46 86.39 88.64 87.56 1, 3, 4, 5, 6 86.36 85.85 85.26 81.50 85.57 86.10 85.95 88.26 87.38 2, 3, 4, 5, 6 86.70 85.95 85.18 81.63 85.23 86.06 85.81 88.37 87.28 1, 2, 3, 4, 5, 6 87.67 87.33 87.02 82.76 86.95 87.35 86.98 89.14 88.01Table 12: Classi ation a ura y of spee h segments using di erent ombination methods(6 lasses)\n355\nAl-Ani & Deri he\nClassi ers WS Av Md Mx MV FI DS0 DS1 DS2 1, 2 84.58 84.61 84.61 83.77 83.66 84.06 84.15 86.11 85.60 1, 6 81.85 78.59 78.59 71.48 70.50 82.53 82.51 84.41 83.72 5, 6 70.48 69.04 69.04 62.87 62.65 71.22 69.97 73.24 71.94 1, 2, 3 85.41 85.42 85.00 83.81 85.02 85.00 85.34 86.85 86.15 1, 2, 6 84.20 83.44 83.53 74.18 82.70 84.11 84.01 86.05 85.44 1, 5, 6 82.18 80.26 79.19 72.32 77.86 81.20 82.53 85.10 84.56 4, 5, 6 75.94 75.23 74.43 67.23 73.46 75.30 75.54 78.21 76.92 1, 2, 3, 4 85.44 85.30 85.20 83.41 84.96 85.11 85.15 87.05 86.13 1, 2, 3, 6 85.34 84.97 84.99 76.49 84.82 85.19 85.14 86.79 86.16 1, 2, 5, 6 84.48 83.77 83.77 74.61 83.98 84.58 84.53 86.67 85.86 1, 4, 5, 6 82.41 81.44 81.03 73.61 81.49 81.61 82.44 85.65 84.76 3, 4, 5, 6 81.02 80.51 80.15 72.74 79.83 80.06 80.41 83.86 82.25 1, 2, 3, 4, 5 85.52 85.43 84.94 82.93 84.77 85.45 85.42 87.32 86.32 1, 2, 3, 4, 6 85.53 85.01 84.41 77.04 84.53 84.96 85.00 87.09 86.09 1, 2, 3, 5, 6 85.54 85.16 84.76 76.62 84.66 85.38 85.25 87.11 86.22 1, 2, 4, 5, 6 84.60 84.03 83.34 75.51 83.69 84.17 84.42 86.82 85.78 1, 3, 4, 5, 6 83.97 83.32 82.74 75.80 83.16 83.67 83.91 86.59 85.45 2, 3, 4, 5, 6 84.14 83.48 82.32 75.40 82.66 83.29 83.44 86.55 85.56 1, 2, 3, 4, 5, 6 85.32 84.93 84.70 77.08 84.51 85.07 85.19 87.26 85.23Table 13: Classi ation a ura y of spee h segments using di erent ombination methods(9 lasses)\n356\nA New Te hnique for Combining Multiple Classifiers\nClassi ers WS Av Md Mx MV FI DS0 DS1 DS2 1, 2 96.10 96.10 96.10 96.10 96.10 95.16 96.02 95.51 95.80 1, 6 95.38 94.95 94.95 94.95 94.95 96.25 95.36 96.17 96.17 5, 6 90.25 90.69 90.69 90.83 90.83 87.51 89.32 88.59 88.66 1, 2, 3 96.68 96.68 96.90 96.97 96.90 96.61 96.98 96.91 96.54 1, 2, 6 95.81 95.74 95.23 95.38 95.23 96.17 95.73 95.51 95.88 1, 5, 6 94.66 94.73 94.37 95.09 94.51 94.95 94.62 96.24 96.17 4, 5, 6 92.20 92.13 91.77 92.64 91.70 91.48 91.75 91.16 91.16 1, 2, 3, 4 96.90 96.90 96.97 96.68 96.61 96.61 97.05 96.69 96.69 1, 2, 3, 6 97.04 96.82 96.97 96.10 96.53 96.97 96.91 96.76 96.69 1, 2, 5, 6 95.88 95.88 95.74 95.16 95.96 96.17 95.95 95.80 95.80 1, 4, 5, 6 95.88 95.81 95.52 95.52 95.45 94.95 95.95 95.88 96.24 3, 4, 5, 6 95.02 94.95 94.30 94.01 94.30 94.30 95.14 94.55 94.40 1, 2, 3, 4, 5 96.82 96.75 96.25 96.53 96.25 96.82 96.69 96.76 96.61 1, 2, 3, 4, 6 96.10 96.10 95.45 96.32 95.45 96.25 96.02 96.24 96.10 1, 2, 3, 5, 6 96.53 96.46 96.17 95.88 96.10 96.75 96.47 96.76 96.54 1, 2, 4, 5, 6 95.74 95.67 95.38 95.60 95.31 95.81 95.88 96.02 95.95 1, 3, 4, 5, 6 96.39 96.32 95.60 96.39 95.52 95.96 96.24 96.39 96.24 2, 3, 4, 5, 6 95.16 95.23 95.02 95.45 94.95 95.16 95.14 95.58 95.58 1, 2, 3, 4, 5, 6 96.53 96.61 96.53 96.17 96.25 95.96 96.54 96.54 96.39Table 14: Speaker identi ation a ura y using di erent ombination methods (2 speakers)\n357\nAl-Ani & Deri he\nClassi ers WS Av Md Mx MV FI DS0 DS1 DS2 1, 2 89.22 89.17 89.17 88.70 88.91 87.82 89.22 89.33 89.17 1, 6 88.91 88.55 88.55 87.61 85.58 89.38 88.86 89.90 89.95 5, 6 80.37 80.90 80.90 79.44 77.93 75.90 79.44 80.37 79.85 1, 2, 3 90.53 90.47 90.32 89.28 90.06 89.80 90.47 90.58 91.20 1, 2, 6 90.37 90.58 89.90 88.65 90.06 89.43 90.58 90.32 90.21 1, 5, 6 88.70 88.60 87.35 87.25 86.93 88.29 88.96 90.11 90.58 4, 5, 6 84.54 84.17 83.60 83.24 83.34 83.08 83.86 84.64 85.06 1, 2, 3, 4 91.10 91.10 90.58 89.59 90.32 90.94 91.31 91.78 91.51 1, 2, 3, 6 91.25 91.51 91.36 88.70 90.06 90.89 91.62 91.62 92.04 1, 2, 5, 6 90.63 90.47 90.47 88.81 88.39 90.16 90.58 90.94 90.21 1, 4, 5, 6 90.37 90.32 89.48 87.87 88.55 89.54 90.47 91.57 91.46 3, 4, 5, 6 86.88 86.78 85.94 84.64 84.54 85.79 86.15 87.40 87.35 1, 2, 3, 4, 5 91.20 91.20 90.58 89.59 89.59 90.84 91.72 91.93 91.78 1, 2, 3, 4, 6 91.36 91.31 91.25 88.96 90.32 90.89 91.36 91.93 92.40 1, 2, 3, 5, 6 90.99 91.15 90.58 88.81 90.06 90.73 91.20 91.62 91.88 1, 2, 4, 5, 6 91.41 91.67 90.99 88.86 90.47 90.89 91.46 91.83 92.09 1, 3, 4, 5, 6 91.05 90.99 89.69 88.34 89.59 89.85 91.04 91.83 91.41 2, 3, 4, 5, 6 90.37 90.27 89.22 88.24 88.81 88.96 89.38 90.42 90.32 1, 2, 3, 4, 5, 6 91.51 91.36 91.78 88.91 90.89 91.46 91.46 91.93 91.98Table 15: Speaker identi ation a ura y using di erent ombination methods (3 speakers)\n358\nA New Te hnique for Combining Multiple Classifiers\nClassi ers WS Av Md Mx MV FI DS0 DS1 DS2 1, 2 87.45 87.53 87.53 87.29 86.98 86.55 87.69 87.73 87.41 1, 6 84.78 83.79 83.79 83.40 81.55 85.48 83.87 85.37 84.78 5, 6 74.00 74.47 74.47 72.03 71.28 69.83 71.99 73.29 73.13 1, 2, 3 89.18 89.18 88.24 88.16 87.92 88.32 89.10 89.38 88.87 1, 2, 6 87.96 88.08 87.53 86.35 87.33 87.25 87.65 88.08 87.41 1, 5, 6 85.60 85.41 83.32 83.52 83.12 84.66 84.62 86.15 85.13 4, 5, 6 80.84 80.68 79.58 77.18 78.76 80.29 78.80 81.51 80.68 1, 2, 3, 4 89.77 89.85 89.65 88.04 88.59 89.65 89.61 90.17 89.61 1, 2, 3, 6 89.26 89.22 88.71 87.29 88.20 88.75 88.91 89.50 88.71 1, 2, 5, 6 87.84 87.33 86.90 86.35 87.10 87.37 87.65 88.32 87.69 1, 4, 5, 6 87.06 86.74 86.23 83.99 85.44 86.82 86.66 88.16 87.14 3, 4, 5, 6 84.74 84.38 84.30 80.72 83.01 84.07 83.83 85.68 84.66 1, 2, 3, 4, 5 89.61 89.61 89.02 88.00 88.47 89.18 89.61 90.01 89.73 1, 2, 3, 4, 6 89.77 89.73 88.83 87.45 88.87 89.06 89.61 89.93 89.50 1, 2, 3, 5, 6 89.26 89.14 88.36 87.29 88.04 88.55 89.18 89.73 88.59 1, 2, 4, 5, 6 89.10 89.06 88.20 86.74 88.36 88.36 88.95 89.65 88.95 1, 3, 4, 5, 6 88.00 87.88 86.98 85.21 87.45 87.92 87.88 89.38 89.02 2, 3, 4, 5, 6 88.99 88.87 87.21 85.80 87.06 88.00 87.92 89.26 88.75 1, 2, 3, 4, 5, 6 89.42 89.26 88.91 87.37 88.24 89.22 89.54 89.89 89.73Table 16: Speaker identi ation a ura y using di erent ombination methods (4 speakers)\n359\nAl-Ani & Deri heReferen esAha, D. (1995). Ma hine learning. Tutorial presented at The 1995 Arti ial Intelligen eand Statisti s Workshop.Alkoot, F., & Kittler, J. (1999). Experimental evaluation of expert fusion strategies. PatternRe ognition Letters, 20, 1361{1369.Chen, K., & Chi, H. (1998). A method of ombining multiple lassi ers through soft om-petition on di erent feature sets. Neuro omputing, 20, 227{252.Cho, S., & Kim, J. (1995a). Combining multiple neural networks by fuzzy integral for robust lassi ation. IEEE Transa tions on Systems, Man and Cyberneti s, 25, 380{384.Cho, S., & Kim, J. (1995b). Multiple networks fusion using fuzzy logi . IEEE Transa tionson Neural Networks, 6, 497{501.Denoeux, T. (2000). A neural network lassi er based on Dempster{Shafer theory. IEEETransa tions on Systems, Man and Cyberneti s, 30, 131{150.Dietteri h, T. (1999). An experimental omparison of three methods for onstru ting en-sembles of de ision trees: Bagging boosting and randomization. Ma hine Learning,40, 139{158.Hansen, L., & Salamon, P. (1990). Neural network ensembles. IEEE Transa tions onPattern Analysis and Ma hine Intelligen e, 12, 993{1001.Hashem, S., & S hmeiser, B. (1995). Improving model a ura y using optimal linear om-binations of trained neural networks. IEEE Transa tions on Neural Networks, 6,792{794.Ho, T., Hull, J., & Srihari, S. (1994). De ision ombination in multiple lassi er system.IEEE Transa tions on Pattern Analysis and Ma hine Intelligen e, 16, 66{75.Kittler, J., Hatef, M., Duin, R., & Matas, J. (1998). On ombining lassi ers. IEEETransa tions on Pattern Analysis and Ma hine Intelligen e, 20, 226{239.Kohlas, J., & Monney, P. (1995). A mathemati al theory of hints. An approa h to theDempster-Shafer theory of eviden e. Berlin: Springer{Verlag.Lam, L., & Suen, C. (1995). Optimal ombinations of pattern lassi ers. Pattern Re ognitionLetters, 16, 945{954.Liu, W., & Bundy, A. (1992). The ombination of di erent pie es of eviden e using in iden e al ulus. Te h. rep. RP 599, Dept. of Arti ial Intelligen e, Univ. of Edinburgh.Mandler, E., & S hurmann, J. (1988). Combining the lassi ation results of independent lassi ers based on the dempster{shafer theory of eviden e. In Gelsema, E., & Kanal,L. (Eds.), Pattern re ognition and arti ial intelligen e, pp. 381{393. North-Holland.MIT, SRI, & TI (1990). DARPA TIMIT a ousti -phoneti ontinuous spee h orpus..http://www.ld .upenn.edu/do /TIMIT.html.Rogova, G. (1994). Combining the results of several neural network lassi ers. NeuralNetworks, 7, 777{781.Shafer, G. (1976). A mathemati al theory of eviden e. Prin eton University Press.360\nA New Te hnique for Combining Multiple ClassifiersSmets, P. (1990). The ombination of eviden e in the transferable belief model. IEEETransa tions on Pattern Analysis and Ma hine Intelligen e, 12, 447{458.Smets, P. (1998). The transferable belief model for quanti ed belief representation. InGabbay, D., & Smets, P. (Eds.), Handbook of defeasible reasoning and un ertainty,pp. 267{301. Kluwer.Smets, P. (2000). Data fusion in transferable belief model. In 3rd Intl. Conf. InformationFusion, pp. 21{33.Sugeno, M. (1977). Fuzzy measures and fuzzy integrals: a survey. In Gupta, M., Saridis, G.,& Gaines, B. (Eds.), Fuzzy automata and de ision pro esses, pp. 89{102. Amsterdam:North-Holland.USC (1981). USC-SIPI image database.. http://sipi.us .edu/servi es/database/.Xu, L., Krzyzak, A., & Suen, C. (1992). Methods of ombining multiple lassi ers and theirappli ations to handwriting re ognition. IEEE Transa tions on Systems, Man andCyberneti s, 22, 418{435.\n361"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : null,
    "creator" : "dvips(k) 5.86 Copyright 1999 Radical Eye Software"
  }
}