{
  "name" : "1708.04469.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Comparison of Decoding Strategies for CTC Acoustic Models",
    "authors" : [ "Thomas Zenkel", "Ramon Sanabria", "Florian Metze", "Jan Niehues", "Matthias Sperber", "Sebastian Stüker", "Alex Waibel" ],
    "emails" : [ "sebastian.stueker}@kit.edu", "ahw}@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 8.\n04 46\n9v 1\n[ cs\n.C L\n] 1\n5 A\nug 2\n01 7\nlot of interest as it offers an elegant approach to building acoustic models (AMs) for speech recognition. The CTC loss function maps an input sequence of observable feature vectors to an output sequence of symbols. Output symbols are conditionally independent of each other under CTC loss, so a language model (LM) can be incorporated conveniently during decoding, retaining the traditional separation of acoustic and linguistic components in ASR.\nFor fixed vocabularies, Weighted Finite State Transducers provide a strong baseline for efficient integration of CTC AMs with n-gram LMs. Character-based neural LMs provide a straight forward solution for open vocabulary speech recognition and all-neural models, and can be decoded with beam search. Finally, sequence-to-sequence models can be used to translate a sequence of individual sounds into a word string.\nWe compare the performance of these three approaches, and analyze their error patterns, which provides insightful guidance for future research and development in this important area. Index Terms: automatic speech recognition, character based language models, decoding, neural networks"
    }, {
      "heading" : "1. Introduction",
      "text" : "Traditionally, Acoustic Models (AMs) of an Automatic Speech Recognition system followed a generative approach based on HMMs [1] where the emission probabilities of each state were modeled with a Gaussian Mixture Model. Since the AM works with phonemes as a target, during decoding the information of the AM had to be combined with a pronunciation lexicon, which maps sequences of phonemes to words, and a word based LM [2].\nMore recent work has been focused on solutions which come close to end-to-end systems. Connectionist Temporal Classification (CTC) acoustic models [3] can directly model the mapping between speech features and symbols without having to rely on an alignment between the audio sequence and the symbol sequence. However, the CTC objective function requires that its output symbols are conditional independent of each other. While this assumption is essential to learn a mapping between the speech features and the output sequence, it also entails to add linguistic information during decoding.\nOther end-to-end approaches that are inspired by recent developments in machine learning system such as [4] are [5, 6]. By attending to different frames for each output symbol attention based speech recognition systems are able to map speech features to an output sequence.\nThis approach has no need to assume conditional independence between its output, and therefore is theoretically able to\njointly learn acoustic and linguistic models implicitly.\nWhile “traditional” from a strictly end-to-end point of view, the separation of acoustic model and language model allows for domain independence and adaptation or re-use of speech recognition components. In this work we therefore investigate different decoding strategies for CTC acoustic models, and provide a comparison of their individual characteristics using the same acoustic model. We provide a performance comparison, and analyze the differences in the output of the model.\nWe compare the following approaches:\n• Greedy Search without linguistic information\n• WFST search with word language model [7, 8]\n• Beam search using character RNN language model [9,\n10]\n• Sequence to Sequence approach using neural machine\ntranslation\nFor each of these categories we implement a search algorithm and evaluate it on the Switchboard Task, thereby providing a fair comparison of the most promising decoding approaches."
    }, {
      "heading" : "2. Related work",
      "text" : "The simplest decoding algorithm is to pick the most likely character at each frame. This is commonly used to provide Character Error Rates (CER) during training of the acoustic model and can also be used to calculate Word Error Rates (WER), given that the acoustic model has a notion of word boundaries. Word boundaries can be modeled with a space symbol or by capitalizing the first letter of each word [11]. While decoding CTC acoustic models without adding external linguistic information works well, a vast amount of training data should be used to get competitive results [12].\nA traditional approach to perform decoding over CTC is to add linguistic information on the word level. Early work did this with an ordinary beam search, that means by performing a breadth first search in the time dimension and keeping a fixed number of partial transcriptions at every time step. For including linguistic information when adding a new character to a transcription, word based LMs were preprocessed [13, 14].\nWeighted Finite State Transducers (WFST) present a generalized word based approach [7, 15]. WFST provide a convenient framework to combine a word based n-gram model and a lexicon which maps a sequence of symbols to a word into a single search graph. While this allows to process both character and phonemes as the output of the acoustic model, it can only generate sequences of words from a fixed vocabulary.\nDue to recent developments of character based LMs [16, 17], it is also a competitive option to directly add character level linguistic information during the beam search. Currently, one of the most promising approaches is to use a character based RNN and query it when a new character is added to the transcription [9, 10]. With its theoretically infinite context a character RNN can encourage the transcription to be linguistically correct while adding its information as soon as possible.\nThe last approach presented is to treat the decoding problem as a general sequence to sequence task. For each frame the acoustic model outputs a probability distribution over all labels. This information can be processed by another CTC model [11] or by an attention based system [4] to produce a more linguistically reasonable transcription. Recent approaches combine a CTC model with an attention based mechanism and are able to train this model jointly [18]."
    }, {
      "heading" : "3. Acoustic Model",
      "text" : "The AM of our system is composed by multiple RNN layers followed by a soft-max layer. RNN layers, which are composed by bidirectional LSTM units [19], provide the ability to learn complex, long term dependencies. A sequence of multiple speech features forms the input of our model. For each input the AM outputs a probability distribution over its target alphabet. The whole model is jointly trained under the CTC loss function [3].\nMore formally, let us define a sequence of n-dimensional acoustic features X = (x1, . . . ,xT ) of length T as the input of our model and L as the set of labels of our alphabet. These labels can be either characters or phonemes. We augment L with a special blank symbol ∅ and define L′ = L ∪ ∅.\nLet z = (z1, .., zU ) ∈ L U be an output sequence of length U ≤ T , which can be seen as the transcription of an input sequence. To define the CTC loss function we additionally need a many to one mapping B that maps a path p = (p1, . . . pT ) ∈ L′T of the CTC model to an output sequence z. This mapping is also referred as the squash function, as it removes all blank symbols of the path and squashes multiple repeated characters into a single one (e.g. B(AA∅AAABB) = AAB). Note that we do not squash characters that are separated by the blank symbol as this still allows us to create repeated characters in the transcription. Let us define the probability of a path as\nP (p|X) = T ∏\nt=1\nytk (1)\nwhere ytk is the probability of observing the label k at time t. To calculate the probability of an output sequence z we sum over all possible paths:\nP (z|X) = ∑\np∈B−1(z)\nP (p|X) (2)\nTo perform the sum over all path we will use a technique inspired by the traditional dynamic programming method used in HMMs, the forward-backward algorithm [1]. We additionally force the appearance of blank symbols in our paths by augmenting the sequence of output labels during training with a blank symbol between each of the labels of z as well as at the beginning and the end of the sequence.\nGiven a sequence of speech features X = (x1, . . . ,xT ), we can now calculate the probability distribution over the augmented label set L′ for each frame. In the remainder of the paper let P tAM (k|X) denote the probability to encounter label\nk ∈ L′ at time step t given the speech features X . The decoding strategies of the subsequent chapter will process this information in different ways to create a linguistically reasonable transcription."
    }, {
      "heading" : "4. Decoding Strategies",
      "text" : "In this section we describe different approaches on how to generate a transcription given the static sequence of probabilities generated by the acoustic model."
    }, {
      "heading" : "4.1. Greedy Search",
      "text" : "To create a transcription without adding any linguistic information we use the decoding procedure of [3] and greedily search the best path p ∈ L′T :\nargmax p\nT ∏\nt=1\nP tAM (pt|X) (3)\nThe mapping of the path to a transcription z is straight forward and works by applying the squash function: z = B(p). For character based CTC acoustic models this procedure can already provide useful transcriptions."
    }, {
      "heading" : "4.2. Weighted Finite State Transducer",
      "text" : "To improve over the simple greedy search, the Weighted Finite State Transducer (WFST) approach adds linguistic information at the word level. First of all we preprocess the probability sequence with the prior probability of each unit of the augmented label set L′. p(X |k) ∝ P (k|X)/P (k) (4)\nThis does not have a proper theoretical motivation since the result is only proportional to a probability distribution. However, by dividing through the prior probability units which are more likely to appear at a particular position than their average will get a high value.\nThe search graph of the WFST is composed of three indi-\nvidual components:\n• A token WFSTmaps a sequence of units inL′ to a single unit in L by applying the squash function B\n• A lexicon WFST maps sequences of units in L to words\n• A grammar WFST encodes the permissible word se-\nquences and can be created given a word based n-Gram language model\nThe search graph is used to find the most probable word sequence. Note that the lexicon of the WFST allows us to deal with character as well as phoneme based acoustic models."
    }, {
      "heading" : "4.3. Beam Search with Char RNN",
      "text" : "In contrast to the WFST based approach we can directly apply the probabilities at the character level with this procedure. For now assume that the alphabet of the character based LM is equal to L. We want to find a transcription which has a high probability based on the acoustic as well as the language model. Since we have to sum over all possible paths p for a transcription z and want to add the LM information as early as possible, our goal is to solve the following equation:\nargmax z\n∑\nB−1(z)=p\nT ∏\nt=1\nytpt · P ′ LM (pt|B(p1:t−1)) (5)\nNote that we cannot estimate a useful probability for the blank label ∅ with the language model, so we set P ′LM (∅|p) = 1∀p ∈ P(L′). To not favor a sequence of blank symbols, we apply an insertion bonus b ∈ R for every pt 6= ∅. This yields the following equation:\nP ′LM (k|p) =\n{\nPLM (k|p) · b, if k 6= ∅ 1, if k = ∅ (6)\nwhere PLM (k|p) is provided by the character LM. As it is infeasible to calculate an exact solution to equation 6, we apply a beam search similar to [10]1.\nFor AMs which do not use spaces nor have another notion of word boundaries, it is possible to add this information based only on the character LM. This can be achieved by adding a copy of each transcription appended by the space symbol at each time step. This works surprisingly well, since spaces at inappropriate position will get a low LM probability. To the best of our knowledge this is a novel approach and can easily be extended to a larger number of characters, for example to punctuation marks.\nWhile this approach is only able to deal with character based acoustic models, it can create arbitrary, open vocabulary transcriptions."
    }, {
      "heading" : "4.4. Attention Based Sequence to Sequence Model",
      "text" : "The attention based approach is an example for a sequence to sequence model. We apply greedy search to the information provided by the acoustic model and get a sequence of units z ∈ P(L). This sequence provides the input to the attention based system. As in common neural machine translation models the input gets transformed into a sequence of words.\nTherefore the system first encodes the character sequence using a RNN-based encoder, creating a sequence of hidden representations h = (h1, . . . , hT ) of length T. During decoding we calculate an attention vector a = (a1, . . . , aT ) with ∑T\nt=1 at = 1 for each output word based on the current hidden state of the decoder. With the hidden representation and the attention vector we can now calculate the context vector c:\nc =\nT ∑\nt=1\nat · ht (7)\nThe decoder uses the context vector to create a probability distribution over the vocabulary of output words. During decoding beam search is used to find the most probable word sequence given the input sequence of characters. By transforming the input to a word sequence the attention model is able to add linguistic information and create an improved transcription."
    }, {
      "heading" : "5. Training",
      "text" : "This section describes the training process of the acoustic model and the linguistically motivated models used in the different decoding approaches."
    }, {
      "heading" : "5.1. Acoustic Model",
      "text" : "We use the Switchboard data set (LDC97S62) to train the AM. This data set consists of 2,400 two-sided telephone conversations with a duration of about 300 hours. It is composed of over 500 speakers with different US accents talking about 50\n1Code is included within EESEN: https://github.com/srvk/eesen\nrandomly picked topics. We pick 4000 utterances as our validation set for hyper parameter tunning. Our target labels are either phonemes or characters.\nWe also augment the training set to get a more generalized model using two techniques. First, by reducing the frame rate, applying a sub sampling and finally adding an offset we augment the number of training samples. Second, we augment our training set by a factor of 10 applying slight changes to the speed, pitch and tempo of the audio files. The model consist of five bidirectional LSTM layers with 320 units in each direction. It is trained using EESEN [20]."
    }, {
      "heading" : "5.2. Weighted Finite State Transducer",
      "text" : "As stated in section 4.2 our WFST implementation is composed by three individual components. These components are implemented using Kaldi’s [21] FST tools. We determine the weights of the lexicon WFST by using a lexicon that maps each word to a sequence of CTC labels. The grammar WFST is modeled by using the probabilities of a trigram and 4-gram language model smoothed with Kneser-Ney [22] discounting. We create the language model based on Fisher transcripts and the transcripts of the acoustic training data using SRILM [23]."
    }, {
      "heading" : "5.3. Character Language Model",
      "text" : "We train the Character LM with Fisher transcripts (LDC2004T19, LDC2005T19) and the transcripts of the acoustic training data (LDC97S62). Validation is done on the transcription of the acoustic validation data. These transcriptions are cleaned by removing punctuation marks and duplicate utterances. This results in a training text of about 23 million words and 112 million characters. The alphabet of the character LM consists of 28 characters, a start and end of sentence symbol, a space symbol and a symbol representing unknown characters. We cut all sentences to a maximum length of 128 characters. We use a embedding size of 64 for the characters, a single layer LSTM with 2048 Units and a softmax layer implemented with DyNet [24] as our neural model. Training is performed with the whole data using Adam [25] by randomly picking a batch until convergence on the validation data. We retrain the resulting model on the Switchboard training data using Stochastic Gradient Descent with a low learning rate of 0.01, which is inspired by [26]. This procedure results in an average entropy of 1.34 bits per character (BPC) on the train set, 1.37 BPC on the validation set and 1.46 BPC on the evaluation set (LDC2002S09)."
    }, {
      "heading" : "5.4. Attention Based Sequence to Sequence Model",
      "text" : "The attention based model was trained on the Switchboard training data. We decode the acoustic model without any linguistic information by applying the greedy method of Section 4.1. The sequence of generated characters is used as the input to our model and we use the sequence of words in the reference transcription as our desired output during training.\nFor implementing the attention based encoder decoder, we use the Nematus toolkit [27]. In our experiments we use GRU units in the encoder and the target sequence is generated using conditional GRU units [27].\nWe use the default network architecture, with an embedding size of 500 and a hidden layer size of 1024 and our output vocabulary consists of almost 30,000 words. For regularization, we use dropout [28]. Due to time constraints, we only use segments with a maximum length of 100 tokens. The system was\ntrained using Adadelta [29] and a mini-batch size of 80. We performed early stopping on the validation data."
    }, {
      "heading" : "6. Experiments",
      "text" : "We use the 2000 HUB5 “Eval2000” (LDC2002S09) dataset for evaluation. It consist of a “Switchboard” subset, which is similar to the training data, and the “Callhome” subset. These subsets allow to analyze the robustness of the individual approaches to some extend.\nFor the Greedy Search, we use an alphabet consisting of upper and lowercase characters. As in [11], an upper case character denotes the start of a new word. For all other character based AMs we only use lowercase characters without a space unit. Table 1 shows the results, and compares our findings (top part) against related results from the literature (bottom part).\nWhile Open Vocabulary approaches such as the character RNN approach are still slightly inferior to word-based approaches, adding linguistic information at the character level yields competitive results compared to a tuned word based WFST system. Using the character LM during the Beam Search significantly reduces incorrectly recognized words, which did not appear in the training text (199), by a factor of 30 compared to a simple beam search (6274). This amounts to a rate of 0.5%, which compares favorably to the out of vocabulary rate of the WFST based approach 0.9%. These remaining errors are for the most part very similar to valid English words, and could be considered spelling mistakes (“penately”) or newly created words (“discussly”). Only on rare occasion does the Character LM not add a space between words (“andboxes”). Most notably, the Open Vocabulary approach was able to generate correct words, which did not appear in the training corpora, including “boger”, “disproven”, “ducting”, “fringing”, “spick” and “peppier”.\nTable 2 shows the insertion, deletion, and substitution rates. We consistently used an insertion bonus of 2.5 in our experiments with the beam search. The application of an insertion\nbonus every time when reducing the probability by the character based LM yields balanced insertion and deletion errors. Additionally the logarithm of the insertion bonus corresponds well with the entropy of the character language model on the validation set (log2(2.5) = 1.3, validation entropy: 1.37 BPC). Overall, the error patterns of all three systems seem remarkably similar, even though the WFST system has been tuned more aggressively than the other two systems, and thus exhibits unbalanced insertions and deletions.\nWhile the attention based system was able to improve over the greedy search results, it did not achieve large gains on the character based AM. We speculate that it might not be the best option to use the transcription of the greedy decoding as the input to the attention based system. We argue that by providing the complete probability distribution over each character the WFST is able to use more information and can outperform the attention based system. Table 3 shows an example utterance to visualize the characteristics of the different systems.\nOur character based system is competitive to the recently published results in [11], which represent state of the art results. We are within 2% WER of the reported number for word based WFST. For open vocabulary, character based speech recognition we report an improvement of over 1% WER compared to previous results [11, 9]."
    }, {
      "heading" : "7. Conclusions",
      "text" : "In this paper, we compare different decoding approaches for CTC acoustic models, which are trained on the same, open source platform. A “traditional” context independent WFST approach performs best, but the open vocabulary character RNN approach performs only about 10% relative worse, and produces a surprisingly small number of “OOV” errors. The Seq2Seq approach produces reasonable performance as well, and is very easy to train, given that the CTCmodel already produces a symbolic tokenization of the input. This trick allows us to outperform true end-to-end approaches such as [30].\nWe believe that these results show that there is currently a multitude of different algorithms that can be used to perform speech recognition in a neural setting, and there may not be a “one size fits all” approach for the foreseeable future. While WFST is well understood and fast to execute, the Seq2Seq approach might integrate well with machine translation, while the character RNN approach might perform well for morphologically complex languages.\nWe are continuing to further develop these approaches, in order to better understand their characteristics also on nonEnglish data, with different vocabulary growth, or under variable acoustic conditions."
    }, {
      "heading" : "8. Acknowledgments",
      "text" : "This work was supported by the Carl-Zeiss-Stiftung and by the CLICS exchange program. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant ACI-1548562."
    }, {
      "heading" : "9. References",
      "text" : "[1] L. Rabiner and B. Juang, “An introduction to hidden markov models,” ieee assp magazine, vol. 3, no. 1, pp. 4–16, 1986.\n[2] H. Soltau, F. Metze, C. Fugen, and A. Waibel, “A one-pass decoder based on polymorphic linguistic context assignment,” in Automatic Speech Recognition and Understanding, 2001. ASRU’01.\nIEEE Workshop on. IEEE, 2001, pp. 214–217.\n[3] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,” in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369–376.\n[4] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” arXiv preprint arXiv:1409.0473, 2014.\n[5] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio, “End-to-end attention-based large vocabulary speech recognition,” in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 4945– 4949.\n[6] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, “Listen, attend and spell,” arXiv preprint arXiv:1508.01211, 2015.\n[7] Y. Miao, M. Gowayyed, and F. Metze, “Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding,” in Automatic Speech Recognition and Understanding (ASRU), 2015\nIEEE Workshop on. IEEE, 2015, pp. 167–174.\n[8] H. Sak, A. Senior, K. Rao, O. Irsoy, A. Graves, F. Beaufays, and J. Schalkwyk, “Learning acoustic frame labeling for speech recognition with recurrent neural networks,” in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Con-\nference on. IEEE, 2015, pp. 4280–4284.\n[9] A. L. Maas, Z. Xie, D. Jurafsky, and A. Y. Ng, “Lexicon-free conversational speech recognition with networks.” in HLT-NAACL, 2015, pp. 345–354.\n[10] K. Hwang and W. Sung, “Character-level incremental speech recognition with recurrent neural networks,” in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Con-\nference on. IEEE, 2016, pp. 5335–5339.\n[11] G. Zweig, C. Yu, J. Droppo, and A. Stolcke, “Advances in all-neural speech recognition,” arXiv preprint arXiv:1609.05935, 2016.\n[12] H. Soltau, H. Liao, and H. Sak, “Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition,” arXiv preprint arXiv:1610.09975, 2016.\n[13] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with recurrent neural networks.” in ICML, vol. 14, 2014, pp. 1764–1772.\n[14] A. Y. Hannun, A. L. Maas, D. Jurafsky, and A. Y. Ng, “Firstpass large vocabulary continuous speech recognition using bidirectional recurrent dnns,” arXiv preprint arXiv:1408.2873, 2014.\n[15] C. Mendis, J. Droppo, S. Maleki, M. Musuvathi, T. Mytkowicz, and G. Zweig, “Parallelizing wfst speech decoders,” in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Inter-\nnational Conference on. IEEE, 2016, pp. 5325–5329.\n[16] J. N. Foerster, J. Gilmer, J. Chorowski, J. Sohl-Dickstein, and D. Sussillo, “Intelligible language modeling with input switched affine networks,” arXiv preprint arXiv:1611.09434, 2016.\n[17] B. Krause, L. Lu, I. Murray, and S. Renals, “Multiplicative lstm for sequence modelling,” arXiv preprint arXiv:1609.07959, 2016.\n[18] S. Kim, T. Hori, and S. Watanabe, “Joint ctc-attention based end-to-end speech recognition using multi-task learning,” arXiv preprint arXiv:1609.06773, 2016.\n[19] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[20] Y. Miao, M. Gowayyed, X. Na, T. Ko, F. Metze, and A. Waibel, “An empirical exploration of ctc acoustic models,” in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-\ntional Conference on. IEEE, 2016, pp. 2623–2627.\n[21] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., “The kaldi speech recognition toolkit,” in IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFLCONF-192584. IEEE Signal Processing Society, 2011.\n[22] S. F. Chen and J. Goodman, “An empirical study of smoothing techniques for language modeling,” in Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996, pp. 310–318.\n[23] A. Stolcke et al., “Srilm-an extensible language modeling toolkit.” in Interspeech, vol. 2002, 2002, p. 2002.\n[24] G. Neubig, C. Dyer, Y. Goldberg, A. Matthews, W. Ammar, A. Anastasopoulos, M. Ballesteros, D. Chiang, D. Clothiaux, T. Cohn, K. Duh, M. Faruqui, C. Gan, D. Garrette, Y. Ji, L. Kong, A. Kuncoro, G. Kumar, C. Malaviya, P. Michel, Y. Oda, M. Richardson, N. Saphra, S. Swayamdipta, and P. Yin, “Dynet: The dynamic neural network toolkit,” arXiv preprint arXiv:1701.03980, 2017.\n[25] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014.\n[26] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey et al., “Google’s neural machine translation system: Bridging the gap between human and machine translation,” arXiv preprint arXiv:1609.08144, 2016.\n[27] R. Sennrich, O. Firat, K. Cho, A. Birch, B. Haddow, J. Hitschler, M. Junczys-Dowmunt, S. L”aubli, A. V. Miceli Barone, J. Mokry, and M. Nadejde, “Nematus: a Toolkit for Neural Machine Translation,” in Proceedings of the Demonstrations at the 15th Conference of the European Chapter of the Association for Computa-\ntional Linguistics, Valencia, Spain, 2017.\n[28] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, “Improving neural networks by preventing coadaptation of feature detectors,” arXiv preprint arXiv:1207.0580, 2012.\n[29] M. D. Zeiler, “Adadelta: an adaptive learning rate method,” arXiv preprint arXiv:1212.5701, 2012.\n[30] L. Lu, X. Zhang, and S. Renais, “On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition,” in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), March 2016, pp. 5060–5064."
    } ],
    "references" : [ {
      "title" : "An introduction to hidden markov models",
      "author" : [ "L. Rabiner", "B. Juang" ],
      "venue" : "ieee assp magazine, vol. 3, no. 1, pp. 4–16, 1986.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "A one-pass decoder based on polymorphic linguistic context assignment",
      "author" : [ "H. Soltau", "F. Metze", "C. Fugen", "A. Waibel" ],
      "venue" : "Automatic Speech Recognition and Understanding, 2001. ASRU’01. IEEE Workshop on. IEEE, 2001, pp. 214–217.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "A. Graves", "S. Fernández", "F. Gomez", "J. Schmidhuber" ],
      "venue" : "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369–376.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473, 2014.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "End-to-end attention-based large vocabulary speech recognition",
      "author" : [ "D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 4945– 4949.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Listen, attend and spell",
      "author" : [ "W. Chan", "N. Jaitly", "Q.V. Le", "O. Vinyals" ],
      "venue" : "arXiv preprint arXiv:1508.01211, 2015.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding",
      "author" : [ "Y. Miao", "M. Gowayyed", "F. Metze" ],
      "venue" : "Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 167–174.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning acoustic frame labeling for speech recognition with recurrent neural networks",
      "author" : [ "H. Sak", "A. Senior", "K. Rao", "O. Irsoy", "A. Graves", "F. Beaufays", "J. Schalkwyk" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4280–4284.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Lexicon-free conversational speech recognition with networks.",
      "author" : [ "A.L. Maas", "Z. Xie", "D. Jurafsky", "A.Y. Ng" ],
      "venue" : "in HLT-NAACL,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Character-level incremental speech recognition with recurrent neural networks",
      "author" : [ "K. Hwang", "W. Sung" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5335–5339.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Advances in all-neural speech recognition",
      "author" : [ "G. Zweig", "C. Yu", "J. Droppo", "A. Stolcke" ],
      "venue" : "arXiv preprint arXiv:1609.05935, 2016.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition",
      "author" : [ "H. Soltau", "H. Liao", "H. Sak" ],
      "venue" : "arXiv preprint arXiv:1610.09975, 2016.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Towards end-to-end speech recognition with recurrent neural networks.",
      "author" : [ "A. Graves", "N. Jaitly" ],
      "venue" : "in ICML, vol",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Firstpass large vocabulary continuous speech recognition using bidirectional recurrent dnns",
      "author" : [ "A.Y. Hannun", "A.L. Maas", "D. Jurafsky", "A.Y. Ng" ],
      "venue" : "arXiv preprint arXiv:1408.2873, 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Parallelizing wfst speech decoders",
      "author" : [ "C. Mendis", "J. Droppo", "S. Maleki", "M. Musuvathi", "T. Mytkowicz", "G. Zweig" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5325–5329.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Intelligible language modeling with input switched affine networks",
      "author" : [ "J.N. Foerster", "J. Gilmer", "J. Chorowski", "J. Sohl-Dickstein", "D. Sussillo" ],
      "venue" : "arXiv preprint arXiv:1611.09434, 2016.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Multiplicative lstm for sequence modelling",
      "author" : [ "B. Krause", "L. Lu", "I. Murray", "S. Renals" ],
      "venue" : "arXiv preprint arXiv:1609.07959, 2016.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Joint ctc-attention based end-to-end speech recognition using multi-task learning",
      "author" : [ "S. Kim", "T. Hori", "S. Watanabe" ],
      "venue" : "arXiv preprint arXiv:1609.06773, 2016.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "An empirical exploration of ctc acoustic models",
      "author" : [ "Y. Miao", "M. Gowayyed", "X. Na", "T. Ko", "F. Metze", "A. Waibel" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 2623–2627.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The kaldi speech recognition toolkit",
      "author" : [ "D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz" ],
      "venue" : "IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL- CONF-192584. IEEE Signal Processing Society, 2011.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An empirical study of smoothing techniques for language modeling",
      "author" : [ "S.F. Chen", "J. Goodman" ],
      "venue" : "Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996, pp. 310–318.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Srilm-an extensible language modeling toolkit.",
      "author" : [ "A. Stolcke" ],
      "venue" : "in Interspeech,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2002
    }, {
      "title" : "Dynet: The dynamic neural network toolkit",
      "author" : [ "G. Neubig", "C. Dyer", "Y. Goldberg", "A. Matthews", "W. Ammar", "A. Anastasopoulos", "M. Ballesteros", "D. Chiang", "D. Clothiaux", "T. Cohn", "K. Duh", "M. Faruqui", "C. Gan", "D. Garrette", "Y. Ji", "L. Kong", "A. Kuncoro", "G. Kumar", "C. Malaviya", "P. Michel", "Y. Oda", "M. Richardson", "N. Saphra", "S. Swayamdipta", "P. Yin" ],
      "venue" : "arXiv preprint arXiv:1701.03980, 2017.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980, 2014.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "author" : [ "Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi", "W. Macherey", "M. Krikun", "Y. Cao", "Q. Gao", "K. Macherey" ],
      "venue" : "arXiv preprint arXiv:1609.08144, 2016.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Nematus: a Toolkit for Neural Machine Translation",
      "author" : [ "R. Sennrich", "O. Firat", "K. Cho", "A. Birch", "B. Haddow", "J. Hitschler", "M. Junczys-Dowmunt", "S. L”aubli", "A.V. Miceli Barone", "J. Mokry", "M. Nadejde" ],
      "venue" : "Proceedings of the Demonstrations at the 15th Conference of the European Chapter of the Association for Computational Linguistics, Valencia, Spain, 2017.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Improving neural networks by preventing coadaptation of feature detectors",
      "author" : [ "G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580, 2012.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "M.D. Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701, 2012.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition",
      "author" : [ "L. Lu", "X. Zhang", "S. Renais" ],
      "venue" : "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), March 2016, pp. 5060–5064.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Traditionally, Acoustic Models (AMs) of an Automatic Speech Recognition system followed a generative approach based on HMMs [1] where the emission probabilities of each state were modeled with a Gaussian Mixture Model.",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "Since the AM works with phonemes as a target, during decoding the information of the AM had to be combined with a pronunciation lexicon, which maps sequences of phonemes to words, and a word based LM [2].",
      "startOffset" : 200,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : "Connectionist Temporal Classification (CTC) acoustic models [3] can directly model the mapping between speech features and symbols without having to rely on an alignment between the audio sequence and the symbol sequence.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "Other end-to-end approaches that are inspired by recent developments in machine learning system such as [4] are [5, 6].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "Other end-to-end approaches that are inspired by recent developments in machine learning system such as [4] are [5, 6].",
      "startOffset" : 112,
      "endOffset" : 118
    }, {
      "referenceID" : 5,
      "context" : "Other end-to-end approaches that are inspired by recent developments in machine learning system such as [4] are [5, 6].",
      "startOffset" : 112,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : "• WFST search with word language model [7, 8]",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "• WFST search with word language model [7, 8]",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : "• Beam search using character RNN language model [9, 10]",
      "startOffset" : 49,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "• Beam search using character RNN language model [9, 10]",
      "startOffset" : 49,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : "Word boundaries can be modeled with a space symbol or by capitalizing the first letter of each word [11].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 11,
      "context" : "While decoding CTC acoustic models without adding external linguistic information works well, a vast amount of training data should be used to get competitive results [12].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 12,
      "context" : "For including linguistic information when adding a new character to a transcription, word based LMs were preprocessed [13, 14].",
      "startOffset" : 118,
      "endOffset" : 126
    }, {
      "referenceID" : 13,
      "context" : "For including linguistic information when adding a new character to a transcription, word based LMs were preprocessed [13, 14].",
      "startOffset" : 118,
      "endOffset" : 126
    }, {
      "referenceID" : 6,
      "context" : "Weighted Finite State Transducers (WFST) present a generalized word based approach [7, 15].",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 14,
      "context" : "Weighted Finite State Transducers (WFST) present a generalized word based approach [7, 15].",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 15,
      "context" : "Due to recent developments of character based LMs [16, 17], it is also a competitive option to directly add character level linguistic information during the beam search.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : "Due to recent developments of character based LMs [16, 17], it is also a competitive option to directly add character level linguistic information during the beam search.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "Currently, one of the most promising approaches is to use a character based RNN and query it when a new character is added to the transcription [9, 10].",
      "startOffset" : 144,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "Currently, one of the most promising approaches is to use a character based RNN and query it when a new character is added to the transcription [9, 10].",
      "startOffset" : 144,
      "endOffset" : 151
    }, {
      "referenceID" : 10,
      "context" : "This information can be processed by another CTC model [11] or by an attention based system [4] to produce a more linguistically reasonable transcription.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "This information can be processed by another CTC model [11] or by an attention based system [4] to produce a more linguistically reasonable transcription.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "Recent approaches combine a CTC model with an attention based mechanism and are able to train this model jointly [18].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 18,
      "context" : "RNN layers, which are composed by bidirectional LSTM units [19], provide the ability to learn complex, long term dependencies.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "The whole model is jointly trained under the CTC loss function [3].",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "To perform the sum over all path we will use a technique inspired by the traditional dynamic programming method used in HMMs, the forward-backward algorithm [1].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 2,
      "context" : "To create a transcription without adding any linguistic information we use the decoding procedure of [3] and greedily search the best path p ∈ L :",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "As it is infeasible to calculate an exact solution to equation 6, we apply a beam search similar to [10].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : "It is trained using EESEN [20].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 20,
      "context" : "These components are implemented using Kaldi’s [21] FST tools.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 21,
      "context" : "The grammar WFST is modeled by using the probabilities of a trigram and 4-gram language model smoothed with Kneser-Ney [22] discounting.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 22,
      "context" : "We create the language model based on Fisher transcripts and the transcripts of the acoustic training data using SRILM [23].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 23,
      "context" : "We use a embedding size of 64 for the characters, a single layer LSTM with 2048 Units and a softmax layer implemented with DyNet [24] as our neural model.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 24,
      "context" : "Training is performed with the whole data using Adam [25] by randomly picking a batch until convergence on the validation data.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 25,
      "context" : "01, which is inspired by [26].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 26,
      "context" : "For implementing the attention based encoder decoder, we use the Nematus toolkit [27].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 26,
      "context" : "In our experiments we use GRU units in the encoder and the target sequence is generated using conditional GRU units [27].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 27,
      "context" : "For regularization, we use dropout [28].",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 28,
      "context" : "trained using Adadelta [29] and a mini-batch size of 80.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "Char Beam [9] Character 30.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 10,
      "context" : "4% Char Beam [11] Character 32.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 10,
      "context" : "8% WFST [11] Character 26.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 10,
      "context" : "1% Seq2Seq [11] Character 37.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 10,
      "context" : "As in [11], an upper case character denotes the start of a new word.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 10,
      "context" : "Our character based system is competitive to the recently published results in [11], which represent state of the art results.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "For open vocabulary, character based speech recognition we report an improvement of over 1% WER compared to previous results [11, 9].",
      "startOffset" : 125,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "For open vocabulary, character based speech recognition we report an improvement of over 1% WER compared to previous results [11, 9].",
      "startOffset" : 125,
      "endOffset" : 132
    }, {
      "referenceID" : 29,
      "context" : "This trick allows us to outperform true end-to-end approaches such as [30].",
      "startOffset" : 70,
      "endOffset" : 74
    } ],
    "year" : 2017,
    "abstractText" : "Connectionist Temporal Classification has recently attracted a lot of interest as it offers an elegant approach to building acoustic models (AMs) for speech recognition. The CTC loss function maps an input sequence of observable feature vectors to an output sequence of symbols. Output symbols are conditionally independent of each other under CTC loss, so a language model (LM) can be incorporated conveniently during decoding, retaining the traditional separation of acoustic and linguistic components in ASR. For fixed vocabularies, Weighted Finite State Transducers provide a strong baseline for efficient integration of CTC AMs with n-gram LMs. Character-based neural LMs provide a straight forward solution for open vocabulary speech recognition and all-neural models, and can be decoded with beam search. Finally, sequence-to-sequence models can be used to translate a sequence of individual sounds into a word string. We compare the performance of these three approaches, and analyze their error patterns, which provides insightful guidance for future research and development in this important area.",
    "creator" : "LaTeX with hyperref package"
  }
}