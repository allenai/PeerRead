{
  "name" : "1602.03963.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Detection of Cooperative Interactions in Logistic Regression Models",
    "authors" : [ "Easton Li" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 2.\n03 96\n3v 2\n[ cs\n.A I]\n2 8\nD ec\n2 01\n6 1\nWhen viewed as a feature selection problem, a simple quantity called influence is proposed as a measure of the marginal effects of the interaction terms on the outcome. For the case when the underlying interaction graph is known to be acyclic, it is shown that a simple algorithm that is based on a maximum-weight spanning tree with respect to the plug-in estimates of the influences not only has strong theoretical performance guarantees, but can also outperform generic feature selection algorithms for recovering the interaction graph from i.i.d. samples of the covariates and the outcome. Our results can also be extended to the model that includes both individual effects and pairwise interactions via the help of an auxiliary covariate.\nI. INTRODUCTION\nConsider a regression problem with d independent covariates x1, . . . , xd and a binary outcome variable y. The covariates are assumed to be uniformly distributed over {+1,−1}, and the conditional probabilities of the outcome given the covariates are assumed to be logistic:\nPr (y = +1|xV = xV ) = σ ( ∑\n{i,j}∈E\nβ{i,j}xixj\n)\n, (1)\nPr (y = −1|xV = xV ) = σ ( − ∑\n{i,j}∈E\nβ{i,j}xixj\n)\n(2)\nfor some real constants β{i,j}’s, where V := {1, . . . , d}, E := {{i, j} : i, j ∈ V, i 6= j}, xV := (xi : i ∈ V ), and σ(x) := ex/(1 + ex) is the sigmoid function. It is straightforward to verify that σ(x) + σ(−x) = 1 for any x ∈ R, so we have Pr (y = +1|xV = xV )+Pr (y = −1|xV = xV ) = 1 for any xV ∈ {+1,−1}d.\nFor any two distinct i, j ∈ V , we say that the covariates xi and xj interact if and only if β{i,j} 6= 0. Let G = (V, I) be a simple graph with the vertex set V and edge set\nThe work of E. L. Xu, X. Qian, T. Liu, and S. Cui was supported in part by grant NSFC-61328102/61629101, by DoD with grant HDTRA1-13-1-0029, and by NSF with grants CNS-1265227, ECCS-1305979, CNS-1343155, CCF1447235, ECCS-1508051, AST-1547436, and IOS-1547557.\nE. L. Xu, X. Qian, and T. Liu are with the Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX 77843, USA (e-mails: xulimc@gmail.com, xqian@tamu.edu, tieliu@tamu.edu), and S. Cui is with the Department of Electrical and Computer Engineering, University of California, Davis, CA 95616, USA (e-mail: sgcui@ucdavis.edu).\nI := {{i, j} ∈ E : β{i,j} 6= 0}. Then G captures all pairwise interactions between the covariates in determining the odds of the outcome of interest. Our goal is to recover the graph G from independently identically distributed (i.i.d.) samples of (xV , y).\nOur main motivation for considering the above pairwise interaction problem is from computational biology, where each covariate xi represents the expression of a biomarker (a gene or an environmental factor), and the variable y represents the phenotypic outcome with respect to a specific phenotype. In computational biology, many complex diseases, such as cancer and diabetes, are conjectured to have complicated underlying disease mechanisms [1]–[7]. Multiple candidate risk factors, either genetic or environmental, and their interactions are known to play critical roles in triggering and determining the development of a large family of diseases [1]–[7]. Identifying interactive effects among profiled variables not only helps with more accurate identification of critical risk factors for outcome prediction, but also helps reveal functional interactions and understand aberrant system changes that are specifically related to the outcome for effective systems intervention. Our model, of course, is a simplified one from real-world situations, and is studied here since it captures some essential features of the problem (as we shall see shortly) while being relatively simple.\nNote that if we let\nz{i,j} := xixj , ∀{i, j} ∈ E (3)\nand consider z{i,j}’s (instead of xi’s) as the covariates, then the problem of recovering the graph G can be viewed as a feature selection problem in statistics and machine learning. In [8], a basic approach for feature selection is to first use Shannon’s mutual information [9] to measure the marginal effects of the covariates on the outcome, and then select the features based on the ranking of the mutual information. More advanced approaches such as the immensely popular mRMR method [10] make incremental selections while taking into account both the relevance to the outcome and the redundancy among the selected features. However, even though Shannon’s mutual information provides a compact and model-free measure of correlation between the covariates and the outcome, which is well accepted in the statistics and computational biology communities, it is a complex function of the underlying joint distribution and hence difficult to analyze and estimate from limited data samples. As a result, when applied to specific regression models, the performance of the generic feature selection algorithms is usually difficult to characterize.\n2 Motivated by the recent progress on learning Ising models over arbitrary graphs [11], in this paper we propose a quantity called influence as a measure of the marginal effects of z{i,j}’s on the outcome y. Compared with Shannon’s mutual information, influence is a simple function of the low-order joint probabilities between xi’s and the outcome y, and hence is much easier to analyze and estimate. When the underlying graph G is known to be acyclic, we show that, a simple algorithm that is based on a maximum-weight spanning tree with respect to the “plug-in” estimate of the influences and followed by simple thresholding operations, not only has strong theoretical performance guarantees, but can also outperform the generic feature selection algorithms for recovering G from i.i.d. samples of (xV , y).\nThe rest of the paper is organized as follows. In Section II, we show that any acyclic G can be identified from the influences of z{i,j}’s on the outcome y. Building on the results from Section II, in Section III we show that any acyclic G can be recovered with probability at least 1 − ǫ from n i.i.d. samples of (xV , y), where n = Θ ( d log(d2/ǫ) )\n. In Section IV, we extend our results of the above sections to the model involving both individual effects and cooperative interactions. In Section V, we use computer simulations to demonstrate that the proposed algorithm can outperform the generic feature selection algorithms. Finally, in Section VI we conclude the paper with some remarks.\nNotation. Random variables are written in serif font, and sets are written in capital letters.\nII. IDENTIFICATION OF COOPERATIVE INTERACTIONS FROM LOW-ORDER JOINT PROBABILITIES\nOur main result in this section is to show that any acyclic G can be identified from the low-order joint probabilities (p(xi, xj , y) : {i, j} ∈ E). Towards this goal, let w be a weight assignment over E given by:\nw{i,j} : = ∣ ∣Pr ( y = +1|xi = +1, xj = +1 ) − Pr (\ny = −1|xi = +1, xj = +1 )∣ ∣ (4)\n= ∣ ∣2Pr ( y = +1|xi = +1, xj = +1 ) − 1 ∣ ∣ (5) = ∣ ∣8Pr (\nxi = +1, xj = +1, y = +1 ) − 1 ∣ ∣ (6)\nfor any {i, j} ∈ E. Here, (5) follows from the fact that\nPr ( y = +1|xi = +1, xj = +1 ) +\nPr ( y = −1|xi = +1, xj = +1 ) = 1\nand (6) is due to the fact that Pr(xi = +1, xj = +1) = 1/4. The following proposition helps to clarify the meaning of the weight assignment w as defined in (4). Proposition 1 (Influence): Assume that G = (V, I) is acyclic. We have\nw{i,j} = ∣ ∣Pr ( y = +1|xi = +1, xj = +1 ) − Pr (\ny = +1|xi = +1, xj = −1 )∣ ∣ (7)\nfor any {i, j} ∈ E. Proof: See Section A-A.\nBy (7), w{i,j} indicates whether the product z{i,j} = xixj has any influence on the event y = +1 and hence can be\na useful indication on whether {i, j} ∈ I . This intuition is partially justified by the following proposition.\nProposition 2 (Direct influence): Assume that G = (V, I) is acyclic. We have w{i,j} > 0 for any {i, j} ∈ I .\nProof: See Section A-B. We say that the product z{i,j} = xixj has a direct influence on the outcome y if {i, j} ∈ I . The above proposition guarantees that direct influences are strictly positive when G is acyclic. The following proposition provides a partial converse to Proposition 2.\nProposition 3 (Zero influence): Assume that G = (V, I) is acyclic. Then for any two distinct i, j ∈ V , we have w{i,j} = 0 if i and j are disconnected in G, or the unique path between i and j in G has an even length.\nProof: See Section A-C. Theorem 1 (Union of stars): Assume that each connected component of G = (V, I) is a star. Then for any two distinct i, j ∈ V , we have {i, j} ∈ I if and only if w{i,j} > 0.\nProof: This follows immediately from Propositions 2 and 3 and the fact that if each connected component of G is a star (which implies that G is acyclic), then any two distinct i, j ∈ V such that {i, j} 6∈ I must be either disconnected (if they belong to two different connected components) or connected by a unique path of length two (if they belong to the same connected component) in G.\nThe following example, however, shows that the converse of Proposition 2 is not true in general. Consider d = 4, I = {{1, 2}, {2, 3}, {3, 4}}, and β{1,2} = β{2,3} = β{3,4} = 1. Note that the graph G = (V, I) here is acyclic and the unique path between 1 and 4 is of length three. It is straightforward to calculate that\nw{1,4} = (e− 1)3 2(e3 + 1) > 0,\neven though {1, 4} 6∈ I . For any {i, j} ∈ E \\ I , we say that the product z{i,j} = xixj has an indirect influence on the outcome y if w{i,j} > 0. Due to the possible existence of indirect influences, unlike the unions of stars, a general acyclic G cannot be recovered via edge-byedge identifications.\nThe following proposition, however, shows that indirect influences are locally dominated by direct influences.\nProposition 4 (Indirect influence): Assume that G = (V, I) is acyclic, and let {\n{i1, i2}, {i2, i3}, . . . , {im, im+1} }\nbe a path of length m ≥ 2 in G. Then, we have w{i1,im+1} < w{is,is+1} for any s ∈ {1, . . . ,m}.\nProof: Note that when m is even, by Propositions 2 and 3 we have w{i1,im+1} = 0 < w{is,is+1} for any s ∈ {1, . . . ,m}. Therefore, we only need to consider the cases where m is odd, for which the proof can be found in Appendix A-D.\nLet D := {{i, j} ∈ E : i and j are disconnected in G}. A weight assignment u over E is said to have strict separation between I and D if there exists a real constant η ≥ 0 such that u{i,j} > η ≥ u{k,l} for any {i, j} ∈ I and {k, l} ∈ D. The consequence of strict separation and local dominance is summarized in the following proposition.\nProposition 5: Assume that G = (V, I) is acyclic, let u be a weight assignment over E satisfying: 1) (strict separa-\n3 tion) there exists a real constant η ≥ 0 such that u{i,j} > η ≥ u{k,l} for any {i, j} ∈ I and {k, l} ∈ D; and 2) (local dominance) u{i1,im+1} < u{is,is+1} for any path {\n{i1, i2}, {i2, i3}, . . . , {im, im+1} } of length m ≥ 2 in G and any s ∈ {1, . . . ,m}. Then for any maximum-weight spanning tree G′ = (V, T ) with respect to the weight assignment u, we have I = T ∩W where W := {{i, j} ∈ E : u{i,j} > η}.\nProof: See Section A-E. The following theorem is the main result of this section. Theorem 2 (Acyclic graphs): Assume that G = (V, I) is acyclic, and let G′ = (V, T ) be a maximum-weight spanning tree with respect to the weight assignment w as defined in (4). Then, for any two distinct i, j ∈ V we have {i, j} ∈ I if and only if {i, j} ∈ T and wi,j > 0.\nProof: Note that by Propositions 2 and 3, we have w{i,j} > 0 = w{k,l} for any {i, j} ∈ I and {k, l} ∈ D. By Proposition 4, we have w{i1,im+1} < w{is,is+1} for any path {\n{i1, i2}, {i2, i3}, . . . , {im, im+1} } of length m ≥ 2 in G and any s ∈ {1, . . . ,m}. The theorem thus follows directly from Proposition 5 with u = w and η = 0."
    }, {
      "heading" : "III. DETECTION OF COOPERATIVE INTERACTIONS FROM FINITE DATA SAMPLES",
      "text" : "Let (xV [t], y[t]), t = 1, . . . , n be n i.i.d. samples of (xV , y). To recover the graph G, we shall assign to each {i, j} ∈ E a weight that is based on the empirical joint probability:\nŵ{i,j} :=\n∣ ∣ ∣ ∣ ∣ 8 n n ∑\nt=1\n1((xi[t], xj [t], y[t]) = (+1,+1,+1))− 1 ∣ ∣ ∣ ∣\n∣\n, (8)\nwhere 1(·) is the indicator function. By (6), for any {i, j} ∈ E we have ŵ{i,j} converging to w{i,j} in probability in the limit as n → ∞. The following simple proposition, which follows directly from the well-known Hoeffding’s inequality [12], provides a bound on the rate at which the weight assignment ŵ converges uniformly to w.\nProposition 6: For any {i, j} ∈ E and η > 0,\nPr ( ∣ ∣\n∣ ŵ{i,j} − w{i,j}\n∣ ∣ ∣ ≥ η ) ≤ 2e−nη 2/32. (9)\nProof: Note that ∣ ∣ ∣ ŵ{i,j} − w{i,j} ∣ ∣ ∣ ≤ 8 ∣ ∣ ∣\n∣\nPr ( xi = +1, xj = +1, y = +1 )\n− 1 n\nn ∑\nt=1\n1((xi[t], xj [t], y[t]) = (+1,+1,+1))\n∣ ∣ ∣ ∣ .\nWe then finish the proof by applying the Hoeffding’s inequality [12].\nThe following propositions, which are generalizations of Propositions 2 and 4 respectively, play a key role in adapting the results of Theorem 2 from the weight assignment w to ŵ.\nProposition 7: Assume that G = (V, I) is acyclic and for any {i, j} ∈ I we have |β{i,j}| ∈ [λ, µ] for some µ ≥ λ > 0. Let\nγ :=\n√\n2\nπd [σ(λ+ 3µ) − σ(−λ+ 3µ)] > 0. (10)\nWe have w{i,j} ≥ γ for any {i, j} ∈ I . Proof: See Section A-B.\nProposition 8: Assume that G = (V, I) is acyclic and for any {i, j} ∈ I we have |β{i,j}| ∈ [λ, µ] for some µ ≥ λ > 0. We have w{i1,im+1} ≤ w{is,is+1} − γ for any path {\n{i1, i2}, {i2, i3}, . . . , {im, im+1} } of length m ≥ 2 in G and any s ∈ {1, . . . , m}, where γ is defined as in (10).\nProof: See Section A-D. Given Propositions 7 and 8, it is clear that if the estimation\nerror ∣ ∣\n∣ ŵ{i,j} − w{i,j}\n∣ ∣ ∣ is uniformly bounded by γ/2, an acyclic\nG can be recovered from {ŵ{i,j} : {i, j} ∈ E}, similar to that from {w{i,j} : {i, j} ∈ E}.\nTheorem 3: Assume that G = (V, I) is acyclic and for any {i, j} ∈ I we have |β{i,j}| ∈ [λ, µ] for some µ ≥ λ > 0. Let G′ = (V,T) be a maximum-weight spanning tree with respect to the weight assignment ŵ. If\n∣ ∣ ∣ ŵ{i,j} − w{i,j} ∣ ∣ ∣ < γ\n2 , ∀{i, j} ∈ E, (11)\nthen for any two distinct i, j ∈ V we have {i, j} ∈ I if and only if {i, j} ∈ T and ŵ{i,j} > γ/2.\nProof: By Proposition 7, we have w{i,j} ≥ γ > 0 = w{k,l} for any {i, j} ∈ I and {k, l} ∈ D. Under assumption (11), this implies that ŵ{i,j} > γ/2 > ŵ{k,l} for any {i, j} ∈ I and {k, l} ∈ D. By Proposition 8, we have w{i1,im+1} ≤ w{is,is+1} −γ for any path { {i1, i2}, {i2, i3}, . . . , {im, im+1} } of length m ≥ 2 in G and any s ∈ {1, . . . ,m}. Under assumption (11), this implies that ŵ{i1,im+1} < ŵ{is,is+1} for any path {\n{i1, i2}, {i2, i3}, . . . , {im, im+1} } of length m ≥ 2 in G and any s ∈ {1, . . . ,m}. The theorem thus follows directly from Proposition 5 with u = ŵ and η = γ/2.\nWe then establish the following algorithm to detect G based on Theorem 3.\nAlgorithm 1 Input: (xd[t], y[t]), t = 1, . . . , n and (µ, λ) such that µ ≥ λ > 0. Output: Ĝ = (V, Î). 1 For all 1 ≤ i < j ≤ d, compute ŵ{i,j} according to (8).\nCompute γ from (µ, λ) according to (10). 2 Find a maximum-weight spanning tree Ĝ′ = (V, T̂) over\nthe vertex set V with respect to the weight assignment (ŵ{i,j} : 1 ≤ i < j ≤ d).\n3 Return Ĝ = (V, Î) with Î = { {i, j} ∈ T̂ : ŵ{i,j} > γ/2 } .\nThe sample complexity of the algorithm is summarized in the following theorem.\nTheorem 4: Assume that G = (V, I) is acyclic and for any {i, j} ∈ I we have |βi,j | ∈ [λ, µ] for some µ ≥ λ > 0. Fix 0 < ǫ < 1 and let n be a positive integer such that\nn ≥ 128 γ2\nlog d2\nǫ =\n64πd\n[σ(λ+ 3µ) − σ(−λ+ 3µ)]2 log\nd2\nǫ . (12)\nThen with probability at least 1−ǫ, the algorithm can successfully detect the graph G from n i.i.d. samples of (xd, y).\nProof: By Proposition 6 and Theorem 3, we have\nPr ( Ĝ = G ) ≥ Pr ( ⋂\n{i,j}∈E\n{\n∣ ∣ŵi,j −wi,j ∣\n∣ < γ\n2\n}\n)\n4 ≥ 1− d(d− 1)e− nγ2\n128 ≥ 1− d2 · ǫ d2 = 1− ǫ.\nThis completes the proof of the theorem."
    }, {
      "heading" : "IV. EXTENSION TO MODELS WITH BOTH INDIVIDUAL EFFECTS AND COOPERATIVE INTERACTIONS",
      "text" : "In this section, we extend the results of Sections II and III to the models that include both individual effects and cooperative interactions. More specifically, we shall assume that the conditional probability of the outcome y given the covariates xV are given by:\nPr (y = +1|xV = xV ) = σ ( ∑\ni∈V\nβixi + ∑\n{i,j}∈E\nβ{i,j}xixj\n)\n,\n(13)\nPr (y = −1|xV = xV ) = σ ( − ∑\ni∈V\nβixi − ∑\n{i,j}∈E\nβ{i,j}xixj\n)\n(14)\nfor some real constants βi’s and β{i,j}’s. For any i ∈ V , we say that the covariate xi has an individual effect on y if and only if βi 6= 0; for any {i, j} ∈ E, we say that the covariates xi and xj interact if and only if β{i,j} 6= 0. Let Ṽ := V ∪ {0}, Ẽ := {{i, j} : i, j ∈ Ṽ , i 6= j}, and β{i,0} := βi for all i ∈ V . Then, the structure of the model (including both individual effects and cooperative interactions) is fully captured by the simple graph G̃ = (Ṽ , Ĩ), where Ĩ := {(i, j) ∈ Ẽ : β{i,j} 6= 0}. As before, our goal is to recover the graph G̃ from i.i.d. samples of (xV , y).\nToward this goal, we shall introduce an additional covariate x0, which we assume to be uniformly over {+1,−1} and independent of xV , and use it to define an auxiliary model, for which the conditional probabilities of the outcome ỹ given the covariates x\nṼ are given by:\nPr (\nỹ = +1|x Ṽ = x Ṽ\n) = σ ( ∑\n{i,j}∈Ẽ\nβ{i,j}xixj\n)\n, (15)\nPr (\nỹ = −1|x Ṽ = x Ṽ\n) = σ ( − ∑\n{i,j}∈Ẽ\nβ{i,j}xixj\n)\n. (16)\nBy the results of Section II, if the underlying graph G̃ is known to be acyclic, it can be recovered from the weight assignment:\nw̃{i,j} := ∣ ∣2Pr ( ỹ = +1|xi = +1, xj = +1 ) − 1 ∣ ∣ (17)\nfor all {i, j} ∈ Ẽ. Note that when j = 0, we trivially have\nPr (ỹ = +1|xi = +1, x0 = +1) = Pr (y = +1|xi = +1) ,\nso\nw̃{i,0} = |2Pr (y = +1|xi = +1)− 1| (18)\nfor all i ∈ V . On the other hand, when i, j 6= 0 and i 6= j, we may write\nPr ( ỹ = +1|xi = +1, xj = +1 )\n= 1\n2 Pr\n( ỹ = +1|xi = +1, xj = +1, x0 = +1 ) +\n1 2 Pr ( ỹ = +1|xi = +1, xj = +1, x0 = −1 )\n= 1\n2 Pr\n( y = +1|xi = +1, xj = +1 ) +\n1 2 Pr ( ỹ = +1|xi = +1, xj = +1, x0 = −1 ) .\nTo proceed, further note that\n2d−2Pr ( ỹ = +1|xi = +1, xj = +1, x0 = −1 )\n= ∑\nxV :(xi,xj)=(+1,+1)\nPr (ỹ = +1|xV = xV , x0 = −1)\n= ∑\nxV :(xi,xj)=(+1,+1)\nσ ( − ∑\nk∈V\nβkxk + ∑\n{k,l}∈E\nβ{k,l}xkxl\n)\n= ∑\nxV :(xi,xj)=(−1,−1)\nσ ( ∑\nk∈V\nβkxk + ∑\n{k,l}∈E\nβ{k,l}xkxl\n)\n(19)\n= ∑\nxV :(xi,xj)=(−1,−1)\nPr (y = +1|xV = xV )\n= 2d−2Pr ( y = +1|xi = −1, xj = −1 )\nwhere (19) follows from the simple change of variable xV → −xV . It thus follows that\nPr ( ỹ = +1|xi = +1, xj = +1, x0 = −1 )\n= Pr ( y = +1|xi = −1, xj = −1 )\nand hence\nPr ( ỹ = +1|xi = +1, xj = +1 )\n= 1\n2 Pr\n( y = +1|xi = +1, xj = +1 ) +\n1 2 Pr ( y = +1|xi = −1, xj = −1 )\ngiving\nw̃{i,j} = ∣ ∣Pr ( y = +1|xi = +1, xj = +1 ) +\nPr ( y = +1|xi = −1, xj = −1 ) − 1 ∣ ∣ (20)\nfor any {i, j} ∈ E. Combining (18) and (20), we conclude that the weight assignment (w̃{i,j} : {i, j} ∈ Ẽ), and hence any acyclic G̃, can be fully recovered from the low-order joint probabilities (p(xi, xj , y) : {i, j} ∈ E). The results of Section III can be extended similarly; the details are omitted."
    }, {
      "heading" : "V. SIMULATION RESULTS",
      "text" : "In our simulations, we randomly generate 5,000 logistic regression models, each including 10 independent binary covariates. For each model that we generate, we randomly choose 5 individual effects and 5 interaction pairs, resulting in an acyclic graph as its underlying structure. The model parameters βi for each individual effect and β{i,j} for each interaction pair are randomly assigned from a uniform distribution over [−µ,−λ] ∪ [λ, µ] with 0 < λ < µ. In Fig. 1, we compare the detection rate of Algorithm 1 for different parameter ranges (λ, µ) = (0.3, 0.5), (0.5, 1), and (1, 2), under the sample sizes of 300, 600, 900, 1,200 and 1,500, respectively. Here, we emphasize that a logistic regression model is correctly detected if and only if all 10 features, including 5 individual effects and 5 interaction pairs, are correctly detected. Clearly, the detection\n5\nrate increases as the lower and upper bounds of the parameter range increase, and Algorithm 1 can achieve a high detection rate (at least 93%) with a reasonably large number of training samples (around 1200). Also in Fig. 1, we plot the fitted curves of the detection rate with respect to the increasing sample size based on the functional form n ∝ log(1/(1 − detection rate)) that we derived in (12) in Section III. It is clear that the curves fit very well with the empirical results, thus validating the order-tightness of the lower bound in (12).\nNext, we shall compare the performance of our algorithm (Algorithm 1) with three generic feature selection algorithms: mRMR forward selection [10], mutual information (MI) ranking [8], and the problem-specific L1-regularized logistic regression algorithm [13], [14]. For the mRMR forward selection, MI ranking, and L1-regularized logistic regression algorithms, we shall view each of the single variables xi’s and the interaction terms xixj ’s as a separate feature, and assume that the number of features to be selected is known. The estimation of mutual information in the mRMR forward selection and MI ranking algorithms is based on [15], [16]. For the L1-regularized logistic regression algorithm, we tune the regularization parameter till the desired number of nonzero coefficients is obtained.\nIn Fig. 2, we compare the detection rate of Algorithm 1 with that of mRMR forward selection, MI ranking, and L1regularized logistic regression algorithms for (λ, µ) = (0.3, 0.5) under a finite number of data samples. As we can see, Algorithm 1 achieves a significantly higher detection rate than the mRMR forward selection and the MI ranking algorithms, especially when the sample size is relatively small. This is due to the facts that: 1) Algorithm 1 exploits the fact that the underlying interaction graph is acyclic while the other two algorithms do not; 2) the proposed influence measure is much easier to estimate than MI. (The performances of the mRMR forward selection and the MI ranking algorithms are nearly\nidentical since the feature candidates are pairwise independent.) On the other hand, the performances of Algorithm 1 and the L1-regularized logistic regression algorithm appear to be very comparable. It is somewhat surprising that the L1-regularized logistic regression algorithm performs well for typical problem instances with finite sample sizes. Intuitively, this is related to the fact that acyclic graphs are “sparse” graphs. However, analyzing the sample complexity of the L1regularized logistic regression algorithm appears to be very challenging.\nFor completeness, in Fig. 3 and Fig. 4 we also compare the miss detection/false positive rate and prediction accuracy of Algorithm 1 with those of the mRMR forward selection, the MI ranking, and the L1-regularized logistic regression algorithms. Note that since the number of features selected is fixed in this case, the miss detection and false positive rates are identical. The prediction accuracy is calculated as follows. For each logistic regression model that we generate, we first obtain the model structure via one of the algorithms under the consideration, followed by standard logistic regression for parameter estimation. Once each logistic regression model is reconstructed, we randomly generate 200 testing samples to estimate the accuracy of the outcome prediction. As we can see, the relative performance among these algorithms is very similar to the case with the detection rate."
    }, {
      "heading" : "VI. CONCLUDING REMARKS",
      "text" : "An important problem in bioinformatics is to identify interactive effects among profiled variables for outcome prediction. In this paper, a simple logistic regression model with pairwise interactions among the binary covariates was considered. Modeling the structure of the interactions by a graph G, our goal was to recover G from i.i.d. samples of the covariates and the outcome. When viewed as a feature selection problem, a simple quantity called influence is proposed as a measure of\n6\nthe marginal effects of the interaction terms on the outcome. For the case where G is known to be acyclic, it is shown that a simple algorithm that is based on a maximum-weight spanning tree with respect to the plug-in estimates of the influences not only has strong theoretical performance guarantees, but can also outperform generic feature selection algorithms for recovering the graph from i.i.d. samples of the covariates and the outcome. A sample complexity analysis for detecting the interaction graph was provided, and the results were further extended to the model that includes both individual effects and pairwise interactions.\nIn our future work, we would like to understand the behavior\nof the L1-regularized logistic regression algorithm from a theoretical standpoint, and also extend our results to the more challenging case where the interaction graph might be cyclic."
    }, {
      "heading" : "APPENDIX A PROOF OF THE PROPOSITIONS",
      "text" : "A. Proof of Proposition 1\nWe begin with the following lemma. Lemma 1: For any acyclic G and any i ∈ V , we have\nPr(xi = +1, y = +1) = 1\n4 .\nA proof of Lemma 1 can be found in Appendix B-A. Now fix {i, j} ∈ E. By Lemma 1, we have\nPr(xi = +1, xj = −1, y = +1) = Pr(xi = +1, y = +1)− Pr(xi = +1, xj = +1, y = +1)\n= 1\n4 − Pr(xi = +1, xj = +1, y = +1)\n7 (a) = Pr(xi = +1, xj = +1)− Pr(xi = +1, xj = +1, y = +1) = Pr(xi = +1, xj = +1, y = −1),\nand hence\nPr(y = +1|xi = +1, xj = −1) (b) = Pr(y = −1|xi = +1, xj = +1),\n(21)\nwhere (a) and (b) are due to the fact that Pr(xi = +1, xj = +1) = Pr(xi = +1, xj = −1) = 1/4. Substituting (21) into (4) completes the proof of Proposition 1.\nB. Proof of Propositions 2 and 7\nFix {i, j} ∈ I . For any xV ∈ {+1,−1}d, let\nζ+ {i,j} (xV ) := σ ( β{i,j} + ∑\n{k,l}∈I\\{{i,j}}\nβ{k,l}xkxl\n)\n, (22)\nζ−{i,j}(xV ) := σ ( − β{i,j} + ∑\n{k,l}∈I\\{{i,j}}\nβ{k,l}xkxl\n)\n, (23)\nζ{i,j}(xV ) := ζ + {i,j}(xV )− ζ − {i,j}(xV ). (24)\nWe have the following lemma, for which a proof is provided in Appendix B-B.\nLemma 2: Assume that G is acyclic. We have\nPr(y = +1|xi = +1, xj = +1)− Pr(y = −1|xi = +1, xj = +1)\n= 2−(d−2) ∑\nxV :(xi,xj)=(+1,+1)\nζ{i,j}(xV ). (25)\nTo prove Proposition 2, note that the sigmoid function σ(·) is strictly monotone increasing. We thus have ζ{i,j}(xV ) > 0 for any xV ∈ {+1,−1}d when β{i,j} > 0, and ζ{i,j}(xV ) < 0 for any xV ∈ {+1,−1}d when β{i,j} < 0. It thus follows from Lemma 2 and (4) that\nw{i,j} = 2 −(d−2)\n∣ ∣ ∣ ∣ ∣ ∣ ∣\n∑\nxV :(xi,xj)=(+1,+1)\nζ{i,j}(xV )\n∣ ∣ ∣ ∣ ∣ ∣ ∣ > 0.\nThis completes the proof of Proposition 2. To prove Proposition 7, let G′ = (V, T ) be a tree that covers G. With a slight abuse of notation, let\nζ{i,j}(zT\\{{i,j}})\n:= σ ( β{i,j} + ∑\n{k,l}∈T\\{{i,j}}\nβ{k,l}z{k,l}\n)\n−\nσ ( − β{i,j} + ∑\n{k,l}∈T\\{{i,j}}\nβ{k,l}z{k,l}\n)\n(a) = σ ( β{i,j} + ∑\n{k,l}∈I\\{{i,j}}\nβ{k,l}z{k,l}\n)\n−\nσ ( − β{i,j} + ∑\n{k,l}∈I\\{{i,j}}\nβ{k,l}z{k,l}\n)\n,\nwhere (a) follows from the fact that β{k,l} = 0 for any {k, l} 6∈ I . Note that: 1) ζ{i,j}(xV ) = ζ{i,j}(zT\\{{i,j}}) for any xV ∈ {+1,−1}d; 2) G′−{i, j} is a union of two trees where i and j are in different trees, such that the mapping between xV and\n(zT\\{{i,j}}, xi, xj) is one-on-one. We thus have ∑\nxV :(xi,xj)=(+1,+1)\nζ{i,j}(xV )\n= ∑\nzT\\{{i,j}}∈{+1,−1} d−2\nζ{i,j}(zT\\{{i,j}}),\nand hence\nPr(y = +1|xi = +1, xj = +1)− Pr(y = −1|xi = +1, xj = +1)\n= 2−(d−2) ∑\nzT\\{{i,j}}∈{+1,−1} d−2\nζ{i,j}(zT\\{{i,j}}).\nAssume without loss of generality that β{i,j} > 0. (Otherwise, we may simply replace β{i,j} by −β{i,j}, and the rest of the proof remains the same.) By the monotonicity of the sigmoid function σ(·), we have ζ{i,j}(zT\\{{i,j}}) > 0 for any zT\\{{i,j}} ∈ {+1,−1}d−2. It thus follows from Lemma 2 and (4) that\nw{i,j} = 2 −(d−2)\n∑\nzT\\{{i,j}}∈{+1,−1} d−2\nζ{i,j}(zT\\{{i,j}})\n≥ 2−(d−2) ∑\nzT\\{{i,j}}∈∆1\nζ{i,j}(zT\\{{i,j}}),\nwhere\n∆1 := { zT\\{{i,j}} ∈ {+1,−1} d−2 :\n∣ ∣ ∣ ∣ ∣ ∣\n∑\n{k,l}∈T\\{i,j}\nβ{k,l}z{k,l}\n∣ ∣ ∣ ∣ ∣ ∣ ≤ µ    .\nFor any zT\\{{i,j}} ∈ ∆1, we have\nζ{i,j}(zT\\{{i,j}})\n≥ min |x|≤3µ\n[ σ ( β{i,j} + x ) − σ ( − β{i,j} + x )]\n= σ ( β{i,j} + 3µ ) − σ ( − β{i,j} + 3µ )\n≥ min 0<β≤λ [σ(β + 3µ) − σ(−β + 3µ)]\n= σ(λ+ 3µ) − σ(−λ+ 3µ).\nIt follows that\nw{i,j} ≥ |∆1| 2d−2 [σ(λ+ 3µ)− σ(−λ+ 3µ)]\n≥ √ 2\nπd [σ(λ+ 3µ)− σ(−λ+ 3µ)] = γ;\nhere the last inequality follows from the following lemma, whose proof can be found in Appendix B-C.\nLemma 3: Let zi, i = 1, . . . , q, be i.i.d. random variables, each uniformly distributed over {+1,−1}. Then, for any real constants ai’s we have\nPr (∣ ∣\n∣\nq ∑\ni=1\naixi\n∣ ∣ ∣ ≤ a ) ≥ √\n2\nπ(q + 2) , (26)\nwhere a := max(|ai| : i = 1, . . . , q).\n8 C. Proof of Proposition 3\nFix {i, j} ∈ E, and assume that i and j are either disconnected in G, or the length of the unique path between i and j in G is even. By the assumption that G is acyclic, there must exist a vertex bipartition (V1, V2) of G such that both i and j are in V1. Note that\nI ⊆ {{k, l} : k ∈ V1, l ∈ V2};\nthus we have\n2d−2Pr(y = +1|xi = +1, xj = +1) = ∑\nxV :(xi,xj)=(+1,+1)\nPr (y = +1|xV = xV )\n= ∑\nxV :(xi,xj)=(+1,+1)\nσ ( ∑\nk∈V1\n∑\nl∈V2\nβ{k,l}xkxl\n)\n(a) =\n∑\nxV :(xi,xj)=(+1,+1)\nσ ( − ∑\nk∈V1\n∑\nl∈V2\nβ{k,l}xkxl\n)\n= ∑\nxV :(xi,xj)=(+1,+1)\nPr (y = −1|xV = xV )\n= 2d−2Pr(y = −1|xi = +1, xj = +1),\nwhere (a) follows from the change of variable xl → −xl for all l ∈ V2. We then have\nPr(y = +1|xi = +1, xj = +1) = Pr(y = −1|xi = +1, xj = +1),\nwhich, together with (4), implies that w{i,j} = 0. This completes the proof of Proposition 3.\nD. Proof of Propositions 4 and 8\nAs mentioned, we only need to consider m ≥ 2 that is odd. For notational convenience, let us assume without loss of generality that is = s for all s ∈ {1, . . . ,m+ 1}.\nTo compare w{1,m+1} with w{s,s+1} for s ∈ {1, . . . ,m}, note that by (4) and the fact that Pr(y = +1|x1 = +1, xm+1 = +1) + Pr(y = −1|x1 = +1, xm+1 = +1) = 1 we have\nw{1,m+1}\n= max { 2Pr(y = +1|x1 = +1, xm+1 = +1)− 1, 2Pr(y = −1|x1 = +1, xm+1 = +1)− 1 } = max {\n1− 2Pr(y = +1|x1 = +1, xm+1 = +1), 1− 2Pr(y = −1|x1 = +1, xm+1 = +1) } .\nBelow, we shall compare Pr(y = +1|xs = +1, xs+1 = +1) with Pr(y = +1|x1 = +1, xm+1 = +1) and Pr(y = −1|x1 = +1, xm+1 = +1) for s = 1, s = m, and s ∈ {2, . . . , m − 1} separately.\nCase 1: s = 1. We have the following lemma, for which a proof is provided in Appendix B-D.\nLemma 4: For any m ≥ 2, we have\nPr(y = +1|x1 = +1, x2 = +1)− Pr(y = +1|x1 = +1, xm+1 = +1)\n= 2−(d−2) ∑\nxV :(x1,x2,xm+1)=(+1,+1,−1)\nζ{1,2}(xV ). (27)\nFurthermore, for any m ≥ 2 that is odd, we have\nPr(y = +1|x1 = +1, x2 = +1)− Pr(y = −1|x1 = +1, xm+1 = +1)\n= 2−(d−2) ∑\nxV :(x1,x2,xm+1)=(+1,+1,+1)\nζ{1,2}(xV ). (28)\nTo show w{1,2} > w{1,m+1}, we shall consider the cases with β{1,2} > 0 and β{1,2} < 0 separately. When β{1,2} > 0, we have ζ{1,2}(xV ) > 0 for any xV ∈ {+1,−1}d. By Lemma 4, we have\nw{1,2} = |2Pr(y = +1|x1 = +1, x2 = +1)− 1| ≥ 2Pr(y = +1|x1 = +1, x2 = +1)− 1 > max {\n2Pr(y = +1|x1 = +1, xm+1 = +1)− 1, 2Pr(y = −1|x1 = +1, xm+1 = +1)− 1 }\n= w{1,m+1}.\nWhen β{1,2} < 0, we have ζ{1,2}(xV ) < 0 for any xV ∈ {+1,−1}d. By Lemma 4, we have\nw{1,2} = |2Pr(y = +1|x1 = +1, x2 = +1)− 1| ≥ 1− 2Pr(y = +1|x1 = +1, x2 = +1) > max {\n1− 2Pr(y = +1|x1 = +1, xm+1 = +1), 1− 2Pr(y = −1|x1 = +1, xm+1 = +1) }\n= w{1,m+1}.\nTo show w{1,2} ≥ w{1,m+1} + γ, let G′ = (V, T ) be a tree that covers G. Note that: 1)\nζ{1,2}(xV ) = ζ{1,2}\n(\nzT\\{{1,2},{2,3}},\nz2,3 = x2xm+1/ m ∏\nr=3\nz{r,r+1}\n)\n,\nfor any xV ∈ {+1,−1}d; 2) G′ − {{1, 2}, {2, 3}} is a union of three trees where 1, 2, and m+1 are in different trees, such that the mapping between xV and (zT\\{{1,2},{2,3}}, x1, x2, xm+1) is one-on-one. We thus have\n∑\nxV :(x1,x2,xm+1)=(+1,+1,a)\nζ{1,2}(xV )\n= ∑\nzT\\{{1,2},{2,3}}∈{+1,−1} d−3\nζ{1,2}\n(\nzT\\{{1,2},{2,3}},\nz{2,3} = a/\nm ∏\nr=3\nz{r,r+1}\n)\nfor any a ∈ {+1,−1}. It follows that\nPr(y = +1|x1 = +1, x2 = +1)− Pr(y = +1|x1 = +1, xm+1 = +1)\n= 2−(d−2) ∑\nzT\\{{1,2},{2,3}}∈{+1,−1} d−3\nζ{1,2}\n(\nzT\\{{1,2},{2,3}}, z{2,3} = − m ∏\nr=3\nz{r,r+1}\n)\n9 and\nPr(y = +1|x1 = +1, x2 = +1)− Pr(y = −1|x1 = +1, xm+1 = +1)\n= 2−(d−2) ∑\nzT\\{{1,2},{2,3}}∈{+1,−1} d−3\nζ{1,2}\n(\nzT\\{{1,2},{2,3}}, z{2,3} = m ∏\nr=3\nz{r,r+1}\n)\n.\nAssume without loss of generality that β{1,2} > 0. (Otherwise, we may simply replace β{1,2} by −β{1,2}, and the rest of the proof remains the same.) By the monotonicity of the sigmoid function σ(·), we have ζ{1,2}(zT\\{{1,2}}) > 0 for any zT\\{{1,2}} ∈ {+1,−1}d−2. We thus have\nPr(y = +1|x1 = +1, x2 = +1)− Pr(y = +1|x1 = +1, xm+1 = +1)\n≥ 2−(d−2) ∑\nzT\\{{1,2},{2,3}}∈∆2\nζ{1,2}\n(\nzT\\{{1,2},{2,3}}, z{2,3} = − m ∏\nr=3\nz{r,r+1}\n)\n,\nwhere\n∆2 := { zT\\{{i,j}} ∈ {+1,−1} d−3 :\n∣ ∣ ∣ ∣ ∣ ∣\n∑\n{k,l}∈T\\{{1,2},{2,3}}\nβ{k,l}z{k,l}\n∣ ∣ ∣ ∣ ∣ ∣ ≤ µ    .\nFor any zT\\{{1,2},{2,3}} ∈ ∆2, we have ∣\n∣ ∣ ∣ ∣ ∣ β{2,3} ( − m ∏\nr=3\nz{r,r+1}\n)\n+ ∑\n{k,l}∈T\\{{1,2},{2,3}}\nβ{k,l}z{k,l}\n∣ ∣ ∣ ∣ ∣ ∣\n≤ ∣ ∣\n∣ β{2,3}\n∣ ∣ ∣ +\n∣ ∣ ∣ ∣ ∣ ∣\n∑\n{k,l}∈T\\{{1,2},{2,3}}\nβ{k,l}z{k,l}\n∣ ∣ ∣ ∣ ∣ ∣\n≤ µ+ µ ≤ 3µ,\nand hence\nζ{1,2}\n( zT\\{{1,2},{2,3}}, z{2,3} = − m ∏\nr=3\nz{r,r+1}\n)\n≥ min |x|≤3µ\n[ σ ( β{1,2} + x ) − σ ( − β{1,2} + x )]\n= σ ( β{1,2} + 3µ ) − σ ( − β{1,2} + 3µ )\n≥ min 0<β≤λ [σ(β + 3µ) − σ(−β + 3µ)]\n= σ(λ+ 3µ) − σ(−λ+ 3µ).\nIt follows from Lemma 3 that\nPr(y = +1|x1 = +1, x2 = +1)− Pr(y = +1|x1 = +1, xm+1 = +1)\n≥ |∆2| 2d−2 [σ(λ+ 3µ)− σ(−λ+ 3µ)] ≥ 1√ 2πd [σ(λ+ 3µ)− σ(−λ+ 3µ)] = γ 2 .\nAlso, by a completely analogous argument, we have\nPr(y = +1|x1 = +1, x2 = +1)− Pr(y = −1|x1 = +1, xm+1 = +1) ≥ γ\n2 ,\nand therefore,\nw{1,2} − w{1,m+1} ≥ 2 · γ\n2 = γ.\nCase 2: s = m. The proof of this case is completely analogous to the previous case with s = 1 and is hence omitted here.\nCase 3: s ∈ {2, . . . , m − 1}. Fix s. We have the following lemma, for which a proof is provided in Appendix B-D.\nLemma 5: For any m ≥ 3, we have\n2d−2 [ Pr(y = +1|xs = +1, xs+1 = +1)− Pr(y = +1|x1 = +1, xm+1 = +1) ]\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,−1)\nζ{s,s+1}(xV )+\n∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,+1)\nζ{s,s+1}(xV ). (29)\nFor any m ≥ 3 that is odd, we have\n2d−2 [ Pr(y = +1|xs = +1, xs+1 = +1)− Pr(y = −1|x1 = +1, xm+1 = +1) ]\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,+1)\nζ{s,s+1}(xV )+\n∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,−1)\nζ{s,s+1}(xV ). (30)\nTo show w{s,s+1} > w{1,m+1}, we shall consider the cases with β{s,s+1} > 0 and β{s,s+1} < 0 separately. When β{s,s+1} > 0, we have ζ{s,s+1}(xV ) > 0 for any xV ∈ {+1,−1}d. By Lemma 5, we have\nw{s,s+1} ≥ 2Pr(y = +1|xs = +1, xs+1 = +1)− 1 > max {\n2Pr(y = +1|x1 = +1, xm+1 = +1)− 1, 2Pr(y = −1|x1 = +1, xm+1 = +1)− 1 }\n= w{1,m+1}.\nWhen β{s,s+1} < 0, we have ζ{s,s+1}(xV ) < 0 for any xV ∈ {+1,−1}d. By Lemma 5, we have\nw{s,s+1} ≥ 1− 2Pr(y = +1|xs = +1, xs+1 = +1) > max {\n1− 2Pr(y = +1|x1 = +1, xm+1 = +1), 1− 2Pr(y = −1|x1 = +1, xm+1 = +1) }\n= w{1,m+1}.\nTo show w{s,s+1} ≥ w{1,m+1} + γ, let G′ = (V, T ) be a tree that covers G. Note that: 1)\nζ{s,s+1}(xV ) = ζ{s,s+1}\n(\nzT\\{{1,2},{s,s+1}{m,m+1}},\nz{1,2} = x1xs/\ns−1 ∏\nr=2\nz{r,r+1},\n10\nz{m,m+1} = xs+1xm+1/\nm−1 ∏\nr=s+1\nz{r,r+1}\n)\n,\nfor any xV ∈ {+1,−1}d; 2) G′−{{1, 2}, {s, s+1}, {m,m+1}} is a union of four trees where 1, s, s + 1 and m + 1 are in different trees, such that the mapping between xV and (zT\\{{1,2},{s,s+1},{m,m+1}}, x1, xs, xs+1, xm+1) is one-onone. We thus have\n∑\nxV :(x1,xs,xs+1,xm+1)=(a,+1,+1,b)\nζ{s,s+1}(xV )\n= ∑\nzT\\{{1,2},{s,s+1},{m,m+1}}∈{+1,−1} d−4\nζ{s,s+1}\n(\nzT\\{{1,2},{s,s+1},{m,m+1}}, z{1,2} = a/ s−1 ∏\nr=2\nz{r,r+1},\nz{m,m+1} = b/\nm−1 ∏\nr=s+1\nz{r,r+1}\n)\nfor any a, b ∈ {+1,−1}. It follows that\nPr(y = +1|xs = +1, xs+1 = +1) − Pr(y = +1|x1 = +1, xm+1 = +1)\n= 2−(d−2) ∑\nzT\\{{1,2},{s,s+1},{m,m+1}}∈{+1,−1} d−4\n[\nζ{s,s+1}\n(\nzT\\{{1,2},{s,s+1},{m,m+1}},\nz{1,2} = s−1 ∏\nr=2\nz{r,r+1}, z{m,m+1} = − m−1 ∏\nr=s+1\nz{r,r+1}\n)\n+\nζ{s,s+1}\n(\nzT\\{{1,2},{s,s+1},{m,m+1}},\nz{1,2} = − s−1 ∏\nr=2\nz{r,r+1}, z{m,m+1} =\nm−1 ∏\nr=s+1\nz{r,r+1}\n)]\nand\nPr(y = +1|xs = +1, xs+1 = +1) − Pr(y = −1|x1 = +1, xm+1 = +1)\n= 2−(d−2) ∑\nzT\\{{1,2},{s,s+1},{m,m+1}}∈{+1,−1} d−4\n[\nζ{s,s+1}\n(\nzT\\{{1,2},{s,s+1},{m,m+1}},\nz{1,2} =\ns−1 ∏\nr=2\nz{r,r+1}, z{m,m+1} =\nm−1 ∏\nr=s+1\nz{r,r+1}\n)\n+\nζ{s,s+1}\n(\nzT\\{{1,2},{s,s+1},{m,m+1}},\nz{1,2} = − s−1 ∏\nr=2\nz{r,r+1}, z{m,m+1} = − m−1 ∏\nr=s+1\nz{r,r+1}\n)]\n.\nAssume without loss of generality that β{s,s+1} > 0. (Otherwise, we may simply replace β{s,s+1} by −β{s,s+1}, and the rest of the proof remains the same.) By the monotonicity of the sigmoid function σ(·), we have ζ{s,s+1}(zT\\{{s,s+1}}) > 0 for any zT\\{{s,s+1}} ∈ {+1,−1}d−2. We thus have\nPr(y = +1|xs = +1, xs+1 = +1)\n− Pr(y = +1|x1 = +1, xm+1 = +1)\n≥ 2−(d−2) ∑\nzT\\{{1,2},{s,s+1},{m,m+1}}∈∆3\n[\nζ{s,s+1}\n(\nzT\\{{1,2},{s,s+1},{m,m+1}},\nz{1,2} =\ns−1 ∏\nr=2\nz{r,r+1}, z{m,m+1} = − m−1 ∏\nr=s+1\nz{r,r+1}\n)\n+\nζ{s,s+1}\n(\nzT\\{{1,2},{s,s+1},{m,m+1}},\nz{1,2} = − s−1 ∏\nr=2\nz{r,r+1}, z{m,m+1} = m−1 ∏\nr=s+1\nz{r,r+1}\n)]\n,\nwhere\n∆3 := { zT\\{{1,2},{s,s+1},{m,m+1}} ∈ {+1,−1} d−4 :\n∣ ∣ ∣ ∣ ∣ ∣\n∑\n{k,l}∈T\\{{1,2},{s,s+1},{m,m+1}}\nβ{k,l}z{k,l}\n∣ ∣ ∣ ∣ ∣ ∣ ≤ µ    .\nFor any zT\\{{1,2},{s,s+1},{m,m+1}} ∈ ∆3, we have ∣\n∣ ∣ ∣ ∣ ∣ β{1,2}\ns−1 ∏\nr=2\nz{r,r+1} + β{m,m+1}\n( − m−1 ∏\nr=s+1\nz{r,r+1}\n)\n+\n∑\n{k,l}∈T\\{{1,2},{s,s+1},{m,m+1}}\nβ{k,l}z{k,l}\n∣ ∣ ∣ ∣ ∣ ∣\n≤ ∣ ∣\n∣ β{1,2}\n∣ ∣ ∣ + ∣ ∣\n∣ β{m,m+1}\n∣ ∣ ∣ +\n∣ ∣ ∣ ∣ ∣ ∣\n∑\n{k,l}∈T\\{{1,2},{s,s+1},{m,m+1}}\nβ{k,l}z{k,l}\n∣ ∣ ∣ ∣ ∣ ∣\n≤ µ+ µ+ µ = 3µ,\nand hence\nζ{s,s+1}\n(\nzT\\{{1,2},{s,s+1},{m,m+1}},\nz{1,2} =\ns−1 ∏\nr=2\nz{r,r+1}, z{m,m+1} = − m−1 ∏\nr=s+1\nz{r,r+1}\n)\n≥ min |x|≤3µ\n[ σ ( β{s,s+1} + x ) − σ ( − β{s,s+1} + x )]\n= σ ( β{s,s+1} + 3µ ) − σ ( − β{s,s+1} + 3µ )\n≥ min 0<β≤λ\n[ σ ( β + 3µ ) − σ ( − β + 3µ )]\n= σ(λ+ 3µ) − σ(−λ+ 3µ).\nSimilarly,\nζ{s,s+1}\n(\nzT\\{{1,2},{s,s+1},{m,m+1}},\nz{1,2} = − s−1 ∏\nr=2\nz{r,r+1}, z{m,m+1} =\nm−1 ∏\nr=s+1\nz{r,r+1}\n)\n= σ(λ+ 3µ) − σ(−λ+ 3µ).\nIt follows from Lemma 3 that\nPr(y = +1|xs = +1, xs+1 = +1)− Pr(y = +1|x1 = +1, xm+1 = +1)\n11\n≥ |∆3| 2d−3 [σ(λ+ 3µ)− σ(−λ+ 3µ)] ≥ 1√ 2πd [σ(λ+ 3µ)− σ(−λ+ 3µ)] = γ 2 .\nAlso, by a completely analogous argument, we have\nPr(y = +1|xs = +1, xs+1 = +1)− Pr(y = −1|x1 = +1, xm+1 = +1) ≥ γ\n2 ,\nand therefore,\nw{s,s+1} − w{1,m+1} ≥ 2 · γ\n2 = γ.\nE. Proof of Proposition 5\nLet us first show that I ⊆ T . Assume on the contrary that there exists an {i, j} ∈ I but with {i, j} 6∈ T . Let {\n{k1, k2}, {k2, k3}, . . . , {km, km+1} } be a path in G′ such that k1 = i, km+1 = j, and m ≥ 2. Such a path must exist since G′ is a spanning tree and {i, j} 6∈ T by the assumption. Let V1 be the set of vertices that are connected to i in G − {i, j} and V2 := V − V1. Since k1 ∈ V1 and km+1 ∈ V2, there must exist an s ∈ {1, . . . ,m} such that ks ∈ V1 and ks+1 ∈ V2.\nIf ks and ks+1 are connected in G, then the unique path between ks and ks+1 in G must include {i, j}. (Otherwise, we would have ks+1 ∈ V1.) By assumption 2), we have uks,ks+1 < ui,j . If ks and ks+1 are disconnected in G, by assumption 1) we have uks,ks+1 < ui,j .\nIn either case, G′ − {ks, ks+1} + {i, j} is a new spanning tree with a larger total weight than G′, violating the assumption that G′ is a maximum-weight spanning tree. We therefore must have I ⊆ T . Furthermore, by assumption 1) we have I ⊆ W . We thus have I ⊆ I ∩W .\nNow to show I = T ∩W , we only need to show that |T ∩ W | ≤ |I |, which can be argued as follows. Let ω(G) be the number of connected components in G. Then T must include at least ω(G) − 1 edges that cross two different connected components of G. By assumption 1) the weights of these edges must be less than or equal to η, and we thus have\n|T ∩ (E −W )| ≥ ω(G)− 1.\nIt follows immediately that\n|T ∩W | = |T − T ∩ (E −W )| = |T | − |T ∩ (E −W )| ≤ (d− 1) − (ω(G)− 1) = |I |.\nThis completes the proof of Proposition 5."
    }, {
      "heading" : "APPENDIX B PROOF OF LEMMAS",
      "text" : "A. Proof of Lemma 1\nBy the assumption that G is acyclic, there must exist a vertex bipartition (V1, V2) of G such that I ⊆ {{k, l} : k ∈ V1, l ∈ V2}. Assume without loss of generality that i ∈ V1. We have\n2d−1Pr(y = +1|xi = +1)\n(a) =\n∑\nxV :xi=+1\nPr(y = +1|xV = xV )\n= ∑\nxV :xi=+1\nσ ( ∑\nk∈V1\n∑\nl∈V2\nβ{k,l}xkxl\n)\n(b) =\n∑\nxV :xi=+1\nσ ( − ∑\nk∈V1\n∑\nl∈V2\nβ{k,l}xkxl\n)\n= ∑\nxV :xi=+1\nPr(y = −1|xV = xV )\n(c) = 2d−1Pr(y = −1|xi = +1),\nand hence\nPr(y = +1|xi = +1) = Pr(y = −1|xi = +1). (31)\nHere, (a) and (c) are due to the fact that Pr (\nxV \\{i} = xV \\{i}|xi = +1 )\n= 2−(d−1) for any\nxV \\{i} ∈ {+1,−1}d−1, and (b) follows from the change of variable xl → −xl for all l ∈ V2. Since Pr(y = +1|xi = +1) + Pr(y = −1|xi = +1) = 1, (31) immediately implies that Pr(y = +1|xi = +1) = 1/2. We thus have\nPr(xi = +1, y = +1) (d) = 1\n2 Pr(y = +1|xi = +1) =\n1 4 ,\nwhere (d) follows from the fact that Pr(xi = +1) = 1/2.\nB. Proof of Lemma 2\nLet Gs = (Vs, Is), s = 1, . . . , t, be the connected components of G − {i, j}. By the assumption that G is acyclic, so is G − {i, j}. Therefore, Gs must be a tree for any s = 1, . . . , t. Further note that i and j are not connected in G − {i, j}, so they must belong to two different trees. Assume without loss of generality that i ∈ V1 and j ∈ V2. For s = 1, . . . , t, let (V (1)s , V (2) s ) be a vertex bipartition of Gs. Further assume without loss of generality that i ∈ V (1)1 and j ∈ V (1) 2 . Then we have\nI \\ {{i, j}} ⊆ {{k, l} : k ∈ V (1), l ∈ V (2)}\nwhere V (1) := ⋃t s=1 V (1) s and V (2) := ⋃t s=1 V (2) s . Note that by construction both i and j are in V (1). It follows that\n2d−2Pr(y = +1|xi = +1, xj = +1) = ∑\nxV :(xi,xj)=(+1,+1)\nPr(y = +1|xV = xV )\n= ∑\nxV :(xi,xj)=(+1,+1)\nζ+{i,j}(xV ) (32)\nand\n2d−2Pr(y = −1|xi = +1, xj = +1) = ∑\nxV :(xi,xj)=(+1,+1)\nPr (y = −1|xV = xV )\n= ∑\nxV :(xi,xj)=(+1,+1)\nσ ( − β{i,j} − ∑\nk∈V (1)\n∑\nl∈V (2)\nβ{k,l}xkxl\n)\n(a) =\n∑\nxV :(xi,xj)=(+1,+1)\nσ ( − β{i,j} + ∑\nk∈V (1)\n∑\nl∈V (2)\nβ{k,l}xkxl\n)\n12\n= ∑\nxV :(xi,xj)=(+1,+1)\nζ− {i,j} (xV ), (33)\nwhere (a) follows from the change of variable xl → −xl for all l ∈ V (2). Combining (32) and (33) completes the proof of (25).\nC. Proof of Lemma 3\nFirst note that the left-hand side of (26) is independent of the signs of the real constants ai’s, and the right-hand side of (26) is monotone decreasing with q. We thus may assume without loss of generality that\na1 ≥ a2 ≥ · · · ≥ aq > 0.\nFor i = 1, . . . , q, let\npi := Pr\n −a1 ≤ a1 i ∑\nj=1\nzj + a1 ai\nq ∑\nj=i+1\najzj < a1\n\n .\nWe have the following claim. Claim 1: pi is monotone decreasing with i.\nProof: Fix i ∈ {1, . . . , q − 1}. We have\npi = Pr\n −a1 ≤ a1 i ∑\nj=1\nzj + a1 ai\nq ∑\nj=i+1\najzj < a1\n\n\n= 2−i ∑\n(z1,...,zi)∈{+1,−1} i\nPr\n\n−\n\n1 +\ni ∑\nj=1\nzj\n\n a1\n≤ a1 ai\nq ∑\nj=i+1\najzj <\n 1− i ∑\nj=1\nzj\n\n a1\n\n\n= 2−i ⌊i/2⌋ ∑\nk=0\nci,k\n· Pr\n\n− (1 + i− 2k) a1 ≤ a1 ai\nq ∑\nj=i+1\najzj < (1 + i− 2k) a1\n\n\n(a) ≥ 2−i\n⌊i/2⌋ ∑\nk=0\nci,k\n· Pr\n\n− (1 + i− 2k) a1 ≤ a1\nai+1\nq ∑\nj=i+1\najzj < (1 + i− 2k) a1\n\n\n= 2−i ∑\n(z1,...,zi)∈{+1,−1} i\nPr\n\n−\n 1 + i ∑\nj=1\nzj\n\n a1\n≤ a1 ai+1\nq ∑\nj=i+1\najzj <\n 1− i ∑\nj=1\nzj\n\n a1\n\n\n= Pr\n −a1 ≤ a1 i ∑\nj=1\nzj + a1\nai+1\nq ∑\nj=i+1\najzj < a1\n\n\n= Pr\n −a1 ≤ a1 i+1 ∑\nj=1\nzj + a1\nai+1\nq ∑\nj=i+2\najzj < a1\n\n\n= pi+1,\nwhere\nci,k :=\n{\n1, if k = 0, ( i k ) − ( i k−1 ) , if 1 ≤ k ≤ ⌊i/2⌋,\nand (a) follows from the assumption that ai ≥ ai+1 > 0. We may thus conclude that pi is monotone decreasing with i.\nBy Claim 1, we have\nPr\n\n\n∣ ∣ ∣ ∣ ∣ ∣ q ∑\nj=1\najzj\n∣ ∣ ∣ ∣ ∣ ∣ ≥ a1   ≥ p1 ≥ pq\n= Pr\n −1 ≤ q ∑\nj=1\nzj < 1\n\n = 1\n2q\n(\nq\n⌊q/2⌋\n)\n(b) ≥\n√\n2\nπ(q + 2) ,\nwhere (b) follows from the well-known Wallis’ product [17] for π. This completes the proof of Lemma 3.\nD. Proof of Lemmas 4 and 5\nLet Gr = (Vr, Ir), r = 1, . . . , t, be the connected components of G − {{1, 2}, {2, 3}, . . . , {m,m + 1}}. By the assumption that G is acyclic, so is G − {{1, 2}, {2, 3}, . . . , {m,m + 1}}. Therefore, each connected component Gr must be a tree, and each of the vertices from {1, . . . ,m+ 1} is in a different tree. Further assume without loss of generality that r ∈ Vr for all r = 1, . . . , t (where apparently we have t ≥ m+ 1). Let V oddr and V evenr be the sets of vertices from Gr whose (graphical) distances to r are odd and even, respectively.\nTo prove Lemma 4, define for any a, b, c ∈ {+1,−1}\np+1,2,m+1(a, b, c)\n:= ∑\nxV :(x1,x2,xm+1)=(a,b,c)\nPr (y = +1|xV = xV )\n= ∑\nxV :(x1,x2,xm+1)=(a,b,c)\nσ ( ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\nand\np−1,2,m+1(a, b, c)\n:= ∑\nxV :(x1,x2,xm+1)=(a,b,c)\nPr (y = −1|xV = xV )\n= ∑\nxV :(x1,x2,xm+1)=(a,b,c)\nσ ( − ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n.\nWe can write\n2d−2Pr(y = +1|x1 = +1, x2 = +1) = p+1,2,m+1(+1,+1,+1) + p + 1,2,m+1(+1,+1,−1), (34) 2d−2Pr(y = +1|x1 = +1, xm+1 = +1) = p+1,2,m+1(+1,+1,+1) + p + 1,2,m+1(+1,−1,+1), (35) 2d−2Pr(y = −1|x1 = +1, xm+1 = +1) = p−1,2,m+1(+1,+1,+1) + p − 1,2,m+1(+1,−1,+1). (36)\nFor any m ≥ 2, we have\np+1,2,m+1(+1,+1,+1)\n= ∑\nxV :(x1,x2,xm+1)=(+1,+1,+1)\nζ+ {1,2} (xV ), (37)\n13\np+1,2,m+1(+1,+1,−1)\n= ∑\nxV :(x1,x2,xm+1)=(+1,+1,−1)\nζ+ {1,2} (xV ), (38)\np+1,2,m+1(+1,−1,+1)\n= ∑\nxV :(x1,x2,xm+1)=(+1,−1,+1)\nσ ( ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n(a) =\n∑\nxV :(x1,x2,xm+1)=(+1,+1,−1)\nσ (\n− β{1,2}+\n∑\n{k,l}∈I\\{{1,2}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,x2,xm+1)=(+1,+1,−1)\nζ−{1,2}(xV ), (39)\nwhere (a) follows from the change of variable xk → −xk for all k ∈ ⋃m+1r=2 Vr. Substituting (38) and (39) into (34) and (35) completes the proof of (27).\nFor any m ≥ 2 that is odd, we have\np−1,2,m+1(+1,+1,+1)\n= ∑\nxV :(x1,x2,xm+1)=(+1,+1,+1)\nσ ( − ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n(b) =\n∑\nxV :(x1,x2,xm+1)=(+1,+1,+1)\nσ (\n− β{1,2}+\n∑\n{k,l}∈I\\{{1,2}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,x2,xm+1)=(+1,+1,+1)\nζ− {1,2} (xV ), (40)\np−1,2,m+1(+1,−1,+1)\n= ∑\nxV :(x1,x2,xm+1)=(+1,−1,+1)\nσ ( − ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n(c) =\n∑\nxV :(x1,x2,xm+1)=(+1,+1,−1)\nσ (\nβ{1,2}+\n∑\n{k,l}∈I\\{{1,2}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,x2,xm+1)=(+1,+1,−1)\nζ+{1,2}(xV ), (41)\nwhere (b) follows from the change of variable xk → −xk for all k ∈ V oddr for r ∈ {1, 2, 4, . . . , m + 1,m + 2, . . . , t} or k ∈ V evenr for r ∈ {3, 5, . . . ,m}, and (c) follows from the change of variable xk → −xk for all k ∈ V oddr for r ∈ {1, 3, . . . , m} or k ∈ V evenr for r ∈ {2, 4, . . . ,m + 1, m + 2, . . . , t}. Substituting (37), (38), (40) and (41) into (35) and (36) completes the proof of (28).\nTo prove Lemma 5, let us first write\n2d−2Pr(y = +1|xs = +1, xs+1 = +1) = p+1,s,s+1,m+1(+1,+1,+1,+1)+\np+1,s,s+1,m+1(+1,+1,+1,−1)+ p+1,s,s+1,m+1(−1,+1,+1,+1)+ p+1,s,s+1,m+1(−1,+1,+1,−1), (42)\n2d−2Pr(y = +1|x1 = +1, xm+1 = +1) = p+1,s,s+1,m+1(+1,+1,+1,+1)+\np+1,s,s+1,m+1(+1,+1,−1,+1)+ p+1,s,s+1,m+1(+1,−1,+1,+1)+ p+1,s,s+1,m+1(+1,−1,−1,+1), (43)\n2d−2Pr(y = −1|x1 = +1, xm+1 = +1) = p−1,s,s+1,m+1(+1,+1,+1,+1)+\np−1,s,s+1,m+1(+1,+1,−1,+1)+ p−1,s,s+1,m+1(+1,−1,+1,+1)+ p−1,s,s+1,m+1(+1,−1,−1,+1). (44)\nFor any m ≥ 3, we have\np+1,s,s+1,m+1(+1,+1,+1,+1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,+1)\nζ+{s,s+1}(xV ), (45)\np+1,s,s+1,m+1(+1,+1,+1,−1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,−1)\nζ+{s,s+1}(xV ), (46)\np+1,s,s+1,m+1(−1,+1,+1,+1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,+1)\nζ+ {s,s+1} (xV ), (47)\np+1,s,s+1,m+1(−1,+1,+1,−1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,−1)\nζ+ {s,s+1} (xV ), (48)\np+1,s,s+1,m+1(+1,+1,−1,+1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,−1,+1)\nσ ( ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n(d) =\n∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,−1)\nσ (\n− β{s,s+1}+\n∑\n{k,l}∈I\\{{s,s+1}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,−1)\nζ− {s,s+1} (xV ), (49)\np+1,s,s+1,m+1(+1,−1,+1,+1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,−1,+1,+1)\nσ ( ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n(e) =\n∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,+1)\nσ (\n− β{s,s+1}+\n∑\n{k,l}∈I\\{{s,s+1}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,+1)\nζ−{s,s+1}(xV ), (50)\np+1,s,s+1,m+1(+1,−1,−1,+1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,−1,−1,+1)\nσ ( ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n14\n(f) =\n∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,−1)\nσ (\nβ{s,s+1}+\n∑\n{k,l}∈I\\{{s,s+1}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,−1)\nζ+ {s,s+1} (xV ), (51)\nwhere (d) follows from the change of variable xk → −xk for all k ∈ ⋃m+1r=s+1 Vr, (e) follows from the change of variable xk → −xk for all k ∈ ⋃s r=1 Vr, and (f) follows from the change of variable xk → −xk for all k ∈ ⋃m+1 r=1 Vr. Substituting (46)–(51) into (42) and (43) completes the proof of (29).\nTo prove (30), assume that m ≥ 3 is odd. When s is even, we have\np−1,s,s+1,m+1(+1,+1,+1,+1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,+1)\nσ ( − ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n(g) =\n∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,−1)\nσ ( − β{s,s+1} + ∑\n{k,l}∈I\\{{s,s+1}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,−1)\nζ−{s,s+1}(xV ), (52)\np−1,s,s+1,m+1(+1,+1,−1,+1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,−1,+1)\nσ ( − ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n(h) =\n∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,+1)\nσ ( β{s,s+1} + ∑\n{k,l}∈I\\{{s,s+1}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,+1)\nζ+ {s,s+1} (xV ), (53)\np−1,s,s+1,m+1(+1,−1,+1,+1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,−1,+1,+1)\nσ ( − ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n(i) =\n∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,−1)\nσ ( β{s,s+1} + ∑\n{k,l}∈I\\{{s,s+1}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,−1)\nζ+{s,s+1}(xV ), (54)\np−1,s,s+1,m+1(+1,−1,−1,+1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,−1,−1,+1)\nσ ( − ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n(j) =\n∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,+1)\nσ ( − β{s,s+1} + ∑\n{k,l}∈I\\{{s,s+1}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,+1)\nζ− {s,s+1} (xV ), (55)\nwhere (g) follows from the change of variable xk → −xk for all k ∈ V oddr for r ∈ {2, 4, . . . , s, s + 1, s + 3, . . . ,m} or k ∈ V evenr for r ∈ {1, 3, . . . , s − 1, s+ 2, s+ 4, . . . ,m + 1, m+ 2, . . . , t}, (h) follows from the change of variable xk → −xk for all k ∈ V oddr for r ∈ {2, 4, . . . ,m + 1,m + 2, . . . , t} or k ∈ V evenr for r ∈ {1, 3, . . . ,m}, (i) follows from the change of variable xk → −xk for all k ∈ V oddr for r ∈ {1, 3, . . . ,m} or k ∈ V evenr for r ∈ {2, 4, . . . ,m + 1,m + 2, . . . , t}, and (j) follows from the change of variable xk → −xk for all k ∈ V oddr for r ∈ {1, 3, . . . , s− 1, s+ 2, s+ 4, . . . ,m+ 1,m+ 2, . . . , t} or k ∈ V evenr for r ∈ {2, 4, . . . , s, s+ 1, s+ 3 . . . ,m}. Substituting (45)-(48) and (52)-(55) into (42) and (44) completes the proof of (30) when s is even.\nWhen s is odd, we have\np−1,s,s+1,m+1(+1,+1,+1,+1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,+1)\nσ ( − ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n(k) =\n∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,+1)\nσ ( − β{s,s+1} + ∑\n{k,l}∈I\\{{s,s+1}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,−1)\nζ−{s,s+1}(xV ), (56)\np−1,s,s+1,m+1(+1,+1,−1,+1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,−1,+1)\nσ ( − ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n(l) =\n∑\nxV :(x1,xs,xs+1,xm+1)=(+1,+1,+1,−1)\nσ ( β{s,s+1} + ∑\n{k,l}∈I\\{{s,s+1}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,−1)\nζ+ {s,s+1} (xV ), (57)\np−1,s,s+1,m+1(+1,−1,+1,+1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,−1,+1,+1)\n15\nσ ( − ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n(m) =\n∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,+1)\nσ ( β{s,s+1} + ∑\n{k,l}∈I\\{{s,s+1}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,+1)\nζ+ {s,s+1} (xV ), (58)\np−1,s,s+1,m+1(+1,−1,−1,+1)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(+1,−1,−1,+1)\nσ ( − ∑\n{k,l}∈I\nβ{k,l}xkxl\n)\n(n) =\n∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,−1)\nσ ( − β{s,s+1} + ∑\n{k,l}∈I\\{{s,s+1}}\nβ{k,l}xkxl\n)\n= ∑\nxV :(x1,xs,xs+1,xm+1)=(−1,+1,+1,−1)\nζ−{s,s+1}(xV ), (59)\nwhere (k) follows from the change of variable xk → −xk for all k ∈ V oddr for r ∈ {1, 3, . . . , s, s + 1, s + 3, . . . ,m + 1,m + 2, . . . , t} or k ∈ V evenr for r ∈ {2, 4, . . . , s−1, s+2, s+4, . . . ,m}, (l) follows from the change of variable xk → −xk for all k ∈ V oddr for r ∈ {1, 3, . . . ,m} or k ∈ V evenr for r ∈ {2, 4, . . . , m+1,m+2, . . . , t}, (m) follows from the change of variable xk → −xk for all k ∈ V oddr for r ∈ {2, 4, . . . ,m+ 1, m + 2, . . . , t} or k ∈ V evenr for r ∈ {1, 3, . . . ,m}, and (n) follows from the change of variable xk → −xk for all k ∈ V oddr for r ∈ {2, 4, . . . , s − 1, s + 2, s + 4, . . . ,m} or k ∈ V evenr for r ∈ {1, 3, . . . , s, s+1, s+3, . . . ,m+1, m+2, . . . , t}. Substituting (45)-(48) and (56)-(59) into (42) and (44) completes the proof of (30) when s is odd."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Abstract—An important problem in the field of bioinformatics is to identify interactive effects among profiled variables for outcome prediction. In this paper, a logistic regression model with pairwise interactions among a set of binary covariates is considered. Modeling the structure of the interactions by a graph, our goal is to recover the interaction graph from independently identically distributed (i.i.d.) samples of the covariates and the outcome. When viewed as a feature selection problem, a simple quantity called influence is proposed as a measure of the marginal effects of the interaction terms on the outcome. For the case when the underlying interaction graph is known to be acyclic, it is shown that a simple algorithm that is based on a maximum-weight spanning tree with respect to the plug-in estimates of the influences not only has strong theoretical performance guarantees, but can also outperform generic feature selection algorithms for recovering the interaction graph from i.i.d. samples of the covariates and the outcome. Our results can also be extended to the model that includes both individual effects and pairwise interactions via the help of an auxiliary covariate.",
    "creator" : "LaTeX with hyperref package"
  }
}