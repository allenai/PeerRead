{
  "name" : "1605.03498.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DEEP NEURAL NETWORKS UNDER STRESS",
    "authors" : [ "Micael Carvalho", "Matthieu Cord", "Sandra Avila", "Nicolas Thome", "Eduardo Valle" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— feature robustness, deep learning, transfer learning, image classification, feature compression"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Deep Convolutional Neural Networks have swept the Computer Vision community, with state-of-the-art performance for many tasks [1, 2, 3]. However, an analytical understanding of their models is still lacking, shrouding their use under a cloud of ad hoc procedures — tricks of the trade — without which they simply fail to work. Therefore, a full understanding of deep representations became the new Holy Grail of research in Machine Learning and Computer Vision [4, 5].\nWe explore here the properties of Deep Networks, measuring to which extent they preserve discriminative information about the input, i.e., measuring the robustness of the feature vectors they generate. Indeed, we may understand a deep model as one that first learns to extract a good representation (feature extraction step) and then uses that representation to make a decision (classification or regression step). Most of the challenge in understanding deep models is due to the unknown nature of the learned features.\nPursuing that understanding, we use transfer learning and “stress” tests to probe the networks. Transfer learning consists in recycling knowledge from one model to another, in the form of model weights, initialization, or architecture (e.g.\nc©2016 IEEE — MANUSCRIPT ACCEPTED AT ICIP 2016.\n[6, 7, 8, 9]), saving both computational resources and training data. Transfer learning is often used, with great success, on deep models, which are very greedy in terms of data and processing power. A straightforward scheme is to choose a pre-trained network, freeze the weights up to a certain layer, and to introduce and train new layers for the new task. By picking different layers from the original network, one controls the degree of transfer between the models. Conceptually, the output of the frozen transferred layers for any image may be seen as a feature vector x. Thus, any classifier, like SVM, may be used for classification on a target dataset.\nIn this paper, we propose stress tests, represented in Figure 1, which consistently interfere in the network to selectively destroy information. We explore two important aspects of deep architectures: dimensionality and numerical precision of their representations. Dimensionality stress tests introduce T : Rn → Rp, with p smaller than the original dimensionality n. Quantization stress tests introduce T : Rn → Qn where Q is a more aggressively quantized subset of real numbers than R. We also combine the two stresses.\nAlthough recent studies reevaluate deep architectures with respect to the size and precision of their representations (e.g. [10, 11, 12, 13]), their primary focus are practical impacts upon the original tasks. Our framework is designed for transfer learning tasks and, as we try to shed light on general properties of the networks, we will see that they show a strong degree of redundancy, opening the opportunity to create powerful compact descriptors.\nar X\niv :1\n60 5.\n03 49\n8v 2\n[ cs\n.C V\n] 2\n3 M\nay 2\n01 6"
    }, {
      "heading" : "2. TRANSFER STRATEGIES",
      "text" : "Our main objective is to explore the VGG-M deep convolutional model [14], which was originally trained on ImageNet, in a transfer scheme for the classification task of the Pascal VOC 2007 dataset [15]. We perform extensive experiments to study the robustness of this architecture, detailed in Table 1, against different types of stresses.\nLet us formalize the pre-trained deep model as a series of functions φi : Rmi → Rni , where φi is the ith layer of the network, m1 is equal to the dimensionality of the input data and ni = mi+1 is the output of such layer.\nIn our stress tests, we choose a layer i up to which we freeze the network (i.e., we keep layers φ1...φi untouched). At first, we use the output of layer φi to train an SVM. Then, we pick a stressing function T and retrain the model using T (φi) as input. Comparing the two scores, we can infer the network’s resiliency to the chosen stress.\nTo better highlight inherent properties of deep models, instead of specific characteristics of VGG-M [14], we also evaluate part of our experiments with GoogLeNet [16]. Furthermore, in order to differentiate these deep models from classical approaches, we also report comparative results with the recent Bag-of-Words (BoW) model’s BossaNova [17]. In all cases, we pre-process the images according to each model’s recommended protocol.\nWe also explore how to extend the results obtained for Pascal VOC 2007 by comparing part of the experiments with two other datasets: MIT-67 – Indoor [18] and UPMC Food101 [19] (67 and 101 classes, respectively)."
    }, {
      "heading" : "2.1. Dimensionality Reduction (DR)",
      "text" : "In order to understand how redundant is the deep representation, the first stress tests drop dimensions from the feature vector. The number of dimensions pi preserved at each step 1 ≤ i ≤ 20 is proportional to the initial size n of the feature vector, according to the expression pi = b n ∗ (21− i)\n20 c.\nWe contrast two strategies for selecting the pi−1 − pi dimensions dropped at each step i: TDR-1 drops randomly; and TDR-2 uses a PCA-based strategy. The latter discards the dimensions encoding less variance. To take in consideration the random choice in DR-1, we repeat the experiment 10 times."
    }, {
      "heading" : "2.2. Quantization (Q)",
      "text" : "The other stressor diminishes the numerical precision of the representation, quantizing the feature vectors. Our objective is not to explore advanced quantization strategies here, but to consider 2 fast and simple scalar quantizations and to analyze their effect on a classification task. In our first one, Q-1, all dimensions are quantized in the same h ∈ [1, 30] regular intervals, using the minimum (min) and maximum (max) scalar values observed in the training set for all dimensions.\nIn our second one, Q-2, we adapt the limits for each dimension individually, according, again, to values observed in the training set.\nFormally, Q-1, using the global step st = max−minh , has a single dictionaryH, generated by\nH = {(min+ st 2 ) + st ∗ i | 0 ≤ i < h}\nFor Q-2, let x be the feature matrix of the training feature vectors, and xt the tth element from all the vectors. Using one step stt = max(xt)−min(xt) h per dimension, Q-2 has n (number of dimensions) dictionaries, generated by\nHt = {(min(xt) + stt 2 ) + stt ∗ i | 0 ≤ i < h}\nFinally, in the quantization step, we assign to each element the value of the closest point in the dictionary. For Q-1 and Q-2, respectively, this is defined by:\nTQ-1(xij) = argmin y\n{abs(xij − y) | y ∈ H}\nTQ-2(xij) = argmin y\n{abs(xij − y) | y ∈ Hj}"
    }, {
      "heading" : "2.3. Feature Compression (FC)",
      "text" : "The final experiment FC, applies both stressors DR-2 and Q-2 simultaneously, dropping dimensions of the feature vector and quantizing the values of the remaining elements. Our goal is to measure any cross-effects between DR-2 and Q-2."
    }, {
      "heading" : "3. EXPERIMENTS",
      "text" : "As explained, for a given experimental point, we freeze a pretrained network at layer φi, discarding all upper layers. We then pick a stressing function T , and use the output of T (φi) as a feature vector in a transfer learning classification task. We `2-normalize those feature vectors, and feed them to a linear SVM model1 [21], measuring the model’s scores for dif-\n1For all setups, we use a regularization parameter C = 1; preliminary experiments shown very little variation when the C was cross-validated.\nferent choices of T . By picking stressing functions of different kinds and intensities (including the identity T (x) = x) we gain insight on the resiliency of deep models to those stresses.\nFor all our experiments, we report the classification scores in Mean Average Precision (mAP) for Pascal VOC 2007, and Accuracy (Acc) for Food-101 and MIT-67, following literature tradition on those datasets. Although we have tested the deep networks extensively, due to space constraints, we only report the experiments with layer 19 for the VGG-M, and with layer 151 for GoogLeNet. Those results are representative of our observations throughout the networks.\nTable 2 shows the scores for our vanilla experiments, using setups without perturbating the feature vectors (i.e., T (x) = x). We simplify BossaNova’s pipeline for Pascal VOC 2007, disabling the concatenation with the classic Bag of Visual Words, and using a linear SVM instead of the recommended RBF kernel.\nThe results for our dimensionality reduction (DR) experiments in the Pascal VOC 2007 dataset are shown in Figure 2. Strong redundancy on the representations is detected, since across runs, small variations in the score were observed. GoogLeNet was the most robust against random dimension-\nality perturbation DR-1, with an average mAP drop of 4.74% for 95% of the dimensions removed. However, GoogLeNet is, from start, 12-times bigger than CNN-M. Considering a direct comparison of descriptions of approximately the same size, the scores of the two models were equivalent.\nAlthough BossaNova has shown similar resiliency to dimensionality reduction, VGG-M held better scores for every test point, despite having feature vectors 15-times smaller (right side of Figure 2).\nThe PCA-based strategy was very effective for preserving information while dropping dimensions. DR-2 held 97.95% of the mAP with 95% of dimensions removed, while DR-1 could only keep 89.16% of the mAP. Choosing the right dimensions to drop improves the robustness of the feature vectors to dimensionality perturbations.\nThe number of classes in the target dataset also seems to play an important role on performance resiliency, as seen in Figure 3. For correctly classifying the data, diverse datasets may need complementary feature points, which can be lost with dimensionality reduction.\nOur quantization (Q) experiments, on the other hand, reduce the size of the feature vectors, from initial 32 ∗mi bits2, by aggressively limiting their values. Q-2 performed better than Q-1, indicating that adaptiveness to scale plays an important role (Figure 4).\nFurthermore, Q-1 kept vanilla scores with 7 values, while Q-2 only needed 4. That represents a strong compression of the feature vectors, from 32 ∗m to dlog2 4e ∗m bits.\nThe main results for DR and Q for VOC 2007 are summarized in Table 3, where each column indicates the maximum desired loss with respect to the original score for an experiment, while the cells indicate the minimum value which satisfies such requirement. For example, the second line of the second column reveals that with only 10% of the dimensions preserved, GoogLeNet score drops less than 2%.\n2For 32-bit single-precision floating-point numbers.\nFinally, the results for FC with the base setup are shown in Figure 5. The flat region on the top represents combinations of parameters from DR-2 and Q-2 with complementary characteristics, indicating that the features can be compressed in terms of dimension and precision at the same time. We point, with the circle, square and cross markers, specific combinations of DR-2 and Q-2 with compression rates of 99.1%, 98.4% and 96.9%, respectively, while maintaining 97.8%, 99.1% and 99.6% of the original score.\nSupplementary results and resources, including the source code for our experiments, are available online3."
    }, {
      "heading" : "4. DISCUSSION",
      "text" : "In this paper we evaluated the robustness of deep representations by introducing perturbations to feature vectors extracted from upper layers of deep networks. We explored in depth the resiliency of features transferred from the VGG-M model to the Pascal VOC 2007 dataset. Our findings show that there is a high level of redundancy in deep representations, and thus, they may be heavily compressed. In our experiments, we achieve a compression rate of 98.4%, while losing only\n3https://github.com/MicaelCarvalho/ DNNsUnderStress\n0.88% of the original score for Pascal VOC 2007. To ensure our conclusions are not dataset- nor model-specific, our two main approaches – Dimensionality Reduction and Quantization – were extensively tested, with supplementary results for MIT-67, Food-101, GoogLeNet and BossaNova. Furthermore, we observed that despite being more compact, deep architectures are also more robust to perturbations, when compared to approaches based on Bags of Visual Words. Those findings are specially useful for image retrieval and metric learning [22], in which the size of the feature vector is crucial to achieve fast response times, and for applications involving portable devices or remote classification, in which data must be efficiently transferred over the network."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "This research was partially supported by CNPq, Santander and Samsung Eletrônica da Amazônia Ltda., in the framework of law No. 8,248/91. We also thank CENAPAD-SP (Project 533), Microsoft Azure and Amazon Web Services for computational resources and Michel Fornaciali for his valuable advices."
    }, {
      "heading" : "5. REFERENCES",
      "text" : "[1] A. Krizhevsky, I. Sutskever, and G. Hinton, “ImageNet classification with deep convolutional neural networks,” Advances in Neural Information Processing Systems (NIPS), pp. 1–9, 2012.\n[2] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” CoRR, vol. abs/1512.03385, 2015.\n[3] Thibaut Durand, Nicolas Thome, and Matthieu Cord, “WELDON: Weakly Supervised Learning of Deep Convolutional Neural Networks,” in Computer Vision and Pattern Recognition (CVPR), 2016.\n[4] J. Bruna and S. Mallat, “Invariant scattering convolution networks,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 35, no. 8, pp. 1872– 1886, 2013.\n[5] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, no. 7553, pp. 436–444, 2015.\n[6] A. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, “CNN features off-the-shelf : an astounding baseline for recognition,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.\n[7] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are features in deep neural networks?,” in Advances in Neural Information Processing Systems (NIPS), 2014, pp. 3320–3328.\n[8] M. Chevalier, N. Thome, M. Cord, J. Fournier, G. Henaff, and E. Dusch, “LR-CNN for fine-grained classification with varying resolution,” 2015 IEEE International Conference on Image Processing (ICIP), Sep 2015.\n[9] Thibaut Durand, Nicolas Thome, and Matthieu Cord, “MANTRA: Minimum Maximum Latent Structural SVM for Image Classification and Ranking,” in International Conference on Computer Vision (ICCV), 2015.\n[10] V. Vanhoucke, A. Senior, and M. Mao, “Improving the speed of neural networks on CPUs,” in Advances in Neural Information Processing Systems (NIPS), 2011, pp. 1–8.\n[11] M. Courbariaux, Y. Bengio, and J.-P. David, “Training deep neural networks with low precision multiplications,” in International Conference on Learning Representations (ICLR), 2015.\n[12] M. Courbariaux, Y. Bengio, and J.-P. David, “BinaryConnect: Training deep neural networks with binary weights during propagations,” in Advances in Neural Information Processing Systems (NIPS), 2015.\n[13] P. Judd, J. Albericio, T. Hetherington, T. Aamodt, N. Jerger, R. Urtasun, and A. Moshovos, “Reduced-precision strategies for bounded memory in deep neural nets,” CoRR, vol. abs/1511.05236, 2015.\n[14] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of the devil in the details: Delving deep into convolutional nets,” in British Machine Vision Conference (BMVC), 2014.\n[15] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman, “The Pascal Visual Object Classes (VOC) Challenge,” International Journal of Computer Vision (IJCV), vol. 88, no. 2, pp. 303–338, 2010.\n[16] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1–9.\n[17] S. Avila, N. Thome, M. Cord, E. Valle, and A. De A. Araújo, “Pooling in image representation: The visual codeword point of view,” Computer Vision and Image Understanding (CVIU), vol. 117, no. 5, pp. 453–465, 2013.\n[18] A. Quattoni and A. Torralba, “Recognizing indoor scenes,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009, pp. 413–420.\n[19] X. Wang, D. Kumar, N. Thome, M. Cord, and F. Precioso, “Recipe recognition with large multimodal food dataset,” in IEEE International Conference on Multimedia & Expo (ICME), 2015, pp. 1–6.\n[20] A. Vedaldi and K. Lenc, “MatConvNet – Convolutional neural networks for MATLAB,” in ACM International Conference on Multimedia (MM), 2015.\n[21] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, “LIBLINEAR: A library for large linear classification,” Journal of Machine Learning Research (JMLR), vol. 9, pp. 1871–187, 2008.\n[22] C. Le Barz, N. Thome, M. Cord, S. Herbin, and M. Sanfourche, “Exemplar based metric learning for robust visual localization,” 2015 IEEE International Conference on Image Processing (ICIP), Sep 2015."
    } ],
    "references" : [ {
      "title" : "ImageNet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), pp. 1–9, 2012.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "CoRR, vol. abs/1512.03385, 2015.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "WELDON: Weakly Supervised Learning of Deep Convolutional Neural Networks",
      "author" : [ "Thibaut Durand", "Nicolas Thome", "Matthieu Cord" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2016.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Invariant scattering convolution networks",
      "author" : [ "J. Bruna", "S. Mallat" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol. 35, no. 8, pp. 1872– 1886, 2013.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1872
    }, {
      "title" : "Deep learning",
      "author" : [ "Y. LeCun", "Y. Bengio", "G. Hinton" ],
      "venue" : "Nature, vol. 521, no. 7553, pp. 436–444, 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "CNN features off-the-shelf : an astounding baseline for recognition",
      "author" : [ "A. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "How transferable are features in deep neural networks",
      "author" : [ "J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), 2014, pp. 3320–3328.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "LR-CNN for fine-grained classification with varying resolution",
      "author" : [ "M. Chevalier", "N. Thome", "M. Cord", "J. Fournier", "G. Henaff", "E. Dusch" ],
      "venue" : "2015 IEEE International Conference on Image Processing (ICIP), Sep 2015.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "MANTRA: Minimum Maximum Latent Structural SVM for Image Classification and Ranking",
      "author" : [ "Thibaut Durand", "Nicolas Thome", "Matthieu Cord" ],
      "venue" : "International Conference on Computer Vision (ICCV), 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Improving the speed of neural networks on CPUs",
      "author" : [ "V. Vanhoucke", "A. Senior", "M. Mao" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), 2011, pp. 1–8.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Training deep neural networks with low precision multiplications",
      "author" : [ "M. Courbariaux", "Y. Bengio", "J.-P. David" ],
      "venue" : "International Conference on Learning Representations (ICLR), 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Binary- Connect: Training deep neural networks with binary weights during propagations",
      "author" : [ "M. Courbariaux", "Y. Bengio", "J.-P. David" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reduced-precision strategies for bounded memory in deep neural nets",
      "author" : [ "P. Judd", "J. Albericio", "T. Hetherington", "T. Aamodt", "N. Jerger", "R. Urtasun", "A. Moshovos" ],
      "venue" : "CoRR, vol. abs/1511.05236, 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Return of the devil in the details: Delving deep into convolutional nets",
      "author" : [ "K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "British Machine Vision Conference (BMVC), 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The Pascal Visual Object Classes (VOC) Challenge",
      "author" : [ "M. Everingham", "L. Van Gool", "C. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : "International Journal of Computer Vision (IJCV), vol. 88, no. 2, pp. 303–338, 2010.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1–9.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Pooling in image representation: The visual codeword point of view",
      "author" : [ "S. Avila", "N. Thome", "M. Cord", "E. Valle", "A. De A. Araújo" ],
      "venue" : "Computer Vision and Image Understanding (CVIU), vol. 117, no. 5, pp. 453–465, 2013.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Recognizing indoor scenes",
      "author" : [ "A. Quattoni", "A. Torralba" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009, pp. 413–420.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Recipe recognition with large multimodal food dataset",
      "author" : [ "X. Wang", "D. Kumar", "N. Thome", "M. Cord", "F. Precioso" ],
      "venue" : "IEEE International Conference on Multimedia & Expo (ICME), 2015, pp. 1–6.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "MatConvNet – Convolutional neural networks for MATLAB",
      "author" : [ "A. Vedaldi", "K. Lenc" ],
      "venue" : "ACM International Conference on Multimedia (MM), 2015.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "LIBLINEAR: A library for large linear classification",
      "author" : [ "R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin" ],
      "venue" : "Journal of Machine Learning Research (JMLR), vol. 9, pp. 1871–187, 2008.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1871
    }, {
      "title" : "Exemplar based metric learning for robust visual localization",
      "author" : [ "C. Le Barz", "N. Thome", "M. Cord", "S. Herbin", "M. Sanfourche" ],
      "venue" : "2015 IEEE International Conference on Image Processing (ICIP), Sep 2015.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Deep Convolutional Neural Networks have swept the Computer Vision community, with state-of-the-art performance for many tasks [1, 2, 3].",
      "startOffset" : 126,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "Deep Convolutional Neural Networks have swept the Computer Vision community, with state-of-the-art performance for many tasks [1, 2, 3].",
      "startOffset" : 126,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : "Deep Convolutional Neural Networks have swept the Computer Vision community, with state-of-the-art performance for many tasks [1, 2, 3].",
      "startOffset" : 126,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "Therefore, a full understanding of deep representations became the new Holy Grail of research in Machine Learning and Computer Vision [4, 5].",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 4,
      "context" : "Therefore, a full understanding of deep representations became the new Holy Grail of research in Machine Learning and Computer Vision [4, 5].",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 5,
      "context" : "[6, 7, 8, 9]), saving both computational resources and training data.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 6,
      "context" : "[6, 7, 8, 9]), saving both computational resources and training data.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 7,
      "context" : "[6, 7, 8, 9]), saving both computational resources and training data.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 8,
      "context" : "[6, 7, 8, 9]), saving both computational resources and training data.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 9,
      "context" : "[10, 11, 12, 13]), their primary focus are practical impacts upon the original tasks.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : "[10, 11, 12, 13]), their primary focus are practical impacts upon the original tasks.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 11,
      "context" : "[10, 11, 12, 13]), their primary focus are practical impacts upon the original tasks.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 12,
      "context" : "[10, 11, 12, 13]), their primary focus are practical impacts upon the original tasks.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 13,
      "context" : "Our main objective is to explore the VGG-M deep convolutional model [14], which was originally trained on ImageNet, in a transfer scheme for the classification task of the Pascal VOC 2007 dataset [15].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "Our main objective is to explore the VGG-M deep convolutional model [14], which was originally trained on ImageNet, in a transfer scheme for the classification task of the Pascal VOC 2007 dataset [15].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 13,
      "context" : "To better highlight inherent properties of deep models, instead of specific characteristics of VGG-M [14], we also evaluate part of our experiments with GoogLeNet [16].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "To better highlight inherent properties of deep models, instead of specific characteristics of VGG-M [14], we also evaluate part of our experiments with GoogLeNet [16].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 16,
      "context" : "Furthermore, in order to differentiate these deep models from classical approaches, we also report comparative results with the recent Bag-of-Words (BoW) model’s BossaNova [17].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 17,
      "context" : "We also explore how to extend the results obtained for Pascal VOC 2007 by comparing part of the experiments with two other datasets: MIT-67 – Indoor [18] and UPMC Food101 [19] (67 and 101 classes, respectively).",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 18,
      "context" : "We also explore how to extend the results obtained for Pascal VOC 2007 by comparing part of the experiments with two other datasets: MIT-67 – Indoor [18] and UPMC Food101 [19] (67 and 101 classes, respectively).",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "In our first one, Q-1, all dimensions are quantized in the same h ∈ [1, 30] regular intervals, using the minimum (min) and maximum (max) scalar values observed in the training set for all dimensions.",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "Description of layers (L) and groups (G) of the VGG-M model, from the MatConvNet toolbox [20], proposed by Chatfield et al.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "[14].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "We `2-normalize those feature vectors, and feed them to a linear SVM model1 [21], measuring the model’s scores for dif-",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : "Those findings are specially useful for image retrieval and metric learning [22], in which the size of the feature vector is crucial to achieve fast response times, and for applications involving portable devices or remote classification, in which data must be efficiently transferred over the network.",
      "startOffset" : 76,
      "endOffset" : 80
    } ],
    "year" : 2016,
    "abstractText" : "In recent years, deep architectures have been used for transfer learning with state-of-the-art performance in many datasets. The properties of their features remain, however, largely unstudied under the transfer perspective. In this work, we present an extensive analysis of the resiliency of feature vectors extracted from deep models, with special focus on the trade-off between performance and compression rate. By introducing perturbations to image descriptions extracted from a deep convolutional neural network, we change their precision and number of dimensions, measuring how it affects the final score. We show that deep features are more robust to these disturbances when compared to classical approaches, achieving a compression rate of 98.4%, while losing only 0.88% of their original score for Pascal VOC 2007.",
    "creator" : "LaTeX with hyperref package"
  }
}