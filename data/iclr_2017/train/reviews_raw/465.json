{"conference": "ICLR 2017 conference submission", "title": "Delving into Transferable Adversarial Examples and Black-box Attacks", "abstract": "An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.", "reviews": [{"is_meta_review": true, "comments": "I reviewed the manuscript as of December 7th.\n\nSummary:\nThe authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label. The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples.\n\nMajor Comments:\n1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed.\n\n2. The section of geometric understanding is similar to results of 'Adversarial Perturbations of Deep Neural Networks' in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them.\n\n3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult.\n\n4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016). \n\nAs far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label. The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work.\n\nAreas to Trim the Paper:\n- Table 1 is not necessary. Just cite other results or write the Top-1 numbers in the text.\n- Condense Section 2.2.1 and cite heavily.\n- Figure 2 panels may be overlaid to highlight a comparison.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The paper is the first to demonstrate that it is possible for an adversary to change the label that a convolutional network predicts for an image to a specific value. Like Papernot et al., it presents a successful attack on Clarifai's image-recognition system. I encourage the authors to condense the paper to its key results (13 pages without / 24 pages with supplemental material is too long for a conference paper).", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "A revision of the paper has been updated", "OTHER_KEYS": "Chang Liu", "comments": "We have updated the paper with the following changes:\n\n1) We shrink the paper in the following aspects:\n   a) Sec 2.2 is condensed\n   b) We remove the original Sec 2.3, which introduces transferability and black-box attack, since all materials have been covered in Sec 1\n   c) The original Table 1 is moved to the appendix Table 7\n   d) The original Table 2 and Table 4 are now merged as two panels of Table 1, so that only one caption needs be provided.\n   e) The alternative approach to generate non-targeted adversarial images is moved to the appendix\n   f) The paragraph discussing different models make the same mistake in original Sec 3.1 is moved to the appendix\n   g) The paragraph discussing that adversarial images may come from multiple intervals along the gradient direction is moved to the appendix\n   h) The results for random perturbation in original Sec 3.2 is moved to the appendix.\n   i) We move the original Table 9 in Section 6 about cosine values between pairs of gradients to the appendix Table 33\n\n2) We provide a contribution and organization paragraph in Section 1 to highlight the the main conclusions of this work and to facilitate readers to follow the arguments in the paper to support our conclusions.\n\n3) In related work, we highlight the difference between our work and Papernot et al (2016ab) by explaining why our black-box attack is harder. Also, we discuss Fawzi et al (2016) at the beginning of Section 6.\n\n4) We provide justification of why we choose three ResNet models in Section 2.3, and highlight the findings about the transferability between both homogeneous architectures (i.e., ResNet models) and heterogeneous architectures in various places (i.e., Section 3.1, Section 5 and Section 6).\n\nWe welcome new comments!", "IS_META_REVIEW": false, "DATE": "14 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "good in-depth exploration but strongly recommend a rewrite", "is_meta_review": false, "comments": "The paper presents an interesting and very detailed study of targeted and non-targeted adversarial examples in CNNs.\n\nI\u2019m on the fence about this paper but am leaning towards acceptance. Such detailed empirical explorations are difficult and time-consuming to construct yet can serve as important stepping stones for future work. I see the length of the paper as a strength since it allows for a very in-depth look into the effectiveness and transferability of different kinds of adversarial examples.  \n\nThere are, however, some concerns:\n\n1) While the length of the paper is a strength in my mind, the key contributions should be made much more clear. As evidenced by my comment earlier, I got confused at some point between the ensemble/non-ensemble method, and about the contribution of the Clarifai evaluation and what I should be focusing on where. I\u2019d strongly suggest a radical revision which more clearly focuses the story: \n\n- First, we demonstrate that non-targeted attacks are easy while targeted attacks are hard (evidenced by a key experiment comparing the two; we refer to appendix or later sections for the extensive exploration of e.g., current Section 3)\n\n- Thus, we propose an ensemble method that is able to handle targeted attacks much better (evidenced by experiments focusing on the comparison between ensemble and non-ensemble method, both in a controlled setting and on Clarifai)\n\n- Also, here are all the other details and explorations. \n\n2) Instead of using ResNet-152, Res-Net-101 and ResNet-50 as three of the five models, it would've been better to use one ResNet architecture and the other two, say, AlexNet and Network-in-Network. This would make the ensemble results a lot more compelling.", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "interesting and insightful work on adversarial examples for deep CNNs for image classification", "is_meta_review": false, "comments": "This paper present an experimental study of the robustness of state-of-the-art CNNs to different types of \"attacks\" in the context of image classication. Specifically, an attack aims to fool the classification system with a specially corrupted image, i.e. making it misclassify the image as (1) any wrong class (non-targeted attack) or (2) a target class, chosen in advance by the attacker (targeted attack). For instance, the attacker could corrupt an image of an ostrich in such a way that it would be classified as a megalith. Even though the attacker's agenda is not so clear in this example, it is still interesting to study the weaknesses of current systems in view of (1) improving them in general and (2) actual risks with e.g. autonomous vehicles.\n\nThe paper is mostly experimental. In short, it compares different strategies (already published in previous papers) for all popular networks  (VGG, GoogLeNet, ResNet-50/101/152) and the two aforementionned types of attacks. The experiments are well conducted and clearly exposed. A convincing point is that attacks are also conducted on \"clarifai.com\" which is a black-box classification system. Some analysis and insightful explanations are also provided to help understanding why CNNs are prone to such attacks (Section 6).\n\nTo sum up, the main findings are that non-targeted attacks are easy to perform, even on a black-box system. Non-targeted attacks are more difficult to realize with existing schemes, but the authors propose a new approach for that that vastly improves over existing attacks (even though it's still far from perfect: ~20% success rate on clarifai.com versus 2% with previous schemes).\n\nArguably, The paper still has some weaknesses:\n\n - The authors are treating the 3 ResNet-based networks as different, yet they are obviously clearly correlated. See Table 7 for instance. This is naturally expected because their architecture is similar (only their depth varies). Hence, it does not sound very fair to state that \"One interesting finding is that [...] the first misclassified label (non-targeted) is the same for all models except VGG-16 and GoogLeNet.\", i.e., the three ResNet-based networks.\n\n - A subjective measure is employed to evaluate the effectiveness of the attacks on the black box system. While this is for a good reason (clarifai.com returns image labels that are different from ImageNet), it is not certain that the reported numbers are fair (even though the qualitative results look convincing).\n\n - The novelty of the proposed approach (optimizing an ensemble of network instead of a single network) is limited. However, this was not really the point of the paper, and it is effective, so it seems ok overall.\n\n - The paper is quite long. This is expected because it is an extensive evaluation study, but still. I suggest the authors prune some near-duplicate content (e.g. Section 2.3 has a high overlap with Section 1, etc.).\n\n - The paper would benefit from additional discussions with the recent and related work of Fawzi et al (NIPS'16) in Section 6. Indeed the work of Fawzi et al. is mostly theoretical and well aligned with the experimental findings and observations (in particular in Section 6).\n\n\nTo conclude, I think that this paper is somewhat useful for the community and could help to further improve existing architectures, as well as better assess their flaws and weaknesses.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review for Liu et al", "is_meta_review": false, "comments": "I reviewed the manuscript as of December 7th.\n\nSummary:\nThe authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label. The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples.\n\nMajor Comments:\n1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed.\n\n2. The section of geometric understanding is similar to results of 'Adversarial Perturbations of Deep Neural Networks' in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them.\n\n3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult.\n\n4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016). \n\nAs far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label. The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work.\n\nAreas to Trim the Paper:\n- Table 1 is not necessary. Just cite other results or write the Top-1 numbers in the text.\n- Condense Section 2.2.1 and cite heavily.\n- Figure 2 panels may be overlaid to highlight a comparison.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "technical detail", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "In equation (7), why did you use f(x) rather than J(x)? Because f(x) = argmax J(x), f(x) is highly non-linear, so the gradient descent might be much more inefficient. Even tough optimizing with J rather than f is not exactly the same objective, the outcome may be very similar. Did you try this, or di I misunderstood something?", "IS_META_REVIEW": false, "DATE": "09 Dec 2016", "is_meta_review": false}, {"TITLE": "commenting a related work", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "08 Dec 2016", "is_meta_review": false}, {"TITLE": "Paper length is too long", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "I am not sure if the authors have revised the paper since the first submission but the manuscript is now 16-24 pages long depending on what sections are artificially labeled as the Appendix.\n\nAlthough there is no strict page limit for this conference, this seems notably longer then the suggested limit of 8 pages mentioned here:\n\n", "IS_META_REVIEW": false, "DATE": "06 Dec 2016", "is_meta_review": false}, {"TITLE": "Reconciling conclusions of Table 5 and 10", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Mitigating transferability with label smoothing", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "01 Dec 2016", "is_meta_review": false}, {"TITLE": "comparison to random noise", "OTHER_KEYS": "(anonymous)", "comments": "It seems that the transferable perturbations reported in the paper are very perceptible. I believe the paper would be stronger if it had included comparisons to a standard baseline: random noise. To evaluate how special are the methods used in the paper in finding transferable adversarial examples (in the regime of large RMSD), one should compare the misclassification rate to that of random noise (sampled uniformly from a sphere with comparable RMSD). A quick experiment on Clarifai.com shows that, with RMSD comparable to the perturbations reported in the paper, one can also cause Clarifai.com to misclassify randomly perturbed images. See ", "IS_META_REVIEW": false, "DATE": "09 Nov 2016", "is_meta_review": false}], "SCORE": 5, "authors": "Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song", "accepted": true, "id": ""}
