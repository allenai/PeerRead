{
  "name" : "1412.6645.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LEARNING OF ACOUSTIC MODELS",
    "authors" : [ "Gabriel Synnaeve", "Emmanuel Dupoux" ],
    "emails" : [ "gabriel.synnaeve@gmail.com", "emmanuel.dupoux@gmail.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Theoretically, algorithms performing unsupervised or weakly supervised discovery of linguistic structure represent plausible models of language acquisition in the human infant (Vallabha et al., 2007). Practically, they can be put to use for low resource languages (Park & Glass, 2008).\nBuilding on the fact that infant can recognize some words (Bergelson & Swingley, 2012) and discriminate between speakers (Johnson et al., 2011) before they have constructed adult-like phoneme representations, we propose to test a neural network architecture where word and talker identity are used as side information to help learning an acoustic model (phone embedding). Previous work has shown that same-different side information can be used for metric learning (Xing et al., 2003), and Synnaeve et al. (2014) demonstrated that it can be used with Deep Neural Network (DNN) architecture for learning phone embeddings. Here, we extend this work using multi-task (word and talker identity) side information. As this paper is a feasibility study, we used gold same-different labels, and leave it to further work to derive them in an unsupervised fashion using spoken term discovery (Jansen et al., 2010) and talker diarization (Anguera Miro et al., 2012)."
    }, {
      "heading" : "2 MODEL",
      "text" : "We used the architure of a Siamese network (Bromley et al., 1993), as shown in Fig. 3.1. It is a duplicated feedforward neural network taking two inputs in parallel. Each of the inputs consists in 11 stacked frames of 40 coefficients log-compressed Mel-filterbanks. Each network contains 3 hidden layers of 500 units with sigmoid activations, and two output embeddings each of 100 dimensions. One of the embeddings is the one in which we compute the similarity between the two inputs according to the same/different “word type” indication, while the other looks at the same/different “speaker” indication. More formally:\nxA and xB ∈ R11×40 ; yA,W , yA,S , yB,W and yB,S ∈ R100\nThe loss function that we use (for two inputs xA and xB) is a simple sum of the COSCOS2 losses in each of the embeddings (see Synnaeve et al. (2014) for a comparison with other loss functions):\nL(A,B) = LW (A,B) + LS(A,B) with W ∈ {0, 1} (different or same word) and S ∈ {0, 1} (different or same speaker), both losses are similar (here for speakers):\nLS(A,B) = S × (1− cos(yA,S , yB,S)) + (1− S)× (cos2(yA,S , yB,S))\nar X\niv :1\n41 2.\n66 45\nv3 [\ncs .S\nD ]\n2 0\nA pr\n2 01"
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 DATASET",
      "text" : "We used about 1/3rd (12 speakers) of the Buckeye corpus1, on which we performed a dynamic timewarping (DTW) alignment of pairs of same words, in the features space (filterbanks), exactly as in (Synnaeve et al., 2014). This consisted in doing 76407 pairs of long “same” word (1057 types in total), said 1/4th of the time by the same speakers (we subsampled “same word and different speakers” pairs). During training, we also sample pairs of tokens coming from different words (often called negative sampling), with a ratio of pairs of same/different words of 1:1. This yields about 5M frames for pairs of same words and 4.3M frames for sampled pairs of different words."
    }, {
      "heading" : "3.2 RESULTS",
      "text" : "We trained the model with Adadelta (Zeiler, 2012), a variant of stochastic gradient descent with an adaptive learning rate method correcting the magnitude of the updates using an accumulation of past gradients (on a sliding window) and a local approximation of the Hessian. We used ρ = 0.95 (hyper-parameter on the momentum) and = 10−6 (precision of the updates), we performed early stopping on a small (10%) held-out development set. We compared three network setups. In the multi-task setup, we use the combined loss function incorporating both the word and the speaker losses. In the single-loss setup, we only use one of the losses (word or speaker), even though the topology of the network remains the same. This means that the weights of only one of the two embeddings is updated, the other remaining in their initial state, thereby implementing a random projection from the last hidden layer. As a control, we also trained a fully supervised DNN using dropout (Srivastava et al., 2014). It has 11 stacked filterbank frames as input, 4 hidden layers of 2400 units, and 46 phones as outputs of the logistic regression (with a 37.9% classification accuracy).\nFigure 3.1 shows the evolution of the cosine similarities for the different conditions, and shows that the training of the speaker task takes more time than the training of the phoneme task, even though the cosine similarities start off less favorably for the former than the latter. In both cases, the difference between the train and the dev sets shows that the network is not really overfitting.\nUnsupervised systems do not necessarily discover phoneme-like units. Therefore, evaluating them with a phone error rate may not be appropriate. Similarly, using them as front end for a word recognizer may not be straightforward using standard HMMs. Here, we follow the lead of Carlin et al. (2011) and Schatz et al. (2013) who propose to use instead a discrimination task, which makes no assumption about the shape of the coded categories (phone-like, Gaussian, linearly separable, etc.)\n1 http://buckeyecorpus.osu.edu\nand does not depend on the training of a classifier or a language model. The ABX discrimination task consists in computing two pairwise distances, between the token pair X and A, and between X and B and deciding which of them is larger. When A and B are tokens of different linguistic categories, and X belongs to the category of either A or B, this metric measures the degree of separation of the two categories A and B in the embedding. Here, we use as categories, minimal pairs of triphones of the shape /a/-/t/-/i/ vs /a/-/p/-/i/, where the left and right context phones are kept identical, and the center phone varies. As distance metric, we use the cosine distance along the DTW path. We setup two tasks, on which we will test our two embeddings:\n• phone.talker is a phoneme discrimination task across speakers. For instance, A=/a/-/p/-/i/, B=/a/-/t/-/i/, both being said by the same speaker, and X is phonetically identical to A or B, but is uttered by a different speaker.\n• talker.phone is a speaker discrimination task, across phonemes. For instance, both A and B share the same triphone (eg., /a/-/p/-/i/) but are said by different speakers; X is uttered by either the speaker of A or the speaker of B, but has different phonemes (eg., /a/-/t/-/i/).\nThe two tasks are mirror image of one another regarding the discrimination of the phonemes or of the talkers. In both cases, the context (left and right) phonemes are kept identical. To run this task, we select the set of all eligible ABX triplets of triphones in the dataset, and compute the aggregate ABX score by averaging across context, phoneme and speaker pairs."
    }, {
      "heading" : "3.3 DISCUSSION",
      "text" : "The ABX scores for the phoneme and speaker tasks are shown in Fig. 3.2. Globally, speaker discrimination seems easier to optimize than phoneme discrimination (even though it starts the other way around when evaluated from the filterbanks). This is probably due to the small number of speaker classes (N=12) compared to the number of phoneme classes (N=48). In addition, the multi-task network gives the best results across the two tasks, compared to single-task networks. Therefore, learning to do two tasks at once using the same network does not incur a decrease in performance, but on the contrary is slightly beneficiary (especially for the talker task). Interestingly the single-task networks behave asymmetrically with respect to the untrained task. Indeed, the performance on phone discrimination is worse for the network that was trained only on the speaker loss, compared to the filterbank performance. This makes sense: if you are trained to ignore phoneme identity, phoneme encoding should be progressively removed from the hidden layers of the network. But vice-versa\nthe performance on speaker discrimination is better for the phoneme-loss network compared to the filterbank base performance. This means that in order to determine speaker identity, it is actually useful for the network to code some information about the phonemes. This last result meshes well with the fact that speaker identification depends not so much on raw acoustic features, but on small deformations relative to a background pronunciation distribution (as encoded in i-vectors, Dehak et al. (2011)). Specialization on the task is even more extreme for the fully supervised DNN trained on phone labels: it gives us a higher bound on the phone accross talkers task (81.9% correct), and shows degraded talker accross phones score (54.8% correct) compared to the filterbank.\nIn order to understand the nature of information encoding in a multi-layer network, it can be useful to inspect the hidden layers in details (Mohamed et al., 2012). Here, we inspected each hidden unit by computing the ratio of between-class to within-class variance in unit activation over the entire corpus (F-test). To compute the phoneme variance ratio, we took the variance of the activation value of the unit across all (between) the phone categories versus within each phone category. We did a similar computation for the speakers categories. Intuitively, a unit with a large phoneme variance ratio is strongly encoding phoneme information; a unit with a small ratio is not very sensitive to that information. Similarly for speaker information. If we split the ratio distribution using the median, this gives rise to a typology of 4 kinds of units according to whether they respond strongly or not to either phone or speaker information. In Figure 3, we can see three phenomena regarding the coding of units in the three hidden layers. First, phone-coding units are predominant in the first layers, and progressively, more and more speaker-coding units appear. Second, the number of doubly-coding units diminishes. Third, the sparsity of the code increases (ie., the number of units not coding anything). Inspection of the task-specific embeddings is interesting, as it reveals an almost pure (and very sparse) coding of speaker identity in the speaker embedding. This is consistent with the high performance of speaker discrimination in this layer. In contrast, inspection of the phone embedding reveals a much less sparse coding and a predominance of doubly-used units. This is consistent with the rather low performance of phoneme discrimination in this layer, and suggests that further layers or more (speaker variability in) training examples would be necessary to “purge” this layer from speaker-specific effects. Finally, inspection of the filterbanks (here coded in shade of red and blue) shows that most of the lower frequency filterbanks are sensitive to phone infrmation (relatively localized in time to the center frames, as we compute it on phonetic annotation), whereas the higher frequency filterbanks are sensitive to speaker information (relatively not localized in time)."
    }, {
      "heading" : "4 CONCLUSION",
      "text" : "We have demonstrated that a Siamese network can perform both phoneme and speaker discrimination using only a moderate amount of side information (indication of same/different word or\nspeaker for only ≈1000 word types and 12 speakers). Further work is needed to study the effect of the amount of information, and whether the obtained speaker embeddings could replace or complement i-vectors. Finally, the phone embedding should be evaluated as a first step in a subsequent word recognizer or language model adapted for this kind of representation."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This project is funded in part by the European Research Council (ERC-2011-AdG-295810 BOOTPHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX0001-02 PSL*), the Fondation de France, the Ecole de Neurosciences de Paris, and the Region Ile de France (DIM cerveau et pense)."
    } ],
    "references" : [ {
      "title" : "Speaker diarization: A review of recent research",
      "author" : [ "Anguera Miro", "Xavier", "Bozonnet", "Simon", "Evans", "Nicholas", "Fredouille", "Corinne", "Friedland", "Gerald", "Vinyals", "Oriol" ],
      "venue" : "Audio, Speech, and Language Processing, IEEE Transactions on,",
      "citeRegEx" : "Miro et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Miro et al\\.",
      "year" : 2012
    }, {
      "title" : "At 6–9 months, human infants know the meanings of many common nouns",
      "author" : [ "Bergelson", "Elika", "Swingley", "Daniel" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Bergelson et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bergelson et al\\.",
      "year" : 2012
    }, {
      "title" : "Signature verification using a siamese time delay neural network",
      "author" : [ "Bromley", "Jane", "Bentz", "James W", "Bottou", "Léon", "Guyon", "Isabelle", "LeCun", "Yann", "Moore", "Cliff", "Säckinger", "Eduard", "Shah", "Roopak" ],
      "venue" : "Internat. Journ. of Pattern Recog. and Artific. Intell.,",
      "citeRegEx" : "Bromley et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Bromley et al\\.",
      "year" : 1993
    }, {
      "title" : "Rapid evaluation of speech representations for spoken term discovery",
      "author" : [ "Carlin", "Michael A", "Thomas", "Samuel", "Jansen", "Aren", "Hermansky", "Hynek" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Carlin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Carlin et al\\.",
      "year" : 2011
    }, {
      "title" : "Front-end factor analysis for speaker verification",
      "author" : [ "Dehak", "Najim", "Kenny", "Patrick", "Réda", "Dumouchel", "Pierre", "Ouellet" ],
      "venue" : "Audio, Speech, and Language Processing, IEEE Transactions on,",
      "citeRegEx" : "Dehak et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dehak et al\\.",
      "year" : 2011
    }, {
      "title" : "Towards spoken term discovery at scale with zero resources",
      "author" : [ "Jansen", "Aren", "Church", "Kenneth", "Hermansky", "Hynek" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Jansen et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jansen et al\\.",
      "year" : 2010
    }, {
      "title" : "Infant ability to tell voices apart rests on language experience",
      "author" : [ "Johnson", "Elizabeth K", "Westrek", "Ellen", "Nazzi", "Thierry", "Cutler", "Anne" ],
      "venue" : "Developmental Science,",
      "citeRegEx" : "Johnson et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2011
    }, {
      "title" : "Understanding how deep belief networks perform acoustic modelling",
      "author" : [ "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey", "Penn", "Gerald" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Mohamed et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mohamed et al\\.",
      "year" : 2012
    }, {
      "title" : "Unsupervised pattern discovery in speech",
      "author" : [ "Park", "Alex S", "Glass", "James R" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing,",
      "citeRegEx" : "Park et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2008
    }, {
      "title" : "Evaluating speech features with the minimal-pair abx task: Analysis of the classical mfc/plp pipeline",
      "author" : [ "Schatz", "Thomas", "Peddinti", "Vijayaditya", "Bach", "Francis", "Jansen", "Aren", "Hynek", "Hermansky", "Dupoux", "Emmanuel" ],
      "venue" : null,
      "citeRegEx" : "Schatz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Schatz et al\\.",
      "year" : 2013
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q1929\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 1929
    }, {
      "title" : "Phonetics embedding learning with side information",
      "author" : [ "Synnaeve", "Gabriel", "Schatz", "Thomas", "Dupoux", "Emmanuel" ],
      "venue" : "In IEEE SLT,",
      "citeRegEx" : "Synnaeve et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Synnaeve et al\\.",
      "year" : 2014
    }, {
      "title" : "Unsupervised learning of vowel categories from infant-directed speech",
      "author" : [ "Vallabha", "Gautam K", "McClelland", "James L", "Pons", "Ferran", "Werker", "Janet F", "Amano", "Shigeaki" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Vallabha et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Vallabha et al\\.",
      "year" : 2007
    }, {
      "title" : "Distance metric learning",
      "author" : [ "Eric P", "Ng", "Andrew Y", "Jordan", "Michael I", "Russell", "Stuart" ],
      "venue" : null,
      "citeRegEx" : "P et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "P et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Theoretically, algorithms performing unsupervised or weakly supervised discovery of linguistic structure represent plausible models of language acquisition in the human infant (Vallabha et al., 2007).",
      "startOffset" : 176,
      "endOffset" : 199
    }, {
      "referenceID" : 6,
      "context" : "Building on the fact that infant can recognize some words (Bergelson & Swingley, 2012) and discriminate between speakers (Johnson et al., 2011) before they have constructed adult-like phoneme representations, we propose to test a neural network architecture where word and talker identity are used as side information to help learning an acoustic model (phone embedding).",
      "startOffset" : 121,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "As this paper is a feasibility study, we used gold same-different labels, and leave it to further work to derive them in an unsupervised fashion using spoken term discovery (Jansen et al., 2010) and talker diarization (Anguera Miro et al.",
      "startOffset" : 173,
      "endOffset" : 194
    }, {
      "referenceID" : 4,
      "context" : "Building on the fact that infant can recognize some words (Bergelson & Swingley, 2012) and discriminate between speakers (Johnson et al., 2011) before they have constructed adult-like phoneme representations, we propose to test a neural network architecture where word and talker identity are used as side information to help learning an acoustic model (phone embedding). Previous work has shown that same-different side information can be used for metric learning (Xing et al., 2003), and Synnaeve et al. (2014) demonstrated that it can be used with Deep Neural Network (DNN) architecture for learning phone embeddings.",
      "startOffset" : 122,
      "endOffset" : 513
    }, {
      "referenceID" : 2,
      "context" : "We used the architure of a Siamese network (Bromley et al., 1993), as shown in Fig.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "We used the architure of a Siamese network (Bromley et al., 1993), as shown in Fig. 3.1. It is a duplicated feedforward neural network taking two inputs in parallel. Each of the inputs consists in 11 stacked frames of 40 coefficients log-compressed Mel-filterbanks. Each network contains 3 hidden layers of 500 units with sigmoid activations, and two output embeddings each of 100 dimensions. One of the embeddings is the one in which we compute the similarity between the two inputs according to the same/different “word type” indication, while the other looks at the same/different “speaker” indication. More formally: xA and xB ∈ R11×40 ; yA,W , yA,S , yB,W and yB,S ∈ R The loss function that we use (for two inputs xA and xB) is a simple sum of the COSCOS losses in each of the embeddings (see Synnaeve et al. (2014) for a comparison with other loss functions): L(A,B) = LW (A,B) + LS(A,B) with W ∈ {0, 1} (different or same word) and S ∈ {0, 1} (different or same speaker), both losses are similar (here for speakers): LS(A,B) = S × (1− cos(yA,S , yB,S)) + (1− S)× (cos(yA,S , yB,S))",
      "startOffset" : 44,
      "endOffset" : 822
    }, {
      "referenceID" : 11,
      "context" : "1 DATASET We used about 1/3rd (12 speakers) of the Buckeye corpus1, on which we performed a dynamic timewarping (DTW) alignment of pairs of same words, in the features space (filterbanks), exactly as in (Synnaeve et al., 2014).",
      "startOffset" : 203,
      "endOffset" : 226
    }, {
      "referenceID" : 3,
      "context" : "Here, we follow the lead of Carlin et al. (2011) and Schatz et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "Here, we follow the lead of Carlin et al. (2011) and Schatz et al. (2013) who propose to use instead a discrimination task, which makes no assumption about the shape of the coded categories (phone-like, Gaussian, linearly separable, etc.",
      "startOffset" : 28,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "This last result meshes well with the fact that speaker identification depends not so much on raw acoustic features, but on small deformations relative to a background pronunciation distribution (as encoded in i-vectors, Dehak et al. (2011)).",
      "startOffset" : 221,
      "endOffset" : 241
    }, {
      "referenceID" : 7,
      "context" : "In order to understand the nature of information encoding in a multi-layer network, it can be useful to inspect the hidden layers in details (Mohamed et al., 2012).",
      "startOffset" : 141,
      "endOffset" : 163
    } ],
    "year" : 2015,
    "abstractText" : "We trained a Siamese network with multi-task same/different information on a speech dataset, and found that it was possible to share a network for both tasks without a loss in performance. The first task was to discriminate between two same or different words, and the second was to discriminate between two same or different talkers.",
    "creator" : "LaTeX with hyperref package"
  }
}