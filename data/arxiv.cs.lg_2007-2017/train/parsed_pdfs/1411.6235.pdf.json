{
  "name" : "1411.6235.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Balanced k-Means and Min-Cut Clustering",
    "authors" : [ "Xiaojun Chang", "Feiping Nie", "Zhigang Ma", "Yi Yang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n62 35\nv1 [\ncs .L\nG ]\n2 3\nN ov\n2 01\n4 1\nIndex Terms—Balanced k-Means, Min-Cut Clustering\n✦"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "C LUSTERING is a fundamental research topic in data miningand is widely used for many applications in the field of artificial intelligence, statistics and social sciences [1] [2] [3] [4] [5] [6]. The objective of clustering is to partition the original data points into a number of groups so that the data points within the same cluster are close to each other while those in different clusters are far away from each other [7] [8] [9] [10].\nAmong various approaches for clustering, K-means and mincut are two most popular choices in reality because of their simplicity and effectiveness [11]. The general procedure of traditional K-means (TKM) is to randomly initialize c clustering centers, assign each data point to its nearest cluster and compute a new clustering center iteratively. Some researchers claim that the curse of dimensionality may deteriorate the performance of TKM [12]. A straightforward solution of this problem is to project the original dataset to a low-dimensional subspace by dimensionality reduction, for example, PCA, before performing TKM. Discriminative analysis has been shown effective in enhancing clustering performance [12] [13] [14]. Motivated by this fact, discriminative k-means (DKM) [15] is proposed to incorporate discriminative analysis and clustering into a single framework to formalize the clustering as a trace maximization problem.\nBy contrast, the min-cut clustering is realized by constructing a weighted undirected graph and then partitioning its vertices into two sets so that the total weight of the set of edges with endpoints in different sets is minimized [16] [17]. Among several graph clustering methods, min-cut tends to provide more balanced clusters as compared to other graph clustering criterion. As the within-cluster similarity in min-cut method is explicitly maximized, solving the min-cut clustering problem is nontrivial. The main difficulty lies\n• Xiaojun Chang and Yi Yang are with School of Information Technology and Electrical Engineering, The University of Queensland, Australia. (E-mail: cxj273@gmail.com, yi.yang@uq.edu.au)\n• Feiping Nie is with Department of Computer Science and Engineering, University of Texas, US. • Zhigang Ma is with School of Computer Science, Carnegie Mellon University, US.\nin the constraint on the solution. Thus, to make this problem tractable, researchers proposed to relax the constraint.\nAlthough k-means and min-cut have achieved promising performance in many applications, they have certain limit.Given that the distribution of the data points is balanced, one would expect the clustering result to reflect such balance. That being said, a clustering algorithm shall avoid partitioning a minority of data points into a cluster. Nonetheless, both K-means and mincut, as well as some other similar clustering algorithms, do not guarantee balanced clustering result. In many real world data mining applications, the data from each cluster are about the same. For example, the male and female populations in the same age range cannot be very different. Therefore, for those data which are equally distributed, it is more reasonable to explicitly guarantee the clustering results balanced.\nMotivated by the limit of k-means and min-cut for handling balanced data, we propose to design a balanced clustering algorithm. Specifically, the exclusive lasso proposed by Zhou et al. [18] has been exploited in our approach to fulfill such purpose. The exclusive lasso was originally used for feature selection across multiple tasks. It models the scenario when variables in the same group compete with each other. With exclusive lasso, if one feature in a group is given a large weight, it tends to assign small or even zero weights to the other features in the same group. Suppose that the exclusive lasso is applied on a bunch of data points across multiple categories. In a similar manner, we introduce a competition among different categories for the same data point. If more data points are clustered into one category, other categories would get fewer data points. The exclusive lass, thus in a sense, measures the balance degree of the clustering result. The smaller value the exclusive lasso obtains, the more balanced the clustering result is. With such insight, we formulate our clustering approach based on minimizing the exclusive lasso. In this paper, we particularly incorporate the exclusive lasso into k-means clustering and min-cut clustering, aiming to promote these two mainstream clustering approaches with stronger ability of attaining balanced clusters.\nThe major contributions of this paper can be summarized as follows:\n2 1) We leverage the exclusive lasso to introduce a competition among different categories for the same data point, thus enhancing the balance of the clustering result. 2) The exclusive lasso is particularly tailored for k-means and min-cut. Thus, these two mostly used clustering approaches are able to achieve more balanced clustering result. 3) The proposed algorithms are non-smooth and difficult to optimize. We propose a new iterative solution to solve the problems.\nThe rest of this paper is organized as follows. After revisiting the related work on k-means, min-cut and the exclusive lasso in Section 2, we detail the proposed balanced k-means and mincut algorithms in Section 3. Extensive experiments are given in Section 4 and Section 5 concludes this paper."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "In this section, we briefly review the research on k-means, min-cut and the exclusive lasso.\n2.1 The Classical K-means\nAs one of the most efficient clustering algorithms, k-means clustering has been widely applied to real-world applications. The centroids of clusters are utilized to characterize the data. The objective of k-means is to minimize the sum of the squared errors defined by:\nJk = K∑\nk=1\n∑\ni∈Ck\n‖xi −mk‖ 2, (1)\nwhere X = (x1, . . . , xn) denotes the data matrix and mk =∑ i∈Ck\nxi/nk is the centroid of a cluster Ck of nk data points. Previous work [19] has shown that H-orthogonal non-negative matrix factorization (NMF) is equivalent to relaxed k-means clustering. Thus, k-means clustering can be reformulated using the clustering indicator as follows:\nmin F,G\n‖X −HFT ‖2F\ns.t. Gik ∈ {0, 1}, K∑\nk=1\nHik = 1, ∀i = 1, 2, . . . , n (2)\nwhere X ∈ Rd×n is the input data matrix with n data represented by d-dimensional features; F ∈ Rd×K is the clustering indicator matrix; H ∈ Rn×K is the clustering assignment matrix and each row of H satisfies the 1-of-K coding scheme (if a data point xi is assigned to the k-th cluster then Hik = 1 and Hik = 0 otherwise). In this paper, given a matrix X = {xij}, its i-th row, j-th column are denoted as xi, xj , respectively.\nIn the literature, the classical K-means and its variants have been applied to many data mining applications. For example, Mehrdad et al. [19] propose a harmony K-means (HKM) algorithm based on harmony search optimization method and applied it to document clustering. HKM can be proved by means of finite Markov chain theory to converge to the global optimum. Zhang et al. [20] propose a new neighborhood density method for selecting initial cluster centers for K-means clustering. Deepak et al. [21] employ quantization schemes to retain the outcome of clustering operations. Although these methods get good performance, they have not considered how to achieve balanced clustering result\nwhen the given data points are evenly distributed. By contrast, we aim to develop a balanced k-means clustering algorithm that well addresses this issue."
    }, {
      "heading" : "2.2 Min-Cut",
      "text" : "The principle of min-cut is rooted in graph theory. It needs a graph based on a weight matrix W ∈ Rn×n built from n data points {x1, . . . , xn}. The min-cut graph clustering objective function can be generalized as:\nJ = ∑\n1≤p<q≤K\ns(Cp, Cq) + s(Cp, Cq) = K∑\nk=1\ns(Ck, Ck) (3)\nwhere K is the number of clusters, Ck is the k-th cluster (subgraph in graph G), Ck is the complement of a subset Ck in graph G, and for any set A and B\ns(A,B) = ∑\ni∈A\n∑\nj∈B\nWij , (4)\ndi = ∑\nj\nWij . (5)\nWe denote qk (k = 1, . . . ,K) as the cluster indicators where the i-th element of qk is set to 1 if the i-th data point xi belongs to the k-th cluster, and 0 otherwise. For example, if the data points within each cluster are adjacent,\nqk = (0, . . . , 0,\nnk︷ ︸︸ ︷ 1, . . . , 1, 0, . . . , 0)T . (6)\nAfter simple mathematical deduction, we can find that\ns(Ck, Ck) = ∑\ni∈Ck\n∑\nj∈Ck\nWij = q T k (D −W )qk\n∑\ni∈Ck\ndi = q T k Dqk\ns(Ck, Ck) = q T k Wqk, (7)\nwhere D is a diagonal matrix with the i-th diagonal element as di. The objective function of min-cut method can therefore be reformulated as:\nJ = K∑\nk=1\nqTk (D −W )qk (8)\nMin-Cut clustering has been applied in various applications. Wang et al. [22] propose a flexible and generalized framework for constrained spectral clustering, interpret the algorithm as finding the normalized min-cut of a labeled graph, and apply it to constrained image segmentation. Dynamic graph clustering algorithm, proposed by [23] can provide strong theoretical quality guarantee on clusters. However, none of the existing work on mincut is capable of balanced clustering when necessary, which shall be addressed by our newly proposed balanced min-cut algorithm.\n3"
    }, {
      "heading" : "2.3 Exclusive Lasso",
      "text" : "Zhou et al. propose the exclusive lasso to model the scenario when variables in the same group compete with each other. They apply it to multi-task feature selection and obtain good performance. The exclusive lasso [18] is defined as follows:\n‖β‖e =\n√√√√ d∑\nj=1\n( m∑\nk=1\n‖βjk‖) 2, (9)\nwhere ‖β‖e is a regularizer that controls the complexity of the combination weights.\nIn [18], the regularizer introduces an l1-norm to combine the weights for the same category used by different data points and an l2-norm to combine the weights of different categories. Since l1-norm tends to achieve a sparse solution, the construction in the exclusive lasso essentially introduces a competition among different categories for the same data points.\nIn our work, the exclusive lasso is used as a balance constraint. We will prove that the value of exclusive lasso indicates the balance degree of our clustering algorithms."
    }, {
      "heading" : "3 THE PROPOSED ALGORITHM",
      "text" : "In this section, we illustrate the proposed approach in details."
    }, {
      "heading" : "3.1 Balance Constraint",
      "text" : "Given F as a cluster indicator matrix, the exclusive lasso of F is written as\n‖F‖e =\n√√√√ c∑\nj=1\n( n∑\ni=1\n‖fij‖)2. (10)\nWith simple mathematical deduction, the exclusive lasso can be rewritten as:\n‖F‖e = Tr(F T11TF ). (11)\nFrom this equation, we can observe that the value of exclusive lasso equals the square-sum of the number of data points in each class. In the following, we prove that the most balanced clustering can be achieved by minimizing the exclusive lasso.\nTheorem 1. Given n1 + n2 + · · · + nk = N and ni|ki=1 ≥ 0,∑k i=1 n 2 i arrives at its minimum when ni = N k .\nProof. According to the Cauchy inequality,\n(n21 + n 2 2 + · · ·+ n 2 k)(b 2 1 + b 2 2 + · · ·+ b 2 k)\n≥(a1b1 + a2b2 + · · ·+ akbk) 2\nLet bi|ki=1 = 1, the equality holds when n1 = n2 = · · · = nk. Hence, we can easily have the conclusion that when ni = Nk ,∑k\ni=1 n 2 i get its minimal value.\nAccording to the above theorem, by minimizing the exclusive lasso, each cluster will have n\nc data points. The most balanced\nclustering result is thus obtained. Hence, we use the the exclusive lasso as the balance constraint.\n3.2 Balanced k-Means\nIn the setting of clustering, given n data points {xi}|ni=1, we have a data matrix X = (x1, . . . , xn) ∈ Rd×n. Our goal in balanced k-means is to partition {xi}|i = 1\nn into K balanced clusters among different categories.\nNoting that the exclusive lasso is capable of introducing competition among different categories, we apply the exclusive lasso to the classical k-means to obtain balanced clusters. The proposed objective function of balanced k-means is formulated as follows:\nmin F∈Ind\n‖X −HFT ‖2F + γ‖F‖e (12)\nBy substituting ‖F‖e with (9), the objective function can be rewritten as follows:\nmin F∈Ind\n‖X −HFT ‖2F + γT r(F T11TF ) (13)\nwhere F ∈ Ind means F ∈ Rn×K is an indicator matrix used for clustering; H ∈ Rd×K is the clustering assignment matrix; γ is a parameter.\nThe optimal H and F would minimize the objective function value. Since it is difficult to compute the optimal H and F simultaneously, we present an iterative approach to optimize this algorithm. To be more specific, we can obtain the optimal H by fixing F by a simple linear equation. Similarly, we can get the optimal F by fixing H .\nFor a fixed F , by setting the derivative of (13) w.r.t H to zero, we obtain\nH = XF (FTF )−1 (14)\nThen we fix H , we update F as follows: we update one row of F each time while fixing the other rows of the prediction matrix F . Specifically, the updating of one row is realized by finding the element being 1 that results in the minimum of (13). We iterate the updating of each row until convergence as shown in Algorithm 1.\nAlgorithm 1 Algorithm to solve the objective function of balanced k-means Input: Data matrix X ∈ Rd×n Output: Indicator matrix F ∈ Rn×K\n1: Initialize the indicator matrix F randomly. 2: repeat 3: Fixing F , compute H according to H = XF (FTF )−1 4: Fixing H , update F as follows: Update each row of F while fixing the remaining rows.\n5: until CONVERGENCE Return: Indicator matrix F .\nComputational Analysis: The computation complexity of Algorithm 1 is O(K). Since the indicator matrix F is sparse, this inverse operation is very efficient. When sufficient computational resources are available and parallel computing is implemented, this algorithm can be solved with desired efficiency.\nConvergence Analysis: The following theorem guarantees the convergence of Algorithm 1.\nTheorem 2. Algorithm 1 decreases the objective value of Eq. (13) in each iteration.\n4 Proof. In each iteration t of Algorithm 1, according to Step 3, we know that:\nHt+1 = min F\n‖X −HFTt ‖ 2 F + γT r(F T t 11 TFt) (15)\nThus, we have:\n‖X −Ht+1F T t ‖ 2 F + γT r(F T t 11 TFt)\n≤‖X −HtF T t ‖ 2 F + γT r(F T t 11 TFt) (16)\nAccording to step 4, we obtain:\n‖X −HtF T t+1‖ 2 F + γT r(Ft+111 TFt+1)\n≤‖X −HtF T t ‖ 2 F + γT r(Ft11 TFt) (17)\nAdding Eq. (16) and Eq. (17), we arrive at:\n‖X −Ht+1F T t+1‖ 2 F + γT r(Ft+111 TFt+1)\n≤‖X −HtF T t ‖ 2 F + γT r(Ft11 TFt) (18)\nwhich proves that the algorithm decreases the objective function value in each iteration.\nAccording to Theorem 2, we can see that the value of the objective function (13) decrease at each iteration of Algorithm 1. In addition, it is clear that (13) is lower bounded by 0. Therefore, Algorithm 1 is guaranteed to converge."
    }, {
      "heading" : "3.3 Balanced Min-Cut",
      "text" : "We similarly aim to cluster n data points X = {x1, . . . , xn} ∈ R d×n into K clusters. To begin with, we use the Gaussian function to construct a weight matrix A. The weight Aij is defined as:\nAij =    exp(− ‖xi−xj‖ 2 δ2 ), xi and xj are k\nnearest neighbors. 0, otherwise\n(19)\nwhere δ is utilized to control the spread of neighbors. Given the weight matrix A and the cluster indicator matrix F , the objective function of min-cut graph clustering is formulated as follows:\nmin F∈Ind\n1TA1− Tr(FTAF ), (20)\nwhich is equivalent to the following objective function:\nmax F∈Ind\nTr(FTAF ) (21)\nWe further incorporate the exclusive lasso into min-cut and get the following objective function:\nmax F∈Ind\nTr(FTAF )− γ‖F‖e (22)\nIn the same manner, we substitute ‖F‖e with (9) and rewrite the objective function as follows:\nmax F∈Ind\nTr(FTAF )− γT r(FT11TF ) (23)\nWith a simple mathematical deduction, the objective function is rewritten as:\nmax F∈Ind\nTr ( FT (ρI +A− γ11T )F ) , (24)\nwhere ρ is a large enough constant to make ρI + A − γ11T positive-definite. Defining B = (ρI + A − γ11T )F , we update\nAlgorithm 2 Algorithm to solve the objective function of balanced min-cut Input: Data matrix X Output: Indicator matrix F\n1: Compute the weight matrix A using Eq (19). 2: repeat 3: Compute B according to B = (ρI +A− γ11T )F 4: Update F by solving maxF∈Ind Tr(FTB) 5: until CONVERGENCE\nReturn: Indicator matrix F\nF by solving maxF∈Ind Tr(FTB). F is iteratively updated until convergence as shown in Algorithm 2.\nCompuational Analysis: The computation complexity of Algorithm 2 is O(n).\nConvergence Analysis: The following theorem guarantees the convergence of Algorithm 2.\nTheorem 3. Algorithm 2 increases the objective function value of Eq. (24) in each iteration.\nProof. In the Steps 3 and 4 of Algorithm 2, we denote the updated B and F by B̂ and F̂ , respectively. Since the updated B and F are the optimal solutions of the problem maxF∈Ind Tr(FTB), we have:\nTr(F̂T (ρI +A− γ11T )F ) ≥ Tr(FT (ρI +A− γ11T )F ), (25) which proves that the algorithm increase the objective function value in each iteration.\nAccording to Theorem 3, we can observe that the value of objective function (24) increases at each iteration of Algorithm 2. Therefore, Algorithm 2 is proved to converge."
    }, {
      "heading" : "4 EXPERIMENT",
      "text" : "In this section, extensive experiments are conducted to evaluate the proposed clustering methods. We give two sets of experiments. The first one is to compare the proposed balanced K-means clustering to K-means based clustering algorithms, including the classical K-means (KM) clustering, DisCluster (DC), DisKmeans (DKM) clustering [15], AKM [19] and HKM [19]. The second one is to compare the proposed balanced min-cut clustering to the classical min-cut clustering, MinMax Cut clustering, Ratio Cut clustering and Normalized Cut clustering algorithms."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "A variety of datasets are used in our experiments which are described as follows.\n1) MNIST Handwritten Digit Dataset: The MNIST handwritten digit dataset [24] is a large-scale dataset of handwritten digits. It is widely used as a test bed in data mining. The dataset contains 60,000 training images and 10,000 testing images. We merge all the training and testing images in the experiments. The pixel values are used as feature representation. 2) USPS handwritten digit dataset: We additionally use the USPS dataset to validate the performance on handwritten digit recognition. The dataset consists of 9298 gray-scale\n5\nTABLE 1 A BRIEF SUMMARY OF THE EXPERIMENTAL DATASETS.\nDataset Size Dimension of Features Class Number MNIST Handwritten digit dataset 70,000 1024 10 USPS Handwritten Digit Data Set 9298 256 10\nYaleB Face Data Set 2414 1024 38 ORL Face Data Set 400 1024 40\nJAFFE Facial Expression Data Set 213 676 10 HumanEVA Motion Data Set 10000 168 10\nCoil20 Object Data Set 1440 1024 20 CMU-PIE face dataset 41,368 1024 68\nUMIST face dataset 564 1024 20\nTABLE 2 Performance comparison (Clustering Accuracy ± STANDARD DEVIATION) of clustering accuracy using k-means, DisCluster, DisKmeans, AKM, HKM and Balanced k-means on nine benchmark datasets. From the experimental result, we can observe that the proposed algorithm consistently outperforms the other comparison algorithms.\nk-means DisCluster DisKmeans AKM HKM Balanced k-means\nMNIST 52.6± 3.3 53.7± 2.4 54.2± 3.4 52.2 ± 3.3 55.4± 3.1 57.3± 2.4\nUSPS 65.8± 2.5 67.4± 2.8 70.4± 2.6 66.3 ± 2.9 71.5± 2.3 73.4± 2.8\nYaleB 16.3± 1.1 35.2± 2.3 39.7± 2.5 16.8 ± 0.8 41.3± 3.2 43.5± 1.8\nORL 37.2± 1.6 41.2± 2.1 43.9± 1.8 37.4 ± 1.5 44.4± 2.7 47.2± 2.2\nJAFFE 58.8± 2.2 59.4± 2.7 59.9± 2.5 59.0 ± 2.8 60.5± 1.9 61.2± 1.8\nHumanEVA 43.2± 3.2 44.2± 3.1 45.1± 2.3 43.8 ± 3.4 46.3± 2.6 47.7± 2.5\nCoil20 68.4± 2.8 65.3± 2.6 61.3± 2.3 67.9 ± 2.7 70.3± 2.4 73.1± 2.3\nCMU-PIE 19.5± 0.8 49.8± 2.7 55.5± 2.9 21.2 ± 1.1 56.1± 2.2 57.8± 2.4\nUMIST 39.5± 2.1 41.3± 2.6 43.2± 2.4 39.1 ± 1.8 44.1± 2.6 46.4± 2.5\nhandwritten digit images. We resize the images to 16×16 and use pixel values as the features. 3) YaleB face dataset: The YaleB dataset [25] contains 2414 near frontal images from 38 persons under different illuminations. Each image is resized to 32 × 32 and the pixel value is used as feature representation. 4) ORL face dataset: The ORL dataset [26] consists of 40 different subjects with 10 images each. We also resize each image to 32 × 32 and use pixel values to represent the images. 5) JAFFE Japanese Female Facial Expression dataset: The JAFFE dataset [27] consists of 213 images of different facial expressions from 10 different Japanese female models. The images are resized to 26 × 26 and represented by pixel values. 6) HumanEVA Motion dataset: The HumanEVA dataset is used to evaluate the performance of our algorithm in terms of 3D motion annotation 1. This dataset contains five types of motions. Based on the 16 joint coordinates in 3D space, 1590 geometric pose descriptors are extracted using the method proposed in [28] to represent 3D motion data. 7) Coil20 Object dataset: We use the Coil20 dataset [29] for object recognition. This dataset includes 1440 gray-scale images with 20 different objects. In our experiment, we resize each image to 32× 32 and use pixel values as the features. 8) CMU-PIE dataset: The CMU-PIE face dataset consists of 41,368 images of 68 people. Each person was imaged\n1. http://vision.cs.brown.edu/humaneva/\nunder 13 different poses, 43 different illumination conditions, and with 4 different expressions. We also use the pixel values as the feature representations. 9) UMIST face dataset: The UMIST face dataset consists of 564 images of 20 individuals with mixed race, gender and appearance. Each individual is shown in a range of poses from profile to frontal views. The pixel value is used as the feature representation.\nTable 1 gives a brief summary of all the experimental datasets."
    }, {
      "heading" : "4.2 Parameter Setting",
      "text" : "There are three parameters in our algorithms. The first one is the number of nearest neighbors and the second one is the parameter δ in Eq. (19). Following , we set the number of nearest neighbors to 5 in the experiments. The self-tune clustering method is utilized to determine the parameter δ. For the regularization parameter γ in Eq. (13) and Eq. (24), we tune them by a ”grid-search” strategy from {10−6, 10−4, 10−2, 100, 102, 104, 106}. We similarly tune the regularization parameters of all the comparison algorithms from the aforementioned range. The best results of all the comparison algorithms are reported."
    }, {
      "heading" : "4.3 Evaluation Metrics",
      "text" : "Following related work, we adopt clustering accuracy (ACC) and normalized mutual information (NMI) as our evaluation metrics in our experiments.\nLet qi represent the clustering label result from a clustering algorithm and pi represent the corresponding ground truth label of arbitrary data point xi. Then ACC is defined as follows:\n6\nACC =\n∑n i=1 δ(pi,map(qi))\nn , (26)\nwhere δ(x, y) = 1 if x = y and δ(x, y) = 0 otherwise. map(qi) is the best mapping function that permutes clustering labels to match the ground truth labels using the Kuhn-Munkres algorithm. A larger ACC indicates a better clustering performance.\nFor any two arbitrary variable P and Q, NMI is defined as follows [30]:\nNMI = I(P,Q)√ H(P )H(Q) , (27)\nwhere I(P,Q) computes the mutual information between P and\n7 10^-610^-410^-210^0 10^2 10^4 10^6 48 50 52 54 56 58 C lu st er in g A cc ur ac y %\n\\gamma\nK-means Balanced K-means\n(a)\n10^-610^-410^-210^0 10^2 10^4 10^6 56\n60\n64\n68\n72\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nK-means Balanced K-means\n(b)\n10^-610^-410^-210^0 10^2 10^4 10^6 15\n20\n25\n30\n35\n40\n45\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nMincut Clustering Balanced Mincut Clustering\n(c)\n10^-610^-410^-210^0 10^2 10^4 10^6\n35\n40\n45\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nK-means Balanced K-means\n(d)\n10^-610^-410^-210^0 10^2 10^4 10^6\n50\n55\n60\n65\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nK-means Balanced K-means\n(e)\n10^-610^-410^-210^0 10^2 10^4 10^6 36\n38\n40\n42\n44\n46\n48\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nK-means Balanced K-means\n(f)\n10^-610^-410^-210^0 10^2 10^4 10^6 60\n62\n64\n66\n68\n70\n72\n74\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nK-means Balanced K-means\n(g)\n10^-610^-410^-210^0 10^2 10^4 10^6\n24\n32\n40\n48\n56\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nMincut Clustering Balanced Mincut Clustering\n(h)\n10^-610^-410^-210^0 10^2 10^4 10^6 24\n30\n36\n42\n48\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nK-means Balanced K-means\n(i)\nFig. 1. Parameter sensitivity of Balanced k-means. (a) MNIST (b) USPS (c) YaleB (d) ORL (e) JAFFE (f) HumanEVA (g) Coil20 (h) CMU-PIE (i) UMIST. From the results, we can observe that the parameter has a significant impact on the performance.\nQ, and H(P ) and H(Q) are the entropies of P and Q. Let tl represent the number of data in the cluster Cl(1 ≤ l ≤ c) generated by a clustering algorithm and t̃h represent the number of data points from the h-th ground truth class. NMI metric is then computed as follows [30]:\nNMI =\n∑c l=1 ∑c h=1 tl,hlog( n×tl,h tl t̃h\n) √ ( ∑c l=1 tl log tl n )( ∑c h=1 t̃h log t̃h n ) , (28)\nwhere tl,h is the number of data samples that lies in the intersection between Cl and hth ground truth class. Similarly, a larger NMI indicates a better clustering performance.\n4.4 Comparison among k-means based methods\nIn this section, we report the performance comparison using kmeans, DisCluster, DisKmeans, AKM, HKM and Balanced kmeans in terms of clustering accuracy (ACC) and NMI in Table 2 and Table 3.\nFrom the experimental results, we have the following observations:\n1) When compared to the classical k-means clustering, DisCluster and DisKmeans algorithms, DisCluster and DisKmeans generally have better performance. This may be because discriminative dimension reduction is integrated into a single framework. Thus, each cluster is more identifiable, which helps enhance the clustering performance. We can therefore conclude that discriminative information is beneficial for clustering. 2) HKM achieves the second best performance among the comparison algorithms, which indicates that most active points changing their cluster assignments at each iteration are located on or near the cluster boundaries. 3) The proposed balanced k-means always gets the best performance on all the datasets. This experimental result demonstrates that the exclusive lasso is able to pose balance constraint to k-means clustering. By minimizing the exclusive lasso, the most balanced clustering result is obtained.\n8 10^-610^-410^-210^0 10^2 10^4 10^6 30 40 50 60 70 80 90 C lu st er in g A cc ur ac y %\n\\gamma\nMincut Clustering Balanced Mincut Clustering\n(a)\n10^-610^-410^-210^0 10^2 10^4 10^6 30\n40\n50\n60\n70\n80\n90\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nMincut Clustering Balanced Mincut Clustering\n(b)\n10^-610^-410^-210^0 10^2 10^4 10^6 30\n35\n40\n45\n50\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nMincut Clustering Balanced Mincut Clustering\n(c)\n10^-610^-410^-210^0 10^2 10^4 10^6 70\n75\n80\n85\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nMincut Clustering Balanced Mincut Clustering\n(d)\n10^-610^-410^-210^0 10^2 10^4 10^6 51\n54\n57\n60\n63\n66\n69\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nMincut Clustering Balanced Mincut Clustering\n(e)\n10^-610^-410^-210^0 10^2 10^4 10^6 40\n45\n50\n55\n60\n65\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nMincut Clustering Balanced Mincut Clustering\n(f)\n10^-610^-410^-210^0 10^2 10^4 10^6 60\n65\n70\n75\n80\n85\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nMincut Clustering Balanced Mincut Clustering\n(g)\n10^-610^-410^-210^0 10^2 10^4 10^6 45 48 51 54 57 60 63 66 69 C lu st er in g A cc ur ac y %\n\\gamma\nMincut Clustering Balanced Mincut Clustering\n(h)\n10^-610^-410^-210^0 10^2 10^4 10^6 51\n54\n57\n60\n63\n66\n69\nC lu\nst er\nin g\nA cc\nur ac\ny %\n\\gamma\nMincut Clustering Balanced Mincut Clustering\n(i)\nFig. 2. Parameter sensitivity of Balanced Min-Cut w.r.t γ. (a) MNIST (b) USPS (c) YaleB (d) ORL (e) JAFFE (f) HumanEVA (g) Coil20 (h) CMU-PIE (i) UMIST. From the results, we can observe that the parameter, γ has a significant impact on the performance. To be more specific, better performance is achieved when γ is in the range of {10−2, 102}."
    }, {
      "heading" : "4.5 Comparison among graph clustering algorithms",
      "text" : "To evaluate performance of the proposed balanced min-cut clustering algorithm, we compare it to the classical Min-Cut clustering, MinMax Cut clustering [31], Ratio Cut clustering [32], Normalized Cut Clustering [33] and Balanced Min-Cut clustering on the nine benchmark datasets.\nWe have the following observations from the experimental results:\n1) Compared with the k-means based clustering, the graph clustering algorithms generally achieve better performance. This observation indicates that it is beneficial to utilize the pairwise similarities between all data points from a weighted graph adjacency matrix that contains much helpful information for clustering. 2) MinMax Cut Clustering always gets the second best performance, which demonstrates that min-max clustering\nprinciple can result in more balanced partitions than the other comparison graph clustering methods. 3) The proposed balanced min-cut clustering algorithm consistently outperforms the other graph clustering algorithms. From this result, we can conclude that the exclusive lasso is able to exert balance constraint on min-cut clustering and thus achieves the most balanced clustering result."
    }, {
      "heading" : "4.6 Parameter Sensitivity of the Proposed Algorithm",
      "text" : "In this section, we study the parameter sensitivity of balanced kmeans and balanced min-cut. Fig 1 shows the accuracy (y-axis) of balanced k-means for different γ values (x-axis). From the experimental result, we can observe that γ has a significant impact on the performance of balanced K-means.\nWe additionally show the parameter sensitivity of balanced min-cut in Fig. 2. Similarly to the proposed balanced k-means,\n9 the performance is heavily influenced by the parameter γ. To be more specific, better performance is usually attained when γ is in the range of {10−2, 102}.\nThe experiments on both algorithms suggest the importance of designing an auto-tuning method for parameter selection. However, how to decide the optimal parameter is currently out of the scope in this work. We shall focus on this problem in the future."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this paper, we have addressed the issue of balanced clustering which has not been studied in data mining. The exclusive lasso has been exploited to exert the balance constraint for introduce its ability to induce competition among different categories for the same data point. Particularly, we incorporated the exclusive lasso into k-means and min-cut clustering algorithms, which shall facilitate these two mainstream clustering algorithms to better cope with balanced data points. On the other hand, our objective functions are non-smooth and difficult to optimize. A new iterative approach is then designed to solve the problems. We have performed extensive experiments on a copious of datasets to evaluate performance of the proposed balanced k-means and balanced mincut in terms of clustering accuracy and NMI. The experimental results show that our proposed algorithms always outperform the other comparison state-of-art clustering algorithms, which validates that utilizing the exclusive lasso indeed helps achieve the most balanced clustering."
    } ],
    "references" : [ {
      "title" : "Data clustering: a review",
      "author" : [ "A.K. Jain", "M.N. Murty", "P.J. Flynn" ],
      "venue" : "ACM computing surveys, vol. 31, no. 3, pp. 264–323, 1999.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Algorithms for clustering data",
      "author" : [ "A.K. Jain", "R.C. Dubes" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1988
    }, {
      "title" : "Mercer kernel-based clustering in feature space",
      "author" : [ "M. Girolami" ],
      "venue" : "IEEE Trans. Neural Networks, vol. 13, no. 3, pp. 780–784, 2002.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Efficient clustering aggregation based on data fragments",
      "author" : [ "O. Wu", "W. Hu", "S.J. Maybank", "M. Zhu", "B. Li" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B, vol. 42, no. 3, pp. 913–926, 2012.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Coupled clustering ensemble: Incorporating coupling relationships both between base clusterings and objects",
      "author" : [ "C. Wang", "Z. She", "L. Cao" ],
      "venue" : "Proc. ICDE, 2013.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Adaptive distance metric learning for clustering",
      "author" : [ "J. Ye", "Z. Zhao", "H. Liu" ],
      "venue" : "Proc. CVPR, 2007, pp. 1–7.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Algorithms for clustering data",
      "author" : [ "A.K. Jain", "R.C. Dubes" ],
      "venue" : "Englewood Cliffs,NJ: Prentice-Hall, 1988.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Initialization independent clustering with actively self-training method",
      "author" : [ "F. Nie", "D. Xu", "X. Li" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B, vol. 42, no. 1, pp. 17–27, 2012.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A convex formulation for shrunk spectral clustering",
      "author" : [ "X. Chang", "F. Nie", "Z. Ma", "Y. Yang", "X. Zhou" ],
      "venue" : "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, January 2530, 2015, Austin Texas, USA., 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A survey of kernel and spectral methods for clustering",
      "author" : [ "M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta" ],
      "venue" : "Pattern recognition, vol. 41, no. 1, pp. 176–190, 2008.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning bregman distance functions for semi-supervised clustering",
      "author" : [ "L. Wu", "S.C. Hoi", "R. Jin", "J. Zhu", "N. Yu" ],
      "venue" : "IEEE Trans. Knowledge and Data Eng., vol. 24, no. 3, pp. 478–491, 2012.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Adaptive dimension reduction using discriminant analysis and k-means clustering",
      "author" : [ "C. Ding", "T. Li" ],
      "venue" : "Proc. ICML, 2007, pp. 521–528.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Discriminative cluster analysis",
      "author" : [ "D. la Torre", "Fernando", "T. Kanade" ],
      "venue" : "Proc. ICML, 2006, pp. 241–248.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Adaptive distance metric learning for clustering",
      "author" : [ "J. Ye", "Z. Zhao", "H. Liu" ],
      "venue" : "Proc. CVPR, 2007, pp. 1–7.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Discriminative k-means for clustering",
      "author" : [ "J. Ye", "Z. Zhao", "M. Wu" ],
      "venue" : "Proc. NIPS, 2007, pp. 1649–1656.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A spatially continuous maxflow and min-cut framework for binary labeling problems",
      "author" : [ "J. Yuan", "E. Bae", "X.-C. Tai", "Y. Boykov" ],
      "venue" : "Numerische Mathematik, 2014.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "An information-theoretic derivation of mincut-based clustering",
      "author" : [ "A. Raj", "C.H. Wiggins" ],
      "venue" : "IEEE PAMI, vol. 32, no. 6, 2010.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Exclusive lasso for multi-task feature selection",
      "author" : [ "Y. Zhou", "R. Jin", "S. Hoi" ],
      "venue" : "Proc. ICAIS, pp. 988–995, 2010.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Fast approximate k-means via cluster closures",
      "author" : [ "J. Wang", "J. Wang", "Q. Ke", "G. Zeng", "S. Li" ],
      "venue" : "Proc. CVPR, 2012.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "K-means clustering algorithm with improved initial center",
      "author" : [ "C. Zhang", "S. Xia" ],
      "venue" : "Proc. Knowledge Discovery and Data Mining, 2009, pp. 790–792.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On k-means cluster preservation using quantization schemes",
      "author" : [ "D.S. Turaga", "M. Vlachos", "O. Verscheure" ],
      "venue" : "Proc. ICDM, 2009.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Flexible constrained spectral clustering",
      "author" : [ "X. Wang", "I. Davidson" ],
      "venue" : "Proc. KDD, 2010, pp. 563–572.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Fast incremental minimum-cut based algorithm for graph clustering",
      "author" : [ "B. Saha", "P. Mitra" ],
      "venue" : "Proc. ICDM, pp. 207–211, 2006.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proc. IEEE, 2011.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "From few to many: Illumination cone models for face recognition under variable lighting and pose",
      "author" : [ "A.S. Georghiades", "P.N. Belhumeur", "D. Kriegman" ],
      "venue" : "IEEE Trans. PAMI, vol. 23, no. 6, pp. 643–660, 2001.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Parameterisation of a stochastic model for human face identification",
      "author" : [ "F.S. Samaria", "A.C. Harter" ],
      "venue" : "Proc. Applications of Computer Vision, 1994, pp. 138–142.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Automatic classification of single facial images",
      "author" : [ "M.J. Lyons", "J. Budynek", "S. Akamatsu" ],
      "venue" : "IEEE Trans. PAMI, vol. 21, no. 12, pp. 1357–1362, 1999.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Learning a 3d human pose distance metric from geometric pose descriptor",
      "author" : [ "C. Chen", "Y. Zhuang", "F. Nie", "Y. Yang", "F. Wu", "J. Xiao" ],
      "venue" : "IEEE Trans. Visualization and Computer Graphics, vol. 17, no. 11, 2011.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Columbia object image library (coil-20)",
      "author" : [ "S.A. Nene", "S.K. Nayar", "H. Murase" ],
      "venue" : "CUCS-005-96, Columbia University, Tech. Rep., 1996.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Cluster ensembles—a knowledge reuse framework for combining multiple partitions",
      "author" : [ "A. Strehl", "J. Ghosh" ],
      "venue" : "Machine Learning Research, vol. 3, pp. 583–617, 2003.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A min-max cut algorithm for graph partitioning and data clustering",
      "author" : [ "C.H. Ding", "X. He", "H. Zha", "M. Gu", "H.D. Simon" ],
      "venue" : "Proc. ICDM, 2001.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "New spectral methods for ratio cut partitioning and clustering",
      "author" : [ "L. Hagen", "A.B. Kahng" ],
      "venue" : "Trans. Computer-aided design of integrated circuits and systems, 1992.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "Trans. PAMI, pp. 888–905, 2000.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 INTRODUCTION C LUSTERING is a fundamental research topic in data mining and is widely used for many applications in the field of artificial intelligence, statistics and social sciences [1] [2] [3] [4] [5] [6].",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 1,
      "context" : "1 INTRODUCTION C LUSTERING is a fundamental research topic in data mining and is widely used for many applications in the field of artificial intelligence, statistics and social sciences [1] [2] [3] [4] [5] [6].",
      "startOffset" : 191,
      "endOffset" : 194
    }, {
      "referenceID" : 2,
      "context" : "1 INTRODUCTION C LUSTERING is a fundamental research topic in data mining and is widely used for many applications in the field of artificial intelligence, statistics and social sciences [1] [2] [3] [4] [5] [6].",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 3,
      "context" : "1 INTRODUCTION C LUSTERING is a fundamental research topic in data mining and is widely used for many applications in the field of artificial intelligence, statistics and social sciences [1] [2] [3] [4] [5] [6].",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 4,
      "context" : "1 INTRODUCTION C LUSTERING is a fundamental research topic in data mining and is widely used for many applications in the field of artificial intelligence, statistics and social sciences [1] [2] [3] [4] [5] [6].",
      "startOffset" : 203,
      "endOffset" : 206
    }, {
      "referenceID" : 5,
      "context" : "1 INTRODUCTION C LUSTERING is a fundamental research topic in data mining and is widely used for many applications in the field of artificial intelligence, statistics and social sciences [1] [2] [3] [4] [5] [6].",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 6,
      "context" : "The objective of clustering is to partition the original data points into a number of groups so that the data points within the same cluster are close to each other while those in different clusters are far away from each other [7] [8] [9] [10].",
      "startOffset" : 228,
      "endOffset" : 231
    }, {
      "referenceID" : 7,
      "context" : "The objective of clustering is to partition the original data points into a number of groups so that the data points within the same cluster are close to each other while those in different clusters are far away from each other [7] [8] [9] [10].",
      "startOffset" : 232,
      "endOffset" : 235
    }, {
      "referenceID" : 8,
      "context" : "The objective of clustering is to partition the original data points into a number of groups so that the data points within the same cluster are close to each other while those in different clusters are far away from each other [7] [8] [9] [10].",
      "startOffset" : 236,
      "endOffset" : 239
    }, {
      "referenceID" : 9,
      "context" : "The objective of clustering is to partition the original data points into a number of groups so that the data points within the same cluster are close to each other while those in different clusters are far away from each other [7] [8] [9] [10].",
      "startOffset" : 240,
      "endOffset" : 244
    }, {
      "referenceID" : 10,
      "context" : "Among various approaches for clustering, K-means and mincut are two most popular choices in reality because of their simplicity and effectiveness [11].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 11,
      "context" : "Some researchers claim that the curse of dimensionality may deteriorate the performance of TKM [12].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : "Discriminative analysis has been shown effective in enhancing clustering performance [12] [13] [14].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "Discriminative analysis has been shown effective in enhancing clustering performance [12] [13] [14].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 13,
      "context" : "Discriminative analysis has been shown effective in enhancing clustering performance [12] [13] [14].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 14,
      "context" : "Motivated by this fact, discriminative k-means (DKM) [15] is proposed to incorporate discriminative analysis and clustering into a single framework to formalize the clustering as a trace maximization problem.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "By contrast, the min-cut clustering is realized by constructing a weighted undirected graph and then partitioning its vertices into two sets so that the total weight of the set of edges with endpoints in different sets is minimized [16] [17].",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 16,
      "context" : "By contrast, the min-cut clustering is realized by constructing a weighted undirected graph and then partitioning its vertices into two sets so that the total weight of the set of edges with endpoints in different sets is minimized [16] [17].",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 17,
      "context" : "[18] has been exploited in our approach to fulfill such purpose.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "Previous work [19] has shown that H-orthogonal non-negative matrix factorization (NMF) is equivalent to relaxed k-means clustering.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 18,
      "context" : "[19] propose a harmony K-means (HKM) algorithm based on harmony search optimization method and applied it to document clustering.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] propose a new neighborhood density method for selecting initial cluster centers for K-means clustering.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] employ quantization schemes to retain the outcome of clustering operations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] propose a flexible and generalized framework for constrained spectral clustering, interpret the algorithm as finding the normalized min-cut of a labeled graph, and apply it to constrained image segmentation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "Dynamic graph clustering algorithm, proposed by [23] can provide strong theoretical quality guarantee on clusters.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : "The exclusive lasso [18] is defined as follows: ‖β‖e = √√√√ d ∑",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "In [18], the regularizer introduces an l1-norm to combine the weights for the same category used by different data points and an l2-norm to combine the weights of different categories.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "The first one is to compare the proposed balanced K-means clustering to K-means based clustering algorithms, including the classical K-means (KM) clustering, DisCluster (DC), DisKmeans (DKM) clustering [15], AKM [19] and HKM [19].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 18,
      "context" : "The first one is to compare the proposed balanced K-means clustering to K-means based clustering algorithms, including the classical K-means (KM) clustering, DisCluster (DC), DisKmeans (DKM) clustering [15], AKM [19] and HKM [19].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 18,
      "context" : "The first one is to compare the proposed balanced K-means clustering to K-means based clustering algorithms, including the classical K-means (KM) clustering, DisCluster (DC), DisKmeans (DKM) clustering [15], AKM [19] and HKM [19].",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 23,
      "context" : "1) MNIST Handwritten Digit Dataset: The MNIST handwritten digit dataset [24] is a large-scale dataset of handwritten digits.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 24,
      "context" : "3) YaleB face dataset: The YaleB dataset [25] contains 2414 near frontal images from 38 persons under different illuminations.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 25,
      "context" : "4) ORL face dataset: The ORL dataset [26] consists of 40 different subjects with 10 images each.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 26,
      "context" : "5) JAFFE Japanese Female Facial Expression dataset: The JAFFE dataset [27] consists of 213 images of different facial expressions from 10 different Japanese female models.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 27,
      "context" : "Based on the 16 joint coordinates in 3D space, 1590 geometric pose descriptors are extracted using the method proposed in [28] to represent 3D motion data.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 28,
      "context" : "7) Coil20 Object dataset: We use the Coil20 dataset [29] for object recognition.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 29,
      "context" : "For any two arbitrary variable P and Q, NMI is defined as follows [30]:",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 29,
      "context" : "NMI metric is then computed as follows [30]:",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 30,
      "context" : "To evaluate performance of the proposed balanced min-cut clustering algorithm, we compare it to the classical Min-Cut clustering, MinMax Cut clustering [31], Ratio Cut clustering [32], Normalized Cut Clustering [33] and Balanced Min-Cut clustering on the nine benchmark datasets.",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 31,
      "context" : "To evaluate performance of the proposed balanced min-cut clustering algorithm, we compare it to the classical Min-Cut clustering, MinMax Cut clustering [31], Ratio Cut clustering [32], Normalized Cut Clustering [33] and Balanced Min-Cut clustering on the nine benchmark datasets.",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 32,
      "context" : "To evaluate performance of the proposed balanced min-cut clustering algorithm, we compare it to the classical Min-Cut clustering, MinMax Cut clustering [31], Ratio Cut clustering [32], Normalized Cut Clustering [33] and Balanced Min-Cut clustering on the nine benchmark datasets.",
      "startOffset" : 211,
      "endOffset" : 215
    } ],
    "year" : 2014,
    "abstractText" : "Clustering is an effective technique in data mining to generate groups that are the matter of interest. Among various clustering approaches, the family of k-means algorithms and min-cut algorithms gain most popularity due to their simplicity and efficacy. The classical k-means algorithm partitions a number of data points into several subsets by iteratively updating the clustering centers and the associated data points. By contrast, a weighted undirected graph is constructed in min-cut algorithms which partition the vertices of the graph into two sets. However, existing clustering algorithms tend to cluster minority of data points into a subset, which shall be avoided when the target dataset is balanced. To achieve more accurate clustering for balanced dataset, we propose to leverage exclusive lasso on k-means and min-cut to regulate the balance degree of the clustering results. By optimizing our objective functions that build atop the exclusive lasso, we can make the clustering result as much balanced as possible. Extensive experiments on several large-scale datasets validate the advantage of the proposed algorithms compared to the state-of-the-art clustering algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}