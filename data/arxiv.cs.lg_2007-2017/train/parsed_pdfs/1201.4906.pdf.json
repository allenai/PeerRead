{
  "name" : "1201.4906.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adaptive Shortest-Path Routing under Unknown and Stochastically Varying Link States",
    "authors" : [ "Keqin Liu", "Qing Zhao" ],
    "emails" : [ "kqliu@ucdavis.edu", "qzhao@ucdavis.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 1.\n49 06\nv1 [\ncs .N\nI] 2\n4 Ja\nn 20\nWe consider the adaptive shortest-path routing problem in wireless networks under unknown and stochastically varying link states. In this problem, we aim to optimize the quality of communication between a source and a destination through adaptive path selection. Due to the randomness and uncertainties in the network dynamics, the quality of each link varies over time according to a stochastic process with unknown distributions. After a path is selected for communication, the aggregated quality of all links on this path (e.g., total path delay) is observed. The quality of each individual link is not observable. We formulate this problem as a multi-armed bandit with dependent arms. We show that by exploiting arm dependencies, a regret polynomial with network size can be achieved while maintaining the optimal logarithmic order with time. This is in sharp contrast with the exponential regret order with network size offered by a direct application of the classic MAB policies that ignore arm dependencies. Furthermore, our results are obtained under a general model of link-quality distributions (including heavy-tailed distributions) and find applications in cognitive radio and ad hoc networks with unknown and dynamic communication environments.\nI. INTRODUCTION\nWe consider a wireless network where the quality state of each link is unknown and stochastically varying over time. Specifically, the state of each link is modeled as a random cost (or reward) evolving i.i.d. over time under an unknown distribution. At each time, a path from the source to the destination is selected as the communication route and the total end-to-end cost, given by the sum of the costs of all links on the path, is subsequently observed. The cost of each individual link is not observable. The objective is to design the optimal sequential path selection policy to minimize the long-term total cost.\n2"
    }, {
      "heading" : "A. Stochastic Online Learning based on Multi-Armed Bandit",
      "text" : "The above problem can be modeled as a generalized Multi-Armed Bandit (MAB) with dependent arms. In the classic MAB [1]–[6], there are N independent arms and a player needs to decide which arm to play at each time. An arm, when played, offers a random cost drawn from an unknown distribution. The performance of a sequential arm selection policy is measured by regret, defined as the difference in expected total cost with respect to the optimal policy in the ideal scenario of known cost models where the player always plays the best arm. The optimal regret was shown to be logarithmic with time in [2]. Furthermore, since cost observations from one arm do not provide information about the cost of other arms, the optimal regret grows linearly with the number of arms. We can model the adaptive routing problem as an MAB by treating each path as an arm. The difference is that paths are dependent through shared links. While the dependency across paths can be ignored in learning and the policies for the classic MAB directly apply, such a naive approach yields poor performance with a regret growing linearly with the number of paths, thus exponentially with the network size (i.e., the number of links in the worst case).\nIn this paper, we show that by exploiting the structure of the path dependencies, a regret polynomial with network size can be achieved while preserving its optimal logarithmic order with time. Specifically, we propose an algorithm that achieves an O(d3 log T ) regret for all light-tailed cost distributions, where d ≤ m is the dimension of the path set, m the number of links, and T the length of time horizon. We further show that a modification to the proposed algorithm leads to a regret linear with d by sacrificing an arbitrarily small regret order with time. This result allows a performance tradeoff in terms of the network size and the time horizon. For example, the algorithm with a smaller regret order in d can perform better over a finite time horizon when the network size is large. For heavy-tailed cost distributions, a regret linear with the number of edges and sublinear with time can be achieved. We point out that any regret sublinear with time implies the convergence of the time-average cost to the minimum one of the best path.\nWe further generalize the adaptive routing problem to stochastic online linear optimization problems, where the action space is a compact subset of Rd and the random cost function is linear on the actions. We show that for all light-tailed cost functions, regret polynomial or linear with d and sublinear with time can be achieved. We point out that for cases where there exists a nonzero gap in the expected cost between the optimal action and the rest of the action space (e.g., when the action set is a polytope or finite), the same regret orders obtained for the adaptive routing problem can be achieved.\n3"
    }, {
      "heading" : "B. Applications",
      "text" : "One application example is adaptive routing in cognitive radio networks where secondary users communicate by exploiting channels temporarily unoccupied by primary users. In this case, the availability of each link dynamically varies according to the communication activities of nearby primary users. The delay on each link can thus be modeled as a stochastic process unknown to the secondary users. The objective is to route through the path with the smallest latency (i.e., the lightest primary traffic) through stochastic online learning.\nOther applications include ad hoc networks where link states vary stochastically due to channel fading\nor random contentions with other users."
    }, {
      "heading" : "C. Related Work",
      "text" : "The classic MAB was addressed by Lai and Robbins in 1985, where they showed that the minimum regret has a logarithmic order with time and proposed specific policies to asymptotically achieve the optimal regret for several cost distributions in the light-tailed family [2]. Since Lai and Robbins’s seminar work, simpler index policies were proposed for different classes of light-tailed cost distributions to achieve the logarithmic regret order with time [3]–[5]. In [6], we proposed the DSEE approach that achieves the logarithmic regret order with time for all light-tailed cost distributions and sublinear regrets with time for heavy-tailed cost distributions. All regrets established in [2]–[6] grow linearly with the number of arms. This work builds upon the general DSEE structure proposed in [6]. By incorporating the exploitation of the arm dependencies into DSEE, we show that the learning efficiency can be significant improved in terms of network size while preserved in terms of time.\nThis work generalizes the previous work [7]–[9] that assumes fully observable link costs on a chosen path. In particular, the adaptive routing problem under this more informative observation model was considered in [9] and an algorithm was proposed to achieve O(m4 log T ) regret, where m is the number of links in the network. The problems considered in this paper were also studied under an adversarial bandit model in which the cost functions are chosen by an adversary and are treated as arbitrary bounded deterministic quantities [10]. Algorithms were proposed to achieve regrets sublinear with time and polynomial with network size. The problem formulation and results established in this paper can be considered as a stochastic version of those in [10].\nFor the special class of stochastic online linear optimization problems where there exists a nonzero gap in expected cost between the optimal action and the rest of the action space and the cost has a finite support, an algorithm was proposed in [11] to achieve an O(d2 log3 T ) regret given that a nontrivial\n4 lower bound on the gap is known. The algorithm proposed in this paper achieves a better regret in terms of both d and T without any knowledge on the cost model. For the general case with finite-support cost distributions, the regret was shown to be lower bounded by O(d √ T ) and an efficient algorithm was proposed to achieve an O((d ln T )3/2 √ T ) regret [11]. Compared to the algorithm in [11], our algorithm for the general stochastic online linear optimization problems performs worse in T but better in d."
    }, {
      "heading" : "II. PROBLEM STATEMENT",
      "text" : "In this section, we address the adaptive routing problem under unknown and stochastically time-varying link states. Consider a network with a source s and a destination r. Let G = (V,E) denote the directed graph consisting of all simple paths from s to r. Let m and n denote, respectively, the number of edges and vertices in graph G.\nAt each time t, a random weight/cost We(t) drawn from an unknown distribution is assigned to each edge in E. We assume that {We(t)} are i.i.d. over time for each edge e. At the beginning of each time slot t, a path pn ∈ P (i ∈ {1, 2, . . . , |P|}) from s to r is chosen, where P is the set of all paths from s to r in G. Subsequently, the total end-to-end cost Ct(pn) of the path pn, given by the sum of the weights of all edges on the path, is revealed in the end of the slot. We point out that the individual cost on each edge e ∈ E is unobservable. The regret of a path selection policy π is defined as the expected extra cost incurred over time T compared to the optimal policy that always selects the best path (i.e., the path with the minimum expected total end-to-end cost). The objective is to design a path selection policy π to minimize the growth rate of the regret. Let σ be a permutation on all paths such that\nE[Ct(σ(1))] ≤ E[Ct(σ(1))] ≤ E[Ct(σ(|P|))].\nWe define regret\nRπ(T ) ∆=E[ T ∑\nt=1\n(Ct(π)− Ct(σ(1)))],\nwhere Ct(π) denotes the total end-to-end cost of the selected path under policy π at time t."
    }, {
      "heading" : "III. ADAPTIVE SHORTEST PATH ROUTING ALGORITHM",
      "text" : "In this section, we present the proposed adaptive shortest-path routing algorithm. We consider both\nlight-tailed and heavy-tailed cost distributions.\n5"
    }, {
      "heading" : "A. Light-Tailed Cost Distributions",
      "text" : "We consider the light-tailed cost distributions as defined below. Definition 1: A random variable X is light-tailed if its moment-generating function exists, i.e., there\nexists a u0 > 0 such that\nM(u) ∆ =E[exp(uX)] < ∞ ∀ u ≤ |u0|;\notherwise X is heavy-tailed.\nFor a zero-mean light-tailed random variable X, we have [12],\nM(u) ≤ exp(ζu2/2), ∀ u ≤ |u0|, ζ ≥ sup{M (2)(u), − u0 ≤ u ≤ u0}, (1)\nwhere M (2)(·) denotes the second derivative of M(·) and u0 the parameter specified in Definition 1. From (1), we have the following extended Chernoff-Hoeffding bound on the deviation of the sample mean from the true mean for light-tailed random variables [6].\nLet {X(t)}∞t=1 be i.i.d. light-tailed random variables. Let Xs = (Σst=1X(t))/s and θ = E[X(1)]. We have, for all δ ∈ [0, ζu0], a ∈ (0, 1/(2ζ)],\nPr(|Xs − θ| ≥ δ) ≤ 2 exp(−aδ2s). (2)\nNote that the path cost is the sum of the costs of all edges on the path. Since the number of edges in a path is upper bounded by m, the bound on the moment generating function in (1) holds on the path cost by replacing ζ by mζ , and so does the Chernoff-Hoeffding bound in (2).\nIn the following, we propose an algorithm to achieve a regret polynomial with m and logarithmic with T . We first represent each path pn as a vector ~pn with m entries consisting of 0s and 1s representing whether or not an edge is on the path. The vector space of all paths is embedded in a d-dimensional (d ≤ m) subspace1 of Rm. The cost on each path pn at time t is thus given by the linear function\n[W1(t),W2(t), . . . ,Wm(t)] · ~pn.\nThe general structure of the algorithm follows the DSEE framework established in [6] for the classic MAB. More specifically, we partition time into an exploration sequence and an exploration sequence. In the exploration sequence, we sample the d basis vectors (barycentric spanner [10] as defined in the following) evenly to estimate the qualities of these vectors. In the exploitation sequence, we select the action estimated as the best by linearly interpolating the estimated qualities of the basis vectors. A detailed algorithm is given in Fig. 1.\n1If graph G is acyclic, then d = m− n+ 2.\n6 Definition 2: A set B = {x1, . . . , xd} is a barycentric spanner for a d-dimensional set A if every x ∈ A can be expressed as a linear combination of elements of B using coefficients in [−1, 1].\nNote that a compact subset of Rd always has a barycentric spanner, which can be constructed efficiently (see an algorithm in [10]). For the problem at hand, the path set P is a compact subset of Rd with dimension d ≤ m. We can thus construct a barycentric spanner consisting of d paths in P by using the algorithm in [10].\nTheorem 1: Construct an exploration sequence as follows. Let a, ζ, u0 be the constants such that (2)\nholds on each edge cost. Choose a constant b > 2m/a, a constant\nc ∈ (0, min j:E[Ct(σ(j))−Ct(σ(1))]>0 {E[Ct(σ(j)) − Ct(σ(1))]}),\nand a constant w ≥ max{b/(mdζu0)2, 4b/c2}. For each t > 1, if |A(t−1)| < d⌈d2w log t⌉, then include t in A(t). Under this exploration sequence, the resulting policy π∗ has regret\nRπ ∗ (T ) ≤ Amd3 log T\nfor some constant A independent of d, m and T > 1.\n7 Proof: Since A(T ) ≤ d⌈d2w log T ⌉, the regret caused in the exploration sequence is at the order of md3 log T . Now we consider the regret caused in the exploitation sequence. Let Ek denote the kth exploitation period which is the kth contiguous segment in the exploitation sequence. Let Ek denote the kth exploitation period. Similar to the proof of Theorem 3.1 in [6], we have\n|Ek| ≤ htk (3)\nfor some constant h independent of d and m. Let tk > 1 denote the starting time of the kth exploitation period. Next, we show that by applying the Chernoff-Hoeffding bound in (2) on the path cost, for any t in the kth exploitation period and i = 1, . . . , d, we have\nPr(|E[Ct(pi)]− θpi(t)| ≥ c/(2d)) ≤ 2t −ab/m k .\nTo show this, we define the parameter ǫi(t) ∆ = √ b log t/τi(t), where τi(t) is the number of times that path i has been sampled up to time t. From the definition of parameter b, we have\nǫi(t) ≤ min{mζu0, c/(2d)}. (4)\nApplying the Chernoff-Hoeffding bound, we arrive at\nPr(|E[Ct(pi)]− θpi(t)| ≥ c/(2d)) ≤ Pr(|E[Ct(pi)]− θpi(t)| ≥ ǫi(t)) ≤ 2t −ab/m k .\nIn the exploitation sequence, the expected times that at least one path in B has a sample mean deviating from its true mean cost by c/(2d) is thus bounded by\nΣ∞k=12dt −ab/m k tk ≤ Σ∞t=12dt1−ab/m = gd. (5)\nfor some constant g independent of d and m. Based on the property of the barycentric spanner, the best path would not be selected in the exploitation sequence only if one of the basis vector in B has a sample mean deviating from its true mean cost by at least c/(2d). We thus proved the theorem.\nIn Theorem 1, we need a lower bound (parameter c) on the difference in the cost means of the best and the second best paths. We also need to know the bounds on parameters ζ and u0 such that the ChernoffHoeffding bound (2) holds. These bounds are required in defining w that specifies the minimum leading constant of the logarithmic cardinality of the exploration sequence necessary for identifying the best path. Similar to [6], we can show that without any knowledge of the cost models, increasing the cardinality of the exploration sequence of π∗ by an arbitrarily small amount leads to a regret linear with d and arbitrarily close to the logarithmic order with time.\n8 Theorem 2: Let f(t) be any positive increasing sequence with f(t) → ∞ as t → ∞. Revise policy π∗ in Theorem 1 as follows: include t (t > 1) in A(t) if |A(t− 1)| < d⌈f(t) log t⌉. Under the revised policy π′, we have\nRπ ′ (T ) = O(df(T ) log T ).\nProof: It is sufficient to show that the regret caused in the exploitation sequence is bounded by O(d),\nindependent of T . Since the exploration sequence is denser than the logarithmic order as in Theorem 1, it is not difficult to show that the bound on |Ek| given in (3) still holds with a different value of h. We consider any positive increasing sequence b(t) such that b(t) = o(f(t)) and b(t) → ∞ as t → ∞. By replacing b in the proof of Theorem 1 with b(t), we notice that after some finite time T0, the parameter ǫi(t) will be small enough to ensure (4) holds and b(t) will be large enough to ensure (5) holds. The proof thus follows."
    }, {
      "heading" : "B. Heavy-Tailed Cost Distributions",
      "text" : "We now consider the heavy-tailed cost distributions where the moment of the cost exists up to the q-th (q > 1) order. From [6], we have the following bound on the deviation of the sample mean from the true mean for heavy-tailed cost distributions.\nLet {X(t)}∞t=1 be i.i.d. random variables drawn from a distribution with the q-th moment (q > 1). Let Xt = (Σ t k=1X(k))/t and θ = E[X(1)]. We have, for all ǫ > 0,\nPr(|X t − θ| > ǫ) = o(t1−q). (6)\nTheorem 3: Construct an exploration sequence as follows. Choose a constant v > 0. For each t > 1, if |A(t − 1)| < vt1/q , then include t in A(t). Under this exploration sequence, the resulting policy πq has regret Rπ q (T ) ≤ DdT 1/q for some constant D independent of d and T .\nProof: Based on the construction of the exploration sequence, it is sufficient to show that the regret\nin the exploitation sequence is o(T 1/q) · d. From (6), we have, for any i = 1, . . . , d,\nPr(|E[Ct(pi)]− θpi(t)| ≥ c/(2d)) = o(|A(t)|1−q).\nFor any exploitation slot t ∈ A(t), we have |A(t)| ≥ vt1/q. We arrive at\nPr(|E[Ct(pi)]− θpi(t)| ≥ c/(2d)) = o(t(1−q)/q).\nSince the best path will not be chosen only if at least one of the basis vector has the sample deviating from the true mean by c/(2d), the regret in the exploitation sequence is thus bounded by T ∑\nt=1\no(t(1−q)/q) · d = o(T 1/q) · d.\n9"
    }, {
      "heading" : "IV. GENERALIZATION TO STOCHASTIC ONLINE LINEAR OPTIMIZATION PROBLEMS",
      "text" : "In this section, we consider the general Stochastic Online Linear Optimization (SOLO) problems that include adaptive routing as a special case. In a SOLO problem, there is a compact set U ⊂ Rd with dimension d, referred to as the action set. At each time t, we choose a point x ∈ U and observe a cost ~Ct ·x where ~Ct ∈ Rd is drawn from an unknown distribution2. We assume that {~Ct}t≥1 are i.i.d. over t. The objective is to design a sequential action selection policy π to minimize the regret Rπ(T ), defined as the total difference in the expected cost over T slots compared to the optimal action x∗ ∆ = argmin{E[ ~Ct ·x]}:\nRπ(T ) ∆=E[ T ∑\nt=1\n( ~Ct · x(π)− ~Ct · x∗)],\nwhere x(π) denotes the action under policy π at time t.\nAs a special case, the action space of the adaptive routing problem consists of all path vectors with dimension d ≤ m. The main difficulty in a general SOLO problem is on identifying the optimal action in an infinite action set. Our basic approach is to implement the shortest-path algorithm in Fig. 1 repeatedly for the chosen action to gradually converge to the optimal action. Specifically, the proposed algorithm runs under an epoch structure where each epoch simulates one round of the algorithm and the epoch lengths form a geometric progression.\nTheorem 4: Assume that the cost for each action is light-tailed. Let Tk be the length of the k-th epoch. Construct an exploration sequence in this epoch as follows. Let a, ζ, u0 be the constants such that (2) holds on each cost. Choose a constant b > 2/a, a constant c = log T 1/3k /T 1/3 k , and a constant w ≥ max{b/(dζu0)2, 4b/c2}. For each t > 1, if |A(t− 1)| < d⌈d2w log t⌉, then include t in A(t). The resulting policy πg has regret\nRπ g (T ) = O(d3T 2/3 log1/3 T ).\nWe point out that the regret order can be improved to linear with d by sacrificing the order with T by an arbitrarily small amount, as similar to Theorem 2."
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "In this paper, we considered the adaptive routing problem in networks with unknown and stochastically varying link states, where only the total end-to-end cost of a path is observable after the path is selected\n2The following result holds in a more general scenario that only requires the expected cost is linear with x.\n10\nfor routing. For both light-tailed and heavy-tailed link-state distributions, we proposed efficient online learning algorithms to minimize the regret in terms of both time and network size. The result was further extended to the general stochastic online liner optimization problems. Future work includes extending the i.i.d. cost evolution over time to more general stochastic processes."
    } ],
    "references" : [ {
      "title" : "Some Aspects of the Sequential Design of Experiments",
      "author" : [ "H. Robbins" ],
      "venue" : "Bull. Amer. Math. Soc., vol. 58, no. 5, pp. 527-535, 1952.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1952
    }, {
      "title" : "Asymptotically Efficient Adaptive Allocation Rules",
      "author" : [ "T. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathematics, vol. 6, no. 1, pp. 4-22, 1985.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Adaptive Treatment Allocation and The Multi-Armed Bandit Problem",
      "author" : [ "T. Lai" ],
      "venue" : "Ann. Statist., vol 15, pp. 1091-1114, 1987.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Sample Mean Based Index Policies with O(logn) Regret for the Multi-armed Bandit Problem",
      "author" : [ "R. Agrawal" ],
      "venue" : "Advances in Applied Probability, vol. 27, pp. 1054-1078, 1995.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Finite-time Analysis of the Multiarmed Bandit Problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning, vol. 47, pp. 235-256, 2002.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Multi-Armed Bandit Problems with Heavy Tail Reward Distributions",
      "author" : [ "K. Liu", "Q. Zhao" ],
      "venue" : "Proc. of Allerton Conference on Communications, Control, and Computing, September, 2011.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Efficient Algorithms for Online Decision Problems",
      "author" : [ "A. Kalai", "S. Vempala" ],
      "venue" : "J. Computer and System Sciences, vol. 71, 291-307, 2005.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Path Kernels and Multiplicative Updates",
      "author" : [ "E. Takimoto", "M. Warmuth" ],
      "venue" : "Journal of Machine Learning Research, vol. 4, pp. 773-818, 2003.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Combinatorial Network Optimization with Unknown Variables: Multi-Armed Bandits with Linear Rewards",
      "author" : [ "Y. Gai", "B. Krishnamachari", "R. Jain" ],
      "venue" : "USC Technical Report, CENG-2010-9.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Online Linear Optimization and Adaptive Routing",
      "author" : [ "B. Awerbuch", "R. Kleinberg" ],
      "venue" : "Journal of Computer and System Sciences, pp. 97-114, 2008.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Stochastic Linear Optimization under Bandit Feedback",
      "author" : [ "V. Dani", "T. Hayes", "S. Kakade" ],
      "venue" : "Proc. of the 21st Annual Conference on Learning Theory, 2008.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Locally Sub-Gaussian Random Variable and the Strong Law of Large Numbers",
      "author" : [ "P. Chareka", "O. Chareka", "S. Kennendy" ],
      "venue" : "Atlantic Electronic Journal of Mathematics, vol. 1, no. 1, pp. 75-81, 2006.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In the classic MAB [1]–[6], there are N independent arms and a player needs to decide which arm to play at each time.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "In the classic MAB [1]–[6], there are N independent arms and a player needs to decide which arm to play at each time.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "The optimal regret was shown to be logarithmic with time in [2].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "The classic MAB was addressed by Lai and Robbins in 1985, where they showed that the minimum regret has a logarithmic order with time and proposed specific policies to asymptotically achieve the optimal regret for several cost distributions in the light-tailed family [2].",
      "startOffset" : 268,
      "endOffset" : 271
    }, {
      "referenceID" : 2,
      "context" : "Since Lai and Robbins’s seminar work, simpler index policies were proposed for different classes of light-tailed cost distributions to achieve the logarithmic regret order with time [3]–[5].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 4,
      "context" : "Since Lai and Robbins’s seminar work, simpler index policies were proposed for different classes of light-tailed cost distributions to achieve the logarithmic regret order with time [3]–[5].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 5,
      "context" : "In [6], we proposed the DSEE approach that achieves the logarithmic regret order with time for all light-tailed cost distributions and sublinear regrets with time for heavy-tailed cost distributions.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 1,
      "context" : "All regrets established in [2]–[6] grow linearly with the number of arms.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "All regrets established in [2]–[6] grow linearly with the number of arms.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "This work builds upon the general DSEE structure proposed in [6].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "This work generalizes the previous work [7]–[9] that assumes fully observable link costs on a chosen path.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "This work generalizes the previous work [7]–[9] that assumes fully observable link costs on a chosen path.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "In particular, the adaptive routing problem under this more informative observation model was considered in [9] and an algorithm was proposed to achieve O(m4 log T ) regret, where m is the number of links in the network.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "The problems considered in this paper were also studied under an adversarial bandit model in which the cost functions are chosen by an adversary and are treated as arbitrary bounded deterministic quantities [10].",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 9,
      "context" : "The problem formulation and results established in this paper can be considered as a stochastic version of those in [10].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "For the special class of stochastic online linear optimization problems where there exists a nonzero gap in expected cost between the optimal action and the rest of the action space and the cost has a finite support, an algorithm was proposed in [11] to achieve an O(d2 log T ) regret given that a nontrivial",
      "startOffset" : 246,
      "endOffset" : 250
    }, {
      "referenceID" : 10,
      "context" : "For the general case with finite-support cost distributions, the regret was shown to be lower bounded by O(d √ T ) and an efficient algorithm was proposed to achieve an O((d ln T )3/2 √ T ) regret [11].",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 10,
      "context" : "Compared to the algorithm in [11], our algorithm for the general stochastic online linear optimization problems performs worse in T but better in d.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "For a zero-mean light-tailed random variable X, we have [12],",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "From (1), we have the following extended Chernoff-Hoeffding bound on the deviation of the sample mean from the true mean for light-tailed random variables [6].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 5,
      "context" : "The general structure of the algorithm follows the DSEE framework established in [6] for the classic MAB.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "In the exploration sequence, we sample the d basis vectors (barycentric spanner [10] as defined in the following) evenly to estimate the qualities of these vectors.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "Note that a compact subset of Rd always has a barycentric spanner, which can be constructed efficiently (see an algorithm in [10]).",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "We can thus construct a barycentric spanner consisting of d paths in P by using the algorithm in [10].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "1 in [6], we have",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "Similar to [6], we can show that without any knowledge of the cost models, increasing the cardinality of the exploration sequence of π by an arbitrarily small amount leads to a regret linear with d and arbitrarily close to the logarithmic order with time.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : "From [6], we have the following bound on the deviation of the sample mean from the true mean for heavy-tailed cost distributions.",
      "startOffset" : 5,
      "endOffset" : 8
    } ],
    "year" : 2012,
    "abstractText" : "We consider the adaptive shortest-path routing problem in wireless networks under unknown and stochastically varying link states. In this problem, we aim to optimize the quality of communication between a source and a destination through adaptive path selection. Due to the randomness and uncertainties in the network dynamics, the quality of each link varies over time according to a stochastic process with unknown distributions. After a path is selected for communication, the aggregated quality of all links on this path (e.g., total path delay) is observed. The quality of each individual link is not observable. We formulate this problem as a multi-armed bandit with dependent arms. We show that by exploiting arm dependencies, a regret polynomial with network size can be achieved while maintaining the optimal logarithmic order with time. This is in sharp contrast with the exponential regret order with network size offered by a direct application of the classic MAB policies that ignore arm dependencies. Furthermore, our results are obtained under a general model of link-quality distributions (including heavy-tailed distributions) and find applications in cognitive radio and ad hoc networks with unknown and dynamic communication environments.",
    "creator" : "LaTeX with hyperref package"
  }
}