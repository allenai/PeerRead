{
  "name" : "1704.04530.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Neural Extractive Summarization with Side Information",
    "authors" : [ "Shashi Narayan", "Nikos Papasarantopoulos", "Mirella Lapata", "Shay B. Cohen" ],
    "emails" : [ "shashi.narayan@ed.ac.uk", "nikos.papasa@ed.ac.uk", "mlap@inf.ed.ac.uk", "scohen@inf.ed.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "With an increased access to information and a massive growth in the global news data, there is a growing demand from readers to spot emerging trends, person mentions and evolution of storylines in the news. A vast majority of this news data contains textual documents, driving the need for automatic document summarization systems aiming at acquiring key points in the form of a short summary from one or more documents.\nWhile it is not so challenging for us humans to summarize text, automatic summarization systems struggle with producing high quality summaries. Such systems require wide-coverage text understanding which unfortunately NLP technology has not developed yet. Most work in automatic sum-\nmarization has focused on extractive summarization, where a summary is created by identifying and subsequently concatenating salient sentences in a document.\nMost extractive methods often focus on the main body of the document from which sentences are extracted. Traditional methods manually define features which are local in the context of each sentence or a set of sentences which form the body of the document. Such features include sentence position and length (Radev et al., 2004), keywords and the presence of proper nouns (Mani, 2001; Spärck Jones, 2007; Kupiec et al., 1995), frequency information such as content word frequency, composition functions for estimating sentence importance from word frequency, and the adjustment of frequency weights based on context (Nenkova et al., 2006) and low-level event-based features describing relationships between important actors in a document (Filatova, 2004). Sentences are ranked for extraction based on the overlap with features. Recent deep learning methods circumvent human-engineered features using continuous sentence features. Kågebäck et al. (2014) and Yin and Pei (2015) map sentences to a continuous vector space which is used for similarity measurement to reduce the redundancy in the generated summaries. Cheng and Lapata (2016) and Nallapati et al. (2016b) used recurrent neural networks to read sequences of sentences to get a document representation which they use to label each sentence for extraction. These methods report results comparable to the state of the art without using any kind of linguistic annotation.\nIt is a challenging task to rely only on the main body of the document for extraction cues, as it requires document understanding. Documents in practice often have side information, such as title, image captions, videos, images and twitter handles, along with the main body of the document.\nar X\niv :1\n70 4.\n04 53\n0v 1\n[ cs\n.C L\n] 1\n4 A\npr 2\n01 7\nThese types of side information are often available for newswire articles. Figure 1 shows an example of a newswire article taken from CNN (CNN.com). It shows the side information such as the title (first block) and the images with their captions (third block) along with the main body of the document (second block). The last block shows the manually written summary of the document in terms of “highlights” to allow readers to quickly gather information on stories. As one can see in this example, gold highlights focus on sentences from the fourth paragraph, i.e., on the key events such as the “PM’s resignation”, “bribery scandal and its investigation”, “suicide” and “leaving an important note”. Interestingly, the essence of the article is explicitly or implicitly mentioned in the title and the image captions of the document.\nIn this paper, we develop a general framework for single-document summarization with side information. Our model includes a neural networkbased hierarchical document encoder and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Nallapati et al. (2016b), in that it derives the document meaning representation from its sentences and their constituent words. We also use recurrent neural networks to read the sequence of sentences in the document. Our novel sentence extractor combines this document meaning representation with an attention mechanism (Bahdanau et al., 2014) over the side information to select sentences of the input document as the output summary. Our attention mechanism differs from both the standard attention mechanism (Bahdanau et al., 2014) which is used to locate the region of focus in the input, and the mechanism of Cheng and Lapata (2016) which directly extracts salient sentences after reading them. Instead, we use the attention mechanism to locate the region of focus in the side information.\nThe idea of using additional information to improve extractive summarization is less explored. Previous work has discussed the importance of manually defined features using the title words, e.g., words in the title, and various pragmatic information for summarization. Edmundson (1969) used subjectively weighted combination of these human-engineered features, whereas Kupiec et al. (1995) and Mani (2001) trained their feature weights using a corpus. We explore the advantages of side information in a neural networkbased summarization framework. Our proposed framework does not use any human-engineered features and could exploit different types of side information. In this paper, we conceptualize side information as the title of the document and the image captions present in the document.1\nWe evaluate our models both automatically (in terms of ROUGE scores) and by conducting human evaluations on the CNN news highlights dataset (Hermann et al., 2015). Our results show that our summarizer informed with side information performs consistently better than ones that do not use any side information.\n1We focus on textual side information. There are studies which show that non-textual side information could be useful as well (e.g. Hitschler et al., 2016) in NLP. However, we leave non-textual side information for future work."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "In this section we formally define our extractive summarization problem with side information. Given a document D consisting of a sequence of sentences (s1, s2, ..., sn) and a sequence of pieces of side information (c1, c2, ..., cp) , we produce a summary S of D by selecting m sentences from D (where m < n). We judge each sentence si for its relevance in the summary and label it with yi ∈ 0, 1 where 1 indicates that si should be considered for the summary and 0, otherwise. In this paper, we approach this problem in a supervised setting where we aim to maximize the likelihood of the set of labels Y = (y1, y2, ..., yn) given the input document D and model parameters θ:\nP (Y |D ; θ) = n∏ i P (yi|D ; θ) (1)"
    }, {
      "heading" : "3 Neural Extractive Summarization",
      "text" : "We model our extractive summarization framework with a hierarchical encoder-decoder architecture assembled by recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The main components of our model are a convolutional neural network sentence encoder, a recurrent neural network document encoder and an attention-based recurrent neural network sentence extractor. Our model exploits the compositionality of the document. It reflects that a document is built of a meaningful sequence of sentences and each sentence is built of a meaningful sequence of words. With that in mind, we first obtain continuous representations of sentences by applying single-layer convolutional neural networks over sequences of word embeddings and then we rely on a recurrent neural network to compose sequence of sentences to get document embeddings. We model extractive summarization as a sequence labelling problem using a standard encoder-decoder architecture (Cho et al., 2014; Sutskever et al., 2014). First, the encoder reads the sequence of sentences (s1, s2, ..., sn) in D and then, the decoder generates a sequence of labels (y1, y2, ..., yn) labelling each sentence in D . Figure 2 presents the layout of our model. In the following, we explain the main components of our model in detail."
    }, {
      "heading" : "3.1 Sentence Encoder",
      "text" : "One core component of our hierarchical model is a convolutional sentence encoder which encodes sentences (from the main body and the side information) into continuous representations.2 CNNs (LeCun et al., 1990) have shown to be very effective in computer vision (Krizhevsky et al., 2012) and for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016). We chose CNNs in our framework for the following reasons. First, singlelayer CNNs can be trained effectively and second, CNNs have been shown to be effective in identifying salient patterns in the input depending on the task. For example, for the caption generation task (Xu et al., 2015), CNNs successfully identify salient objects in the image for the corresponding words in the caption. We believe that CNNs\n2We tried sentence/paragraph vector (Le and Mikolov, 2014) to infer sentence embeddings in advance, but the results were inferior to those presented in this paper with CNNs.\ncan similarly identify salient terms, e.g., namedentites and events, in sentences that correlate with the gold summary. This should in turn (i) optimize intermediate document representations in both of our document encoder (§3.2) and sentence extractor (§3.3) and (ii) assist the attention mechanism (§3.3) to correlate salient information in the side information and sentences, for extractive summarization.\nOur model is a variant of the models presented by Collobert et al. (2011), Kim (2014) and Cheng and Lapata (2016). A sentence s of length k in D can be represented as a dense matrix W = [w1 ⊕ w2 ⊕ . . . ⊕ wk] ∈ Rk×d where wi ∈ Rd is the word embedding of the ith word in s and ⊕ is the concatenation operator. We apply a temporal narrow convolution by applying a kernel filter K ∈ Rh×d of width h to a window of h words in s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f = [f1, f2, . . . , fk−h+1] ∈ Rk−h+1 where fi is defined as:\nfi = ReLU(K ◦Wi:i+h−1 + b) (2)\nwhere, ◦ is the Hadamard Product followed by a sum over all elements, ReLU is a rectified linear activation3 and b ∈ R is a bias term. We use the ReLU activation function to accelerate the convergence of stochastic gradient descent compared to sigmoid or tanh functions (Krizhevsky et al., 2012). We then apply max pooling over time (Collobert et al., 2011) over the feature map f and get fmax = max(f) as the feature corresponding to this particular filter K. Max-pooling is followed by local response normalization for better generalization (Krizhevsky et al., 2012). We use multiple kernelsKh of width h to compute a list of features fKh . In addition, we use kernels of varying widths to learn a set of feature lists (fKh1 , fKh2 , . . .). We concatenate all feature lists to get the final sentence representation.4\nThe bottom part of Figure 2 briefly presents our convolutional sentence encoder. Kernels of sizes 2 (shown in red) and 4 (shown in blue) are applied 3 times each. The max pooling over time operation leads to two feature lists fK2 and fK4 ∈ R3. The final sentence embeddings have six dimensions.\n3We use the softplus function: ReLU(x) = ln(1 + ex). 4Cheng and Lapata (2016) sum over feature lists to get the final sentence embedding. In contrast, we follow Kim et al. (2016) and concatenate them. This seems to work best in our settings."
    }, {
      "heading" : "3.2 Document Encoder",
      "text" : "The document encoder (shown in Figure 2, top left) composes a sequence of sentences to get a document representation. The sentence extractor (§3.3), along with attending the side information, crucially exploits the document representation to identify the local and global importance of a sentence in the document to make a decision on whether it should be considered for the summary.\nWe use a recurrent neural network with Long Short-Term Memory (LSTM) cells to avoid the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997). Given a document D consisting of a sequence of sentences (s1, s2, ..., sn), we follow the common practice of feeding sentences in the reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015). This way we make sure that the network does not omit top sentences of the document which are particularly important for summarization (Rush et al., 2015; Nallapati et al., 2016a). At time step t, the hidden state ht = LSTM(sn−t+t, ht−1) is updated as:\nft it ot c̃t\n =  σ σ σ\ntanh\nW · [ ht−1sn−t+1 ]\n(3)\nct = ft ct−1 + it c̃t (4) ht = ot tanh(ct) (5)\nwhere the operator denotes element-wise multiplication and W are the learned parameters of the model."
    }, {
      "heading" : "3.3 Sentence Extractor",
      "text" : "Our sentence extractor (Figure 2, top right) labels each sentence in the document with labels 1 or 0 by implicitly estimating its relevance in the document and by directly attending to the side information for importance cues. It is implemented with another recurrent neural network with LSTM cells and an attention mechanism (Bahdanau et al., 2014). Our attention mechanism differs from the standard practice of attending intermediate states of input (encoder). Instead, our extractor attends to the side information in the document for cues. Given a document D : 〈(s1, s2, ..., sn), (c1, c2, ..., cp)〉, it reads sentences in order and labels them one by one. It also reads the side information in order as they appear in the document. For example, if we consider\nthe title and the image captions as the side information, c1 will be the title and c2, . . . , cp will be the image captions as they appear in the document. Given sentence st at time step t, it returns a probability distribution over labels as:\nP (yt|st, D) = softmax(g(ht, h′t)) (6) g(ht, h ′ t) = Uo(Vhht +W ′ hh ′ t) (7)\nht = LSTM(st, ht−1) (8)\nh′t = p∑ i=1 α(t,i)ci, (9)\nwhere α(t,i) = exp(htci)∑ j exp(htcj)\n(10)\nwhere g(·) is a single-layer neural network with parameters Uo, Vh and W ′h. ht is an intermediate RNN state at time step t. The dynamic context vector h′t is essentially the weighted sum of the side information in the document. Figure 2 summarizes our model. For each labelling decision, our network considers both the encoded document meaning representation and the side information."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "This section presents our experimental setup for the assessment of our models. We discuss the training and the evaluation dataset. We also explain how we augment existing datasets with side information and describe implementation details to facilitate the replication of our results. We present a brief description of our baseline systems."
    }, {
      "heading" : "4.1 Training and Test data",
      "text" : "We need documents annotated with sentence importance information, i.e., each sentence in a document is labelled with 1 (summary-worthy) or 0 (not summary-worthy). For our purposes, we used an augmented version of the CNN dataset (Hermann et al., 2015).5\nOur dataset is an evolved version of the CNN dataset first collected by Svore et al. (2007) for highlight generation. Svore et al. (2007) noticed that CNN articles often come with “story highlights” to allow readers to quickly gather information on stories. They collected a small dataset for\n5Hermann et al. (2015) have also released the DailyMail dataset, but we do not report our results on this dataset. We found that the script written by Hermann et al. to crawl DailyMail articles mistakenly extracts image captions as part of the main body of the document. As image captions often don’t have sentence boundaries, they blend with the sentences of the document unnoticeably. This leads to the production of erroneous summaries.\nevaluation purposes. Woodsend and Lapata (2010) improved on this by collecting 9,000 articles and manually annotating them for sentence extraction. Recently, Hermann et al. (2015) crawled 93K CNN articles to build a large-scale corpus to set a benchmark for deep learning methods. Since then, this dataset has been used for single-document summarization (Nallapati et al., 2016a; Cheng and Lapata, 2016; Nallapati et al., 2016b). Cheng and Lapata (2016) annotated this dataset with Woodsend and Lapata (2010) style gold annotation. We further extended this dataset in two ways:6\nSide information We augmented each article with the side information. We used a modified script of Hermann et al. (2015) to extract title and image captions, and we associated them with the corresponding articles.\nCollective oracle The gold annotation of Woodsend and Lapata (2010) labels each sentence of the document in isolation for sentence extraction given the gold summary based on their semantic correspondence. We refer to these gold labels as SENTEXTLABELS. Positively labelled sentences using SENTEXTLABELS as whole may lead to a lengthy and redundant summary. Hence we took a different approach and annotated each sentence of a document with a collective oracle label (COLLECTIVEORACLE) to train our sentence extractor. COLLECTIVEORACLE is defined as the set of sentences from an article which collectively give highest ROUGE score with respect to the gold summary. ROUGE (Lin and Hovy, 2003), a recall-oriented metric, is often used to evaluate summarization systems. See §5.1 for more details. We approach this exponential problem of selecting the best subset of sentences using a greedy approach and add one sentence at a time to the summary such that the ROUGE score of the current summary is the highest with respect to the gold summary. We stop adding new sentences to the summary when the additions do not improve the ROUGE score or the maximum number of sentences in the summary is reached.7 Our approach is similar to previous work which tackles the problem of converting abstractive summaries to extractive ground truth (Svore et al., 2007; Nallapati et al., 2016b; Cao et al., 2016).\n6Our dataset is publicly available at http: //anonymized.com.\n7We choose maximum three sentences in the summary for the collective oracle. See an explanation for this in §5.1.\nWe train our network on the named-entityanonymized version of news articles.8 However, we generate de-anonymized summaries and evaluate them against de-anonymized gold summaries to facilitate human evaluation and to make human evaluation comparable to automatic evaluation.\nWe use the standard splits of Hermann et al. (2015) for training, validation and test datasets. This divides our dataset into training, validation and test sets of sizes 90K, 1220 and 1093 documents respectively."
    }, {
      "heading" : "4.2 Comparison Systems",
      "text" : "We compared the output of our model against the standard baseline of simply selecting the first three sentences from each document as the summary. We refer to this baseline as LEAD in the rest of the paper.\nWe also compared our system against the sentence extraction system of Cheng and Lapata (2016). We refer to this system as POINTERNET.9 It does not exploit any side information, but does have an attention mechanism to attend to sentences while reading them. As the results of their system is not available on the CNN dataset, we have implemented POINTERNET in TensorFlow.10"
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "We used our training data to train word embeddings using the Word2vec (Mikolov et al., 2013) skip-gram model with context window size 6, negative sampling size 10 and hierarchical softmax 1. For known words, word embedding variables were initialized with pre-trained word embeddings of size 200. For unknown words, embeddings were initialized to zero, but trained during training. All sentences, including titles and image captions, were padded to a sentence length of 100. For the convolutional sentence encoder, we followed Kim et al. (2016), and used a list of kernels of widths 1 to 7, each with output channel size of 50. This leads the sentence embedding size in our model to be 350. For the recurrent neural\n8We also experimented with the de-anonymized articles, but the results were inferior to those presented here with the anonymized data.\n9The neural attention architecture in Cheng and Lapata (2016) resembles the one in Pointer Networks (Vinyals et al., 2015).\n10For sanity check, we have trained our implementation of POINTERNET on the DailyMail dataset and achieved comparable results to what has been reported by Cheng and Lapata (2016).\nnetwork component in document encoder and sentence extractor, we used a single-layered LSTM network with size 600. All input documents were padded to a maximum document length of 126. For each document, we consider a maximum of 10 image captions. We experimented with various numbers (1, 3, 5, 10 and 20) of image captions and we found that our model performed best with 10 image captions. We performed a mini-batch cross-entropy training with a batch size of 20 documents for 10 training epochs. After each epoch, we evaluated our model on the validation set and chose the best performing model for the test set. We trained our models with the optimizer Adam (Kingma and Ba, 2014) with initial learning rate 0.001. Our system is fully implemented in TensorFlow (Abadi et al., 2015).11"
    }, {
      "heading" : "5 Results and Discussion",
      "text" : "We conducted both an automatic and a human evaluation. We start this section with an ablation study on the validation set. The best model from this study is chosen for the test set. In the rest of the paper, we refer to our model as SIDENET for its ability to exploit side information."
    }, {
      "heading" : "5.1 Automatic Evaluation",
      "text" : "To automatically assess the quality of our summaries, we used ROUGE (Lin and Hovy, 2003),\n11Our TensorFlow code is publicly available at http:// anonymized.com.\na recall-oriented metric, to compare our modelgenerated summaries to manually-written highlights.12 Previous work has reported ROUGE-1 (R1) and ROUGE-2 (R2) scores to access informativeness, and ROUGE-L (RL) to access fluency. In addition to R1, R2 and RL, we also report ROUGE-3 (R3) and ROUGE-4 (R4) capturing higher order n-grams overlap to assess informativeness and fluency simultaneously.\nWe report on both full length (three sentences with the top scores as the summary) and fixed length (first 75 bytes and 275 bytes as the summary) summaries. For full length summaries, our decision of selecting three sentences is guided by the fact that there are 3.11 sentences in average in the gold highlights of the training set. We conduct our ablation study on the validation set with full length ROUGE scores, but we report both fixed and full length ROUGE scores for the test set.\nWe experimented with two types of side information: title (TITLE) and image captions (CAPTION). In addition, we experimented with the first sentence (FS) of the document as side information. Note that the latter is not strictly speaking side information, it is a sentence in the document. However, we wanted to explore the idea that the first sentence of the document plays a crucial part in generating summaries (Rush et al., 2015; Nallapati et al., 2016a). FS acts as a baseline for SIDENET.\nWe report the performance levels of several variants of SIDENET on the validation set in Table 1. We also compare them against the LEAD baseline. SEQ2SEQ is a simple sequence to sequence\n12We used pyrouge, a Python package, to compute all our rouge scores with parameters “-a -c 95 -m -n 4 -w 1.2.”\nvariant of SIDENET which does not use any side information. We trained SEQ2SEQ using both SENTEXTLABELS and COLLECTIVEORACLE labels. SEQ2SEQ with SENTEXTLABELS achieves scores of 53.3%, 19.7%, 10.4%, 6.4%, and 47.2% for R1, R2, R3, R4 and RL respectively. In comparison, SEQ2SEQ with COLLECTIVEORACLE achieves significantly better scores (54.3%, 21.1%, 11.1%, 6.9% and 48.9%). Following this, the rest of the models are trained with COLLECTIVEORACLE. When the title (TITLE), image captions (CAPTION) and the first sentence (FS) are used separately as side information, SIDENET performs best with TITLE as its side information. Our result demonstrates the importance of the title of the document in extractive summarization (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001). Interestingly, in all three cases average recall improves over SEQ2SEQ, however, the performance with TITLE and CAPTION is better than that with FS. We also tried possible combinations of TITLE, CAPTION and FS. All SIDENET models are superior to the LEAD baseline and it performs the best when TITLE and CAPTION are jointly used as side information (55.4%, 21.8%, 11.8%, 7.5%, and 49.2% for R1, R2, R3, R4, and RL respectively). It is better than the best SEQ2SEQ model by 0.7 points on average, indicating that the side information is useful to identify the gist of the document. We use this model for the testing purposes.\nOur final results on the test set are shown in Table 2. We present both fixed length (first 75 bytes and 275 bytes) and full length (three highest scoring sentences) ROUGE scores. It turns out that for smaller summaries (75 bytes) LEAD and POINTERNET are superior to SIDENET. This result could be because LEAD (always) and POINTERNET (often) include the first sentence in their summaries, whereas, SIDENET is better capable of exploring sentences at various levels of the document. This is not captured by smaller summaries of 75 bytes, but it becomes more evident with longer summaries (275 bytes and full length) where SIDENET performs the best across all ROUGE scores. It is interesting to note that POINTERNET performs better than LEAD for 75- byte summaries, then its performance drops behind LEAD for 275-byte summaries, but then it performs better than LEAD for full length summaries for R1, R2 and RL. It shows that POINTERNET with its attention over sentences in the\ndocument is capable of exploring more than first few sentences in the document. But, it is still far behind SIDENET which is better at identifying salient sentences in the document. SIDENET performs better than POINTERNET by 0.8 points for 275-byte summaries and by 1.9 points for full length summaries, on average for all ROUGE scores."
    }, {
      "heading" : "5.2 Human Evaluation",
      "text" : "We complement our automatic evaluation results with human evaluation. We randomly selected 20 articles from the test set. Annotators were presented with a news article and summaries from four different systems. These include the LEAD baseline, POINTERNET, SIDENET and the human authored highlights. We followed the guidelines in Cheng and Lapata (2016), and asked our partic-\nipants to rank the summaries from best to worst in order of informativeness (does the summary capture important information in the article?) and fluency (is the summary written in well-formed English?). We did not allow any ties, we only sampled articles with non-identical summaries. We assigned this task to five annotators who were proficient native or bilingual English speakers. Each annotator was presented with all 20 articles.\nThe results of our human evaluation study are shown in Table 3. We compare our SIDENET against LEAD, POINTERNET and HUMAN on how frequently each system gets ranked 1st, 2nd and so on, in terms of best-to-worst summaries. As one might imagine, HUMAN gets ranked 1st most of the time (41%). However, it is closely followed by SIDENET with ranked 1st 28% of the time. In comparison, POINTERNET and LEAD were mostly ranked at 3rd and 4th places. We also carried out pairwise comparisons between all models in Table 3 for their statistical significance using a oneway ANOVA with post-hoc Tukey HSD tests with (p < 0.01). It showed that SIDENET is significantly better than LEAD and POINTERNET, and it does not differ significantly from HUMAN. On the other hand, POINTERNET does not differ significantly from LEAD and it differs significantly from both SIDENET and HUMAN. The human evaluation results justifies our empirical results in Table 1 and Table 2 that SIDENET is better than LEAD and POINTERNET in producing informative and fluent summaries.\nIn the end, Figure 3 shows output summaries from various systems for the article shown in Figure 1. As can be seen, both SIDENET and POINTERNET were able to select the most relevant sentence for the summary from anywhere in the article, but SIDENET is better skilled in producing summaries which are close to human authored summaries."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we developed a neural network framework for single-document extractive summarization with side information. We evaluated our system on the large scale CNN dataset. Our experiments show that the side information is useful for extracting salient sentences from the document for the summary. Our framework is very general and it could exploit different types of side information."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org",
      "author" : [ "sudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "sudevan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "sudevan et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv .",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Tgsum: Build tweet guided multi-document summarization dataset",
      "author" : [ "Ziqiang Cao", "Chengyao Chen", "Wenjie Li", "Sujian Li", "Furu Wei", "Ming Zhou." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelli-",
      "citeRegEx" : "Cao et al\\.,? 2016",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural summarization by extracting sentences and words",
      "author" : [ "Jianpeng Cheng", "Mirella Lapata." ],
      "venue" : "ACL.",
      "citeRegEx" : "Cheng and Lapata.,? 2016",
      "shortCiteRegEx" : "Cheng and Lapata.",
      "year" : 2016
    }, {
      "title" : "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "J. Mach. Learn. Res. 12:2493–2537. http://dl.acm.org/citation.cfm?id=1953048.2078186.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "New methods in automatic extracting",
      "author" : [ "H.P. Edmundson." ],
      "venue" : "J. ACM 16(2):264–285.",
      "citeRegEx" : "Edmundson.,? 1969",
      "shortCiteRegEx" : "Edmundson.",
      "year" : 1969
    }, {
      "title" : "Event-based extractive summarization",
      "author" : [ "Elena Filatova." ],
      "venue" : "ACL Workshop on Summarization.",
      "citeRegEx" : "Filatova.,? 2004",
      "shortCiteRegEx" : "Filatova.",
      "year" : 2004
    }, {
      "title" : "Sentence compression by deletion with lstms",
      "author" : [ "Katja Filippova", "Enrique Alfonseca", "Carlos A. Colmenares", "Lukasz Kaiser", "Oriol Vinyals." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Filippova et al\\.,? 2015",
      "shortCiteRegEx" : "Filippova et al\\.",
      "year" : 2015
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomáš Kočiský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Proceedings of the 28th International Conference on Neural",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Multimodal pivots for image caption translation",
      "author" : [ "Julian Hitschler", "Shigehiko Schamoni", "Stefan Riezler." ],
      "venue" : "ACL. Association for Computational Linguistics, Berlin, Germany, pages 2399–2409. http://www.aclweb.org/anthology/P16-1227.",
      "citeRegEx" : "Hitschler et al\\.,? 2016",
      "shortCiteRegEx" : "Hitschler et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation 9(8):1735–1780. https://doi.org/10.1162/neco.1997.9.8.1735.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom." ],
      "venue" : "ACL.",
      "citeRegEx" : "Kalchbrenner et al\\.,? 2014",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Extractive summarization using continuous vector space models",
      "author" : [ "Mikael Kågebäck", "Olof Mogren", "Nina Tahmasebi", "Devdatt Dubhashi." ],
      "venue" : "Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Composition-",
      "citeRegEx" : "Kågebäck et al\\.,? 2014",
      "shortCiteRegEx" : "Kågebäck et al\\.",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Krizhevsky et al\\.,? 2012",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "A trainable document summarizer",
      "author" : [ "Julian Kupiec", "Jan Pedersen", "Francine Chen." ],
      "venue" : "Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. New York, NY, USA, SIGIR ’95,",
      "citeRegEx" : "Kupiec et al\\.,? 1995",
      "shortCiteRegEx" : "Kupiec et al\\.",
      "year" : 1995
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc V. Le", "Tomas Mikolov." ],
      "venue" : "ICML. volume 32, pages 1188–1196.",
      "citeRegEx" : "Le and Mikolov.,? 2014",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "Handwritten digit recognition with a backpropagation network",
      "author" : [ "Y. LeCun", "B. Boser", "J.S. Denker", "R.E. Howard", "W. Habbard", "L.D. Jackel", "D. Henderson." ],
      "venue" : "David S. Touretzky, editor, Advances in Neural Information Processing Systems",
      "citeRegEx" : "LeCun et al\\.,? 1990",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1990
    }, {
      "title" : "Molding cnns for text: non-linear, non-consecutive convolutions",
      "author" : [ "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational",
      "citeRegEx" : "Lei et al\\.,? 2015",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2015
    }, {
      "title" : "A hierarchical neural autoencoder for paragraphs and documents",
      "author" : [ "Jiwei Li", "Thang Luong", "Dan Jurafsky." ],
      "venue" : "ACL. Association for Computational Linguistics, Beijing, China, pages 1106– 1115. http://www.aclweb.org/anthology/P15-1107.",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic evaluation of summaries using ngram co-occurrence statistics",
      "author" : [ "Chin-Yew Lin", "Eduard Hovy." ],
      "venue" : "NAACL. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 71–78.",
      "citeRegEx" : "Lin and Hovy.,? 2003",
      "shortCiteRegEx" : "Lin and Hovy.",
      "year" : 2003
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of the 26th International Conference on Neural",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Abstractive text summarization using sequence-tosequence rnns and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Cı́cero Nogueira dos Santos", "Çaglar Gülçehre", "Bing Xiang" ],
      "venue" : "In Proceedings of the 20th SIGNLL Conference on Computational Natural",
      "citeRegEx" : "Nallapati et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Classify or select: Neural architectures for extractive document summarization",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Mingbo Ma." ],
      "venue" : "CoRR abs/1611.04244. http://arxiv.org/abs/1611.04244.",
      "citeRegEx" : "Nallapati et al\\.,? 2016b",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "A compositional context sensitive multi-document summarizer: Exploring the factors that influence summarization",
      "author" : [ "Ani Nenkova", "Lucy Vanderwende", "Kathleen McKeown." ],
      "venue" : "ACM SIGIR. ACM, New York, NY, USA, pages 573–580.",
      "citeRegEx" : "Nenkova et al\\.,? 2006",
      "shortCiteRegEx" : "Nenkova et al\\.",
      "year" : 2006
    }, {
      "title" : "MEAD — A platform for multidocument multilingual text summarization",
      "author" : [ "Winkel", "Zhu Zhang." ],
      "venue" : "Conference on Language Resources and Evaluation (LREC). Lisbon, Portugal.",
      "citeRegEx" : "Winkel and Zhang.,? 2004",
      "shortCiteRegEx" : "Winkel and Zhang.",
      "year" : 2004
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "EMNLP. pages 379–389.",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic summarising: The state of the art",
      "author" : [ "Karen Spärck Jones." ],
      "venue" : "Inf. Process. Manage. 43(6):1449–1481.",
      "citeRegEx" : "Jones.,? 2007",
      "shortCiteRegEx" : "Jones.",
      "year" : 2007
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Enhancing singledocument summarization by combining ranknet and third-party sources",
      "author" : [ "Krysta Marie Svore", "Lucy Vanderwende", "Christopher J.C. Burges." ],
      "venue" : "EMNLP-CoNLL. ACL, pages 448–457.",
      "citeRegEx" : "Svore et al\\.,? 2007",
      "shortCiteRegEx" : "Svore et al\\.",
      "year" : 2007
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, Curran Associates, Inc.,",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic generation of story highlights",
      "author" : [ "Kristian Woodsend", "Mirella Lapata." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,",
      "citeRegEx" : "Woodsend and Lapata.,? 2010",
      "shortCiteRegEx" : "Woodsend and Lapata.",
      "year" : 2010
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Lei Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Optimizing sentence modeling and selection for document summarization",
      "author" : [ "Wenpeng Yin", "Yulong Pei." ],
      "venue" : "Proceedings of the 24th International Conference on Artificial Intelligence. AAAI Press, IJCAI’15, pages 1383–1389.",
      "citeRegEx" : "Yin and Pei.,? 2015",
      "shortCiteRegEx" : "Yin and Pei.",
      "year" : 2015
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Proceedings of the 28th International Conference on Neural Information Processing Systems.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "(Nenkova et al., 2006) and low-level event-based features describing relationships between important actors in a document (Filatova, 2004).",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : ", 2006) and low-level event-based features describing relationships between important actors in a document (Filatova, 2004).",
      "startOffset" : 107,
      "endOffset" : 123
    }, {
      "referenceID" : 15,
      "context" : "Kågebäck et al. (2014) and Yin and Pei (2015) map sentences to a continuous vector space which is used for similarity measurement to reduce the redundancy in the generated summaries.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "Kågebäck et al. (2014) and Yin and Pei (2015) map sentences to a continuous vector space which is used for similarity measurement to reduce the redundancy in the generated summaries.",
      "startOffset" : 0,
      "endOffset" : 46
    }, {
      "referenceID" : 3,
      "context" : "Cheng and Lapata (2016) and Nallapati et al.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "Cheng and Lapata (2016) and Nallapati et al. (2016b) used recurrent neural networks to read sequences of sentences to get a document representation which they use to label each sentence for extraction.",
      "startOffset" : 0,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016)",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 25,
      "context" : "and Nallapati et al. (2016b), in that it derives the document meaning representation from its sentences and their constituent words.",
      "startOffset" : 4,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "tence extractor combines this document meaning representation with an attention mechanism (Bahdanau et al., 2014) over the side information to select sentences of the input document as the output summary.",
      "startOffset" : 90,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "both the standard attention mechanism (Bahdanau et al., 2014) which is used to locate the region of focus in the input, and the mechanism of Cheng and Lapata (2016) which directly extracts salient sentences after reading them.",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "both the standard attention mechanism (Bahdanau et al., 2014) which is used to locate the region of focus in the input, and the mechanism of Cheng and Lapata (2016) which directly extracts salient sentences after reading them.",
      "startOffset" : 39,
      "endOffset" : 165
    }, {
      "referenceID" : 6,
      "context" : "Edmundson (1969)",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 18,
      "context" : "used subjectively weighted combination of these human-engineered features, whereas Kupiec et al. (1995) and Mani (2001) trained their feature weights using a corpus.",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 18,
      "context" : "used subjectively weighted combination of these human-engineered features, whereas Kupiec et al. (1995) and Mani (2001) trained their feature weights using a corpus.",
      "startOffset" : 83,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "We evaluate our models both automatically (in terms of ROUGE scores) and by conducting human evaluations on the CNN news highlights dataset (Hermann et al., 2015).",
      "startOffset" : 140,
      "endOffset" : 162
    }, {
      "referenceID" : 4,
      "context" : "We model extractive summarization as a sequence labelling problem using a standard encoder-decoder architecture (Cho et al., 2014; Sutskever et al., 2014).",
      "startOffset" : 112,
      "endOffset" : 154
    }, {
      "referenceID" : 31,
      "context" : "We model extractive summarization as a sequence labelling problem using a standard encoder-decoder architecture (Cho et al., 2014; Sutskever et al., 2014).",
      "startOffset" : 112,
      "endOffset" : 154
    }, {
      "referenceID" : 20,
      "context" : "2 CNNs (LeCun et al., 1990) have shown to be very effective in computer vision (Krizhevsky et al.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 17,
      "context" : ", 1990) have shown to be very effective in computer vision (Krizhevsky et al., 2012) and for various NLP tasks (Collobert et al.",
      "startOffset" : 59,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : ", 2012) and for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016).",
      "startOffset" : 34,
      "endOffset" : 176
    }, {
      "referenceID" : 13,
      "context" : ", 2012) and for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016).",
      "startOffset" : 34,
      "endOffset" : 176
    }, {
      "referenceID" : 12,
      "context" : ", 2012) and for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016).",
      "startOffset" : 34,
      "endOffset" : 176
    }, {
      "referenceID" : 37,
      "context" : ", 2012) and for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016).",
      "startOffset" : 34,
      "endOffset" : 176
    }, {
      "referenceID" : 21,
      "context" : ", 2012) and for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016).",
      "startOffset" : 34,
      "endOffset" : 176
    }, {
      "referenceID" : 14,
      "context" : ", 2012) and for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016).",
      "startOffset" : 34,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : ", 2012) and for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016).",
      "startOffset" : 34,
      "endOffset" : 176
    }, {
      "referenceID" : 35,
      "context" : "For example, for the caption generation task (Xu et al., 2015), CNNs successfully identify salient objects in the image for the corresponding words in the caption.",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 19,
      "context" : "We tried sentence/paragraph vector (Le and Mikolov, 2014) to infer sentence embeddings in advance, but the results were inferior to those presented in this paper with CNNs.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "by Collobert et al. (2011), Kim (2014) and Cheng and Lapata (2016).",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "by Collobert et al. (2011), Kim (2014) and Cheng and Lapata (2016).",
      "startOffset" : 3,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : "(2011), Kim (2014) and Cheng and Lapata (2016). A sentence s of length k in D can be represented as a dense matrix W = [w1 ⊕ w2 ⊕ .",
      "startOffset" : 23,
      "endOffset" : 47
    }, {
      "referenceID" : 5,
      "context" : "We then apply max pooling over time (Collobert et al., 2011) over the feature map f and get fmax = max(f) as the feature corresponding to this particular filter K.",
      "startOffset" : 36,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : "Max-pooling is followed by local response normalization for better generalization (Krizhevsky et al., 2012).",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "Cheng and Lapata (2016) sum over feature lists to get the final sentence embedding.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "Cheng and Lapata (2016) sum over feature lists to get the final sentence embedding. In contrast, we follow Kim et al. (2016) and concatenate them.",
      "startOffset" : 0,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "Short-Term Memory (LSTM) cells to avoid the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 100,
      "endOffset" : 134
    }, {
      "referenceID" : 31,
      "context" : ", sn), we follow the common practice of feeding sentences in the reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015).",
      "startOffset" : 79,
      "endOffset" : 144
    }, {
      "referenceID" : 22,
      "context" : ", sn), we follow the common practice of feeding sentences in the reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015).",
      "startOffset" : 79,
      "endOffset" : 144
    }, {
      "referenceID" : 8,
      "context" : ", sn), we follow the common practice of feeding sentences in the reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015).",
      "startOffset" : 79,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "It is implemented with another recurrent neural network with LSTM cells and an attention mechanism (Bahdanau et al., 2014).",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "For our purposes, we used an augmented version of the CNN dataset (Hermann et al., 2015).",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "For our purposes, we used an augmented version of the CNN dataset (Hermann et al., 2015).5 Our dataset is an evolved version of the CNN dataset first collected by Svore et al. (2007) for highlight generation.",
      "startOffset" : 67,
      "endOffset" : 183
    }, {
      "referenceID" : 9,
      "context" : "For our purposes, we used an augmented version of the CNN dataset (Hermann et al., 2015).5 Our dataset is an evolved version of the CNN dataset first collected by Svore et al. (2007) for highlight generation. Svore et al. (2007) noticed that CNN articles often come with “story highlights” to allow readers to quickly gather information on stories.",
      "startOffset" : 67,
      "endOffset" : 229
    }, {
      "referenceID" : 9,
      "context" : "Recently, Hermann et al. (2015) crawled 93K CNN articles to build a large-scale corpus to set a benchmark for deep learning methods.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "this dataset has been used for single-document summarization (Nallapati et al., 2016a; Cheng and Lapata, 2016; Nallapati et al., 2016b).",
      "startOffset" : 61,
      "endOffset" : 135
    }, {
      "referenceID" : 26,
      "context" : "this dataset has been used for single-document summarization (Nallapati et al., 2016a; Cheng and Lapata, 2016; Nallapati et al., 2016b).",
      "startOffset" : 61,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : ", 2016a; Cheng and Lapata, 2016; Nallapati et al., 2016b). Cheng and Lapata (2016) annotated this dataset with Woodsend and Lapata (2010) style gold annotation.",
      "startOffset" : 9,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : ", 2016a; Cheng and Lapata, 2016; Nallapati et al., 2016b). Cheng and Lapata (2016) annotated this dataset with Woodsend and Lapata (2010) style gold annotation.",
      "startOffset" : 9,
      "endOffset" : 138
    }, {
      "referenceID" : 9,
      "context" : "We used a modified script of Hermann et al. (2015) to extract title and image captions, and we associated them with the corresponding articles.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 34,
      "context" : "Collective oracle The gold annotation of Woodsend and Lapata (2010) labels each sentence of the document in isolation for sentence extraction",
      "startOffset" : 41,
      "endOffset" : 68
    }, {
      "referenceID" : 23,
      "context" : "ROUGE (Lin and Hovy, 2003), a recall-oriented metric, is often used to evaluate summarization systems.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 24,
      "context" : "We used our training data to train word embeddings using the Word2vec (Mikolov et al., 2013) skip-gram model with context window size 6, negative sampling size 10 and hierarchical softmax",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "For the convolutional sentence encoder, we followed Kim et al. (2016), and used a list of kernels of widths 1 to 7, each with output channel size of 50.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 33,
      "context" : "The neural attention architecture in Cheng and Lapata (2016) resembles the one in Pointer Networks (Vinyals et al., 2015).",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "The neural attention architecture in Cheng and Lapata (2016) resembles the one in Pointer Networks (Vinyals et al.",
      "startOffset" : 37,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "The neural attention architecture in Cheng and Lapata (2016) resembles the one in Pointer Networks (Vinyals et al., 2015). For sanity check, we have trained our implementation of POINTERNET on the DailyMail dataset and achieved comparable results to what has been reported by Cheng and Lapata (2016). MODELS R1 R2 R3 R4 RL Avg.",
      "startOffset" : 37,
      "endOffset" : 300
    }, {
      "referenceID" : 15,
      "context" : "(Kingma and Ba, 2014) with initial learning rate 0.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 23,
      "context" : "To automatically assess the quality of our summaries, we used ROUGE (Lin and Hovy, 2003),",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 29,
      "context" : "However, we wanted to explore the idea that the first sentence of the document plays a crucial part in generating summaries (Rush et al., 2015; Nallapati et al., 2016a).",
      "startOffset" : 124,
      "endOffset" : 168
    }, {
      "referenceID" : 6,
      "context" : "Our result demonstrates the importance of the title of the document in extractive summarization (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001).",
      "startOffset" : 96,
      "endOffset" : 146
    }, {
      "referenceID" : 18,
      "context" : "Our result demonstrates the importance of the title of the document in extractive summarization (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001).",
      "startOffset" : 96,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "We followed the guidelines in Cheng and Lapata (2016), and asked our participants to rank the summaries from best to worst in order of informativeness (does the summary cap-",
      "startOffset" : 30,
      "endOffset" : 54
    } ],
    "year" : 2017,
    "abstractText" : "Most extractive summarization methods focus on the main body of the document from which sentences need to be extracted. The gist of the document often lies in the side information of the document, such as title and image captions. These types of side information are often available for newswire articles. We propose to explore side information in the context of singledocument extractive summarization. We develop a framework for single-document summarization composed of a hierarchical document encoder and an attentionbased extractor with attention over side information. We evaluate our models on a large scale news dataset. We show that extractive summarization with side information consistently outperforms its counterpart (that does not use any side information), in terms on both informativeness and",
    "creator" : "LaTeX with hyperref package"
  }
}