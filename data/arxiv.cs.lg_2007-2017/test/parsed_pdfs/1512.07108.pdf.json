{
  "name" : "1512.07108.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Recent Advances in Convolutional Neural Networks",
    "authors" : [ "Jiuxiang Gu", "Zhenhua Wang", "Jason Kuen", "Lianyang Ma", "Amir Shahroudy", "Bing Shuai", "Ting Liu", "Xingxing Wang", "Gang Wang" ],
    "emails" : [ "jxgu@ntu.edu.sg;", "wzh@ntu.edu.sg;", "jasonkuen@ntu.edu.sg;", "lyma@ntu.edu.sg;", "amir3@ntu.edu.sg;", "BSHUAI001@e.ntu.edu.sg;", "LIUT0016@e.ntu.edu.sg;", "wangxx@ntu.edu.sg;", "WangGang@ntu.edu.sg)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Convolutional Neural Network, Deep learning.\nI. INTRODUCTION\nCONVOLUTIONAL Neural Network (CNN) is first intro-duced by LeCun et al. in [1] and improved in [2]. They developed a multi-layer artificial neural network called LeNet5 which can classify handwritten digits. Like other neural networks, LeNet-5 has multiple layers and can be trained with the backpropagation algorithm [3]. It can obtain effective representations of the original image, which makes it possible to recognize visual patterns directly from raw pixels with little-to-none preprocessing. However, due to the lack of large training data and computing power at that time, LeNet-5 can not perform well on more complex problems, e.g., large-scale image and video classification.\nSince 2006, many methods have been developed to overcome the difficulties encountered in training deep neural networks. Most notably, Krizhevsky et al. propose a classic CNN architecture and show significant improvements upon previous methods on the image classification task. The overall architecture of their method, i.e., AlexNet [4], is similar to LeNet-5 but with a deeper structure. It contains 8 learned layers (5 convolutional layers with pooling interspersed and 3 fully-connected layers) where the early layers are splitted over the two GPUs. ReLU [5] is used as the nonlinear activation function and Dropout [6] is used to reduce overfitting.\nWith the success of AlexNet, several works are proposed to improve its performance. Among them, three representative works are ZFNet [7], VGGNet [8] and GoogleNet [9]. ZFNet improves AlexNet by reducing the filter size of first layer from 11×11 to 7×7 as well as reducing the stride of the convolution\nJX. Gu, ZH. Wang, J. Kuen, LY. Ma, A. Shahroudy, B. Shuai, T. Liu, XX. Wang, G. Wang are with the Rapid-Rich Object Search (ROSE) Lab at the Nanyang Technological University, Singapore (e-mail: jxgu@ntu.edu.sg; wzh@ntu.edu.sg; jasonkuen@ntu.edu.sg; lyma@ntu.edu.sg; amir3@ntu.edu.sg; BSHUAI001@e.ntu.edu.sg; LIUT0016@e.ntu.edu.sg; wangxx@ntu.edu.sg; WangGang@ntu.edu.sg). ∗ equal contribution.\nfrom 4 to 2. In such a setting, the sizes of middle convolutional layers are expanded so as to capture more meaningful features. VGGNet pushes the network depth up to 19 weight layers and uses a very small filter size 3× 3 in each convolutional layer. The results demonstrate that the depth is a critical factor for good performance. GoogleNet increases both the depth and the width of the network. It achieves a significant quality gain at modest increase of computational requirement compared to shallower and less wider networks.\nOther than these works, there are also lots of works that aim to improve CNN in different aspects, e.g., layer design, activation function, loss function, regularization, optimization and fast computing, or apply CNN in different kinds of computer vision tasks. In the following sections, we identify broad categories of works related to CNN. We first give an overview of the basic components of CNN in Section II. Then, some recent improvements on different aspects of CNN are introduced in Section III and the fast computing techniques are introduced in Section IV. Next, we discuss some typical applications of CNN in Section V and finally we conclude this paper in Section VI."
    }, {
      "heading" : "II. BASIC CNN COMPONENTS",
      "text" : "There are numerous variants of CNN architectures in the literature. However, their basic components are very similar. The basic CNN architecture typically consists of three types of layers, namely convolutional layer, pooling layer and fullyconnected layer. Fig. 1 shows the architecture of LeNet-5 [1] which is introduced by Yann LeCun.\nThe convolutional layer aims to learn feature representations of the inputs. As shown in Fig. 1, convolutional layer is composed of several feature maps. Each neuron of a feature map is connected to a neighborhood of neurons in the previous layer. Such a neighborhood is referred to as the neuron’s receptive field in the previous layer. To compute a new feature map, the input feature maps are first convolved with a learned kernel and then the results are passed into a nonlinear activation function. By applying several different kernels, the complete new feature maps are obtained. Note that the kernel\nar X\niv :1\n51 2.\n07 10\n8v 1\n[ cs\n.C V\n] 2\n2 D\nec 2\n01 5\n2 for generating a single feature map is the same. Such a shareweight mode has several advantages such as it can reduce the model complexity and make the network easier to train. The activation function introduces non-linearities to CNN, which are desirable for multi-layer networks to detect non-linear features. The typical activation functions are sigmoid, tanh and ReLU [5].\nThe pooling layer aims to achieve spatial invariance by reducing the resolution of the feature maps. It is usually placed between two convolutional layers. Each feature map of pooling layer is connected to its corresponding feature map of the preceding convolutional layer. Thus, they has the same number of feature maps. The typical pooling operations are average pooling [10] and max pooling [11–13]. By stacking several convolutional and pooling layers, we could extract more abstract feature representations.\nAfter several convolutional and pooling layers, there may be one or more fully-connected layers which aim to perform high level reasoning. They take all neurons in the previous layer and connect them to every single neuron of current layer. There are no spatial information preserved in fully-connected layers.\nThe outputs of the last fully-connected layer will be fed to an output layer. For classification tasks, softmax regression is commonly used as it generates a well-formed probability distribution of the outputs [4]. Another commonly used method is SVM, which can be combined with CNNs to solve different classification tasks [14].\nIII. IMPROVEMENTS ON CNNS\nThere has been various improvements on CNNs since the success of AlexNet in 2012. In this section, we describe the major improvements on CNNs from six aspects: convolutional layer, pooling layer, activation function, loss function, regularization, and optimization."
    }, {
      "heading" : "A. Convolutional layer",
      "text" : "Convolution filter in basic CNNs is a generalized linear model (GLM) for the underlying local image patch. It works well for abstraction when instances of latent concepts are linearly separable. Here we introduce two works which aim to enhance its representation ability.\n1) Network in network: Network In Network (NIN) is a general network structure proposed by Lin et al. [15]. It replaces the linear filter of the convolutional layer by a micro network, e.g., multilayer perceptron convolution (mlpconv) layer in the paper, which makes it capable of approximating more abstract representations of the latent concepts. The overall structure of NIN is the stacking of such micro networks. To see the difference between convolutional layer and mlpconv layer, let us consider how the feature maps are computed in each of them. Formally, the feature maps of convolutional layers are computed as:\nfi,j,k = max(wkxi,j , 0). (1)\nwhere i and j are the pixel indexes in the feature map, xij is the input patch centered at (i, j), and k is the channel index of\nthe feature map. As a comparison, the computation performed by mlpconv layer is formulated as: f1i,j,k1 = max(w 1 k1 xi,j + bk1 , 0).\n... fni,j,kn = max(w n kn fn−1i,j + bkn , 0).\n(2)\nwhere n is the number of layers in the mlpconv layer. It can be found that Eq. (2) is equivalent to cascaded cross channel parametric pooling on a normal convolutional layer.\n2) Inception module: Inception module is introduced by Szegedy et al. [9] which can be seen as a logical culmination of NIN. [9] uses variable filter sizes to capture different visual patterns of different sizes, and approximates the optimal sparse structure by the inception module. Specifically, inception module consists of one pooling operation and three types of convolution operations. 1×1 convolutions are placed before 3 × 3 and 5 × 5 convolutions as dimension reduction modules, which allow for increasing the depth and width of CNN without increasing the computational complexity. With the help of inception module, the network parameters can be dramatically reduced to 5 millions which are much less than those of AlexNet (60 millions) and ZFNet (75 millions). In their recent paper [16], to find high performance networks with a relatively modest computation cost, they propose several design principles to scale up CNNs according to their experimental evaluation. Specifically, they suggest that: (1) One should avoid representation bottlenecks, especially early in the network. In general, the representation size should gently decrease from inputs to outputs. (2) Higher dimensional representations are easier to process locally. (3) Spatial aggregation can be done over lower dimensional embeddings without much loss in representational power. (4) Optimal performance of the network can be reached by balancing the number of filters per layer and the depth of the network."
    }, {
      "heading" : "B. Pooling layer",
      "text" : "Pooling is an important concept of CNN. It lowers the computational burden by reducing the number of connections between convolutional layers. In this section, we introduce some recent pooling methods used in CNNs.\n1) Lp Pooling: Lp pooling is a biologically inspired pooling process modelled on complex cells [17], [18]. It has been theoretically analysed in [19], [20], which suggests that Lp pooling provides better generalization than max pooling. Lp pooling can be represented as ( ∑N i=1 |xIi |p)1/p, where {xI1 , ..., xIN } is a finite set of input nodes. When p = 1, Lp reduces to average pooling, and when p = 2, Lp reduces to L2 pooling. Finally, when p =∞, Lp reduces to max pooling.\n2) Mixed Pooling: Inspired by random Dropout [6] and DropConnect [21], Yu et al. [22] propose a mixed pooling method which is the combination of max pooling and average pooling. The function of mixed pooling can be formulated as follows:\nykij = λ max (p,q)∈Rij\nxkpq + (1− λ) 1 |Rij | ∑\n(p,q)∈Rij\nxkpq (3)\n3 where ykij is the output of the pooling operator related to position (i, j) in k-th feature map, λ is a random value being either 0 or 1 which indicates the choice of using max pooling or average pooling, Rij is a local neighbourhood around the position (i, j), and xkpq is the element at (p, q) within the pooling region Rij in k-th feature map. During forward propagation process, λ is recorded and will be used for the backpropagation operation. Experiments in [22] show that mixed pooling can better address the overfitting performs and it performs better than max pooling and average pooling.\n3) Stochastic pooling: Stochastic pooling [23] ensures that the non-maximal activations of feature maps are also possible to be utilized. Specifically, stochastic pooling first computes the probabilities p for each region Rj by normalizing the activations within the region, i.e., pi = ai/ ∑ k∈Rj (ak). Then it samples from the multinomial distribution based on p to pick a location l within the region. The pooled activation is sj = al, where l ∼ P (p1, ..., p|Rj |). Stochastic pooling has the advantages of max pooling, and can avoid overfitting due to the stochastic component.\n4) Spectral pooling: Spectral pooling [24] performs dimensionality reduction by cropping the representation of input in frequency domain. Given an input feature map x ∈ RM×N , suppose the dimension of desired output feature map is H ×W , spectral pooling first computes the discrete Fourier transform (DFT) of the input feature map, then crops the frequency representation by maintaining only the central H×W submatrix of the frequencies, and finally uses inverse DFT to map the approximation back into spatial domain. Compared with max pooling, the linear low-pass filtering operation of spectral pooling can preserve more information for the same output dimensionality. Meanwhile, it also does not suffer from the sharp reduction in output map dimensionality exhibited by other pooling methods. What is more, the process of spectral pooling is achieved by matrix truncation, which makes it capable of being implemented with little computational cost in CNNs (e.g., [25]) that employ FFT for convolution kernels.\n5) Spatial pyramid pooling: Spatial pyramid pooling (SPP) is introduced by He et al. [26]. The key advantage of SPP is that it can generate a fixed-length representation regardless of the input sizes. SPP pools input feature map in local spatial bins with sizes proportional to the image size, resulting in a fixed number of bins. This is different from the sliding window pooling in the previous deep networks, where the number of sliding windows depends on the input size. By replacing the last pooling layer with SPP, they propose a new SPP-net which is able to deal with images with different sizes.\n6) Multi-scale Orderless Pooling: Inspired by [27], Gong et al. [28] use multi-scale orderless pooling (MOP) to improve the invariance of CNNs without degrading their discriminative power. They extract deep activation features for both the whole image and local patches of several scales. The activations of the whole image is the same as those of previous CNNs, which aim to capture the global spatial layout information. The activations of local patches are aggregated by VLAD encoding [29], which aim to capture more local, fine-grained details of the image as well as enhancing invariance. The new image representation is obtained by concatenating the\nglobal activations and the VLAD features of the local patch activations."
    }, {
      "heading" : "C. Activation function",
      "text" : "A proper activation function significantly improves the performance of a CNN for a certain task. In this section, we introduce the recent used activation functions in CNNs.\n1) ReLU: Rectified linear unit (ReLU) [5] is one of the most notable non-saturated activation functions. The ReLU activation function is defined as:\nyi = max(0, zi) (4)\nwhere zi is the input of i-th channel. ReLU is a piecewise linear function which prunes the negative part to zero and retains the positive part (see Fig. 2(a)). The simple max operation of ReLU allows it to compute much faster than sigmoid or tanh activation functions, and it also induces the sparsity in the hidden units and allows the network to easily obtain sparse representations. It has been shown that deep networks can be trained efficiently using ReLU even without pre-training [4]. Even though the discontinuity of ReLU at 0 may hurt the performance of backpropagation, many works have shown that ReLU works better than sigmoid and tanh activation functions empirically.\n2) Leaky ReLU: A potential disadvantage of ReLU unit is that it has zero gradient whenever the unit is not active. This may cause units that do not active initially never active as the gradient-based optimization will not adjust their weights. Also, it may slow down the training process due to the constant zero gradients. To alleviate this problem, Mass et al. introduce leaky ReLU (LReLU) [30] which is defined as:\nyi = { zi zi ≥ 0 azi zi < 0\n(5)\nwhere a is a predefined parameter in range (0, 1). Compared with ReLU, Leaky ReLU compresses the negative part rather than mapping it to constant zero, which makes it allow for a small, non-zero gradient when the unit is not active.\n3) Parametric ReLU: Rather than using a predefined parameter in Leaky ReLU, e.g. a in Eq.(5), He et al. [31] propose Parametric Rectified Linear Unit (PReLU) which adaptively learns the parameters of the rectifiers in order to improve accuracy. Mathematically, PReLU function is defined as\nyi = { zi zi ≥ 0 aizi zi < 0\n(6)\nwhere ai is the learned parameters for the i-th channel. As PReLU only introduces a very small number of extra parameters, e.g., the extra parameter number is the same as the channel number of the whole network, there is no extra risk of overfitting and the extra computational cost is negligible. It also can be simultaneously trained with other parameters by backpropagation.\n4 4) Randomized ReLU: Another variant of Leaky ReLU is Randomized Leaky Rectified Linear Unit (RReLU) [32]. In RReLU, the parameters of negative parts are randomly sampled from a uniform distribution in training, and then fixed in testing (see Fig. 2(c)). Formally, RReLU function is defined as:\ny (j) i =\n{ z (j) i z (j) i ≥ 0\najiz (j) i z (j) i < 0\n(7)\nwhere z(j)i denotes the i-th channel in j-th example, a (j) i denotes its corresponding sampled parameter, and y(j)i denotes its corresponding output. It could reduce overfitting due to its randomized nature. [32] also evaluates ReLU, LReLU, PReLU and RReLU on standard image classification task, and concludes that incorporating a non-zero slop for negative part in rectified activation units could consistently improve the performance.\n5) ELU: [33] introduces Exponential Linear Unit (ELU) which enables faster learning of deep neural networks and leads to higher classification accuracies. Like ReLU, LReLU, PReLU and RReLU, ELU avoids the vanishing gradient problem by setting the positive part to identity. In contrast to ReLU, ELU has a negative part which is beneficial for fast learning. Compared with LReLU, PReLU and RReLU who also have negative parts, ELU employs a saturation function as negative part which is more robust to noise. The function of ELU is defined as:\nyi = { zi zi ≥ 0 a(exp(zi)− 1) zi < 0\n(8)\nwhere a is a predefined parameter for controlling the value to which an ELU saturate for negative inputs.\n6) Maxout: Maxout [34] is an alternative non-linear function that takes the maximum response across multiple channels at each spatial position. As stated in [34], the maxout function is defined as:\ny = max i∈[1,k] zi (9)\nwhere zi is the i-th channel of the feature map. It is worth noting that maxout enjoys all the benefits of ReLU, since ReLU is actually a special case of maxout, e.g., max(w1x+ b1, w2x+b2) where w1 is a zero vector and b1 is zero. Besides, maxout is particularly well suited for training with Dropout.\n7) Probout: [35] proposes a probabilistic variant of maxout called probout. They replace the maximum operation in maxout with a probabilistic sampling procedure, and combine the Dropout with probout. Specifically, they first define a probability for each of the k linear units as:\npi = eλzi∑k j=1 e λzj (10)\nwhere λ is a hyperparameter for controlling the variance of the distribution. To incorporating with dropout, they actually re-define the probabilities as:\np̂0 = 0.5, p̂i = eλzi 2. ∑k j=1 e λzj (11)\nThe activation function is then sampled as:\nyi = { 0 if i = 0 zi else\n(12)\nwhere i ∼Multinomial{p̂0, ..., p̂k}. Probout can achieve the balance between preserving the desirable properties of maxout units and improving their invariance properties. However, in testing process, probout is computationally expensive than maxout due to the additional probability calculations."
    }, {
      "heading" : "D. Loss function",
      "text" : "It is important to choose an appropriate loss functions for a specific task. We introduce three representative ones in this subsection: softmax loss, hinge loss, and contrastive loss.\n1) Softmax loss: Softmax loss is a commonly used loss function which is essentially a combination of multinomial logistic loss and softmax. Given a training set {(x(i), y(i)); i ∈ 1, . . . , N, y(i) ∈ 0, . . . ,K − 1}, where x(i) is the i-th input image patch, and y(i) is its class label. The prediction a(i)j of j-th class for i-th input is transformed with the following softmax function:\npij = e a (i) j / K−1∑ l=0 ea (i) l (13)\nSoftmax turns the predictions into non-negative values and normalizes them to get a probability distribution over classes. Such probabilistic predictions are used to compute the multinomial logistic loss, i.e., the softmax loss, as follows:\nLsoftmax = − 1\nN [ N∑ i=1 K−1∑ j=0 1{y(i) = j}logpij ] (14)\n5 2) Hinge loss: Hinge loss is usually used to train large margin classifiers such as Support Vector Machine (SVM). The hinge Loss function of a multi-class SVM is defined in Eq.(15), where xn is the given feature vector and `n ∈ [0, 1, 2, ...,K − 1] indicates its correct class label among the K classes.\nLHinge = 1\nN N∑ n=1 K−1∑ k=0 [max(0, 1− δ(`n, k)wTxn)]p\nδ(`n, k) = { 1, if `n = k −1, if `n 6= k\n(15)\nNote that if p = 1, Eq.(15) is Hinge-Loss (L1-Loss), while if p = 2, it is the Squared Hinge-Loss (L2-Loss) [36]. The L2Loss is differentiable and imposes a larger loss for point which violates the margin comparing with L1-Loss. [14] investigates and compares the performance of softmax with L2-SVMs in deep networks. The results on MNIST [37] demonstrate the superiority of L2-SVM over softmax.\n3) Contrastive loss: Contrastive loss [38] is commonly used to train Siamese network. It is a weakly-supervised scheme for learning a similarity measure from pairs of data instances labelled as matching or non-matching. The contrastive loss is usually defined for every layer l ∈ [1, . . . , L] and the backpropagations for the loss of individual layers are performed at the same time. Given a pair of data (z0α, z 0 β), let (zlα, z l β) denote the output pair of layer l, the contrastive loss Ll for layer l is defined as :\nLl = (y)dl + (1− y)max(m− dl, 0) dl = ||zlα − zlβ ||22\n(16)\nwhere dl is the similarity between zlα and z l β , and m is a margin parameter affecting non-matching pairs. If (z0α, z 0 β) is a matching pair, then y = 1. Otherwise, y = 0. This loss function is also referred to as a single margin parameter loss function. Lin et al. [39] find that the recall rate quickly collapses when using this function. The reason may be that the indefinite contraction of matching pairs well beyond what is necessary to distinguish them from non-matching pairs is a damaging behaviour. To solve this problem, they propose a double margin loss function which adds another parameter to affect the matching pairs:\nLl = (y)max(dl −m1, 0) + (1− y)max(m2 − dl, 0) (17)"
    }, {
      "heading" : "E. Regularization",
      "text" : "Overfitting is an unneglectable problem in deep CNNs, which can be effectively reduced by regularization. In the following subsection, we introduce some effective regularization techniques: Dropout [6], [40], [41] and DropConnect [21].\n1) Dropout: Dropout is first introduced by Hinton et al. [6], and it has been proven to be very effective in reducing overfitting. In [6], they apply Dropout to fully-connected layers. The output of Dropout is r = m ∗ a(Wv), where v = [v1, v2, . . . , vn] T is the output of the feature extractor,\n(a) No-Drop (b) DropOut (c) DropConnect\nFig. 3: The illustration of No-Drop network, DropOut network and DropConnect network.\nW (of size d×n) is a fully-connected weight matrix, a(·) is a non-linear activation function, and m is a binary mask of size d whose elements are independently drawn from a Bernoulli distribution, i.e. mi ∼ Bernoulli(p). Dropout can prevent the network from becoming too dependent on any one (or any small combination) of neurons, and can force the network to be accurate even in the absence of certain information. Several methods have been proposed to improve Dropout. [40] proposes a fast Dropout method which can perform fast Dropout training by sampling from or integrating a Gaussian approximation. [41] proposes an adaptive Dropout method, where the Dropout probability for each hidden variable is computed using a binary belief network that shares parameters with the deep network. In [42], they find that applying standard Dropout before 1 × 1 convolutional layer generally increases training time but does not prevent overfitting. Therefore, they propose a new Dropout method called SpatialDropout, which extends the Dropout value across the entire feature map. This new Dropout method works well especially when the training data size is small.\n2) DropConnect: DropConnect [21] takes the idea of Dropout a step further. Instead of setting the output of neurons to zero, DropConnect chooses the weights W of fully-connected layer with a probability p. The output of DropConnect is given by r = a((m ∗ W )v), where mij ∼ Bernoulli(p). Additionally, the biases are also masked out during the training process. Fig. 3 illustrates the differences among No-Drop, Dropout and DropConnect networks."
    }, {
      "heading" : "F. Optimization",
      "text" : "In this subsection, we discuss some key techniques for optimizing CNNs.\n1) Weights initialization: Training a deep CNN model is difficult as generally the model has a huge amount of parameters and the loss function is non-convex. To achieve a fast convergency in training, a proper initialization is one of the most important prerequisites. The bias parameters can be initialized to zero, while the weight parameters should be initialized carefully to break the symmetry among hidden units of the same layer. For example, if we simply initialize all the weights to the same value, e.g., zero or one, each hidden unit of the same layer will get exactly the same signal.\nThe most commonly used initialization method is to randomly set the weights according to Gaussian distributions [4],\n6 [6], [8], [43]. Glorot and Bengio [44] propose a normalized initialization which sets the weights according to a distribution with zero mean and a specific variance V ar(W ) =√ 6/ √ nj + nj+1, where nj is the size of layer j. One of its variants is called “Xavier” in Caffe [45]. He et al. [31] derive a robust initialization method that particularly considers the rectifier nonlinearities. Their method allows for the training of extremely deep models (e.g., [9]) to converge while the “Xavier” method [44] cannot. In their method, weights are initialized according to a zero-mean Gaussian distribution whose standard deviation is √ 2/nl, where nl = k2l dl−1, kl is the spatial filter size in layer l and dl−1 is the number of filters in layer l − 1.\n2) Stochastic gradient descent: The backpropagation algorithm [46] is the standard training method which uses gradient descent to update the parameters. Standard gradient descent algorithm updates the parameters θ of the objective J(θ) as θt+1 = θt − α∇θE[J(θt)], where E[J(θt)] is the expectation of J(θ) over the full training set and α is the learning rate. Instead of computing E[J(θt)], stochastic gradient descent (SGD) [47], [48] estimates the gradients on the basis of a single randomly picked example (x(t), y(t)) from the training set:\nθt+1 = θt − αt∇θJ(θt;x(t), y(t)) (18)\nIn practice, each parameter update in SGD is computed with respect to a mini-batch as opposed to a single example. This could help to reduce the variance in the parameter update and can lead to more stable convergency. The convergence speed is controlled by the learning rate αt. A common method is to use a small constant learning rate that gives stable convergence in the initial stage, and then reduce the learning rate as the convergence slows down.\nParallelized SGD methods [49–51] improve SGD to be suitable for parallel, large-scale machine learning. Unlike standard (synchronous) SGD in which the training will be delayed if one of the machines is slow, these parallelized methods use the asynchronous mechanism so that no other optimizations will be delayed except for the one on the slowest machine. Jeffrey Dean et al. [52] use another asynchronous SGD procedure called Downpour SGD to speed up the large-scale distributed training process on clusters with many CPUs. There are also some works that use asynchronous SGD with multiple GPUs. [53] basically combines asynchronous SGD with GPUs to accelerate the training time by several times compared to training on a single machine. [54] also uses multiple GPUs to asynchronously calculate gradients and update the global model parameters, which achieves 3.2 times of speedup on 4 GPUs compared to training on a single GPU.\n3) Batch Normalization: Batch Normalization is proposed by Sergey Ioffe and Christian Szeged [55], which aims to accelerate the entire training process of deep neural networks. In [55], they suggest that the Internal Covariate Shift, i.e., the change in the distributions of internal nodes of a deep network, will slow down the network training. To achieve faster training, they propose an efficient method called Batch Normalization to partially alleviate this phenomenon. It accomplishes this\nby a normalization step that fixes the means and variances of layer inputs. In addition to accelerate the training, Batch Normalization also allows us to use much higher learning rates without the risk of divergence, and it makes it possible to use saturating nonlinearies by preventing the network from getting stuck in the saturated modes."
    }, {
      "heading" : "IV. FAST PROCESSING OF CNNS",
      "text" : "With the increasing challenges in the computer vision and machine learning tasks, the models of deep neural networks get more and more complex. These powerful models require more data for training in order to avoid overfitting. Meanwhile, the big training data also brings new challenges such as how to train the networks in a feasible amount of times. In this section, we introduce some fast processing methods of CNNs."
    }, {
      "heading" : "A. FFT",
      "text" : "Mathieu et al. [25] carry out the convolutional operation in the Fourier domain with FFTs. Using FFT-based methods has many advantages. Firstly, the Fourier transformations of filters can be reused as the filters are convolved with multiple images in a mini-batch. Secondly, the Fourier transformations of the output gradients can be reused when backpropagating gradients to both filters and input images. Finally, summation over input channels can be performed in the Fourier domain, so that inverse Fourier transformations are only required once per output channel per image. There have already been some GPU-based libraries developed to speed up the training and testing process, such as cuDNN [56] and fbfft [57]. However, using FFT to perform convolution needs additional memory to store the feature maps in the Fourier domain, since the filters must be padded to be the same size as the inputs. This is especially costly when the striding parameter is larger than 1, which is common in many state-of-art networks, such as the early layers in [58] and [9]."
    }, {
      "heading" : "B. Matrix Factorization",
      "text" : "Low-rank matrix factorization has been exploited in a variety of contexts to improve the optimization problems. Given an m × n matrix A of rank r, there exists a factorization A = B×C where B is an m×r full column rank matrix and C is an r × n full row rank matrix. Thus, we can replace A by B and C. To reduce the parameters of A by a fraction p, it is essential to ensure that mr+ rn < pmn, i.e., the rank of A should satisfy that r < pmn/(m+n). To this end, [59] applies the low-rank matrix factorization to the final weight layer in a deep CNN, resulting about 30-50% speedup in training time at little loss in accuracy. Similarly, [60] applies singular value decomposition on each layer of a deep CNN to reduce the model size by 71% with less than 1% relative accuracy loss. Inspired by [61] which demonstrates the redundancy in the parameters of deep neural networks, Denton et al. [62] and Jaderberg et al. [63] independently investigate the redundancy within the convolutional filters and develop approximations to reduced the required computations.\n7 C. Vector quantization\nVector Quantization (VQ) is a method for compressing densely connected layers to make CNN models smaller. Similar to scalar quantization where a large set of numbers is mapped to a smaller set [64], VQ quantizes groups of numbers together rather than addressing them one at a time. In 2013, Denil et al. [61] demonstrate the presence of redundancy in neural network parameters, and use VQ to significantly reduce the number of dynamic parameters in deep models. Gong et al. [65] investigate the information theoretical vector quantization methods for compressing the parameters of CNNs, and they obtain parameter prediction results similar to those of [61]. They also find that VQ methods have a clear gain over existing matrix factorization methods, and among the VQ methods, structured quantization methods such as product quantization work significantly better than other methods (e.g., residual quantization [66], scalar quantization [67])."
    }, {
      "heading" : "V. APPLICATIONS OF CNNS",
      "text" : "In this section, we introduce some recent works that apply CNNs to achieve state-of-the-art performance, including image classification, object tracking, pose estimation, text detection, visual saliency detection, action recognition and scene labeling.\nA. Image Classification\nCNNs have been applied in image classification for a long time [68–71]. Compared with other methods, CNNs can achieve better classification accuracy on large scale datasets [4], [8], [72], [73] due to their capability of joint feature learning and classifier learning. The breakthrough of large scale image classification comes in 2012. Alex Krizhevsky et al. [4] develop the AlexNet and achieve the best performance in ILSVRC 2012. After the success of AlexNet, several works have made significant improvements in classification accuracy by either reducing filter size [7] or expanding the network depth [8], [9].\nBuilding a hierarchy of classifiers is a common strategy for image classification with a large number of classes [74]. [75] is one of the earliest attempts to introduce category hierarchy in CNN, in which a discriminative transfer learning with treebased priors is proposed. They use a hierarchy of classes for sharing information among related classes in order to improve performance for classes with very few training examples. Similar to [75], [76] builds a tree structure to learn finegrained features for subcategory recognition. [77] proposes a training method that grows a network not only incrementally but also hierarchically. In their method, classes are grouped according to similarities and are self-organized into different levels. [78] introduces a hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. They decompose the classification task into two steps. The coarse category CNN classifier is first used to separate easy classes from each other, and then those more challenging classes are routed downstream to fine category classifiers for further\nprediction. This architecture follows the coarse-to-fine classification paradigm and can achieve lower error at the cost of affordable increase of complexity.\nSubcategory classification is another rapidly growing subfield of image classification. There are already some finegrained image datasets (such as Flower [79], Birds [80], [81], Dogs [82], Cars [83] and Shoes [84]). Using object part information is beneficial for fine-grained classification. Generally, the accuracy can be improved by localizing important parts of objects and representing their appearances discriminatively. Along this way, Branson et al. [85] propose a method which detects parts and extracts CNN features from multiple posenormalized regions. Part annotation informations is used to learn a compact pose normalization space. They also build a model that integrates lower-level feature layers with posenormalized extraction routines and higher-level feature layers with unaligned image features to improve the classification accuracy. Zhang et al. [43] propose a part-based R-CNN which can learn whole-object and part detectors. They use selective search [86] to generate the part proposals, and apply nonparametric geometric constraints to more accurately localize parts. Lin et al. [87] incorporate part localization, alignment, and classification into one recognition system which is called Deep LAC. Their system is composed of three sub-networks: localization sub-network is used to estimate the part location, alignment sub-network receives the location as input and performs template alignment [88], and classification sub-network takes pose aligned part images as input to predict the category label. They also propose a value linkage function to link the sub-networks and make them work as a whole in training and testing.\nAs can be noted, all the above mentioned methods make use of part annotation information for supervised training. However, these annotations are not easy to collect and these systems are difficult to scale up to handle many types of fine grained classes. To avoid this problem, some researchers propose to find localized parts or regions in an unsupervised manner. Krause et al. [89] use the ensemble of localized learned feature representations for fine-grained classification, they use co-segmentation and alignment to generate parts, and then compare the appearance of each part and aggregate the similarities together. In their latest paper [90]. They combine co-segmentation and alignment in a discriminative mixture to generate parts for facilitating fine-grained classification. [91] applies visual attention in CNN for fine-grained classification. Their classification pipeline is composed of three types of attentions: the bottom-up attention proposes candidate patches, the object-level top-down attention selects relevant patches of a certain object, and the part-level top-down attention localizes discriminative parts. These attentions are combined to train domain specific networks which can help to find foreground object or object parts and extract discriminative features. [92] proposes bilinear models for fine-grained image classification. The recognition architecture consists of two feature extractors. The outputs of two feature extractors are multiplied using outer product at each location of the image, and are pooled to obtain an image descriptor.\n8"
    }, {
      "heading" : "B. Object tracking",
      "text" : "Object tracking has played important roles in a wide range of computer vision applications. The success in object tracking relies heavily on how robust the representation of target appearance is against several challenges such as view point changes, illumination changes, and occlusions.\nCNNs have recently drawn a lot of attentions in computer vision community as well as visual tracking field. There are several attempts to employ CNNs for visual tracking. Fan et al. [93] use CNN as a base learner. It learns a separate class-specific network to track objects. In [93], the authors design a CNN tracker with a shift-variant architecture. Such an architecture plays a key role so that it turns the CNN model from a detector into a tracker. The features are learned during offline training. Different from traditional trackers which only extract local spatial structures, this CNN based tracking method extracts both spatial and temporal structures by considering the images of two consecutive frames. Because the large signals in the temporal information tend to occur near objects that are moving, the temporal structures provide a crude velocity signal to tracking.\nLi et al. [94] propose a target-specific CNN for object tracking, where the CNN is trained incrementally during tracking with new examples obtained online. They employ a candidate pool of multiple CNNs as a data-driven model of different instances of the target object. Individually, each CNN maintains a specific set of kernels that favourably discriminate object patches from their surrounding background using all available low-level cues. These kernels are updated in an online manner at each frame after being trained with just one instance at the initialization of the corresponding CNN. Instead of learning one complicated and powerful CNN model for all the appearance observations in the past, [94] uses a relatively small number of filters in the CNN within a framework equipped with a temporal adaptation mechanism. Given a frame, the most promising CNNs in the pool are selected to evaluate the hypothesises for the target object. The hypothesis with the highest score is assigned as the current detection window and the selected models are retrained using a warm-start backpropagation which optimizes a structural loss function.\nIn [95], a CNN object tracking method is proposed to address limitations of handcrafted features and shallow classifier structures in object tracking problem. The discriminative features are first automatically learned via a CNN. To alleviate the tracker drifting problem caused by model update, the tracker exploits the ground truth appearance information of the object labeled in the initial frames and the image observations obtained online. A heuristic schema is used to judge whether updating the object appearance models or not.\nHong et al. [96] propose a visual tracking algorithm based on a pre-trained CNN, where the network is trained originally for large-scale image classification and the learned representation is transferred to describe target. On top of the hidden layers in the CNN, they put an additional layer of an online SVM to learn a target appearance discriminatively against background. The model learned by SVM is used to compute\na target-specific saliency map by back-projecting the information relevant to target to input image space. And they exploit the target-specific saliency map to obtain generative target appearance models and perform tracking with understanding of spatial configuration of target."
    }, {
      "heading" : "C. Pose estimation",
      "text" : "Since the breakthrough in deep structure learning, many recent works pay more attention to learn multiple levels of representations and abstractions for human-body pose estimation task with CNNs. Like other visual recognition tasks, state-of-the-art performance on the task of human-body pose estimation has achieved a significant improvement due to the large learning capacity of CNNs and the availability of more comprehensive training.\nDeepPose [97] is the first application of CNNs to human pose estimation problem. In this work, pose estimation is formulated as a CNN-based regression problem to body joint coordinates. A cascade of 7-layered CNNs are presented to reason about pose in a holistic manner. Unlike the previous works that usually explicitly design graphical model and part detectors, the DeepPose ascribes to a holistic view of human pose estimation to capture the full context of each body joint by taking the whole image as the input and output the final human pose. From the experiments, DeepPose can outperform the deformable part models based methods [98–100] on two widely used datasets, FLIC [101] and LSP [102].\nExcept this holistic manner, some recent works exploit CNN to directly learn representation of local body parts instead of using hand-crafted low-level features. Ajrun et al. [103] present a CNN based end-to-end learning approach for fullbody human pose estimation, in which CNN part detectors and an Markov Random Field (MRF)-like spatial model are jointly trained, and pair-wise potentials in the graph are computed using convolutional priors. In a series of papers, Tompson et al. [104] use a multi-resolution CNN to compute heat-map for each body part. Different from [103], Tompson et al. [104] learn the body part prior model and implicitly the structure of the spatial model. Specifically, They start by connecting every body part to itself and to every other body part in a pair-wise fashion, and use a fully-connected graph to model the spatial prior. As an extension of [104], Tompson et al. [42] propose a CNN architecture which includes a position refinement model after a rough pose estimation CNN. This refinement model, which is a Siamese network [105], is jointly trained in cascade with the off-the-shelf model [104]. In a similar work with [104], Chen et al. [106], [107] also combine graphical model with CNN. They exploit a CNN to learn conditional probabilities for the presence of parts and their spatial relationships, which are used in unary and pairwise terms of the graphical model. The learned conditional probabilities can be regarded as low-dimensional representations of the body pose. There is also a pose estimation method called dual-source CNN [108] that integrates graphical models and holistic style. It takes the full body image and the holistic view of the local parts as inputs to combine both local and contextual information.\nIn addition to still image pose estimation with CNN, recently researchers also apply CNN to human pose estimation\n9 in videos. Based on the work [104], Jain et al. [109] also incorporate RGB features and motion features to a multiresolution CNN architecture to further improve accuracy. Specifically, The CNN works in a sliding-window manner to perform pose estimation. The input of the CNN is a 3D tensor which consists of an RGB image and its corresponding motion features, and the output is a 3D tensor containing responsemaps of the joints. In each response map, the value of each location denote the energy for presence the corresponding joint at that pixel location. The multi-resolution processing is achieved by simply down sampling the inputs and feeding them to the network."
    }, {
      "heading" : "D. Text detection and recognition",
      "text" : "The task of recognizing text in image has been widely studied for a long time. Traditionally, optical character recognition (OCR) is the major focus. OCR techniques mainly perform text recognition on images in rather constrained visual environments (e.g., clean background, well-aligned text). Recently, the focus has been shifted to text recognition on scene images due to the growing trend of high-level visual understanding in computer vision research. The scene images are captured in unconstrained environments where there exists a large amount of appearance variations which poses great difficulties to existing OCR techniques. Such a concern can be mitigated by using stronger and richer feature representations such as those learned by CNN models. Along the line of improving the performance of scene text recognition with CNN, a few works have been proposed. The works can be coarsely categorized into three types: (1) text detection and localization without recognition, (2) text recognition on cropped text images, and (3) end-to-end text spotting that integrates both text detection and recognition:\n1) Text detection: One of the pioneering works to apply CNN for scene text detection is [110]. The CNN model employed by [110] learns on cropped text patches and non-text scene patches to discriminate between the two. The text are then detected on the response maps generated by the CNN filters given the multiscale image pyramid of the input. To reduce the search space for text detection, [111] proposes to obtain a set of character candidates via Maximally Stable Extremal Regions (MSER) and filter the candidates by CNN classification. Another work that combines MSER and CNN for text detection is [112]. In [112], CNN is used to distinguish text-like MSER components from non-text components, and cluttered text components are split by applying CNN in a sliding window manner followed by Non-Maximal Suppression (NMS). Other than localization of text, there is an interesting work [113] that makes use of CNN to determine whether the input image contains text, without telling where the text is exactly located. In [113], text candidates are obtained using MSER which are then passed into a CNN to generate visual features, and lastly the global features of the images are constructed by aggregating the CNN features in a Bag-ofWords (BoW) framework.\n2) Text recognition: [114] proposes a CNN model with multiple softmax classifiers in its final layer, which is formulated in such a way that each classifier is responsible\nfor character prediction at each sequential location in the multi-digit input image. As an attempt to recognize text without using lexicon and dictionary, [115] introduces a novel Conditional Random Fields (CRF)-like CNN model to jointly learn character sequence prediction and bigram generation for scene text recognition. The more recent text recognition methods supplement conventional CNN models with variants of recurrent neural networks (RNN) to better model the sequence dependencies between characters in text. In [116], CNN extracts rich visual features from character-level image patches obtained via sliding window, and the sequence labelling is carried out by an enhanced RNN variant called Long Short-Term Memory (LSTM) [117]. The method presented in [118] is very similar to [116], except that in [118], lexicon can be taken into consideration to enhance text recognition performance.\n3) End-to-end text spotting: For end-to-end text spotting, [10] applies a CNN model originally trained for character classification to perform text detection. Going in a similar direction as [10], the CNN model proposed in [119] enables feature sharing across the four different subtasks of an end-to-end text spotting system: text detection, character case-sensitive and insensitive classification, and bigram classification. [120] makes use of CNNs in a very comprehensive way to perform end-to-end text spotting. In [120], the major subtasks of its proposed system, namely text bounding box filtering, text bounding box regression, and text recognition are each tackled by a separate CNN model.\nE. Visual saliency detection\nThe technique to locate important regions in imagery is referred to as visual saliency prediction. It is a challenging research topic, with a vast number of computer vision and image processing applications facilitated by it. Recently, a couple of works have been proposed to harness the strong visual modeling capability of CNNs for visual saliency prediction.\nMulti-contextual information is a crucial prior in visual saliency prediction, and it has been used concurrently with CNN in most of the considered works [121–125]. [121] introduces a novel saliency detection algorithm which sequentially exploits local context and global context. The local context is handled by a CNN model which assigns a local saliency value to each pixel given the input of local image patches, while the global context (object-level information) is handled by a deep fully-connected feedforward network. In [122], the CNN parameters are shared between the global-context and local-context models, for predicting the saliency of superpixels found within object proposals. The CNN model adopted in [123] is pre-trained on large-scale image classification dataset and then shared among different contextual levels for feature extraction. The outputs of the CNN at different contextual levels are then concatenated as input to be passed into a trainable fully-connected feedforward network for saliency prediction. Similar to [122], [123], the CNN model used in [124] for saliency prediction are shared across three CNN streams, with each stream taking input of a different contextual scale. [125] derives a spatial kernel and a range kernel to\n10\nproduce two meaningful sequences as 1-D CNN inputs, to describe color uniqueness and color distribution respectively. The proposed sequences are advantageous over inputs of raw image pixels because they can reduce the training complexity of CNN, while being able to encode the contextual information among superpixels.\nThere are also CNN-based saliency prediction approaches [126–128] that do not consider multi-contextual information. Instead, they rely very much on the powerful representation capability of CNN. In [126], an ensemble of CNNs is derived from a large number of randomly instantiated CNN models, to generate good features for saliency detection. The CNN models instantiated in [126] are however not deep enough because the maximum number of layers is capped at three. By using a pre-trained and deeper CNN model with 5 convolutional layers, [127] (Deep Gaze) learns a separate saliency model to jointly combine the responses from every CNN layer and predict saliency values. [128] is the only work making use of CNN to perform visual saliency prediction in an end-to-end manner, which means the CNN model accepts raw pixels as input and produces saliency map as output. [128] argues that the success of the proposed end-to-end method is attributed to its not-so-deep CNN architecture which attempts to prevent overfitting."
    }, {
      "heading" : "F. Action recognition",
      "text" : "Action recognition, the behaviour analysis of human subjects and classifying their activities based on their visual appearance and motion dynamics, is one of the challenging problems in computer vision. Generally, this problem can be divided to two major groups: action analysis in still images and in videos. For both of these two groups, effective CNN based methods are proposed. In this subsection we briefly introduce the latest advances on these two groups.\n1) Action Recognition in Still Images: The work of [129] has shown the output of last few layers of a trained CNN can be used as a general visual feature descriptor for a variety of tasks. The same intuition is utilized for action recognition by [8], [130], in which they use the outputs of the penultimate layer of a pre-trained CNN to represent full images of actions as well as the human bounding boxes inside them, and achieve a high level of performance in action classification. Gkioxari et al. [131] add a part detection to this framework. Their part detector is a CNN based extension to the original Poselet [132] method.\nCNN based representation of contextual information is utilized for action recognition in [133]. They search for the most representative secondary region within a large number of object proposal regions in the image and add contextual features to the description of the primary region (ground truth bounding box of human subject) in a bottom-up manner. They utilize a CNN to represent and fine-tune the representations of the primary and the contextual regions.\n2) Action Recognition in Video Sequences: Applying CNNs on videos is challenging because traditional CNNs are designed to represent two dimentional pure spatial signals but in videos a new temporal axis is added which is essentially\ndifferent from the spatial variations in images. The sizes of the video signals are also in higher orders in comparison to those of images which makes it more difficult to apply convolutional networks on.\nJi et al. [134] propose to consider the temporal axis in a similar manner as other spatial axes and introduce a network of 3D convolutional layers to be applied on video inputs. Recently Tran et al. [135] study the performance, efficiency, and effectiveness of this approach and show its strengths compared to other approaches.\nAnother approach to apply CNNs on videos is to keep the convolutions in 2D and fuse the feature maps of consecutive frames, as proposed by [136]. They evaluate three different fusion policies: late fusion, early fusion, and slow fusion, and compare them with applying the CNN on individual single frames. One more step forward for better action recognition via CNNs is to separate the representation to spatial and temporal variations and train individual CNNs for each of them, as proposed by Simonyan and Zisserman [137]. First stream of this framework is a traditional CNN applied on all the frames and the second receives the dense optical flow of the input videos and trains another CNN which is identical to the spatial stream in size and structure. The output of the two streams are combined in a class score fusion step. Chéron et al. [138] utilize the two stream CNN on the localized parts of the human body and show the aggregation of part-based local CNN descriptors can effectively improve the performance of action recognition. Another approach to model the dynamics of videos differently from spatial variations, is to feed the CNN based features of individual frames to a sequence learning module e.g., a recurrent neural network. [139] studies different configurations of applying LSTM units as the sequence learner in this framework."
    }, {
      "heading" : "G. Scene Labeling",
      "text" : "Scene Labeling (also termed as scene parsing, scene semantic segmentation) builds a bridge towards deeper scene understanding. The goal is to relate one semantic class (road, water, sea, etc.) to each pixel. Generally, “thing” pixels (car, person, etc.) in real world images can be quite different due to their scale, illumination and pose variation, meanwhile “stuff” pixels are very similar (road, sea, etc) in a local close-up view. Therefore, the local classification for pixels is quite challenging.\nThe recent advance of Convolutional Neural Networks (CNNs) [1], [4] has revolutionized the computer vision community due to their outstanding performance in a wide variety of tasks. Recently, CNNs have also been successfully applied to scene labeling tasks. In this scenario, CNNs are used to model the class likelihood of pixels directly from local image patches. They are able to learn strong features and classifiers to discriminate the local visual subtleties.\nFarabet et al. [140] have pioneered to apply CNNs to scene labeling tasks. They feed their Multi-scale ConvNet with different scale image patches, and they show that the learned network is able to perform much better than systems with handcrafted features, which implicitly elucidate the discriminative\n11\npower of the features generated from CNNs. Besides, this network is also successfully applied to RGB-D scene labeling [141]. To enable the CNNs to have a large field of view over pixels, Pinheiro et al. [142] develop the recurrent CNNs. More specifically, the identical CNNs are applied recurrently to the output maps of CNNs in the previous iterations. By doing this, they can achieve slightly better labeling results while significantly reduces the inference times. Shuai et al. [143– 145] train the parametric CNNs by sampling image patches, which speeds up the training time dramatically. They find that patch-based CNNs suffer from local ambiguity problems, and [143] solve it by integrating global beliefs. [144] and [145] use the recurrent neural networks to model the contextual dependencies among image features, and dramatically boost the labeling performance.\nMeanwhile, researchers are exploiting to use the pre-trained deep CNNs for object semantic segmentation. Mostajabi et al. [146] apply the local and proximal features from a ConvNet and apply the Alex-net [4] to obtain the distant and global features, and their concatenation gives rise to the zoom-out features. They achieve very competitive results on the semantic segmentation tasks. Long et al. [147] train a fully convolutional Network (FCN) to directly map the input images to dense label maps. The convolution layers of the FCNs are initialized from the model pre-trained on ImageNet classification dataset, and the deconvolution layers are learned to upsample the resolution of label maps. They also achieve very promising labeling performance on semantic segmentation. Chen et al. [148] also apply the pre-trained deep CNNs to emit the labels of pixels. Considering that the imperfectness of boundary alignment, they further use fully connected Conditional Random Fields (CRF) to boost the labeling performance."
    }, {
      "heading" : "VI. CONCLUSIONS",
      "text" : "Deep CNNs have made breakthroughs in processing images, video, speech and text. In this paper, we give an extensive survey on recent advances of CNNs. We have discussed the improvements of CNN on different aspects, namely, layer design, activation function, loss function, regularization, optimization and fast computation. We focus on applying CNNs in computer vision tasks, and introduce some CNN based works that achieve state-of-the-art performance, including image classification, object tracking, pose estimation, text detection, visual saliency detection, action recognition and scene labeling.\nAlthough there has been great success of CNN in experimental evaluation, it still lacks of theoretical proof of why does it work. It is desirable to make more efforts on investigating the fundamental principles of CNNs. We hope that this paper does not only provide a better understanding of CNNs but also facilitates future research activities and application developments in the field of CNNs."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This research was carried out at the Rapid-Rich Object Search (ROSE) Lab at the Nanyang Technological University,\nSingapore. The ROSE Lab is supported by the National Research Foundation, Prime Ministers Office, Singapore, under its IDM Futures Funding Initiative and administered by the Interactive and Digital Media Programme Office."
    } ],
    "references" : [ {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Handwritten digit recognition with a backpropagation network",
      "author" : [ "B.B. Le Cun", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel" ],
      "venue" : "Advances in neural information processing systems. Citeseer, 1990.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Theory of the backpropagation neural network",
      "author" : [ "R. Hecht-Nielsen" ],
      "venue" : "International Joint Conference on Neural Networks, 1989, pp. 593– 605.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, 2012, pp. 1097–1105.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "ICML, 2010, pp. 807–814.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Improving neural networks by preventing coadaptation of feature detectors",
      "author" : [ "G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580, 2012.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "ECCV, 2014.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "ICLR, 2015.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "CoRR, vol. abs/1409.4842, 2014.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "End-to-end text recognition with convolutional neural networks",
      "author" : [ "T. Wang", "D. Wu", "A. Coates", "A. Ng" ],
      "venue" : "International Conference on Pattern Recognition (ICPR), 2012, pp. 3304–3308.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Linear spatial pyramid matching using sparse coding for image classification",
      "author" : [ "J. Yang", "K. Yu", "Y. Gong", "T. Huang" ],
      "venue" : "CVPR, 2009.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A theoretical analysis of feature pooling in visual recognition",
      "author" : [ "Y. Boureau", "J. Ponce", "Y. LeCun" ],
      "venue" : "ICML, 2010, pp. 111–118.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Unsupervised learning of invariant feature hierarchies with applications to object recognition",
      "author" : [ "M. Ranzato", "F.J. Huang", "Y. Boureau", "Y. LeCun" ],
      "venue" : "CVPR, 2007.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Deep learning using linear support vector machines",
      "author" : [ "Y. Tang" ],
      "venue" : "arXiv preprint arXiv:1306.0239, 2013.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Network in network",
      "author" : [ "M. Lin", "Q. Chen", "S. Yan" ],
      "venue" : "CoRR, vol. abs/1312.4400, 2013.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Rethinking the Inception Architecture for Computer Vision",
      "author" : [ "C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna" ],
      "venue" : "2015, arXiv:1512.00567v1.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A model of neuronal responses in visual area mt",
      "author" : [ "E.P. Simoncelli", "D.J. Heeger" ],
      "venue" : "Vision research, vol. 38, no. 5, pp. 743–761, 1998.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Complex cell pooling and the statistics of natural images",
      "author" : [ "A. Hyvärinen", "U. Köster" ],
      "venue" : "Network: Computation in Neural Systems, vol. 18, no. 2, pp. 81–100, 2007.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Signal recovery from pooling representations",
      "author" : [ "J.B. Estrach", "A. Szlam", "Y. Lecun" ],
      "venue" : "ICML, 2014, pp. 307–315.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learned-norm pooling for deep feedforward and recurrent neural networks",
      "author" : [ "C. Gulcehre", "K. Cho", "R. Pascanu", "Y. Bengio" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases. Springer, 2014, pp. 530–546.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus" ],
      "venue" : "ICML, 2013, pp. 1058–1066.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Mixed pooling for convolutional neural networks",
      "author" : [ "D. Yu", "H. Wang", "P. Chen", "Z. Wei" ],
      "venue" : "Rough Sets and Knowledge Technology. Springer, 2014, pp. 364–375.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Stochastic pooling for regularization of deep convolutional neural networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "CoRR, vol. abs/1301.3557, 2013.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Spectral representations for convolutional neural networks",
      "author" : [ "O. Rippel", "J. Snoek", "R.P. Adams" ],
      "venue" : "arXiv preprint arXiv:1506.03767, 2015.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fast training of convolutional networks through ffts",
      "author" : [ "M. Mathieu", "M. Henaff", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1312.5851, 2013.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Spatial pyramid pooling in deep convolutional networks for visual recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "Computer Vision– ECCV 2014, 2014, pp. 346–361.  12",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Unsupervised discovery of mid-level discriminative patches",
      "author" : [ "S. Singh", "A. Gupta", "A. Efros" ],
      "venue" : "ECCV, pp. 73–86, 2012.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multi-scale orderless pooling of deep convolutional activation features",
      "author" : [ "Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik" ],
      "venue" : "ECCV, 2014.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Aggregating local image descriptors into compact codes",
      "author" : [ "H. Jégou", "F. Perronnin", "M. Douze", "J. Sanchez", "P. Perez", "C. Schmid" ],
      "venue" : "PAMI, vol. 34, no. 9, pp. 1704–1716, 2012.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Rectifier nonlinearities improve neural network acoustic models",
      "author" : [ "A.L. Maas", "A.Y. Hannun", "A.Y. Ng" ],
      "venue" : "ICML, 2013.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1502.01852, 2015.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1852
    }, {
      "title" : "Empirical evaluation of rectified activations in convolutional network",
      "author" : [ "B. Xu", "N. Wang", "T. Chen", "M. Li" ],
      "venue" : "arXiv preprint arXiv:1505.00853, 2015.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fast and accurate deep network learning by exponential linear units (elus)",
      "author" : [ "D.-A. Clevert", "T. Unterthiner", "S. Hochreiter" ],
      "venue" : "arXiv preprint arXiv:1511.07289, 2015.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Maxout networks",
      "author" : [ "I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1302.4389, 2013.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Improving deep neural networks with probabilistic maxout units",
      "author" : [ "J.T. Springenberg", "M. Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1312.6116, 2013.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Solving large scale linear prediction problems using stochastic gradient descent algorithms",
      "author" : [ "T. Zhang" ],
      "venue" : "Proceedings of the twenty-first international conference on Machine learning. ACM, 2004, p. 116.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The mnist database of handwritten digits",
      "author" : [ "Y. LeCun", "C. Cortes", "C.J. Burges" ],
      "venue" : "1998.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Learning a similarity metric discriminatively, with application to face verification",
      "author" : [ "S. Chopra", "R. Hadsell", "Y. LeCun" ],
      "venue" : "CVPR, 2005.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Deephash: Getting regularization, depth and fine-tuning right",
      "author" : [ "J. Lin", "O. Morere", "V. Chandrasekhar", "A. Veillard", "H. Goh" ],
      "venue" : "arXiv preprint arXiv:1501.04711, 2015.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fast dropout training",
      "author" : [ "S. Wang", "C. Manning" ],
      "venue" : "ICML, 2013.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Adaptive dropout for training deep neural networks",
      "author" : [ "J. Ba", "B. Frey" ],
      "venue" : "Advances in Neural Information Processing Systems, 2013, pp. 3084–3092.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient object localization using convolutional networks",
      "author" : [ "J. Tompson", "R. Goroshin", "A. Jain", "Y. LeCun", "C. Bregler" ],
      "venue" : "arXiv preprint arXiv:1411.4280, 2014.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Part-based r-cnns for fine-grained category detection",
      "author" : [ "N. Zhang", "J. Donahue", "R. Girshick", "T. Darrell" ],
      "venue" : "ECCV, 2014, pp. 834–849.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "X. Glorot", "Y. Bengio" ],
      "venue" : "International conference on artificial intelligence and statistics, 2010, pp. 249–256.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "Proceedings of the ACM International Conference on Multimedia. ACM, 2014, pp. 675–678.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Bottou" ],
      "venue" : "Proceedings of COMPSTAT, 2010, pp. 177–186.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Fast training of object detection using stochastic gradient descent",
      "author" : [ "R.G.J. Wijnhoven", "P.H.N. de With" ],
      "venue" : "ICPR, 2010, pp. 424– 427.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Parallelized stochastic gradient descent",
      "author" : [ "M. Zinkevich", "M. Weimer", "L. Li", "A.J. Smola" ],
      "venue" : "Advances in neural information processing systems, 2010, pp. 2595–2603.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Hogwild: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "B. Recht", "C. Re", "S. Wright", "F. Niu" ],
      "venue" : "Advances in Neural Information Processing Systems, 2011, pp. 693–701.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Deep learning of representations: Looking forward",
      "author" : [ "Y. Bengio" ],
      "venue" : "Statistical Language and Speech Processing. Springer, 2013, pp. 1– 37.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le" ],
      "venue" : "Advances in Neural Information Processing Systems, 2012, pp. 1223–1231.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Gpu asynchronous stochastic gradient descent to speed up neural network training",
      "author" : [ "T. Paine", "H. Jin", "J. Yang", "Z. Lin", "T. Huang" ],
      "venue" : "arXiv preprint arXiv:1312.6186, 2013.",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A fast parallel sgd for matrix factorization in shared memory systems",
      "author" : [ "Y. Zhuang", "W.-S. Chin", "Y.-C. Juan", "C.-J. Lin" ],
      "venue" : "Proceedings of the 7th ACM conference on Recommender systems. ACM, 2013, pp. 249–256.",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167, 2015.",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "cudnn: Efficient primitives for deep learning",
      "author" : [ "S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer" ],
      "venue" : "arXiv preprint arXiv:1410.0759, 2014.",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fast convolutional nets with fbfft: A gpu performance evaluation",
      "author" : [ "N. Vasilache", "J. Johnson", "M. Mathieu", "S. Chintala", "S. Piantino", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1412.7580, 2014.",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun" ],
      "venue" : "CoRR, vol. abs/1312.6229, 2013.",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Low-rank matrix factorization for deep neural network training with high-dimensional output targets",
      "author" : [ "T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran" ],
      "venue" : "ICASSP, 2013.",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Restructuring of deep neural network acoustic models with singular value decomposition.",
      "author" : [ "J. Xue", "J. Li", "Y. Gong" ],
      "venue" : "in INTER- SPEECH,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2013
    }, {
      "title" : "Predicting parameters in deep learning",
      "author" : [ "M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas" ],
      "venue" : "Advances in Neural Information Processing Systems, 2013, pp. 2148–2156.",
      "citeRegEx" : "61",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Exploiting linear structure within convolutional networks for efficient evaluation",
      "author" : [ "E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus" ],
      "venue" : "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, 2014, pp. 1269– 1277.",
      "citeRegEx" : "62",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Speeding up convolutional neural networks with low rank expansions",
      "author" : [ "M. Jaderberg", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "arXiv preprint arXiv:1405.3866, 2014.",
      "citeRegEx" : "63",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Efficient scalar quantization of exponential and laplacian random variables",
      "author" : [ "G.J. Sullivan" ],
      "venue" : "Information Theory, IEEE Transactions on, vol. 42, no. 5, pp. 1365–1374, 1996.",
      "citeRegEx" : "64",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Compressing deep convolutional networks using vector quantization",
      "author" : [ "Y. Gong", "L. Liu", "M. Yang", "L. Bourdev" ],
      "venue" : "arXiv preprint arXiv:1412.6115, 2014.",
      "citeRegEx" : "65",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Approximate nearest neighbor search by residual vector quantization",
      "author" : [ "Y. Chen", "T. Guan", "C. Wang" ],
      "venue" : "Sensors, vol. 10, no. 12, pp. 11 259– 11 273, 2010.",
      "citeRegEx" : "66",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Sequential scalar quantization of color images",
      "author" : [ "R. Balasubramanian", "C.A. Bouman", "J.P. Allebach" ],
      "venue" : "Journal of Electronic Imaging, vol. 3, no. 1, pp. 45–59, 1994.",
      "citeRegEx" : "67",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Face recognition: A convolutional neural-network approach",
      "author" : [ "S. Lawrence", "C.L. Giles", "A.C. Tsoi", "A.D. Back" ],
      "venue" : "Neural Networks, IEEE Transactions on, vol. 8, no. 1, pp. 98–113, 1997.",
      "citeRegEx" : "68",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Best practices for convolutional neural networks applied to visual document analysis",
      "author" : [ "P.Y. Simard", "D. Steinkraus", "J.C. Platt" ],
      "venue" : "null. IEEE, 2003, p. 958.",
      "citeRegEx" : "69",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Large-scale learning with svm and convolutional for generic object categorization",
      "author" : [ "F.J. Huang", "Y. LeCun" ],
      "venue" : "CVPR, 2006, pp. 284–291.",
      "citeRegEx" : "70",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Flexible, high performance convolutional neural networks for image classification",
      "author" : [ "D.C. Ciresan", "U. Meier", "J. Masci", "L. Maria Gambardella", "J. Schmidhuber" ],
      "venue" : "IJCAI Proceedings-International Joint Conference on Artificial Intelligence, vol. 22, no. 1, 2011, p. 1237.",
      "citeRegEx" : "71",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L. Li", "K. Li", "F. Li" ],
      "venue" : "CVPR, 2009, pp. 248–255.",
      "citeRegEx" : "72",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The pascal visual object classes challenge: A retrospective",
      "author" : [ "M. Everingham", "S.A. Eslami", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : "IJCV, vol. 111, no. 1, pp. 98–136, 2014.",
      "citeRegEx" : "73",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Semantic hierarchies for image annotation: A survey",
      "author" : [ "A.-M. Tousch", "S. Herbin", "J.-Y. Audibert" ],
      "venue" : "Pattern Recognition, vol. 45, no. 1, pp. 333–345, 2012.",
      "citeRegEx" : "74",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Discriminative transfer learning with tree-based priors",
      "author" : [ "N. Srivastava", "R.R. Salakhutdinov" ],
      "venue" : "Advances in Neural Information Processing Systems, 2013, pp. 2094–2102.",
      "citeRegEx" : "75",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning fine-grained features via a cnn tree for large-scale classification",
      "author" : [ "Z. Wang", "X. Wang", "G. Wang" ],
      "venue" : "arXiv preprint arXiv:1511.04534, 2015.",
      "citeRegEx" : "76",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Error-driven incremental learning in deep convolutional neural network for largescale image classification",
      "author" : [ "T. Xiao", "J. Zhang", "K. Yang", "Y. Peng", "Z. Zhang" ],
      "venue" : "Proceedings of the ACM International Conference on Multimedia. ACM, 2014, pp. 177–186.",
      "citeRegEx" : "77",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Hd-cnn: Hierarchical deep convolutional neural network for image classification",
      "author" : [ "Z. Yan", "V. Jagadeesh", "D. DeCoste", "W. Di", "R. Piramuthu" ],
      "venue" : "arXiv preprint arXiv:1410.0736, 2014.  13",
      "citeRegEx" : "78",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Automated flower classification over a large number of classes",
      "author" : [ "M.-E. Nilsback", "A. Zisserman" ],
      "venue" : "ICVGIP, 2008, pp. 722–729.",
      "citeRegEx" : "79",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The caltech-ucsd birds-200-2011 dataset",
      "author" : [ "C. Wah", "S. Branson", "P. Welinder", "P. Perona", "S. Belongie" ],
      "venue" : "2011.",
      "citeRegEx" : "80",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Birdsnap: Large-scale fine-grained visual categorization of birds",
      "author" : [ "T. Berg", "J. Liu", "S.W. Lee", "M.L. Alexander", "D.W. Jacobs", "P.N. Belhumeur" ],
      "venue" : "CVPR, 2014, pp. 2019–2026.",
      "citeRegEx" : "81",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Novel dataset for fine-grained image categorization",
      "author" : [ "A. Khosla", "N. Jayadevaprakash", "B. Yao", "L. Fei-Fei" ],
      "venue" : "CVPR, 2011.",
      "citeRegEx" : "82",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A large-scale car dataset for fine-grained categorization and verification",
      "author" : [ "L. Yang", "P. Luo", "C.C. Loy", "X. Tang" ],
      "venue" : "CVPR, 2015.",
      "citeRegEx" : "83",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fine-grained visual comparisons with local learning",
      "author" : [ "A. Yu", "K. Grauman" ],
      "venue" : "CVPR, 2014, pp. 192–199.",
      "citeRegEx" : "84",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Improved bird species recognition using pose normalized deep convolutional nets",
      "author" : [ "S. Branson", "G. Van Horn", "P. Perona", "S. Belongie" ],
      "venue" : "British Machine Vision Conference, 2014.",
      "citeRegEx" : "85",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Selective search for object recognition",
      "author" : [ "J.R. Uijlings", "K.E. van de Sande", "T. Gevers", "A.W. Smeulders" ],
      "venue" : "International journal of computer vision, vol. 104, no. 2, pp. 154–171, 2013.",
      "citeRegEx" : "86",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep lac: Deep localization, alignment and classification for fine-grained recognition",
      "author" : [ "D. Lin", "X. Shen", "C. Lu", "J. Jia" ],
      "venue" : "CVPR, 2015, pp. 1666–1674.",
      "citeRegEx" : "87",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Mutual-informationbased registration of medical images: a survey",
      "author" : [ "J.P. Pluim", "J.A. Maintz", "M. Viergever" ],
      "venue" : "Medical Imaging, IEEE Transactions on, vol. 22, no. 8, pp. 986–1004, 2003.",
      "citeRegEx" : "88",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Learning features and parts for fine-grained recognition",
      "author" : [ "J. Krause", "T. Gebru", "J. Deng", "L.-J. Li", "L. Fei-Fei" ],
      "venue" : "ICPR, 2014, pp. 26–33.",
      "citeRegEx" : "89",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fine-grained recognition without part annotations",
      "author" : [ "J. Krause", "H. Jin", "J. Yang", "L. Fei-Fei" ],
      "venue" : "CVPR, 2015, pp. 5546–5555.",
      "citeRegEx" : "90",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The application of two-level attention models in deep convolutional neural network for fine-grained image classification",
      "author" : [ "T. Xiao", "Y. Xu", "K. Yang", "J. Zhang", "Y. Peng", "Z. Zhang" ],
      "venue" : "CVPR, 2015.",
      "citeRegEx" : "91",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Bilinear cnn models for fine-grained visual recognition",
      "author" : [ "T.-Y. Lin", "A. RoyChowdhury", "S. Maji" ],
      "venue" : "arXiv preprint arXiv:1504.07889, 2015.",
      "citeRegEx" : "92",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Human tracking using convolutional neural networks",
      "author" : [ "J. Fan", "W. Xu", "Y. Wu", "Y. Gong" ],
      "venue" : "Neural Networks, IEEE Transactions on, vol. 21, no. 10, pp. 1610–1623, 2010.",
      "citeRegEx" : "93",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Deeptrack: Learning discriminative feature representations by convolutional neural networks for visual tracking",
      "author" : [ "H. Li", "Y. Li", "F. Porikli" ],
      "venue" : "Proceedings of the British Machine Vision Conference, 2014.",
      "citeRegEx" : "94",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Cnntracker: Online discriminative object tracking via deep convolutional neural network",
      "author" : [ "Y. Chen", "X. Yang", "B. Zhong", "S. Pan", "D. Chen", "H. Zhang" ],
      "venue" : "Applied Soft Computing, 2015.",
      "citeRegEx" : "95",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Online tracking by learning discriminative saliency map with convolutional neural network",
      "author" : [ "S. Hong", "T. You", "S. Kwak", "B. Han" ],
      "venue" : "arXiv preprint arXiv:1502.06796, 2015.",
      "citeRegEx" : "96",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deeppose: Human pose estimation via deep neural networks",
      "author" : [ "A. Toshev", "C. Szegedy" ],
      "venue" : "CVPR, 2014, pp. 1653–1660.",
      "citeRegEx" : "97",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Articulated pose estimation with flexible mixtures-of-parts",
      "author" : [ "Y. Yang", "D. Ramanan" ],
      "venue" : "CVPR. IEEE, 2011, pp. 1385–1392.",
      "citeRegEx" : "98",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Beyond physical connections: Tree models in human pose estimation",
      "author" : [ "F. Wang", "Y. Li" ],
      "venue" : "CVPR, 2013, pp. 596–603.",
      "citeRegEx" : "99",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Poselet conditioned pictorial structures",
      "author" : [ "L. Pishchulin", "M. Andriluka", "P. Gehler", "B. Schiele" ],
      "venue" : "CVPR, 2013, pp. 588–595.",
      "citeRegEx" : "100",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Modec: Multimodal decomposable models for human pose estimation",
      "author" : [ "B. Sapp", "B. Taskar" ],
      "venue" : "CVPR, 2013, pp. 3674–3681.",
      "citeRegEx" : "101",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Clustered pose and nonlinear appearance models for human pose estimation.",
      "author" : [ "S. Johnson", "M. Everingham" ],
      "venue" : "in BMVC,",
      "citeRegEx" : "102",
      "shortCiteRegEx" : "102",
      "year" : 2010
    }, {
      "title" : "Learning human pose estimation features with convolutional networks",
      "author" : [ "A. Jain", "J. Tompson", "M. Andriluka", "G.W. Taylor", "C. Bregler" ],
      "venue" : "April 2014.",
      "citeRegEx" : "103",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Joint training of a convolutional network and a graphical model for human pose estimation",
      "author" : [ "J.J. Tompson", "A. Jain", "Y. LeCun", "C. Bregler" ],
      "venue" : "Advances in Neural Information Processing Systems, 2014, pp. 1799–1807.",
      "citeRegEx" : "104",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Signature verification using a siamese time delay neural network",
      "author" : [ "J. Bromley", "J.W. Bentz", "L. Bottou", "I. Guyon", "Y. LeCun", "C. Moore", "E. Säckinger", "R. Shah" ],
      "venue" : "International Journal of Pattern Recognition and Artificial Intelligence, vol. 7, no. 04, pp. 669–688, 1993.",
      "citeRegEx" : "105",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Articulated pose estimation by a graphical model with image dependent pairwise relations",
      "author" : [ "X. Chen", "A.L. Yuille" ],
      "venue" : "Advances in Neural Information Processing Systems, 2014, pp. 1736–1744.",
      "citeRegEx" : "106",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Parsing occluded people by flexible compositions",
      "author" : [ "X. Chen", "A. Yuille" ],
      "venue" : "CVPR, 2015.",
      "citeRegEx" : "107",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation",
      "author" : [ "X. Fan", "K. Zheng", "Y. Lin", "S. Wang" ],
      "venue" : "CVPR, 2015.",
      "citeRegEx" : "108",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Modeep: A deep learning framework using motion features for human pose estimation",
      "author" : [ "A. Jain", "J. Tompson", "Y. LeCun", "C. Bregler" ],
      "venue" : "ACCV, 2014, pp. 302–315.",
      "citeRegEx" : "109",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Text detection with convolutional neural networks",
      "author" : [ "M. Delakis", "C. Garcia" ],
      "venue" : "VISAPP, 2008.",
      "citeRegEx" : "110",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Robust seed localization and growing with deep convolutional features for scene text detection",
      "author" : [ "H. Xu", "F. Su" ],
      "venue" : "ICMR, 2015, pp. 387–394.",
      "citeRegEx" : "111",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Robust scene text detection with convolution neural network induced mser trees",
      "author" : [ "W. Huang", "Y. Qiao", "X. Tang" ],
      "venue" : "ECCV, 2014.",
      "citeRegEx" : "112",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Automatic discrimination of text and non-text natural images",
      "author" : [ "C. Zhang", "C. Yao", "B. Shi", "X. Bai" ],
      "venue" : "ICDAR, 2015.",
      "citeRegEx" : "113",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multi-digit number recognition from street view imagery using deep convolutional neural networks",
      "author" : [ "I.J. Goodfellow", "J. Ibarz", "S. Arnoud", "V. Shet" ],
      "venue" : "ICLR, 2014.",
      "citeRegEx" : "114",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep structured output learning for unconstrained text recognition",
      "author" : [ "M. Jaderberg", "K. Simonyan", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "ICLR, 2015.",
      "citeRegEx" : "115",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reading scene text in deep convolutional sequences",
      "author" : [ "P. He", "W. Huang", "Y. Qiao", "C.C. Loy", "X. Tang" ],
      "venue" : "CoRR, vol. abs/1506.04395, 2015.",
      "citeRegEx" : "116",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning to forget: Continual prediction with lstm",
      "author" : [ "F.A. Gers", "J. Schmidhuber", "F. Cummins" ],
      "venue" : "Neural computation, vol. 12, no. 10, pp. 2451–2471, 2000.",
      "citeRegEx" : "117",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition",
      "author" : [ "B. Shi", "X. Bai", "C. Yao" ],
      "venue" : "CoRR, vol. abs/1507.05717, 2015.",
      "citeRegEx" : "118",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep features for text spotting",
      "author" : [ "M. Jaderberg", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "ECCV, 2014, pp. 512–528.",
      "citeRegEx" : "119",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Reading text in the wild with convolutional neural networks",
      "author" : [ "M. Jaderberg", "K. Simonyan", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "IJCV, pp. 1–20, 2015.",
      "citeRegEx" : "120",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep networks for saliency detection via local estimation and global search",
      "author" : [ "L. Wang", "H. Lu", "X. Ruan", "M.-H. Yang" ],
      "venue" : "CVPR, 2015.",
      "citeRegEx" : "121",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Saliency detection by multi-context deep learning",
      "author" : [ "R. Zhao", "W. Ouyang", "H. Li", "X. Wang" ],
      "venue" : "CVPR, June 2015.",
      "citeRegEx" : "122",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Visual saliency based on multiscale deep features",
      "author" : [ "G. Li", "Y. Yu" ],
      "venue" : "CVPR, June 2015.",
      "citeRegEx" : "123",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Predicting eye fixations using convolutional neural networks",
      "author" : [ "N. Liu", "J. Han", "D. Zhang", "S. Wen", "T. Liu" ],
      "venue" : "CVPR, 2015.",
      "citeRegEx" : "124",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Supercnn: A superpixelwise convolutional neural network for salient object detection",
      "author" : [ "S. He", "R.W. Lau", "W. Liu", "Z. Huang", "Q. Yang" ],
      "venue" : "IJCV, pp. 1–15, 2015.",
      "citeRegEx" : "125",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Large-scale optimization of hierarchical features for saliency prediction in natural images",
      "author" : [ "E. Vig", "M. Dorr", "D. Cox" ],
      "venue" : "CVPR, 2014.",
      "citeRegEx" : "126",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet",
      "author" : [ "M. Kmmerer", "L. Theis", "M. Bethge" ],
      "venue" : "ICLR, 2015.",
      "citeRegEx" : "127",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "End-to-end convolutional network for saliency prediction",
      "author" : [ "J. Pan", "X. Gir-i Nieto" ],
      "venue" : "CVPR, 2015.",
      "citeRegEx" : "128",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Decaf: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "2014.",
      "citeRegEx" : "129",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning and transferring mid-level image representations using convolutional neural networks",
      "author" : [ "M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic" ],
      "venue" : "CVPR, 2014, pp. 1717–1724.",
      "citeRegEx" : "130",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Actions and attributes from wholes and parts",
      "author" : [ "G. Gkioxari", "R. Girshick", "J. Malik" ],
      "venue" : "2014.",
      "citeRegEx" : "131",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Poselet conditioned pictorial structures",
      "author" : [ "L. Pishchulin", "M. Andriluka", "P. Gehler", "B. Schiele" ],
      "venue" : "CVPR, 2013, pp. 588–595.",
      "citeRegEx" : "132",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Contextual action recognition with r*cnn",
      "author" : [ "G. Gkioxari", "R.B. Girshick", "J. Malik" ],
      "venue" : "CoRR, vol. abs/1505.01197, 2015.",
      "citeRegEx" : "133",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "3d convolutional neural networks for human action recognition",
      "author" : [ "S. Ji", "W. Xu", "M. Yang", "K. Yu" ],
      "venue" : "PAMI, vol. 35, no. 1, pp. 221–231, 2013.",
      "citeRegEx" : "134",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "C3D: generic features for video analysis",
      "author" : [ "D. Tran", "L.D. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri" ],
      "venue" : "CoRR, vol. abs/1412.0767, 2014.",
      "citeRegEx" : "135",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Large-scale video classification with convolutional neural networks",
      "author" : [ "A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei" ],
      "venue" : "CVPR, 2014, pp. 1725–1732.",
      "citeRegEx" : "136",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Two-stream convolutional networks for action recognition in videos",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "Advances in Neural Information Processing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, Eds., 2014, pp. 568–576.",
      "citeRegEx" : "137",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "P-CNN: pose-based CNN features for action recognition",
      "author" : [ "G. Chéron", "I. Laptev", "C. Schmid" ],
      "venue" : "CoRR, vol. abs/1506.03607, 2015.  14",
      "citeRegEx" : "138",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell" ],
      "venue" : "June 2015.",
      "citeRegEx" : "139",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning hierarchical features for scene labeling",
      "author" : [ "C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun" ],
      "venue" : "PAMI, pp. 1915–1929, 2013.",
      "citeRegEx" : "140",
      "shortCiteRegEx" : null,
      "year" : 1915
    }, {
      "title" : "Indoor semantic segmentation using depth information",
      "author" : [ "C. Couprie", "C. Farabet", "L. Najman", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1301.3572, 2013.",
      "citeRegEx" : "141",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Recurrent convolutional neural networks for scene labeling",
      "author" : [ "P. Pinheiro", "R. Collobert" ],
      "venue" : "Proceedings of The 31st International Conference on Machine Learning, 2014, pp. 82–90.",
      "citeRegEx" : "142",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Integrating parametric and non-parametric models for scene labeling",
      "author" : [ "B. Shuai", "G. Wang", "Z. Zuo", "B. Wang", "L. Zhao" ],
      "venue" : "CVPR, 2015, pp. 4249–4258.",
      "citeRegEx" : "143",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Quaddirectional 2d-recurrent neural networks for image labeling",
      "author" : [ "B. Shuai", "Z. Zuo", "W. Gang" ],
      "venue" : "Signal Processing Letters, 2015.",
      "citeRegEx" : "144",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Dag-recurrent neural networks for scene labeling",
      "author" : [ "B. Shuai", "Z. Zuo", "G. Wang", "B. Wang" ],
      "venue" : "arXiv preprint arXiv:1509.00552, 2015.",
      "citeRegEx" : "145",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Feedforward semantic segmentation with zoom-out features",
      "author" : [ "M. Mostajabi", "P. Yadollahpour", "G. Shakhnarovich" ],
      "venue" : "CVPR, 2015.",
      "citeRegEx" : "146",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "J. Long", "E. Shelhamer", "T. Darrell" ],
      "venue" : "CVPR, 2015.",
      "citeRegEx" : "147",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Semantic image segmentation with deep convolutional nets and fully connected crfs",
      "author" : [ "L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille" ],
      "venue" : "ICLR, 2015.",
      "citeRegEx" : "148",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "in [1] and improved in [2].",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 1,
      "context" : "in [1] and improved in [2].",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "Like other neural networks, LeNet-5 has multiple layers and can be trained with the backpropagation algorithm [3].",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : ", AlexNet [4], is similar to LeNet-5 but with a deeper structure.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 4,
      "context" : "ReLU [5] is used as the nonlinear activation function and Dropout [6] is used to reduce overfitting.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "ReLU [5] is used as the nonlinear activation function and Dropout [6] is used to reduce overfitting.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "Among them, three representative works are ZFNet [7], VGGNet [8] and GoogleNet [9].",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 7,
      "context" : "Among them, three representative works are ZFNet [7], VGGNet [8] and GoogleNet [9].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "Among them, three representative works are ZFNet [7], VGGNet [8] and GoogleNet [9].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "1 shows the architecture of LeNet-5 [1] which is introduced by Yann LeCun.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "1: The architecture of LeNet-5 network, which works well on digit classification task (Adapted from [1]).",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "The typical activation functions are sigmoid, tanh and ReLU [5].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "The typical pooling operations are average pooling [10] and max pooling [11–13].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "The typical pooling operations are average pooling [10] and max pooling [11–13].",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "The typical pooling operations are average pooling [10] and max pooling [11–13].",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : "The typical pooling operations are average pooling [10] and max pooling [11–13].",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : "For classification tasks, softmax regression is commonly used as it generates a well-formed probability distribution of the outputs [4].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 13,
      "context" : "Another commonly used method is SVM, which can be combined with CNNs to solve different classification tasks [14].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "[15].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[9] which can be seen as a logical culmination of NIN.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] uses variable filter sizes to capture different visual patterns of different sizes, and approximates the optimal sparse structure by the inception module.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "In their recent paper [16], to find high performance networks with a relatively modest computation cost, they propose several design principles to scale up CNNs according to their experimental evaluation.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 16,
      "context" : "1) Lp Pooling: Lp pooling is a biologically inspired pooling process modelled on complex cells [17], [18].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 17,
      "context" : "1) Lp Pooling: Lp pooling is a biologically inspired pooling process modelled on complex cells [17], [18].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 18,
      "context" : "been theoretically analysed in [19], [20], which suggests that Lp pooling provides better generalization than max pooling.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 19,
      "context" : "been theoretically analysed in [19], [20], which suggests that Lp pooling provides better generalization than max pooling.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "2) Mixed Pooling: Inspired by random Dropout [6] and DropConnect [21], Yu et al.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : "2) Mixed Pooling: Inspired by random Dropout [6] and DropConnect [21], Yu et al.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 21,
      "context" : "[22] propose a mixed pooling method which is the combination of max pooling and average pooling.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "Experiments in [22] show that mixed pooling can better address the overfitting performs and it performs better than max pooling and average pooling.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 22,
      "context" : "3) Stochastic pooling: Stochastic pooling [23] ensures that the non-maximal activations of feature maps are also possible to be utilized.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 23,
      "context" : "4) Spectral pooling: Spectral pooling [24] performs dimensionality reduction by cropping the representation of input in frequency domain.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 24,
      "context" : ", [25]) that employ FFT for convolution kernels.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 25,
      "context" : "[26].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "6) Multi-scale Orderless Pooling: Inspired by [27], Gong et al.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 27,
      "context" : "[28] use multi-scale orderless pooling (MOP) to improve the invariance of CNNs without degrading their discriminative power.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "The activations of local patches are aggregated by VLAD encoding [29], which aim to capture more local, fine-grained details of the image as well as enhancing invariance.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "1) ReLU: Rectified linear unit (ReLU) [5] is one of the most notable non-saturated activation functions.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "It has been shown that deep networks can be trained efficiently using ReLU even without pre-training [4].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : "introduce leaky ReLU (LReLU) [30] which is defined as:",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 30,
      "context" : "[31] propose Parametric Rectified Linear Unit (PReLU) which adaptively",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "is Randomized Leaky Rectified Linear Unit (RReLU) [32].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 31,
      "context" : "[32] also evaluates ReLU, LReLU, PReLU and RReLU on standard image classification task, and concludes that incorporating a non-zero slop for negative part in rectified activation units could consistently improve the performance.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "5) ELU: [33] introduces Exponential Linear Unit (ELU) which enables faster learning of deep neural networks and leads to higher classification accuracies.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 33,
      "context" : "6) Maxout: Maxout [34] is an alternative non-linear function that takes the maximum response across multiple channels at each spatial position.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 33,
      "context" : "As stated in [34], the maxout function is defined as:",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 34,
      "context" : "7) Probout: [35] proposes a probabilistic variant of maxout called probout.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 35,
      "context" : "(15) is Hinge-Loss (L1-Loss), while if p = 2, it is the Squared Hinge-Loss (L2-Loss) [36].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : "[14] investigates and compares the performance of softmax with L2-SVMs in deep networks.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "The results on MNIST [37] demonstrate the superiority of L2-SVM over softmax.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 37,
      "context" : "3) Contrastive loss: Contrastive loss [38] is commonly used to train Siamese network.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 38,
      "context" : "[39] find that the recall rate quickly collapses when using this function.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "In the following subsection, we introduce some effective regularization techniques: Dropout [6], [40], [41] and DropConnect [21].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 39,
      "context" : "In the following subsection, we introduce some effective regularization techniques: Dropout [6], [40], [41] and DropConnect [21].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 40,
      "context" : "In the following subsection, we introduce some effective regularization techniques: Dropout [6], [40], [41] and DropConnect [21].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "In the following subsection, we introduce some effective regularization techniques: Dropout [6], [40], [41] and DropConnect [21].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 5,
      "context" : "[6], and it has been proven to be very effective in reducing overfitting.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "In [6], they apply Dropout to fully-connected layers.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 39,
      "context" : "[40] proposes a fast Dropout method which can perform fast Dropout training by sampling from or integrating a Gaussian approximation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 40,
      "context" : "[41] proposes an adaptive Dropout method, where the Dropout probability for each hidden variable is computed using a binary belief network that shares parameters with the deep network.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 41,
      "context" : "In [42], they find that applying standard Dropout before 1 × 1 convolutional layer generally increases training time but does not prevent overfitting.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 20,
      "context" : "2) DropConnect: DropConnect [21] takes the idea of Dropout a step further.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "The most commonly used initialization method is to randomly set the weights according to Gaussian distributions [4],",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 5,
      "context" : "[6], [8], [43].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[6], [8], [43].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 42,
      "context" : "[6], [8], [43].",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 43,
      "context" : "Glorot and Bengio [44] propose a normalized",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 44,
      "context" : "One of its variants is called “Xavier” in Caffe [45].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 30,
      "context" : "[31] derive a robust initialization method that particularly considers the rectifier nonlinearities.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : ", [9]) to converge while the “Xavier” method [44] cannot.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 43,
      "context" : ", [9]) to converge while the “Xavier” method [44] cannot.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 45,
      "context" : "Instead of computing E[J(θt)], stochastic gradient descent (SGD) [47], [48] estimates the gradients on the basis of a single randomly picked example (x, y) from the training set:",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 46,
      "context" : "Instead of computing E[J(θt)], stochastic gradient descent (SGD) [47], [48] estimates the gradients on the basis of a single randomly picked example (x, y) from the training set:",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 47,
      "context" : "Parallelized SGD methods [49–51] improve SGD to be suitable for parallel, large-scale machine learning.",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 48,
      "context" : "Parallelized SGD methods [49–51] improve SGD to be suitable for parallel, large-scale machine learning.",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 49,
      "context" : "Parallelized SGD methods [49–51] improve SGD to be suitable for parallel, large-scale machine learning.",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 50,
      "context" : "[52] use another asynchronous SGD procedure called Downpour SGD to speed up the large-scale distributed training process on clusters with many CPUs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 51,
      "context" : "[53] basically combines asynchronous SGD with GPUs to accelerate the training time by several times compared to training on a single machine.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 52,
      "context" : "[54] also uses multiple GPUs to asynchronously calculate gradients and update the global",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 53,
      "context" : "3) Batch Normalization: Batch Normalization is proposed by Sergey Ioffe and Christian Szeged [55], which aims to accelerate the entire training process of deep neural networks.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 53,
      "context" : "In [55], they suggest that the Internal Covariate Shift, i.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 24,
      "context" : "[25] carry out the convolutional operation in the Fourier domain with FFTs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 54,
      "context" : "There have already been some GPU-based libraries developed to speed up the training and testing process, such as cuDNN [56] and fbfft [57].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 55,
      "context" : "There have already been some GPU-based libraries developed to speed up the training and testing process, such as cuDNN [56] and fbfft [57].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 56,
      "context" : "This is especially costly when the striding parameter is larger than 1, which is common in many state-of-art networks, such as the early layers in [58] and [9].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "This is especially costly when the striding parameter is larger than 1, which is common in many state-of-art networks, such as the early layers in [58] and [9].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 57,
      "context" : "To this end, [59] applies the low-rank matrix factorization to the final weight layer in a deep CNN, resulting about 30-50% speedup in training time at little loss in accuracy.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 58,
      "context" : "Similarly, [60] applies singular value decomposition on each layer of a deep CNN to reduce the",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 59,
      "context" : "Inspired by [61] which demonstrates the redundancy in the parameters of deep neural networks, Denton et al.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 60,
      "context" : "[62] and Jaderberg et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 61,
      "context" : "[63] independently investigate the redundancy within the convolutional filters and develop approximations to reduced the required computations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 62,
      "context" : "Similar to scalar quantization where a large set of numbers is mapped to a smaller set [64], VQ quantizes groups of numbers together rather than addressing them one at a time.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 59,
      "context" : "[61] demonstrate the presence of redundancy in neural network parameters, and use VQ to significantly reduce the number of dynamic parameters in deep models.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 63,
      "context" : "[65] investigate the information theoretical vector quantization methods for compressing the parameters of CNNs, and they obtain parameter prediction results similar to those of [61].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 59,
      "context" : "[65] investigate the information theoretical vector quantization methods for compressing the parameters of CNNs, and they obtain parameter prediction results similar to those of [61].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 64,
      "context" : ", residual quantization [66], scalar quantization [67]).",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 65,
      "context" : ", residual quantization [66], scalar quantization [67]).",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 66,
      "context" : "CNNs have been applied in image classification for a long time [68–71].",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 67,
      "context" : "CNNs have been applied in image classification for a long time [68–71].",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 68,
      "context" : "CNNs have been applied in image classification for a long time [68–71].",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 69,
      "context" : "CNNs have been applied in image classification for a long time [68–71].",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "Compared with other methods, CNNs can achieve better classification accuracy on large scale datasets [4], [8], [72], [73] due to their capability of joint feature learning and classifier learning.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "Compared with other methods, CNNs can achieve better classification accuracy on large scale datasets [4], [8], [72], [73] due to their capability of joint feature learning and classifier learning.",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 70,
      "context" : "Compared with other methods, CNNs can achieve better classification accuracy on large scale datasets [4], [8], [72], [73] due to their capability of joint feature learning and classifier learning.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 71,
      "context" : "Compared with other methods, CNNs can achieve better classification accuracy on large scale datasets [4], [8], [72], [73] due to their capability of joint feature learning and classifier learning.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "[4] develop the AlexNet and achieve the best performance in ILSVRC 2012.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "After the success of AlexNet, several works have made significant improvements in classification accuracy by either reducing filter size [7] or expanding the network depth [8], [9].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 7,
      "context" : "After the success of AlexNet, several works have made significant improvements in classification accuracy by either reducing filter size [7] or expanding the network depth [8], [9].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 8,
      "context" : "After the success of AlexNet, several works have made significant improvements in classification accuracy by either reducing filter size [7] or expanding the network depth [8], [9].",
      "startOffset" : 177,
      "endOffset" : 180
    }, {
      "referenceID" : 72,
      "context" : "Building a hierarchy of classifiers is a common strategy for image classification with a large number of classes [74].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 73,
      "context" : "[75]",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 73,
      "context" : "Similar to [75], [76] builds a tree structure to learn finegrained features for subcategory recognition.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 74,
      "context" : "Similar to [75], [76] builds a tree structure to learn finegrained features for subcategory recognition.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 75,
      "context" : "[77] proposes a training method that grows a network not only incrementally but also hierarchically.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 76,
      "context" : "[78] introduces a hierarchical deep CNNs (HD-CNNs)",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 77,
      "context" : "There are already some finegrained image datasets (such as Flower [79], Birds [80], [81], Dogs [82], Cars [83] and Shoes [84]).",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 78,
      "context" : "There are already some finegrained image datasets (such as Flower [79], Birds [80], [81], Dogs [82], Cars [83] and Shoes [84]).",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 79,
      "context" : "There are already some finegrained image datasets (such as Flower [79], Birds [80], [81], Dogs [82], Cars [83] and Shoes [84]).",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 80,
      "context" : "There are already some finegrained image datasets (such as Flower [79], Birds [80], [81], Dogs [82], Cars [83] and Shoes [84]).",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 81,
      "context" : "There are already some finegrained image datasets (such as Flower [79], Birds [80], [81], Dogs [82], Cars [83] and Shoes [84]).",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 82,
      "context" : "There are already some finegrained image datasets (such as Flower [79], Birds [80], [81], Dogs [82], Cars [83] and Shoes [84]).",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 83,
      "context" : "[85] propose a method which detects parts and extracts CNN features from multiple posenormalized regions.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 42,
      "context" : "[43] propose a part-based R-CNN which can learn whole-object and part detectors.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 84,
      "context" : "They use selective search [86] to generate the part proposals, and apply nonparametric geometric constraints to more accurately localize parts.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 85,
      "context" : "[87] incorporate part localization, alignment, and classification into one recognition system which is called Deep LAC.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 86,
      "context" : "Their system is composed of three sub-networks: localization sub-network is used to estimate the part location, alignment sub-network receives the location as input and performs template alignment [88], and classification sub-network takes pose aligned part images as input to predict the category label.",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 87,
      "context" : "[89] use the ensemble of localized learned feature representations for fine-grained classification,",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 88,
      "context" : "In their latest paper [90].",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 89,
      "context" : "[91] applies visual attention in CNN for fine-grained classification.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 90,
      "context" : "[92] proposes bilinear models for fine-grained image classification.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 91,
      "context" : "[93] use CNN as a base learner.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 91,
      "context" : "In [93], the authors design a CNN tracker with a shift-variant architecture.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 92,
      "context" : "[94] propose a target-specific CNN for object tracking, where the CNN is trained incrementally during tracking with new examples obtained online.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 92,
      "context" : "Instead of learning one complicated and powerful CNN model for all the appearance observations in the past, [94] uses a relatively small number of filters in the CNN within a framework equipped with a temporal adaptation mechanism.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 93,
      "context" : "In [95], a CNN object tracking method is proposed to address limitations of handcrafted features and shallow classifier structures in object tracking problem.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 94,
      "context" : "[96] propose a visual tracking algorithm based on a pre-trained CNN, where the network is trained originally for large-scale image classification and the learned represen-",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 95,
      "context" : "DeepPose [97] is the first application of CNNs to human pose estimation problem.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 96,
      "context" : "From the experiments, DeepPose can outperform the deformable part models based methods [98–100] on two widely used datasets, FLIC [101] and LSP [102].",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 97,
      "context" : "From the experiments, DeepPose can outperform the deformable part models based methods [98–100] on two widely used datasets, FLIC [101] and LSP [102].",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 98,
      "context" : "From the experiments, DeepPose can outperform the deformable part models based methods [98–100] on two widely used datasets, FLIC [101] and LSP [102].",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 99,
      "context" : "From the experiments, DeepPose can outperform the deformable part models based methods [98–100] on two widely used datasets, FLIC [101] and LSP [102].",
      "startOffset" : 130,
      "endOffset" : 135
    }, {
      "referenceID" : 100,
      "context" : "From the experiments, DeepPose can outperform the deformable part models based methods [98–100] on two widely used datasets, FLIC [101] and LSP [102].",
      "startOffset" : 144,
      "endOffset" : 149
    }, {
      "referenceID" : 101,
      "context" : "[103] present a CNN based end-to-end learning approach for fullbody human pose estimation, in which CNN part detectors and an Markov Random Field (MRF)-like spatial model are jointly trained, and pair-wise potentials in the graph are computed using convolutional priors.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 102,
      "context" : "[104] use a multi-resolution CNN to compute heat-map for each body part.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 101,
      "context" : "Different from [103], Tompson et al.",
      "startOffset" : 15,
      "endOffset" : 20
    }, {
      "referenceID" : 102,
      "context" : "[104] learn the body part prior model and implicitly the structure of the spatial model.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 102,
      "context" : "As an extension of [104], Tompson et al.",
      "startOffset" : 19,
      "endOffset" : 24
    }, {
      "referenceID" : 41,
      "context" : "[42] propose a",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 103,
      "context" : "This refinement model, which is a Siamese network [105], is jointly trained in cascade with the off-the-shelf model [104].",
      "startOffset" : 50,
      "endOffset" : 55
    }, {
      "referenceID" : 102,
      "context" : "This refinement model, which is a Siamese network [105], is jointly trained in cascade with the off-the-shelf model [104].",
      "startOffset" : 116,
      "endOffset" : 121
    }, {
      "referenceID" : 102,
      "context" : "In a similar work with [104], Chen et al.",
      "startOffset" : 23,
      "endOffset" : 28
    }, {
      "referenceID" : 104,
      "context" : "[106], [107] also combine graphical model with CNN.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 105,
      "context" : "[106], [107] also combine graphical model with CNN.",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 106,
      "context" : "There is also a pose estimation method called dual-source CNN [108]",
      "startOffset" : 62,
      "endOffset" : 67
    }, {
      "referenceID" : 102,
      "context" : "Based on the work [104], Jain et al.",
      "startOffset" : 18,
      "endOffset" : 23
    }, {
      "referenceID" : 107,
      "context" : "[109] also",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 108,
      "context" : "The works can be coarsely categorized into three types: (1) text detection and localization without recognition, (2) text recognition on cropped text images, and (3) end-to-end text spotting that integrates both text detection and recognition: 1) Text detection: One of the pioneering works to apply CNN for scene text detection is [110].",
      "startOffset" : 332,
      "endOffset" : 337
    }, {
      "referenceID" : 108,
      "context" : "The CNN model employed by [110] learns on cropped text patches and non-text scene patches to discriminate between the two.",
      "startOffset" : 26,
      "endOffset" : 31
    }, {
      "referenceID" : 109,
      "context" : "To reduce the search space for text detection, [111] proposes to obtain a set of character candidates via Maximally Stable Extremal Regions (MSER) and filter the candidates by CNN classification.",
      "startOffset" : 47,
      "endOffset" : 52
    }, {
      "referenceID" : 110,
      "context" : "for text detection is [112].",
      "startOffset" : 22,
      "endOffset" : 27
    }, {
      "referenceID" : 110,
      "context" : "In [112], CNN is used to distinguish text-like MSER components from non-text components, and cluttered text components are split by applying CNN in a sliding window manner followed by Non-Maximal Suppression (NMS).",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 111,
      "context" : "Other than localization of text, there is an interesting work [113] that makes use of CNN to determine whether the input image contains text, without telling where the text is exactly located.",
      "startOffset" : 62,
      "endOffset" : 67
    }, {
      "referenceID" : 111,
      "context" : "In [113], text candidates are obtained using MSER which are then passed into a CNN to generate visual features, and lastly the global features of the images are constructed by aggregating the CNN features in a Bag-ofWords (BoW) framework.",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 112,
      "context" : "2) Text recognition: [114] proposes a CNN model with multiple softmax classifiers in its final layer, which is formulated in such a way that each classifier is responsible for character prediction at each sequential location in the multi-digit input image.",
      "startOffset" : 21,
      "endOffset" : 26
    }, {
      "referenceID" : 113,
      "context" : "As an attempt to recognize text without using lexicon and dictionary, [115] introduces a novel Conditional Random Fields (CRF)-like CNN model to jointly learn character sequence prediction and bigram generation",
      "startOffset" : 70,
      "endOffset" : 75
    }, {
      "referenceID" : 114,
      "context" : "In [116], CNN extracts rich visual features from character-level image patches obtained via sliding window, and the sequence labelling is carried out by an enhanced RNN variant called Long Short-Term Memory (LSTM) [117].",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 115,
      "context" : "In [116], CNN extracts rich visual features from character-level image patches obtained via sliding window, and the sequence labelling is carried out by an enhanced RNN variant called Long Short-Term Memory (LSTM) [117].",
      "startOffset" : 214,
      "endOffset" : 219
    }, {
      "referenceID" : 116,
      "context" : "The method presented in [118] is very similar to [116], except that in [118], lexicon can be taken into consideration to enhance text recognition performance.",
      "startOffset" : 24,
      "endOffset" : 29
    }, {
      "referenceID" : 114,
      "context" : "The method presented in [118] is very similar to [116], except that in [118], lexicon can be taken into consideration to enhance text recognition performance.",
      "startOffset" : 49,
      "endOffset" : 54
    }, {
      "referenceID" : 116,
      "context" : "The method presented in [118] is very similar to [116], except that in [118], lexicon can be taken into consideration to enhance text recognition performance.",
      "startOffset" : 71,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "3) End-to-end text spotting: For end-to-end text spotting, [10] applies a CNN model originally trained for character classification to perform text detection.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "Going in a similar direction as [10], the CNN model proposed in [119] enables feature sharing across the four different subtasks of an end-to-end text spotting system: text detection, character case-sensitive and insensitive classification, and bigram classification.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 117,
      "context" : "Going in a similar direction as [10], the CNN model proposed in [119] enables feature sharing across the four different subtasks of an end-to-end text spotting system: text detection, character case-sensitive and insensitive classification, and bigram classification.",
      "startOffset" : 64,
      "endOffset" : 69
    }, {
      "referenceID" : 118,
      "context" : "[120] makes use of CNNs in a very comprehensive way to perform end-to-end text spotting.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 118,
      "context" : "In [120], the major subtasks of its proposed system, namely text bounding box filtering, text bounding box regression, and text recognition are each tackled by a separate CNN model.",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 119,
      "context" : "Multi-contextual information is a crucial prior in visual saliency prediction, and it has been used concurrently with CNN in most of the considered works [121–125].",
      "startOffset" : 154,
      "endOffset" : 163
    }, {
      "referenceID" : 120,
      "context" : "Multi-contextual information is a crucial prior in visual saliency prediction, and it has been used concurrently with CNN in most of the considered works [121–125].",
      "startOffset" : 154,
      "endOffset" : 163
    }, {
      "referenceID" : 121,
      "context" : "Multi-contextual information is a crucial prior in visual saliency prediction, and it has been used concurrently with CNN in most of the considered works [121–125].",
      "startOffset" : 154,
      "endOffset" : 163
    }, {
      "referenceID" : 122,
      "context" : "Multi-contextual information is a crucial prior in visual saliency prediction, and it has been used concurrently with CNN in most of the considered works [121–125].",
      "startOffset" : 154,
      "endOffset" : 163
    }, {
      "referenceID" : 123,
      "context" : "Multi-contextual information is a crucial prior in visual saliency prediction, and it has been used concurrently with CNN in most of the considered works [121–125].",
      "startOffset" : 154,
      "endOffset" : 163
    }, {
      "referenceID" : 119,
      "context" : "[121] intro-",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 120,
      "context" : "In [122], the",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 121,
      "context" : "The CNN model adopted in [123] is pre-trained on large-scale image classification dataset and then shared among different contextual levels for feature extraction.",
      "startOffset" : 25,
      "endOffset" : 30
    }, {
      "referenceID" : 120,
      "context" : "Similar to [122], [123], the CNN model used in [124] for saliency prediction are shared across three CNN streams, with each stream taking input of a different contextual",
      "startOffset" : 11,
      "endOffset" : 16
    }, {
      "referenceID" : 121,
      "context" : "Similar to [122], [123], the CNN model used in [124] for saliency prediction are shared across three CNN streams, with each stream taking input of a different contextual",
      "startOffset" : 18,
      "endOffset" : 23
    }, {
      "referenceID" : 122,
      "context" : "Similar to [122], [123], the CNN model used in [124] for saliency prediction are shared across three CNN streams, with each stream taking input of a different contextual",
      "startOffset" : 47,
      "endOffset" : 52
    }, {
      "referenceID" : 123,
      "context" : "[125] derives a spatial kernel and a range kernel to",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 124,
      "context" : "There are also CNN-based saliency prediction approaches [126–128] that do not consider multi-contextual information.",
      "startOffset" : 56,
      "endOffset" : 65
    }, {
      "referenceID" : 125,
      "context" : "There are also CNN-based saliency prediction approaches [126–128] that do not consider multi-contextual information.",
      "startOffset" : 56,
      "endOffset" : 65
    }, {
      "referenceID" : 126,
      "context" : "There are also CNN-based saliency prediction approaches [126–128] that do not consider multi-contextual information.",
      "startOffset" : 56,
      "endOffset" : 65
    }, {
      "referenceID" : 124,
      "context" : "In [126], an ensemble of CNNs is derived from a large number of randomly instantiated CNN models, to generate good features for saliency detection.",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 124,
      "context" : "The CNN models instantiated in [126] are however not deep enough because the maximum number of layers is capped at three.",
      "startOffset" : 31,
      "endOffset" : 36
    }, {
      "referenceID" : 125,
      "context" : "By using a pre-trained and deeper CNN model with 5 convolutional layers, [127] (Deep Gaze) learns a separate saliency model to jointly combine the responses from every CNN layer and predict saliency values.",
      "startOffset" : 73,
      "endOffset" : 78
    }, {
      "referenceID" : 126,
      "context" : "[128] is the only work making use of CNN to perform visual saliency prediction in an end-to-end manner, which means the CNN model accepts raw pixels as input and produces saliency map as output.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 126,
      "context" : "[128] argues that the success of the proposed end-to-end method is attributed to its not-so-deep CNN architecture which attempts to prevent overfitting.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 127,
      "context" : "1) Action Recognition in Still Images: The work of [129] has shown the output of last few layers of a trained CNN can be used as a general visual feature descriptor for a variety of tasks.",
      "startOffset" : 51,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : "The same intuition is utilized for action recognition by [8], [130], in which they use the outputs of the penultimate layer of a pre-trained CNN to represent full images of actions as well as the human bounding boxes inside them, and achieve a high level of performance in action classification.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 128,
      "context" : "The same intuition is utilized for action recognition by [8], [130], in which they use the outputs of the penultimate layer of a pre-trained CNN to represent full images of actions as well as the human bounding boxes inside them, and achieve a high level of performance in action classification.",
      "startOffset" : 62,
      "endOffset" : 67
    }, {
      "referenceID" : 129,
      "context" : "[131] add a part detection to this framework.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 130,
      "context" : "Their part detector is a CNN based extension to the original Poselet [132] method.",
      "startOffset" : 69,
      "endOffset" : 74
    }, {
      "referenceID" : 131,
      "context" : "CNN based representation of contextual information is utilized for action recognition in [133].",
      "startOffset" : 89,
      "endOffset" : 94
    }, {
      "referenceID" : 132,
      "context" : "[134] propose to consider the temporal axis in a similar manner as other spatial axes and introduce a network of 3D convolutional layers to be applied on video inputs.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 133,
      "context" : "[135] study the performance, efficiency, and effectiveness of this approach and show its strengths compared to other approaches.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 134,
      "context" : "Another approach to apply CNNs on videos is to keep the convolutions in 2D and fuse the feature maps of consecutive frames, as proposed by [136].",
      "startOffset" : 139,
      "endOffset" : 144
    }, {
      "referenceID" : 135,
      "context" : "One more step forward for better action recognition via CNNs is to separate the representation to spatial and temporal variations and train individual CNNs for each of them, as proposed by Simonyan and Zisserman [137].",
      "startOffset" : 212,
      "endOffset" : 217
    }, {
      "referenceID" : 136,
      "context" : "[138] utilize the two stream CNN on the localized parts of the human body and show the aggregation of part-based local CNN descriptors can effectively improve the performance of action recognition.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 137,
      "context" : "[139] studies different configurations of applying LSTM units as the sequence learner in this framework.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 0,
      "context" : "The recent advance of Convolutional Neural Networks (CNNs) [1], [4] has revolutionized the computer vision community due to their outstanding performance in a wide variety of tasks.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "The recent advance of Convolutional Neural Networks (CNNs) [1], [4] has revolutionized the computer vision community due to their outstanding performance in a wide variety of tasks.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 138,
      "context" : "[140] have pioneered to apply CNNs to scene labeling tasks.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 139,
      "context" : "network is also successfully applied to RGB-D scene labeling [141].",
      "startOffset" : 61,
      "endOffset" : 66
    }, {
      "referenceID" : 140,
      "context" : "[142] develop the recurrent CNNs.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 143,
      "context" : "[143– 145] train the parametric CNNs by sampling image patches, which speeds up the training time dramatically.",
      "startOffset" : 0,
      "endOffset" : 10
    }, {
      "referenceID" : 141,
      "context" : "They find that patch-based CNNs suffer from local ambiguity problems, and [143] solve it by integrating global beliefs.",
      "startOffset" : 74,
      "endOffset" : 79
    }, {
      "referenceID" : 142,
      "context" : "[144] and [145] use the recurrent neural networks to model the contextual dependencies among image features, and dramatically boost the labeling performance.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 143,
      "context" : "[144] and [145] use the recurrent neural networks to model the contextual dependencies among image features, and dramatically boost the labeling performance.",
      "startOffset" : 10,
      "endOffset" : 15
    }, {
      "referenceID" : 144,
      "context" : "[146] apply the local and proximal features from a ConvNet and apply the Alex-net [4] to obtain the distant and global features, and their concatenation gives rise to the zoom-out features.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 3,
      "context" : "[146] apply the local and proximal features from a ConvNet and apply the Alex-net [4] to obtain the distant and global features, and their concatenation gives rise to the zoom-out features.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 145,
      "context" : "[147] train a fully convolutional Network (FCN) to directly map the input images to dense label maps.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 146,
      "context" : "[148] also apply the pre-trained deep CNNs to emit the labels of pixels.",
      "startOffset" : 0,
      "endOffset" : 5
    } ],
    "year" : 2015,
    "abstractText" : "In the last few years, deep learning has lead to very good performance on a variety of problems, such as object recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Due to the lack of training data and computing power in early days, it is hard to train a large high-capacity convolutional neural network without overfitting. Recently, with the rapid growth of data size and the increasing power of graphics processor unit, many researchers have improved the convolutional neural networks and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. Besides, we also introduce some applications of convolutional neural networks in computer vision.",
    "creator" : "TeX"
  }
}