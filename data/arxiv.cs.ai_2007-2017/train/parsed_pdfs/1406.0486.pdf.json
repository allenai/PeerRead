{
  "name" : "1406.0486.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Monte Carlo Tree Search with Heuristic Evaluations using Implicit Minimax Backups",
    "authors" : [ "Marc Lanctot", "Mark H.M. Winands", "Tom Pepels", "Nathan R. Sturtevant" ],
    "emails" : [ "marc.lanctot@maastrichtuniversity.nl,", "m.winands@maastrichtuniversity.nl,", "tom.pepels@maastrichtuniversity.nl,", "sturtevant@cs.du.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Monte Carlo Tree Search (MCTS) [9, 19] is a simulation-based best-first search technique that has been shown to increase performance in domains such as turn-taking games, general-game playing, real-time strategy games, single-agent planning, and more [4]. While the initial applications have been to games where heuristic evaluations are difficult to obtain, progress in MCTS research has shown that heuristics can be effectively be combined in MCTS, even in games where classic minimax search has traditionally been preferred.\nThe most popular MCTS algorithm is UCT [19], which performs a single simulation from the root of the search tree to a terminal state at each iteration. During the iterative process, a game tree is incrementally built by adding a new leaf node to the tree on each iteration, whose nodes maintain statistical estimates such as average payoffs. With each new simulation, these estimates improve and help to guide future simulations.\nIn this work, we propose a new technique to augment the quality of MCTS simulations with an implicitly-computed minimax search which uses heuristic evaluations. Unlike previous work, these heuristic evaluations are used as separate source of information, and backed up in the same way as in classic minimax search. Furthermore, these minimax-style backups are done implicitly, as a simple extra step during the standard updates to the tree nodes, and always maintained separately from win rate estimates obtained from playouts. These two separate information sources are then used to guide MCTS simulations. We show that combining heuristic evaluations in this way can\nar X\niv :1\n40 6.\n04 86\nv4 [\ncs .A\nI] 1\n9 Ju\nlead to significantly stronger play performance in three separate domains: Kalah, Breakthrough, and Lines of Action."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Several techniques for minimax-influenced backup rules in the simulation-based MCTS framework have been previously proposed. The first was Coulom’s original maximum backpropagation [9]. This method of backpropagation suggests, after a number of simulations to a node has been reached, to switch to propagating the maximum value instead of the simulated (average) value. The rationale behind this choice is that after a certain point, the search algorithm should consider a node converged and return an estimate of the best value. Maximum backpropagation has also recently been used in other Monte Carlo search algorithms and demonstrated success in probabilistic planning, as an alternative type of forecaster in BRUE [11] and as Bellman backups for online dynamic programming in Trial-based Heuristic Tree Search [17].\nThe first use of enhancing MCTS using prior knowledge was in Computer Go [13]. In this work, offline-learned knowledge initialized values of expanded nodes increased performance against a significantly strong benchmark player. This technique was also confirmed to be advantageous in Breakthrough [20]. Another way to introduce prior knowledge is via a progressive bias during selection [8], which has significantly increased performance in Go play strength [7].\nIn games where minimax search performs well, such as Kalah, modifying MCTS to use minimaxstyle backups and heuristic values instead to replace playouts offers a worthwhile trade-off under different search time settings [26]. Similarly, there is further evidence suggesting not replacing the playout entirely, but terminating them early using heuristic evaluations, has increased the performance in Lines of Action (LOA) [35], Amazons [18, 21], and Breakthrough [20]. In LOA and Amazons, the MCTS players enhanced with evaluation functions outperform their minimax counterparts using the same evaluation function.\nOne may want to combine minimax backups or searches without using an evaluation function. The prime example is MCTS-Solver [33], which backpropagates proven wins and losses as extra information in MCTS. When a node is proven to be a win or a loss, it no longer needs to be searched. This domain-independent modification greatly enhances MCTS with negligible overhead. Scorebounded MCTS extends this idea to games with multiple outcomes, leading to αβ-style pruning in the tree [5]. One can use shallow-depth minimax searches in the tree to initialize nodes during expansion, enhance the playout, or to help MCTS-Solver in backpropagation [2].\nFinally, recent work has attempted to explain and identify some of the shortcomings that arise from estimates in MCTS, specifically compared to situations where classic minimax search has historically performed well [25, 24]. Attempts have been made to overcome the problem of traps or optimistic moves, i.e., moves that initially seem promising but then later prove to be bad, such as sufficiency thresholds [14] and shallow minimax searches [2]."
    }, {
      "heading" : "2 Adversarial Search in Turn-Taking Games",
      "text" : "A finite deterministic Markov Decision Process (MDP) is 4-tuple (S,A, T ,R). Here, S is a finite non-empty set of states. A is a finite non-empty set of actions, where we denote A(s) ⊆ A the set of available actions at state s. T : S × A 7→ ∆S is a transition function mapping each state and action to a distribution over successor states. Finally, R : S × A × S 7→ R is a reward function mapping (state, action, successor state) triplets to numerical rewards.\nA two-player perfect information game is an MDP with a specific form. Denote Z = {s ∈ S : A(s) = ∅} ⊂ S the set of terminal states. In addition, for all nonterminal states s′ ∈ S − Z , R(s, a, s′) = 0. There is a player identity function τ : S −Z 7→ {1, 2}. The rewardsR(s, a, s′) are always with respect to the same player and we assume zero-sum games so that rewards with respect to the opponent player are simply negated. In this paper, we assume fully deterministic domains, so T (s, a) maps s to a single successor state. However, the ideas proposed can be easily extended to domains with stochastic transitions. When it is clear from the context and unless otherwise stated, we denote s′ = T (s, a).\nMonte Carlo Tree Search (MCTS) is a simulation-based best-first search algorithm that incrementally builds a tree, G, in memory. Each search starts with from a root state s0 ∈ S − Z , and initially sets G = ∅. Each simulation samples a trajectory ρ = (s0, a0, s1, a1, · · · , sn), where sn ∈ Z unless the playout is terminated early. The portion of the ρ where si ∈ G is called the tree portion and the remaining portion is called the playout portion. In the tree portion, actions are chosen according to some selection policy. The first state encountered in the playout portion is expanded, added to G. The actions chosen in the playout portion are determined by a specific playout policy. States s ∈ G are referred to as nodes and statistics are maintained for each node s: the cumulative reward, rs, and visit count, ns. By popular convention, we define rs,a = rs′ where s′ = T (s, a), and similarly ns,a = ns′ . Also, we use rτs to denote the reward at state s with respect to player τ(s).\nLet Q̂(s, a) be an estimator for the value of state-action pair (s, a), where s ∈ A(s). One popular estimator is the observed meanQ(s, a) = rτs,a/ns,a. The most widely-used selection policy is based on a bandit algorithm called Upper Confidence Bounds (UCB) [1], used in adaptive multistage sampling [6] and in UCT [19], which selects action a′ using\na′ = argmax a∈A(s)\n{ Q̂(s, a) + C √ lnns ns,a } , (1)\nwhere C is parameter determining the weight of exploration."
    }, {
      "heading" : "3 Implicit Minimax Backups in MCTS",
      "text" : "Our proposed technique is based on the following principle: if an evaluation function is available, then it should be possible to augment MCTS to make use of it for a potential gain in performance. Suppose we are given an evaluation function v0(s) whose range is the same as that of the reward function R. How should MCTS make use of this information? We propose a simple and elegant solution: add another value to maintain at each node, the implicit minimax evaluation with respect to player τ(s), vτs , with v τ s,a defined similarly as above. This new value at node s only maintains a heuristic minimax value built from the evaluations of subtrees below s. During backpropagation, rs and ns are updated in the usual way, and additionally vτs is updated using minimax backup rule based on children values. Then, similarly to RAVE [13], rather than using Q̂ = Q for selection in Equation 1, we use\nQ̂IM (s, a) = (1− α) rτs,a ns,a + αvτs,a, (2)\nwhere α weights the influence of the heuristic minimax value. The entire process is summarized in Algorithm 1. There are a few simple additions to standard MCTS, located on lines 2, 8, 13, and 14. During selection, Q̂IM from Equation 2 replaces Q in Equation 1. During backpropagation, the implicit minimax evaluations vτs are updated based on\n1 SELECT(s): 2 Let A′ be the set of actions a ∈ A(s) maximizing Q̂IM (s, a) + C √\nlnns ns,a\n3 return a′ ∼UNIFORM(A′) 4 5 UPDATE(s, r): 6 rs ← rs + r 7 ns ← ns + 1 8 vτs ← maxa∈A(s) vτs,a 9\n10 SIMULATE(sparent, aparent, s): 11 if ∃a ∈ A(s), s′ = T (s, a) 6∈ G then 12 EXPAND(s) 13 for a ∈ A(s), s′ = T (s, a) do vs′ ← v0(s′) 14 vτs ← maxa∈A(s) vτs,a 15 r ←PLAYOUT(s) 16 UPDATE(s, r) 17 return r 18 else 19 if s ∈ Z then returnR(sparent, aparent, s) 20 a←SELECT(s) 21 s′ ← T (s, a) 22 r ←SIMULATE(s, a, s′) 23 UPDATE(s, r) 24 return r 25 26 MCTS(s0): 27 while time left do SIMULATE(−,−, s0) 28 return argmaxa∈A(s0) ns0,a\nAlgorithm 1: MCTS with implicit minimax backups.\nthe children’s values. For simplicity, a single max operator is used here since the evaluations are assumed to be in view of player τ(s). Depending on the implementation, the signs of rewards may depend on τ(s) and/or τ(s′). For example, a negamax implementation would include sign inversions at the appropriate places to ensure that the payoffs are in view of the current player at each node. Finally, EXPAND adds all children nodes to the tree, sets their implicit minimax values to their initial heuristic values on line 13, and does a one-ply backup on line 14. A more memory-efficient implementation could add just a single child without fundamentally changing the algorithm, as was done in our experiments in Lines of Action.\nIn essence, this defines a new information scheme where each node is augmented with heuristic estimates which are backed-up differently than the Monte Carlo statistics. When MCTS-Solver is enabled, proven values take precedence in the selection policy and the resulting scheme is informative and consistent [27], so Algorithm 1 converges to the optimal choice eventually. However, before a node becomes a proven win or loss, the implicit minimax values act like an heuristic approximation of MCTS-Solver for the portion of the search tree that has not reached terminal states."
    }, {
      "heading" : "4 Empirical Evaluation",
      "text" : "In this section, we thoroughly evaluate the practical performance of the implicit minimax backups technique. Before reporting head-to-head results, we first describe our experimental setup and summarize the techniques that have been used to improve playouts. We then present results on three game domains: Kalah, Breakthrough, and Lines of Action.\nUnless otherwise stated, our implementations expand a new node every simulation, the first node encountered that is not in the tree. MCTS-Solver is enabled in all of the experiments since its overhead is negligible and never decreases performance. After the simulations, the move with the highest visit count is chosen on line 28. Rewards are in {−1, 0, 1} representing a loss, draw, and win. Evaluation function values are scaled to [−1, 1] by passing a domain-dependent score differences through a cache-optimized sigmoid function. When simulating, a single game state is modified and moves are undone when returning from the recursive call. Whenever possible, evaluation functions are updated incrementally. All of the experiments include swapped seats to ensure that each player type plays an equal number of games as first player and as second player. All reported win rates are over 1000 played games and search time is set to 1 second unless specifically stated otherwise. Domain-dependent playout policies and optimizations are reported in each subsection.\nWe compare to and combine our technique with a number of other ones to include domain knowledge. A popular recent technique is early playout terminations. When a leaf node of the tree is reached, a fixed-depth early playout termination, hereby abbreviated to “fetx”, plays x moves according to the playout policy resulting in state s, and then terminates the playout returning v0(s). This method has shown to improve performance against standard MCTS in Amazons, Kalah, and Breakthrough [20, 26, 21].\nA similar technique is dynamic early terminations, which periodically checks the evaluation function (or other domain-dependent features) terminating only when some condition is met. This approach has been used as a “mercy rule” in Go [3] and quite successfully in Lines of Action [34]. In our version, which we abbreviate “detx”, a playout is terminated and returns 1 if v0(s) ≥ x and −1 if v0(s) ≤ −x. Another option is to use an -greedy playout policy that chooses a successor randomly with probability and successor state with the largest evaluation with probability 1 − , with improved performance in Chinese Checkers [29, 22], abbreviated “ege ”.\nTo facilitate the discussion, we refer to each enhancement and setting using different labels. These enhancements and labels are described in the text that follows. But, we also include, for reference, a summary of each in Table 1.\nExperiments are performed in three domains: Kalah, Breakthrough, and Lines of Action. Example images of each game are shown in Appendix A. To tune parameters in Kalah and Breakthrough, hierarchical elimination tournaments are run where each head-to-head match consisted of at least 200 games with seats swapped halfway. Detailed results of these tournaments and comparisons are contained in Appendix B."
    }, {
      "heading" : "4.1 Kalah",
      "text" : "Kalah is a turn-taking game in the Mancala family of games. Each player has six houses, each initially containing four stones, and a store on the endpoint of the board, initially empty. On their turn, a player chooses one of their houses, removes all the stones in it, and “sows” the stones one per house in counter-clockwise fashion, skipping the opponent’s store. If the final stone lands in the player’s store, that player gets another turn, and there is no limit to the number of consecutive turns\ntaken by same player. If the stone ends on a house owned by the player that contains no stones, then that player captures all the stones in the adjacent opponent house, putting it into the player’s store. The game plays until one player’s houses are all empty; the opponent then moves their remaining stones to their store. The winner is the player who has collected the most stones in their store. Kalah has been weakly solved for several different variants of Kalah [16], and was used as a domain to compare MCTS variants to classic minimax search [26].\nIn running experiments from the initial position, we observed a noticeable first-player bias. Therefore, as was done in [26], our experiments produce random starting board positions without any stones placed in the stores. Competing players play one game and then swap seats to play a second game using the same board. A player is declared a winner if that player won one of the games and at least tied the other game. If the same side wins both games, the game is discarded.\nThe default playout policy chooses a move uniformly at random. We determined which playout enhancement led to the best player. Tournament results revealed that a fet4 early termination worked best. The evaluation function was the same one used in [26], the difference between stones in each player’s stores. Results with one second of search time are shown in Figure 1. Here, we notice that within the range α ∈ [0.1, 0.5] there is a clear advantage in performance when using implicit minimax backups against the base player."
    }, {
      "heading" : "4.2 Breakthrough",
      "text" : "Breakthrough is a turn-taking alternating move game played on an 8-by-8 chess board. Each player has 16 identical pieces on their first two rows. A piece is allowed to move forward to an empty square, either straight or diagonal, but may only capture diagonally like Chess pawns. A player wins by moving a single piece to the furthest opponent row.\nBreakthrough was first introduced in general game-playing competitions and has been identified as a domain that is particularly difficult for MCTS due to traps and uninformed playouts [14]. Our playout policy always chooses one-ply “decisive” wins and prevents immediate “anti-decisive” losses [30]. Otherwise, a move is selected non-uniformly at random, where capturing undefended pieces are four times more likely than other moves. MCTS with this improved playout policy (abbreviated “ipp”) beats the one using uniform random 94.3% of the time. This playout policy leads\nto a clear improvement over random playouts, and so it is enabled by default from this point on. In Breakthrough, two different evaluation functions were used. The first one is a simple one found in Maarten Schadd’s thesis [28] that assigns each piece a score of 10 and the further row achieved as 2.5, which we abbreviate “efMS”. The second one is the more sophisticated one giving specific point values for each individual square per player described in a recent paper by Lorentz & Horey [20], which we abbreviate “efLH”. We base much of our analysis in Breakthrough on the Lorentz & Horey player, which at the time of publication had an ELO rating of 1910 on the Little Golem web site.\nOur first set of experiments uses the simple evaluation function, efMS. At the end of this subsection, we include experiments for the sophisticated evaluation function efLH.\nWe first determined the best playout strategy amongst fixed and dynamic early terminations and -greedy playouts. Our best fixed early terminations player was fet20 and best -greedy player was ege0.1. Through systematic testing on 1000 games per pairing, we determined that the best playout policy when using efMS is the combination (ege0.1,det0.5). The detailed test results are found in Appendix B. To ensure that this combination of early termination strategies is indeed superior to just the improved playout policy on its own, we also played MCTS(ege0.1,det0.5) against MCTS(ipp). MCTS(ege0.1,det0.5) won 68.8% of these games. MCTS(ege0.1,det0.5) is the best baseline player that we could produce given three separate parameter-tuning tournaments, for all the playout enhancements we have tried using efMS, over thousands of played games. Hence, we use it as our primary benchmark for comparison in the rest of our experiments with efMS. For convenience, we abbreviate this baseline player (MCTS(ege0.1,det0.5)) to MCTS(bl).\nWe then played MCTS with implicit minimax backups, MCTS(bl,imα), against MCTS(bl) for a variety different values for α. The results are shown in the top of Figure 2. Implicit minimax backups give an advantage for α ∈ [0.1, 0.6] under both one- and five-second search times. When α > 0.6, MCTS(bl,imα) acts like greedy best-first minimax. To verify that the benefit was not only due to the optimized playout policy, we performed two experiments. First, we played MCTS without playout terminations, MCTS(ipp,im0.4) against MCTS(ipp). MCTS(ipp,im0.4) won 82.3% of these games. We then tried giving both players fixed early terminations, and played MCTS(ipp,fet20,im0.4) ver-\nsus MCTS(ipp,fet20). MCTS(ipp,fet20,im0.4) won 87.2% of these games. The next question was whether the mixing static evaluation values themselves (v0(s)) at node s was the source of the benefit or whether the minimax backup values (vτs ) were the contributing factor. Therefore, we tried MCTS(bl, im0.4) against a baseline player that uses constant bias over the static evaluations, i.e., uses\nQ̂CB(s, a) = (1− α)Q+ αv0(s′), where s′ = T (s, a),\nand also against a player using a progressive bias of the implicit minimax values, i.e.,\nQ̂PB(s, a) = (1− α)Q+ αvτs,a/(ns,a + 1),\nwith α = 0.4 in both cases. MCTS(bl,im0.4) won 67.8% against MCTS(bl,Q̂CB). MCTS(bl,im0.4)\nwon 65.5% against MCTS(bl,Q̂PB). A different decay function for the weight placed on vτs could further improve the advantage of implicit minimax backups. We leave this as a topic for future work.\nWe then evaluated MCTS(im0.4) against maximum backpropagation proposed as an alternative backpropagation in the original MCTS work [9]. This enhancement modifies line 24 of the algorithm to the following:\nif ns ≥ T then return max a∈A(s) Q̂(s, a) else return r.\nThe results for several values of T are given in Table 3. Another question is whether to prefer implicit minimax backups over node priors (abbreviated np) [13], which initializes each new leaf node with wins and losses based on prior knowledge. Node priors were first used in Go, and have also used in path planning problems [10]. We use the scheme that worked well in [20] which takes into account the safety of surrounding pieces, and scales the counts by the time setting (10 for one second, 50 for five seconds). We ran an experiment against the baseline player with node priors enabled, MCTS(bl,imα,np) versus MCTS(bl,np). The results are shown at the bottom of Figure 2. When combined at one second of search time, implicit minimax backups still seem to give an advantage for α ∈ [0.5, 0.6], and at five seconds gives an advantage for α ∈ [0.1, 0.6]. To verify that the combination is complementary, we played MCTS(bl,im0.6) with and without node priors each against the baseline player. The player with node priors won 77.9% and the one without won 63.3%.\nA summary of these comparisons is given in Table 2."
    }, {
      "heading" : "MCTS Using Lorentz & Horey Evaluation Function",
      "text" : "We now run experiments using the more sophisticated evaluation function from [20], efLH, that assigns specific piece count values depending on their position on the board. Rather than repeating all of the above experiments, we chose simply to compare baselines and to repeat the initial experiment, all using 1 second of search time.\nThe best playout with this evaluation function is fet20 with node priors, which we call the alternative baseline, abbreviated bl’. That is, we abbreviate MCTS(ipp,fet20,np) to MCTS(bl’). We rerun the initial α experiment using the alternative baseline, which uses the Lorentz & Horey evaluation function, to find the best implicit minimax player using this more sophisticated evaluation function. Results are shown in Figure 3. In this case the best range is α ∈ [0.5, 0.6] for one second and α ∈ [0.5, 0.6] for five seconds. We label the best player in this figure using the alternative baseline MCTS(efLH,bl’,im0.6).\nIn an effort to explain the relative strengths of each evaluation function, we then compared the two baseline players. Our baseline MCTS player, MCTS(efMS,bl), wins 40.2% of games against the alternative baseline, MCTS(efLH,bl’). When we add node priors, MCTS(efMS,bl,np) wins 78.0% of games against MCTS(efLH,bl’). When we also add implicit minimax backups (α = 0.4), the win rate of MCTS(efMS,bl,im0.4,np) versus MCTS(efLH,bl’) rises again to 84.9%. Implicit minimax backups improves performance against a stronger benchmark player, even when using a simpler evaluation function.\nWe then played 2000 games of the two best players for the respective evaluation functions against each other, that is we played MCTS(efMS,bl,np,im0.4) against MCTS(efLH,bl’,im0.6). MCTS(efMS,bl,np,im0.4) wins 53.40% of games. Given these results, it could be that a more defensive and less granular evaluation function is preferred in Breakthrough when given only 1 second of search time. The results in our comparison to αβ in the next subsection seem to suggest this as well."
    }, {
      "heading" : "Comparison to αβ Search",
      "text" : "A natural question is how MCTS with implicit minimax backups compares to αβ search. So, here we compare MCTS with implicit minimax backups versus αβ search. Our αβ search player uses iterative deepening and a static move ordering. The static move ordering is based on the same information used in the improved playout policies: decisive and anti-decisive moves are first, then captures of defenseless pieces, then all other captures, and finally regular moves. The results are listed in Table 4.\nThe first observation is that the performance of MCTS (vs. αβ) increases as search time increases. This is true in all cases, using either evaluation function, with and without implicit minimax backups. This is similar to observations in Lines of Action [32] and multiplayer MCTS [29, 23].\nThe second observation is that MCTS(imα) performs significantly better against αβ than the baseline player at the same search time. Using efMS in Breakthrough with 5 seconds of search time, MCTS(im0.4) performs significantly better than both the baseline MCTS player and αβ search on their own.\nThe third observation is that MCTS(imα) benefits significantly from weak heuristic information, more so than αβ. When using efMS, MCTS takes less long to do better against αβ, possibly because\nMCTS makes better use of weaker information. When using efLH, αβ preforms significantly better against MCTS at low time settings. However, it unclear whether this due to αβ improving or MCTS worsening. Therefore, we also include a comparison of the αβ players using efMS versus efLH. What we see is that at 1 second, efMS benefits αβ more, but as time increases efLH seems to be preferred. Nonetheless, when using efLH, there still seems to be a point where, if given enough search time the performance of MCTS(im0.6) surpasses that of αβ."
    }, {
      "heading" : "4.3 Lines of Action",
      "text" : "In subsection 4.2, we compared the performance of MCTS(imα) to a basic αβ search player. Our main question at this point is how MCTS(imα) could perform in a game with stronger play due to using proven enhancements in both αβ and MCTS. For this analysis, we now consider the wellstudied game Lines of Action (LOA).\nLOA is a turn-taking alternating-move game played on an 8-by-8 board that uses checkers board and pieces. The goal is to connect all your pieces into a single connected group (of any size), where the pieces are connected via adjacent and diagonals squares. A piece may move in any direction, but the number of squares it may move depends on the total number of pieces in the line, including opponent pieces. A piece may jump over its own pieces but not opponent pieces. Captures occur by landing on opponent pieces.\nThe MCTS player is MC-LOA, whose implementation and enhancements are described in [35]. MC-LOA is a world-champion engine winning the latest Olympiad. The benchmark αβ player is MIA, the world-best αβ-player upon which MC-LOA is based, winning 4 Olympiads. MCLOA uses MCTS-Solver, progressive bias, and highly-optimized αβ playouts. MIA includes the following enhancements: static move ordering, iterative deepening, killer moves, history heuristic, enhanced transposition table cutoffs, null-move pruning, multi-cut, realization probability search, quiescence search, and negascout/PVS. The evaluation function used is the used in MIA [36]. All of the results in LOA are based 100 opening board positions.1\nWe repeat the implicit minimax backups experiment with varying α. At first, we use standard UCT without enhancements and a simple playout that is selects moves non-uniformly at random based on the move categories, and uses the early cut-off strategy. Then, we enable shallow αβ searches in the playouts described in [32]. Finally, we enable the progressive bias based on move categories in addition to the αβ playouts. The results for these three different settings are shown in Figure 4. As before, we notice that in the first two situations, implicit minimax backups with α ∈ [0.1, 0.5] can lead to better performance. When the progressive bias based on move categories is added, the advantage diminishes. However, we do notice that α ∈ [0.05, 0.3] seems to not significantly decrease the performance.\nAdditional results are summarized in Table 5. From the graph, we reranα = 0.2 with progressive bias for 32000 games giving a statistically significant (95% confidence) win rate of 50.59%. We also tried increasing the search time, in both cases (with and without progressive bias), and observed a gain in performance at five and ten seconds. In the past, the strongest LOA player was MIA, which was based on αβ search. Therefore, we also test our MCTS with implicit minimax backups against an αβ player based on MIA. When progressive bias is disabled, implicit minimax backups increases the performance by 11 percentage points. There is also a small increase in performance when progressive bias is enabled. Also, at α = 0.2, it seems that there is no statistically significant case of implicit minimax backups hurting performance."
    }, {
      "heading" : "4.4 Discussion: Traps and Limitations",
      "text" : "The initial motivation for this work was driven by the trap moves, which pose problems in MCTS [26, 2, 14]. However, in LOA we observed that implicit minimax backups did not speed up MCTS when solving a test set of end game positions. We tried to construct an example board in Breakthrough to demonstrate how implicit minimax backups deals with problems with traps. We were unable to do so. In our experience, traps are effectively handled by the improved playout policy. Even without early terminations, simply having decisive and anti-decisive moves and preferring good capture moves seems to be enough to handle traps in Breakthrough. Also, even with random playouts, an efficient implementation with MCTS-Solver handles shallow traps. Therefore, we believe that the explanation for the advantage offered by implicit minimax backups is more subtle than simply\n1https://dke.maastrichtuniversity.nl/m.winands/loa/\ndetecting and handling traps. In watching several Breakthrough games, it seems that MCTS with implicit minimax backups builds “fortress” structures [15] that are then handled better than standard MCTS.\nWhile we have shown positive results in a number of domains, we recognize that this technique is not universally applicable. We believe that implicit minimax backups work because there is shortterm tactical information, which is not captured in the long-term playouts, but is captured by the implicit minimax procedure. Additionally, we suspect that there must be strategic information in the playouts which is not captured in the shallower minimax backups. Thus, success depends on both the domain and the evaluation function used. We also ran experiments for implicit minimax backups in Chinese Checkers and the card game Hearts, and there was no significant improvement in performance, but more work has to be performed to understand if we would find success with a better evaluation function."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have introduced a new technique called implicit minimax backups for MCTS. This technique stores the information from both sources separately, only combining the two sources to guide selection. Implicit minimax can lead to stronger play even with simple evaluation functions, which are often readily available. In Breakthrough, our evaluation shows that implicit minimax backups increases the strength of MCTS significantly compared to similar techniques for improving MCTS using domain knowledge. Furthermore, the technique improves performance in LOA, a more complex domain with sophisticated knowledge and strong MCTS and αβ players. The range α ∈ [0.15, 0.4] seems to be a safe choice. In Breakthrough, this range is higher, [0.5, 0.6], when using node priors at lower time settings and when using the alternative baseline.\nFor future work, we would like to apply the technique in other games, such as Amazons, and plan to investigate improving initial evaluations v0(s) using quiescence search. We hope to compare or combine implicit minimax backups to/with other minimax hybrids from [2]. Differences between vτs,a and Q(s, a) could indicate parts of the tree that require more search and hence help guide selection. Parameters could be modified online. For example, α could be changed based on the outcomes of each choice made during the game, and Q(s, a) could be used for online search bootstrapping of evaluation function weights [31]. Finally, the technique could also work in general game-playing using learned evaluation functions [12].\nAcknowledgments. This work is partially funded by the Netherlands Organisation for Scientific Research (NWO) in the framework of the project Go4Nature, grant number 612.000.938."
    }, {
      "heading" : "A Game Images",
      "text" : "This appendix shows images of each game to help visualize the objectives."
    }, {
      "heading" : "B Tournaments and Playout Comparisons",
      "text" : "This appendix includes details of the results of played games to determine the best baseline players."
    }, {
      "heading" : "B.1 Parameter Values for Breakthrough and Kalah",
      "text" : "Table 6 shows the parameter values that were used in parameter-tuning experiments."
    }, {
      "heading" : "B.2 Kalah Playout Optimization",
      "text" : "In Kalah, each matchup included 1000 games, but as mentioned in the main part of the paper, only the wins and losses are shown and unbalanced boards are removed.\nB.2.1 Fixed Early Termination Tournament\nround 1 winner mcts_h_fet0 (368) vs. loser mcts_h_fet1000 (61) winner mcts_h_fet1 (408) vs. loser mcts_h_fet100 (61) winner mcts_h_fet2 (458) vs. loser mcts_h_fet50 (61) winner mcts_h_fet3 (460) vs. loser mcts_h_fet30 (37) winner mcts_h_fet4 (429) vs. loser mcts_h_fet20 (44) winner mcts_h_fet5 (223) vs. loser mcts_h_fet10 (83) mcts_h_fet8 gets a by\nround 2 winner mcts_h_fet0 (181) vs. loser mcts_h_fet8 (169) winner mcts_h_fet5 (189) vs. loser mcts_h_fet1 (116) winner mcts_h_fet4 (166) vs. loser mcts_h_fet2 (115) mcts_h_fet3 gets a by\nround 3 winner mcts_h_fet3 (161) vs. loser mcts_h_fet0 (124) winner mcts_h_fet4 (132) vs. loser mcts_h_fet5 (122)\nround 4 winner mcts_h_fet4 (139) vs. loser mcts_h_fet3 (110)\nWinner: mcts_h_fet4\nB.2.2 Epsilon-greedy Playout Tournament\nround 1 winner mcts_h_ege1.0 (232) vs. loser mcts_h_ege0.0 (205) winner mcts_h_ege0.9 (224) vs. loser mcts_h_ege0.05 (210) winner mcts_h_ege0.1 (207) vs. loser mcts_h_ege0.8 (195) winner mcts_h_ege0.15 (219) vs. loser mcts_h_ege0.7 (213) winner mcts_h_ege0.2 (244) vs. loser mcts_h_ege0.6 (211) winner mcts_h_ege0.3 (233) vs. loser mcts_h_ege0.5 (197) mcts_h_ege0.4 gets a by\nround 2 winner mcts_h_ege1.0 (276) vs. loser mcts_h_ege0.4 (173) winner mcts_h_ege0.9 (231) vs. loser mcts_h_ege0.3 (195) winner mcts_h_ege0.1 (212) vs. loser mcts_h_ege0.2 (196) mcts_h_ege0.15 gets a by\nround 3 winner mcts_h_ege1.0 (218) vs. loser mcts_h_ege0.15 (206) winner mcts_h_ege0.9 (206) vs. loser mcts_h_ege0.1 (194)\nround 4 winner mcts_h_ege1.0 (229) vs. loser mcts_h_ege0.9 (177)\nWinner: mcts_h_ege1.0"
    }, {
      "heading" : "B.2.3 Kalah Tournament Winner Comparisons",
      "text" : "Table 7 shows the results of the top Kalah playout winners. Each data point in the following table includes 2000 games. As mentioned in the main part of the paper, draws due to unbalanced boards are not counted.\nB.3 Breakthrough Playout Enhancements (using efMS evaluator) B.3.1 Fixed Early Terminations Tournament\nround 1 winner mcts_h_fet1000 (115) vs. loser mcts_h_fet0 (85) winner mcts_h_fet100 (117) vs. loser mcts_h_fet1 (83) winner mcts_h_fet50 (108) vs. loser mcts_h_fet2 (92) winner mcts_h_fet30 (138) vs. loser mcts_h_fet3 (62) winner mcts_h_fet20 (129) vs. loser mcts_h_fet4 (71) winner mcts_h_fet10 (129) vs. loser mcts_h_fet5 (71) mcts_h_fet8 gets a by\nround 2 winner mcts_h_fet8 (108) vs. loser mcts_h_fet1000 (92) winner mcts_h_fet10 (112) vs. loser mcts_h_fet100 (88) winner mcts_h_fet20 (128) vs. loser mcts_h_fet50 (72) mcts_h_fet30 gets a by\nround 3 winner mcts_h_fet30 (113) vs. loser mcts_h_fet8 (87) winner mcts_h_fet20 (104) vs. loser mcts_h_fet10 (96)\nround 4 winner mcts_h_fet20 (104) vs. loser mcts_h_fet30 (96)\nWinner: mcts_h_fet20\nB.3.2 Epsilon-greedy Playout Tournament\nround 1 winner mcts_h_ege0.0 (156) vs. loser mcts_h_ege1.0 (44) winner mcts_h_ege0.05 (155) vs. loser mcts_h_ege0.9 (45) winner mcts_h_ege0.1 (156) vs. loser mcts_h_ege0.8 (44) winner mcts_h_ege0.15 (153) vs. loser mcts_h_ege0.7 (47) winner mcts_h_ege0.2 (151) vs. loser mcts_h_ege0.6 (49) winner mcts_h_ege0.3 (119) vs. loser mcts_h_ege0.5 (81) mcts_h_ege0.4 gets a by\nround 2 winner mcts_h_ege0.0 (115) vs. loser mcts_h_ege0.4 (85) winner mcts_h_ege0.05 (119) vs. loser mcts_h_ege0.3 (81) winner mcts_h_ege0.1 (125) vs. loser mcts_h_ege0.2 (75) mcts_h_ege0.15 gets a by\nround 3 winner mcts_h_ege0.15 (103) vs. loser mcts_h_ege0.0 (97) winner mcts_h_ege0.1 (110) vs. loser mcts_h_ege0.05 (90)\nround 4 winner mcts_h_ege0.1 (108) vs. loser mcts_h_ege0.15 (92)\nWinner: mcts_h_ege0.1\nB.3.3 Dynamic Early Terminations Tournament\nround 1 winner mcts_h_det1.0 (121) vs. loser mcts_h_det0.1 (79) winner mcts_h_det0.95 (115) vs. loser mcts_h_det0.15 (85) winner mcts_h_det0.9 (120) vs. loser mcts_h_det0.2 (80) winner mcts_h_det0.25 (101) vs. loser mcts_h_det0.85 (99) winner mcts_h_det0.3 (119) vs. loser mcts_h_det0.8 (81) winner mcts_h_det0.35 (117) vs. loser mcts_h_det0.75 (83) winner mcts_h_det0.4 (107) vs. loser mcts_h_det0.7 (93) winner mcts_h_det0.45 (132) vs. loser mcts_h_det0.65 (68) winner mcts_h_det0.5 (106) vs. loser mcts_h_det0.6 (94) mcts_h_det0.55 gets a by\nround 2 winner mcts_h_det0.55 (129) vs. loser mcts_h_det1.0 (71) winner mcts_h_det0.5 (124) vs. loser mcts_h_det0.95 (76) winner mcts_h_det0.45 (115) vs. loser mcts_h_det0.9 (85) winner mcts_h_det0.25 (134) vs. loser mcts_h_det0.4 (66) winner mcts_h_det0.3 (108) vs. loser mcts_h_det0.35 (92)\nround 3 winner mcts_h_det0.3 (133) vs. loser mcts_h_det0.55 (67) winner mcts_h_det0.25 (120) vs. loser mcts_h_det0.5 (80) mcts_h_det0.45 gets a by\nround 4 winner mcts_h_det0.3 (135) vs. loser mcts_h_det0.45 (65) mcts_h_det0.25 gets a by\nround 5 winner mcts_h_det0.3 (106) vs. loser mcts_h_det0.25 (94)\nWinner: mcts_h_det0.3"
    }, {
      "heading" : "B.3.4 Breakthrough Tournament Winner Comparisons (using efMS)",
      "text" : "Table 8 shows the results of the top Breakthrough playout winners using efMS.\nB.4 Breakthrough Playout Enhancements (using efLH evaluator) B.4.1 Fixed Early Terminations Tournament\nround 1 winner mcts_h_efv1_fet0 (118) vs. loser mcts_h_efv1_fet1000 (82) winner mcts_h_efv1_fet1 (129) vs. loser mcts_h_efv1_fet100 (71) winner mcts_h_efv1_fet2 (113) vs. loser mcts_h_efv1_fet50 (87) winner mcts_h_efv1_fet3 (101) vs. loser mcts_h_efv1_fet30 (99) winner mcts_h_efv1_fet20 (121) vs. loser mcts_h_efv1_fet4 (79) winner mcts_h_efv1_fet5 (100) vs. loser mcts_h_efv1_fet16 (100) winner mcts_h_efv1_fet8 (102) vs. loser mcts_h_efv1_fet12 (98) mcts_h_efv1_fet10 gets a by\nround 2 winner mcts_h_efv1_fet10 (108) vs. loser mcts_h_efv1_fet0 (92) winner mcts_h_efv1_fet8 (112) vs. loser mcts_h_efv1_fet1 (88) winner mcts_h_efv1_fet5 (115) vs. loser mcts_h_efv1_fet2 (85) winner mcts_h_efv1_fet20 (123) vs. loser mcts_h_efv1_fet3 (77)\nround 3 winner mcts_h_efv1_fet20 (110) vs. loser mcts_h_efv1_fet10 (90) winner mcts_h_efv1_fet8 (101) vs. loser mcts_h_efv1_fet5 (99)\nround 4 winner mcts_h_efv1_fet8 (106) vs. loser mcts_h_efv1_fet20 (94)\nWinner: mcts_h_efv1_fet8\nB.4.2 Epsilon-greedy Playout Tournament\nround 1 winner mcts_h_efv1_ege1.0 (136) vs. loser mcts_h_efv1_ege0.0 (64) winner mcts_h_efv1_ege0.9 (121) vs. loser mcts_h_efv1_ege0.05 (79) winner mcts_h_efv1_ege0.1 (110) vs. loser mcts_h_efv1_ege0.8 (90)\nwinner mcts_h_efv1_ege0.7 (103) vs. loser mcts_h_efv1_ege0.15 (97) winner mcts_h_efv1_ege0.6 (104) vs. loser mcts_h_efv1_ege0.2 (96) winner mcts_h_efv1_ege0.3 (100) vs. loser mcts_h_efv1_ege0.5 (100) mcts_h_efv1_ege0.4 gets a by\nround 2 winner mcts_h_efv1_ege1.0 (122) vs. loser mcts_h_efv1_ege0.4 (78) winner mcts_h_efv1_ege0.3 (101) vs. loser mcts_h_efv1_ege0.9 (99) winner mcts_h_efv1_ege0.6 (116) vs. loser mcts_h_efv1_ege0.1 (84) mcts_h_efv1_ege0.7 gets a by\nround 3 winner mcts_h_efv1_ege0.7 (102) vs. loser mcts_h_efv1_ege1.0 (98) winner mcts_h_efv1_ege0.3 (105) vs. loser mcts_h_efv1_ege0.6 (95)\nround 4 winner mcts_h_efv1_ege0.3 (110) vs. loser mcts_h_efv1_ege0.7 (90)\nWinner: mcts_h_efv1_ege0.3"
    }, {
      "heading" : "B.5 Breakthrough Tournament Winner Comparisons (using efLH)",
      "text" : "Table 9 shows the results of the top Breakthrough playout winners using efLH."
    } ],
    "references" : [ {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "Monte-Carlo tree search and minimax hybrids",
      "author" : [ "H. Baier", "M.H.M. Winands" ],
      "venue" : "IEEE Conference on Computational Intelligence and Games (CIG), pages 129–136",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Old-fashioned computer Go vs Monte-Carlo Go",
      "author" : [ "B. Bouzy" ],
      "venue" : "IEEE Symposium on Computational Intelligence in Games (CIG)",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A survey of Monte Carlo tree search methods",
      "author" : [ "C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton" ],
      "venue" : "IEEE Trans. on Comput. Intel. and AI in Games, 4(1):1–43",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Score bounded Monte-Carlo tree search",
      "author" : [ "T. Cazenave", "A. Saffidine" ],
      "venue" : "International Conference on Computers and Games ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An adaptive sampling algorithm for solving Markov Decision Processes",
      "author" : [ "H.S. Chang", "M.C. Fu", "J. Hu", "S.I. Marcus" ],
      "venue" : "Operations Research, 53(1):126–139",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Adding expert knowledge and exploration in Monte-Carlo tree search",
      "author" : [ "G. Chaslot", "C. Fiter", "J-B. Hoock", "A. Rimmel", "O. Teytaud" ],
      "venue" : "Advances in Computer Games, volume 6048 of LNCS, pages 1–13",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "H",
      "author" : [ "G.M. J-B. Chaslot", "M.H.M. Winands", "J.W.H.M. Uiterwijk" ],
      "venue" : "J. van den Herik, and B. Bouzy. Progressive strategies for Monte-Carlo tree search. New Mathematics and Natural Computation, 4(3):343–357",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Efficient selectivity and backup operators in Monte-Carlo tree search",
      "author" : [ "R. Coulom" ],
      "venue" : "5th International Conference on Computers and Games, volume 4630 of LNCS, pages 72–83",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "High-quality policies for the Canadian travelers problem",
      "author" : [ "P. Eyerich", "T. Keller", "M. Helmert" ],
      "venue" : "Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI), pages 51– 58",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Monte-Carlo planning: Theoretically fast convergence meets practical efficiency",
      "author" : [ "Z. Feldman", "C. Domshlak" ],
      "venue" : "International Conference on Uncertainty in Artificial Intelligence (UAI), pages 212–221",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning simulation control in general game playing agents",
      "author" : [ "H. Finnsson", "Y. Björnsson" ],
      "venue" : "Twenty-Fourth AAAI Conference on Artificial Intelligence, pages 954–959",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Combining online and offline knowledge in UCT",
      "author" : [ "S. Gelly", "D. Silver" ],
      "venue" : "Proceedings of the 24th Annual International Conference on Machine Learning ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Sufficiency-based selection strategy for MCTS",
      "author" : [ "S.F. Gudmundsson", "Y. Björnsson" ],
      "venue" : "Proceedings of the 23rd International Joint Conference on Artificial Intelligence, pages 559– 565",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Detecting fortresses in chess",
      "author" : [ "M. Guid", "I. Bratko" ],
      "venue" : "Elektrotehniški Vestnik, 79(1–2):35–40",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Solving Kalah",
      "author" : [ "G. Irving", "H.H.L.M. Donkers", "J.W.H.M. Uiterwijk" ],
      "venue" : "ICGA Journal, 23(3):139–148",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Trial-based heuristic tree search for finite horizon MDPs",
      "author" : [ "T. Keller", "M. Helmert" ],
      "venue" : "International Conference on Automated Planning and Scheduling (ICAPS)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Monte-Carlo Techniques: Applications to the Game of Amazons",
      "author" : [ "J. Kloetzer" ],
      "venue" : "PhD thesis, School of Information Science, JAIST, Ishikawa, Japan",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Bandit-based Monte Carlo planning",
      "author" : [ "L. Kocsis", "C. Szepesvári" ],
      "venue" : "15th European Conference on Machine Learning, volume 4212 of LNCS, pages 282–293",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Programming Breakthrough",
      "author" : [ "R. Lorentz", "T. Horey" ],
      "venue" : "8th International Conference on Computers and Games (CG)",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Amazons discover Monte-Carlo",
      "author" : [ "Richard Lorentz" ],
      "venue" : "Proceedings of the 6th International Conference on Computers and Games (CG),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Playout Search for Monte-Carlo Tree Search in Multi-Player Games",
      "author" : [ "J.A.M. Nijssen", "M.H.M. Winands" ],
      "venue" : "ACG 2011, volume 7168 of LNCS, pages 72–83",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Search policies in multi-player games",
      "author" : [ "J.A.M. Nijssen", "M.H.M. Winands" ],
      "venue" : "ICGA Journal, 36(1):3–21",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On adversarial search spaces and samplingbased planning",
      "author" : [ "R. Ramanujan", "A. Sabharwal", "B. Selman" ],
      "venue" : "20th International Conference on Automated Planning and Scheduling (ICAPS), pages 242–245",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Understanding sampling style adversarial search methods",
      "author" : [ "R. Ramanujan", "A. Sabharwal", "B. Selman" ],
      "venue" : "26th Conference on Uncertainty in Artificial Intelligence (UAI), pages 474–483",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Trade-offs in sampling-based adversarial planning",
      "author" : [ "R. Ramanujan", "B. Selman" ],
      "venue" : "21st International Conference on Automated Planning and Scheduling (ICAPS), pages 202–209",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Solving Games and All That",
      "author" : [ "Abdallah Saffidine" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Selective Search in Games of Different Complexity",
      "author" : [ "M.P.D. Schadd" ],
      "venue" : "PhD thesis, Maastricht University, Maastricht, The Netherlands",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An analysis of UCT in multi-player games",
      "author" : [ "N.R. Sturtevant" ],
      "venue" : "ICGA Journal, 31(4):195–208",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "On the huge benefit of decisive moves in Monte-Carlo tree search algorithms",
      "author" : [ "F. Teytaud", "O. Teytaud" ],
      "venue" : "IEEE Conference on Computational Intelligence in Games (CIG), pages 359– 364",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Bootstrapping from game tree search",
      "author" : [ "J. Veness", "D. Silver", "A. Blair", "W.W. Cohen" ],
      "venue" : "Advances in Neural Information Processing Systems 22, pages 1937–1945",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "αβ-based play-outs in Monte-Carlo tree search",
      "author" : [ "M.H.M. Winands", "Y. Björnsson" ],
      "venue" : "IEEE Conference on Computational Intelligence and Games (CIG), pages 110–117",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Monte-Carlo tree search solver",
      "author" : [ "M.H.M. Winands", "Y. Björnsson", "J-T. Saito" ],
      "venue" : "Computers and Games ",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Monte-Carlo tree search solver",
      "author" : [ "M.H.M. Winands", "Y. Björnsson", "J-T. Saito" ],
      "venue" : "6th International Conference on Computers and Games ",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Monte Carlo tree search in Lines of Action",
      "author" : [ "M.H.M. Winands", "Y. Björnsson", "J-T. Saito" ],
      "venue" : "IEEE Transactions on Computational Intelligence and AI in Games, 2(4):239–250",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Monte Carlo Tree Search (MCTS) [9, 19] is a simulation-based best-first search technique that has been shown to increase performance in domains such as turn-taking games, general-game playing, real-time strategy games, single-agent planning, and more [4].",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "Monte Carlo Tree Search (MCTS) [9, 19] is a simulation-based best-first search technique that has been shown to increase performance in domains such as turn-taking games, general-game playing, real-time strategy games, single-agent planning, and more [4].",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "Monte Carlo Tree Search (MCTS) [9, 19] is a simulation-based best-first search technique that has been shown to increase performance in domains such as turn-taking games, general-game playing, real-time strategy games, single-agent planning, and more [4].",
      "startOffset" : 251,
      "endOffset" : 254
    }, {
      "referenceID" : 18,
      "context" : "The most popular MCTS algorithm is UCT [19], which performs a single simulation from the root of the search tree to a terminal state at each iteration.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "The first was Coulom’s original maximum backpropagation [9].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "Maximum backpropagation has also recently been used in other Monte Carlo search algorithms and demonstrated success in probabilistic planning, as an alternative type of forecaster in BRUE [11] and as Bellman backups for online dynamic programming in Trial-based Heuristic Tree Search [17].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 16,
      "context" : "Maximum backpropagation has also recently been used in other Monte Carlo search algorithms and demonstrated success in probabilistic planning, as an alternative type of forecaster in BRUE [11] and as Bellman backups for online dynamic programming in Trial-based Heuristic Tree Search [17].",
      "startOffset" : 284,
      "endOffset" : 288
    }, {
      "referenceID" : 12,
      "context" : "The first use of enhancing MCTS using prior knowledge was in Computer Go [13].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "This technique was also confirmed to be advantageous in Breakthrough [20].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "Another way to introduce prior knowledge is via a progressive bias during selection [8], which has significantly increased performance in Go play strength [7].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "Another way to introduce prior knowledge is via a progressive bias during selection [8], which has significantly increased performance in Go play strength [7].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 25,
      "context" : "In games where minimax search performs well, such as Kalah, modifying MCTS to use minimaxstyle backups and heuristic values instead to replace playouts offers a worthwhile trade-off under different search time settings [26].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 34,
      "context" : "Similarly, there is further evidence suggesting not replacing the playout entirely, but terminating them early using heuristic evaluations, has increased the performance in Lines of Action (LOA) [35], Amazons [18, 21], and Breakthrough [20].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 17,
      "context" : "Similarly, there is further evidence suggesting not replacing the playout entirely, but terminating them early using heuristic evaluations, has increased the performance in Lines of Action (LOA) [35], Amazons [18, 21], and Breakthrough [20].",
      "startOffset" : 209,
      "endOffset" : 217
    }, {
      "referenceID" : 20,
      "context" : "Similarly, there is further evidence suggesting not replacing the playout entirely, but terminating them early using heuristic evaluations, has increased the performance in Lines of Action (LOA) [35], Amazons [18, 21], and Breakthrough [20].",
      "startOffset" : 209,
      "endOffset" : 217
    }, {
      "referenceID" : 19,
      "context" : "Similarly, there is further evidence suggesting not replacing the playout entirely, but terminating them early using heuristic evaluations, has increased the performance in Lines of Action (LOA) [35], Amazons [18, 21], and Breakthrough [20].",
      "startOffset" : 236,
      "endOffset" : 240
    }, {
      "referenceID" : 32,
      "context" : "The prime example is MCTS-Solver [33], which backpropagates proven wins and losses as extra information in MCTS.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "Scorebounded MCTS extends this idea to games with multiple outcomes, leading to αβ-style pruning in the tree [5].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "One can use shallow-depth minimax searches in the tree to initialize nodes during expansion, enhance the playout, or to help MCTS-Solver in backpropagation [2].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 24,
      "context" : "Finally, recent work has attempted to explain and identify some of the shortcomings that arise from estimates in MCTS, specifically compared to situations where classic minimax search has historically performed well [25, 24].",
      "startOffset" : 216,
      "endOffset" : 224
    }, {
      "referenceID" : 23,
      "context" : "Finally, recent work has attempted to explain and identify some of the shortcomings that arise from estimates in MCTS, specifically compared to situations where classic minimax search has historically performed well [25, 24].",
      "startOffset" : 216,
      "endOffset" : 224
    }, {
      "referenceID" : 13,
      "context" : ", moves that initially seem promising but then later prove to be bad, such as sufficiency thresholds [14] and shallow minimax searches [2].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : ", moves that initially seem promising but then later prove to be bad, such as sufficiency thresholds [14] and shallow minimax searches [2].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "The most widely-used selection policy is based on a bandit algorithm called Upper Confidence Bounds (UCB) [1], used in adaptive multistage sampling [6] and in UCT [19], which selects action a′ using",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 5,
      "context" : "The most widely-used selection policy is based on a bandit algorithm called Upper Confidence Bounds (UCB) [1], used in adaptive multistage sampling [6] and in UCT [19], which selects action a′ using",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 18,
      "context" : "The most widely-used selection policy is based on a bandit algorithm called Upper Confidence Bounds (UCB) [1], used in adaptive multistage sampling [6] and in UCT [19], which selects action a′ using",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "Then, similarly to RAVE [13], rather than using Q̂ = Q for selection in Equation 1, we use Q̂ (s, a) = (1− α) r s,a ns,a + αv s,a, (2)",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 26,
      "context" : "When MCTS-Solver is enabled, proven values take precedence in the selection policy and the resulting scheme is informative and consistent [27], so Algorithm 1 converges to the optimal choice eventually.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 19,
      "context" : "This method has shown to improve performance against standard MCTS in Amazons, Kalah, and Breakthrough [20, 26, 21].",
      "startOffset" : 103,
      "endOffset" : 115
    }, {
      "referenceID" : 25,
      "context" : "This method has shown to improve performance against standard MCTS in Amazons, Kalah, and Breakthrough [20, 26, 21].",
      "startOffset" : 103,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "This method has shown to improve performance against standard MCTS in Amazons, Kalah, and Breakthrough [20, 26, 21].",
      "startOffset" : 103,
      "endOffset" : 115
    }, {
      "referenceID" : 2,
      "context" : "This approach has been used as a “mercy rule” in Go [3] and quite successfully in Lines of Action [34].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 33,
      "context" : "This approach has been used as a “mercy rule” in Go [3] and quite successfully in Lines of Action [34].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 28,
      "context" : "Another option is to use an -greedy playout policy that chooses a successor randomly with probability and successor state with the largest evaluation with probability 1 − , with improved performance in Chinese Checkers [29, 22], abbreviated “ege ”.",
      "startOffset" : 219,
      "endOffset" : 227
    }, {
      "referenceID" : 21,
      "context" : "Another option is to use an -greedy playout policy that chooses a successor randomly with probability and successor state with the largest evaluation with probability 1 − , with improved performance in Chinese Checkers [29, 22], abbreviated “ege ”.",
      "startOffset" : 219,
      "endOffset" : 227
    }, {
      "referenceID" : 15,
      "context" : "Kalah has been weakly solved for several different variants of Kalah [16], and was used as a domain to compare MCTS variants to classic minimax search [26].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 25,
      "context" : "Kalah has been weakly solved for several different variants of Kalah [16], and was used as a domain to compare MCTS variants to classic minimax search [26].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 25,
      "context" : "Therefore, as was done in [26], our experiments produce random starting board positions without any stones placed in the stores.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 25,
      "context" : "The evaluation function was the same one used in [26], the difference between stones in each player’s stores.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "Breakthrough was first introduced in general game-playing competitions and has been identified as a domain that is particularly difficult for MCTS due to traps and uninformed playouts [14].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 29,
      "context" : "Our playout policy always chooses one-ply “decisive” wins and prevents immediate “anti-decisive” losses [30].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 27,
      "context" : "The first one is a simple one found in Maarten Schadd’s thesis [28] that assigns each piece a score of 10 and the further row achieved as 2.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 19,
      "context" : "The second one is the more sophisticated one giving specific point values for each individual square per player described in a recent paper by Lorentz & Horey [20], which we abbreviate “efLH”.",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 8,
      "context" : "4) against maximum backpropagation proposed as an alternative backpropagation in the original MCTS work [9].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "Another question is whether to prefer implicit minimax backups over node priors (abbreviated np) [13], which initializes each new leaf node with wins and losses based on prior knowledge.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : "Node priors were first used in Go, and have also used in path planning problems [10].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 19,
      "context" : "We use the scheme that worked well in [20] which takes into account the safety of surrounding pieces, and scales the counts by the time setting (10 for one second, 50 for five seconds).",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 19,
      "context" : "MCTS Using Lorentz & Horey Evaluation Function We now run experiments using the more sophisticated evaluation function from [20], efLH, that assigns specific piece count values depending on their position on the board.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 31,
      "context" : "This is similar to observations in Lines of Action [32] and multiplayer MCTS [29, 23].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 28,
      "context" : "This is similar to observations in Lines of Action [32] and multiplayer MCTS [29, 23].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "This is similar to observations in Lines of Action [32] and multiplayer MCTS [29, 23].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 34,
      "context" : "The MCTS player is MC-LOA, whose implementation and enhancements are described in [35].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 31,
      "context" : "Then, we enable shallow αβ searches in the playouts described in [32].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : "The initial motivation for this work was driven by the trap moves, which pose problems in MCTS [26, 2, 14].",
      "startOffset" : 95,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "The initial motivation for this work was driven by the trap moves, which pose problems in MCTS [26, 2, 14].",
      "startOffset" : 95,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "The initial motivation for this work was driven by the trap moves, which pose problems in MCTS [26, 2, 14].",
      "startOffset" : 95,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "In watching several Breakthrough games, it seems that MCTS with implicit minimax backups builds “fortress” structures [15] that are then handled better than standard MCTS.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "We hope to compare or combine implicit minimax backups to/with other minimax hybrids from [2].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 30,
      "context" : "For example, α could be changed based on the outcomes of each choice made during the game, and Q(s, a) could be used for online search bootstrapping of evaluation function weights [31].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 11,
      "context" : "Finally, the technique could also work in general game-playing using learned evaluation functions [12].",
      "startOffset" : 98,
      "endOffset" : 102
    } ],
    "year" : 2014,
    "abstractText" : "Monte Carlo Tree Search (MCTS) has improved the performance of game engines in domains such as Go, Hex, and general game playing. MCTS has been shown to outperform classic αβ search in games where good heuristic evaluations are difficult to obtain. In recent years, combining ideas from traditional minimax search in MCTS has been shown to be advantageous in some domains, such as Lines of Action, Amazons, and Breakthrough. In this paper, we propose a new way to use heuristic evaluations to guide the MCTS search by storing the two sources of information, estimated win rates and heuristic evaluations, separately. Rather than using the heuristic evaluations to replace the playouts, our technique backs them up implicitly during the MCTS simulations. These minimax values are then used to guide future simulations. We show that using implicit minimax backups leads to stronger play performance in Kalah, Breakthrough, and Lines of Action.",
    "creator" : "LaTeX with hyperref package"
  }
}