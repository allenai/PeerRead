{
  "name" : "1705.08049.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Neural Network Memory Architectures for Autonomous Robot Navigation",
    "authors" : [ "Steven W. Chen", "Nikolay Atanasov", "Arbaaz Khan", "Konstantinos Karydis", "Daniel D. Lee", "Vijay Kumar" ],
    "emails" : [ "kumar}@seas.upenn.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nAutonomous robot navigation in real-world settings involves planning and control with limited information in dynamic and partially-known environments. Traditional approaches close perception-action feedback loops by maintaining a global map representation of the environment and employing feedback motion planning algorithms (e.g., optimization-based [1], [2], search-based [3], [4], or sampling-based [5], [6], [7], [8], [9]). While a global map allows navigation in environments with cul-de-sacs and complex obstacle configurations, maintaining one can be challenging due to localization drift, noisy features, environment changes and limited on-board computation [10], [11].\nAn alternative approach to global mapping and replanning is to determine a closed-loop policy that maps the history of sensor positions and observations to the next action at each time step. The benefit of computing an action directly from the observation history is that there is no need to maintain a map, and requires just one function evaluation as opposed to a computationally-expensive search or optimization. Unfortunately, the state or action spaces in robotics applications can be very large, to the extent that keeping the measurement history and representing such policy functions efficiently (e.g., as a lookup table or a linear function) is\nThis work is supported in part by ARL # W911NF-08-2-0004, DARPA # HR001151626/HR0011516850, ARO # W911NF-13-1-0350, and ONR # N00014-07-1-0829. The authors are with the GRASP Laboratory, University of Pennsylvania. Email: {chenste, atanasov, arbaazk, kkarydis, ddlee, kumar}@seas.upenn.edu\ninfeasible [12]. Due to their representational power, neuralnetwork-based learning techniques have become increasingly of interest to the robotics community as a method to encode such perception-action policies or value functions [13], [14]. Furthermore, traditional methods for navigating in unknown environments can be used to supervise the training process by providing action labels to the neural network. Recent work has shown that neural networks can be used to navigate a wheeled robot in cluttered environments [15].\nDespite their success in several fields, deep learning techniques have several limitations when considering robot navigation. Current network architectures are not well-suited for long-term sequential tasks [16], [17], which typically appear in robotics (e.g., path planning), because they are inherently reactive. More precisely, while maintaining the complete observation history is not feasible, the spatiotemporal correlation present in observation data necessitates the use of memory structures that summarize observed features (e.g., the map plays this role in traditional navigation approaches). However, the effect of memory on neural network performance and generalization ability is not well-understood. Current methods for evaluating network structures are based on empirical performance on a held-out test set. A limitation of this approach is that it is dependent on the test set choice and is prone to over-fitting during the model-selection phase [18]. This limitation is especially important in a robotics context where the unknowns of realworld scenarios may not be easily captured by a test set.\nThis paper investigates the ability of several neural network memory structures to separate action classes and generalize to unseen environments in the context of robot navigation. Motivated by the reliance of traditional approaches on accurate global maps, our work seeks to reveal new directions and limitations of applying deep learning to the robot navigation problem. We argue that understanding the effect of memory on separability and generalization ability is fundamental for successfully applying neural networks to robotics. Our work makes three main contributions. • We investigate the relationship between memory, sep-\narability and generalization ability of neural-network perception-action policies in cul-de-sac environments. • We estimate the Vapnik-Chervonenkis (VC) dimension [19] of the last layer of a neural network (after all upstream layer transformations) as a measure of the network generalization ability that depends only on the training set choice. • We develop a new parallel training algorithm for supervised learning of perception-action policies in sequential prediction problems.\nar X\niv :1\n70 5.\n08 04\n9v 1\n[ cs\n.R O\n] 2\n3 M\nay 2\n01 7"
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "Recent works have considered the application of deep learning techniques to autonomous robot navigation. Tamar et al. developed Value Iteration Networks (VIN) that outperform regular convolutional networks in planning-based sequential reasoning [20]. However, that work only explored planning in known maps. Zhang et al. used Model Predictive Control (MPC) in guided policy search to train a neural network to navigate a quadrotor using lidar inputs [21]. Ross et al. used the Dataset Aggregation (DAgger) supervised learning algorithm to navigate in a dense forest environment using image inputs, and concluded that incorporating memory could improve failure cases [22]. Existing works consider only convex obstacles, and as a result they may have limited success in autonomous robot navigation in environments which contain complex obstacles such as cul-de-sacs [23].\nThese prior works and observations suggest that some form of memory is necessary. The most commonly used method to incorporate memory in robotics learning problems are through Recurrent Neural Networks (RNN) such as the Long-Short Term Memory (LSTM) [24]. Heess et al. demonstrate that recurrent neural networks are able to learn in partially-observable problems [25]. Mnih et al. incorporate the LSTM layers into their Asynchronous Advantage ActorCritic (A3C) algorithm and demonstrate improved performance over a feedforward network [26]. Zhang et al. argue that memory is necessary in partially-observed tasks, and augment the state space to include memory states [27].\nGraves et al. observe that while RNN’s can in principle be used to simulate arbitrary procedures, learning the optimal network is not easy in practice [28]. This observation inspired a new family of neural network architectures called Memory Augmented Neural Networks (MANN) which utilize an explicit memory matrix [29]. Among them, the Differentiable Neural Computer (DNC) has demonstrated improved performance over standard RNNs in tasks such as copying sequences and planning shortest paths [30]. However, MANN structures have been mostly applied to natural language processing and speech recognition; their application to robotics has received less attention."
    }, {
      "heading" : "III. PROBLEM FORMULATION",
      "text" : "Consider a bounded connected set X representing the workspace of a robot. Let X obs and X goal, called the obstacle region and the goal region, respectively, be subsets of X . Denote the obstacle-free portion of the workspace as X free := X\\X obs. The dynamics of the robot are specified by the Probability Density Function (PDF) pf (· | xt, ut) of the robot state xt+1 ∈ X at time t+1 given the previous state xt ∈ X and control input ut ∈ U . We assume that the control input space U is a finite discrete set.1 The robot perceives its environment through observations zt ∈ Z generated from a depth sensor (e.g., lidar, depth camera), whose model is\n1For instance, the control space U for a differential-drive robot in SE(2) can be a set of motion primitives, parameterized by linear velocity, angular velocity and duration. For a quadrotor, U may be a set of short-range dynamically feasible motions.\nspecified by the PDF ph(· | X obs, xt). The information available to the robot at time t to compute the control input ut is it := (x0:t, z0:t, u0:t−1,X goal) ∈ I, consisting of current and previous observations z0:t, current and previous states x0:t and previous control inputs u0:t−1 Problem. Given an initial state x0 ∈ X free and a goal region X goal ⊂ X free, find a function µ : I → U , if one exists, such that applying the control ut := µ(it) results in a sequence of states that satisfies {x0, x1, . . . , xT } ⊂ X free and xT ∈ X goal.\nIn this problem setting, the obstacle region X obs is a partially observable state. Instead of trying to estimate it using a mapping approach, our goal is to learn a policy µ̂ that maps the sequence of sensor observations z0, z1, . . . directly to control inputs for the robot. The partial observability requires an explicit consideration of memory in order to learn µ̂ successfully. A partially observable problem can be represented via a Markov Decision Process (MDP) over the information space. More precisely, we consider a finitehorizon discounted MDP defined by (I,U , T ,R, γ), where γ ∈ (0, 1] is a discount factor, I is the state space, U is the action space, T : I × U × I → [0, 1] is the transition function, and R : I × U × I → R is the reward function. The latter two are defined as follows:\nT (i(1)t , u, i (2) t+1) := ph(z (2) t+1 | X obs, x (2) t+1)pf (x (2) t+1 | x (1) t , u)\nR(i(1)t , u, i (2) t+1) :=  1, if x(2)t+1 ∈ X goal −1, if x(2)t+1 ∈ X obs\n0, otherwise\nIn the rest of the paper we consider a 2-D grid world, an instance of the feasible planning problem in which X ⊂ R2 and U := {down, right, up, left}. To investigate the need for memory, we consider U-shape cul-de-sac maps, illustrated in Fig. 1. The traditional approach to the feasible planning problem in this setting is simultaneous mapping and planning. In contrast, we consider learning a feasible policy by using the outputs of an A∗ path planner [31] for supervision. Let qt := ( xt, zt, ut−1,X goal ) be the information available at time t and decompose the information state as it = q0:t. Our idea is to rely on a neural network to estimate a feasible control policy µ̂ that maps the current information qt to a control input ut, by computing a |U|-dimensional probability distribution over the possible controls and returning the maximum likelihood control. Such a network needs a hidden memory state ht in order to remember past information q0:t−1 and represents the policy µ̂(qt, ht; θ) via parameters θ. Our goal is to optimize the network parameters θ in order to match the output of simultaneous occupancy grid mapping [32, Ch.9] and A∗ planning."
    }, {
      "heading" : "IV. MEMORY ARCHITECTURE",
      "text" : "We describe three neural network architectures that use different structures to represent the memory state ht: feedforward (FF), long short-term memory (LSTM) and differentiable neural computer (DNC)—see Fig. 2.\nFeedforward: Deep FF networks, such as the multilayer perceptron [33, Ch. 6] and ConvNets, have been very successful in various vision and robotic tasks [34][35]. Our first architecture uses only FF layers to model the mapping from inputs qt to actions ut. This structure does not include a memory state ht, and the policy can be rewritten as µ̂(qt; θ).\nWe hypothesize that the lack of a memory state will be a problem in the cul-de-sac environment because there exist two different states i(1)t =q (1) 0:t and i (2) t =q (2) 0:t (when the robot is entering and exiting the cul-de-sac) such that µt(i (1) t ) 6= µt(i (2) t ) but q (1) t = q (2) t . In other words, the expert (i.e. the A∗ planner) maps the same input qt to two different actions, depending on the history q0:t−1, but the FF network will not be able to distinguish this based only on qt.\nLong Short-Term Memory: The second architecture we consider remedies the memory problem by introducing a long-short term memory (LSTM) layer [33, Ch. 10], which contains an internal state ht. The LSTM has a memory cell that removes and adds information to the hidden state ht based on the previous state ht−1 and the current input qt using the input, forget and update gates.\nThe hidden recurrent state can thus be seen as a form of implicit memory since it can read from the inputs (the tape) to modify its internal state, but cannot write to the tape to affect future decisions. We hypothesize that the addition of this memory feature is necessary since, for example, the previous entering and exiting scenario where µt(i (1) t ) 6= µt(i (2) t ) and q (1) t =q (2) t , will not be a problem for the LSTM network because h(1)t 6=h (2) t .\nDifferentiable Neural Computer: Our third architecture uses a more explicit representation of memory ht in the form of a memory matrix, which may provide better memory features to separate the action classes. The use of external memory in neural network architectures was inspired by Turing Machines [28]. Whereas the LSTM can only read\nfrom the tape, the DNC is similar to a Turing Machine in that it can both read and write to the tape (now the inputs and the memory matrix) to modify its internal state. An external memory architecture has been shown to improve performance in natural language processing and other fields, but such an architecture has never been considered in robotics. We expect that it would be very useful for navigation where long sequences of actions may have to be backtracked.\nThe neural network reads from the memory matrix by first using a set of R read heads to get R read vectors r1t−1, . . . , r R t−1. These read vectors are then concatenated with the normal inputs (xt, zt) and fed through the neural network computation graph. The neural network outputs a vector vt, and an interface vector ξt. The interface vector ξt is used to update the memory matrix in a write update and read update step. After the memory matrix is updated, the neural network reads from the memory matrix again to get R read vectors r1t , . . . , r R t . These read vectors are combined with vt via a fully connected layer to get the final action output. The weights that determine the interface vector ξt are the parameters to be learned.2 We also consider a regularized version of the DNC architecture, where we incorporate an L2 weight penalty to the parameters that correspond to memory (external memory and LSTM)."
    }, {
      "heading" : "V. ASYNCHRONOUS DAGGER",
      "text" : "This section describes how we optimize the parameters θ of the networks representing the policy µ̂(qt, ht; θ). In sequential prediction problems, a common concern is the temporal dependence of the outputs ut on the previous inputs q0:t. This correlation poses a problem in stochastic gradient descent methods, as the gradient estimated from the most recent steps may no longer be representative of the true gradient. Further, the difference between the state distributions between the expert and the learner is a common concern in sequential prediction problems. A naive implementation of supervised learning will have poor performance because the states and observations encountered by the expert will be different than those encountered by the policy [36].\nThe DAgger [36] algorithm addresses both of these problems. At each training iteration, the current policy collects a set of new trajectories and aggregates it to a replay data set. The intuition behind this algorithm is that rather than training on a distribution of states that the expert encounters, by sampling with the previous and current policies, the next policy is being trained on a distribution of states that the policy is likely to encounter. In addition, this sampling from an aggregated dataset reduces correlation.\nWe develop an asynchronous variant of the DAgger algorithm that breaks correlation through asynchronous gradient updates estimated from independent parallel learners. Asynchronous DAgger is inspired by the Asynchronous Advantage Actor Critic (A3C) algorithm [26], but differs since the A3C algorithm is an actor-critic reinforcement learning algorithm, while ours is a supervised sequential prediction\n2See [30] for details on the computations and memory matrix updates.\nalgorithm. Similar to how our treatment of correlated training data is the supervised analogue to the A3C reinforcement algorithm, the original DAgger algorithm is analogous to the original Deep Q-Network (DQN) algorithm [37] in that both store an experience replay databank. The pseudo-algorithm 1 describes the developed Asynchronous DAgger algorithm.\nAlgorithm 1 Asynchronous DAgger (for each learner thread) 1: // Assume global shared parameter vector θ 2: Initialize global shared update counter J ← 0 3: Initialize thread update counter j ← 1 4: Initialize thread episode counter t← 1 5: Initialize thread dataset D ← ∅ 6: Initialize thread network gradients dθ ← 0 7: repeat 8: repeat 9: Observe qt 10: Execute action ut sampled from current global 11: action policy µ̂J(qt, ht; θ) 12: Retrieve optimal action µ(it) from expert 13: and convert to standard basis vector eµ 14: Add (µ̂J(qt, ht), eµ) to D 15: j ← j + 1 16: until terminal it or j == jmax 17: for (µ̂J(qt, ht), eµ) ∈ D do 18: Accumulate gradients wrt θ: dθ←dθ + dH(µ̂J ,eµ) dθ 19: where H(·, ·) is the cross-entropy loss 20: if terminal it or t == tmax then 21: Reset episode 22: t← 1 23: Perform asynchronous update of θ using dθ 24: Reset thread update counter j ← 1 25: Reset thread dataset D ← ∅ 26: Reset thread network gradients dθ ← 0 27: J ← J + 1 28: until J > Jmax\nEach parallel learner executes actions in the simulation, estimates a gradient calculated from its most recent actions, and applies that gradient asynchronously to the global neural network. Note that in Asynchronous DAgger, the state distribution is determined by the current policy µ̂J as opposed to the optimal policy µ. However, also notice that rather than accumulating a dataset and sampling randomly from it, each thread has its own dataset D consisting of its previous t examples, and this dataset D is reset after applying the aynchronous gradient. As a result, Asynchronous DAgger encounters the state distribution of its current policy, as opposed to a mixture of its current and previous policies. This algorithm extends the exploitation of parallel asynchronous learners to supervised sequential prediction problems."
    }, {
      "heading" : "VI. GENERALIZATION ABILITY",
      "text" : "We present a new technique for determining the efficacy of various neural network architectures. The technique is based on the maximum margin theory of generalization for Support Vector Machines (SVM). Given a network, we estimate the VC-dimension of a similar architecture that combines the original network with an SVM as the final readout layer. This estimate is calculated using only training data and can be used as an alternative to held-out test sets.\nThe architecture of most deep neural networks can be broken into two parts: a hierarchy of upstream layers followed by a single readout layer [38]. The neural network can thus be viewed as a perceptron in the feature space learned by the upstream layers. A good neural network architecture contains upstream layers that effectively “separate” the various classes with a large margin, which the last linear readout layer can then easily classify. One specific form of perceptron, the SVM, can be used to evaluate the generalization ability of the neural network. Previous works have recognized the benefit of applying maximum margin SVMs to neural networks by using them as the final readout layer in tasks such as image recognition [39], [40], [41], [42]. These works offer small, but consistent improvement over standard softmax layers.\nWe employ the SVM to evaluate the generalization ability of a trained neural network. This is achieved by estimating the VC-dimension of the neural network with the final readout layer replaced by a linear SVM. Consider a binary classification problem and let\nΨ(qi, hi) = (ψ1(qi, hi), . . . , ψD(qi, hi))\nbe a vector in feature space of dimension D, and w = (w1, . . . , wD) be the vector of weights determining a hyperplane in this space. We use i instead of t to emphasize that all actions are aggregated into one data set, effectively ignoring the temporal nature of the sequential prediction. The VCdimension η of the maximal margin hyperplanes separating N vectors Ψ(q1, h1) . . .Ψ(qN , hN ) can be estimated by\nηest = R 2|w0|2 ,\nwhere R is the radius of the smallest sphere that contains Ψ(qi, hi) for all i and |w0| is the norm of the optimal weights [19]. The norm of the weights is related to the optimal margin ∆ = 1|w0| . Thus good generalizability (low VC-dimension) occurs when the margin ∆ is large with respect to R.3\nCalculating the margin ∆ is trivial based on the norm of the weights w and comes standard with most SVM packages. The radius R can be found by using a simple quadratic program solved through standard convex optimization packages.\nGiven the set of N vectors (Ψ(q1, h1) . . .Ψ(qN , hN )) in D-dimensional feature space, define the D×N matrix C := (Ψ(q1, h1), . . .Ψ(qN , hN )), consider the quadratic program\nminimize z pTCTCp− N∑ i=1 Ψ(qi, hi) T Ψ(qi, hi)pi\nsubject to N∑ i=1 pi = 1\npi ≥ 0 ∀i,\nand let p∗ = (p∗1, . . . , p ∗ n) be some optimal solution. The vector Ψ∗ = ∑n\ni=1 Ψ(qi, hi)p ∗ i is the center of the smallest\nenclosing sphere and the squared radius R2 is the negative value of the objective function at p∗ [44]. The upstream layers of a neural network thus learn the function Ψ which\n3We refer the reader to [19] and [43] for in-depth details and proofs.\ntransforms the raw vectors (qi, hi) from the original feature space into vectors in the new feature space Ψ(qi, hi). The linear readout layer learns a hyperplane with weights w in this feature space, however w is not necessarily optimal.\nTo estimate the VC-dimension ηest of the optimal hyperplane w0 we calculate the margin of the optimal hyperplane and radius of smallest bounding sphere on the training data set in this new feature space. Thus, a better neural network learns a Ψ that results in a lower ηest, so that the linear readout layer separates the classes with a large margin.\nThe action policy network takes the form µ̂(qi, hi) := σ(AΨ(qi, hi) + b) where A is a matrix representing the weights of the linear readout layer, b is the bias, and σ is the softmax function to turn it into a probability vector. This technique can be applied to analyze linear readout neural networks in any classification problem such as an image recognition or text classification. However, the analysis in our problem is further complicated since we are performing sequential prediction, and it is unclear what data set D we need to estimate ηest from. We cannot use the current policies µ̂J to generate this data set, because each network will generate a different data set and the ηest of each network will not be comparable. As a result, we instead generate the data set by executing the optimal policy µ.\nThis technique is a new application of maximum margin theory to estimate the generalization ability of neural networks. Compared to traditional empirical test accuracies, the reported method yields a better estimate of generalization ability for two reasons. First, it explicitly defines the generalization ability as opposed to other proxy performance measures. Second, it is not dependent on the choice of the test set. This method can be combined with traditional empirical test measures in any deep learning problem to yield better insight into the performance of a network."
    }, {
      "heading" : "VII. RESULTS AND ANALYSIS",
      "text" : "Our experiments have two purposes: (1) determine the effect of incorporating neural network memory architectures in robot navigation; and (2) evaluate the predictive capacity of our VC dimension estimates on empirical test error. Our environment is a grid-world involving cul-de-sac obstacles, and we test for each network’s ability to interpolate and extrapolate to different length obstacles."
    }, {
      "heading" : "A. Cul-de-sac vs Parallel Walls",
      "text" : "In the grid-world environment, the state xt := (xt, yt)T is the 2D position of the robot. The goal is denoted by xgt := (xgt , y g t ) T . The robot can take four actions (down, right, up,\nleft), and moves a distance of 1 m at each step. The robot cannot turn in place, and starts oriented toward the goal. The robot is equipped with a laser range finder, with a 360◦ field-of-view, that reports relative distance to any perceived obstacles within its field of view. The sensor measurement zt at time t consists of NB = 144 laser beams with maximum range 5 m that report the distance to the closest obstacle along the beam. The obstacle structure is either a cul-desac or parallel walls (see Fig. 1), which the robot cannot determine until it reaches the end of the obstacle.\nNeural networks learn to exploit peculiarities in the simulation design. For example, the A∗ expert enters in the center of the obstacle and exits near the edges, and we found that the network learns to enter the U-shape if the robot is far away from the edges, and exit if it is near. This behavior is analogous to “off-loading” memory onto the physical state of the system observed by [27], but is not desirable in our case because it does not actually test the network’s ability to retain memory. To prevent this memory off-load, we narrow the width of the obstacle to constrain the laser observations to be the same while entering and exiting, thus making the task more challenging by forcing the network to utilize its memory. Table I contains the map generation parameters."
    }, {
      "heading" : "B. Neural Networks",
      "text" : "We evaluate four network architectures: FF, LSTM, DNC LSTM, and regularized DNC LSTM. The inputs at each time step are the 144-dimensional LIDAR reading zt, and the 1-dimensional position heading atan2(yt − ygt , xt − x g t ) representing the heading of the goal from the robot. The FF and LSTM networks have 3 layers of sizes 128 (fullyconnected), 128 (fully-connected), and 128 (fully-connected or LSTM respectively). The DNC LSTM network has the same initial structure as the LSTM, in addition to a memory matrix and a fourth fully-connected layer of size 128. The memory matrix is of size 128x32 and has 2 read heads and 1 write head. The original DNC architecture has two final fully-connected layers (LSTM output and memory matrix output) that are summed together. We have converted this architecture into a single linear readout network by combining the LSTM and memory matrix output in the fourth fully-connected layer. This makes the estimation of the VC dimension easier. The regularized DNC LSTM network has the same architecture as the normal DNC LSTM network, except that we include an L2 regularization penalty on the parameters that correspond to utilizing or updating the DNC and LSTM. The regularization penalty used is λ = 0.1. The last upstream layer for all 4 models has dimension 128."
    }, {
      "heading" : "C. Training Implementation",
      "text" : "The Asynchronous DAgger (Alg. 1) is used to train the neural networks. For the feedforward networks, the training batch size is 5. For the recurrent networks, the backpropagation through time (BPTT) algorithm is truncated at 5 steps. We do not backpropagate the external DNC memory parameters through time. We use the RMSProp [45] algorithm with a learning rate of 10−4 to calculate gradients."
    }, {
      "heading" : "D. Results",
      "text" : "We compute three empirical measures of performance: (1) Success rate; (2) classification accuracy; and (3) ratio of path lengths compared to A∗. The success rate measures how often the neural network reaches the goal region. The classification accuracy measures of how often the neural network outputs the A∗ action. The path length ratio measures the quality of the successful paths versus optimal A∗ paths.\nIn addition, we estimate the VC dimension of our neural networks, using the method described in Section VI. Our navigation problem is a multi-classification problem. However, we follow the strategy in [19] and present our results in a one-vs-all binary classification framework. We estimate ηest by training a linear SVM with a slack penalty C = 1. Table II presents the training and VC-dimension measures.\nSeparability: The LSTM, DNC LSTM and regularized DNC LSTM all achieve near-perfect training accuracy, while the FF does not (∼8% error). This result indicates that memory-less neural networks do not have the capacity to correctly separate the action classes and navigate cul-de-sacs.\nGeneralization ability: Our VC dimension estimates rank the architectures in the following order: (1) regularized DNC LSTM; (2) LSTM; (3) DNC LSTM; and (4) FF. Fig. 3 shows a PCA visualization of the last upstream layer and the neural networks can be viewed as perceptrons separating the classes in these spaces. The prediction performance of ηest will be evaluated on two data sets: interpolation and extrapolation.\nInterpolation: The interpolation data set consists of maps with obstacle lengths that are interpolated in between the obstacle lengths encountered in the training set. None of these maps have been encountered in training. Table III displays the interpolation results and shows that all 3 memory networks are able to successfully generalize. For the FF network, it is interesting to note that the A∗ ratio is greater than 1 for parallel walls, and less than 1 for cul-de-sacs. This pattern indicates that the FF network turns around before it reaches the end of the obstacle length, which is not desirable.\nExtrapolation: The extrapolation data set consists of maps with obstacle lengths that range from 20 to 120 m, which corresponds to 6× the length of the maximum obstacle length encountered during training. Table IV ranks the architectures in the following order: (1) regularized DNC LSTM; (2) LSTM; (3) DNC LSTM; and (3) FF, confirming the predictions made by our VC dimension estimates. The regularized DNC LSTM is able to generalize almost perfectly. The LSTM is able to successfully complete all episodes, but it exhibits the same pattern in the A∗ ratio that indicates it is turning around before it reaches the end of the obstacle. The DNC LSTM exhibits similar behavior, but is only able to successfully complete ∼55% of the episodes. These results indicate that the LSTM and DNC LSTM have overfit the training set, with the LSTM generalizing slightly better.\nThis relative degree of overfitting is reasonable, since the DNC LSTM has strictly more “memory parameters” than the LSTM. Overfitting is expected to occur as our networks are complex, yet the amount of memory to actually navigate cul-de-sac environments is relatively low. Indeed, we only need 3 states that determine when the end of the obstacle has been reached, whether the end is closed or open, and when the robot has exited the entrance of the obstacle. It is thus not surprising that regularization greatly improves the quality of the learned network.\nFigure 4 shows the classification accuracy and A∗ path length ratio of all networks against obstacle length. The regularized DNC LSTM has 100% classification accuracy and\nan A∗ ratio of 1 for all obstacle lengths, indicating perfect generalization. The DNC LSTM has perfect classification accuracy up to obstacle lengths of 30 m (1.5× maximum training length), while the LSTM has perfect classification up to obstacle lengths of 50 m (2.5× maximum training length).\nFor obstacle lengths < 50 m, the A∗ path length ratio graph shows that the DNC LSTM and LSTM are matching the A∗ paths. With longer obstacles, we see 2 regimes of points. The top regime corresponds to the path lengths in the parallel wall scenarios, while the bottom regime corresponds to the cul-de-sac scenario. In addition, notice the downward pattern in these points.\nIf the robot almost reaches the end of the obstacle before turning around, it will have entered the obstacle, exited the obstacle, and gone down the side of the obstacle (1+1+1 = 3). Thus, the A∗ path ratio should be ∼3× and ∼1× for the parallel wall and cul-de-sac environments, respectively. If the robot turns around halfway, it would have entered halfway into the obstacle, exited halfway, and gone down the side of the obstacle ( 12 + 1 2 + 1 = 2). As a result, the A\n∗ path ratio should be ∼2× for the parallel wall and ∼ 23× for the cul-desac. The downward trend in the ratio as the length increases indicate that in successful episodes, the DNC LSTM and LSTM seems to always turn around at 60-80 m. Likewise, we also see 2 regimes of points for the FF A∗ path ratios centered around 1× and 1/3×. This pattern reveals that when the FF network is successful, it entirely bypasses the obstacle. This is also not desirable because the network is just exploiting another peculiarity in the simulation design.\nThe attached video visualizes these behaviors."
    }, {
      "heading" : "E. Discussion",
      "text" : "Our empirical test results match the predictions made by our VC dimension estimation method, indicating that our method is a good indicator of generalization. One benefit of our method is that it has a clear measure of generalization ηest. While a simple classification problem may have a clear measure of performance, in our sequential prediction problem, we presented 3 measures of performance which all captured different behaviors of the network. It is unclear which one is the most desirable and should be optimized.\nMore importantly, only the extrapolation data set was able to differentiate the models in our experiments. All the memory models had the same performance on the interpolation data set, highlighting the second benefit of our method.\nRemark. Assessing generalization ability from empirical test sets is dependent on the choice of training set and the testing set, while our VC dimension estimation method is dependent only on the choice of the training set.\nThe standard evaluation method in deep learning uses a held-out test set that is randomly selected from the training set. This choice of test set is similar to our interpolation test set choice. The test error on a poorly chosen test set may not provide a good metric of generalization because the distribution is too similar to that of the train set. These observations highlight the benefits of complementing current empirical test measures with our VC dimension estimates."
    }, {
      "heading" : "VIII. CONCLUSION",
      "text" : "This paper considered the problem of learning closedloop perception-action policies for autonomous robot navigation. Unlike traditional feedback motion planning approaches that rely on accurate global maps, our approach can infer appropriate actions directly from sensed information by using a neural network policy representation. We argued that including memory in the network structure is fundamental for summarizing past information and achieving good performance as measured by its ability to separate the correct action from other choices and to generalize to unseen environments. Our main contribution is a method for estimating the VC dimension of the last network layer (after all upstream layer transformations) that can be used\nas an accurate generalization ability measure that depends only on the training set choice. Finally, we proposed a new parallel training algorithm for supervised learning of closedloop policies in sequential prediction problems. Our analysis and results demonstrated the need for and superiority of including external memory when increasing the depth of the cul-de-sacs present in the environment.\nFuture work will focus on transfering learned perceptionaction policies to a real robot and evaluating the regret against simulataneous mapping and planning algorithms in a physical environment. We are also interested in extending the approach for robot systems with higher state, control, and measurement dimensions such as velocity-actuated ground robots in SE(2) and force-actuated aerial robots in SE(3) using 3-D depth sensors or image-based information."
    } ],
    "references" : [ {
      "title" : "Exact robot navigation using artificial potential functions",
      "author" : [ "E. Rimon", "D. Koditschek" ],
      "venue" : "IEEE Transactions on Robotics and Automation, vol. 8, no. 5, pp. 501–518, 1992.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "LQR-trees: Feedback Motion Planning via Sums-of-Squares Verification",
      "author" : [ "R. Tedrake", "I. Manchester", "M. Tobenkin", "J. Roberts" ],
      "venue" : "The International Journal of Robotics Research, vol. 29, no. 8, pp. 1038– 1052, 2010.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A formal basis for the heuristic determination of minimum cost paths",
      "author" : [ "P.E. Hart", "N.J. Nilsson", "B. Raphael" ],
      "venue" : "IEEE Transactions on Systems Science and Cybernetics, vol. 4, no. 2, pp. 100–107, 1968.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1968
    }, {
      "title" : "Anytime Dynamic A*: An Anytime, Replanning Algorithm.",
      "author" : [ "M. Likhachev", "D. Ferguson", "G. Gordon", "A. Stentz", "S. Thrun" ],
      "venue" : "ICAPS,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Rapidly-exploring random trees: A new tool for path planning",
      "author" : [ "S.M. Lavalle" ],
      "venue" : "Tech. Rep., 1998.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Sampling-based algorithms for optimal motion planning",
      "author" : [ "S. Karaman", "E. Frazzoli" ],
      "venue" : "The International Journal of Robotics Research, vol. 30, no. 7, pp. 846–894, 2011.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Cross-Entropy Randomized Motion Planning",
      "author" : [ "M. Kobilarov" ],
      "venue" : "Robotics: Science and Systems (RSS), 2011.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Use of relaxation methods in samplingbased algorithms for optimal motion planning",
      "author" : [ "O. Arslan", "P. Tsiotras" ],
      "venue" : "IEEE Int. Conf. on Robotics and Automation (ICRA). IEEE, 2013, pp. 2421–2428.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Probabilistic roadmaps for path planning in high-dimensional configuration spaces",
      "author" : [ "L. Kavraki", "P. Svestka", "J.-C. Latombe", "M. Overmars" ],
      "venue" : "IEEE Transactions on Robotics and Automation, vol. 12, no. 4, pp. 566–580, 1996.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "High speed navigation for quadrotors with limited onboard sensing",
      "author" : [ "S. Liu", "M. Watterson", "S. Tang", "V. Kumar" ],
      "venue" : "IEEE Int. Conf. on Robotics and Automation (ICRA), 2016, pp. 1484–1491.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Robotic mapping: A survey",
      "author" : [ "S. Thrun" ],
      "venue" : "Exploring Artificial Intelligence in the New Millenium, G. Lakemeyer and B. Nebel, Eds. Morgan Kaufmann, 2002.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Temporal difference learning and td-gammon",
      "author" : [ "G. Tesauro" ],
      "venue" : "Commun. ACM, vol. 38, no. 3, pp. 58–68, 1995.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Learning to plan for visibility in navigation of unknown environments",
      "author" : [ "C. Richter", "N. Roy" ],
      "venue" : "Intl. Symposium on Experimental Robotics, 2016.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "End-to-end training of deep visuomotor policies",
      "author" : [ "S. Levine", "C. Finn", "T. Darrell", "P. Abbeel" ],
      "venue" : "Journal of Machine Learning Research, vol. 17, no. 39, pp. 1–40, 2016.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "From perception to decision: A data-driven approach to end-toend motion planning for autonomous ground robots",
      "author" : [ "M. Pfeiffer", "M. Schaeuble", "J. Nieto", "R. Siegwart", "C. Cadena" ],
      "venue" : "arXiv preprint arXiv:1609.07910, 2016.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Cascade models of synaptically stored memories",
      "author" : [ "S. Fusi", "P.J. Drew", "L. Abbott" ],
      "venue" : "Neuron, vol. 45, no. 4, pp. 599–611, 2005.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Memory traces in dynamical systems",
      "author" : [ "S. Ganguli", "D. Huh", "H. Sompolinsky" ],
      "venue" : "Proc. of the National Academy of Sciences, vol. 105, no. 48, pp. 18 970–18 975, 2008.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "On over-fitting in model selection and subsequent selection bias in performance evaluation",
      "author" : [ "G.C. Cawley", "N.L. Talbot" ],
      "venue" : "Journal of Machine Learning Research, vol. 11, no. Jul, pp. 2079–2107, 2010.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The Nature of Statistical Learning Theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1995
    }, {
      "title" : "Value iteration networks",
      "author" : [ "A. Tamar", "Y. WU", "G. Thomas", "S. Levine", "P. Abbeel" ],
      "venue" : "Advances in Neural Information Processing Systems 29, 2016, pp. 2154–2162.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learning Deep Control Policies for Autonomous Aerial Vehicles with MPC-Guided Policy Search",
      "author" : [ "T. Zhang", "G. Kahn", "S. Levine", "P. Abbeel" ],
      "venue" : "IEEE Int. Conf. on Robotics and Automation (ICRA), 2016.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learning monocular reactive uav control in cluttered natural environments",
      "author" : [ "S. Ross", "N. Melik-Barkhudarov", "K.S. Shankar", "A. Wendel", "D. Dey", "A. Bagnell", "M. Hebert" ],
      "venue" : "IEEE Int. Conf. on Robotics and Automation (ICRA). IEEE, 2013, pp. 1765–1772.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Vision and learning for deliberative monocular cluttered flight",
      "author" : [ "D. Dey", "K.S. Shankar", "S. Zeng", "R. Mehta", "M.T. Agcayazi", "C. Eriksen", "S. Daftry", "M. Hebert", "J.A. Bagnell" ],
      "venue" : "Field and Service Robotics. Springer, 2016, pp. 391–409.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural Comput., vol. 9, no. 8, pp. 1735–1780, Nov. 1997.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Memory-based control with recurrent neural networks",
      "author" : [ "N. Heess", "J.J. Hunt", "T.P. Lillicrap", "D. Silver" ],
      "venue" : "arXiv:1512.04455, 2015.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Asynchronous Methods for Deep Reinforcement Learning",
      "author" : [ "V. Mnih", "A. Puigdomènech Badia", "M. Mirza", "A. Graves", "T. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "ArXiv e-print:1602.01783, 2016.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Policy learning with continuous memory states for partially observed robotic control",
      "author" : [ "M. Zhang", "S. Levine", "Z. McCarthy", "C. Finn", "P. Abbeel" ],
      "venue" : "IEEE Int. Conf. on Robotics and Automation (ICRA), 2015.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Neural turing machines",
      "author" : [ "A. Graves", "G. Wayne", "I. Danihelka" ],
      "venue" : "CoRR, vol. abs/1410.5401, 2014.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Scaling memory-augmented neural networks with sparse reads and writes",
      "author" : [ "J. Rae", "J.J. Hunt", "I. Danihelka", "T. Harley", "A.W. Senior", "G. Wayne", "A. Graves", "T. Lillicrap" ],
      "venue" : "Advances In Neural Information Processing Systems, 2016, pp. 3621–3629.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Hybrid computing using a neural network with dynamic external memory",
      "author" : [ "A. Graves", "G. Wayne", "M. Reynolds", "T. Harley", "I. Danihelka", "A. Grabska-Barwińska", "S. Colmenarejo", "E. Grefenstette", "T. Ramalho" ],
      "venue" : "Nature, vol. 538, no. 7626, pp. 471–476, 2016.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "ARA*: Anytime A* with Provable Bounds on Sub-Optimality",
      "author" : [ "M. Likhachev", "G. Gordon", "S. Thrun" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), 2004, pp. 767–774.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, 2012, pp. 1097–1105.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Guided policy search.",
      "author" : [ "S. Levine", "V. Koltun" ],
      "venue" : "in Intl. Conf. on Machine Learning,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2013
    }, {
      "title" : "A reduction of imitation learning and structured prediction to no-regret online learning.",
      "author" : [ "S. Ross", "G.J. Gordon", "D. Bagnell" ],
      "venue" : "in AISTATS,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2011
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Rusu", "J. Veness", "M. Bellemare", "A. Graves", "M. Riedmiller", "A. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "I. Sadik", "A. Antonoglou", "H. King", "D. Kumaran", "D. Wierstar", "S. Legg", "D. Hassabis" ],
      "venue" : "Nature 518, 529-533, 2015.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Linear readout of object manifolds",
      "author" : [ "S. Chung", "D.D. Lee", "H. Sompolinsky" ],
      "venue" : "Phys. Rev. E, vol. 93, p. 060301, 2016.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Decision boundary focused neural network classifier",
      "author" : [ "S. Zhong", "J. Ghosh" ],
      "venue" : "Intelligent Engineering Systems Through Artificial Neural Networks, 2000.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A gentle hessian for efficient gradient descent",
      "author" : [ "R. Collobert", "S. Bengio" ],
      "venue" : "Acoustics, Speech, and Signal Processing, 2004., vol. 5, 2004, pp. V–517.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Convolutional neural support vector machines: hybrid visual pattern classifiers for multi-robot systems",
      "author" : [ "J. Nagi", "G.A. Di Caro", "A. Giusti", "F. Nagi", "L.M. Gambardella" ],
      "venue" : "Machine Learning and Applications (ICMLA), vol. 1, 2012, pp. 27–32.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep learning using linear support vector machines",
      "author" : [ "Y. Tang" ],
      "venue" : "Workshop on Representational Learning, ICML, 2013.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Quadratic programming in geometric optimization: Theory",
      "author" : [ "S. Schonherr" ],
      "venue" : "Implementation and Applications, 2002.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "T. Tieleman", "G. Hinton" ],
      "venue" : "COURSERA: Neural networks for machine learning, vol. 4, no. 2, 2012.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", optimization-based [1], [2], search-based [3], [4], or sampling-based [5], [6], [7], [8], [9]).",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : ", optimization-based [1], [2], search-based [3], [4], or sampling-based [5], [6], [7], [8], [9]).",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : ", optimization-based [1], [2], search-based [3], [4], or sampling-based [5], [6], [7], [8], [9]).",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : ", optimization-based [1], [2], search-based [3], [4], or sampling-based [5], [6], [7], [8], [9]).",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 4,
      "context" : ", optimization-based [1], [2], search-based [3], [4], or sampling-based [5], [6], [7], [8], [9]).",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 5,
      "context" : ", optimization-based [1], [2], search-based [3], [4], or sampling-based [5], [6], [7], [8], [9]).",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : ", optimization-based [1], [2], search-based [3], [4], or sampling-based [5], [6], [7], [8], [9]).",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 7,
      "context" : ", optimization-based [1], [2], search-based [3], [4], or sampling-based [5], [6], [7], [8], [9]).",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : ", optimization-based [1], [2], search-based [3], [4], or sampling-based [5], [6], [7], [8], [9]).",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "lenging due to localization drift, noisy features, environment changes and limited on-board computation [10], [11].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 10,
      "context" : "lenging due to localization drift, noisy features, environment changes and limited on-board computation [10], [11].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 11,
      "context" : "edu infeasible [12].",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "Due to their representational power, neuralnetwork-based learning techniques have become increasingly of interest to the robotics community as a method to encode such perception-action policies or value functions [13], [14].",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 13,
      "context" : "Due to their representational power, neuralnetwork-based learning techniques have become increasingly of interest to the robotics community as a method to encode such perception-action policies or value functions [13], [14].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 14,
      "context" : "Recent work has shown that neural networks can be used to navigate a wheeled robot in cluttered environments [15].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "Current network architectures are not well-suited for long-term sequential tasks [16], [17], which typically appear in robotics (e.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "Current network architectures are not well-suited for long-term sequential tasks [16], [17], which typically appear in robotics (e.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "the model-selection phase [18].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 18,
      "context" : "• We estimate the Vapnik-Chervonenkis (VC) dimension [19] of the last layer of a neural network (after all upstream layer transformations) as a measure of the network generalization ability that depends only on the",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : "developed Value Iteration Networks (VIN) that outperform regular convolutional networks in planning-based sequential reasoning [20].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "Control (MPC) in guided policy search to train a neural network to navigate a quadrotor using lidar inputs [21].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 21,
      "context" : "ory could improve failure cases [22].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 22,
      "context" : "Existing works consider only convex obstacles, and as a result they may have limited success in autonomous robot navigation in environments which contain complex obstacles such as cul-de-sacs [23].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 23,
      "context" : "method to incorporate memory in robotics learning problems are through Recurrent Neural Networks (RNN) such as the Long-Short Term Memory (LSTM) [24].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 24,
      "context" : "demonstrate that recurrent neural networks are able to learn in partially-observable problems [25].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 25,
      "context" : "Critic (A3C) algorithm and demonstrate improved performance over a feedforward network [26].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 26,
      "context" : "argue that memory is necessary in partially-observed tasks, and augment the state space to include memory states [27].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 27,
      "context" : "be used to simulate arbitrary procedures, learning the optimal network is not easy in practice [28].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 28,
      "context" : "This observation inspired a new family of neural network architectures called Memory Augmented Neural Networks (MANN) which utilize an explicit memory matrix [29].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 29,
      "context" : "improved performance over standard RNNs in tasks such as copying sequences and planning shortest paths [30].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "More precisely, we consider a finitehorizon discounted MDP defined by (I,U , T ,R, γ), where γ ∈ (0, 1] is a discount factor, I is the state space, U is the action space, T : I × U × I → [0, 1] is the transition function, and R : I × U × I → R is the reward function.",
      "startOffset" : 187,
      "endOffset" : 193
    }, {
      "referenceID" : 30,
      "context" : "In contrast, we consider learning a feasible policy by using the outputs of an A∗ path planner [31] for supervision.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 31,
      "context" : "6] and ConvNets, have been very successful in various vision and robotic tasks [34][35].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 32,
      "context" : "6] and ConvNets, have been very successful in various vision and robotic tasks [34][35].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 27,
      "context" : "memory in neural network architectures was inspired by Turing Machines [28].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 33,
      "context" : "states and observations encountered by the expert will be different than those encountered by the policy [36].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 33,
      "context" : "The DAgger [36] algorithm addresses both of these problems.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 25,
      "context" : "Asynchronous DAgger is inspired by the Asynchronous Advantage Actor Critic (A3C) algorithm [26], but differs since the A3C algorithm is an actor-critic reinforcement learning algorithm, while ours is a supervised sequential prediction",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 29,
      "context" : "2See [30] for details on the computations and memory matrix updates.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 34,
      "context" : "algorithm, the original DAgger algorithm is analogous to the original Deep Q-Network (DQN) algorithm [37] in that both store an experience replay databank.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 35,
      "context" : "by a single readout layer [38].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 36,
      "context" : "Previous works have recognized the benefit of applying maximum margin SVMs to neural networks by using them as the final readout layer in tasks such as image recognition [39], [40], [41], [42].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 37,
      "context" : "Previous works have recognized the benefit of applying maximum margin SVMs to neural networks by using them as the final readout layer in tasks such as image recognition [39], [40], [41], [42].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 38,
      "context" : "Previous works have recognized the benefit of applying maximum margin SVMs to neural networks by using them as the final readout layer in tasks such as image recognition [39], [40], [41], [42].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 39,
      "context" : "Previous works have recognized the benefit of applying maximum margin SVMs to neural networks by using them as the final readout layer in tasks such as image recognition [39], [40], [41], [42].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 18,
      "context" : "Ψ(qi, hi) for all i and |w0| is the norm of the optimal weights [19].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 40,
      "context" : "The vector Ψ∗ = ∑n i=1 Ψ(qi, hi)p ∗ i is the center of the smallest enclosing sphere and the squared radius R is the negative value of the objective function at p∗ [44].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 18,
      "context" : "3We refer the reader to [19] and [43] for in-depth details and proofs.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 26,
      "context" : "of the system observed by [27], but is not desirable in our case because it does not actually test the network’s ability to retain memory.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 41,
      "context" : "We use the RMSProp [45] algorithm with a learning rate of 10−4 to calculate gradients.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 18,
      "context" : "However, we follow the strategy in [19] and present our results in a one-vs-all binary classification framework.",
      "startOffset" : 35,
      "endOffset" : 39
    } ],
    "year" : 2017,
    "abstractText" : "This paper highlights the significance of including memory structures in neural networks when the latter are used to learn perception-action loops for autonomous robot navigation. Traditional navigation approaches rely on global maps of the environment to overcome cul-de-sacs and plan feasible motions. Yet, maintaining an accurate global map may be challenging in real-world settings. A possible way to mitigate this limitation is to use learning techniques that forgo handengineered map representations and infer appropriate control responses directly from sensed information. An important but unexplored aspect of such approaches is the effect of memory on their performance. This work is a first thorough study of memory structures for deep-neural-network-based robot navigation, and offers novel tools to train such networks from supervision and quantify their ability to generalize to unseen scenarios. We analyze the separation and generalization abilities of feedforward, long short-term memory, and differentiable neural computer networks. We introduce a new method to evaluate the generalization ability by estimating the VC-dimension of networks with a final linear readout layer. We validate that the VC estimates are good predictors of actual test performance. The reported method can be applied to deep learning problems beyond robotics.",
    "creator" : "LaTeX with hyperref package"
  }
}