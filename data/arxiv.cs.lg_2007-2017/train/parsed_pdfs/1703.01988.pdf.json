{
  "name" : "1703.01988.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Neural Episodic Control",
    "authors" : [ "Alexander Pritzel", "Benigno Uria", "Sriram Srinivasan", "Adrià Puigdomènech", "Oriol Vinyals", "Daan Wierstra", "Charles Blundell" ],
    "emails" : [ "APRITZEL@GOOGLE.COM", "BURIA@GOOGLE.COM", "SRSRINIVASAN@GOOGLE.COM", "ADRIAP@GOOGLE.COM", "VINYALS@GOOGLE.COM", "DEMISHASSABIS@GOOGLE.COM", "WIERSTRA@GOOGLE.COM", "CBLUNDELL@GOOGLE.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Deep reinforcement learning agents have achieved state-ofthe-art results in a variety of complex environments (Mnih et al., 2015; 2016), often surpassing human performance (Silver et al., 2016). Although the final performance of these agents is impressive, these techniques usually require several orders of magnitude more interactions with their environment than a human in order to reach an equivalent level of expected performance. For example, in the Atari 2600 set of environments (Bellemare et al., 2013), deep Q-networks (Mnih et al., 2016) require more than 200 hours of gameplay in order to achieve scores similar to those a human player achieves after two hours (Lake et al., 2016).\nThe glacial learning speed of deep reinforcement learning has several plausible explanations and in this work we focus on addressing these:\n1. Stochastic gradient descent optimisation requires the use of small learning rates. Due to the global approximation nature of neural networks, high learning rates cause catastrophic interference (McCloskey & Cohen, 1989). Low learning rates mean that experience can only be incorporated into a neural network slowly.\n2. Environments with a sparse reward signal can be difficult for a neural network to model as there may be very few instances where the reward is non-zero. This can be viewed as a form of class imbalance where low-reward samples outnumber high-reward samples by an unknown number. Consequently, the neural network disproportionately underperforms at predicting larger rewards, making it difficult for an agent to take the most rewarding actions.\n3. Reward signal propagation by value-bootstrapping techniques, such as Q-learning, results in reward information being propagated one step at a time through the history of previous interactions with the environment. This can be fairly efficient if updates happen in reverse order in which the transitions occur. However, in order to train on uncorrelated minibatches DQN-style, algorithms train on randomly selected transitions, and, in order to further stabilise training, require the use of a slowly updating target network further slowing down reward propagation.\nIn this work we shall focus on addressing the three concerns listed above; we must note, however, that other recent advances in exploration (Osband et al., 2016), hierarchical reinforcement learning (Vezhnevets et al., 2016) and transfer learning (Rusu et al., 2016; Fernando et al., 2017) also make substantial contributions to improving data efficiency in deep reinforcement learning over baseline agents.\nIn this paper we propose Neural Episodic Control (NEC), a method which tackles the limitations of deep reinforcement learning listed above and demonstrates dramatic im-\nar X\niv :1\n70 3.\n01 98\n8v 1\n[ cs\n.L G\n] 6\nM ar\nprovements on the speed of learning for a wide range of environments. Critically, our agent is able to rapidly latch onto highly successful strategies as soon as they are experienced, instead of waiting for many steps of optimisation (e.g., stochastic gradient descent) as is the case with DQN (Mnih et al., 2015) and A3C (Mnih et al., 2016).\nOur work is in part inspired by the hypothesised role of the Hippocampus in decision making (Lengyel & Dayan, 2007; Blundell et al., 2016) and also by recent work on one-shot learning (Vinyals et al., 2016) and learning to remember rare events with neural networks (Kaiser et al., 2016). Our agent uses a semi-tabular representation of its experience of the environment possessing several of the features of episodic memory such as long term memory, sequentiality, and context-based lookups. The semi-tabular representation is an append-only memory that binds slow-changing keys to fast updating values and uses a context-based lookup on the keys to retrieve useful values during action selection by the agent. Thus the agent’s memory operates in much the same way that traditional table-based RL methods map from state and action to value estimates. A unique aspect of the memory in contrast to other neural memory architectures for reinforcement learning (explained in more detail in Section 3) is that the values retrieved from the memory can be updated much faster than the rest of the deep neural network. This helps alleviate the typically slow weight updates of stochastic gradient descent applied to the whole network and is reminiscent of work on fast weights (Ba et al., 2016; Hinton & Plaut, 1987), although the architecture we present is quite different. Another unique aspect of the memory is that unlike other memory architectures such as LSTM and the differentiable neural computer (DNC; Graves et al., 2016), our architecture does not try to learn when to write to memory, as this can be slow to learn and take a significant amount of time. Instead, we elect to write all experiences to the memory, and allow it to grow very large compared to existing memory architectures (in contrast to Oh et al. (2015); Graves et al. (2016) where the memory is wiped at the end of each episode). Reading from this large memory is made efficient using kd-tree based nearest neighbour (Bentley, 1975).\nThe remainder of the paper is organised as follows: in Section 2 we review deep reinforcement learning, in Section 3 the Neural Episodic Control algorithm is described, in Section 4 we report experimental results in the Atari Learning Environment, in Section 5 we discuss other methods that use memory for reinforcement learning, and finally in Section 6 we outline future work and summarise the main advantages of the NEC algorithm."
    }, {
      "heading" : "2. Deep Reinforcement Learning",
      "text" : "The action-value function of a reinforcement learning agent (Sutton & Barto, 1998) is defined as Qπ(s, a) = Eπ [ ∑ t γ\ntrt | s, a], where a is the initial action taken by the agent in the initial state s and the expectation denotes that the policy π is followed thereafter. The discount factor γ ∈ (0, 1) trades off favouring short vs. long term rewards.\nDeep Q-Network agents (DQN; Mnih et al., 2015) use Qlearning (Watkins & Dayan, 1992) to learn a value function Q(st, at) to rank which action at is best to take in each state st at step t. The agent then executes an -greedy policy based upon this value function to trade-off exploration and exploitation: with probability the agent picks an action uniformly at random, otherwise it picks the action at = argmaxaQ(st, a).\nIn DQN, the action-value function Q(st, at) is parameterised by a convolutional neural network that takes a 2D pixel representation of the state st as input, and outputs a vector containing the value of each action at that state. When the agent observes a transition, DQN stores the (st, at, rt, st+1) tuple in a replay buffer, the contents of which are used for training. This neural network is trained by minimizing the squared error between the network’s output and the Q-learning target yt = rt + γmaxa Q̃(st+1, a), for a subset of transitions sampled at random from the replay buffer. The target network Q̃(st+1, a) is an older version of the value network that is updated periodically. The use of a target network and uncorrelated samples from the replay buffer are critical for stable training.\nA number of extensions have been proposed that improve DQN. Double DQN (Van Hasselt et al., 2016) reduces bias on the target calculation. Prioritised Replay (Schaul et al., 2015b) further improves Double DQN by optimising the replay strategy. Several authors have proposed methods of improving reward propagation and the back up mechanism of Q learning (Harutyunyan et al., 2016; Munos et al., 2016; He et al., 2016) by incorporating on-policy rewards or by adding constraints to the optimisation. Q∗(λ) (Harutyunyan et al., 2016) and Retrace(λ) (Munos et al., 2016) change the form of the Q-learning target to incorporate on-policy samples and fluidly switch between on-policy learning and off-policy learning. Munos et al. (2016) show that by incorporating on-policy samples allows an agent to learn faster in Atari environments, indicating that reward propagation is indeed a bottleneck to efficiency in deep reinforcement learning.\nA3C (Mnih et al., 2016) is another well known deep reinforcement learning algorithm that is very different from DQN. It is based upon a policy gradient, and learns both a policy and its associated value function, which is learned\nentirely on-policy (similar to the λ = 1 case of Q(λ)). Interestingly, Mnih et al. (2016) also added an LSTM memory to the otherwise convolutional neural network architecture to give the agent a notion of memory, although this did not have significant impact on the performance on Atari games."
    }, {
      "heading" : "3. Neural Episodic Control",
      "text" : "Our agent consists of three components: a convolutional neural network that processes pixel images s, a set of memory modules (one per action), and a final network that converts read-outs from the action memories into Q(s, a) values. For the convolutional neural network we use the same architecture as DQN (Mnih et al., 2015)."
    }, {
      "heading" : "3.1. Differentiable Neural Dictionary",
      "text" : "For each action a ∈ A, NEC has a simple memory module Ma = (Ka, Va), where Ka and Va are dynamically sized arrays of vectors, each containing the same number of vectors. The memory module acts as an arbitrary association from keys to corresponding values, much like the dictionary data type found in programs. Thus we refer to this kind of memory module as a differentiable neural dictionary (DND). There are two operations possible on a DND: lookup and write, as depicted in Figure 1. Performing a lookup on a DND maps a key h to an output value o:\no = ∑ i wivi, (1)\nwhere vi is the ith element of the array Va and wi = k(h, hi)/ ∑ j k(h, hj), (2)\nwhere hi is the ith element of the array Ka and k(x, y) is a kernel between vectors x and y, e.g., Gaussian or inverse kernels. Thus the output of a lookup in a DND is a weighted sum of the values in the memory, whose weights are given by normalised kernels between the lookup key and the corresponding key in memory. To make queries into very large memories scalable we shall make two approximations in practice: firstly, we shall limit (1) to the top p-nearest neighbours (typically p = 50). Secondly, we use an approximate nearest neighbours algorithm to perform the lookups, based upon kd-trees (Bentley, 1975).\nAfter a DND is queried, a new key-value pair is written into the memory. The key written corresponds to the key that was looked up. The associated value is application-specific (below we specify the update for the NEC agent). Writes to a DND are append-only: keys and values are written to the memory by appending them onto the end of the arrays Ka and Va respectively. If a key already exists in the memory, then its corresponding value is updated, rather than being duplicated.\nNote that a DND is a differentiable version of the memory module described in Blundell et al. (2016). It is also a generalisation to the memory and lookup schemes described in (Vinyals et al., 2016; Kaiser et al., 2016) for classification."
    }, {
      "heading" : "3.2. Agent Architecture",
      "text" : "Figure 2 shows a DND as part of the NEC agent for a single action, whilst Algorithm 1 describes the general outline of the NEC algorithm. The pixel state s is processed by a convolutional neural network to produce a key h. The key h is then used to lookup a value from the DND, yielding weights wi in the process for each element of the memory arrays. Finally, the output is a weighted sum of the values in the DND. The values in the DND, in the case of an NEC agent, are the Q values corresponding to the state that originally resulted in the corresponding key-value pair to be written to the memory. Thus this architecture produces an estimate of Q(s, a) for a single given action a. The architecture is replicated once for each action a the agent can take, with the convolutional part of the network shared among each separate DND Ma. The NEC agent acts by taking the action with the highest Q-value estimate at each time step. In practice, we use -greedy policy during training with a low .\nAlgorithm 1 Neural Episodic Control D: replay memory. Ma: a DND for each action a. N : horizon for N -step Q estimate. for each episode do\nfor t = 1, 2, . . . , T do Receive observation st from environment with embedding h. Estimate Q(st, a) for each action a via (1) from Ma at ← -greedy policy based on Q(st, a) Take action at, receive reward rt+1 Append (h,Q(N)(st, at)) to Mat . Append (st, at, Q(N)(st, at)) to D. Train on a random minibatch from D.\nend for end for\n3.3. Adding (s, a) pairs to memory\nAs an NEC agent acts, it continually adds new key-value pairs to its memory. Keys are appended to the memory of the corresponding action, taking the value of the query key h encoded by the convolutional neural network. We now turn to the question of an appropriate corresponding value. In Blundell et al. (2016), Monte Carlo returns were written to memory. We found that a mixture of Monte Carlo returns (on-policy) and off-policy backups worked better and so for NEC we elect to use N -step Q-learning as in Mnih et al.\n(2016) (see also Watkins, 1989; Peng & Williams, 1996). This adds the following N on-policy rewards and bootstraps the sum of discounted rewards for the rest of the trajectory, off-policy. The N -step Q-value estimate is then\nQ(N)(st, a) = N−1∑ j=0 γjrt+j + γ N max a′ Q(st+N , a ′) . (3)\nThe bootstrap term of (3), maxa′ Q(st+N , a′) is found by querying all memories Ma for each action a and taking the highest estimated Q-value returned. Note that the earliest such values can be added to memory is N steps after a particular (s, a) pair occurs.\nWhen a state-action value is already present in a DND (i.e the exact same key h is already in Ka), the corresponding value present in Va, Qi, is updated in the same way as the classic tabular Q-learning algorithm:\nQi ← Qi + α(Q(N)(s, a)−Qi) . (4)\nwhere α is the learning rate of the Q update. If the state is not already present Q(N)(st, a) is appended to Va and h is appended to Ka. Note that our agent learns the value function in much the same way that a classic tabular Q-learning agent does, except that the Q-table grows with time. We found that α could take on a high value, allowing repeatedly visited states with a stable representation to rapidly update their value function estimate. Additionally, batching up memory updates (e.g., at the end of the episode) helps with computational performance. We overwrite the item that has least recently shown up as a neighbour when we reach the memory’s maximum capacity."
    }, {
      "heading" : "3.4. Learning",
      "text" : "Agent parameters are updated by minimising the L2 loss between the predicted Q value for a given action and the Q(N) estimate on randomly sampled mini-batches from a replay buffer. In particular, we store tuples (st, at, Rt) in\nthe replay buffer, where N is the horizon of the N -step Q rule, and Rt = Q(N)(st, a) plays the role of the target network seen in DQN (our replay buffer is significantly smaller than DQN’s). These (st, at, Rt)-tuples are then sampled uniformly at random to form minibatches for training. Note that the architecture in Figure 2 is entirely differentiable and so we can minimize this loss by gradient descent. Backpropagation updates the the weights and biases of the convolutional embedding network and the keys and values of each action-specific memory using gradients of this loss, using a lower learning rate than is used for updating pairs after queries (α)."
    }, {
      "heading" : "4. Experiments",
      "text" : "We investigated whether neural episodic control allows for more data efficient learning in practice in complex domains. As a problem domain we chose the Atari Learning Environment(ALE; Bellemare et al., 2013). We tested our method on the 57 Atari games used by Schaul et al. (2015a), which form an interesting set of tasks as they contain diverse challenges such as sparse rewards and vastly different magnitudes of scores across games. Most common algorithms applied in these domains, such as variants of DQN and A3C, require in the thousands of hours of in-game time, i.e. they are data inefficient.\nWe consider 5 variants of A3C and DQN as baselines as well as MFEC (Blundell et al., 2016). We compare to the basic implementations of A3C (Mnih et al., 2016) and DQN (Mnih et al., 2015). We also compare to two algorithms incorporating λ returns (Sutton, 1988) aiming at more data efficiency by faster propagation of credit assignments, namely Q∗(λ) (Harutyunyan et al., 2016) and Retrace(λ) (Munos et al., 2016). We also compare to DQN with Prioritised Replay, which improves data efficiency by replaying more salient transitions more frequently. We did not directly compare to DRQN (Hausknecht & Stone, 2015) nor FRMQN (Oh et al., 2016) as results were not available\nfor all Atari games. Note that in the case of DRQN, reported performance is lower than that of Prioritised Replay.\nAll algorithms were trained using discount rate γ = 0.99, except MFEC that uses γ = 1. In our implementation of MFEC we used random projections as an embedding function, since in the original publication it obtained better performance on the Atari games tested.\nIn terms of hyperparameters for NEC, we chose the same convolutional architecture as DQN, and store up to 5 × 105 memories per action. We used the RMSProp algorithm (Tieleman & Hinton, 2012) for gradient descent training. We apply the same preprocessing steps as (Mnih et al., 2015), including repeating each action four times. For the N -step Q estimates we picked a horizon of N = 100. Our replay buffer stores the only last 105 states (as opposed to 106 for DQN) observed and their N -step Q estimates. We do one replay update for every 16 observed frames with a minibatch of size 32. We set the number of nearest neighbours p = 50 in all our experiments. For the kernel function we chose a function that interpolates between the mean for short distances and weighted inverse distance for large distances, more precisely:\nk(h, hi) = 1\n‖h− hi‖22 + δ . (5)\nIntuitively, when all neighbours are far away we want to avoid putting all weight onto one data point. A Gaussian kernel, for example, would exponentially suppress all neighbours except for the closest one. The kernel we chose has the advantage of having heavy tails. This makes the algorithm more robust and we found it to be less sensitive to kernel hyperparameters. We set δ = 10−3.\nIn order to tune the remaining hyperparameters (SGD learning-rate, fast-update learning-rate α in Equation 4, dimensionality of the embeddings, Q(N) in Equation 3, and - greedy exploration-rate) we ran a hyperparameter sweep on six games: Beam Rider, Breakout, Pong, Q*Bert, Seaquest and Space Invaders. We picked the hyperparameter values that performed best on the median for this subset of games (a\ncommon cross validation procedure described by Bellemare et al. (2013), and adhered to by Mnih et al. (2015)).\nData efficiency results are summarised in Table 1. In the small data regime (less than 20 million frames) NEC clearly outperforms all other algorithms. The difference is especially pronounced before 5 million frames have been observed. Only at 40 million frames does DQN with Prioritised Replay outperform NEC on average; note that this corresponds to 185 hours of gameplay.\nIn order to provide a more detailed picture of NEC’s performance, Figures 3 to 7 show learning curves on 6 games (Alien, Bowling, Boxing, Frostbite, HERO, Ms. Pac-Man, Pong), where several stereotypical cases of NEC’s performance can be observed. All learning curves show the average performance over 5 different initial random seeds. We evaluate MFEC and NEC every 200.000 frames, and the other algorithms are evaluated every million steps.\nAcross most games, NEC is significantly faster at learning in the initial phase (see also Table 1), only comparable to MFEC, which also uses an episodic-like Q-function.\nNEC also outperforms MFEC on average (see Table 2). In contrast with MFEC, NEC uses the reward signal to learn an embedding adequate for value interpolation. This difference is especially significant in games where a few pixels determine the value of each action. The simpler version of MFEC uses an approximation to L2 distances in pixel-space by means of random projections, and cannot focus on the small but most relevant details. Another version of MFEC calculated distances on the latent representation of a variational autoencoder (Kingma & Welling, 2013) trained to model frames. This latent representation does not depend on rewards and will be subject to irrelevant details like, for example, the display of the current score.\nA3C, DQN and related algorithms require rewards to be clipped to the range [−1, 1] for training stability1(Mnih\n1See Pop–Art (van Hasselt et al., 2016) for a DQN-like algorithm that does not require reward-clipping. NEC also outperforms\net al., 2015). NEC and MFEC do not require reward clipping, which results in qualitative changes in behaviour and better performance relative to other algorithms on games requiring clipping (Bowling, Frostbite, H.E.R.O., Ms. PacMan, Alien out of the seven shown).\nFigure 3. Learning curve on Bowling.\nAlien and Ms. Pac-Man both involve controlling a character, where there is an easy way to collect small rewards by collecting items of which there are plenty, while avoiding enemies, which are invulnerable to the agent. On the other hand the agent can pick up a special item making enemies vulnerable, allowing the agent to attack them and get significantly larger rewards than from collecting the small rewards. Agents trained using existing parametric methods tend to show little interest in this as clipping implies there is no difference between large and small rewards. Therefore, as NEC does not need reward clipping, it can strongly\nPop–Art.\nFigure 4. Learning curve on Frostbite.\noutperform other algorithms, since NEC is maximising the non-clipped score (the true score). This can also be seen when observing the agents play: parametric methods will tend to collect small rewards, while NEC will try to actively make the enemies vulnerable and attack them to get large rewards.\nNEC also outperforms the other algorithms on Pong and Boxing where reward clipping does not affect any of the algorithms as all original rewards are in the range [−1, 1]; as can be expected, NEC does not outperform others in terms of maximally achieved score, but it is vastly more data efficient.\nIn Figure 10 we show a chart of human-normalised scores across all 57 Atari games at 10 million frames comparing to Prioritised Replay and MFEC. We rank the games independently for each algorithm, and on the y-axis the deciles are\nFigure 5. Learning curve on H.E.R.O.\nFigure 6. Learning curve on Ms. Pac-Man.\nshown.\nWe can see that NEC gets to a human level performance in about 25% of the games within 10 million frames. As we can see NEC outperforms MFEC and Prioritised Replay."
    }, {
      "heading" : "5. Related work",
      "text" : "There has been much recent work on memory architectures for neural networks (LSTM; Hochreiter & Schmidhuber, 1997), DNC (Graves et al., 2016), memory networks (Sukhbaatar et al., 2015; Miller et al., 2016)). Recurrent neural network representations of memory (LSTMs and DNCs) are trained by truncated backpropagation through time, and are subject to the same slow learning of non-recurrent neural networks.\nSome of these models have been adapted to their use in RL agents (LSTMs; Bakker et al., 2003; Hausknecht & Stone, 2015), DNCs (Graves et al., 2016), memory networks (Oh et al., 2016). However, the contents of these memories is typically reset at the beginning of every episode. This is ap-\nFigure 7. Learning curve on Alien.\nFigure 8. Learning curve on Pong.\npropriate when the goal of the memory is tracking previous observations in order to maximise rewards in partially observable or non-Markovian environments. Therefore, these implementations can be thought of as a type of working memory, and solve a different problem than the one addressed in this work.\nRNNs can learn to quickly write highly rewarding states into memory and may even be able to learn entire reinforcement learning algorithms (Wang et al., 2016; Duan et al., 2016). However, doing so can take an arbitrarily long time and the learning time likely scales strongly with the complexity of the task.\nThe work of Oh et al. (2016) is also reminiscent of the ideas presented here. They introduced (FR)MQN, an adaptation of memory networks used in the top layers of a Q-network.\nKaiser et al. (2016) introduced a differentiable layer of keyvalue pairs that can be plugged into a neural network. This layer uses cosine similarity to calculate a weighted average of the values associated with the k most similar memories. Their use of a moving average update rule is reminiscent of\nthe one presented in Section 3. The authors reported results on a set of supervised tasks, however they did not consider applications to reinforcement learning. Other deep RL methods keep a history of previous experience. Indeed, DQN itself has an elementary form of memory: the replay buffer central to its stable training can be viewed as a memory that is frequently replayed to distil the contents into DQN’s value network. Kumaran et al. (2016) suggest that training on replayed experiences from the replay buffer in DQN is similar to the replay of experiences from episodic memory during sleep in animals. DQN’s replay buffer differs from most other work on memory for deep reinforcement learning in its sheer scale: it is common for DQN’s replay buffer to hold millions of (s, a, r, s′) tuples. The use of local regression techniques for Q-function approximation has been suggested before: Santamarı́a et al. (1997) proposed the use of k-nearest-neighbours regression with a heuristic for adding memories based on the distance to previous memories. Munos & Moore (1998) proposed barycentric interpolators to model the value function and proved their convergence to the optimal value function under mild conditions, but no empirical results were presented. Gabel & Riedmiller (2005) also suggested the use of local regression, under the paradigm of case-based-reasoning that included heuristics for the deletion of stored cases. Blundell et al. (2016, MFEC) recently used local regression for Q-function estimation using the mean of the k-nearest neighbours, except in the case of an exact match of the query point, in which case the stored value was returned. They also propose the use of the latent variable obtained from a variational autoencoder (Rezende et al., 2014) as an embedding space, but showed random projections often obtained better results. In contrast with the ideas presented here, none of the localregression work aforementioned uses the reward signal to learn an embedding space of covariates in which to perform the local-regression. We learn this embedding space using temporal-difference learning; a crucial difference, as we\nshowed in the experimental comparison to MFEC."
    }, {
      "heading" : "6. Discussion",
      "text" : "We have proposed Neural Episodic Control (NEC): a deep reinforcement learning agent that learns significantly faster than other baseline agents on a wide range of Atari 2600 games. At the core of NEC is a memory structure: a Differentiable Neural Dictionary (DND), one for each potential action. NEC inserts recent state representations paired with corresponding value functions into the appropriate DND.\nOur experiments show that NEC requires an order of magnitude fewer interactions with the environment than agents previously proposed for data efficiency, such as Prioritised Replay (Schaul et al., 2015b) and Retrace(λ) (Munos et al., 2016). We speculate that NEC learns faster through a com-\nbination of three features of the agent: the memory architecture (DND), the use of N -step Q estimates, and a state representation provided by a convolutional neural network.\nThe memory architecture, DND, rapidly integrates recent experience—state representations and corresponding value estimates—allowing this information to be rapidly integrated into future behaviour. Such memories persist across many episodes, and we use a fast approximate nearest neighbour algorithm (kd-trees) to ensure that such memories can be efficiently accessed. Estimating Q-values by using the N -step Q value function interpolates between Monte Carlo value estimates and backed up off-policy estimates. Monte Carlo value estimates reflect the rewards an agent is actually receiving, whilst backed up off-policy estimates should be more representative of the value function at the optimal policy, but evolve much slower. By using both estimates, NEC can trade-off between these two estimation procedures and their relative strengths and weaknesses (speed of reward propagation vs optimality). Finally, by having a slow changing, stable representation provided by a convolutional neural network, keys stored in the DND remain relative stable.\nOur work suggests that non-parametric methods are a promising addition to the deep reinforcement learning toolbox, especially where data efficiency is paramount. In our experiments we saw that at the beginning of learning NEC outperforms other agents in terms of learning speed. We saw that later in learning Prioritised Replay has higher performance than NEC. We leave it to future work to further improve NEC so that its long term final performance is significantly superior to parametric agents. Another avenue of further research would be to apply the method discussed in this paper to a wider range of tasks such as visually more complex 3D worlds or real world tasks where data efficiency is of great importance due to the high cost of acquiring data.\nAcknowledgements The authors would like to thank Daniel Zoran, Dharshan Kumaran, Jane Wang, Dan Belov, Ruiqi Guo, Yori Zwols, Jack Rae, Andreas Kirsch, Peter Dayan, David Silver and many others at DeepMind for insightful discussions and feedback. We also thank Georg Ostrovski, Tom Schaul, and Hubert Soyer for providing baseline learning curves."
    } ],
    "references" : [ {
      "title" : "Using fast weights to attend to the recent past",
      "author" : [ "Ba", "Jimmy", "Hinton", "Geoffrey E", "Mnih", "Volodymyr", "Leibo", "Joel Z", "Ionescu", "Catalin" ],
      "venue" : "In Advances In Neural Information Processing Systems,",
      "citeRegEx" : "Ba et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research, 47:253–279,",
      "citeRegEx" : "Bellemare et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael" ],
      "venue" : "J. Artif. Intell. Res.(JAIR),",
      "citeRegEx" : "Bellemare et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "Multidimensional binary search trees used for associative searching",
      "author" : [ "Bentley", "Jon Louis" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "Bentley and Louis.,? \\Q1975\\E",
      "shortCiteRegEx" : "Bentley and Louis.",
      "year" : 1975
    }, {
      "title" : "Model-free episodic control",
      "author" : [ "Blundell", "Charles", "Uria", "Benigno", "Pritzel", "Alexander", "Li", "Yazhe", "Ruderman", "Avraham", "Leibo", "Joel Z", "Rae", "Jack", "Wierstra", "Daan", "Hassabis", "Demis" ],
      "venue" : "arXiv preprint arXiv:1606.04460,",
      "citeRegEx" : "Blundell et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Blundell et al\\.",
      "year" : 2016
    }, {
      "title" : "Rl: Fast reinforcement learning via slow reinforcement learning",
      "author" : [ "Duan", "Yan", "Schulman", "John", "Chen", "Xi", "Bartlett", "Peter L", "Sutskever", "Ilya", "Abbeel", "Pieter" ],
      "venue" : "arXiv preprint arXiv:1611.02779,",
      "citeRegEx" : "Duan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2016
    }, {
      "title" : "Pathnet: Evolution channels gradient descent in super neural networks",
      "author" : [ "Fernando", "Chrisantha", "Banarse", "Dylan", "Blundell", "Charles", "Zwols", "Yori", "Ha", "David", "Rusu", "Andrei A", "Pritzel", "Alexander", "Wierstra", "Daan" ],
      "venue" : "arXiv preprint arXiv:1701.08734,",
      "citeRegEx" : "Fernando et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Fernando et al\\.",
      "year" : 2017
    }, {
      "title" : "Cbr for state value function approximation in reinforcement learning",
      "author" : [ "Gabel", "Thomas", "Riedmiller", "Martin" ],
      "venue" : "In International Conference on Case-Based Reasoning,",
      "citeRegEx" : "Gabel et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Gabel et al\\.",
      "year" : 2005
    }, {
      "title" : "lambda) with off-policy corrections",
      "author" : [ "Harutyunyan", "Anna", "Bellemare", "Marc G", "Stepleton", "Tom", "Munos", "Rémi" ],
      "venue" : "In International Conference on Algorithmic Learning Theory,",
      "citeRegEx" : "Harutyunyan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Harutyunyan et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep recurrent qlearning for partially observable mdps",
      "author" : [ "Hausknecht", "Matthew", "Stone", "Peter" ],
      "venue" : "arXiv preprint arXiv:1507.06527,",
      "citeRegEx" : "Hausknecht et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hausknecht et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to play in a day: Faster deep reinforcement learning by optimality tightening",
      "author" : [ "He", "Frank S", "Liu", "Yang", "Schwing", "Alexander G", "Peng", "Jian" ],
      "venue" : "arXiv preprint arXiv:1611.01606,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Using fast weights to deblur old memories",
      "author" : [ "Hinton", "Geoffrey E", "Plaut", "David C" ],
      "venue" : "In Proceedings of the ninth annual conference of the Cognitive Science Society,",
      "citeRegEx" : "Hinton et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 1987
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Sepp", "Schmidhuber", "Jürgen" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Learning to remember rare",
      "author" : [ "Kaiser", "Lukasz", "Nachum", "Ofir", "Roy", "Aurko", "Bengio", "Samy" ],
      "venue" : null,
      "citeRegEx" : "Kaiser et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kaiser et al\\.",
      "year" : 2016
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Kingma", "Diederik P", "Welling", "Max" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2013
    }, {
      "title" : "What learning systems do intelligent agents need? complementary learning systems theory updated",
      "author" : [ "Kumaran", "Dharshan", "Hassabis", "Demis", "McClelland", "James L" ],
      "venue" : "Trends in Cognitive Sciences,",
      "citeRegEx" : "Kumaran et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kumaran et al\\.",
      "year" : 2016
    }, {
      "title" : "Building machines that learn and think like people",
      "author" : [ "Lake", "Brenden M", "Ullman", "Tomer D", "Tenenbaum", "Joshua B", "Gershman", "Samuel J" ],
      "venue" : "arXiv preprint arXiv:1604.00289,",
      "citeRegEx" : "Lake et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2016
    }, {
      "title" : "Hippocampal contributions to control: The third way",
      "author" : [ "M. Lengyel", "P. Dayan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lengyel and Dayan,? \\Q2007\\E",
      "shortCiteRegEx" : "Lengyel and Dayan",
      "year" : 2007
    }, {
      "title" : "Catastrophic interference in connectionist networks: The sequential learning problem",
      "author" : [ "McCloskey", "Michael", "Cohen", "Neal J" ],
      "venue" : "Psychology of learning and motivation,",
      "citeRegEx" : "McCloskey et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "McCloskey et al\\.",
      "year" : 1989
    }, {
      "title" : "Keyvalue memory networks for directly reading documents",
      "author" : [ "Miller", "Alexander", "Fisch", "Adam", "Dodge", "Jesse", "Karimi", "Amir-Hossein", "Bordes", "Antoine", "Weston", "Jason" ],
      "venue" : "arXiv preprint arXiv:1606.03126,",
      "citeRegEx" : "Miller et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "Mnih", "Volodymyr", "Badia", "Adria Puigdomenech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Barycentric interpolators for continuous space and time reinforcement learning",
      "author" : [ "Munos", "Remi", "Moore", "Andrew W" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Munos et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Munos et al\\.",
      "year" : 1998
    }, {
      "title" : "Safe and efficient off-policy reinforcement learning",
      "author" : [ "Munos", "Rémi", "Stepleton", "Tom", "Harutyunyan", "Anna", "Bellemare", "Marc" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Munos et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Munos et al\\.",
      "year" : 2016
    }, {
      "title" : "Action-conditional video prediction using deep networks in atari games",
      "author" : [ "Oh", "Junhyuk", "Guo", "Xiaoxiao", "Lee", "Honglak", "Lewis", "Richard L", "Singh", "Satinder" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Oh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2015
    }, {
      "title" : "Control of memory, active perception, and action in minecraft",
      "author" : [ "Oh", "Junhyuk", "Chockalingam", "Valliappa", "Lee", "Honglak" ],
      "venue" : "In Proceedings of The 33rd International Conference on Machine Learning,",
      "citeRegEx" : "Oh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep exploration via bootstrapped dqn",
      "author" : [ "Osband", "Ian", "Blundell", "Charles", "Pritzel", "Alexander", "Van Roy", "Benjamin" ],
      "venue" : "In Advances In Neural Information Processing Systems,",
      "citeRegEx" : "Osband et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Osband et al\\.",
      "year" : 2016
    }, {
      "title" : "Incremental multi-step q-learning",
      "author" : [ "Peng", "Jing", "Williams", "Ronald J" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Peng et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 1996
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan" ],
      "venue" : "In Proceedings of The 31st International Conference on Machine Learning,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Experiments with reinforcement learning in problems with continuous state and action spaces",
      "author" : [ "Santamarı́a", "Juan C", "Sutton", "Richard S", "Ram", "Ashwin" ],
      "venue" : "Adaptive behavior,",
      "citeRegEx" : "Santamarı́a et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Santamarı́a et al\\.",
      "year" : 1997
    }, {
      "title" : "Prioritized experience replay",
      "author" : [ "Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David" ],
      "venue" : "arXiv preprint arXiv:1511.05952,",
      "citeRegEx" : "Schaul et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2015
    }, {
      "title" : "End-to-end memory networks",
      "author" : [ "Sukhbaatar", "Sainbayar", "Weston", "Jason", "Fergus", "Rob" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "Sutton", "Richard S" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Sutton and S.,? \\Q1988\\E",
      "shortCiteRegEx" : "Sutton and S.",
      "year" : 1988
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "Sutton", "Richard S", "Barto", "Andrew G" ],
      "venue" : "MIT press,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "Tieleman", "Tijmen", "Hinton", "Geoffrey" ],
      "venue" : "COURSERA: Neural Networks for Machine Learning,",
      "citeRegEx" : "Tieleman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tieleman et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning functions across many orders of magnitudes",
      "author" : [ "H. van Hasselt", "A. Guez", "M. Hessel", "D. Silver" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep reinforcement learning with double q-learning",
      "author" : [ "Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2016
    }, {
      "title" : "Strategic attentive writer for learning macro-actions",
      "author" : [ "Vezhnevets", "Alexander", "Mnih", "Volodymyr", "Osindero", "Simon", "Graves", "Alex", "Vinyals", "Oriol", "Agapiou", "John" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Vezhnevets et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vezhnevets et al\\.",
      "year" : 2016
    }, {
      "title" : "Matching networks for one shot learning",
      "author" : [ "Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Tim", "Wierstra", "Daan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to reinforcement learn",
      "author" : [ "Wang", "Jane X", "Kurth-Nelson", "Zeb", "Tirumala", "Dhruva", "Soyer", "Hubert", "Leibo", "Joel Z", "Munos", "Remi", "Blundell", "Charles", "Kumaran", "Dharshan", "Botvinick", "Matt" ],
      "venue" : "arXiv preprint arXiv:1611.05763,",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning from delayed rewards",
      "author" : [ "Watkins", "Christopher John Cornish Hellaby" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Watkins and Hellaby.,? \\Q1989\\E",
      "shortCiteRegEx" : "Watkins and Hellaby.",
      "year" : 1989
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Deep reinforcement learning agents have achieved state-ofthe-art results in a variety of complex environments (Mnih et al., 2015; 2016), often surpassing human performance (Silver et al.",
      "startOffset" : 110,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "For example, in the Atari 2600 set of environments (Bellemare et al., 2013), deep Q-networks (Mnih et al.",
      "startOffset" : 51,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : ", 2013), deep Q-networks (Mnih et al., 2016) require more than 200 hours of gameplay in order to achieve scores similar to those",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "a human player achieves after two hours (Lake et al., 2016).",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 26,
      "context" : "In this work we shall focus on addressing the three concerns listed above; we must note, however, that other recent advances in exploration (Osband et al., 2016), hierarchical reinforcement learning (Vezhnevets et al.",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 37,
      "context" : ", 2016), hierarchical reinforcement learning (Vezhnevets et al., 2016) and transfer learning (Rusu et al.",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : ", 2016) and transfer learning (Rusu et al., 2016; Fernando et al., 2017) also make substantial contributions to improving data efficiency in deep reinforcement learning over baseline agents.",
      "startOffset" : 30,
      "endOffset" : 72
    }, {
      "referenceID" : 20,
      "context" : ", stochastic gradient descent) as is the case with DQN (Mnih et al., 2015) and A3C (Mnih et al.",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : ", 2015) and A3C (Mnih et al., 2016).",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "Our work is in part inspired by the hypothesised role of the Hippocampus in decision making (Lengyel & Dayan, 2007; Blundell et al., 2016) and also by recent work on one-shot learning (Vinyals et al.",
      "startOffset" : 92,
      "endOffset" : 138
    }, {
      "referenceID" : 38,
      "context" : ", 2016) and also by recent work on one-shot learning (Vinyals et al., 2016) and learning to remember rare events with neural networks (Kaiser et al.",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : ", 2016) and learning to remember rare events with neural networks (Kaiser et al., 2016).",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "This helps alleviate the typically slow weight updates of stochastic gradient descent applied to the whole network and is reminiscent of work on fast weights (Ba et al., 2016; Hinton & Plaut, 1987), although the architecture we present is quite different.",
      "startOffset" : 158,
      "endOffset" : 197
    }, {
      "referenceID" : 0,
      "context" : "This helps alleviate the typically slow weight updates of stochastic gradient descent applied to the whole network and is reminiscent of work on fast weights (Ba et al., 2016; Hinton & Plaut, 1987), although the architecture we present is quite different. Another unique aspect of the memory is that unlike other memory architectures such as LSTM and the differentiable neural computer (DNC; Graves et al., 2016), our architecture does not try to learn when to write to memory, as this can be slow to learn and take a significant amount of time. Instead, we elect to write all experiences to the memory, and allow it to grow very large compared to existing memory architectures (in contrast to Oh et al. (2015); Graves et al.",
      "startOffset" : 159,
      "endOffset" : 711
    }, {
      "referenceID" : 0,
      "context" : "This helps alleviate the typically slow weight updates of stochastic gradient descent applied to the whole network and is reminiscent of work on fast weights (Ba et al., 2016; Hinton & Plaut, 1987), although the architecture we present is quite different. Another unique aspect of the memory is that unlike other memory architectures such as LSTM and the differentiable neural computer (DNC; Graves et al., 2016), our architecture does not try to learn when to write to memory, as this can be slow to learn and take a significant amount of time. Instead, we elect to write all experiences to the memory, and allow it to grow very large compared to existing memory architectures (in contrast to Oh et al. (2015); Graves et al. (2016) where the memory is wiped at the end of each episode).",
      "startOffset" : 159,
      "endOffset" : 733
    }, {
      "referenceID" : 20,
      "context" : "Deep Q-Network agents (DQN; Mnih et al., 2015) use Qlearning (Watkins & Dayan, 1992) to learn a value function Q(st, at) to rank which action at is best to take in each state st at step t.",
      "startOffset" : 22,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "Several authors have proposed methods of improving reward propagation and the back up mechanism of Q learning (Harutyunyan et al., 2016; Munos et al., 2016; He et al., 2016) by incorporating on-policy rewards or by adding constraints to the optimisation.",
      "startOffset" : 110,
      "endOffset" : 173
    }, {
      "referenceID" : 23,
      "context" : "Several authors have proposed methods of improving reward propagation and the back up mechanism of Q learning (Harutyunyan et al., 2016; Munos et al., 2016; He et al., 2016) by incorporating on-policy rewards or by adding constraints to the optimisation.",
      "startOffset" : 110,
      "endOffset" : 173
    }, {
      "referenceID" : 10,
      "context" : "Several authors have proposed methods of improving reward propagation and the back up mechanism of Q learning (Harutyunyan et al., 2016; Munos et al., 2016; He et al., 2016) by incorporating on-policy rewards or by adding constraints to the optimisation.",
      "startOffset" : 110,
      "endOffset" : 173
    }, {
      "referenceID" : 8,
      "context" : "Q∗(λ) (Harutyunyan et al., 2016) and Retrace(λ) (Munos et al.",
      "startOffset" : 6,
      "endOffset" : 32
    }, {
      "referenceID" : 23,
      "context" : ", 2016) and Retrace(λ) (Munos et al., 2016) change the form of the Q-learning target to incorporate on-policy samples and fluidly switch between on-policy learning and off-policy learning.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "Several authors have proposed methods of improving reward propagation and the back up mechanism of Q learning (Harutyunyan et al., 2016; Munos et al., 2016; He et al., 2016) by incorporating on-policy rewards or by adding constraints to the optimisation. Q∗(λ) (Harutyunyan et al., 2016) and Retrace(λ) (Munos et al., 2016) change the form of the Q-learning target to incorporate on-policy samples and fluidly switch between on-policy learning and off-policy learning. Munos et al. (2016) show that by incorporating on-policy samples allows an agent to learn faster in Atari environments, indicating that reward propagation is indeed a bottleneck to efficiency in deep reinforcement learning.",
      "startOffset" : 111,
      "endOffset" : 489
    }, {
      "referenceID" : 21,
      "context" : "A3C (Mnih et al., 2016) is another well known deep reinforcement learning algorithm that is very different from DQN.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 20,
      "context" : "Interestingly, Mnih et al. (2016) also added an LSTM memory to the otherwise convolutional neural network architecture to give the agent a notion of memory, although this did not have significant impact on the performance on Atari games.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "For the convolutional neural network we use the same architecture as DQN (Mnih et al., 2015).",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 38,
      "context" : "It is also a generalisation to the memory and lookup schemes described in (Vinyals et al., 2016; Kaiser et al., 2016) for classification.",
      "startOffset" : 74,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "It is also a generalisation to the memory and lookup schemes described in (Vinyals et al., 2016; Kaiser et al., 2016) for classification.",
      "startOffset" : 74,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Note that a DND is a differentiable version of the memory module described in Blundell et al. (2016). It is also a generalisation to the memory and lookup schemes described in (Vinyals et al.",
      "startOffset" : 78,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "In Blundell et al. (2016), Monte Carlo returns were written to memory.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "As a problem domain we chose the Atari Learning Environment(ALE; Bellemare et al., 2013).",
      "startOffset" : 59,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "As a problem domain we chose the Atari Learning Environment(ALE; Bellemare et al., 2013). We tested our method on the 57 Atari games used by Schaul et al. (2015a), which form an interesting set of tasks as they contain diverse challenges such as sparse rewards and vastly different magnitudes of scores across games.",
      "startOffset" : 65,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "We consider 5 variants of A3C and DQN as baselines as well as MFEC (Blundell et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 21,
      "context" : "We compare to the basic implementations of A3C (Mnih et al., 2016) and DQN (Mnih et al.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 20,
      "context" : ", 2016) and DQN (Mnih et al., 2015).",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "We also compare to two algorithms incorporating λ returns (Sutton, 1988) aiming at more data efficiency by faster propagation of credit assignments, namely Q∗(λ) (Harutyunyan et al., 2016) and Retrace(λ) (Munos et al.",
      "startOffset" : 162,
      "endOffset" : 188
    }, {
      "referenceID" : 23,
      "context" : ", 2016) and Retrace(λ) (Munos et al., 2016).",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 25,
      "context" : "We did not directly compare to DRQN (Hausknecht & Stone, 2015) nor FRMQN (Oh et al., 2016) as results were not available",
      "startOffset" : 73,
      "endOffset" : 90
    }, {
      "referenceID" : 20,
      "context" : "We apply the same preprocessing steps as (Mnih et al., 2015), including repeating each action four times.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "We picked the hyperparameter values that performed best on the median for this subset of games (a common cross validation procedure described by Bellemare et al. (2013), and adhered to by Mnih et al.",
      "startOffset" : 145,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "We picked the hyperparameter values that performed best on the median for this subset of games (a common cross validation procedure described by Bellemare et al. (2013), and adhered to by Mnih et al. (2015)).",
      "startOffset" : 145,
      "endOffset" : 207
    }, {
      "referenceID" : 31,
      "context" : ", 2016), memory networks (Sukhbaatar et al., 2015; Miller et al., 2016)).",
      "startOffset" : 25,
      "endOffset" : 71
    }, {
      "referenceID" : 19,
      "context" : ", 2016), memory networks (Sukhbaatar et al., 2015; Miller et al., 2016)).",
      "startOffset" : 25,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : ", 2016), memory networks (Oh et al., 2016).",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 39,
      "context" : "RNNs can learn to quickly write highly rewarding states into memory and may even be able to learn entire reinforcement learning algorithms (Wang et al., 2016; Duan et al., 2016).",
      "startOffset" : 139,
      "endOffset" : 177
    }, {
      "referenceID" : 5,
      "context" : "RNNs can learn to quickly write highly rewarding states into memory and may even be able to learn entire reinforcement learning algorithms (Wang et al., 2016; Duan et al., 2016).",
      "startOffset" : 139,
      "endOffset" : 177
    }, {
      "referenceID" : 24,
      "context" : "The work of Oh et al. (2016) is also reminiscent of the ideas presented here.",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 28,
      "context" : "They also propose the use of the latent variable obtained from a variational autoencoder (Rezende et al., 2014) as an embedding space, but showed random projections often obtained better results.",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 14,
      "context" : "Kumaran et al. (2016) suggest that training on replayed experiences from the replay buffer in DQN is similar to the replay of experiences from episodic memory during sleep in animals.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 14,
      "context" : "Kumaran et al. (2016) suggest that training on replayed experiences from the replay buffer in DQN is similar to the replay of experiences from episodic memory during sleep in animals. DQN’s replay buffer differs from most other work on memory for deep reinforcement learning in its sheer scale: it is common for DQN’s replay buffer to hold millions of (s, a, r, s′) tuples. The use of local regression techniques for Q-function approximation has been suggested before: Santamarı́a et al. (1997) proposed the use of k-nearest-neighbours regression with a heuristic for adding memories based on the distance to previous memories.",
      "startOffset" : 0,
      "endOffset" : 495
    }, {
      "referenceID" : 14,
      "context" : "Kumaran et al. (2016) suggest that training on replayed experiences from the replay buffer in DQN is similar to the replay of experiences from episodic memory during sleep in animals. DQN’s replay buffer differs from most other work on memory for deep reinforcement learning in its sheer scale: it is common for DQN’s replay buffer to hold millions of (s, a, r, s′) tuples. The use of local regression techniques for Q-function approximation has been suggested before: Santamarı́a et al. (1997) proposed the use of k-nearest-neighbours regression with a heuristic for adding memories based on the distance to previous memories. Munos & Moore (1998) proposed barycentric interpolators to model the value function and proved their convergence to the optimal value function under mild conditions, but no empirical results were presented.",
      "startOffset" : 0,
      "endOffset" : 649
    }, {
      "referenceID" : 14,
      "context" : "Kumaran et al. (2016) suggest that training on replayed experiences from the replay buffer in DQN is similar to the replay of experiences from episodic memory during sleep in animals. DQN’s replay buffer differs from most other work on memory for deep reinforcement learning in its sheer scale: it is common for DQN’s replay buffer to hold millions of (s, a, r, s′) tuples. The use of local regression techniques for Q-function approximation has been suggested before: Santamarı́a et al. (1997) proposed the use of k-nearest-neighbours regression with a heuristic for adding memories based on the distance to previous memories. Munos & Moore (1998) proposed barycentric interpolators to model the value function and proved their convergence to the optimal value function under mild conditions, but no empirical results were presented. Gabel & Riedmiller (2005) also suggested the use of local regression, under the paradigm of case-based-reasoning that included heuristics for the deletion of stored cases.",
      "startOffset" : 0,
      "endOffset" : 861
    }, {
      "referenceID" : 23,
      "context" : ", 2015b) and Retrace(λ) (Munos et al., 2016).",
      "startOffset" : 24,
      "endOffset" : 44
    } ],
    "year" : 2017,
    "abstractText" : "Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.",
    "creator" : "LaTeX with hyperref package"
  }
}