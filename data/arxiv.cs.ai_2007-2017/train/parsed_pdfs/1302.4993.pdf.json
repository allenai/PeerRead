{
  "name" : "1302.4993.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Nevin Lianwen Zhang", "Hong Kong" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Introduction\nThe CPSC network is a multilevel, multivalued medi cal Bayesian network (BN). It was created by Pradhan et a/ (1994) based on the Computer-based Patient Case Simulation system (CPSC-PM) developed by Parker and Miller.\nThe CPSC network is one of the largest BNs in use at the present time. To the best of my knowledge, none of the existing implementations of BN are able to make inference with the network.\nThe CPSC network contains abundant causal indepen dencies. This makes it a good test case for the infer ence algorithm proposed by Zhang and Poole (1994b), the theme of which is to exploit causal independen cies for efficiency gains. Experiments have been per formed. It is found that the algorithm is able to an swer 420 of the 422 possible zero-observation queries, 94 of 100 randomly generated five-observation queries, 87 of 100 randomly generated ten-observation queries, and 69 of 100 randomly generated twenty-observation queries. Here, a five-observation query means a query about the posterior probability of one variable given five observations.\nIn addition to the reporting of experiment results (Sec tion 5), this paper also gives a somewhat new presen tation of the algorithm to help the reader in gaining a good understanding of the key issue and of the essence\nof the algorithm. Terminological and technical modi fications are also introduced.\nIt is well known that conditional independencies lead to the factorization of a joint probability into the mul tiplication of a list of conditional probabilities. The concept of causal independence we use (Section 2) al lvws one to further factorize each of those conditional probabilities into a combination of \"even-smaller\" fac tors, resulting in a finer-grain factorization of the joint probability (S· ction 3). The key issue is that factors in this finer-grain factorization usually cannot be com bined in arbitrary order (Section 4.2). This difficulty is overcome through a general combination operator (section 4.3), the concept of deputation (Section 4.4), and a constraint on the elimination ordering (Section 4. 5).\n2 Causal independence\nCausal independence refers to the situation where sev eral causes (or variables) c1, c2, .. . , em contribute independently to an effect (or variable) e. The con tribution ei by Ci probabilistically depends on Ci itself and is independent of all other causes given Ci. The total contribution that e receives is an combination e = el * 6 * . .. * em of the individual contributions, where * is a certain associative and commutative bi nary operator. When it is the case, we call the variable e a convergent variable since it is where independent contributions from different sources are collected and combined. The operator * is called the base combina tion operator of e. The noisy-OR gate (Pearl 1988) is an example of causal independence where all the variables are bi nary and the logic OR operator \"V\" is used to com bine individual contributions. Pra.dhan et a/ (1994) introduce a. generalization of the noisy-OR gate model called the noisy-MAX gate and use it extensively in the CPSC network. The noisy-MAX gate is another example of causal independence where the possible val ues of e are ordered and the \"MAX\" operator is used to combine individual contributions. Other examples of causal independence include noisy-AND gates and\nInference with Causal Independence in the CPSC Network 583\nnoisy-adders.\nIn a causal independence model, the conditional prob ability P(elc1, c2, ... , em) can be obtained from the conditional probabilities P(ei lci)· To see this, let us first define a function /i(e, Ci) for each i as follows: /i(e=o:, ci=/3) =def P(ei=o:lci=/3) for any value a: of e and any value f3 of Ci. We also need to define an operator to combine the /; 's. Let f(e, A, B) and g(e, A, C) be two functions, where A, B, and .Care three lists of variables and B and C do not intersect. The combination /®.g of f and g is defined as follows: for any particular value a: of e,\n/®.g(e=o:, A, B, C)\n=def l: f(e=o:t.A,B) g(e=o:2,A,C). (1)\nWe shall refer ®. as the functional combination oper ator of e. It is important to notice that * combines values of e, while®. combines functions of e. One can easily verify that the functional combination operator ®. is also commutative and associative.\nIt can be shown that the conditional probability P( e lc1, c2, ... , em) of the convergent variable e can be expressed as the combination of the /;'s, i.e.\nP(elc1, . . . , Cm)=ft(e, cl) ® • . . . ® .. fm(e, Cm)· (2)\nThe right hand of the equation makes sense because®. is commutative and associative. Again, the base com bination operator determines how contributions from indifferent sources are combined, while the functional combination operator is the reflection of the base op erator in terms of conditional probability.\nFor example, the conditional probability P(elct. c2) of the convergent variable e in a noisy-OR gate with causes c1 and c2 is given by\nl: P(e1=o:llcl)P(6=o:2lc2), a1Va2=a\nwhere ei is the contribution by Ci and Q can be either 0 or 1. Hence,\nP(elc1,c2)=ft(e,c1)®v h(e,c2)·\nIt is interesting to notice the similarity between equa tion (2) and the following property of conditional in dependence: if a variable x is independent of another variable z given a third variable y, then there exist non-negative functions f(x, y) and g(y, z) such that\nP(x, y, z)=f(x, y)g(y, z). (3)\nIn equation ( 3) conditional independence allows us to factorize a joint probability into factors that involve less variables, while in equation (2) causal indepen dence allows us to factorize a conditional probability\ninto factors that involve less variables. The only dif ference lies in the way the factors are combined.\nConditional independence has been used to reduce in ference complexity in Bayesian networks (Pearl 1988, Lauritzen and Spiegehalter 1988 and Jensen et a/ 1990). The rest of this paper investigates how to use causal independence for the same purpose.\n3 Heterogeneous factorization of joint probabilities\nThis section discusses factorization of joint probabili ties and introduces the concept of heterogeneous fac torization (HF). A fundamental assumption under the theory of proba bilistic reasoning is that a joint probability is adequate for capturing experts' knowledge and beliefs relevant to a reasoning task. Factorization and Bayesian net works come into play because joint probability is dif ficult, if not impossible, to directly assess, store, and reason with.\nLet P(xt. x2, ... , Xn) be a joint probability over variables x1, x2, ... , x,.. By the chain rule of probabilities, we have\nP(xt. X2, ... , x,.) =P(x1)P(x2lx1) ... P(xnlxl, ... , Xn-1)· (4)\nFor any i, there might be a subset 11'i � {x1, ... , Xi-tl such that Xi is conditionally independent of all the other variables in {xt. ... , Xi-d given variables in 11'i, i.e. P(x;lx1, ... , Xi-l)=P(x;l11';). Equation (4) can hence be rewritten as\nn P(xt. x2, ... , xn)= IT P(x;l11'i)·\ni=l (5)\nEquation (5) factorizes the joint probabil ity P(x1, x2, ... , Xn) into a multiplication of factors P{x;l11';). While the joint probability involves all the n variables, each of the factors might involve only a small number of variables. This fact implies savings in assessing, storing, and reasoning with probabilities.\nA Bayesian network is constructed from the factoriza tion as follows: build a directed graph with nodes x1, x2, ... , Xn such that there is an arc from Xj to x; if and only if Xj E 11';, and associate the conditional probability P(x;l11'i) with the node x;. P(xl!···,xn) is said to be the joint probability of the Bayesian net work. Also nodes in 11'; are called parents of x;. The term node will be use interchangeably with the term variable in the rest of the paper.\nThe factorization given by equation (5) is homogeneous in the sense that all the factors are combined in the same way, i.e. by multiplication.\nLet Xil, . . • , Xim; be the parents of Xi. If Xi is a convergent variable, then the conditional probability\n584 Zhang\nwhere ®i is the functional combination operator of x,. The fact that ®i might be other than multiplication leads to the concept of heterogeneous factorization. The word heterogeneous reflects the fact that differ ent factors might be combined in different manners.\nAs an example, consider the Bayesian network in Fig ure 1. The network states that P(a, b, c, et, e2, ea, y) can be factorized into a multiplication of P(a), P(b), P(c), P(e1!a,b), P(e2!a,b,c), P(eale1,e2), and P(ylea).\nIf the e, 's are convergent variables, then the condi tional probabilities of the e, 's can be further factorized as follows:\nP(e1!a,b) = P(e2la, b, c) = P(eale1,e2) /u(el, a)®d12(e1, b) /21(e2,a)®2/22(e2,b)®2/23(e2,c) /al(ea,el)®a/a2(ea,el)\nwhere the factor /11(e1,a), for instance, captures the contribution of a to e1, and where the ®i is the func tional combination operator of the e,.\nThe factorization of P(a, b, c, et, e2, ea, y) into the factors: P(a), P(b), P(c), P(ylea), /u(e1,a), !t2(e1,b), h1(e2,a), !22(e2,b), ha(e2,c), h1(ea,e1), and h2(ea, e2) is called a heterogeneous factorization (HF) . We shall call the fa; 's heterogeneous factors since they might be combined with other factors by operators other than multiplication. In contrast, we shall say that the factors P(a), P(b), P(c), and P(ylea) are homogeneous. Since the heterogeneous factoriza tion can be read from the BN in Figure 1, we say that the BN denotes the factorization.\n4 Exploiting causal independencies in inference\nThe question is how to make use of causal independen cies in inference. This section reviews the approach\nproposed by Zhang and Poole (1994b). Minor techni cal modifications are introduced.\nLet us consider queries of the form P(X, Y =Yo), where X is a list of interesting variables, Y is a list of ob served variables, and Yo is the corresponding list of ob served values. It suffices to only consider such queries because the posterior probability P(XIY =Yo) can be readily obtained from P(X, Y =Yo) and P(Y =Yo).\n4.1 Irrelevance\nThere might be variables in a BN that are irrelevant to a query (Geiger et al1988, Lauritzen et al1990). The paper assumes that all the irrelevant variables have been pruned.\nA factor can be represented as a multidimensional ar ray. Portions of the array that represents a factor might also be irrelevant to a query. In a BN, a reg ular variable is one that is not a convergent variable. If a factor f(y, Z) involves regular variable y and y is observed to be y0, then the values in the cells where y-:f;yo are irrelevant.\nNote that the same cannot be done if y is a conver gent variable. In this case, f(y, Z) is a heterogeneous factor. There might exist other heterogeneous factors that contain y. When combining those factors, we might need values of f(y, Z) for cases where y-:f;yo (see equation ( 1)).\nWe assume irrelevant portions of all the factor arrays have been pruned and treat the observed regular vari ables as special variables with only one possible value. Pruning irrelevant portions of factor arrays is espe cially important when there is a large number of ob servations.\n4.2 A difference between homogeneous and heterogeneous factorizations\nOne way to compute P(X, Y =Yo) is to sum out the variables outside X one by one. With a homogeneous factorization, summing out one variable z is easy. One can simply remove all the factors that involve z from the list of factors; combine them by multiplication; sum out z from the combination; and put the result ing factor onto the list of factors (Zhang and Poole 1994a). This is essentially what takes place in the well known clique tree propagation algorithm (Lauritzen and Spiegehalter 1988 and Jensen et al 1990). The correctness of doing so is guaranteed by the fact that factors in a homogeneous factorization can be com bined in arbitrary order.\nUnlike in a homogeneous factorization, factors in a heterogeneous factorization in general cannot be com bined in arbitrary order. In our example, summing out variable a requires combining /u ( e1, a) and !21 ( e2, a). But by definition j11 needs to be combined with !t2 before being combined with any other factors, includ ing f21• Thus, we need more flexibility on the order by\nInference with Causal Independence in the CPSC Network 585\nwhich heterogeneous factors can be combined. This is the key issue one needs to address in order to make use of causal independence in inference.\nZhang and Poole (1994b) achieve such flexibility through a general combination operator, the concept of deputation, and a constraint on the order by which variables are summed out.\n4.3 A general combination operator\nSuppose e1, . . . , e�c are conv,ergent variables with base combination operator *t. . . . , *A:· Let /(el, . .. , e�c, A, B) and g(e1, . .. , e�c, A, C) be two func tions, where the A is a list of regular variables and B and C do not intersect (they can contain convergent as well as regular variables). Then, the combination f®g of f and g is defined as follows: for any particular value Oi of ei,\n/®g(e1=o1, ... , e�c=o�c, A,B, C)\n=dej I: I: !( e1 =ou, .. . , e�c=ou, A, B)\n(7)\ng(e1=o12, ... , e�c=o�c2, A, C). (8)\nA few notes are in order. First, fixing a list of conver gent variables and their base combination operators, one can use the operator ® to combined two arbitrary functions. Second, since the base combination opera tors are commutative and associative, the operator ® is also commutative and associative.\nIn the following, we shall work with a given BN, which has a fixed list of convergent variables. Consequently, we can use ® to combine any two heterogeneous factors and the heterogeneous factors can be combined in any order. We shall refer to ® as the general combination operator.\nThird, when k = 1 equation (7) reduces to equation (1). Finally when k = 0, f®g is simply the multipli cation of f and g.\n4.4 Deputation\nLet e be a convergent node in a BN. To depute e is to make a copy e' of e, make the children of e to be chil dren of e', make e' a child of e, and set the conditional probability P(e'le) to be as follows:\nP( 'I ) { 1 if e = e' e e = 0 otherwise (9) We shall call e' the deputy of e. We shall also call P(ele') the deputing function and sometimes write it as I(e, e') since P(ele') ensures that e and e' be the same.\nThe BN in Figure 1 becomes the one in Figure 2 af ter the deputation of all the convergent variables. It is called the the deputation of the BN in Figure 1.\nFigure 2: The BN in Figure 1 after the deputation of convergent variables.\nThe heterogeneous factorization denoted by the depu tation BN consists of heterogeneous factors: /u ( e1, a), /12(e1, b), h1(e2, a), h2(e2, b), h3(e2, c), fa1(e3, eD, and /32(e3,e�); and homogeneous factors: P(a), P(b), P(c), P(yle�), I(e1,eD, I(e2,e�), and I(e3,e�). Note that deputy variables are regular variables by defini tion and deputing functions are a homogeneous factor by definition.\nAlso note that in the deputation BN, the combination of all the heterogeneous factors is the same as the mul tiplication of the conditional probabilities of all the convergent variables. The same is not true without deputation. One implication is that in a deputation BN, the joint probability equals to the combination of all the heterogeneous factors times the multiplication of all the homogeneous factors.\n4.5 A constraint on elimination ordering\nThe first two steps in summing out a variable z from a deputation BN can be: (1) remove from the list of heterogeneous factors all the factors that involve z, combine them by the general combination operator re sulting in, say,/; and (2) remove from the list of homo geneous factor all the factors that involve z, combine them by multiplication resulting in, say, g. The next step would be to combine f and g by multiplication. To guarantee the correctness of doing so, deputy vari ables must be summed out after their corresponding convergent variable (Zhang and Poole 1994b ).\nAn ordering by which variables outside X is summed out is usually referred to as an elimination ordering. A legitimate elimination ordering is one where convergent variables always appear before their deputies.\nThe legitimacy constraint on elimination ordering can be enforced in two steps1• First, replace the convergent\n1This improvement over Zhang and Poole (1994b) is suggested by Wei Xiong.\n586 Zhang\nvariables in X with their deputies, resulting in a new list of variables X'. It is evident that P( X', Y =Yo) is the same as P(X, Y=Yo). Second, find a legitimate elimination ordering of vari ables outside X'. Such an ordering can be found by using, with minor adaptations, the maximum cardinal ity search heuristic (Tarjan and Yannakakis 1984) or the minimum deficiency heuristic (Bertele and Brioschi 1972).\nNote that the first step is necessary, because other wise we will not be able to sum out the deputies of the convergent variables in X due to the legitimacy constraint.\nAlso note that an legitimate elimination ordering con tains all the variables in Y. Remember that irrelevant parts of factor arrays have been pruned and a regular variable yEY is treated as a dummy variable with only one possible value Yo· However, a convergent variable in Y still have more than one possible values.\n4.6 An algorithm\nGiven a legitimate elimination ordering p and the het erogeneous factorization of the deputation BN under discussion, P(X', Y=Yo) can be computed by using the ICI (Inference with Causal independence) algo rithm given in the following.\nProcedure ICI 1. While p is not empty,\n• Remove the first variable z from p. • Remove from the list of heteroge\nneous factors all the factors It , ... , fk that involve z, and set\n!= ®�=1 /i· Let B be the set of all the variables that appear in/.\n• Remove from the list of homogeneous factors all the factors g1, . .. , Ym that involve z, and set\nm\ng= ITuj· j=l\nLet C be the set of all the variables that appear in g.\n• H k=O, define a function h by\nh(C-{z})= { g(C)Iy=�o if z=y�Y Ez g �C) otherw1se Put h onto the homogeneous factor list,\n• Else if m=O, define a function h by\nif z=yEY otherwise\nPut h onto the heterogeneous factor list,\n• Else define a function h by\nh(BuC-{z})- { /(B)g(C)I!I=!Io - Ez f(B)g(C) ifz=yEY otherwise\nPut h onto the heterogeneous factor list. Endwhile\n2. Combine all the heterogeneous factors by® resulting in, say, f.\n3. Combine all the homogeneous factors by multiplication resulting in, say, g. 4. Multiply f and g and return the result ing factor.\nNote that in the ICI algorithm, summing out a variable requires combining only the factors that involve the variable. This is why ICI lead to efficiency gains wheh causal independencies are present. More specifically, if causal independencies were ignored, summing out one variable would require combining all the conditional probabilities that involve the variable. With ICI, we combine all the factors that involve the variable. There are efficiency gains because the factors might contain less variables than the conditional probabilities.\nIn Figure 1, for instance, summing out variable a would require combining P(e1!a, b) and P(e2la, b, c) when causal independencies were ignored. Five vari ables participate in the process. By using ICI, on the other hand, we need to combine only /u(e1, a) and /21(e2,a). There are only three variables involved in the process.\n5 Experiments\nExperiments have been performed on the CPSC net work (the version released in August 1994) to answer the following two questions: How much efficiency gains one can expect by making use of causal independen cies? How effective the ICI algorithm is in answering queries posed to the CPSC network?\n5.1 Efficiency gains due to causal independence\nTo answer the first question, we consider the task of computing the marginal probability for each variable in the CPSC network, and compare the computational costs incurred by the ICI algorithm and those incurred by clique tree propagation.\nThe size of a factor is defined to be the multiplication of the numbers of possible values of all the variables in the factor. A factor containing three binary variables, for instance, has a size of 8.\nWhen computing the marginal probability of a vari able, new factors are created. The maximum factor size is said to the cost of the computing the marginal probability of the variable, or simply the cost of the variable. If the inference algorithm used is ICI, we call\nInference with Causal Independence in the CPSC Network 587\nit the I CI cost of the variable. On the other hand, if the inference algorithm used is clique tree propagation we call it the CTP cost of the variable. Table 1 shows the distribution of variables according to their ICI costs. The \"cost\" -columns show ICI costs, while the \"CNV\" columns show the numbers of vari ables with ICI costs no larger than the ICI costs in the same rows. CNV is a shorthand for Cumulative Num . ber of Variables. Table 2 shows the distribution of variables according to their CTP costs. Those statis tics were computed from elimination orderings gener ated by the maximum cardinality search heuristic.\nWe see that the CTP costs of the variables are much larger than their ICI costs. For example, there are 194 variables with ICI costs in the range (8, 512], while there are only 87 variables whose CTP costs are in the range (8, 576). There are 293 variables with ICI costs in the range (8, 786432], while there are only 144 variables whose CTP costs are in the same range. The number of variables with ICI costs no larger than 8 is roughly the same as the number of variables with CTP costs in the same range. Those variables are trivial in the sense that the portions of the CPSC network relevant to them are trees.\nTable 1: Variable distribution by ICI costs\nI cost I CNV II cost I CNV II cost I CNV I\n8 124 1920 384 98304 412 96 255 2048 385 196608 414 192 285 3072 388 786432 417 384 301 6144 392 1179648 418 512 318 12288 397 3145728 420 768 330 36864 400 12582912 422\nTable 2: Variable distribution by CTP costs\nI cost I CNV II cost I CNV II cost I CNV I\n8 123 1024 211 6.7xl07 407 24 156 49152 227 108 410 64 187 786432 270 2.6x108 412 128 200 1179648 370 109 418 256 202 9437184 387 1.6x109 422 576 210 12582912 390\nThere are also 31 variables whose CTP costs are equal to or larger than the maximum ICI cost 12582912. Ex periments have shown that a factor size of 12582812 is too large to handle (for SPARCclassic with 16MG memory). Thus with CTP, one would not be able to compute the marginal probabilities for those 31 vari ables. On the other hand, with ICI we have been able to compute the marginal probabilities for all the vari ables but 2.\n5.2 Effectiveness of the ICI algorithm\nTo determine the effectiveness of the ICI algorithm, we first attempted to compute the marginal probabil ity for each variable in the CPSC network. We were able to compute the marginal probabilities for all the\nvariables except for 2. Table 3 shows the distribu tion of variables according the time it took to compute their marginal probabilities. The \"time\" -columns dis play the CPU time consumption in seconds, and the \"CNV\" columns show the number of variables whose marginal probabilities were computed in a time less than or equal to the corresponding CPU time. Those statistics were collected on a SPARCclassic worksta tion, which has a clock rate of 50mhz .\nWe see the for 396 out of the 422 variables, marginal probabilities can be computed in less than 1 second CPU time. The marginal probability of the vari able abdominal-pain-excerbated-by-meals, whose ICI cost being 3145728, took 23 second to compute.\nThe variables vomiting and vomiting-vomitus-normal-gastric-contents have ICI cost 12582912. The computer ran out of memory while computing the marginal probabilities of those two variables.\nTo predict the performance of the ICI algorithm on real life queries, we computed the ICI costs of 100 ran domly generated five-observation queries, of 100 ran domly generated ten-observation queries, and of 100 randomly generated twenty-observation queries. The distributions of the queries according to their ICI costs are displayed in Tables 4, 5, and 6. The ICI cost of a query is defined in the same way as the ICI cost of a variable. Those statistics were computed from elimi nation orderings generated by the minimum deficiency heuristic, which is found to be slightly better than the maximum cardinality heuristic in our case.\n588 Zhang\nTable 6: Distribution of twenty-observation queries by their ICI costs Acknowledgement\ncost I CNV II cost I CNV II cost I CNV 1 I thank Maclcolm Pradhan and Gregory Provan for sharing the CPSC network with me. The paper has benefited from a course project by Jin Gu and Wei Xiong. The term convergent variable was suggested by Glenn Shafer. Research was supported by HKUST grant DAG94/95.EG14. 1536 2 294912 33 3145728 69 6144 6 524288 49 12582912 80 12288 8 786432 57 4x108 97 98304 24 1048576 58 109 98\n196608 30 1572864 64 1.6x109 100\nSince we were able to compute the marginal proba bility of the variable abdominal-pain-excerbated by-meals in 23 CPU seconds and the ICI cost of the variable is 3145728, we predict that the ICI algorithm is able to answer 94% of the five-observation queries, 87% of the ten-observation queries, and 69% of the twenty-observation queries.\nFinally, our purpose in the experiments has been to demonstrate the benefits of making use of causal in dependence. As a consequence, other ideas such as zero compression has not been incorporated in the im plementation. Program tracing revealed that in the arrays representing large factors, the majority of the array cells are zero. Thus the performance statistics can be much improved with zero compression.\n6 Related work\nThe concept of causal independence given in this paper is a special case of the more general definition given by Beckerman (1993) and Beckerman and Breese (1994). It is also a special case of the generalized noisy-OR model proposed by Srinivas (1993).\nKim and Pearl (1983) proposed an approach for mak ing use of causal independence in BNs which are polytrees based on a message-passing algorithm by Pearl (1988). D'Ambrosio (1994) proposed another approach for two level BNs with binary variables based on his earlier work on symbolic probabilistic inference. This paper has been concerned with general BNs.\nBeckerman (1993) uses causal independence to al ter the topologies of BNs in order to gain inference speedups. With the ICI algorithm, summing out one variable requires combining only those factors that contain the variable. The same is not true for Hecker man's approach.\n7 Conclusion\nThis paper has described the ICI algorithm for BN inference. The algorithm exploits causal independen cies to gain computational efficiency. Experiments on the CPSC network show that it is able to answer 420 of the 422 possible zero-observation queries, 94 of 100 randomly generated five-observation queries, 87 of 100 randomly generated ten-observation queries, and 69 of 100 randomly generated twenty-observation queries.\nReferences\n[1] U. Bertele and F. Brioschi (1972), Nonserial dy namic programming, Mathematics in Science and Engineering, Vol. 91, Academic Press.\n[2] B. D'Ambrosio {1994), Symbolic probabilistic in ference in large BN20 networks, in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, pp. 128-135.\n[3] D. Geiger, T. Verma, and J. Pearl {1990), d separation: From theorems to algorithms, in Un certainty in Artificial Intelligence 5, pp. 139-148.\n[4] D. Beckerman (1993), Causal independence for knowledge acquisition and inference, in Proceed ings of the Ninth Conference on Uncertainty in Artificial Intelligence, pp. 122-127.\n[5] D. Beckerman and J. Breese (1994), A new look at causal independence, in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelli gence, pp. 286-292.\n[6] F. V. Jensen, K. G. Olesen, and K. Anderson (1990), An algebra of Bayesian belief universes for knowledge-based systems, Networks, 20, pp. 637 - 659.\n[7] J. Kim and J. Pearl (1983), A computational model for causal and diagnostic reasoning in infer ence engines, in Proceedings of the Eighth Interna tional Joint Conference on Artificial Intelligence, Karlsruhe, Germany, pp. 190-193.\n[8] S. L. Lauritzen and D. J. Spiegehalter (1988), Lo cal computations with probabilities on graphical structures and their applications to expert sys tems, Journal of Royal Statistical Society B, 50: 2, pp. 157 - 224.\n[9] S. L. Lauritzen, A. P. Dawid, B. N. Larsen, and H. G. Leimer (1990), Independence Properties of Directed Markov Fields, Networks, 20, pp. 491- 506.\n[10] J. Pearl {1988), Probabilistic Reasoning in Intel ligence Systems: Networks of Plausible Inference, Morgan Kaufmann Publishers, Los Altos, CA.\n[11] M. Pradhan, G. Provan, B. Middleton, and M. Henrion (1994), Knowledge engineering for large belief networks, in Proceedings of the Tenth Con ference on Uncertainty in Artificial Intelligence, pp. 484-490.\n[12] S. Srinivas (1993), A generalization of the Noisy Or model, in Proceedings of the Ninth Conference\nInference with Causal Independence in the CPSC Network 589\non Uncertainty in Artificial Intelligence, pp. 208- 215.\n[13] R. E. Tarjan and M. Yannakakis (1984), Simple linear time algorithm to test chordality of graphs, test acyclicity of hypergraphs, and selectively re duce acyclic hypergraphs, SIAM J. Comput., 13, pp. 566-579.\n[14] N. L. Zhang and D. Poole (1994a), A simple ap proach to Bayesian network computations, in Pro ceedings of the Tenth Canadian Conference on Artificial Intelligence, pp. 171-178.\n[15] N. L. Zhang and D. Poole (1994b), Intercausal independence and heterogeneous factorization, in Proceedings of the Tenth Conference on Uncer tainty in Artificial Intelligence, pp. 606-614."
    } ],
    "references" : [ {
      "title" : "Nonserial dy­ namic programming",
      "author" : [ "U. Bertele", "F. Brioschi" ],
      "venue" : "Mathematics in Science and Engineering,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1972
    }, {
      "title" : "Symbolic probabilistic in­ ference in large BN20",
      "author" : [ "B. D'Ambrosio" ],
      "venue" : "Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1994
    }, {
      "title" : "Causal independence for knowledge acquisition and inference, in Proceed­",
      "author" : [ "D. Beckerman" ],
      "venue" : "ings of the Ninth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1993
    }, {
      "title" : "A new look at causal independence",
      "author" : [ "D. Beckerman", "J. Breese" ],
      "venue" : "Proceedings of the Tenth Conference on Uncertainty in Artificial Intelli­ gence,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1994
    }, {
      "title" : "and K",
      "author" : [ "F.V. Jensen", "K.G. Olesen" ],
      "venue" : "Anderson ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "A computational model for causal and diagnostic reasoning in infer­ ence engines",
      "author" : [ "J. Kim", "J. Pearl" ],
      "venue" : "Proceedings of the Eighth Interna­ tional Joint Conference on Artificial Intelligence, Karlsruhe,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1983
    }, {
      "title" : "Lo­ cal computations with probabilities on graphical structures and their applications to expert sys­ tems",
      "author" : [ "D.J.S.L. Lauritzen" ],
      "venue" : "Spiegehalter",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1988
    }, {
      "title" : "and H",
      "author" : [ "S.L. Lauritzen", "A.P. Dawid", "B.N. Larsen" ],
      "venue" : "G. Leimer ",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Probabilistic Reasoning in Intel­ ligence Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1988
    }, {
      "title" : "and M",
      "author" : [ "M. Pradhan", "G. Provan", "B. Middleton" ],
      "venue" : "Henrion ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "A generalization of the Noisy­ Or model, in Proceedings of the Ninth Conference  Inference with Causal Independence in the CPSC",
      "author" : [ "S. Srinivas" ],
      "venue" : "Network 589 on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1993
    }, {
      "title" : "Simple linear time algorithm to test chordality of graphs, test acyclicity of hypergraphs, and selectively re­ duce acyclic hypergraphs",
      "author" : [ "R.E. Tarjan", "M. Yannakakis" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1984
    }, {
      "title" : "A simple ap­ proach to Bayesian network computations",
      "author" : [ "N.L. Zhang", "D. Poole" ],
      "venue" : "in Pro­ ceedings of the Tenth Canadian Conference on Artificial Intelligence,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1994
    }, {
      "title" : "Intercausal independence and heterogeneous factorization",
      "author" : [ "N.L. Zhang", "D. Poole" ],
      "venue" : "Proceedings of the Tenth Conference on Uncer­ tainty in Artificial Intelligence,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1994
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "This paper reports experiments with the causal independence inference algorithm pro­ posed by Zhang and Poole (1994b) on the CPSC network created by Pradhan et a/ (1994). It is found that the algorithm is able to answer 420 of the 422 possible zero­ observation queries, 94 of 100 randomly gen­ erated five-observation queries, 87 of 100 randomly generated ten-observation queries, and 69 of 100 randomly generated twenty­ observation queries.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}