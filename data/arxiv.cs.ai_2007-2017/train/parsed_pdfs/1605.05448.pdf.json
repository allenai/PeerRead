{
  "name" : "1605.05448.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "The Bees Algorithm for the Vehicle Routing Problem",
    "authors" : [ "Aish Fenton" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "The Bees Algorithm for the\nVehicle Routing Problem\nAish Fenton\nDepartment of Computer Science University of Auckland\nAuckland, New Zealand\nar X\niv :1\n60 5.\n05 44\n8v 1\n[ cs\n.N E\n] 1\n8 M\nay 2\n01 6\nColophon: Typeset in LATEX using typefaces Computer Modern. Title-page image is courtesy of [1].\nPreface\nThis MSc. thesis has been prepared by Aish Fenton at the University of Auckland, Department of Computer Science. It has been supervised by Dr. Michael Dinneen. The work undertaken in this thesis has grown out of a research project sponsored by New Zealand Trade and Enterprise (NZTE) for the company vWorkApp Inc. to research vehicle route optimisation for use within their software product."
    }, {
      "heading" : "Acknowledgements",
      "text" : "I’d like to thank my partner in crime, Anna Jobsis, for her encouragement, cajoling, threatening, bribing, doing my dishes, proof reading, guilt tripping, grammar policing, comforting. . . doing whatever it took to help me get it done. Also, I’d like to thank my mum and dad for their encouragement and for always making me feel like higher education was within my reach.\nThanks to Steve Taylor and Steve Harding for holding down the fort at work while I disappeared to do this thesis. And likewise thanks to my team, Jono, Rash, Elena, Bob, Marcus, Yuri, and Robin for being on the ball (as always) despite my absence. Thanks to Brendon Petrich and vWorkApp Inc. for providing me with time off work and being supportive of me undertaking this.\nAnd lastly, I especially owe Dr. Michael Dinneen, my supervisor, a big thank you for persevering with me even though he must have doubted that I’d ever finish.\niii\nAbstract\nIn this thesis we present a new algorithm for the Vehicle Routing Problem called the Enhanced Bees Algorithm. It is adapted from a fairly recent algorithm, the Bees Algorithm, which was developed for continuous optimisation problems. We show that the results obtained by the Enhanced Bees Algorithm are competitive with the best meta-heuristics available for the Vehicle Routing Problem—it is able to achieve results that are within 0.5% of the optimal solution on a commonly used set of test instances. We show that the algorithm has good runtime performance, producing results within 2% of the optimal solution within 60 seconds, making it suitable for use within real world dispatch scenarios. Additionally, we provide a short history of well known results from the literature along with a detailed description of the foundational methods developed to solve the Vehicle Routing Problem.\nContents"
    }, {
      "heading" : "1 Introduction 5",
      "text" : "1.1 Content Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
    }, {
      "heading" : "2 Background 7",
      "text" : "2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.1.1 TSP Introduction and History . . . . . . . . . . . . . . . . . . 10\n2.2 Exact Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.3 Classic Heuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.3.1 Constructive Heuristics . . . . . . . . . . . . . . . . . . . . . . 13\n2.3.2 Two-phase Heuristics . . . . . . . . . . . . . . . . . . . . . . . 15\n2.3.3 Iterative Improvement Heuristics . . . . . . . . . . . . . . . . . 17\n2.4 Meta-heuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.4.1 Simulated Annealing . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.4.2 Genetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.4.3 Tabu Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n2.4.4 Large Neighbourhood Search . . . . . . . . . . . . . . . . . . . 25\n2.5 Swarm Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n1\n2.5.1 Ant Colony Optimisation . . . . . . . . . . . . . . . . . . . . . 28\n2.5.2 Bees Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 31"
    }, {
      "heading" : "3 Problem Definition 35",
      "text" : "3.1 Capacitated Vehicle Routing Problem . . . . . . . . . . . . . . . . . . 35\n3.2 Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n3.2.1 Multiple Depot Vehicle Routing Problem . . . . . . . . . . . . 37\n3.2.2 Vehicle Routing with Time Windows . . . . . . . . . . . . . . . 37\n3.2.3 Pickup and Delivery Problem . . . . . . . . . . . . . . . . . . . 38"
    }, {
      "heading" : "4 Algorithm 39",
      "text" : "4.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n4.2 Problem Representation . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n4.3 Enhanced Bees Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 41\n4.3.1 Bee Movement . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n4.3.2 Search Space Coverage . . . . . . . . . . . . . . . . . . . . . . . 44\n4.4 Search Neighbourhood . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.4.1 Destroy Heuristic . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.4.2 Repair Heuristic . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.4.3 Neighbourhood Extent . . . . . . . . . . . . . . . . . . . . . . . 46"
    }, {
      "heading" : "5 Results 49",
      "text" : "5.1 Enhanced Bees Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 49\n5.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3 5.2.1 Bees Algorithm versus Enhanced Bees Algorithm . . . . . . . . 52\n5.2.2 Large Neighbourhood Search . . . . . . . . . . . . . . . . . . . 53\n5.2.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n5.3 Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n6 Conclusion 63\n4\nChapter 1\nIntroduction\nIn this thesis we present a new algorithm to solve the Vehicle Routing Problem. The Vehicle Routing Problem describes the problem of assigning and ordering geographically distributed work to a pool of resources. The aim is minimise the travel cost required to complete the work, while meeting any specified constraints, such as a maximum shift duration. Often the context used is that of a fleet of vehicles delivering goods to their customers, although the problem can equally be applied across many different industries and scenarios, and has been applied to non-logistics applications, such as microchip layout.\nInterest in the Vehicle Routing Problem has increased over the last two decades as the cost of transporting and delivering goods has become a key factor of most developed economies. Even a decrease of a few percent on transportation costs can offer a savings of billions for an economy. In the context of New Zealand (our home country) virtually every product grown, made, or used is carried on a truck at least once during its lifetime [33]. The success of New Zealand’s export industries are inextricably linked to the reliability and cost effectiveness of our road transport. Moreover, a 1% growth in national output requires a 1.5% increase in transport services [33].\nThe Vehicle Routing Problem offers real benefits to transport and logistics companies. Optimisation of the planning and distribution process, such as modelled by the Vehicle Routing Problem, can offer savings of anywhere between 5% to 20% of the transportation costs [52]. Accordingly, the Vehicle Routing Problem has been the focus of intense research since its first formal introduction in the fifties. The Vehicle Routing Problem is one of the most studied of all combinatorial optimisation problems, with hundreds of papers covering it and the family of related problems since its introduction fifty years ago.\nThe challenge of the Vehicle Routing Problem is that it combines two (or more, in the case of some of its variants) combinatorially hard problems that are themselves\n5"
    }, {
      "heading" : "6 CHAPTER 1. INTRODUCTION",
      "text" : "known to be NP-hard. Its membership in the family of NP-hard problems makes it very unlikely that an algorithm exists, with reasonable runtime performance, that is able to solve the problem exactly. Therefore heuristic approaches must be developed to solve all but the smallest sized problems.\nMany methods have been suggested for solving the Vehicle Routing Problem. In this thesis we develop a new meta-heuristic algorithm that we call the Enhanced Bees Algorithm. We adapt this from another fairly recent algorithm, named the Bees Algorithm, that was developed for solving continuous optimisation problems.\nWe show that the results obtained by the Enhanced Bees Algorithm are competitive with the best modern meta-heuristics available for the Vehicle Routing Problem. Additionally, the algorithm has good runtime performance, producing results within 2% of the optimal solution within 60 seconds. This makes the Enhanced Bees Algorithm suitable for use within real world dispatch scenarios where often the dispatch process is dynamic and hence it is impractical for a dispatcher to wait minutes (or hours) for an optimal solution1."
    }, {
      "heading" : "1.1 Content Outline",
      "text" : "We start in Chapter 2 by providing a short history of the Vehicle Routing Problem, as well as providing background material necessary for understanding the Enhanced Bees Algorithm. In particular, we review the classic methods that have been brought to bear on the Vehicle Routing Problem along with the influential results achieved in the literature. In Chapter 3 we provide a formal definition of the Vehicle Routing Problem and briefly describe the variant problems that have been developed in the literature. In Chapter 4 we provide a detailed description of the Enhanced Bees Algorithm and its operation, along with a review of the objectives that the algorithm is designed to meet, and a description of how the algorithm internally represents the Vehicle Routing Problem. In Chapter 5 we provide a detailed breakdown of the results obtained by the Enhanced Bees Algorithm. The algorithm is tested against the well known set of test instances due to Christofides, Mingozzi and Toth [11] and is contrasted with some well known results from the literature. Finally, in Chapter 6 we provide a summary of the results achieved by the Enhanced Bees Algorithm in context of the other methods available for solving the Vehicle Routing Problem from the literature. Additionally, we offer our thoughts on future directions and areas that warrant further research.\n1The Enhanced Bees Algorithm was developed as part of a New Zealand Trade and Enterprise Research and Development grant for use within the dispatch software product, vWorkApp. Hence more consideration has been given to its runtime performance than is typically afforded in the literature.\nChapter 2\nBackground\nThis chapter provides a short history and background material on the Vehicle Routing Problem. In particular we review the solution methods that have been brought to bear on the Vehicle Routing Problem and some of the classic results reported in the literature.\nThis chapter is laid out as follows. We start in Section 2.1 by informally defining what the Vehicle Routing Problem is and by providing a timeline of the major milestones in its research. We also review a closely related problem, the Traveling Salesman Problem, which is a cornerstone of the Vehicle Routing Problem. We then review in Section 2.2 the Exact Methods that have been developed to solve the Vehicle Routing Problem. These are distinguished from the other methods we review in that they provide exact solutions, where the globally best answer is produced. We follow this in Section 2.3 by reviewing the classic Heuristics methods that have been developed for the Vehicle Routing Problem. These methods are not guaranteed to find the globally best answer, but rather aim to produce close to optimal solutions using algorithms with fast running times that are able to scale to large problem instances. In Section 2.4 we review Meta-heuristic methods that have been adapted for the Vehicle Routing Problem. These methods provide some of the most competitive results available for solving the Vehicle Routing Problem and are considered state-of-the-art currently. Lastly, in Section 2.5 we review a modern family of meta-heuristics called Swarm Intelligence that has been inspired by the problem solving abilities exhibited by some groups of animals and natural processes. These last methods have become a popular area of research recently and are starting to produce competitive results to many problems. This thesis uses a Swarm Intelligence method for solving the Vehicle Routing Problem.\n7"
    }, {
      "heading" : "8 CHAPTER 2. BACKGROUND",
      "text" : ""
    }, {
      "heading" : "2.1 Overview",
      "text" : "The Vehicle Routing Problem (commonly abbreviated to VRP) describes the problem of assigning and ordering work for a finite number of resources, such that the cost of undertaking that work is minimised. Often the context used is that of a fleet of vehicles delivering goods to a set of customers, although the problem can equally be applied across many different industries and scenarios (including non-logistics scenarios, such as microchip layout). The aim is to split the deliveries between the vehicles and to specify an order in which each vehicle undertakes its work, such that the distance travelled by the vehicles is minimised and any pre-stated constraints are met. In the classic version of the VRP the constraints that must be met are:\n1. Each vehicle must start and end its route at the depot.\n2. All goods must be delivered.\n3. The goods can only be dropped off a single time and by a single vehicle.\n4. Each good requires a specified amount of capacity. However, each vehicle has a finite amount of capacity that cannot be exceeded. This adds to the complexity of the problem as it necessarily influences the selection of deliveries assigned to each vehicle.\nMore formally, the VRP can be represented as a graph, (V,E). The vertices of the graph, V , represent all locations that can be visited; this includes each customer location and the location of the depot. For convenience let vd denote the vertex that represents the depot. We denote the set of customers as C = {1, 2, . . . , n}. Next let the set of edges, E, correspond to the valid connections between customers and connections to the depot—typically for the VRP all connections are possible. Each edge, (i, j) ∈ E, has a corresponding cost cij . This cost is typically the travel distance between the two locations."
    }, {
      "heading" : "2.1. OVERVIEW 9",
      "text" : "A solution to a given VRP instance can be represented as a family of routes, denoted by S. Each route itself is a sequence of customer visits that are performed by a single vehicle, denoted by R = [v1, v2, . . . , vk] such that vi ∈ V , and v1, vk = vd. Each customer has a demand di, i ∈ C, and q is the maximum demand permissible for any route (i.e. its maximum capacity). The cost of the solution, and the value we aim to minimise, is given by the following formula:\n∑ R∈S ∑ vi∈R cvi,vi+1\nWe can now formalise the VRP constraints as follows:\n⋃ R∈S = V (2.1) (Ri − vd) ∩Rj = ∅ ∀Ri, Rj ∈ S (2.2) vi = vj ∀Ri ∈ S,¬∃vi, vj ∈ (Ri − vd) (2.3) v0, vk ∈ Ri = vd ∀Ri ∈ S (2.4)∑ v∈Ri dv < q ∀Ri ∈ S (2.5)\nEquation (2.1) specifies that all customers are included in at least one route. Equations (2.2) and (2.3) ensure that each customer is only visited once, across all routes. Equation (2.4) ensures that each route starts and ends at the depot. Lastly, Equation (2.5) ensures that each route doesn’t exceed its capacity.\nThis version of the problem has come to be known as the Capacitated Vehicle Routing Problem (often appreciated to CVRP in the literature). See Chapter 3 for an alternative formation, which states the problem as an Integer Linear Programming problem, as is more standard in the VRP literature1.\nVRP was first formally introduced in 1959 by Dantzig and Ramser in their paper, the Truck Scheduling Problem [14]. The VRP has remained an important problem in logistics and transport, and is one of the most studied of all combinatorial optimisation problems. Hundreds of papers have been written on it over the intervening fifty years. From the large number of implementations in use today it is clear that the VRP has real benefits to offer transport and logistics companies. Anywhere from\n1We believe that the formation provided in this chapter is simpler and more precise for understanding the algorithmic methods described in this chapter. However, we do provide a more standard formation in Chapter 3."
    }, {
      "heading" : "10 CHAPTER 2. BACKGROUND",
      "text" : "5% to 20% savings have been reported where a vehicle routing procedure has been implemented [52].\nFrom the VRP comes a family of related problems. These problems model other constraints that are encountered in real world applications of the VRP. Classic problems include: VRP with Time Windows (VRPTW), which introduces a time window constraint against each customer that the vehicle must arrive within; VRP with Multiple Depots (MDVRP), where the vehicles are dispatched from multiple starting points; and the Pickup and Delivery Problem (PDP), where goods are both picked up and delivered during the course of the route (such as a courier would do)."
    }, {
      "heading" : "2.1.1 TSP Introduction and History",
      "text" : "The VRP is a combination of two problems that are combinatorial hard in themselves: the Traveling Salesman Problem (more precisely the Multiple Traveling Salesman Problem), and the Bin Packing Problem.\nThe Traveling Salesman Problem (TSP) can informally be defined as follows. Given n points on a map, provide a route through each of the n points such that each point is only used once and the total distance travelled is minimised. The problem’s name, the Traveling Salesman, comes from the classic real world example of the problem. A salesman is sent on a trip to visit n cities. They must select the order in which to visit the cities, such that they travel the least amount of distance.\nAlthough the problem sounds like it might be easily solvable, it is in fact NP-hard. The best known exact algorithms for solving the TSP still require a running time of O(2n). Karp’s famous paper, Reducibility Among Combinatorial Problems [26], in 1972 showed that the Hamiltonian Circuit problem is NP-complete. This implied the NP-hardness of TSP, and thus supplied the mathematical explanation for the apparent difficulty of finding optimal traveling salesman tours.\nTSP has a history reaching back many years. It is itself related to another classic graph theory problem, the Hamiltonian circuit. Hamiltonian circuits have been studied since 1856 by both Hamilton [24] and Kirkman [27]. Whereas the TSP has been informally discussed for many years [46], it didn’t become actively studied until after 1928, where Menger, Whitney, Flood and Robinson produced much of the early results in the field. Robinson’s RAND report [43] is probably the first article to call the problem by the name it has since become known as, the Traveling Salesman Problem."
    }, {
      "heading" : "2.1. OVERVIEW 11",
      "text" : "From Robinson’s RAND report:\nThe purpose of this note is to give a method for solving a problem related to the traveling salesman problem. One formulation is to find the shortest route for a salesman starting from Washington, visiting all the state capitals and then returning to Washington. More generally, to find the shortest closed curve containing n given points in the plane.\nAn early result was provided by Dantzig, Fulkerson, and Johnson [13]. Their paper gave an exact method for solving a 49 city problem, a large number of cities for the time. Their algorithm used the cutting plane method to provide an exact solution. This approach has been the inspiration for many subsequent approaches, and is still the bedrock of algorithms that attempt to provide an exact solution.\nA generalisation of the TSP is Multiple Traveling Salesman Problem (MTSP), where multiple tours are constructed (i.e. multiple salesman can be used to visit the cities). The pure MTSP can trivially be turned into a TSP by constructing a graph G with n−1 additional copies of the starting vertex and by forbidding travel directly between the n starting vertices. However, the pure formulation of MTSP places no additional constraints on how the routes are constructed. Real life applications of the MTSP typically require additional constraints, such as limiting the size or duration of each route (i.e. one salesman shouldn’t be working a 12 hour shift, while another has no work).\nMTSP leads us naturally into the family of problems given by the VRP. VRP, and its family of related problems, can be understood as being a generalisation of MTSP that incorporates additional constraints. Some of these constraints, such as capacity limits, introduce additional dimensions to the problem that are in themselves hard combinatorial problems."
    }, {
      "heading" : "12 CHAPTER 2. BACKGROUND",
      "text" : ""
    }, {
      "heading" : "2.2 Exact Methods",
      "text" : "The first efforts at providing a solution to the VRP were concerned with exact methods. These started by sharing many of the techniques brought to bear on TSP. We follow Laporte and Nobert’s survey [29] and classify exact methods for the VRP into three families: Direct Tree Search methods, Dynamic Programming, and Integer Linear Programming.\nThe first classic Direct Tree Search results are due to Christolds and Ellison. Their 1969 paper provided the first branch and bound algorithm for exactly solving the VRP [10]. Unfortunately its time and memory requirements were large enough that it was only able to solve problems of up to 13 customers. This result was later improved upon by Christolds in 1976 by using a different branch model. This improvement allowed him to solve for up to 31 customers.\nChristofides, Mingozzi, and Toth [11], provide a lower bound method that is sufficiently quick (in terms of runtime performance) to be used as a lower bound for excluding nodes from the search tree. Using this lower bound they were able to provide solutions for problems containing up to 25 customers. Laporte, Mercure and Nobert [28] used MTSP as a relaxation of the VRP within a branch and bound framework to provide solutions for more realistically sized problems, containing up to 250 customers.\nA Dynamic Programming approach was first applied to the VRP by Eilon, WatsonGandy and Christofides [16]. Their approach allowed them to solve exactly for problems of 10 to 25 customers. Since then, Christofides has made improvements to this algorithm to solve exactly for problems up to fifty customers.\nA Set Partitioning method was given by Balinski, and Quandt in 1964 [5] to produce exact VRP solutions. However, the problem sets they used were very small, only containing between 5 to 15 customers; and even then they were not able to produce solutions for some of the problems. However, taking their approach as a starting point, many authors have been able to produce more powerful methods. Rao and Zionts [40], Foster and Ryan [18], and Desrochers, Desrosiers and Solomon [15] have all extended the basic set partitioning algorithm using the Column Generation method from Integer Programming. These later papers have produced some of the best exact results.\nNotwithstanding the preceding discussion, exact methods have been of more use in advancing the theoretical understanding of the VRP than they have been in providing solutions to real life routing problems. This can mostly be attributed to the fact that real life VRP instances often involve at least tens of customers (and often hundreds), and involve richer constraints than are modelled in the classic VRP."
    }, {
      "heading" : "2.3. CLASSIC HEURISTICS 13",
      "text" : ""
    }, {
      "heading" : "2.3 Classic Heuristics",
      "text" : "In this section we review the classic heuristic methods that have been developed for the VRP. These methods are not guaranteed to find the globally best answer, but rather aim to produce close to optimal solutions using algorithms with fast running times that are able to scale to large problem instances. Classic heuristics for the VRP can be classified into three families: constructive heuristics; two-phase heuristics, which can again be divided into two subfamilies, cluster first and then route, and route first and then cluster; and finally improvement methods."
    }, {
      "heading" : "2.3.1 Constructive Heuristics",
      "text" : "We start by looking at the Constructive Heuristics. Constructive heuristics build a solution from the ground up. They typically provide a recipe for building each route, such that the total cost of all routes is minimised.\nA trivial but intuitive constructive heuristic is the Nearest Neighbour method. In this method routes are built up sequentially. At each step the customer nearest to the last routed customer is chosen. This continues until the route reaches its maximum capacity, at which point a new route is started. In practice the Nearest Neighbour algorithm tends to provide poor results and is rarely used.\nAn early and influential result was given by Clarke and Wright in their 1964 paper [12]. In their paper they present a heuristic extending Dantzig and Ramser’s earlier work, which has since become known as the Clarke Wright Savings heuristic. The heuristic is based on the simple premise of iteratively combining routes in order of those pairs that provide the largest saving.\n14 CHAPTER 2. BACKGROUND\nThe algorithm works as follows:\nAlgorithm 1: Clark Write Savings Algorithm\ninitialiseRoutes() M = savingsMatrix(V ) L = sortBySavings(SM) for lij ← L do\nRi, Rj = findRoutes(lij) if feasibleMerge(Ri, Rj) then\ncombineRoute(Ri, Rj) end\nend\nThe algorithm starts by initialising a candidate solution. For this it creates a route R = [vd, vi, v\nd] for all v ∈ V . It then calculates a matrix M that contains the savings sij = ci0 + cj0− cij for all edges (i, j) ∈ E. It then produces a list, L, that enumerates each cell i, j of the matrix in descending order of the savings. For each entry in the list, lij ∈ L, it selects the two routes, Ri, Rj , that contain customers i, j ∈ V and tests to see if the two routes can be merged. A merge is permissible if and only if:\n1. Ri 6= Rj .\n2. i, j are the first or last vertices (excluding the depot vd) of their respective routes.\n3. The combined demand of the two routes doesn’t exceed the maximum allowed, q.\nThe heuristic comes in two flavours, sequential and parallel. The sequential version adds the additional constraint that only one route can be constructed at a time. In this case one of the two routes considered, Ri, Rj , must be the route under construction. If"
    }, {
      "heading" : "2.3. CLASSIC HEURISTICS 15",
      "text" : "neither of the routes are the route under construction then the list item is ignored and processing continues down the list. If the merge is permissible then we merge routes Ri, Rj such that R′ = [v0, . . . , i, j, . . . , vk]. In the parallel version, once the entire list of savings has been enumerated then the resulting solution is returned as the answer. In the sequential version the for loop is repeated until no feasible merges remain.\nThe Clark Write Savings heuristic has been used to solve problems of up to 1000 customers with results often within 10% of the optimal solution using only a 180 seconds of runtime [52]. The parallel version of the Clark Write Savings Algorithm outperforms the sequential version in most cases [30] and is typically the one employed.\nThe heuristic has proven to be surprisingly adaptable and has been extended to deal with more specialised vehicle routing problems where additional objectives and constraints must be factored in. Its flexibility is a result of its algebraic treatment of the problem [30]. Unlike many other VRP heuristics that exploit the problem’s spatial properties (such as many of the two-phase heuristics, see Section 2.3.2), the savings formula can easily be adapted to take into consideration other objectives. An example of this is Solomon’s equally ubiquitous algorithm [48] which extends the Clark Wright Savings algorithm to cater for time constraints.\nThis classic algorithm has been extended by Gaskell [19], Yellow [55] and Paessens [37], who have suggested alternatives to the savings formulas used by Clarke and Wright. These approaches typically introduce additional parameters to guide the algorithm towards selecting routes with geometric properties that are likely to produce better combinations. Altinkemer and Gavish provide an interesting variation on the basic savings heuristic [4]. They use a matching algorithm to combine multiple routes in each step. To do this they construct a graph such that each vertex represents a route, each edge represents a feasible saving, and the edges’ weights represent the savings that can be realised by the merge of the two routes. The algorithm proceeds by solving a maximum cost weighted matching of the graph."
    }, {
      "heading" : "2.3.2 Two-phase Heuristics",
      "text" : "We next look at two-phase heuristics. We start by looking at the cluster first, route second subfamily. One of the foundational algorithms for this method is given to us by Gillett and Miller who provided a new approach called the Sweep Algorithm in their 1974 paper [22]. This popularised the two-phase approach, although a similar method was suggested earlier by Wren in his 1971 book, and subsequently in Wren and Holliday’s 1972 paper [54]. In this approach, an initial clustering phase is used to cluster the customers into a base set of routes. From here the routes are treated as separate TSP instances and optimised accordingly. The two-phase approach typically doesn’t prescribe a method for how the TSP is solved and assumes that already developed TSP methods can be used. The classic Sweep algorithm uses a simple geometric method to cluster the customers. Routes are built by sweeping a ray, centered at"
    }, {
      "heading" : "16 CHAPTER 2. BACKGROUND",
      "text" : "the depot, clockwise around the space enclosing the problem’s locations. The Sweep method is surprisingly effective and has been shown to solve several benchmark VRP problems to within 2% to 9% of the best known solutions [52].\nFisher and Jaikumars’s 1981 paper [17] builds upon the two-phase approach by providing a more sophisticated clustering method. They solve a General Assignment Problem to form the clusters instead. A limitation of their method is that the amount of vehicle routes must be fixed up front. Their method often produces results that are 1% to 2% better than results produced by the classic Sweep algorithm [52].\nChristofides, Mingozzi, and Toth expanded upon this approach in [11] and proposed a method that uses a truncated branch and bound technique (similar to Christofides’s Exact method). At each step it builds a collection of candidate routes for a particular customer, i. It then evaluates each route by solving it as a TSP, from which it then selects the shortest TSP as the route.\nThe Petal algorithm is a natural extension to the Sweep algorithm. It was first proposed by Balinski and Quandt [5] and then extended by Foster and Ryan [18]. The basic process is to produce a collection of overlapping candidate routes (called petals) and then to solve a set partitioning problem to produce a feasible solution. As with other two-phase approaches it is assumed that the order of the customers within each route is solved using an existing TSP heuristic. The petal method has produced competitive results for small solutions, but quickly becomes impractical where the set of candidate routes that must be considered is large.\nLastly, there are route first, cluster second methods. The basic premise of these techniques are to first construct a ‘grand’ TSP tour such that all customers are visited. The second phase is then concerned with splitting this tour into feasible routes. Route first, cluster second methods are generally thought to be less competitive than other methods [30], although interestingly, Haimovich and Rinnooy Kan have shown that if all customers have unit demand then a simple shortest path algorithm (which can be solved in polynomial time) can be used to produce a solution from a TSP tour that"
    }, {
      "heading" : "2.3. CLASSIC HEURISTICS 17",
      "text" : "is asymptotically optimal [23]."
    }, {
      "heading" : "2.3.3 Iterative Improvement Heuristics",
      "text" : "Iterative Improvement methods follow an approach where an initial candidate solution is iteratively improved by applying an operation that improves the candidate solution, typically in a small way, many thousands of times. The operations employed are typically simple and only change a small part of the candidate solution, such as the position of a single customer or edge within the solution. The set of solutions that are obtainable from the current candidate solution, S, by applying an operator Op is known as S’s neighbourhood. Typically, with Iterative Improvement heuristics, a new solution S′ is selected by exhaustively searching the entire neighbourhood of S for the best improvement possible. If no improvement can be found then the heuristic terminates. The initial candidate solution (i.e. the starting point of the algorithm) can be randomly selected or can be produced using another heuristic. Constructive Heuristics are typically used for initially seeding an improvement heuristic, see Section 2.3.1 for more information on these.\nProbably one of the best known improvement operators is 2-Opt. The 2-Opt operator takes two edges (i, j), (k, l) ∈ T , where T are the edges traversed by a particular route R = [v1, . . . , vi, vj , . . . , vk, vl, . . . , vn], and removes these from the candidate solution. This splits the route into two disconnected components, D1 = [vj , . . . , vk], D2 = [v1, . . . , vi, vl, . . . , vn]. A new candidate solution is produced by reconnecting D1 to D2 using the same vertices i, j, l, k but with alternate edges, such that (i, j), (j, l) ∈ T .\nThe rationale behind 2-Opt is that, due to the triangle inequality, edges that cross themselves are unlikely to be optimal. 2-Opt aims to detangle a route."
    }, {
      "heading" : "18 CHAPTER 2. BACKGROUND",
      "text" : "There are a number of other operations suggested in the literature. Christofides and Eilon give one of the earliest iterative improvement methods in their paper [10]. In the paper they make a simple change to 2-Opt to increase the amount of edges removed from two to three—the operation fittingly being called 3-Opt. They found that their heuristic produced superior results than 2-Opt.\nIn general, operations such as 3-Opt, that remove edges and then search for a more optimal recombination of components take O(ny) where y is the number of edges removed. A profitable strain of research has focused on producing operations that reduce the amount of recombinations that must be searched. Or presents an operation that has since come to be known as Or-Opt [35]. Or-Opt is a restricted 3-Opt. It searches for a relocation of all sets of 3 consecutive vertices (which Or calls chains), such that an improvement is made. If an improvement cannot be made then it tries again with chains of 2 consecutive vertices, and so on. Or-Opt has been shown to produce similar results to that of 3-Opt, but with a running time of O(n2). More recently Renaud, Boctor, and Laptorte [42] have presented a restricted version of 4- Opt, called 4-Opt*, that operates in a similar vein to Or-Opt. 4-Opt* has a running time ofO(wn2) where w denotes the number of edges spanned by 4-Opt* when building a chain.\nIterative improvement heuristics are often used in combination with other heuristics. In this case they are run on the candidate solution after the initial heuristic has completed. However, if used in this way there is often a fine balance between producing an operation that improves a solution, and one that is sufficiently destructive enough to escape a local minimum. Interest in Iterative Improvement heuristics has grown as the operations developed for them, such as Or-Opt, are directly applicable to more modern heuristics, such as the family known as meta-heuristics presented in the next section."
    }, {
      "heading" : "2.4 Meta-heuristics",
      "text" : "Meta-heuristics are a broad collection of methods that make few or no assumptions about the type of problem being solved. They provide a framework that allows for individual problems to be modelled and ‘plugged in’ to the meta-heuristic. Typically, meta-heuristics take an approach where a candidate solution (or solutions) is initially produced and then is iteratively refined towards the optimal solution. Intuitively meta-heuristics can be thought of searching a problem’s search space. Each iteration searches the neighbourhood of the current candidate solution(s) looking for new candidate solutions that move closer to the global optimum.\nA limitation of meta-heuristics is that they are not guaranteed to find an optimal solution (or even a good one!). Moreover, the theoretical underpinnings of what makes one meta-heuristic more effective than another are still poorly understood.\n2.4. META-HEURISTICS 19\nMeta-heuristics within the literature tend to be tuned for specific problems and then validated empirically.\nThere have been a number of meta-heuristics produced for the VRP in recent years and many of the most competitive results produced in the last ten years are due to them. We next review some of the more well known meta-heuristic results for the VRP."
    }, {
      "heading" : "2.4.1 Simulated Annealing",
      "text" : "Simulated Annealing is inspired by the annealing process used in metallurgy. The algorithm starts with a candidate solution (which can be randomly selected) and then moves to nearby solutions with a probability dependent on the quality of the solution and a global parameter T , which is reduced over the course of the algorithm. In classic implementations the following formula is used to control the probability of a move:\ne− f(s′)−f(s) T\nWhere f(s) and f(s′) represent the solution quality of the current solution, and the new solution respectively. By analogy with the metallurgy process, T represents the current temperature of the solution. Initially T is set to a high value. This lets the algorithm free itself from any local optima that it may be caught in. It is then cooled over the course of the algorithm forcing the search to converge on a solution."
    }, {
      "heading" : "20 CHAPTER 2. BACKGROUND",
      "text" : "One of the first Simulated Annealing results for the VRP was given by Robuste, Daganzo and Souleyrette [44]. They define the search neighbourhood as being all solutions that can be obtained from the current solution by applying one of two operations: relocating part of a route to another position within the same route, or exchanging customers between routes. They tested their solution on some large real world instances of up to 500 customers. They reported some success with their approach, but as their test cases were unique, no direct comparison is possible.\nOsman has given the best known Simulated Annealing results for the VRP [36]. His algorithm expands upon many areas of the basic Simulated Annealing approach. The method starts by using the Clark and Wright algorithm to produce an initial position. It defines its neighbourhood as being all candidate solutions that can be reached by applying an operator he names the λ-interchange operation.\nλ-interchange works by selecting two sequences (i.e. chains) of customers Cp, Cq from two routes, Rp and Rq, such that |Cp|, |Cq| < λ (note that the chains are not necessarily of the same length). The customers within each chain are then exchanged with each other in turn, until an exchange produces an infeasible solution. As the neighbourhood produced by λ-interchange is typically quite large, Osman restricts λ to being less than 2 and suggests that the first move that provides an improvement is used rather than exhaustively searching the entire neighbourhood.\nOsman also uses a sophisticated cooling schedule. His main change being that the temperature is cooled only while improvements are found. If no improvement is found he then resets the temperature using Ti = max( Tr 2 , Tb), where Tr is the reset temperature, and Tb is temperature of the best solution found so far.\nAlthough Simulated Annealing has produced some good results, and in many cases outperforms classic heuristics (compare [30] with [21]), it is not competitive with the Tabu Search methods discussed in Section 2.4.3."
    }, {
      "heading" : "2.4. META-HEURISTICS 21",
      "text" : ""
    }, {
      "heading" : "2.4.2 Genetic Algorithms",
      "text" : "Genetic Algorithms were first proposed in [25]. They have since been applied to many problem domains and are particularly well suited to applications that must work across a number of different domains. In fact they were the first evolutionary-inspired algorithm to be applied to combinatorial problems [39]. The basic operation of a Genetic Algorithm is as follows:\nAlgorithm 2: Simple Genetic Algorithm\nGenerate the initial population while termination condition not met do\nEvaluate the fitness of each individual Select the fittest pairs Mate pairs and produce next generation Mutate (optional)\nend\nIn a classic Genetic Algorithm each candidate solution is encoded as a binary string (i.e. chromosome). Each individual (i.e. candidate solution) is initially created randomly and used to seed the population. A technique often employed in the literature is to initially ‘bootstrap’ the population by making use of another heuristic to produce the initial population. However, special care must be taken with this approach to ensure that diversity is maintained across the population, as you risk premature convergence by not introducing enough diversity in the initial population.\nNext, the fittest individuals are selected from the population and are mated in order to produce the next generation. The mating process uses a special operator called a crossover operator that takes two parents and produces offspring from these by combining parts of each parent. Optionally, a mutation operation is also applied, that introduces a change that doesn’t exist in either parent. The classic crossover operation takes two individuals encoded as binary strings and splits these at one or two points along the length of the string. The strings are then recombined to form a new binary string, which in turn encodes a new candidate solution. The entire process is continued until a termination condition is met (often a predetermined running time), or until the population has converged on a single solution.\nSpecial consideration needs to be given to how problems are encoded and to how the crossover and mutation operators work when using Genetic Algorithms to solve discrete optimisation problems, such as the VRP. For example, the classic crossover operation, which works on binary strings, would not work well on a TSP tour. When two components of two tours are combined in this way they are likely to contain duplicates. Therefore, it is more common for the VRP (and the TSP) to use a direct representation and to use specially designed crossover operators. In this instance the VRP is represented as a set of sequences, each holding an ordered list of customers."
    }, {
      "heading" : "22 CHAPTER 2. BACKGROUND",
      "text" : "The crossover operators are then designed so that they take into consideration the constraints of the VRP.\nTwo crossover operators commonly used with combinatorial problems are the Order Crossover (OX) and the Edge Assembly Crossover (EAX). OX [34] operates by selecting two cut points within each route. The substring between the two cut points is copied from the second parent directly into the offspring. Likewise, the string outside the cut points is copied from the first parent into the offspring, but with any duplicates removed. This potentially leaves a partial solution, where not all customers have been routed. The partial solutions is then repaired by inserting any unrouted customers into the child in the same order that they appeared in the second parent.\nAnother common crossover operator is EAX. EAX was originally designed for the TSP but has been adapted to the VRP by [32]. EAX operates using the following process:\n1. Combine the two candidate solutions into a single graph by merging each solution’s edge sets.\n2. Create a partition set of the graph’s cycles by alternately selecting an edge from each graph.\n3. Randomly select a subset of the cycles.\n4. Generate a (incomplete) child by taking one of the parents and removing all edges from the selected subset of cycles, then add back in the edges from the parent that wasn’t chosen.\n5. Not all cycles in the child are connected to the route. Repair them by iteratively merging the disconnected cycles to the connected cycles.\nAn alternative and interesting approach found in the literature is to instead encode a set of operations and parameters that are fed to another heuristic, that in turn"
    }, {
      "heading" : "2.4. META-HEURISTICS 23",
      "text" : "produces a candidate solution. A well known example of this approach was suggested in [7] which encoded an ordering of the customers. The ordering is then fed into an insertion heuristic to produce the actual candidate solutions.\nAn influential result that uses Genetic Algorithms to solve VRPTW is given in [50] with their GIDEON algorithm. GIDEON uses an approach inspired by the Sweep method (an overview of the Sweep method is provided in Section 2.3.2). It builds routes by sweeping a ray, centered at the depot, clockwise around the geographic space enclosing the customer’s locations. Customers are collected into candidate routes based on a set of parameters that are refined by the Genetic Algorithm. GIDEON uses the Genetic Algorithm to evolve the parameters used by the algorithm, rather than to operate on the problem directly. Finally, GIDEON uses a local search method to optimise customers within each route, making use of the λ-interchange operator (A description of this operator is provided in Section 2.4.1).\nGenerally speaking, Genetic Algorithms are not as competitive as other meta-heuristics"
    }, {
      "heading" : "24 CHAPTER 2. BACKGROUND",
      "text" : "at solving the VRP. However, more recently there have been two very promising applications of Genetic Algorithms being used to solve the VRP. Nagata [32] has adapted the EAX operator for use with the VRP. And Berger and Barkaoui have presented a Hybrid Genetic Algorithm called HGA-VRP in [6]. HGA-VRP adapts a construction heuristic for use as a crossover operator. The basic premise is to select a set of routes from each parent that are located close to one another. Customers are then removed from one parent and inserted into the second using an operation inspired by Solomon’s construction heuristic for VRPTW [48].\nBoth methods have reached the best known solution for a number of the classic VRP benchmark instances by Christofides, Mingozzi and Toth [11] and are competitive with the best Tabu Search methods."
    }, {
      "heading" : "2.4.3 Tabu Search",
      "text" : "Tabu Search follows the general approach shared by many meta-heuristics; it iteratively improves a candidate solution by searching for improvements within the current solution’s neighbourhood. Tabu search starts with a candidate solution, which may be generated randomly or by using another heuristic. Unlike Simulated Annealing, the best improvement within the current neighbourhood is always taken as the next move. This introduces the problem of cycling between candidate solutions. To overcome this Tabu Search introduces a list of solutions that have already been investigated and are forbidden as next moves (hence its name).\nThe first instance of Tabu Search being used for VRP is by Willard [53]. Willard’s approach made use of the fact that VRP instances can be transformed into MTSP instances and solved. The algorithm uses a combination of simple vertex exchange and relocation operations. Although opening the door for further research, its results weren’t competitive with the best classic heuristics.\nOsman gives a more competitive use of Tabu Search in [36]. As with his Simulated Annealing method he makes use of the λ-interchange operation to define the search neighbourhood. Osman provides two alternative methods to control how much of the neighbourhood is searched for selecting the next move: Best-Improvement (BI) and First-Improvement (FI). Best-Improvement searches the entire neighbourhood and selects the move that is the most optimal. First-Improvement searches only until a move is found that is more optimal than the current position. Osman’s heuristic produced competitive results that outperformed many other heuristics. However, it has since been refined and improved upon by newer Tabu Search methods.\nToth and Vigo introduced the concept of Granular Tabu Search (GTS) [51]. Their method makes use of a process that removes moves from the neighbourhood that are unlikely to produce good results. They reintroduce these moves back into the process if the algorithm is stuck in a local minimum. Their idea follows from an existing idea"
    }, {
      "heading" : "2.4. META-HEURISTICS 25",
      "text" : "known as Candidate Lists. Toth and Vigo’s method has produced many competitive results.\nTaillard has provided one of the most successful methods for solving the VRP in his Tabu Search method in [49]. Talliard’s Tabu Search uses Or’s λ-interchange as its neighbourhood structure. It borrows two novel concepts from [20]: the use of a more sophisticated tabu mechanism, where the duration (or number of iterations) that an item is tabu for is chosen randomly; and a diversification strategy, where vertices that are frequently moved without giving an improvement are penalised. A novel aspect of Taillard’s algorithm is its decomposition of the problem into sub-problems. Each problem is split into regions using a simple segmentation of the region centred about the depot (Taillard also provides an alternative approach for those problems where the customers are not evenly distributed around the depot). From here each subproblem is solved individually, with customers being exchanged between neighbouring segments periodically. Taillard observes that exchanging customers beyond geographically neighbouring segments is unlikely to produce an improvement, so these moves are safely ignored. Taillard’s method has produced some of the currently best known results for the standard Christofides, Mingozzi and Toth problem sets [11]."
    }, {
      "heading" : "2.4.4 Large Neighbourhood Search",
      "text" : "Large Neighbourhood Search (commonly abbreviated to LNS) was recently proposed as a heuristic by Shaw [47]. Large Neighbourhood Search is a type of heuristic belonging to the family of heuristics known as Very Large Scale Neighbourhood search (VLSN)2. Very Large Scale Neighbourhood search is based on a simple premise; rather than searching within a neighbourhood of solutions that can be obtained from a single (and typically quite granular) operation, such as 2-opt, it might be profitable to consider a much broader neighbourhood—a neighbourhood of candidate solutions that are obtained from applying many simultaneous changes to a candidate solution. What distinguishes these heuristics from others is that the neighbourhoods under consideration are typically exponentially large, often rendering them infeasible to search. Therefore much attention is given to providing methods that can successfully traverse these neighbourhoods.\n2LNS is somewhat confusingly named given that it a type of VLSN, and not a competing approach."
    }, {
      "heading" : "26 CHAPTER 2. BACKGROUND",
      "text" : "Large Neighbourhood Search uses a Destroy and Repair metaphor for how it searches within its neighbourhood. Its basic operation is as follows:\nAlgorithm 3: Large Neighbourhood Search\nx = an initial solution while termination condition not met do\nxt = x destroy(xt) repair(xt) if xt better than current solution then\nx = xt end\nend Result: x\nIt starts by selecting a starting position. This can be done randomly or by using another heuristic. Then for each iteration of the algorithm a new position is generated by destroying part of the candidate solution and then by repairing it. If the new solution is better than the current solution, then this is selected as the new position. This continues until the termination conditions are met. Large Neighbourhood Search can be seen as being a type of Very Large Scale Neighbourhood search because at each iteration the number of neighbouring solutions is exponentially large, based on the number of items removed (i.e. destroyed).\nObviously the key components of this approach are the functions used to destroy and repair the solution. Care must be given to how these functions are constructed. They must pinpoint an improving solution from a very large neighbourhood of candidates, while also providing enough degrees of freedom to escape a local optimum.\nEmpirical evidence in the literature shows that even surprisingly simple destroy and repair functions can be effective [47] [45]. In applications of Large Neighbourhood Search for VRP a pair of simple operations are commonly used (often alongside more complex ones too) for the destroy and repair functions. Specifically, the solution is destroyed by randomly selecting and removing n customers. It is then repaired by finding the least cost reinsertion points back into the solution of the n customers.\nShaw applied Large Neighbourhood Search to VRP in his original paper introducing the method [47]. In this he introduced a novel approach for his destroy and repair functions. The destroy function removes a set of ‘related’ customers. He defines a related customer to be any two customers that share a similar geographic location, that are sequentially routed, or that share a number of similar constraints (such as overlapping time windows if time constraints are used). The idea of removing related customers, over simply removing random customers, is that related customers are more likely to be profitably exchanged—or stated another way, unrelated customers are more likely to be reinserted back in their original positions. Shaw’s repair func-"
    }, {
      "heading" : "2.5. SWARM INTELLIGENCE 27",
      "text" : "tion makes use of a simple branch and bound method that finds the minimum cost reinsertion points within the partial solution. His results were immediately impressive and reached many of the best known solutions on the Christofides, Mingozzi and Toth problems [11].\nMore recently Ropke proposed an extension to the basic Large Neighbourhood Search process in [45]. His method adds the concept of using a collection of destroy and repair functions, rather than using a single pair. Which function to use is selected at each iteration based on its previous performance. In this way the algorithm adapts itself to use the most effective function to search the neighbourhood.\nRopke makes use of several destroy functions. He uses a simple random removal heuristic, Shaw’s removal heuristic, and a worst removal heuristic, which removes the most costly customers (in terms of that customer’s contribution to the route’s overall cost). Likewise, he makes use of several different insertion functions. These include a simple greedy insertion heuristic, and a novel insertion method he calls the ‘regret heuristic’. Informally, the regret heuristic reinserts those customers first who are most impacted (in terms of increased cost) by not being inserted into their optimum positions. Specifically, let U be the set of customers to be reinserted and let xik be a variable that gives the k’th lowest cost for inserting customer i ∈ U into the partial solution. Now let c∗i = xi2 − xi1, in other words the cost difference between inserting customer i into its second best position and its first. Now in each iteration of the repair function choose a customer that maximises:\nmax i∈U\nc∗i\nRopke presents a series of results that show that his Large Neighbourhood Search is very competitive for solving the VRP and its related problems (i.e. VRPTW, PDPTW, and DARP). Considering that Large Neighbourhood Search was only proposed in 1998, it has been very successful. In a short space of time it has attracted a large amount of research and has produced some of the most competitive results for solving the VRP."
    }, {
      "heading" : "2.5 Swarm Intelligence",
      "text" : "A recent area of research is in producing heuristics that mimic certain aspects of swarm behaviour. Probably the most well known heuristics in this family are Particle Swarm Optimisation (PSO) and Ant Colony Optimisation (ACO). Real life swarm intelligence is interesting to combinatorial optimisation researchers as it demonstrates a form of emergent intelligence, where individual members with limited reasoning capability and simple behaviours, are still able to arrive at optimal solutions to complex resource allocation problems."
    }, {
      "heading" : "28 CHAPTER 2. BACKGROUND",
      "text" : "In the context of combinatorial optimisation, these behaviours can be mimicked and exploited. Algorithms that make use of this approach produce their solutions by simulating behaviour across a number of agents, who in themselves, typically only perform rudimentary operations. A feature of this class of algorithms is the ease with which they can be parallelised, making them more easily adaptable to large scale problems.\nSwarm Intelligence algorithms have been employed to solve a number of problems. We look at two examples here, Ant Colony Optimisation and the Bees Algorithm, which this thesis makes use of."
    }, {
      "heading" : "2.5.1 Ant Colony Optimisation",
      "text" : "Ant Colony Optimisation is inspired by how ants forage for food and communicate promising sites back to their colony. Real life ants initially forage for food randomly. Once they find a food source they return to the colony and in the process lay down a pheromone trail. Other ants that then stumble upon the pheromone trail follow it with a probability dependent on how strong (and therefore how old) the pheromone trail is. If they do follow it and find food, they then return to the colony, thus also strengthening the pheromone trail. The strength of the pheromone trail reduces over time meaning that younger and shorter pheromone trails, that do not take as long to traverse, attract more ants.\nAnt Colony Optimisation mimics this behaviour on a graph by simulating ants marching along a graph that represents the problem being solved. The basic operation of"
    }, {
      "heading" : "2.5. SWARM INTELLIGENCE 29",
      "text" : "the algorithm is as follows:\nAlgorithm 4: Ant Colony Optimisation\nData: A graph representing the problem while termination condition not met do\npositionAnts() while solution being built do\nmarchAnts() end updatePheromones()\nend\nAt each iteration of the algorithm the ants are positioned randomly within the graph. The ants are then stochastically marched through the graph until they have completed a candidate solution (in the case of a TSP this would be a tour of all vertices). At each stage of the march each ant selects their next edge based on the following probability formula:\npkij = [ταij ][η β ij ]∑\nl∈Nk [τ α il ][η β il]\nWhere pkij is the probability that ant k will traverse edge (i,j), N k is the set of all edges that haven’t been traversed by ant k yet, τ is the amount of pheromone that has been deposited at an edge, η is the desirability of an edge (based on a priori knowledge specific to the problem), and α and β are global parameters that control how much influence each term has.\nOnce the march is complete and a set of candidate solutions have been constructed (by each ant, k), pheromone is deposited on each edge using the following equation:\nτij = (1− ρ)τij + m∑ k=1 ∆τkij\nWhere 0 < ρ ≤ 1 is the pheromone persistence, and ∆τkij is a function that gives the amount of pheromone deposited by ant k. The function is defined as:\n∆τkij = { 1/Ck if edge (i, j) is visited by ant k 0 otherwise"
    }, {
      "heading" : "30 CHAPTER 2. BACKGROUND",
      "text" : "Where Ck represents the total distance travelled through the graph by ant k. This ensures that shorter paths result in more pheromone being deposited.\nAs an example of Ant Colony Optimisation’s use in combinatorial problems, we show how it can be applied to the TSP. We build a weighted graph with i ∈ V representing each city to be visited and (i, j) ∈ E and wij representing the cost of travel between each city. Then at each step of the iteration we ensure that the following constraints are met:\n• Each city is visited at most once.\n• We set ηij to be equal to wij .\nWhen the Ant Colony Optimiser starts, it positions each ant at a randomly selected vertex (i.e. city) within the graph. Each step of an ant’s march then builds a tour through the cities. Once an ant has completed a tour it serves as a candidate solution for the TSP. Initially the solutions will be of low quality, so we use the length of the tours to ensure that more pheromone is deposited on the shorter tours. At the end of n iterations the ants will have converged on a near optimal solution (but like all meta-heuristics there’s no guarantee that this will be the global optimum).\nAnt Colony Optimisation has been applied to VRP by Bullnheimer, Hartl, and Strauss in [8] [9]. They adapted the straightforward implementation used for the TSP, detailed in the preceding discussion, by forcing the ant to create a new route each time it exceeds the capacity or maximum distance constraint. They also use a modified edge selection rule that takes into account the vehicle’s capacity and its proximity to the depot. Their updated rule is given by:\n2.5. SWARM INTELLIGENCE 31\npkij = [ταij ][η β ij ][sij ][κij ]∑\nl∈Nk [τ α il ][η β il][sil][κil]\nWhere s represents the proximity of customers i, j to the depot, and κ = (Qi+qj)/Q— Q, giving the maximum capacity, Qi giving the capacity already used on the vehicle, and qj is the additional load to be added. κ influences the ants to take advantage of the available capacity.\nBullnheimer et al.’s implementation of Ant Colony Optimisation for VRP produces good quality solutions for the Christofides, Mingozzi and Toth problems [11], but is not competitive with the best modern meta-heuristics.\nMore recently Reimann, Stummer, and Doerner have presented a more competitive implementation of Ant Colony Optimisation for VRP [41]. Their implementation operates on a graph where (i, j) ∈ E represent the savings of combining two routes, as given by the classic Clark and Wright Savings heuristic (see Section 2.3.1 for more on this heuristic). Each ant selects an ordering of how the merges are applied. This implementation is reported to be competitive with the best meta-heuristics [39]."
    }, {
      "heading" : "2.5.2 Bees Algorithm",
      "text" : "Over the last decade, and inspired by the success of Ant Colony Optimisation, there have been a number of algorithms proposed that aim exploit the collective behaviour of bees. This includes: Bee Colony Optimisation, which has been applied to many combinatorial problems, Marriage in Honey Bees Optimization (MBO) that has been used to solve propositional satisfiability problems, BeeHive that has been used for timetabling problems, the Virtual Bee Algorithm (VBA) that has been used for function optimisation problems, Honey-bee Mating Optimisation (HBMO) that has been used for cluster analysis, and finally, the Bees Algorithm that is the focus of this thesis. See [31] for a bibliography and high level overview on many of these algorithms.\nThe Bees Algorithm was first proposed in [38]. It is inspired by the foraging behaviour of honey bees. Bee colonies must search a large geographic area around their hive in order to find sites with enough pollen to sustain a hive. It is essential that the colony makes the right choices in which sites are exploited and how much resource is expended on a particular site. They achieve this by sending scout bees out in all directions from the hive. Once a scout bee has found a promising site it then returns to the hive and recruits hive mates to forage at the site too. The bee does this by performing a waggle dance. The dance communicates the location and quality of the site (i.e. fitness). Over time, as more bees successfully forage at the site, more are recruited to exploit the site—in this aspect bee behaviour shares some similarities with ant foraging behaviour."
    }, {
      "heading" : "32 CHAPTER 2. BACKGROUND",
      "text" : "Informally the algorithm can be described as follows. Bees are initially sent out to random locations. The fitness of each site is then calculated. A proportion of the bees are reassigned to those sites that had the highest fitness values. Here each bee searches the local neighbourhood of the site looking to improve the site’s fitness. The remainder of the bees are sent out scouting for new sites, or in other words, they are set to a new random position. This process repeats until one of the sites reaches a satisfactory level of fitness—or a predetermined termination condition is met.\nMore formally, the algorithm operates as follows:\nAlgorithm 5: Bees Algorithm\nB = {b1, b2, . . . , bn} setToRandomPosition(B) while termination condition not met do\nsortByFitness(B) E = {b1, b2, . . . , be} R = {be+1, be+2, . . . , bm} searchNeighbourhood(E ∪ {c1, . . . , cnep}) searchNeighbourhood(R ∪ {d1, . . . , dnsp}) setToRandomPosition(B − (E ∪R))\nend\nB is the set of bees that are used to explore the search space. Initially the bees are set to random positions. Function sortByFitness sorts the bees in order of maximum fitness. It then proceeds by taking the m most promising sites found by the bees. It does this by partitioning these into two sets, E,N ⊂ B. E is the first e best sites, and represents the so called elite bees. N is the m− e next most promising sites. The searchNeighbourhood function explores the neighbourhood around a provided set of bees. Each site in E and N is explored. nep bees are recruited for the search of each"
    }, {
      "heading" : "2.5. SWARM INTELLIGENCE 33",
      "text" : "b ∈ E, and nsp are recruited for the search of each b ∈ N . In practice this means that nep and nsp number of positions are explored within the neighbourhoods of E and N ’s sites, respectively. These moves are typically made stochastically, but it is possible for a deterministic approach to be used too. The remaining n −m bees (in other words, those not in E and N) are set to random positions. This is repeated until the termination condition is met, which may be a running time threshold or a predetermined fitness level.\nThe advantage promised by the Bees Algorithm over other meta-heuristics is its ability to escape local optima and its ability to navigate search topologies with rough terrain (such as in Figure 2.14). It achieves this by scouting the search space for the most promising sites, and then by committing more resources to the exploration of those sites that produce better results.\nThe Bees Algorithm has been applied to manufacturing cell formation, training neural networks for pattern recognition, scheduling jobs for a production machine, data clustering, and many others areas. See [2] for more examples and a comprehensive bibliography. However, to the best of our knowledge, the Bees Algorithm hasn’t been adapted for the Vehicle Routing Problem until now.\n34 CHAPTER 2. BACKGROUND\nChapter 3\nProblem Definition\nIn this chapter we provide a formal definition of the VRP and briefly describe the variant problems that have arisen in the literature. The Capacitated Vehicle Routing Problem (CVRP) is the more correct name for the VRP that distinguishes it from its variants. We start in Section 3.1 by providing a formal definition of the CVRP. We formulate it as an integer linear programming problem, as has become standard in the VRP literature. And follow this in Section 3.2 by an overview of the VRP variants that are commonly used."
    }, {
      "heading" : "3.1 Capacitated Vehicle Routing Problem",
      "text" : "We formulate the CVRP here as an integer linear programming problem. Although it is possible to solve the CVRP using an integer programming solver, this is uncommon in practice as the best solvers are still only able to solve for small problem sizes. We provide this formulation as it has become the lingua franca of combinatorial problems.\nWe start the formation by specifying the variables used within it. We represent the CVRP on a weighted graph, G = (V,E). The vertices of the graph V represent all locations that can be visited, this includes each customer location and the location of the depot. For convenience we let vd denote the vertex that represents the depot, and we denote the set of customers as C = {1, 2, . . . , n}. Thus the set of vertices is given by V = vd ∪ C. We now let the set of edges, E, correspond to the valid connections between customers and connections to the depot. For the CVRP all connections are possible, in other words, we set G to be a clique. Each edge (i, j) ∈ E has a corresponding cost cij . We let the cost be the euclidian distance between the two locations cij = √ (xj − xi)2 + (yj − yi)2. Where xi and yi for i ∈ V represent the coordinates of the customer’s location.\n35"
    }, {
      "heading" : "36 CHAPTER 3. PROBLEM DEFINITION",
      "text" : "We use K to denote the set of vehicles that are used to visit customers, such that |K| = m and m is the maximum number of vehicles allowed. We define q and t to be the maximum capacity and the maximum work duration, respectively, allowable for a vehicle. The demand (i.e. required capacity) for each customer is denoted by di, i ∈ C. Likewise, we denote the service time required by each customer as ti, i ∈ C. We then use the decision variable Xkij to denote if a particular edge (i, j) ∈ E is traversed by vehicle k ∈ K, in other words, k travels between customers i, j ∈ C. Where this is true we let Xkij = 1, and X k ij = 0 where it is not. We use ui, i ∈ C as a sequencing variable that gives the position of customer i within the route of the vehicle that visits it.\nWe are now able to define the problem as follows:\nMinimise:∑ k∈K ∑ (ij)∈E cijX k ij (3.1)\nSubject to:∑ k∈K ∑ j∈V\nXkij = 1 ∀i ∈ C (3.2)∑ i∈C di ∑ j∈C\nXkij ≤ q ∀k ∈ K (3.3)∑ i∈C ti ∑ j∈C Xkij + ∑ (ij)∈E\ncijX k ij ≤ t ∀k ∈ K (3.4)∑ j∈V Xkvdj = 1 ∀k ∈ K (3.5)∑ j∈V Xkjvd = 1 ∀k ∈ K (3.6)∑ i∈V Xkic − ∑ j∈V Xkcj = 0 ∀c ∈ C and ∀k ∈ K (3.7) ui − uj + |V |Xkij ≤ |V | − 1 ∀(i, j) ∈ E − vd and ∀k ∈ K (3.8) Xkij ∈ {0, 1} ∀(i, j) ∈ E and ∀k ∈ K (3.9)\nThe objective function (3.1) minimises the costs cij . Constraint (3.2) ensures that each customer can only be serviced by a single vehicle. Constraint (3.3) enforces the capacity constraint; each vehicle cannot exceed its maximum vehicle capacity q. Likewise Constraint (3.4) enforces the vehicle’s work duration constraint. A vehicle’s work duration is the sum of its service times (ti, i ∈ C where customer i is visited by the vehicle) and its travel time. By convention the travel time is taken to be equal to"
    }, {
      "heading" : "3.2. VARIANTS 37",
      "text" : "the distance traversed by the vehicle, which in turn is equal to the costs, cij , of the edges it traverses. Constraints (3.5) and (3.6) ensure that each vehicle starts at the depot and finishes at the depot, and that they do this exactly once. Constraint (3.7) and Constraint (3.8) are flow constraints that ensure that the number of vehicles entering a customer is equal to the number of vehicles leaving, and that sub-tours are eliminated. Lastly, Constraint (3.9) ensures the integrality conditions.\nConstraint (3.4), which enforces a maximum vehicle work duration, t, is often left out of the traditional CVRP formation but is included here as it is present in the problem instances we use for benchmarks in Chapter 5."
    }, {
      "heading" : "3.2 Variants",
      "text" : "In this section we provide an overview of the common variations of the VRP that are used. These variations have arisen from real world vehicle routing scenarios, where the constraints are often more involved than is modelled in the CVRP."
    }, {
      "heading" : "3.2.1 Multiple Depot Vehicle Routing Problem",
      "text" : "A simple extension to the CVRP is to allow each vehicle to start from a different depot. Part of the problem now becomes assigning customers to depots, which in itself is a hard combinatorial problem. The CVRP formation can easily be relaxed to allow this. There are two variations of the problem. One constrains each vehicle to finish at the same depot that it starts from. The other allows vehicles to start and finish at any depot, as long as the same number of vehicles return to the depot as left from it."
    }, {
      "heading" : "3.2.2 Vehicle Routing with Time Windows",
      "text" : "The Vehicle Routing Problem with Time Windows (VRPTW) adds the additional constraint to the classic VRP that each customer must be visited within a time window specified by the customer. More formally, for VRPTW each customer i ∈ C also has a corresponding time window [ai, bi] in which the goods must be delivered. The vehicle is permitted to arrive before the start time, ai. However, in this case the vehicle must wait until time ai adding to the time it takes to complete the route. However, it is not permitted for the job to start after time bi.\nAn additional constraint is added to the formation of CVRP to ensure that time window constraints are met: ai ≤ Ski ≤ bi where the decision variable Ski provides the time that each vehicle k ∈ K arrives at customer i ∈ V ."
    }, {
      "heading" : "38 CHAPTER 3. PROBLEM DEFINITION",
      "text" : ""
    }, {
      "heading" : "3.2.3 Pickup and Delivery Problem",
      "text" : "The Pickup and Delivery Problem (PDP) generalises the VRP. In this problem goods are both picked up and delivered by the vehicle along its route. The vehicle’s work now comes in two flavours: pickup jobs, P = {p1, p2, . . . , pk}, and delivery jobs, D = {d1, d2, . . . , dl}, such that C = P ∪D. Additional constraints are added to the CVRP formation to ensure that:\n1. Pickup and deliver jobs are completed by the same vehicle, that is pi ∈ Rk ⇒ di ∈ Rk where Rk represents a sequence of jobs undertaken by a vehicle k.\n2. The pickup job, pi, appears before its corresponding delivery job, di, in the sequence of jobs undertaken by a vehicle.\n3. The vehicles capacity is not exceeded as goods are loaded and unloaded from it. This requires the use of an intermediate variable, yki , i ∈ V, k ∈ K that represents the load of vehicle k at customer i. It adds constraints: yk0 = 0, Xkij = 1⇒ ykj = yki + di, and ∑ i∈V y k i ≤ q for all k ∈ K, to enforce this.\nThere is also a variation on PDP that adds time windows, called PDPTW. In this case the extra constraints from the VRPTW problem are merged with those given here. PDP is a much harder problem computationally than CVRP, as its extra constraints add new dimensions to the problem. Because of its complexity PDP has only been actively researched in the last decade.\nChapter 4\nAlgorithm\nThis chapter provides a detailed description of the Enhanced Bees Algorithm, the algorithm developed for this thesis, and its operation. We start by reviewing the objectives that the algorithm was designed to meet in Section 4.1. In Section 4.2 we provide a description of how the algorithm internally represents the VRP problem and its candidate solutions. Next in Section 4.3 we provide a detailed description of the operation of the algorithm. Finally, in Section 4.4 we describe the neighbourhood structures that are used by the algorithm to define its search space."
    }, {
      "heading" : "4.1 Objectives",
      "text" : "The Enhanced Bees Algorithm was built for use in a commercial setting. It was developed as part of a New Zealand Trade and Enterprise grant for the company vWorkApp Inc.’s scheduling and dispatch software. Accordingly, different objectives were aimed for with its design (such as runtime performance) than are typically sought in the VRP literature. The algorithm’s objectives, in order of priority, are as follows:\n1. Ensure that all constraints are met. Specifically that the route’s maximum duration is observed.\n2. Have a good runtime performance. It is more desirable for the algorithm to produce a reasonable quality result quickly (within 60 seconds), than for it to produce a better result but require a longer processing time. Specifically if the algorithm could reach 5% of the optimum value within 60 seconds then this would be sufficient.\n3. Produce good quality results. Notwithstanding objective 2, the results produced must be close to the global optimum.\n39"
    }, {
      "heading" : "40 CHAPTER 4. ALGORITHM",
      "text" : "4. Have a design that lends itself to parallelisation and is able to make use of the additional processing cores available within modern hardware."
    }, {
      "heading" : "4.2 Problem Representation",
      "text" : "The Enhanced Bees Algorithm represents the problem in a direct and straightforward manner. It directly manipulates a candidate solution S, where S is a set of routes R ∈ S, and each route contains an ordered sequence of customers vi ∈ R starting and ending at the depot vertex vd.\nMore general representations are sometimes used for meta-heuristics, as is commonly seen with Genetic Algorithms, as they allow the algorithm to be easily adapted to other combinatorial problems. However, this often comes at a cost of added complexity and inferior results1. This algorithm was designed specifically for solving instances of the VRP so a direct representation was chosen.\nThe algorithm makes use of a fitness concept, common to many meta-heuristics, to describe the cost of the solution. The fitness function f() includes terms for the distance (i.e. cost) of the solution and penalties for breaking the capacity and maximum route duration constraints. The Enhanced Bees Algorithm uses penalties to encourage feasible solutions to be produced. Rather than outright barring infeasible solutions, the fitness function allows the algorithm some wriggle room to traverse through these on its way towards a feasible solution.\n1This occurs because the operators that act on the problem representation can no longer exploit information that is specific to the problem domain and must rely on general purpose operations instead."
    }, {
      "heading" : "4.3. ENHANCED BEES ALGORITHM 41",
      "text" : "Specifically f() is defined as follows: c(R) = ∑ i∈R ci,i+1 (4.1)\nd(R) = max (∑ i∈R di − q, 0 ) (4.2)\nt(R) = max (∑ i∈R ti + c(R)− t, 0 ) (4.3)\nf(S) = ∑ R∈S (αc(R) + βd(R) + γc(R)) (4.4)\nFunction c(R) calculates the cost (i.e distance) of a given route, and function d(R) calculates how overcapacity the given route is. We define overcapacity to be how much larger the sum of the route’s demands, di, i ∈ R, are than the stated maximum allowable capacity q. Likewise function t(R) calculates the overtime of a given route. A route’s duration is calculated as being the sum of its customer’s service times, ti, i ∈ R, and its travel time. By convention the travel time is equal to the distance of the route. Function t(R) then returns how much over the maximum allowable route duration, t, the duration is. Lastly, the fitness function f() is the weighted sum of these three terms. Parameters α, β, and γ are used used to control how much influence each term has on determining the candidate solution’s fitness.\nFor the purposes of benchmarking our algorithm (see Chapter 5) we use a travel cost that is equal to the 2D Euclidian distance2 between the two points. For real life problems we have found that using a manhattan distance3 often provides superior results. This is presumably due to the manhattan distance better modelling the road system we tested on (Auckland, New Zealand), which although not a strict grid, is still closer to this than the Euclidian distance models."
    }, {
      "heading" : "4.3 Enhanced Bees Algorithm",
      "text" : "Our Algorithm is based on the Bees Algorithm (see Section 2.5.2 for an overview of the standard Bees Algorithm). The Enhanced Bees Algorithm makes some changes to adapt the Bees Algorithm to the VRP domain. An interesting aspect of the Bees Algorithm is that it covers a broad search area, minimising the risk of being stuck in a local optimum. It achieves this by randomly probing (or in the Bees Algorithm parlance, ‘scouting’) many areas of the search space through its entire run. However, this approach is not well suited to hard combinatorial problems, where a newly constructed\n2Specifically, we use: cij = √\n(xj − xi)2 + (yj − yi)2 3Specifically, we use: cij = (xj − xi) + (yj − yi)"
    }, {
      "heading" : "42 CHAPTER 4. ALGORITHM",
      "text" : "solution, let alone a randomly generated one, is often far from optimal (for instance, the Clark Wright Savings heuristic still produces solutions that are up to 15% from the global optimum and would require many operations to get close to optimal). We have adapted the Bees Algorithm such that many of its unique characteristics, like its relative robustness, are maintained while working well with hard combinatorial problems, such as the VRP.\nThe Enhanced Bees Algorithm can be summarised, at a high level, as follows:\nAlgorithm 6: Enhanced Bees Algorithm\nS = seedSites() while termination condition not met do\nfor si ∈ S do explore(si, d) if i < λ then\nremoveWorstSite end\nend\nend\nThe algorithm maintains a collection of sites S, and each site si ∈ S maintains a collection of bees, Bi. Each bee is a proxy to the problem domain that we are trying to solve. In our case this is the VRP problem representation covered in Section 4.2.\nInitially each site is seeded, such that each site, si ∈ S, contains a collection of bees Bi, and each bee has a corresponding VRP candidate solution, S. Each candidate solution is initialised by seeding each route with a randomly chosen customer, which is then filled out using the insertion heuristic outlined in Section 4.4.2. Each site is then in turn improved upon. This is achieved by iteratively exploring the neighbourhood of each site. The process used to explore each site is where the majority of the algorithm’s processing takes place and where the interesting aspects of the algorithm come into play. The exploration process is covered in detail in Sections 4.3.1, 4.3.2, and 4.4.\nThe number of sites explored is reduced over the run of the algorithm. This borrows from the idea of a cooling schedule used in Simulated Annealing. Sites are reduced using the formula:\nS = S − sw if i mod λ ≡ 0 (4.5)\nWhere sw represents the worst site, in terms of fitness, i represents the current iteration of the algorithm, and λ represents the period of iterations with which the number of sites are reduced. Once the algorithm is complete the solution, S, with the best"
    }, {
      "heading" : "4.3. ENHANCED BEES ALGORITHM 43",
      "text" : "overall fitness is returned as the answer. In the next section we review in more detail each aspect of the algorithm."
    }, {
      "heading" : "4.3.1 Bee Movement",
      "text" : "Bees are moved around the search space to look for improvements to the collection of candidate solutions being maintained. Each bee represents a candidate solution, S, so a valid bee move is any new candidate solution S′ that can be reached within the neighbourhood of S (see Section 4.4 for the operations under which the neighbourhood is defined).\nA feature of the Enhanced Bees Algorithm is that two Bees cannot occupy the same position. The algorithm maintains a register of the current positions occupied by each bee. We use the current fitness, f(S), as a quick and simple representation of a bee’s current position4. If a bee tries to occupy the same position as another bee (i.e. they share the same candidate solution) then the bee trying to occupy that position is forced to explore the neighbourhood again and find another position.\n4This obviously will not work in circumstances where there is a reasonable likelihood of two candidate solutions, Si and Sj having f(Si) = f(Sj). While this is not the case with the problem instances we have used in this thesis, this may need to be modified if the algorithm is to be used on more general problem instances."
    }, {
      "heading" : "44 CHAPTER 4. ALGORITHM",
      "text" : "Enforcing the constraint that each bee must occupy a unique position has two benefits: it forces diversification between the bees and sites, hence encouraging a greater proportion of the search space to be explored; and it increases the chance of a local optimum being escaped, as a bee ensnared in the local optimum now forces the remainder of the hive to explore alternative positions. This feature has a similar intent and effect to the tabu lists used by Tabu Search.\nAnother feature of the Enhanced Bees Algorithm is the role that sites play in concentrating exploration on certain areas of the search space. Each site maintains a list, M = [S1, . . . ,S ], of the last best positions. Each Si ∈ M is then taken as a launching point for a site’s bees to explore. θ bees are recruited for the exploration of each Si ∈ M . Once all positions in M have been explored then the best positions are again taken and used as the launching points for the site’s next round of exploration. This exploration method has two purposes: firstly, it allows for a simple type of branching, as of the most promising positions that were traversed through on the way to the current position are also explored; secondly, it prevents cycling between promising solutions that are in close vicinity to each other.\nConversely, sites do not interact with each other, as each site maintains its own unique list of promising positions. The constraint that no two bees can occupy the same position ensures that each site covers a non overlapping area of the search space. In practice we have found that this is sufficient to encourage sites to diverge and explore distinct areas of the search space."
    }, {
      "heading" : "4.3.2 Search Space Coverage",
      "text" : "As mentioned, one of the unique aspects of the Bees Algorithm is its ability to produce robust results through probing a large area of the search space. However, this doesn’t work well with hard combinatorial problems, where it cannot be ascertained quickly if an area in the search space shows promise or not.\nTo overcome this limitation we instead use an approach inspired by Simulated Annealing’s use of a cooling schedule. Bees are initially divided equally between each site si ∈ S, ensuring that each site is explored equally. Then every λ period of iterations we reduce the number of sites maintained, such that S = S − sw, where sw is the site with lowest fitness. We measure each site’s fitness from the fitness of its best position found to date.\nThis process continues until a single site remains. We show experimentally in Chapter 5 that this process improves the robustness of the algorithm and produces better results overall than the standard Bees Algorithm."
    }, {
      "heading" : "4.4. SEARCH NEIGHBOURHOOD 45",
      "text" : ""
    }, {
      "heading" : "4.4 Search Neighbourhood",
      "text" : "As already discussed, each bee seeks to improve upon its current fitness by exploring the local neighbourhood of the solution it represents. The Enhanced Bees Algorithm does this by applying a Large Neighbourhood Search (LNS) operator to its candidate solution S. The LNS operator differs from the more common VRP operators in that a single operation applies many changes to the candidate solution S. This widens the neighbourhood of S to encompass exponentially many candidate solutions. LNS navigates through the vast space it spans by selecting only those changes that have a high likelihood of improving the solution.\nThe LNS operation is comprised of two-phases: a destroy phase, and a repair phase. LNS’s destroy phase, when used for the VRP, typically involves removing a proportion of the customers from the solution. The Enhanced Bees Algorithm’s destroy phase uses two heuristics along those lines: a somewhat intelligent heuristic that attempts to remove those customers that are more likely to be able to be recombined in a profitable way; and a simple random selection. These heuristics are covered in more detail in Section 4.4.1. The second phase of LNS is used to repair the partial solution. The Enhanced Bees Algorithm uses a simple insertion heuristic that inserts the customers into those locations that have the lowest insertion cost. This heuristic is covered more formally in Section 4.4.2."
    }, {
      "heading" : "4.4.1 Destroy Heuristic",
      "text" : "The Enhanced Bees Algorithm employs two destroy heuristics. The first simply selects l customers randomly from a solution S and removes these from their routes. The second is slightly more complicated and is due to Shaw [47]. Shaw’s removal heuristic stochastically selects customers such that there is a higher likelihood of customers that are related to one another being removed. For our purposes we define related to mean that for any two customers, vi, vj ∈ V , then either vi, vj are geographically close to one another (i.e. cij is small), or they share an adjacent position within the same route, R = [. . . , vi, vj , . . .].\nThe rationale to removing related customers is that they are the most likely to be profitably exchanged with one another. Conversely, unrelated customers are more likely to be reinserted back into the same positions that they were removed from."
    }, {
      "heading" : "4.4.2 Repair Heuristic",
      "text" : "The repair heuristic used by the Enhanced Bees Algorithm randomly selects one of the removed customers vj , and calculates a cost for reinserting vj between each pair"
    }, {
      "heading" : "46 CHAPTER 4. ALGORITHM",
      "text" : "of jobs vi, vk ∈ Ri for all Ri ∈ S (actually not all reinsertion positions are considered, see Section 4.4.3 for a description of which positions are considered). The reinsertion cost is calculated as follows:\nc∗ = cij + cjk − cik (4.6) cost = c∗ + d(R′)− d(R) + t(R′)− t(R) (4.7)\nWhere c∗ calculates the cost difference in terms of travel distance. R and R′ are defined as the route before and after the customer is inserted, respectively. And functions d(R) and t(R) are all defined as they are in Section 4.2. The final cost is the sum of the added travel distance and the two extra penalties, if the route is now overcapacity or over its maximum duration. The algorithm selects the position with the lowest insertion cost to reinsert the customer. This is repeated until all customers are reinserted into the solution.\nThe reason that customers are reinserted in a random order is that it adds a beneficial amount of noise to the heuristic. This ensures that a healthy diversity of solutions are generated from the heuristic."
    }, {
      "heading" : "4.4.3 Neighbourhood Extent",
      "text" : "We use two techniques to adjust the extent of the neighbourhood being searched. The first technique that we use allows the algorithm some flexibility in selecting infeasible solutions. As can be seen from our formulation of the candidate solutions’ fitness values (see Section 4.2) violations of the problem’s capacity and duration constraints are penalised rather than forbidden. This allows the bees to navigate through infeasible solutions, where other aspects of that solution are sufficiently attractive enough to outweigh the penalties. However, only feasible solutions are allowed to be counted as final solutions returned by the algorithm.\nThe second technique that we use is to adjust the number of insertion positions considered as part of the repair heuristic. The number of insertion positions considered starts with both sides of the three closest customers and increases as the site ages. More formally, let vi ∈ V be the customer that is being inserted. An ordered sequence of candidate insertion points, Lvi = [v1, . . . , vn] such that vj ∈ V − vi, is kept that lists customers in increasing geographic distance from vi, that is cij∀j ∈ V − i. The LNS repair operator tests µ positions from Lvi to find the cheapest insertion point. The repair operator tests both possible insertion points represented by vj ∈ Lvi , that is, it tests both the insertion cost of inserting vi immediately before and after vj in the route R that contains vj ."
    }, {
      "heading" : "4.4. SEARCH NEIGHBOURHOOD 47",
      "text" : "For each site si ∈ S we also maintain a counter ai, i ∈ S that denotes the age of the site. A site’s age is incremented for each iteration that a site doesn’t improve upon its currently best known solution (as defined by the solution’s fitness, f(S)). Whenever a site improves upon its best known solution then the counter is reset, such that ai = 0.\nWe then use the following formula to increase how much of Lvi is considered as the site ages.\nµ = |Lvi |min (ai k , 1 )\nWhere k is a constant that controls the rate at which the search area is expanded.\nAs this process extends the number of insertion positions that are considered by the repair heuristic, this also serves to extend the neighbourhood of solutions surrounding a candidate solution S. In this way the algorithm also dynamically extends the size of a neighbourhood surrounding a site, si ∈ S, if si becomes stuck in a local optimum.\n48 CHAPTER 4. ALGORITHM\nChapter 5\nResults\nIn this chapter we provide a detailed breakdown of the results obtained by the Enhanced Bees Algorithm. The algorithm is tested against the well known set of test instances from Christofides, Mingozzi and Toth [11]. We start in Section 5.1 by presenting the results obtained by running the algorithm in its two standard configurations: the first configuration is optimised to produce the best overall results, regardless of the runtime performance; the second configuration is optimised to produce the best results possible within a 60 second runtime threshold. We follow this in Section 5.2 by providing contrasting results on the same problem instances instead using a standard Bees Algorithm and a LNS local search. Here we aim to demonstrate that the enhancements suggested in this thesis do in fact improve the solution quality. Finally, we end in Section 5.3 by comparing and ranking how the Enhanced Bees Algorithm performs compared to the results in the literature."
    }, {
      "heading" : "5.1 Enhanced Bees Algorithm",
      "text" : "The results depicted in Figures 5.1, 5.2, 5.3 and 5.4 show the algorithm’s performance on two configurations. The first configuration is optimised to produce the best overall results, that is, the minimum travel distance that meets all capacity and duration constraints. No consideration is made for the algorithm’s runtime performance in this configuration. This configuration is denoted as Best in the following diagrams and tables. The second configuration is optimised to produce the best results possible within a 60 second runtime window. This configuration is denoted as Fast in the following diagrams and tables.\nThe results depicted in Figures 5.1, 5.2, 5.3 and 5.4 are for the standard 14 VRP problem instances due to Christofides, Mingozzi and Toth [11] and were all obtained\n49\n50 CHAPTER 5. RESULTS\non a MacBook Pro 2.8 GHz Intel Core 2 Duo. The best result for each problem instance was selected from 10 runs of the algorithm.\nIn Figures 5.1 and 5.2 (i.e. the Best configuration) the algorithm was set to start with |S| = 100 (i.e. 100 sites), and reduce this number each 50 iterations (λ = 50) by 1% until only |S| = 3. The number of promising solutions remembered by each site was set to |M | = 5. The algorithm was left to run for 30 minutes on each problem before being terminated. Infeasible solutions (i.e. solutions over their capacity or duration constraints) were allowed to be traversed through, but were scored as being 0% of the best known solution.\nConversely, in Figures 5.3 and 5.4 (i.e. the Fast configuration) the algorithm was set to start with |S| = 25 (i.e. 25 sites), and reduce this number each iteration (λ = 1) by 1% until only |S| = 1. The number of promising solutions remembered by each site was set to |M | = 5. The algorithm was left to run for 60 seconds on each problem before\n5.2. EXPERIMENTS 51\nbeing terminated. The LNS improvement heuristic was set to destroy between 0% and 80% (with a mean of 40%) of the solution at each step. The repair operator initially only considers the first 3 closest customers as reinsertion points, but it increases this to 50% of the closest customers as the site ages. Infeasible solutions (i.e. solutions over duration or service time constraints) were allowed to be traversed through, but were scored as being 0% of the best known solution.\nTable 5.1 provides a tabular summary of the results covered in this section."
    }, {
      "heading" : "5.2 Experiments",
      "text" : "In this section we review the results obtained by implementing the standard Bees Algorithm and a LNS local search. The aim of these experiments is to prove that the algorithmic enhancements suggested in this thesis do in fact produce better results than would have been obtained if we had used a standard Bees Algorithm. We also demonstrate that the combination of the Bees Algorithm with the LNS local search produces better results than if either algorithm were used separately.\n52 CHAPTER 5. RESULTS"
    }, {
      "heading" : "5.2.1 Bees Algorithm versus Enhanced Bees Algorithm",
      "text" : "We start in Figures 5.5 and 5.6 by showing the results obtained by using the standard Bees Algorithm as described by Pham et al. in [38]. The same problem instances as Section 5.1 are used, so that the results can be compared directly.\nThe results depicted in Figures 5.5 and 5.6 were obtained on a MacBook Pro 2.8 GHz Intel Core 2 Duo. The best result for each instance was selected from 10 runs of the algorithm. The algorithm was configured with the following parameters. It used 25 sites, and selected the best 6 sites as being elite. Each elite site had 3 bees recruited for the search. Another 6 sites were selected as being non-elite, and had 2 bees recruited for the search. The bees from the remaining 13 sites were left to search randomly. The algorithm was left to run for 60 seconds on each problem before being terminated. Infeasible solutions (i.e. solutions over their capacity or duration constraints) were allowed to be traversed through, but were scored as being 0% of the\n5.2. EXPERIMENTS 53\nbest known solution. A λ-interchange (with λ = 2) improvement heuristic was used for the improvement phase of each bee (see Chapter 2 for an overview on how this heuristic works)."
    }, {
      "heading" : "5.2.2 Large Neighbourhood Search",
      "text" : "Next we show in Figures 5.7 and 5.8 the results obtained by using a standalone LNS search embedded within a hill climb meta-heuristic. The LNS search used in this section is the same one that is employed by the Enhanced Bees Algorithm. It should be noted that there are more sophisticated LNS algorithms available than the comparatively simple one used here. And that these would most probably produce better results than the LNS results presented here. However, we believe one of the attractive features of the Enhanced Bees Algorithm is that it uses a fairly simple local search procedure. Moreover, our aim in this experiment was to demonstrate that the limitations of our simple local method are offset by it being embedded within a Bees Algorithm.\nThe results depicted in Figures 5.7 and 5.8 were obtained on a MacBook Pro 2.8 GHz Intel Core 2 Duo. The best result for each problem instance was selected from 10 runs"
    }, {
      "heading" : "54 CHAPTER 5. RESULTS",
      "text" : "of the algorithm. The algorithm was initialised to a starting position generated by a simple insertion heuristic. The LNS heuristic was set to destroy between 0% and 80% (with a mean of 40%) of the solution at each step. The repair enumerated all customers when deciding the best reinsertion point. Infeasible solutions (i.e. solutions over their capacity or duration constraints) were allowed to be traversed through, but were scored as being 0% of the best known solution."
    }, {
      "heading" : "5.2.3 Summary",
      "text" : "Table 5.2 provides a summary of the results obtained by the Bees Algorithm and LNS experiments, alongside the results obtained by our Enhanced Bees Algorithm. The Bees Algorithm is the worst of the three. This is not surprising given that the Bees Algorithm was devised to solve continuous problems, rather than discrete problems.\n5.2. EXPERIMENTS 55\nWhere the Bees Algorithm has been used for discrete problems in the literature it has been adapted to incorporate more sophisticated local search techniques, much as the Enhanced Bees Algorithm has been here. Also of note is that two of the problem instances didn’t produce feasible solutions at all within the 60 second runtime threshold. We believe that given a longer running time the algorithm would most probably have found a feasible solution. However, one of the objectives of the Enhanced Bees Algorithm is to produce robust results reliably; in this count the standard Bees Algorithm is not competitive.\nThe LNS improvement heuristic produced much stronger results. This shows that the LNS improvement plays an important part in the results obtained by the Enhanced Bees Algorithm. The LNS heuristic is a fairly new heuristic (in the VRP research at least); nevertheless it has produced some of the most competitive results. This is borne out by the results obtained here and in Section 5.1. However, the LNS local search did fail to find a feasible solution for one of the problem instances. Again this\n56 CHAPTER 5. RESULTS\nis probably due to the limited runtime permitted. If this problem instance is removed from the results, then the LNS search’s average result becomes 96.39%, getting us much closer to the results obtained by the Enhanced Bees Algorithm."
    }, {
      "heading" : "5.3 Comparison",
      "text" : "Lastly, in Table 5.3 and Figure 5.9 we provide a comparison of the Enhanced Bees Algorithm along with other well known results from the literature. As can be seen from Table 5.3 and Figure 5.9 some of the best results known are due to Taillard’s Tabu Search heuristic. He reaches 12 of the best known solutions from the set of 14 problems. The Enhanced Bees Algorithm, by comparison, finds 8 of the 14 best known solutions. However, the Enhanced Bees Algorithm is still very competitive. The runtime duration required to find a best known solution is smaller than many of the other meta-heuristics (although a direct comparison is hard to make as many of the reported results were run on significantly older hardware). Additionally, the solutions produced by the Enhanced Bees Algorithm are within 0.5% of the best known\n5.3. COMPARISON 57\nsolutions on average, meaning that the algorithm is very competitive with the best meta-heuristics available for the VRP.\n58 CHAPTER 5. RESULTS"
    }, {
      "heading" : "5.3. COMPARISON 59",
      "text" : ""
    }, {
      "heading" : "60 CHAPTER 5. RESULTS",
      "text" : "T a b le 5 .3 : R esu lts C om p ariso n . G iven is a co m pa riso n o f th e E n h a n ced B ees A lgo rith m a lo n gsid e o th er w ell kn o w n resu lts fro m th e litera tu re. T h e resu lts a re fo r th e sta n d a rd 1 4 C h risto fi d es, M in go zzi a n d T o th p ro blem in sta n ces co m m o n ly u sed in th e V R P litera tu re. R u n n in g tim es a re given in pa ren th esis w h ere kn o w n .\nIn sta n ce\nC W\n1 S w eep\n2 G en A sgn 3\n3 -O p t 4\nS A\n5 T S 6\nA C O 7\nE B A 8\nB est K n ow n 9\nP 0 1 E 5 1K 05 5 84.64\n532 524\n57 8 .5 6 5 2 8 (1 6 7 )\n5 2 4 .6 1 (360)\n5 2 4 .6 1 (6)\n5 2 4 .6 1 (5) 524.61 P 02E 76 K 1 0 90 0 .2 6 8 7 4 857 88 8 .0 4 8 3 8 .6 2 (6 4 3 4 ) 8 3 5 .2 6 (3228) 844.31 (78) 8 3 5 .2 6 (225) 8 35.26 P 03E 10 1 K 08 8 8 6.83 8 51 833 8 7 8 .7 0 8 2 9 .1 8 (9 3 3 4 ) 8 2 6 .1 4 (1104) 832.32 (228) 8 2 6 .1 4 (50) 826.14 P 04E 15 1 K 12 11 3 3.43 1 0 79 1014 112 8 .2 4 1 0 5 8 (5 0 1 2 ) 1 0 2 8 .4 2 (3528) 1061.55 (1104) 1036.12 (1215) 1028.42 P 05E 20 0 K 17 13 9 5.74 1 3 89 1420 138 6 .8 4 1 3 7 8 (1 2 9 1 ) 1 2 9 8 .7 9 (5454) 1343.46 (5256) 1327.48 (785) 1291.45 P 06D 5 1K 06 6 18.40 560 560 61 6 .6 6 5 5 5 .4 3 (3 4 1 0 ) 5 5 5 .4 3 (810) 560.24 (6) 5 5 5 .4 3 (5) 555.43 P 07D 7 6K 11 9 75.46 933 916 97 4 .7 9 9 0 9 .6 8 (6 2 6 ) 9 0 9 .6 8 (3276) 916.21 (102) 9 0 9 .6 8 (175) 909.68 P 08D 1 01K 0 9 9 73.94 888 885 96 8 .7 3 8 6 6 .7 5 (9 5 7 ) 8 6 5 .9 4 (1536) 866.74 (288) 8 6 5 .9 4 (20) 865.94 P 09D 1 51K 1 4 1 287 .6 4 123 0 1 230 128 4 .6 4 1 1 6 4 .1 2 (8 4 3 0 1 ) 1 1 6 2 .5 5 (4260) 1195.99 (1650) 1169.24 (1610) 1162.55 P 10D 2 00K 1 8 1 538 .6 6 151 8 1 518 153 8 .6 6 1 4 1 7 .8 5 (5 7 0 8 ) 1 3 9 7 .9 4 (5988) 1451.65 (4908) 1428.54 (1540) 1395.85 P 11E 12 1 K 07 10 7 1.07 1 2 66 - 104 9 .4 3 1 1 7 6 (3 1 5 ) 1 0 4 2 .1 1 (1332) 1065.21 (552) 1048.24 (960) 1042.11 P 12E 10 1 K 10 8 3 3.51 9 37 824 8 2 4 .4 2 8 2 6 (6 3 2 ) 8 1 9 .5 6 (960) 8 1 9 .5 6 (300) 8 1 9 .5 6 (35) 819.56 P 13D 1 21K 1 1 1 596 .7 2 177 6 - 158 7 .9 3 1 5 4 5 .9 8 (7 6 2 2 ) 1 5 4 1 .1 4 (3552) 1559.92 (660) 1545.19 (1500) 1541.14 P 14D 1 01K 1 1 8 75.75 949 876 86 8 .5 0 8 9 0 (3 0 5 ) 8 6 6 .3 7 (3942) 867.07 (348) 8 6 6 .3 7 (40) 866.37\n1 C la rk W rite’s S av in g s (P a ra llel) a lg o rith m . Im p lem en ted b y L a p o rte a n d S em et [3 0 ]. 2 S w eep A lg o rith m d u e to G illett a n d M iller [2 2 ]. Im p lem en ted b y C h risto fi d es, M in\ng o zzi a n d T o th [1 1 ]. R ep o rted in [3 0 ].\n3 G en era lised A ssig n m en t d u e to F ish er a n d J a ik u m a r [1 7 ]. R ep o rted in [3 0 ]. 4 3 -O p t lo ca l sea rch a p p lied a fter C la rk W rite’s S av in g s (P a ra llel) a lg o rith m . F irst im p rov em en t ta k en . Im p lem en ted b y L a p o rte a n d S em et [3 0 ]. 5 S im u la ted A n n ea lin g d u e to O sm a n [3 6 ]. R u n tim e d u ra tio n is g iv en in p a ren th eses a n d is rep o rted in seco n d s o n a V A X 8 6 0 0 0 . 6 T a b u S ea rch d u e to T a illa rd [4 9 ]. R u n tim e d u ra tio n is g iv en in p a ren th eses a n d is rep o rted in seco n d s o n a S illico n G ra p h ics W o rk sta tio n , 3 6 M h z. 7 A n t C o lo n y O p tim isa tio n B u lln h eim er, H a rtl, S tra u ss [9 ]. B u lln h eim er et a l. p rov id ed tw o p a p ers o n A n t C o lo n y O p tim isa tio n fo r V R P , th e b\netter o f th e tw o is u sed . R u n tim e d u ra tio n is g iv en in p a ren th eses a n d is rep o rted in seco n d s o n a P en tiu m 1 0 0 .\n8 E n h a n ced B ees A lg o rith m . R esu lts a re sh ow n fro m th e best co n fi g u ra tio n in S ectio n 5 .1 . R u n tim e d u ra tio n is g iv en in p a ren th eses a n d is rep o rted in seco n d s o n a M a cB o o k P ro 2 .8 G H z In tel C o re 2 D u o . 9 B est k n ow n resu lts a s rep o rted b y G en d rea u , L a p o rte, a n d P o tv in in [2 1 ].\n5.3. COMPARISON 61\n62 CHAPTER 5. RESULTS\nChapter 6\nConclusion\nIn this thesis we have described a new meta-heuristic for the VRP called the Enhanced Bees Algorithm. The results obtained are competitive with the best meta-heuristics available for the Vehicle Routing Problem. Additionally, the algorithm has good runtime performance, producing results within 2% of the optimal solution within 60 seconds.\nWe took the Bees Algorithm as our starting point. The Bees Algorithm was originally developed for solving continuous optimisation problems, so part of the work undertaken for this thesis was to adapt it for use on the VRP (although, it could be argued that the Enhanced Bees Algorithm is more ‘inspired by’ than ‘adapted from’). The approach developed for the Enhanced Bees Algorithm could equally be applied to other combinatorial optimisation problems, such as the Traveling Salesman Problem, the Job Shop Scheduling Problem, or the Cutting Stock Problem.\nWe showed empirically that the quality of the solutions obtained by the Enhanced Bees Algorithm are competitive with the best modern meta-heuristics available for the VRP. Additionally, we showed that the algorithm has good runtime performance, producing results within 2% of the optimal solution within 60 seconds. The runtime performance of the algorithm makes it suitable for use within real world dispatch scenarios, where often the dispatch process is fluid and hence it is impractical for the optimisation to take minutes (or hours). In these environments, it is acceptable to trade a fraction of a percent off the solution quality for quicker runtime performance.\nWe also gave results that demonstrated that the algorithmic enhancements suggested in this thesis did in fact produce better results than would have been obtained by using a standard Bees Algorithm. We also demonstrated that the combination of the Bees Algorithm with a LNS local search heuristic produced better results than if the algorithms had been used separately.\n63"
    }, {
      "heading" : "64 CHAPTER 6. CONCLUSION",
      "text" : "Additionally, we provided a comprehensive survey of the VRP literature. In this we provided a short history of the results that have been foundational to VRP research, as well as providing in depth material and descriptions for some of the classic algorithms developed for the VRP.\nThere are a number of areas where the research undertaken in this thesis could be continued. These include:\n• Introduce a mating process to generate new sites. An interesting extension to the Enhanced Bees Algorithm would be to incorporate a crossover operation for generating new sites. In our version of the Enhanced Bees Algorithm unpromising sites are simply culled off. An alternative approach, borrowing a concept from Genetic Algorithms and Tabu Search (in this case, Taillard’s Adaptive Memory), would be to replace the site with a recombination of two other successful sites. The advantage to this approach is that it would open a new area of the search space to exploration; an area that should be promising as it combines components from two already successful sites. The crossover process could be as simple as using an existing VRP crossover operator, such as OX, on the two fittest solutions from each site. Although much more sophisticated crossover operations are imaginable.\n• Extend to other combinatorial problems. It should be possible to follow a similar approach that we have taken in adapting the Bees Algorithm to the VRP and apply this to other combinatorial problems. The Bees Algorithm is especially strong at providing robust solutions where the search space contains many local optima. We imagine that there are many other combinatorial problems where this would be an advantage. An obvious starting point would be to apply it to the Job Shop Scheduling Problem, which shares many characteristics with the VRP.\n• Include more real world constraints. Although the literature is filled with variations of the VRP that add additional constraints (i.e. VRPTW, PDP, etc), the focus has been on tackling the computationally hard constraints, such as time windows, multiple deploys, etc. An area that is not often addressed in the literature is how to deal with soft constraints, such as, on the day disruptions, differing skill sets across a fleet, or factoring in a dispatcher’s assignment preferences. Unfortunately, these constraints are a barrier to vehicle route optimisation being adopted in many logistics companies.\nAn interesting line of research would be to extend the optimisation methods developed for the VRP to include feedback from a dispatcher—we envision an interactive process where the dispatcher can feedback into the optimisation process the soft constraints that are not modelled within the algorithm. It would also be very interesting to see if the supervised learning methods from artificial intelligence, such as the Naive Bayes classifier, could be incorporated into the optimisation process."
    }, {
      "heading" : "66 BIBLIOGRAPHY",
      "text" : "[13] Dantzig, Fulkerson, and Johnson. Solution of a large-scale traveling salesman problem. Operations Research, (2):393410, 1954.\n[14] G. B. Dantzig and J. H. Ramser. The truck dispatching problem. Management Science, 6(1):80–91, Oct, 1959.\n[15] Martin Desrochers, Jacques Desrosiers, and Marius Solomon. A new optimization algorithm for the vehicle routing problem with time windows. Operations Research, 40:342–354, March 1992.\n[16] S. Eilon, C.D. Watson-Gandy, and N. Christofides. Distribution management: Mathematical modelling and practical analysis. Griffin (London), 1971.\n[17] M.L. Fisher and R. Jaikumar. A generalized assignment heuristic for solving the vrp. Networks, 11:109–124, 1981.\n[18] B. A. Foster and D.M. Ryan. An integer programming approach to the vehicle scheduling problem. Operations Research, 27:367384, 1976.\n[19] T. Gaskell. Bases for vehicle fleet scheduling. Operational Research Quarterly, 18:281–295, 1967.\n[20] M. Gendreau, A. Hertz, and G. Laporte. A tabu search heuristic for the vehicle routing problem. Management Science, 40:1276–1290, 1994.\n[21] M Gendreau, G Laporte, and J Potvin. Metaheuristics for the vehicle routing problem. Technical report, Les Cahiers du Gerad, 1998, revised 1999.\n[22] B.E. Gillett and L.R. Miller. A heuristic algorithm for the vehicle dispatch problem. Operations Research, 22:340–349, 1974.\n[23] M Haimovich and A H G R Kan. Bounds and heuristics for capacitated routing problems. Mathematics of Operations Research, 10:527–542, 1985.\n[24] W.R. Hamilton. Memorandum respecting a new system of roots of unity (the icosian calculus). Philosophical Magazine, 12, 1856.\n[25] J. H. Holland. Adaptation in natural and artificial systems. The University of Michigan Press, 1975.\n[26] R. Karp. Reducibility among combinatorial problems. In R. Miller and J. Thatcher, editors, Complexity of Computer Computations, pages 85–103. Plenum Press, 1972.\n[27] T.P. Kirkman. On the representation of polyhedra. Philosophical Transactions of the Royal Society of London Series A, 146:413–418, 1856.\n[28] G. Laporte, H. Mercure, and Y. Nobert. An exact algorithm for the asymmetrical capacitated vehicle routing problem. Networks, 16:33–46, 1986.\n[29] G. Laporte and Y. Nobert. Surveys in Combinatorial Optimization, chapter Exact algorithms for the vehicle routing problem. 1987.\nBIBLIOGRAPHY 67\n[30] G. Laporte and F. Semet. Classical heuristics for the vehicle routing problem. Technical report, Les Cahiers du Gerad, 1999.\n[31] Chee Peng Lim, Lakhmi C. Jain, and Satchidananda Dehuri, editors. Innovations in Swarm Intelligence, volume 248 of Studies in Computational Intelligence. Springer, 2009.\n[32] Y. Nagata. Edge assembly crossover for the capacitated vehicle routing problem. EvoCOP, LNCS, 4446:142–153, 2007.\n[33] Road Transport Forum NZ. Road transport forum nz—transport facts. Website. http://www.rtfnz.co.nz/cm-transport-facts.php.\n[34] I.M Oliver, D.J. Smith, and J.R.C Holland. A study of permutation crossover operators on the traveling salesman problem. Grefenstette—Proceedings of the Second International Conference on Genetic Algorithms and Their Applications, pages 224–230, 1987.\n[35] I Or. Traveling salesman-type combinatorial problems and their relation to the logistics of regional blood banking. PhD thesis, Evanston, IL: Northwestern University, 1976.\n[36] I. H. Osman. Metastrategy simulated annealing and tabu search algorithm for the vehicle routing problem. Annals of Operations Research, 41:421–451, 1993.\n[37] H. Paessens. The savings algorithm for the vehicle routing problem. European Journal of Operational Research, 34:336–344, 1988.\n[38] Pham, Ghanbarzadeh, Koc, S. Otri, S. Rahim, and M. Zaidi. The bees algorithm. Technical report, 2005.\n[39] Jean-Yves Potvin. A review of bio-inspired algorithms for vehicle routing. In Bio-inspired Algorithms for the Vehicle Routing Problem, pages 1–34. 2009.\n[40] M. R. Rao and S. Zionts. Allocation of transportation units to alternative tripsa column generation scheme with out-of-kilter subproblems. Operations Research, 16:52–63, 1968.\n[41] M. Reimann, M. Stummer, and K. Doerner. A savings-based ant system for the vehicle routing problem. Proceedings of the Genetic and Evolutionary Computation Conference, pages 1317–1325, 2002.\n[42] J Renaud, F F Boctor, and G Laporte. A fast composite heuristic for the symmetric traveling salesman problem. INFORMS Journal on Computing, 8:134–143, 1996.\n[43] J. Robinson. On the hamiltonian game (a traveling salesman problem). Research Memorandum RM-303, 1949.\n[44] F. Robuste, C.F. Daganzo, and R. Souleyrette II. Implementing vehicle routing models. Transportation Research, 24B:263–286, 1990."
    }, {
      "heading" : "68 BIBLIOGRAPHY",
      "text" : "[45] Stefan Ropke. Heuristic and exact algorithms for vehicle routing problems. PhD thesis, Department of Computer Science, University of Copenhagen (DIKU), 2005.\n[46] Alexander Schrijver. On the history of combinatorial optimization (till 1960). In Operations Research and Management. Elsevier, 2005.\n[47] P. Shaw. Using constraint programming and local search methods to solve vehicle routing problems. pages 417–431. 1998.\n[48] M. M. Solomon. Algorithms for the vehicle routing and scheduling problems with time window constraints. Oper. Res., 35(2):254–265, 1987.\n[49] E. D. Taillard. Parallel iterative search methods for vehicle routing problems. Networks, 23:661–673, 1993.\n[50] S.R. Thangiah, K.E. Nygard, and P.L. Juell. Gideon: A genetic algorithm system for vehicle routing with time windows. Proceedings of 7th IEEE Conference on Artificial Intelligence Applications, pages 322–328, 1991.\n[51] P. Toth and D. Vigo. The granular tabu search (and its application to the vehicle routing problem). Technical report, 1998.\n[52] Paolo Toth and Daniele Vigo, editors. The vehicle routing problem. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2001.\n[53] J. A. G. Willard. Vehicle routing using r-optimal tabu search. Master’s thesis, The Management School, Imperial College, London, 1989.\n[54] A. Wren and A. Holliday. Computer scheduling of vehicles from one or more depots to a number of delivery points. Operations Research Quarterly, pages 333–344, 1972.\n[55] P. Yellow. A computational modification to the savings method of vehicle scheduling. Operational Research Quarterly, 21:281–283, 1970."
    } ],
    "references" : [ {
      "title" : "Classical heuristics for the vehicle routing problem",
      "author" : [ "G. Laporte", "F. Semet" ],
      "venue" : "Technical report, Les Cahiers du Gerad,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1999
    }, {
      "title" : "Innovations in Swarm Intelligence, volume 248 of Studies in Computational Intelligence",
      "author" : [ "Chee Peng Lim", "Lakhmi C. Jain", "Satchidananda Dehuri", "editors" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2009
    }, {
      "title" : "Edge assembly crossover for the capacitated vehicle routing problem",
      "author" : [ "Y. Nagata" ],
      "venue" : "EvoCOP, LNCS,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2007
    }, {
      "title" : "A study of permutation crossover operators on the traveling salesman problem",
      "author" : [ "I.M Oliver", "D.J. Smith", "J.R.C Holland" ],
      "venue" : "Grefenstette—Proceedings of the Second International Conference on Genetic Algorithms and Their Applications,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1987
    }, {
      "title" : "Traveling salesman-type combinatorial problems and their relation to the logistics of regional blood banking",
      "author" : [ "I Or" ],
      "venue" : "PhD thesis, Evanston, IL: Northwestern University,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1976
    }, {
      "title" : "Metastrategy simulated annealing and tabu search algorithm for the vehicle routing problem",
      "author" : [ "I.H. Osman" ],
      "venue" : "Annals of Operations Research,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 1993
    }, {
      "title" : "The savings algorithm for the vehicle routing problem",
      "author" : [ "H. Paessens" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1988
    }, {
      "title" : "The bees algorithm",
      "author" : [ "Pham", "Ghanbarzadeh", "Koc", "S. Otri", "S. Rahim", "M. Zaidi" ],
      "venue" : "Technical report,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2005
    }, {
      "title" : "A review of bio-inspired algorithms for vehicle routing. In Bio-inspired Algorithms for the Vehicle Routing Problem, pages",
      "author" : [ "Jean-Yves Potvin" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2009
    }, {
      "title" : "Allocation of transportation units to alternative tripsa column generation scheme with out-of-kilter subproblems",
      "author" : [ "M.R. Rao", "S. Zionts" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 1968
    }, {
      "title" : "A savings-based ant system for the vehicle routing problem",
      "author" : [ "M. Reimann", "M. Stummer", "K. Doerner" ],
      "venue" : "Proceedings of the Genetic and Evolutionary Computation Conference,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2002
    }, {
      "title" : "A fast composite heuristic for the symmetric traveling salesman problem",
      "author" : [ "J Renaud", "F F Boctor", "G Laporte" ],
      "venue" : "INFORMS Journal on Computing,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 1996
    }, {
      "title" : "On the hamiltonian game (a traveling salesman problem)",
      "author" : [ "J. Robinson" ],
      "venue" : "Research Memorandum RM-303,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 1949
    }, {
      "title" : "Implementing vehicle routing models",
      "author" : [ "F. Robuste", "C.F. Daganzo", "R. Souleyrette II" ],
      "venue" : "Transportation Research,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 1990
    }, {
      "title" : "Heuristic and exact algorithms for vehicle routing problems",
      "author" : [ "Stefan Ropke" ],
      "venue" : "PhD thesis, Department of Computer Science, University of Copenhagen (DIKU),",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2005
    }, {
      "title" : "On the history of combinatorial optimization (till 1960)",
      "author" : [ "Alexander Schrijver" ],
      "venue" : "In Operations Research and Management. Elsevier,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2005
    }, {
      "title" : "Using constraint programming and local search methods to solve vehicle routing problems",
      "author" : [ "P. Shaw" ],
      "venue" : null,
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 1998
    }, {
      "title" : "Algorithms for the vehicle routing and scheduling problems with time window constraints",
      "author" : [ "M.M. Solomon" ],
      "venue" : "Oper. Res.,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 1987
    }, {
      "title" : "Parallel iterative search methods for vehicle routing problems",
      "author" : [ "E.D. Taillard" ],
      "venue" : "Networks, 23:661–673,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 1993
    }, {
      "title" : "Gideon: A genetic algorithm system for vehicle routing with time windows",
      "author" : [ "S.R. Thangiah", "K.E. Nygard", "P.L. Juell" ],
      "venue" : "Proceedings of 7th IEEE Conference on Artificial Intelligence Applications,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 1991
    }, {
      "title" : "The granular tabu search (and its application to the vehicle routing problem)",
      "author" : [ "P. Toth", "D. Vigo" ],
      "venue" : "Technical report,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1998
    }, {
      "title" : "The vehicle routing problem",
      "author" : [ "Paolo Toth", "Daniele Vigo", "editors" ],
      "venue" : "Society for Industrial and Applied Mathematics,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2001
    }, {
      "title" : "Vehicle routing using r-optimal tabu search",
      "author" : [ "J.A.G. Willard" ],
      "venue" : "Master’s thesis,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 1989
    }, {
      "title" : "Computer scheduling of vehicles from one or more depots to a number of delivery points",
      "author" : [ "A. Wren", "A. Holliday" ],
      "venue" : "Operations Research Quarterly,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 1972
    }, {
      "title" : "A computational modification to the savings method of vehicle scheduling",
      "author" : [ "P. Yellow" ],
      "venue" : "Operational Research Quarterly,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 1970
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "In this thesis we present a new algorithm for the Vehicle Routing Problem called the Enhanced Bees Algorithm. It is adapted from a fairly recent algorithm, the Bees Algorithm, which was developed for continuous optimisation problems. We show that the results obtained by the Enhanced Bees Algorithm are competitive with the best meta-heuristics available for the Vehicle Routing Problem—it is able to achieve results that are within 0.5% of the optimal solution on a commonly used set of test instances. We show that the algorithm has good runtime performance, producing results within 2% of the optimal solution within 60 seconds, making it suitable for use within real world dispatch scenarios. Additionally, we provide a short history of well known results from the literature along with a detailed description of the foundational methods developed to solve the Vehicle Routing Problem.",
    "creator" : "LaTeX with hyperref package"
  }
}