{
  "name" : "1611.05239.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "How to do lexical quality estimation of a large OCRed historical Finnish newspaper collection with scarce resources",
    "authors" : [ "Kimmo Kettunen", "Tuula Pääkkönen" ],
    "emails" : [ "kimmo.kettunen@helsinki.fi,", "tuula.paakkonen@helsinki.fi" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Digitization of both hand-written and printed historical material during the last 10–15 years has been an ongoing academic and non-academic industry. Most probably this activity will only increase in the current Digital Humanities era. As a result of past and current work we have lots of digital historical document collections available and will have more of them in the future.\nThe National Library of Finland has digitized a large proportion of the historical newspapers published in Finland between 1771 and 1910 (Bremer-Laamanen 2001, 2005, 2014; Kettunen et al. 2014). This collection contains approximately 1.95 million pages in Finnish and Swedish. Finnish part of the collection consists of about 2.40 billion words. The National Library’s Digital Collections are offered via the digi.kansalliskirjasto.fi web service, also known as Digi. Part of the newspaper material (years 1771–1874) is also available freely downloadable in The Language Bank of Finland provided by the FIN-CLARIN consortium 1 . The collection can also be accessed through the Korp 2 environment that has been developed by Språkbanken at the University of Gothenburg and extended by FIN-CLARIN team at the University of Helsinki to provide concordances of text resources. A Cranfield style information retrieval test collection has also been produced out of a small part of the Digi newspaper material at the University of Tampere (Järvelin et al. 2015). An open data package of the whole collection will be released during the year 2016 (Pääkkönen et al., 2016)\nThe web service digi.kansalliskirjasto.fi contains different material besides newspapers, including journals, and ephemera (different small prints). Recently a new service was created: it enables marking of clips and storing of them to a personal scrapbook. The user can also save links to his search keys and results in an Excel file. The web service is used, for example, by genealogists, heritage societies, researchers, and history enthusiast laymen (Hölttä, 2016). There is also an\n1 https://kitwiki.csc.fi/twiki/bin/view/FinCLARIN/KielipankkiAineistotDigilibPub 2 https://korp.csc.fi/\nincreasing desire to offer the material more widely for educational use. In 2015 the service had over 14 million page loads. User statistics of year 2014 show that about 88.5 % of the usage of the Digi comes from Finland, but a 11.5 % share of use is coming outside of Finland.\nQuality of OCRed collections is an important topic in digital humanities, as it affects general usability and searchability of collections. There is no single available method to assess quality of large collections, but different methods can be used to approximate quality. This paper discusses different corpus analysis style methods to approximate overall lexical quality of the Finnish part of the Digi collection. Methods include usage of parallel samples and word error rates, usage of morphological analysers, frequency analysis of words and comparisons to comparable edited lexical data. Our aim in the quality analysis is twofold: firstly to analyse the present state of the lexical data and secondly, to establish a set of assessment methods that build up a compact procedure for quality assessment after e.g. re-OCRing or post-correction of the material. In the discussion part of the paper we shall synthesise results of our different analyses.\nKeywords: OCR quality, lexical quality estimation, 19 th century Finnish newspaper collection\n1. Introduction\nNewspapers of the 19 th and early 20 th century were mostly printed in the Gothic (Fraktur, blackletter) typeface in Europe. It is well known that the typeface is difficult to recognize for OCR software (Holley 2008; Furrer and Volk 2011; Volk et al. 2011). Other aspects that affect the quality of the OCR recognition are the following, among others (cf. Holley 2008; Klijn 2008, for a more detailed list):\n quality of the original source and microfilm\n scanning resolution and file format\n layout of the page\n OCR engine training\n unknown fonts\n etc.\nAs a result of these difficulties scanned and OCRed document collections have a varying amount of errors in their content. A quite typical figure is that of the 19 th Century Newspaper Project of the British Library (Tanner et al. 2009): they report that 78 % of the words in the collection are correct.\nThis kind of quality is not very good, but quite realistic. The amount of errors depends heavily on the period and printing form of the original data. Older newspapers and magazines are more difficult for OCR; newspapers from the early 20 th century are easier (cf. for example data of Niklas 2010 that consists of a 200 year period of The Times of London from 1785 to 1985). There is no clear measure of the amount of errors that makes OCRed material useful or less useful for some purpose, and the use purposes and research tasks of the users of digitized material vary hugely (Traub et al. 2015). A linguist who is interested in the forms of words needs as errorless data as possible; a historian who interprets texts on a more general level may be satisfied with text data that has more errors. In any case, the quality of the OCRed word data is of crucial importance.\nDigital collections may be small, medium sized or large and different methods of quality assessment are useful or practical for different sizes of collections. Smallish and perhaps even medium sized collections may be assessed and corrected intellectually, by human inspection (cf. Strange et al. 2014). When size of the collection increases, human inspection becomes impossible, or human inspection can only be used to assess samples of the collection. In our case, the size of the collection makes comprehensive human inspection impossible: almost 2 million newspaper pages of varying quality cannot be assessed by human labour.\nThus quality assessment of OCRed collections is most of the times sample-based, as in the case of the British Library's 19 th Century Newspapers Database (Tanner et. al. 2009) 3 . A representative part of the collection is assessed e.g. by using a parallel digital clean collection, when such is available or can be produced cost effectively. Word and character level comparisons can then be made and error rates of the OCRed collections can be reported and compared. Holley (2008) gives the following, mainly practical, OCR quality accuracy figures and quality estimations that are based on discussions with OCR contractors and academic librarians:\nGood OCR accuracy 98–99% accurate (1–2% of OCR incorrect) Average OCR accuracy 90–98% accurate (2–10% of OCR incorrect) Poor OCR accuracy below 90% accurate (more than 10% of OCR incorrect) 4\n3 “To discover the actual OCR accuracy of the newspaper digitization program at the BL we sampled a significant proportion (roughly 1%) of the total 2 million plus pages...” This kind of approach where a clean parallel data for the OCRed sample is produced in house or by a contractor, is beyond our means. 4 Unfortunately it is not clear, whether accuracy here is on character or word level, but for the sake of discussion we’ll suggest that the figures are word level accuracy figures, as even high character level accuracy can mean quite low word level accuracies (Tanner et al. 2009).\nAnother, fully automatic possibility to assess quality of the collection is usage of digital dictionaries Niklas (2010), for example, uses dictionary look-up to check the overall word level quality of The Times of London collection from 1785 to 1985 in his OCR post-correction work. This kind of approach gives a word accuracy approximation for the data (Strange et al. 2014).\nUnfortunately usage of dictionaries suits only languages like English that have only little inflection in words and thus the words in texts can be found in dictionaries as dictionary entries. A heavily inflected, morphologically complex language like Finnish needs other means: full morphological analysis of the material is needed for this type of language. We shall discuss this approach with our material later on.\nSome other methods could also be used. Baeza-Yates and Rello (2012) suggest a simple spelling error based look-up method to evaluate lexical quality of web content, based on the original idea of Gelman and Barletta (2008). We believe that this method might also be useful in analysis of our data, but we are not able to discuss its possible use at present.\nStatistical methods are used many times in corpus linguistics (e.g. Kilgariff 2001), especially when different corpora are compared. In our case usage of statistics is perhaps not very feasible at present stage, where we need to establish a base level quality approximation for our data. Very high level statistical methods, where the status of the OCRed historical documents are profiled (e.g. Reffle and Ringlstetter 2013) show also promise, but their use at the present is beyond our means.\n2. Quality assessment of the Digi\nThere has been on-going work on the assessment of the quality of the Digi since 2014. Part of this work has been described in Kettunen et al. (2014) and Kettunen (2015). These publications describe mainly first post-correction trials of the Finnish newspaper material. To that effort we set up semiautomatically seven smallish parallel corpora (ca. 212 000 words) upon which post-correction trials were done. Results of the evaluation showed that the quality of the evaluation data varied from about 60 % word accuracy at worst to about 90 % accuracy at best, the mean being about 75 % word accuracy. The evaluation samples, however, were small, and on the basis of the parallel corpora it is hard to estimate what the overall quality of the data is. Scarce availability of edited 19 th century parallel newspaper material makes this approach also hard to continue any further (Lauerma, 2012) and there are no resources to set up larger parallel data for evaluation purposes by ourselves. Thus another type of approach is needed.\nSince the first trials we have done further work on lexical level with our data. In winter 2015 we extracted the database of the Digi collection and extracted the words from the page texts of the dump. Punctuation of the text was discarded in the dump but distinction between lower and upper case letters was kept in the resulting word lists.\nWe got two different word lists: the first and smaller one consists of all the Finnish newspaper and magazine word material up to year 1850. It has about 22 million word form tokens, which is less than 1 % of the whole data. The second and more interesting word list consists of the Finnish words in the newspapers during the period 1851–1910 and it contains about 2.39 billion word form tokens. As the main volume of the lexical data of the collection is in the 1851–1910 section of the corpus, we shall concentrate mainly on the analysis of this part of the corpus, but will show also some results of the time period of 1771–1850.\nAs far as we know there is no single method or IT system available that could be used for analyzing the quality of word data in a very large historical newspaper collection. 5 Thus we ended up in using a few simple ways to approximate quality of our data. Firstly we analyzed all the words of the index with two modern Finnish morphological analyzers, commercial FINTWOL 6 and open source Omorfi 7 . As there is no fully developed morphological analyzer of historical Finnish available, this is the only possible way to do morphological analysis for the data 8 . A typical morphological analyzer consists of a rule component and a lexicon (Pirinen, 2015). If the analyzer can relate an input word after application of rules to a base form or forms in its lexicon, it has successfully recognized/analyzed the word. We ran our data through the analyzers and counted how many of the words were recognized or unrecognized by the analyzers. Obviously the number of unrecognized words contains both historically/dialectically unknown words for the modern Finnish analyzers (out-of-vocabulary, OOV, includes also words in foreign language) and OCR errors. A positive recognition does not also guarantee that the word was what it was in the original text. However, when the figures of our analyzed data are compared to analyses of existing edited dictionary and other data of the same period, we can approximate, what amount of our data could be OCR errors.\n5 EU Project IMPACT, http://www.digitisation.eu/, has produced lots of tools for different purposes, but there does not seem to be a suitable tool for this purpose. Nor does CLARIN’s inventory (http://www.clarin.eu/content/languageresource-inventory) have any suitable software. System described in Reffle and Ringlstetter (2013) may be suitable, but its availability is unknown. 6 http://www2.lingsoft.fi/doc/fintwol/; We use FINTWOL’s version 1999/12/20. 7 https://github.com/flammie/omorfi; We use omorfi-analyse version 0.1, dated 2012. 8 A statistical lemmatizer described in Loponen and Järvelin (2010) might also be suitable for our purposes, but unfortunately this software is not available. Morfessor type (http://www.cis.hut.fi/projects/morpho/) unsupervised morph segmenting software, on the other hand, is useless for our purposes.\nSecondly, we made frequency calculations of the word data and took different samples out of that data for further analysis with the morphological analyzers. These analyses show a more detailed picture of the data.\nTable 1 shows initial recognition rates of all the word tokens and types in the Digi with the two morphological analyzers. In Table 2 we show results with a more history aware version of Omorfi 9 (we call this HisOmorfi) and a later version of Omorfi, v. 0.2 (Kettunen and Pääkkönen, 2016).\n9 https://github.com/jiemakel/omorfi 10 Analysis with latest Omorfi version 0.3 gives recognition rate of 69.74 %.\nOmorfi 0.2 does not recognize words much better than version 0.1, but HisOmorfi achieves improved recognition of 3 % units with the main part of the data. There is improvement in recognition with HisOmorfi for every type of data, although for word types improvement is small.\nFigure 1. shows recognition rates of words in the data decade by decade without data of 1780–1819 as it consists of Swedish only. Recognition rates are mainly between 65 and 77 per cent. Data of 1770–1779 and 1840–1849 are recognized slightly worse than other data.\nAt this stage we need also comparable recognition rates for edited lexical data of the same period. For comparison purposes we used material from the Institute for the Languages of Finland. From their web pages 11 we collected two different word corpuses from two different historical periods of Finnish and four different dictionaries from the 19 th century. Figures of this data are shown in Table 3. Sizes of dictionaries refer to unique dictionary entries extracted from the data, not to all of the words in the material. Unless otherwise mentioned, the data consists of word types.\n11 http://www.kotus.fi/aineistot/tietoa_aineistoista/sahkoiset_aineistot_kootusti\nWe can summarize the recognition rates of the Digi and comparable same period lexical data as a graph shown in Figure 2.\n12 Recognition rates of Omorfi are slightly harmed by the fact that all the data is lower cased and Omorfi recognizes proper nouns from upper case initial letter only 13 This material contains the dictionary materials 14 This material contains the dictionary materials\nAs can be seen from Tables 1–3 and Figure 2, type level recognition rates of the Digi data are very low compared to edited comparable material of the same period. The main reason for this is the high number of once occurring words (hapax legomena), most of which are OCR errors, which will be shown later (cf. also Ringlstetter et al. 2006). When token level of the Digi data is examined, recognition rates are quite reasonable, 66–79 %. There is a 17–20 % unit difference to the edited comparable data on token level. The exceptionally high recognition rate of the VNS corpus is partly due to the fact that 1000 most frequent types in the corpus consist already 44.6 % of the whole corpus on token level and among these 2.17 M tokens recognition rate is 99.2 %. In the rest of the VNS corpus’s 2.69 M of tokens the recognition rate is 76.4 %. Thus a realistic recognition rate for late 19 th century data could be around 75–80 % with the tools used.\nInterestingly, there is no big variation in the recognition rates of earlier and late 19 th century, although it would be expectable that older data contains more old vocabulary that is not recognized. One reason for quite good recognition of older data may be simpler column structures and larger fonts in older publications, which could have decreased OCR errors. Towards the end of the 19 th century number of columns in newspapers increased and also fonts got smaller. Even if Finnish of the late 19 th century as such should be easier to recognize for morphological analyzers (cf. also Table 3), it may have more OCR errors due to printing format. We believe these two phenomena have a contrary overall effect on the recognition rate. Also the amount of data may have an effect.\nWhen we view the results of the morphological analyzers with the edited data, we can also see the clear difference of the historical phase of Finnish between the two larger corpora, VKS and VNS. VKS data is mainly from the period before 19 th century (extending to mid-16 th century), and thus recognition rates are expectedly much lower. Data in the rest of the corpora are mainly from the 19 th\ncentury, and their recognition rates are much higher. The two oldest dictionary data, Renvall and Helenius dictionaries, from years 1826 and 1838, get the lowest recognition rates, but even these contain twice or thrice recognized words for the analyzers compared to the VKS data. The two latest dictionaries, Ahlman and Europaeus, get recognition rates of round 70 % and over, four dictionaries combined a rate of about 62 %.\nOn the basis of the edited data analyses we can approximate, that 56–76 % of the words on type level from the 19 th century data can be recognized by modern language morphological analyzers. On token level the recognition rate can be up to 89 %. If there is older material in the data, recognition will drop, and the drop can be quite large.\nNext we proceed to frequency analysis of parts of the data. Table 4. shows results of morphological analyses of 1000–1 M of the most frequent word types of the 1851–1910 part of the Digi word data. At 1 M the frequency of the words is 79, and the frequency range is from 71 257 605 to 79, mean being 2044 and median 194.\nNumber of unknown words for the analyzers is shown on word type level in columns two and three. Column four shows how many tokens of the data each sample covers, and column five shows percentage of the tokens in the whole data."
    }, {
      "heading" : "N of word",
      "text" : ""
    }, {
      "heading" : "1K 134 13.4 % 120 12 % 790 710 542 33.1 %",
      "text" : ""
    }, {
      "heading" : "1 M 563 130 56.3 % 577 974 57. 8 % 2 043 976 151 85.6 %",
      "text" : "For simplicity we analyzed the words on token level with FINTWOL only, and the figures are shown in Table 5."
    }, {
      "heading" : "1K 790 710 542 61 170 210 7.7 %",
      "text" : ""
    }, {
      "heading" : "1 M 2 043 976 151 427 214 868 20.9 %",
      "text" : "Data in Tables 4 and 5 show that the 1M of the most frequent words are of quite good quality. On token level only about 21 % of them are unrecognized by FINTWOL, on type level the percentage is about 58 %. The bottom line of Table 5 is that 1.62 G of the tokens of 1 M of the most frequent word types are recognized. Out of the whole data this is 67.6 %. Thus for the rest circa 765 M of tokens the recognition rate is very low, only 30 M of them are recognized.\nAfter analyzing quality of the top of the frequency list, we need to scrutinize the least frequent end of the data. Common to large corpuses are word form types that occur only once in the data, so called hapax legomena (Baayen 2001). Number of these in the data is 145 056 481, 81.8 % of the data on type level. Out of these 141 934 402, 97.8 %, are unrecognized by Omorfi 0.1 and 142 221 709, 98 %, are unrecognized by FINTWOL.\nIn order to confirm occurrence of OCR errors in the least frequent word type classes we analyzed all word types that occur 1–10 times in the data. Table 6. shows the number of these word types, marked as V(m, N), where m is the index for frequency class, N the size of the sample (Baayen 2001, 8). It can be seen that 85–98 % of these types are unrecognized by the recognizers.\nWhen we count the number of unknown 2–10 times occurring word types on token level for FINTWOL, we get about 83.4 M words. This added to the number of hapax legomena tokens unrecognized by FINTWOL makes about 225.6 M unrecognized word forms. We believe that these circa 9.4 % of the word form tokens unrecognized by morphological analysis are mostly very hard OCR errors, which are quality wise the worst part of the whole collection (cf. Ringlstetter et al. 2006). The slowly increasing recognition rate among the ten least frequent types suggests that the number of hard OCR errors is somewhere between 225 M and 305 M on token level. Recognition rate at 20 least frequent word types is about 80 %, at 30 about 77 %, at 40 about 75 %, and at 50 about 73 %. Even at 100 the recognition rate is still only about 66 %. Thus the number of unknown word types stays very high at the bottom of the data in a long range."
    }, {
      "heading" : "2.1 Other considerations",
      "text" : "Orthography of Finnish was already reasonably stable in the mid-19 th century, although there were phenomena that differ from modern language (cf. table 1. in Järvelin et. al 2015). Also dialectical word forms were more common in newspapers of the 19 th century. The biggest and most visible difference between modern Finnish and 19 th century Finnish is variation of w/v, which does not exist in modern language. Thus words that have w and are not proper names like Wien (Vienna in Finnish) are not usually recognized by modern morphological analyzers. To approximate effect of this, we counted the occurrences of w in the 1 M of the most frequent words of the data. The data\ncontains 92 749 word types with w, which makes 78 438 010 tokens (3.3 % of all the tokens). Out of the types 91 886 (99.1 %) are unrecognized by FINTWOL. This is 76 450 673 words (97.5 %) on token level. If we replace w’s with v’s, 54 049 types (58.3 %) are unrecognized by FINTWOL on type level and 24 016 996 (30.6 %) on token level. 15 Thus out of the unrecognized 427 M words of Table 4. 52.4 M (12.2 %) can be recognized with replacement of w. Out of the whole 2.044 G of word tokens of the 1 M of most common types this makes 2.2 %. So effect of the w/v variation among the unrecognized words is significant although the number of the words in all the data is not very high.\nTo get an approximation of relation between OOV words of the analyzers and OCR errors proper we browsed through the 1 000 most common word types and their 120 unrecognized word types to FINTWOL. Out of these about 85 (70. 8 %) can be considered to be OCR errors, the rest being OOV’s. Demarcation out of textual context is not always clear, but we can take the 70 % OCR error figure as a low estimate, and as OCR errors tend to increase with less frequent word types, OCR error percentage could be about 70–90 %.\nIt is obvious that length of words plays an important role in OCR of words. We analyzed mean lengths of the 1K–1M range of most common word types and mean lengths of 1-10 least common word types. Results are shown in Figures 3 and 4.\n15 When same analysis is carried out with the 4.86 M tokens of the VNS_Kotus, the original recognition rate of 86.5 improves only to 86.7 %. VNS_Kotus data contains considerably less w’s due to the editing policy of the data, cf. http://kaino.kotus.fi/korpus/1800/viite/1800-luvun_korpuksen_koodauksesta.php\n5,21 6,70 7,91 8,51 8,71\n13,4 17,7\n30\n47,4\n56,3\n0,00\n10,00\n20,00\n30,00\n40,00\n50,00\n60,00\n1 K 10 K 10 K 500 K 1 M\nMean lengths and % of unrecognized words in 1K-1M of most common word types\nMean length in chars Unk rate %\nFig. 3. Mean lengths and per centages of unrecognized words in 1K–1M of most common word types\nFig. 4. Mean lengths and per centages of unrecognized words in 1–10 of least common word types\nFigure 3 shows that mean length of words among the 1M most common types grows steadily with decreasing recognition rate. Inverse relation can be seen for least common words: mean length decreases and rate of recognition increases in five least common types, but after that mean length increases again although recognition rate keeps increasing.\nIt should, of course, be kept in mind, that recognizability of words is not the same as correctness in the original text. A word may be wrongly OCRed, but still recognizable as a form of some other word. Nonexistent compounds may be recognized, if their composite parts are in the lexicon of the analyzer. As Omorfi has a very large lexicon (424 259 lexemes according to Pirinen, 2015), this may cause lots of false recognitions of compounds. Many words in the Digi’s database are split wrongly to parts due to hyphenation in the original text, which may cause both false positive recognition and false negative recognition. Compounds were also written differently in the 19 th century Finnish. OOVs, words that are not in the lexicon of the analyzer, bring complexity of their own to results. Some examples of false recognitions and false misrecognitions are shown in the following list:\n11,48 10,18 9,96 9,83 9,75 11,69 11,64 11,59 11,56 12,52\n97,8 93,4 91,3 89,9 88,8 87,8 87 86,3 85,6 85\n0,00\n20,00\n40,00\n60,00\n80,00\n100,00\n120,00\n1 2 3 4 5 6 7 8 9 10\nMean lengths and % of unrecognized words in 1-10 times occuring least common word types\nMean length in chars Unk rate %\n mli mli Num Roman Nom Sg  probably an OCR error\n huu huu Part\ntain tai N Gen Sg  wrong division into two parts based on hyphenation,\nshould be huutain which is unrecogcnizable, although it is a correct form in the 19 th century Finnish\n Hei He Pron Nom Pl  should be heidan, unrecognizable (heidän would be\ncorrect)\ndan +?\n Samoinkuin +?  not recognized because written as a compound, correct otherwise\n ylöskannetaan +?  not recognized because written as a compound, correct otherwise\nAmount and effect of these kinds of phenomena are hard to estimate, but it is clear that all these phenomena cause uncertainty in the results and make an estimation of error margins in the analysis hard to establish.\nWe have now reached a reasonably comprehensive result out of the quality assessment of our data. We have three different parameters that affect the results: number of OOV vocabulary, number of OCR errors proper and effect of w/v variation in the data. The effect of the OOV factor in the clean VNS_Kotus data is on token level about 14 % and in the VKS_Kotus about 50 %. Their mean is 32 %, but a fair approximation could be 14–20 % in edited material of the latter part of the 19 th century word data. We believe that in the Digi data OCR errors tend to override vastly the OOV words. The variation of w/v has an effect of about 12 % among the unrecognized words.\nThe initial analysis without considering the w/v variation and effect of OOV’s is shown concisely in Figure 5. We have 1.65 G of recognized words and 733 M of unrecognized words (cf. Table 1.). The 69 % recognition rate can be called raw recognition rate of the data.\nWe can now proceed to a more detailed analysis of the 733 M unrecognized words. It is safest to assume, that w/v variation and OOV’s have most of their effect in the 427 M part of the unrecognized words, because they belong to the most frequent words and constitute 85.6 % of the whole word data. If the number of words recognized when w’s are replaced with v (52 M) is taken into consideration, the share of recognized words goes up to 71.3 % and share of unrecognized words drops to 28.7 %, absolute number of unrecognized remaining words being circa 375 M. The approximate share of OOV words among the still unrecognized 375 M of words could be somewhere between 50–75 M. Thus the real number of OCR errors in the data is round 600–625 M, approximately about 25–26 % of the whole. Thus the approximated recognition rate of the words in the data could be 74–75 %.\nFigure 6 shows how the circa 625 M of unrecognized words are divided. As was shown in Table 5, 225.6 M of the words are from the 1–10 least frequent word types classes and thus they are probably hard OCR errors. The same is probably true for the 80 M of unrecognized words found beyond the 1 M of most frequent types and 1–10 least frequent types. The major initial bulk, 427 M, was reduced to about 300 M, and this part is probably easier OCR errors, as the words are among the 1 M of the most frequent types.\n3. Discussion\nOut of all analyses presented, we can make the following conclusions:\n- main part, over 99 % of the data in the Digi is between the years 1851–1910. This implies\nthat the effect of OOV words in the data should not be prohibitively large for analysis with modern language morphological analyzers (Tables 1. and 2.)\n- vocabulary of this time period can be recognized reasonably well with modern language\nmorphological analyzers; about 56–76 % of the word types in edited data are recognized, on token level the recognition rate can be as high as 86.5 % (Table 2.)\n- in the Digi data, raw token level recognition rate is round 65–69 % (Table 1.) When effect\nof v/w variation and OOV words is taken into account, the approximated recognition rate is about 74–75 %; This is near the 75 % mean word correctness of our earlier sample-based analysis in Kettunen et al., 2014\n- type level recognition rate in Digi data is low (Table 1.), which is due to large set of hapax\nlegomena and other rare word types, that are mainly OCR errors (Table 5.)\n- modern Finnish morphological analyzers lack about 30-40 % of the vocabulary of the\nmainly 19 th century edited data on type level; on token level the figure is at minimum about 14 % (Table 2.)\n- in up to almost 75 % of the most frequent data (almost 1.8 G of words), raw recognition rate\nof the word form tokens is about 68.5–70 %. After this the recognition rate drops heavily (Table 3.)\n- out of the 625 M of unrecognized words at least about 225 M, 9.4 %, in the data are\nprobably very hard OCR errors, this figure can be up to 313 M (Tables 4. and 5., Figure 6.). About 300 M could be easier OCR errors and perhaps also OOVs (Figure 6.)\n- unrecognized words tend to be longer than recognized words (Fig. 3. and 4.)\nThe lexical quality approximation process we have set up is relatively straightforward and does not need complicated tools. It is based on frequency calculations and usage of off-the-shelf modern Finnish morphological analyzers. It can be automatized fully, even if we have done it partly step by step half automatically. It is apparent that we need to be cautious in conclusions, as different data are of different sizes which may cause errors in estimations (Baayen 2001; Kilgariff 2001). However, we believe that our analyses have shed considerable light into quality of the Digi collection.\nAt this stage we can also reflect usefulness of the analysis procedure from point of view of improvement of the OCR quality of the Digi collection. The main message that our analysis gives, is that the collection has a relatively good quality part, about 69–75 %, and a very bad quality part, about 9-12 %. The set of about 13 % of the words that are not recognized, is harder to estimate. As a part of them belongs to the most frequent part of the data, they could be at least partly easier OCR errors and OOVs. All in all about a 25–30 % share of the collection needs further processing so that the overall quality of the data would improve.\nIf correction of the data is performed it should be focused on the 24–25 % unrecognized part of the data. Out of this the ca. 300 M possibly easier part could be improved by post-correction of the material with algorithmic correction software. We have tried post-correction with a sample (Kettunen 2015), but the results were not good enough for realistic post-correction. If postcorrection would be focused to only the easier part of the Digi’s erroneous data, it could work quite well. General experience from algorithmic post-correction of OCR errors shows, that good quality word material can be corrected relatively well (e.g. Niklas 2010; Reynaert 2008). This may also apply to the medium quality word data. But the worst 9–12 % part of the Digi data cannot be corrected with post-processing; only re-OCRing could help with it, as there is so much of it.\nTaken that some action had been taken to improve the quality of the Digi data, we have to consider, whether our procedure would be useful in showing quality improvement, if such had been achieved. We suggest that improvement of the lexical quality could be shown e.g. with following analyses:\n- clear improvement in overall recognition rate of the data: at least 3–5 % units in both type\nand token level analyses\n- recognition rate in the top 1 M of the most frequent word types should improve\nsignificantly, especially in the 100 K–1 M range, that is now beyond mean recognition rate of edited data\n- a very large drop in the number of unrecognized hapax legomena and other rare word types;\nin practice this would mean tens of millions of word forms to be become recognizable\n4. Conclusion\nIn this paper we have suggested how to assess the overall lexical quality of a mainly 19 th century OCRed Finnish historical newspaper collection with circa 2.40 billion words. The procedure uses elementary corpus statistics and morphological analyzers of modern Finnish and is straightforward to use. We also propose how to measure quality improvement after correcting the corpus using the suggested procedure.\nAdvantages of the procedure are the following:\n coverage: the procedure gives an approximation of the quality of the whole corpus and in\nthe same time different parts of the whole can be analyzed; thus it is not based on samples only\n period sensibility: comparable same period edited lexical corpora are used in assessment and\nthus the procedure is reasonably sensible to time variation in the data; the method works reasonably well for the so called period of early modern Finnish (ca. 1820-1870) and beginning of modern Finnish (from ca. 1870-), but would be more vulnerable with earlier material, as lexical coverage of the morphological recognizers would be lower\n simplicity: available modern language technology tools and basic corpus statistic methods\nare used, and no high-level tools need to be developed;\nThe main vulnerability of the proposed procedure at present is possible sampling error and its effects with corpora used. This, however, can be taken into account with adding advanced statistics to the procedure. They may sharpen the procedure, but at present we are satisfied with the current approach and believe that the measures the procedure produces are useful in quality assessment and quality control after improvements in the word data.\nThe major reason for lexical quality assessment of our data is the fact, that OCR errors in the data may have several harmful effects for users of the data. One of the most important effects of poor OCR quality – besides worse readability and comprehensibility - is worse on-line searchability of the documents in the collections (Taghva et al. 1996). In a recent study Savoy and Naji (2011), for example, showed how retrieval performance decreases with OCR error corrupted documents quite severely. With mean reciprocal rank as a performance measure, they showed that degradation in retrieval effectiveness is around 17% when dealing with an error rate of 5%. By increasing the error rate to 20%, the average decrease in retrieval is around 46%. Same and larger level of decrease in retrieval effectiveness is shown also in results of the TREC-5’s confusion track (Kantor and Voorhees 2000). The effect of errors is not clear cut, however. Tanner et al. (2009) suggest that word accuracy rates less than 80 % are harmful for search, but when the word accuracy is over 80 %, fuzzy search capabilities of search engines should manage the problems caused by word errors. Mittendorf and Schäuble’s (2000) probabilistic model for data corruption seems to support this. Information retrieval is robust even with corrupted data, but IR works best with longer documents and long queries. Empirical results of Järvelin et al. (2015) with the Finnish historical newspaper search collection, for example, show that even impractically heavy usage of fuzzy matching will help only to a limited degree in search of a low quality OCRed newspaper collection, when short queries and their query expansions are used. Evershed and Fitch (2014), on the other hand, show that if OCR word errors are corrected and word error rate decreased with about 10 % units, recall in document retrieval may have about 9–10 % unit boost with historical OCRed English documents.\nUsers of the Digi collection have complained about the poor OCR of the collection relatively little, but some of them have reported curious search results and been annoyed by the OCR quality (Hölttä, 2016; Kettunen, Pääkkönen, Koistinen, 2016). Basing on the empirical search results with the evaluation collection derived from a small subset of the whole Digi material (Järvelin et al., 2016), it is evident that search results in the Digi collection itself are not optimal, and better OCR quality would probably improve them.\nBesides retrieval performance effects poor OCR quality has an effect on ranking of the documents (Taghva et al. 1996; Mittendorf and Schäuble 2000). In practice these kinds of drops in retrieval and ranking performance mean that the user will lose relevant documents: either they are not found at all by the search engine or the documents are so low in the ranking list that the user may skip them. Some examples of this in the work of digital humanities scholars are discussed e.g. in Traub et al. (2015).\nWeaker searchability of the OCRed collection is only one dimension of poor OCR quality. Other effects of poor OCR quality may show in the more detailed processing of the documents, such as sentence boundary detection, tokenization and part-of-speech-tagging, which are important in higher-level natural language processing tasks (Lopresti 2009). Part of the problems may be local, but part will cumulate in the whole pipeline of NLP processing causing errors. Thus the quality of the OCRed texts is the cornerstone for any kind of further usage of the material, and we need to be able to assess the quality of the data in order to be also able to improve it and show the possible improvements meaningfully."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by European Regional Development Fund (ERDF), Leverage from the EU, 2014–2020."
    } ],
    "references" : [ {
      "title" : "Word Frequency Distributions",
      "author" : [ "H. Baayen" ],
      "venue" : "On Measuring the Lexical Quality of the Web. WebQuality",
      "citeRegEx" : "Baayen,? \\Q2001\\E",
      "shortCiteRegEx" : "Baayen",
      "year" : 2001
    }, {
      "title" : "A Nordic Digital Newspaper Library",
      "author" : [ "Bremer-Laamanen", "M-L" ],
      "venue" : "International Preservation News",
      "citeRegEx" : "Bremer.Laamanen and M.L.,? \\Q2001\\E",
      "shortCiteRegEx" : "Bremer.Laamanen and M.L.",
      "year" : 2001
    }, {
      "title" : "Reducing OCR Errors in Gothic-Script Documents",
      "author" : [ "L. Furrer", "M. Volk" ],
      "venue" : "In Proceedings of Language Technologies for Digital Humanities and Cultural Heritage Workshop,",
      "citeRegEx" : "Furrer and Volk,? \\Q2011\\E",
      "shortCiteRegEx" : "Furrer and Volk",
      "year" : 2011
    }, {
      "title" : "A “quick and dirty” website data quality indicator. In The 2nd ACM workshop on Information credibility on the Web (WICOW ’08), (pp. 43–46)",
      "author" : [ "I.A. Gelman", "A.L. Barletta" ],
      "venue" : "Holley, R",
      "citeRegEx" : "Gelman and Barletta,? \\Q2008\\E",
      "shortCiteRegEx" : "Gelman and Barletta",
      "year" : 2008
    }, {
      "title" : "Digitoitujen kulttuuriperintöaineistojen tutkimuskäyttö ja tutkijat",
      "author" : [ "T. Hölttä" ],
      "venue" : "M. Sc. thesis (in Finnish),",
      "citeRegEx" : "Hölttä,? \\Q2016\\E",
      "shortCiteRegEx" : "Hölttä",
      "year" : 2016
    }, {
      "title" : "The TREC-5 Confusion Track: Comparing Retrieval Methods for Scanned Texts",
      "author" : [ "P.B. Kantor", "E.M. Voorhees" ],
      "venue" : "Journal of the Association for Information Science and Technology,",
      "citeRegEx" : "Kantor and Voorhees,? \\Q2000\\E",
      "shortCiteRegEx" : "Kantor and Voorhees",
      "year" : 2000
    }, {
      "title" : "Measuring Lexical Quality of a Historical Finnish Newspaper Collection – Analysis of Garbled OCR Data with Basic Language Technology Tools and Means",
      "author" : [ "T. Pääkkönen" ],
      "venue" : "Digital Libraries - IRCDL 2015, Bozen-Bolzano,",
      "citeRegEx" : "Kettunen and Pääkkönen,? \\Q2015\\E",
      "shortCiteRegEx" : "Kettunen and Pääkkönen",
      "year" : 2015
    }, {
      "title" : "The Current State-of-art in Newspaper Digitization",
      "author" : [ "A. Kilgariff" ],
      "venue" : "Comparing Corpora. International Journal of Corpus Linguistics,",
      "citeRegEx" : "Kilgariff,? \\Q2001\\E",
      "shortCiteRegEx" : "Kilgariff",
      "year" : 2001
    }, {
      "title" : "Lounela (eds.): Genreanalyysi – tekstilajitutkimuksen käytäntöä, s. 308–312. Kotimaisten kielten keskuksen verkkojulkaisuja 29. Helsinki: Kotimaisten kielten keskus",
      "author" : [ "Eero Voutilainen", "Petri Lauerma", "Ulla Tiililä", "Mikko" ],
      "venue" : "CLEF",
      "citeRegEx" : "Voutilainen et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Voutilainen et al\\.",
      "year" : 2010
    }, {
      "title" : "Information retrieval can cope with many errors. Information Retrieval, 3(3):189–216",
      "author" : [ "E. Mittendorf", "P. Schäuble" ],
      "venue" : "Niklas, K",
      "citeRegEx" : "Mittendorf and Schäuble,? \\Q2000\\E",
      "shortCiteRegEx" : "Mittendorf and Schäuble",
      "year" : 2000
    }, {
      "title" : "Unsupervised profiling of OCRed historical documents",
      "author" : [ "U. Reffle", "C. Ringlstetter" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "Reffle and Ringlstetter,? \\Q2013\\E",
      "shortCiteRegEx" : "Reffle and Ringlstetter",
      "year" : 2013
    }, {
      "title" : "Orthographic Errors in Web Pages: Toward Cleaner Web Corpora",
      "author" : [ "C. Ringlstetter", "K. Schulz", "S. Mihanov" ],
      "venue" : "Computational Linguistics",
      "citeRegEx" : "Ringlstetter et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ringlstetter et al\\.",
      "year" : 2006
    }, {
      "title" : "Evaluation of Model-Based Retrieval Effectiveness with OCR Text",
      "author" : [ "K. Taghva", "J. Borsack", "A. Condit" ],
      "venue" : "OCR Quality on the Use of Digitized Historical Newspapers. Digital Humanitites Quarterly,",
      "citeRegEx" : "Taghva et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Taghva et al\\.",
      "year" : 1996
    }, {
      "title" : "Strategies for reducing and correcting OCR errors",
      "author" : [ "M. Volk", "L. Furrer", "R. Sennrich" ],
      "venue" : "Language Technology for Cultural Heritage (pp. 3–22)",
      "citeRegEx" : "Volk et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Volk et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "The web service is used, for example, by genealogists, heritage societies, researchers, and history enthusiast laymen (Hölttä, 2016).",
      "startOffset" : 118,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : "It is well known that the typeface is difficult to recognize for OCR software (Holley 2008; Furrer and Volk 2011; Volk et al. 2011).",
      "startOffset" : 78,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : "It is well known that the typeface is difficult to recognize for OCR software (Holley 2008; Furrer and Volk 2011; Volk et al. 2011).",
      "startOffset" : 78,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "Baeza-Yates and Rello (2012) suggest a simple spelling error based look-up method to evaluate lexical quality of web content, based on the original idea of Gelman and Barletta (2008). We believe that this method might also be useful in analysis of our data, but we are not able to discuss its possible use at present.",
      "startOffset" : 156,
      "endOffset" : 183
    }, {
      "referenceID" : 10,
      "context" : "System described in Reffle and Ringlstetter (2013) may be suitable, but its availability is unknown.",
      "startOffset" : 20,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "System described in Reffle and Ringlstetter (2013) may be suitable, but its availability is unknown. 6 http://www2.lingsoft.fi/doc/fintwol/; We use FINTWOL’s version 1999/12/20. 7 https://github.com/flammie/omorfi; We use omorfi-analyse version 0.1, dated 2012. 8 A statistical lemmatizer described in Loponen and Järvelin (2010) might also be suitable for our purposes, but unfortunately this software is not available.",
      "startOffset" : 20,
      "endOffset" : 330
    }, {
      "referenceID" : 0,
      "context" : "Common to large corpuses are word form types that occur only once in the data, so called hapax legomena (Baayen 2001).",
      "startOffset" : 104,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "It is apparent that we need to be cautious in conclusions, as different data are of different sizes which may cause errors in estimations (Baayen 2001; Kilgariff 2001).",
      "startOffset" : 138,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "It is apparent that we need to be cautious in conclusions, as different data are of different sizes which may cause errors in estimations (Baayen 2001; Kilgariff 2001).",
      "startOffset" : 138,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "One of the most important effects of poor OCR quality – besides worse readability and comprehensibility - is worse on-line searchability of the documents in the collections (Taghva et al. 1996).",
      "startOffset" : 173,
      "endOffset" : 193
    }, {
      "referenceID" : 5,
      "context" : "Same and larger level of decrease in retrieval effectiveness is shown also in results of the TREC-5’s confusion track (Kantor and Voorhees 2000).",
      "startOffset" : 118,
      "endOffset" : 144
    }, {
      "referenceID" : 10,
      "context" : "One of the most important effects of poor OCR quality – besides worse readability and comprehensibility - is worse on-line searchability of the documents in the collections (Taghva et al. 1996). In a recent study Savoy and Naji (2011), for example, showed how retrieval performance decreases with OCR error corrupted documents quite severely.",
      "startOffset" : 174,
      "endOffset" : 235
    }, {
      "referenceID" : 5,
      "context" : "Same and larger level of decrease in retrieval effectiveness is shown also in results of the TREC-5’s confusion track (Kantor and Voorhees 2000). The effect of errors is not clear cut, however. Tanner et al. (2009) suggest that word accuracy rates less than 80 % are harmful for search, but when the word accuracy is over 80 %, fuzzy search capabilities of search engines should manage the problems caused by word errors.",
      "startOffset" : 119,
      "endOffset" : 215
    }, {
      "referenceID" : 5,
      "context" : "Same and larger level of decrease in retrieval effectiveness is shown also in results of the TREC-5’s confusion track (Kantor and Voorhees 2000). The effect of errors is not clear cut, however. Tanner et al. (2009) suggest that word accuracy rates less than 80 % are harmful for search, but when the word accuracy is over 80 %, fuzzy search capabilities of search engines should manage the problems caused by word errors. Mittendorf and Schäuble’s (2000) probabilistic model for data corruption seems to support this.",
      "startOffset" : 119,
      "endOffset" : 455
    }, {
      "referenceID" : 5,
      "context" : "Same and larger level of decrease in retrieval effectiveness is shown also in results of the TREC-5’s confusion track (Kantor and Voorhees 2000). The effect of errors is not clear cut, however. Tanner et al. (2009) suggest that word accuracy rates less than 80 % are harmful for search, but when the word accuracy is over 80 %, fuzzy search capabilities of search engines should manage the problems caused by word errors. Mittendorf and Schäuble’s (2000) probabilistic model for data corruption seems to support this. Information retrieval is robust even with corrupted data, but IR works best with longer documents and long queries. Empirical results of Järvelin et al. (2015) with the Finnish historical newspaper search collection, for example, show that even impractically heavy usage of fuzzy matching will help only to a limited degree in search of a low quality OCRed newspaper collection, when short queries and their query expansions are used.",
      "startOffset" : 119,
      "endOffset" : 678
    }, {
      "referenceID" : 5,
      "context" : "Same and larger level of decrease in retrieval effectiveness is shown also in results of the TREC-5’s confusion track (Kantor and Voorhees 2000). The effect of errors is not clear cut, however. Tanner et al. (2009) suggest that word accuracy rates less than 80 % are harmful for search, but when the word accuracy is over 80 %, fuzzy search capabilities of search engines should manage the problems caused by word errors. Mittendorf and Schäuble’s (2000) probabilistic model for data corruption seems to support this. Information retrieval is robust even with corrupted data, but IR works best with longer documents and long queries. Empirical results of Järvelin et al. (2015) with the Finnish historical newspaper search collection, for example, show that even impractically heavy usage of fuzzy matching will help only to a limited degree in search of a low quality OCRed newspaper collection, when short queries and their query expansions are used. Evershed and Fitch (2014), on the other hand, show that if OCR word errors are corrected and word error rate decreased with about 10 % units, recall in document retrieval may have about 9–10 % unit boost with historical OCRed English documents.",
      "startOffset" : 119,
      "endOffset" : 979
    }, {
      "referenceID" : 4,
      "context" : "Users of the Digi collection have complained about the poor OCR of the collection relatively little, but some of them have reported curious search results and been annoyed by the OCR quality (Hölttä, 2016; Kettunen, Pääkkönen, Koistinen, 2016).",
      "startOffset" : 191,
      "endOffset" : 243
    }, {
      "referenceID" : 12,
      "context" : "Besides retrieval performance effects poor OCR quality has an effect on ranking of the documents (Taghva et al. 1996; Mittendorf and Schäuble 2000).",
      "startOffset" : 97,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "Besides retrieval performance effects poor OCR quality has an effect on ranking of the documents (Taghva et al. 1996; Mittendorf and Schäuble 2000).",
      "startOffset" : 97,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "1996; Mittendorf and Schäuble 2000). In practice these kinds of drops in retrieval and ranking performance mean that the user will lose relevant documents: either they are not found at all by the search engine or the documents are so low in the ranking list that the user may skip them. Some examples of this in the work of digital humanities scholars are discussed e.g. in Traub et al. (2015).",
      "startOffset" : 6,
      "endOffset" : 394
    } ],
    "year" : 2016,
    "abstractText" : "Digitization of both hand-written and printed historical material during the last 10–15 years has been an ongoing academic and non-academic industry. Most probably this activity will only increase in the current Digital Humanities era. As a result of past and current work we have lots of digital historical document collections available and will have more of them in the future. The National Library of Finland has digitized a large proportion of the historical newspapers published in Finland between 1771 and 1910 (Bremer-Laamanen 2001, 2005, 2014; Kettunen et al. 2014). This collection contains approximately 1.95 million pages in Finnish and Swedish. Finnish part of the collection consists of about 2.40 billion words. The National Library’s Digital Collections are offered via the digi.kansalliskirjasto.fi web service, also known as Digi. Part of the newspaper material (years 1771–1874) is also available freely downloadable in The Language Bank of Finland provided by the FIN-CLARIN consortium 1 . The collection can also be accessed through the Korp 2 environment that has been developed by Språkbanken at the University of Gothenburg and extended by FIN-CLARIN team at the University of Helsinki to provide concordances of text resources. A Cranfield style information retrieval test collection has also been produced out of a small part of the Digi newspaper material at the University of Tampere (Järvelin et al. 2015). An open data package of the whole collection will be released during the year 2016 (Pääkkönen et al., 2016) The web service digi.kansalliskirjasto.fi contains different material besides newspapers, including journals, and ephemera (different small prints). Recently a new service was created: it enables marking of clips and storing of them to a personal scrapbook. The user can also save links to his search keys and results in an Excel file. The web service is used, for example, by genealogists, heritage societies, researchers, and history enthusiast laymen (Hölttä, 2016). There is also an 1 https://kitwiki.csc.fi/twiki/bin/view/FinCLARIN/KielipankkiAineistotDigilibPub 2 https://korp.csc.fi/",
    "creator" : "Microsoft® Word 2010"
  }
}