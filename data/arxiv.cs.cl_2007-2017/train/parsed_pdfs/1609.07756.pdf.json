{
  "name" : "1609.07756.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Factorized Model for Transitive Verbs in Compositional Distributional Semantics",
    "authors" : [ "Lilach Edelstein" ],
    "emails" : [ "lilach.edelstein@mail.huji.ac.il", "roiri@ie.technion.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 9.\n07 75\n6v 1\n[ cs\n.C L\n] 2\n5 Se\nWe present a factorized compositional distributional semantics model for the representation of transitive verb constructions. Our model first produces (subject, verb) and (verb, object) vector representations based on the similarity of the nouns in the construction to each of the nouns in the vocabulary and the tendency of these nouns to take the subject and object roles of the verb. These vectors are then combined into a final (subject,verb,object) representation through simple vector operations. On two established tasks for the transitive verb construction our model outperforms recent previous work."
    }, {
      "heading" : "1 Introduction",
      "text" : "In recent years, vector space models, deriving word meaning representations from word cooccurrence patterns in text, have become prominent in lexical semantics research (Turney et al., 2010; Clark, 2012). Following this success recent attempts have been devoted to compositional distributional semantics (CDS): combining the distributional word representations, often in a syntax-driven fashion, to produce representations of phrases and sentences.\nSeveral tasks and techniques have been proposed for CDS. Some work aims at representing sentences that vary in length and structure mostly with neural network models (Socher et al., 2012; Marelli et al., 2014; Le and Mikolov, 2014; Pham et al., 2015). Another approach, which we take in this paper, is to focus on specific syntactic constructions. At the expense of\ngenerality, this approach enables an in-depth investigation of a specific linguistic phenomenon. Mitchell and Lapata (2008) proposed various additive and multiplicative operators for the combinations of word vectors, and applied them to intransitive verbs and their subjects. Recently, the categorical framework (Coecke et al., 2011; Baroni et al., 2014) has been proposed, where each word is represented by a tensor whose order is determined by the categorical grammar type of the word. For example, Baroni and Zamparelli (2010) represent nouns by a vector, and adjectives by matrices transforming one noun vector into another.\nIn this paper we focus on the transitive verb construction, which recently attracts much attention. In the categorical framework it is represented with a third order tensor that takes the noun vectors representing the subject and object and returns a vector in the sentence space (Grefenstette et al., 2013; Polajnar et al., 2014). The main limitation of this approach is the excessive number of involved parameters. For example, a thirdorder tensor for a given transitive verb, mapping two 100-dimensional noun spaces to a 100- dimensional sentence space, would have 1003 parameters in its full form. Indeed, several recent works have tried to reduce the size of these models (Polajnar et al., 2014; Fried et al., 2015) while others proposed matrix based representations (Polajnar et al., 2014; Milajevs et al., 2014; Paperno et al., 2014).\nWe propose a factorized model for the representation of transitive verb constructions. Given a subj-verb-obj (s, v, o) construction, our model\nbuilds vector representations for the (s, v) and the (v, o) pairs, based on the similarity of s and o to each of the nouns in the vocabulary and the tendency of these nouns to take the subject and object roles of v. The dimensionality of these (s, v) and (v, o) vectors (15701 in our case) equals to the number of nouns in the vocabulary that take the subject and object positions of v frequently enough. The (s, v) and (v, o) vectors are then combined to a final (s, v, o) vector through simple vector operations. Our model outperforms recent previous work on two established tasks for the transitive verb construction (Grefenstette and Sadrzadeh, 2011b; Kartsaklis et al., 2014)."
    }, {
      "heading" : "2 Model",
      "text" : "The goal of our model is to generate vector representations (embeddings) for subject-verb-object (s, v, o) constructions, where s is the subject noun, v is a verb and o is the object noun. The model consists of two steps: (1) Embed the (s, v) and (v, o) pairs based on co-occurrence statistics of the members of each pair with all other nouns in the vocabulary; and (2) Combine the (s, v) and (v, o) representations to create a final (s, v, o) embedding.\nOur model is a factorized model, generating an (s, v, o) representation from its pair components ((s, v) and (v, o)). As such it is compact: the size of the (s, v) and (v, o) vectors is the number of nouns in the vocabulary that appear frequently enough both as subjects and as objects of v, and the (s, v, o) vector is a simple derivation of these vectors. In what follows we describe each of the above steps."
    }, {
      "heading" : "2.1 Pair Representations",
      "text" : "We represent the (s, v) and (v, o) pairs through the relations between the verb (v) and the noun (s or o) with each other noun in the vocabulary. Particularly, we consider the tendency of each noun to come as a subject (for (s, v)) or object (for (v, o)) of v, and the similarity of that noun to s or o, respectively. By considering all the nouns in the vocabulary we get a smooth estimate of the tendency of the noun (s or o) to take the subject (for s) or object (for o) position of v. In what follows, we describe the representation in details for (s, v) pairs. A very similar process is employed for (v, o) pairs.\nFor an (s, v) pair we construct a vector representation whose size is the number of nouns that appear frequently enough at both subject and object positions in the training corpus. The k’th coordinate in this representation is given by:\nusubjs,v [k] = NV Subj(nk, v) ·NNSim(nk, s)\nwhere nk is the k’th noun in the vocabulary, NV Subj(x, y) reflects the tendency of the noun x to be the subject of the verb y, and NNSim(x, y) reflects the similarity between the nouns x and y. We next describe how NV Subj(x, y) and NNSim(x, y) are computed.\nNVSubj For a noun x and a verb y, we compute the positive point-wise mutual information (PPMI) of x appearing as the subject of y, and the score for the (x, y) pair is then given by:\nNV Subj(x, y) = PPMIsubj(x, y) =\nmax(0, log((P subj,verb(x, y))/(P subj(x)P verb(y))))\nwhere P subj,verb(x, y) is the probability that a (subject,verb) pair in the corpus is (x, y), and P subj(x) and P verb(y) are the probabilities that x appears at the subject position of any verb in the corpus and that y appears as a verb in the corpus, respectively.\nNNSim This score reflects the similarity between two nouns: NNSim(x, y) = sim(x, y), where sim(x, y) is any function that returns the similarity between its two word arguments.\nWe apply the same considerations for (v, o) pairs: for the k-th coordinate, NVObj represents the tendency of nk to be an object of v, while NNSim represents the similarity between nk and o.\n2.2 (s, v, o) Construction Representation\nGiven the (s, v) and (v, o) representations described above, our next step is to combine them so that to get an effective representation of the (s, v, o) triplet.\nWe consider three combination methods. In the first two a single vector is constructed for (s, v, o) through: (1) concatenation of the two vectors; and (2) coordination-wise multiplication:\nu(s,v,o)[k] = usubjs,v [k]·u obj v,o [k] = NV Subj(nk, v)·\nNNSim(nk, s)·NV Obj(nk, v)·NNSim(nk, o)\nThat is, the k-th coordinate represents the similarity of nk to both s and o, and its co-occurrence statistics with v as both a subject and an object. Under these two combination methods the similarity score of two (s, v, o) constructions is defined to be the cosine similarity between their vectors.\nAs an alternative, the third combination method keeps the (s, v) and (v, o) vectors as a representation of (s, v, o). The similarity between two constructions (s, v, o)1 and (s, v, o)2 is computed through the similarities between their components: sim((s, v, o)1, (s, v, o)2) = cosine(u(s,v)1 , u(s,v)2) · cosine(u(v,o)1 , u(v,o)2)"
    }, {
      "heading" : "3 Experiments",
      "text" : "Data Preprocessing and Training We trained our models on the cleaned and tokenized Polyglot Wikipedia corpus (Al-Rfou et al., 2013),1 consisting of approximately 75M sentences and 1.5G word tokens. The corpora were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using the TurboTagger (Martins et al., 2013),2 trained with default settings (SVM MIRA with 20 iterations) without any further parameter fine-tuning, on the TRAIN+DEV portion of the UD treebank. Following, the corpus was parsed with Universal Dependencies3 using the Mate parser v3.61 (Bohnet, 2010),4 trained on the same UD treebank portion as the tagger and with default settings.\nAfter parsing the corpus and before further statistics were collected, the corpus was lemattized to facilitate robust estimation. Therefore we also considered the lemmas of the words in our evaluation sets when computing an (s, v, o) representation. We extracted all (n, v) and (v, o) pairs based on dependency labels: a noun or a pronoun modifying a verb were considered its subject if their dependency arc is labeled ”subj” or ”nsubjpass”, and its object if their dependency arc is labeled ”dobj”, ”iobj”, ”nmod” or ”xcomp”. In order to reduce sparsity, our vocabulary contains only verbs that appear at least 50 times in the corpus and nouns that appear at least 50 times\n1https://sites.google.com/site/rmyeid/projects/polyglot 2http://www.cs.cmu.edu/ ark/TurboParser/ 3We use UD so that in future work we can apply our model to other languages without major language-specific adaptations. 4https://code.google.com/archive/p/mate-tools/\nboth at subject and at object positions, a total of 6934 transitive verbs and 15701 nouns.\nTo compute the similarity between two nouns with NNSim, we trained the word2vec skip-gram model with negative sampling (Mikolov et al., 2013) on our (unparsed) training corpus; context-window size was set to 5 and vector dimensionality to 200.\nEvaluation We evaluate the performance of our models on two well established tasks for transitive verb constructions. Both tasks require ranking of transitive sentence pairs for semantic similarity. The gold standard ranking is derived from similarity scores, on a 1-7 scale, provided by human evaluators. The model ranking is evaluated against the gold standard ranking using Spearmans ρ.\nThe first task (GS11, (Grefenstette and Sadrzadeh, 2011b)) involves verb disambiguation: each of the 200 pairs in the dataset consists of two sentences that differ in their transitive verb, but share the same subject and object. For example, the members of the pair ”(man, draw, sword), (man, attract, sword)” are less similar than those of ”(report, draw, attention), (report, attract, attention)”.\nThe second task uses the transitive sentence similarity dataset (KS14, (Kartsaklis et al., 2014)). This dataset consists of 108 subject-verb-object pairs, derived from 72 subject-verb-object triplets arranged into pairs. Unlike GS11, here each pair is composed of two triplets that differ in all three words. For example, the pair ”(programme, offer, support), (service, provide, help)” is expected to get a higher similarity score compared to ”(school, encourage, child), (employee, leave, company)”.\nIn both tasks, we consider two different aggregation methods over the annotator scores of a pair: (a) the human scores of each annotator are paired with the model scores without averaging, and a ρ score is computed between the two vectors; and (b) the human scores are first averaged, a human ranking is derived from the averaged scores and compared to the model ranking. While the second method may seem more robust, it was not used in most previous works (see discussion in Mitchell and Lapata (2008)).\nModels and Baselines We compare the results of our models, distinguished by the three combi-\nnation methods: concatenation (w2v-PPMI-concat), coordination-wise multiplication (w2v-PPMI-coormult) and multiplication of the (s, v) and (v, o) scores (w2v-PPMI-mult-score). We consider several baselines. In a first, simple baseline (w2v-sum), each (s, v, o) construction is represented as the sum of the word2vec vectors of its words. This baseline, which corresponds to the unsupervised additive method of Mitchell and Lapata (2008), captures the strength of the word2vec word level representations, ignoring word order and syntactic structure considerations.5\nA second baseline (w2v-all) is similar to our method but the syntactic information is replaced with word similarity based on word2vec scores:\nusubjs,v [k] = NV Sim(nk, v) ·NNSim(nk, s)\nuobjv,o [k] = NV Sim(nk, v) ·NNSim(nk, o)\nWhere NV Sim(x, y) is the cosine similarity between the word2vec vectors of x and y. This method quantifies the importance syntax to our model.\nFinally, we compare to state-of-the-art previous work: (a) the most recent study on our tasks ((Fried et al., 2015), their table 1, 2015- best-tensor: their best model; 2015-best-simple: best result with additive or multiplicative combination as in Mitchell and Lapata (2008)); and (b) Milajevs et al. (2014) which performed exhaustive comparison of models and vector representations for our tasks (see their table 2, 2014-best-simple: best result with additive or\n5We do not report results with coordination-wise multiplication of w2v vectors, as they lag behind other reported models.\nmultiplicative combination; 2014-best-non-simple: best result with tensor and matrix combinations based on (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b; Kartsaklis et al., 2012; Kartsaklis et al., 2014)).6"
    }, {
      "heading" : "4 Results",
      "text" : "Results are presented in Table 1. Our models are superior on the GS11 task: the gaps between our best model and the best baseline are 13.2 and 4.2 ρ points for the averaged and the non-averaged conditions, respectively. When comparing to the best previous work the gaps are 13.5 and 9.7 ρ points, respectively.\nFor the KS14 task it is the simple w2v-sum baseline that performs best (ρ = 0.76 for averaged scores, ρ = 0.6 for non-averaged scores), but in both conditions it is one of our models that is second best (w2v-PPMI-concat with ρ = 0.743 for averaged scores, w2v-PPMI-coor-mult with ρ = 0.567 for non-averaged scores). Yet, in this task our gap from the baselines are smaller compared to GS11.\nThe superiority of a simple additive model for KS14 is in line with previously reported results. While in KS14 the compared (s, v, o) constructions do not overlap in their lexical content, in GS11 paired constructions differ only in the verb, hence requiring finer grained distinctions. The relative difficulty of GS11 is also reflected by the lower scores all participating models achieve on this task.\nImportantly, while the w2v-sum excels on the KS14 task, its performance substantially degrade on the GS11 task where it achieves ρ values of only 0.28 and 0.21 for the averaged and non-averaged cases respectively. Our models hence provide a sweet spot of good performance on both tasks.\nFinally, the three variants of our model perform very similarly in three out of four test conditions. It is only for KS14 with averaged human scores that w2v-PPMI-mult-score lags 5.3 and 9 ρ points behind w2v-PPMI-coor-mult and w2v-PPMI-concat, respectively. Hence, our model is flexible with respect to combination method selection.\n6Note that *-tensor, *-simple and *-non-simple do not necessarily refer to the same model at all conditions: we pick the best result for each task and human scoring combination among the models in each of these model groups."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We presented a factorized model for (s, v, o) embeddings which provides a simple and compact alternative to existing methods, and showed that it excels on two recent CDS tasks. In future work we intend to extend our model so that it accounts for more complex syntactic constructions, such as, e.g., ditransitive verb constructions and constructions that include adjectives and adverbs."
    } ],
    "references" : [ {
      "title" : "Polyglot: Distributed word representations for multilingual NLP",
      "author" : [ "Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena" ],
      "venue" : "In Proc. of CoNLL",
      "citeRegEx" : "Al.Rfou et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Al.Rfou et al\\.",
      "year" : 2013
    }, {
      "title" : "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space",
      "author" : [ "Baroni", "Zamparelli2010] Marco Baroni", "Roberto Zamparelli" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Baroni et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2010
    }, {
      "title" : "Frege in space: A program of compositional distributional semantics",
      "author" : [ "Baroni et al.2014] Marco Baroni", "Raffaela Bernardi", "Roberto Zamparelli" ],
      "venue" : "Linguistic Issues in Language Technology,",
      "citeRegEx" : "Baroni et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2014
    }, {
      "title" : "Top accuracy and fast dependency parsing is not a contradiction",
      "author" : [ "Bernd Bohnet" ],
      "venue" : "In Proc. of COLING",
      "citeRegEx" : "Bohnet.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bohnet.",
      "year" : 2010
    }, {
      "title" : "Vector space models of lexical meaning. Handbook of Contemporary Semantics, Wiley-Blackwell, à paraı̂tre",
      "author" : [ "Stephen Clark" ],
      "venue" : null,
      "citeRegEx" : "Clark.,? \\Q2012\\E",
      "shortCiteRegEx" : "Clark.",
      "year" : 2012
    }, {
      "title" : "Mathematical foundations for a compositional distributional model of meaning",
      "author" : [ "Coecke et al.2011] Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark" ],
      "venue" : "Linguistic Analysis,",
      "citeRegEx" : "Coecke et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Coecke et al\\.",
      "year" : 2011
    }, {
      "title" : "Low-rank tensors for verbs in compositional distributional semantics",
      "author" : [ "Fried et al.2015] Daniel Fried", "Tamara Polajnar", "Stephen Clark" ],
      "venue" : "In Proc. of ACL (short papers)",
      "citeRegEx" : "Fried et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fried et al\\.",
      "year" : 2015
    }, {
      "title" : "Experimental support for a categorical compositional distributional model of meaning",
      "author" : [ "Grefenstette", "Mehrnoosh Sadrzadeh" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Grefenstette et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Grefenstette et al\\.",
      "year" : 2011
    }, {
      "title" : "Experimenting with transitive verbs in a discocat",
      "author" : [ "Grefenstette", "Mehrnoosh Sadrzadeh" ],
      "venue" : "In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics",
      "citeRegEx" : "Grefenstette et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Grefenstette et al\\.",
      "year" : 2011
    }, {
      "title" : "Multi-step regression learning for compositional distributional semantics",
      "author" : [ "Georgiana Dinu", "Yao-Zhong Zhang", "Mehrnoosh Sadrzadeh", "Marco Baroni" ],
      "venue" : "In Proc. of IWCS",
      "citeRegEx" : "Grefenstette et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Grefenstette et al\\.",
      "year" : 2013
    }, {
      "title" : "A unified sentence space for categorical distributionalcompositional semantics: Theory and experiments",
      "author" : [ "Mehrnoosh Sadrzadeh", "Stephen Pulman" ],
      "venue" : "In Proc. of COLING",
      "citeRegEx" : "Kartsaklis et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kartsaklis et al\\.",
      "year" : 2012
    }, {
      "title" : "A study of entanglement in a categorical framework of natural language",
      "author" : [ "Mehrnoosh Sadrzadeh", "Chris Heunen", "Manuel L Reyes", "Ravi Kunjwal", "Tobias Fritz" ],
      "venue" : "In Proceedings of the 11th Workshop on Quantum",
      "citeRegEx" : "Kartsaklis et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kartsaklis et al\\.",
      "year" : 2014
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Le", "Mikolov2014] Quoc Le", "Tomas Mikolov" ],
      "venue" : "In Proc. of ICML",
      "citeRegEx" : "Le et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2014
    }, {
      "title" : "A sick cure for the evaluation of compositional distributional semantic models",
      "author" : [ "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli" ],
      "venue" : "In Proc. of LREC",
      "citeRegEx" : "Marelli et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Marelli et al\\.",
      "year" : 2014
    }, {
      "title" : "Turning on the turbo: Fast third-order non-projective turbo parsers",
      "author" : [ "Miguel B. Almeida", "Noah A. Smith" ],
      "venue" : "In Proc. of ACL (short papers)",
      "citeRegEx" : "Martins et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Martins et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781",
      "author" : [ "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Evaluating neural word representations in tensor-based compositional settings",
      "author" : [ "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Matthew Purver" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Milajevs et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Milajevs et al\\.",
      "year" : 2014
    }, {
      "title" : "Vector-based models of semantic composition",
      "author" : [ "Mitchell", "Lapata2008] Jeff Mitchell", "Mirella Lapata" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Mitchell et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2008
    }, {
      "title" : "A practical and linguistically-motivated approach to compositional distributional semantics",
      "author" : [ "Nghia The Pham", "Marco Baroni" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Paperno et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Paperno et al\\.",
      "year" : 2014
    }, {
      "title" : "A universal part-of-speech tagset",
      "author" : [ "Petrov et al.2012] Slav Petrov", "Dipanjan Das", "Ryan T. McDonald" ],
      "venue" : "In In Proc. of LREC",
      "citeRegEx" : "Petrov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Petrov et al\\.",
      "year" : 2012
    }, {
      "title" : "Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model",
      "author" : [ "Pham et al.2015] Nghia The Pham", "Germán Kruszewski", "Angeliki Lazaridou", "Marco Baroni" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Pham et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2015
    }, {
      "title" : "Reducing dimensions of tensors in type-driven distributional semantics",
      "author" : [ "Luana Fagarasan", "Stephen Clark" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Polajnar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Polajnar et al\\.",
      "year" : 2014
    }, {
      "title" : "Semantic compositionality through recursive matrixvector spaces",
      "author" : [ "Brody Huval", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "In Proc. of EMNLP-CoNLL",
      "citeRegEx" : "Socher et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2012
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "Patrick Pantel" ],
      "venue" : "Journal of artificial intelligence research,",
      "citeRegEx" : "Turney and Pantel,? \\Q2010\\E",
      "shortCiteRegEx" : "Turney and Pantel",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "In recent years, vector space models, deriving word meaning representations from word cooccurrence patterns in text, have become prominent in lexical semantics research (Turney et al., 2010; Clark, 2012).",
      "startOffset" : 169,
      "endOffset" : 203
    }, {
      "referenceID" : 22,
      "context" : "Some work aims at representing sentences that vary in length and structure mostly with neural network models (Socher et al., 2012; Marelli et al., 2014; Le and Mikolov, 2014; Pham et al., 2015).",
      "startOffset" : 109,
      "endOffset" : 193
    }, {
      "referenceID" : 13,
      "context" : "Some work aims at representing sentences that vary in length and structure mostly with neural network models (Socher et al., 2012; Marelli et al., 2014; Le and Mikolov, 2014; Pham et al., 2015).",
      "startOffset" : 109,
      "endOffset" : 193
    }, {
      "referenceID" : 20,
      "context" : "Some work aims at representing sentences that vary in length and structure mostly with neural network models (Socher et al., 2012; Marelli et al., 2014; Le and Mikolov, 2014; Pham et al., 2015).",
      "startOffset" : 109,
      "endOffset" : 193
    }, {
      "referenceID" : 5,
      "context" : "Recently, the categorical framework (Coecke et al., 2011; Baroni et al., 2014) has been proposed, where each word is represented by a tensor whose order is determined by the categorical grammar type of the word.",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "Recently, the categorical framework (Coecke et al., 2011; Baroni et al., 2014) has been proposed, where each word is represented by a tensor whose order is determined by the categorical grammar type of the word.",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : ", 2012; Marelli et al., 2014; Le and Mikolov, 2014; Pham et al., 2015). Another approach, which we take in this paper, is to focus on specific syntactic constructions. At the expense of generality, this approach enables an in-depth investigation of a specific linguistic phenomenon. Mitchell and Lapata (2008) proposed various additive and multiplicative operators for the combinations of word vectors, and applied them to intransitive verbs and their subjects.",
      "startOffset" : 8,
      "endOffset" : 310
    }, {
      "referenceID" : 1,
      "context" : ", 2011; Baroni et al., 2014) has been proposed, where each word is represented by a tensor whose order is determined by the categorical grammar type of the word. For example, Baroni and Zamparelli (2010) represent nouns by a vector, and adjectives by matrices transforming one noun vector into another.",
      "startOffset" : 8,
      "endOffset" : 204
    }, {
      "referenceID" : 21,
      "context" : "Indeed, several recent works have tried to reduce the size of these models (Polajnar et al., 2014; Fried et al., 2015) while others proposed matrix based representations (Polajnar et al.",
      "startOffset" : 75,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : "Indeed, several recent works have tried to reduce the size of these models (Polajnar et al., 2014; Fried et al., 2015) while others proposed matrix based representations (Polajnar et al.",
      "startOffset" : 75,
      "endOffset" : 118
    }, {
      "referenceID" : 21,
      "context" : ", 2015) while others proposed matrix based representations (Polajnar et al., 2014; Milajevs et al., 2014; Paperno et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : ", 2015) while others proposed matrix based representations (Polajnar et al., 2014; Milajevs et al., 2014; Paperno et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 127
    }, {
      "referenceID" : 18,
      "context" : ", 2015) while others proposed matrix based representations (Polajnar et al., 2014; Milajevs et al., 2014; Paperno et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 127
    }, {
      "referenceID" : 11,
      "context" : "Our model outperforms recent previous work on two established tasks for the transitive verb construction (Grefenstette and Sadrzadeh, 2011b; Kartsaklis et al., 2014).",
      "startOffset" : 105,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "Data Preprocessing and Training We trained our models on the cleaned and tokenized Polyglot Wikipedia corpus (Al-Rfou et al., 2013),1 consisting of approximately 75M sentences and 1.",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "The corpora were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using the TurboTagger (Martins et al.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : ", 2012) using the TurboTagger (Martins et al., 2013),2 trained with default settings (SVM MIRA with 20 iterations) without any further parameter fine-tuning, on the TRAIN+DEV portion of the UD treebank.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "61 (Bohnet, 2010),4 trained on the same UD treebank portion as the tagger and with default settings.",
      "startOffset" : 3,
      "endOffset" : 17
    }, {
      "referenceID" : 15,
      "context" : "To compute the similarity between two nouns with NNSim, we trained the word2vec skip-gram model with negative sampling (Mikolov et al., 2013) on our (unparsed) training corpus; context-window size was set to 5 and vector dimensionality to 200.",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 11,
      "context" : "The second task uses the transitive sentence similarity dataset (KS14, (Kartsaklis et al., 2014)).",
      "startOffset" : 71,
      "endOffset" : 96
    }, {
      "referenceID" : 10,
      "context" : "The second task uses the transitive sentence similarity dataset (KS14, (Kartsaklis et al., 2014)). This dataset consists of 108 subject-verb-object pairs, derived from 72 subject-verb-object triplets arranged into pairs. Unlike GS11, here each pair is composed of two triplets that differ in all three words. For example, the pair ”(programme, offer, support), (service, provide, help)” is expected to get a higher similarity score compared to ”(school, encourage, child), (employee, leave, company)”. In both tasks, we consider two different aggregation methods over the annotator scores of a pair: (a) the human scores of each annotator are paired with the model scores without averaging, and a ρ score is computed between the two vectors; and (b) the human scores are first averaged, a human ranking is derived from the averaged scores and compared to the model ranking. While the second method may seem more robust, it was not used in most previous works (see discussion in Mitchell and Lapata (2008)).",
      "startOffset" : 72,
      "endOffset" : 1005
    }, {
      "referenceID" : 6,
      "context" : "Finally, we compare to state-of-the-art previous work: (a) the most recent study on our tasks ((Fried et al., 2015), their table 1, 2015best-tensor: their best model; 2015-best-simple: best result with additive or multiplicative combination as in Mitchell and Lapata (2008)); and (b) Milajevs et al.",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : "Finally, we compare to state-of-the-art previous work: (a) the most recent study on our tasks ((Fried et al., 2015), their table 1, 2015best-tensor: their best model; 2015-best-simple: best result with additive or multiplicative combination as in Mitchell and Lapata (2008)); and (b) Milajevs et al.",
      "startOffset" : 96,
      "endOffset" : 274
    }, {
      "referenceID" : 6,
      "context" : "Finally, we compare to state-of-the-art previous work: (a) the most recent study on our tasks ((Fried et al., 2015), their table 1, 2015best-tensor: their best model; 2015-best-simple: best result with additive or multiplicative combination as in Mitchell and Lapata (2008)); and (b) Milajevs et al. (2014) which performed exhaustive comparison of models and vector representations for our tasks (see their table 2, 2014-best-simple: best result with additive or",
      "startOffset" : 96,
      "endOffset" : 307
    }, {
      "referenceID" : 10,
      "context" : "multiplicative combination; 2014-best-non-simple: best result with tensor and matrix combinations based on (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b; Kartsaklis et al., 2012; Kartsaklis et al., 2014)).",
      "startOffset" : 107,
      "endOffset" : 227
    }, {
      "referenceID" : 11,
      "context" : "multiplicative combination; 2014-best-non-simple: best result with tensor and matrix combinations based on (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b; Kartsaklis et al., 2012; Kartsaklis et al., 2014)).",
      "startOffset" : 107,
      "endOffset" : 227
    } ],
    "year" : 2016,
    "abstractText" : "We present a factorized compositional distributional semantics model for the representation of transitive verb constructions. Our model first produces (subject, verb) and (verb, object) vector representations based on the similarity of the nouns in the construction to each of the nouns in the vocabulary and the tendency of these nouns to take the subject and object roles of the verb. These vectors are then combined into a final (subject,verb,object) representation through simple vector operations. On two established tasks for the transitive verb construction our model outperforms recent previous work.",
    "creator" : "LaTeX with hyperref package"
  }
}