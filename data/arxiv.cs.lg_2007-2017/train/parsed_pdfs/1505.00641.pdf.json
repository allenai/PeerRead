{
  "name" : "1505.00641.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "fastFM: A Library for Factorization Machines",
    "authors" : [ "Immanuel Bayer", "Cheng Soon Ong" ],
    "emails" : [ "immanuel.bayer@uni-konstanz.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Python, MCMC, matrix factorization, context-aware recommendation"
    }, {
      "heading" : "1. Introduction",
      "text" : "This work aims to facilitate research for matrix factorization based machine learning (ML) models. Factorization Machines are able to express many different latent factor models and are widely used for collaborative filtering tasks (Rendle, 2012b). An important advantage of FM is that the model equation\nw0 ∈ R, x, w ∈ Rp, vi ∈ Rk\nŷFM (x) := w0 + p∑ i=1 wixi + p∑ i=1 p∑ j>i 〈vi, vj〉xixj (1)\nconforms to the standard notation for vector based ML. FM learn a factorized coefficient 〈vi, vj〉 for each feature pair xixj (eq. 1). This makes it possible to model very sparse feature\ninteractions, as for example, encoding a sample as x = {· · · , 0, xi︷︸︸︷ 1 , 0, · · · , 0, xj︷︸︸︷ 1 , 0, · · · } yields ŷFM (x) = w0 + wi + wj + v T i vj which is equivalent to (biased) matrix factorization Ri,j ≈ b0 + bi + bj + uTi vj (Srebro et al., 2004). Please refer to Rendle (2012b) for more encoding examples. FM have been the top performing model in various machine learning competitions (Rendle and Schmidt-Thieme, 2009; Rendle, 2012a; Bayer and Rendle, 2013) with different objectives (e.g. What Do You Know? Challenge1, EMI Music Hackathon2). fastFM includes solvers for regression, classification and ranking problems (see Table 1) and addresses the following needs of the research community: (i) easy interfacing for dynamic\n1. http://www.kaggle.com/c/WhatDoYouKnow 2. http://www.kaggle.com/c/MusicHackathon\nc©2016 Immanuel Bayer.\nar X\niv :1\n50 5.\n00 64\n1v 3\n[ cs\n.L G\n] 2\n3 N\nov 2\nand interactive languages such as R, Python and Matlab; (ii) a Python interface allowing interactive work; (iii) a publicly available test suite strongly simplifying modifications or adding of new features; (iv) code is released under the BSD-license allowing the integration in (almost) any open source project."
    }, {
      "heading" : "2. Design Overview",
      "text" : "The fastFM library has a multi layered software architecture (Figure 1) that separates the interface code from the performance critical parts (fastFM-core). The core contains the solvers, is written in C and can be used stand alone. Two user interfaces are available: a command line interface (CLI) and a Python interface. Cython (Behnel et al., 2011) is used to create a Python extension from the C library. Both, the Python and C interface, serve as reference implementation for bindings to additional languages.\n2.1 fastFM-core\nfastFM (Py) Cython CLI\nfastFM-core (C)\nstraight forward. fastFM contains a test suite that is run on each commit to the GitHub repository via a continuous integration server4. Solvers are tested using state of the art techniques, such as Posterior Quantiles (Cook et al., 2006) for the MCMC sampler and Finite Differences for the SGD based solvers."
    }, {
      "heading" : "2.2 Solver and Loss Functions",
      "text" : "fastFM provides a range of solvers for all supported tasks (Table 1). The MCMC solver implements the Bayesian Factorization Machine model (Freudenthaler et al., 2011) via Gibbs sampling. We use the pairwise Bayesian Personalized Ranking (BPR) loss (Rendle et al., 2009) for ranking. More details on the classification and regression solvers can be found in Rendle (2012b).\n3. CXSparse is LGPL licensed. 4. https://travis-ci.org/ibayer/fastFM-core"
    }, {
      "heading" : "2.3 Python Interface",
      "text" : "The Python interface is compatible with the API of the widely-used scikit-learn library (Pedregosa et al., 2011) which opens the library to a large user base. The following code snippet shows how to use MCMC sampling for an FM classifier and how to make predictions on new data.\nfm = mcmc.FMClassification(init std=0.01, rank=8) y pred = fm.fit predict(X train, y train, X test)\nfastFM provides additional features such as warm starting a solver from a previous solution (see MCMC example).\nfm = als.FMRegression(init std=0.01, rank=8, l2 reg=2) fm.fit(X train, y train)"
    }, {
      "heading" : "3. Experiments",
      "text" : "libFM5 is the reference implementation for FM and the only one that provides ALS and MCMC solver. Our experiments show, that the ALS and MCMC solver in fastFM compare favorable to libFM with respect to runtime (Figure 2) and are indistinguishable in terms of accuracy. The experiments have been conducted on the MovieLens 10M data set using the original split with a fixed number of 200 iterations for all experiments. The x-axis indicates the number of latent factors (rank), and the y-axis the runtime in seconds. The plots show that the runtime scales linearly with the rank for both implementations. The code snippet\nbelow shows how simple it is to write Python code that allows model inspection after every iteration. The induced Python function call overhead occurs only once per iteration and is therefore neglectable. This feature can be used for Bayesian Model Checking as demonstrated in Figure 3. The figure shows MCMC summary statistics for the first order hyper parameter σw. Please note that the MCMC solver uses Gaussian priors for the model parameter (Freudenthaler et al., 2011).\n5. http://libfm.org\nfm = mcmc.FMRegression(n iter=0) # initialize coefficients fm.fit predict(X train, y train, X test)\nfor i in range(number of iterations): y pred = fm.fit predict(X train, y train, X test, n more iter=1) # save, or modify (hyper) parameter print(fm.w , fm.V , fm.hyper param )\nMany other analyses and experiments can be realized with a few lines of Python code without the need to read or recompile the performance critical C code."
    }, {
      "heading" : "4. Related Work",
      "text" : "Factorization Machines are available in the large scale machine learning libraries GraphLab (Low et al., 2014) and Bidmach (Canny and Zhao, 2013). The toolkit Svdfeatures by Chen et al. (2012) provides a general MF model that is similar to a FM. The implementations in GraphLab, Bidmach and Svdfeatures only support SGD solvers and don’t provide a ranking loss. It’s not our objective to replace these distributed machine learning frameworks: but to be provide a FM implementation that is easy to use and easy to extend without sacrificing performance."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the DFG under grant Re 3311/2-1."
    } ],
    "references" : [ {
      "title" : "Cython: The best of both worlds",
      "author" : [ "Stefan Behnel", "Robert Bradshaw", "Craig Citro", "Lisandro Dalcin", "Dag Sverre Seljebotn", "Kurt Smith" ],
      "venue" : "Computing in Science & Engineering,",
      "citeRegEx" : "Behnel et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Behnel et al\\.",
      "year" : 2011
    }, {
      "title" : "Bidmach: Large-scale learning with zero memory allocation",
      "author" : [ "John Canny", "Huasha Zhao" ],
      "venue" : "In BigLearn Workshop,",
      "citeRegEx" : "Canny and Zhao.,? \\Q2013\\E",
      "shortCiteRegEx" : "Canny and Zhao.",
      "year" : 2013
    }, {
      "title" : "Svdfeature: a toolkit for feature-based collaborative filtering",
      "author" : [ "Tianqi Chen", "Weinan Zhang", "Qiuxia Lu", "Kailong Chen", "Zhao Zheng", "Yong Yu" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Chen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2012
    }, {
      "title" : "Validation of software for bayesian models using posterior quantiles",
      "author" : [ "Samantha R Cook", "Andrew Gelman", "Donald B Rubin" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "Cook et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cook et al\\.",
      "year" : 2006
    }, {
      "title" : "Direct methods for sparse linear systems, volume 2. Siam",
      "author" : [ "Timothy A Davis" ],
      "venue" : null,
      "citeRegEx" : "Davis.,? \\Q2006\\E",
      "shortCiteRegEx" : "Davis.",
      "year" : 2006
    }, {
      "title" : "Bayesian factorization machines",
      "author" : [ "Christoph Freudenthaler", "Lars Schmidt-thieme", "Steffen Rendle" ],
      "venue" : "In Proceedings of the NIPS Workshop on Sparse Representation and Low-rank Approximation,",
      "citeRegEx" : "Freudenthaler et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Freudenthaler et al\\.",
      "year" : 2011
    }, {
      "title" : "Graphlab: A new framework for parallel machine learning",
      "author" : [ "Yucheng Low", "Joseph E Gonzalez", "Aapo Kyrola", "Danny Bickson", "Carlos E Guestrin", "Joseph Hellerstein" ],
      "venue" : "arXiv preprint arXiv:1408.2041,",
      "citeRegEx" : "Low et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Low et al\\.",
      "year" : 2014
    }, {
      "title" : "Scikit-learn: Machine learning in Python",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Social network and click-through prediction with factorization machines",
      "author" : [ "Steffen Rendle" ],
      "venue" : "In KDD-Cup Workshop,",
      "citeRegEx" : "Rendle.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rendle.",
      "year" : 2012
    }, {
      "title" : "Factorization machines with libFM",
      "author" : [ "Steffen Rendle" ],
      "venue" : "ACM Trans. Intell. Syst. Technol.,",
      "citeRegEx" : "Rendle.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rendle.",
      "year" : 2012
    }, {
      "title" : "Factor models for tag recommendation in bibsonomy",
      "author" : [ "Steffen Rendle", "Lars Schmidt-Thieme" ],
      "venue" : "ECML/PKDD",
      "citeRegEx" : "Rendle and Schmidt.Thieme.,? \\Q2008\\E",
      "shortCiteRegEx" : "Rendle and Schmidt.Thieme.",
      "year" : 2008
    }, {
      "title" : "Bpr: Bayesian personalized ranking from implicit feedback",
      "author" : [ "Steffen Rendle", "Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt-Thieme" ],
      "venue" : "In UAI",
      "citeRegEx" : "Rendle et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Rendle et al\\.",
      "year" : 2009
    }, {
      "title" : "Maximum-margin matrix factorization",
      "author" : [ "Nathan Srebro", "Jason Rennie", "Tommi S Jaakkola" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Srebro et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "This makes it possible to model very sparse feature interactions, as for example, encoding a sample as x = {· · · , 0, xi {}}{ 1 , 0, · · · , 0, xj {}}{ 1 , 0, · · · } yields ŷFM (x) = w0 + wi + wj + v T i vj which is equivalent to (biased) matrix factorization Ri,j ≈ b0 + bi + bj + ui vj (Srebro et al., 2004).",
      "startOffset" : 290,
      "endOffset" : 311
    }, {
      "referenceID" : 8,
      "context" : "Please refer to Rendle (2012b) for more encoding examples.",
      "startOffset" : 16,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "Cython (Behnel et al., 2011) is used to create a Python extension from the C library.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "We use the standard compressed row storage (CRS) matrix format as underlying data structure and rely on the CXSparse3 library (Davis, 2006) for fast sparse matrix / vector operations.",
      "startOffset" : 126,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : "Solvers are tested using state of the art techniques, such as Posterior Quantiles (Cook et al., 2006) for the MCMC sampler and Finite Differences for the SGD based solvers.",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "The MCMC solver implements the Bayesian Factorization Machine model (Freudenthaler et al., 2011) via Gibbs sampling.",
      "startOffset" : 68,
      "endOffset" : 96
    }, {
      "referenceID" : 11,
      "context" : "We use the pairwise Bayesian Personalized Ranking (BPR) loss (Rendle et al., 2009) for ranking.",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "Task Solver Loss Regression ALS, MCMC, SGD Square Loss Classification ALS, MCMC, SGD Probit (MAP), Probit, Sigmoid Ranking SGD BPR (Rendle et al., 2009) Table 1: Supported solvers and tasks",
      "startOffset" : 131,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "The MCMC solver implements the Bayesian Factorization Machine model (Freudenthaler et al., 2011) via Gibbs sampling. We use the pairwise Bayesian Personalized Ranking (BPR) loss (Rendle et al., 2009) for ranking. More details on the classification and regression solvers can be found in Rendle (2012b). Task Solver Loss Regression ALS, MCMC, SGD Square Loss Classification ALS, MCMC, SGD Probit (MAP), Probit, Sigmoid Ranking SGD BPR (Rendle et al.",
      "startOffset" : 69,
      "endOffset" : 302
    }, {
      "referenceID" : 7,
      "context" : "3 Python Interface The Python interface is compatible with the API of the widely-used scikit-learn library (Pedregosa et al., 2011) which opens the library to a large user base.",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "Please note that the MCMC solver uses Gaussian priors for the model parameter (Freudenthaler et al., 2011).",
      "startOffset" : 78,
      "endOffset" : 106
    }, {
      "referenceID" : 6,
      "context" : "Related Work Factorization Machines are available in the large scale machine learning libraries GraphLab (Low et al., 2014) and Bidmach (Canny and Zhao, 2013).",
      "startOffset" : 105,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : ", 2014) and Bidmach (Canny and Zhao, 2013).",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : ", 2014) and Bidmach (Canny and Zhao, 2013). The toolkit Svdfeatures by Chen et al. (2012) provides a general MF model that is similar to a FM.",
      "startOffset" : 21,
      "endOffset" : 90
    } ],
    "year" : 2016,
    "abstractText" : "Factorization Machines (FM) are currently only used in a narrow range of applications and are not yet part of the standard machine learning toolbox, despite their great success in collaborative filtering and click-through rate prediction. However, Factorization Machines are a general model to deal with sparse and high dimensional features. Our Factorization Machine implementation (fastFM) provides easy access to many solvers and supports regression, classification and ranking tasks. Such an implementation simplifies the use of FM for a wide range of applications. Therefore, our implementation has the potential to improve understanding of the FM model and drive new development.",
    "creator" : "LaTeX with hyperref package"
  }
}