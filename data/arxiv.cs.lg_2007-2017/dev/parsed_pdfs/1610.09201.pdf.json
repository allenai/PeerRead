{
  "name" : "1610.09201.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Conceptual Development of Quench Prediction App build on LSTM and ELQA framework",
    "authors" : [ "Matej Mertika", "Maciej Wielgoszb", "Andrzej Skoczeńc" ],
    "emails" : [ "matej.mertik@cern.ch", "wielgosz@agh.edu.pl", "skoczen@ftj.agh.edu.pl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "This article presents a development of web application for quench prediction in Technology Department, Machine Protection and Electrical Integrity group, Electrical Engineering (TE-MPE-EE) at CERN. The authors describe an ELectrical Quality Assurance (ELQA) framework, a platform which was designed for rapid development of web integrated data analysis applications for different analysis needed during the hardware commissioning of the Large Hadron Collider (LHC). In second part the article describes a research carried out with the data collected from Quench Detection System by means of using an LSTM recurrent neural network. The article discusses and presents a conceptual work of implementing quench prediction application for TEMPE-EE based on the ELQA and quench prediction algorithm.\nKeywords: Software development, Data analysis, Web applications, Electrical Quality Assurance (ELQA), Large Hadron Collider, Deep Learning, Recurrent neural networks"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "The Large Hadron Collider (LHC) is the world’s largest and complex experimental facility ever created. It was built by the European Organization for Nuclear Research (CERN) between 1998 and 2008 in collaboration with over 10,000 scientists and engineers from over 100 countries, as well as hundreds of universities and laboratories. Its 27 kilometre circumference incorporates (among others) 1232 superconducting dipole magnets that lie beneath France – Switzerland border near Geneva.\nStarting up this largest and complex experimental facility requires many teams of professionals and complex procedures due to its superconducting characteristic in the domain of cryogenics, electrical engineering, electronics, computer science, material science, physics and others. The TE-MPE-EE at CERN provides an important task by\nEmail addresses: matej.mertik@cern.ch (Matej Mertik), wielgosz@agh.edu.pl (Maciej Wielgosz), skoczen@ftj.agh.edu.pl (Andrzej Skoczeń)\nPreprint submitted to ArXiv October 31, 2016\nar X\niv :1\n61 0.\n09 20\n1v 1\n[ cs\n.L G\n] 2\n5 O\nhardware commissioning of the LHC machine, where all circuits are verified with many electrical measurements in the machine in different conditions during the complex procedure. The signals from measurements are stored in different systems at CERN, where data are available for their analysis. ELQA team started to develop a data analytic application to look through patterns implemented in the hardware commissioning in 2008 and 2014. As such a framework for prototyping data analysis apps was developed in the TE-MPE-EE.\nThe ELQA framework provides building blocks for designing web application for data analysis. In this paper we present a conceptual development of Quench Prediction Application from the research work on the prediction of quenches and its preliminary results. We show the concept of integration of\n(a) a research on a deep learning algorithm for quench prediction and (b) a research in software design for prototyping data analysis application for TE-\nMPE-EE.\nThe paper is structured as following. In first chapters there is a description of the ELQA framework and software libraries used by software design, then LSTM concept for predicting quenches with deep learning is described on the real data. In the last part we are presenting a way and a concept of implementing the Quench Prediction App for TE-MPE-EE at CERN."
    }, {
      "heading" : "2. ELQA FRAMEWORK",
      "text" : "ELQA framework was developed for rapid design of data analysis application at TE-MPE-EE. Framework was designed for ELQA group where there is a need to address a vast amount of data that are collected in the hardware commissioning of the Large Hadron Collider (LHC). In the commissioning procedure the conditions of the circuits of the machine are verified and implemented by different measurements through all 8 sectors of the machine. Based on software design following requirements were identified carefully:\n(a) searching signal’s similarities over the vast amount of tests, (b) visualizing trends of measurements through time domain in data, (c) developing corresponding data models for different flavors of ELQA measure-\nments such as TP4 (Test Procedure 4), DOC (Dipole Orbit Correctors), MIC (Magnet Instrumentation Check),\n(d) developing different scenarios of visualization of different group of measurements.\nTwo additional software characteristics were specially addressed:\n(a) application should run in web environment and (b) framework should support high interaction for the user when exploring the data.\nAs such ELQA framework provide different components for building the web applications. There are three main libraries used based on which the concept was defined:\n(a) Bokeh Python interactive visualization library that targets modern web browsers for presentation [1], library was selected when testing the visualisation libraries and requirements [2],\n(b) Django high-level Python Web framework for Object Oriented paradigm to accessing relational dataset of ELQA [3], and\n(c) Scikit-learn kit, a Machine Learning library used for the implementation of the KDD algorithms [4].\nWithin these three main libraries other dependent libraries and components of CERN software infrastructure were integrated. The relationships between them are shown in Fig. 1.\nFollowing subsections explain some detailed characteristic of the framework."
    }, {
      "heading" : "2.1. ELQA FRAMEWORK CHARACTERISTICS",
      "text" : ""
    }, {
      "heading" : "2.1.1. Data models",
      "text" : "Access to the data is addressed with the Object-Relational Mapping. Django framework handles this mapping providing all the functionality of a Structure Query Language (SQL). The architecture of Django is layered with\n(a) the bottom layer which is the ELQA database, followed by (b) access library that is responsible for communication between Python and the database\nthrough SQL statements, here we used a Python library cx oracle [5] and\n(c) specific Django database back-end where any kind of database supported by Django (PostgreSQL, MySql, SQLite and Oracle) is supported [3].\nAbove Django’s layers a data model is then defined for accessing the data of ELQA. Data models can be efficient programmed using the Django tools such as inspectdb for automatically defining the models from database tables, where relationships between the tables are separately defined."
    }, {
      "heading" : "2.1.2. Preprocessors",
      "text" : "Preprocessors are classes for analytical preprocessing of the signals within defined operators by the ELQA engineers. For the extraction of signals currently following basic statistics are used as extractors (and can be extended): average, minimal value, maximal value, skewness, kurtosis and properties of the linear regression applied to each signal such as slope and standard error. Scipy and numpy libraries [6, 7] are used by preprocessors implementation."
    }, {
      "heading" : "2.1.3. Data-miners",
      "text" : "Scikit-learn, a Machine Learning library written in Python is selected for data miners in the ELQA framework. It is built on NumPy, SciPy, and matplotlib and it is licenced under BSD open source licence [4]. Although there are many solutions and tools for data analysis available as open-source or commercial packages, some of the most used tools from the domains of Data Mining, Analytics, Big Data, and Data Science can be found on KDnuggets network maintained by a group of professionals and researchers [8], Scikit-learn was selected as most appropriate as is written in Python and importantly has a strong community around the library.\nThe data-miner layer defines general class for analyzer. Currently two different clustering algorithms are integrated from the scikit-learn such as K-means clustering [9] and DBScan clustering, a density-based spatial clustering of data with noise proposed by Martin Ester, Hans-Peter Kriegel, Jorg Sander and Xiaowei Xu in 1996 [10].\nThe analyzer class can be extended with other techniques from scikit-learn library or other libraries and algorithms in the framework. In the following section we present a concept of using LSTM-based solution for detecting and predicting quenches."
    }, {
      "heading" : "2.1.4. Visualizers",
      "text" : "Visualizers serve user interaction and visualization of the data. When searching for appropriate tools in communities many frameworks in different programming languages were reconsidered. Due to the required architecture (see other evaluated libraries on the Fig. 1) Bokeh was selected after studying the appropriate available solutions. Main reason was Bokeh’s ability to act as a web server in order to expose a fully-fledged data visualization and analysis tool to multiple users [2]."
    }, {
      "heading" : "3. CONCEPT OF USING THE LSTM-BASED SOLUTION",
      "text" : "The idea behind using LSTM algorithm for quench detection and prediction is based on unique and very useful properties of this deep learning algorithm in time series modelling. It turns out that in some cases a currently used quench protection\nsolution [11, 12] can be supplemented with an additional monitoring layer which monitors a performance of the Quench Protection System.\nOne of the biggest challenges in implementing of the LSTM concept is the construction of the network architecture by choosing a right set of hyper parameters such as number of layers, neurons within each layer and input size. This is done in trial-anderror fashion which can be substantially leveraged by using an appropriate framework. Such a framework allows for visualization of the performance a given network instance as well as easy manipulation of its macro parameter values."
    }, {
      "heading" : "3.1. RECURRENT NEURAL NETWORKS",
      "text" : "Phenomena occurring in a real world may be perceived in a spatial and temporal space. Since the neural models are supposed to mimic the real world the two different branches of neural networks evolved which address applications belonging to the aforementioned categories. Feed-forward and CNN networks are meant and most often used in applications of the spatial domain, whereas various kinds of recurrent models such as RNN, LSTM and GRU are especially useful in temporal modelling tasks.\nUnlike traditional models that are mostly based on hand-crafted features deep learning neural networks can operate directly on raw data. This makes them especially useful in applications where extracting features is very hard and even sometimes impossible. It turns out that there are many fields of applications where no experts exist who can handle feature extraction or the area is simply uncharted and we do not know whether the data contains latent patterns worth exploring [13–15].\nFoundations of the most currently used neural networks architectures were laid between 1960 and 1990. For almost the last two decades researchers were not able to take full advantage of these powerful models and there were claims that they are inherently ineffective. The whole machine learning landscape changed in early 2010 where deep learning algorithms started to achieve state-of-the-art results in a wide range of learning tasks. The breakthrough was brought about by several factors, among which computing power, deluge of widely available data and affordable storage are considered to be the critical ones. It is worth noting that in the presence of large amount of data the conventional simple linear models tend to under-fit or under-utilize computing resources.\nDeep learning models can operate in both spatial and temporal domain. However, standard feed-forward and CNN neural networks are well suited for spatial domain [16–18], in contrast to recurrent neural networks which perform much better in the applications of temporal domain [19–21]. CNNs and feed-forward networks rely on the assumption of independence of data within training and testing set as presented in Fig. 2. This means that after each training item is presented to the model, the current state of the network is lost i.e. temporal information is not taken into account in training a model.\nIn a case of the independent data it is not an issue. But for data which contain crucial time or space relationships it may lead to a loss of majority of the information which is located in between steps. Additionally, feed-forward models expect fixed length of training vectors which is not always the case, especially when dealing with time domain data.\nRecurrent neural networks (RNNs) are models with the ability to process sequential data as one element at a time. Thus they can simultaneously model sequential and time dependencies on multiple scales. Unfortunately, a range of practical applications of standard RNN architectures is quite limited. This is caused by the influence of a given input on a hidden and output layer during the training of the network. It either decays or blows up exponentially as it moves across recurrent connections. This effect is described as the vanishing gradient problem [22]. There were many unsuccessful attempts to address the problem before LSTM was eventually introduced by Hochreiter and Schmidhuber [23] which ultimately solved it.\nRecurrent neural networks may be visualized both as a looped-back architectures of interconnected neurons which was presented in Fig. 2. Originally RNNs were meant to be used with single variable signals but they were also adapted for multiple stream inputs [22].\nIt is a common practice to use feed-forward recurrent layers together in order to map outputs from RNN or LSTM to the result space as presented in Fig. 4.\nIn practical applications, the LSTM has shown extraordinary ability to learn longrange dependencies as compared to standard RNNs. Therefore, most of the state-ofthe-art applications use LSTM model [24].\nThe LSTM internal structure is based on a set of connected memory blocks as presented in Fig. 5. The memory blocks contain memory cells with loop-backed connections storing the temporal state of the network in addition to boundary units called cell which serve as an interface for information propagation within the network.\nThere are three different gates in each memory block:\n(a) input gate which controls input activations into the memory cell, (b) output gate controls cell outflow of activations into the rest of the network,\n(c) f orgate gate scales the internal state of the cell before summing it with the input through the self-recurrent connection of the cell. This enables gradual forgetting of a cell memory state.\nModern LSTM architectures may also contain peephole connections [25]. Since we do not use in our experiment it was not depicted in Fig. 5 and addressed in this description.\nThe output of each LSTM block is calculated according to the following set of equations:\ng(t) = φ(Wgxx(t) + Wghh(t−1) + bg) (1)\ni(t) = σ(Wixx(t) + Wihh(t−1) + bi) (2)\nf (t) = σ(W f xx(t) + W f hh(t−1) + b f ) (3)\no(t) = σ(Woxx(t) + Wohh(t−1) + bo) (4)\ns(t) = g(t) i(t) + s(t−1) f (t) (5)\nh(t) = φ(s(t)) o(t) (6) While examining equations 1, 2, 3, 4 and 5 it may be noticed that instances for a current and previous time step are used for the value of hidden layer vector h as well as for internal state s. Consequently, ht denotes a value of a hidden state vector at a current time step, whereas ht−1 refers to previous step. It is also worth noting the equations contain vector notation which means that they address whole set of LSTM cells. In order to address a single cell a subscript c is used as it is presented in Fig. 5 where for instance htc refers to the value of hidden state within this particular cell.\nLSTM based network learns when to let activation into the internal states of its cells and when to let the activation out. This is implemented with gating mechanism in which all the gates are considered as separate constitutes of LSTM block with their own learning capability. This means that they adapt in a training process as separate units to preserve a proper information flow. When both the gates are closed, the activation is cut-off from the outside connections, neither growing or shrinking, nor affecting the output at intermediate time steps. In order to make it possible a hard sigmoid function σ was used which can output 0 and 1 as given by Eq. 7. This means that the gates can be fully open or closed.\nσ(x) =  0 if x ≤ tl ax + b if x ∈ (tl, th) 1 if x ≥ th\n(7)\nIn terms of the backward pass, so-called constant error carousel enables the gradient to propagate back through many time steps, neither exploding or vanishing [23, 24]."
    }, {
      "heading" : "3.2. EXPERIMENTAL SETUP",
      "text" : "Timber database stores a record of many years of the magnets activity. This is a huge amount of data with relatively few quench events. The first research question we had to address was related to a time span surrounding a quench event which should be taken into account for the LSTM model training. One day-long record of a single voltage time series for only one magnet occupies roughly 100 MB and there are several voltage time series associated with a single magnet [26] but ultimately we decided to use only one in our experiments, namely Ures.\nVarious kinds of magnets are used in LHC and they generate different number of quench events. It is highly beneficial from a perspective of our research to pick a magnet for which large possible number of quenches is provided. The longest history of quenches was provided for 600A magnets in Timber database. Therefore, we decided to focus our initial research on 600A magnets. Unfortunately, the data covering a quench periods of 600A magnets is very large i.e. an order of several gigabytes. However, as it was mentioned before, the activity record of superconducting magnets during operational time of LHC is composed mostly of sections of normal work and partially of quench events. Furthermore, Timber does not enable automated quench periods extraction, despite having many useful features for data preprocessing and information extraction.\nTraining deep learning models takes a long time even when fast GPUs are employed. Therefore, a choice of a size of a training data and a set of hyper-parameters of a model is critical in terms of the application performance. In course of experiments we discovered that it is faster to first train a model with a small data set and preliminarily adjust hyper-parameters and then tweak them on bigger data sets.\nConsequently, we have created 3 different data sets: small, medium and the large ones as presented in Tab. 1.\nThe tests are in progress with a main goal of validating the LSTM-based setup capability of modeling superconducting magnets behavior. The resolution of the data acquired from Timber database is far too low to determine its ability to predict or detect quenches. This is will require using more data of higher resolution, such as post-mortem records, and is planned as the future work."
    }, {
      "heading" : "4. INTEGRATING LSTM MODEL INTO ELQA FRAMEWORK",
      "text" : "Described LSTM concept developed at TE-MPE-EE can be efficiently used within ELQA framework for implementing quench prediction application for the end user.\nFollowing characteristics and key benefits are identified for joining these two pieces of research\n(a) efficient application of the research conducted on quench detection and prediction, (b) incorporating quench detection application into web environment (c) need of an efficient graphical user interface for end user, (d) availability of high user interaction experience on the developed application for\nquench detection.\nFig. 6 presents possible integration of the LSTM into the framework. It can be seen that LSTM concept should be wrapped by the new LSTM analyzer class and then used in the ELQA framework. The graphical user interface is a next step that should be designed through the definition of the Quench Prediction App Dashboard, a key part for any web apps developed for data analysis based on ELQA.\nFollowing subsection provides an overview for integration process of the deep learning LSTM."
    }, {
      "heading" : "4.1. INTEGRATING LSTM-BASED SOLUTION",
      "text" : "Analyser class in ELQA is a general class where data miner algorithms are integrated from Scikit-learn data mining library. LSTM concept for quench prediction is implemented in Python at TE-MPE-EE and can be logical integrated into ELQA with an extended LSTM from the Analyser.\nAccessing the data in Timber database will be extended by new Data model. ELQA framework allows to define new stack for integrating the access to the various datasets and stacks through Django library. Fig. 7 presents a possible integration.\nWhen model and algorithm are incorporated into the ELQA classes, full development of the dashboard can be then applied through the framework for user interface and interaction developed for a Quench prediction app."
    }, {
      "heading" : "4.2. IMPLEMENTING QUENCH PREDICTION DASHBOARD - APP DEVELOPMENT",
      "text" : "With incorporated data model to access to Timber dataset and wrapped LSTM algorithm of Quench Prediction Software within Analyser class, efficient development of the web application for Quench prediction can be applied through programming of inherited LSTM dashboard. IN LSTM Dashboard widgets for Graphical User Interface are defined and then integrated through Bokeh library (input forms, plots), where data are applied to LSTM algorithm. Fig. 8 presents the concept of the Dashboard implementation by ELQA.\nIt can be seen (Fig. 8) that conceptual building blocks when programming Dashboard for quench prediction application follow objects that need to be defined in the Dashboard:\n(a) input widget for user interaction (lists, input boxes, sliders etc..),\n(b) plots widget for plotting series, (c) data sources from the data model (Django model for accessing Timber) and (d) LSTM algorithm as data-miner for quench prediction application."
    }, {
      "heading" : "5. CONCLUSIONS",
      "text" : "We presented a concept paper of integrating the LSTM techniques for quench detection into the web application used the ELQA framework for prototyping data analysis application developed at TE-MPE-EE at CERN. Main characteristics of the framework were presented as the concept of LSTM for quench detection. We showed the mechanism and key benefits of joining these two entities for developing a quench prediction app for TE-MPE-EE at CERN. As explained, the LSTM concept can be integrated efficiently in order to provide an application that can be used by the engineers when providing hardware commissioning procedures of the LHC at CERN."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The framework is being tested to prototype data application by the ELQA team. We would like to thank the entire ELQA team and TE-MPE-EE at CERN for their persistent invaluable help throughout the development process."
    } ],
    "references" : [ {
      "title" : "V",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss" ],
      "venue" : "Dubourg, et al., Scikit-learn: Machine learning in python, Journal of Machine Learning Research 12 (Oct) ",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "et al",
      "author" : [ "E. Jones", "T. Oliphant", "P. Peterson" ],
      "venue" : "SciPy: Open source scientific tools for Python [online] ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The NumPy array: a structure for efficient numerical computation, Computing in Science & Engineering",
      "author" : [ "S. Van Der Walt", "S.C. Colbert", "G. Varoquaux" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Algorithm AS 136: A K-Means Clustering Algorithm",
      "author" : [ "J.A. Hartigan", "M.A. Wong" ],
      "venue" : "Journal of the Royal Statistical Society. Series C (Applied Statistics) 28 (1) ",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "A density-based algorithm for discovering clusters in large spatial databases with noise",
      "author" : [ "M. Ester", "H.-P. Kriegel", "J. Sander", "X. Xu" ],
      "venue" : "in: Kdd, Vol. 96, AAAI Press",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "LHC magnet quench protection system",
      "author" : [ "L. Coull", "D. Hagedorn", "V. Remondino", "F. Rodriguez-Mateos" ],
      "venue" : "IEEE Transactions on Magnetics 30 (4) ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Reliability assessment of the LHC machine protection system",
      "author" : [ "R. Filippini", "B. Dehning", "G. Guaglio", "F. Rodriguez-Mateos", "R. Schmidt", "B. Todd", "J. Uythoven", "A. Vergara-Fernandez", "M. Zerlauth" ],
      "venue" : "in: Proceedings of the 2005 Particle Accelerator Conference",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Application of deep learning for recognizing infant cries",
      "author" : [ "C.Y. Chang", "J.J. Li" ],
      "venue" : "in: 2016 IEEE International Conference on Consumer Electronics-Taiwan (ICCE- TW)",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deep maxout neural networks for speech recognition",
      "author" : [ "M. Cai", "Y. Shi", "J. Liu" ],
      "venue" : "in: Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Phone recognition with deep sparse rectifier neural networks",
      "author" : [ "L. Tth" ],
      "venue" : "in: 2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep learning",
      "author" : [ "Y. LeCun", "Y. Bengio", "G. Hinton" ],
      "venue" : "Nature 521 (7553) ",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning hierarchical features for scene labeling",
      "author" : [ "C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence 35 (8) ",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep learning convolutional networks",
      "author" : [ "Y. LeCun" ],
      "venue" : "in: 2015 IEEE Hot Chips 27 Symposium (HCS)",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Analysis of recurrent neural networks for probabilistic modeling of driver behavior",
      "author" : [ "J. Morton", "T.A. Wheeler", "M.J. Kochenderfer" ],
      "venue" : "IEEE Transactions on Intelligent Transportation Systems PP (99) ",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Recurrent neural networks for sequential phenotype prediction in genomics",
      "author" : [ "F. Pouladi", "H. Salehinejad", "A.M. Gilani" ],
      "venue" : "in: 2015 International Conference on Developments of E-Systems Engineering (DeSE)",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Efficient training and evaluation of recurrent neural network language models for automatic speech recognition",
      "author" : [ "X. Chen", "X. Liu", "Y. Wang", "M.J.F. Gales", "P.C. Woodland" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing 24 (11) ",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Supervised Sequence Labelling with Recurrent Neural Networks",
      "author" : [ "A. Graves" ],
      "venue" : "Springer Berlin Heidelberg",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural Comput. 9 (8) ",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "C",
      "author" : [ "Z.C. Lipton", "J. Berkowitz" ],
      "venue" : "Elkan, A critical review of recurrent neural networks for sequence learning ",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "J",
      "author" : [ "K. Greff", "R.K. Srivastava", "J. Koutnk", "B.R. Steunebrink" ],
      "venue" : "Schmidhuber, LSTM: A search space odyssey ",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The CERN accelerator logging service - 10 years in operation: A look at the past",
      "author" : [ "C. Roderick", "L. Burdzanowski", "G. Kruk" ],
      "venue" : "present, and future, Proceedings of ICALEPCS2013 ",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "(a) Bokeh Python interactive visualization library that targets modern web browsers for presentation [1], library was selected when testing the visualisation libraries and requirements [2], (b) Django high-level Python Web framework for Object Oriented paradigm to accessing relational dataset of ELQA [3], and (c) Scikit-learn kit, a Machine Learning library used for the implementation of the KDD algorithms [4].",
      "startOffset" : 410,
      "endOffset" : 413
    }, {
      "referenceID" : 1,
      "context" : "Scipy and numpy libraries [6, 7] are used by preprocessors implementation.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 2,
      "context" : "Scipy and numpy libraries [6, 7] are used by preprocessors implementation.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "It is built on NumPy, SciPy, and matplotlib and it is licenced under BSD open source licence [4].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "Currently two different clustering algorithms are integrated from the scikit-learn such as K-means clustering [9] and DBScan clustering, a density-based spatial clustering of data with noise proposed by Martin Ester, Hans-Peter Kriegel, Jorg Sander and Xiaowei Xu in 1996 [10].",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "Currently two different clustering algorithms are integrated from the scikit-learn such as K-means clustering [9] and DBScan clustering, a density-based spatial clustering of data with noise proposed by Martin Ester, Hans-Peter Kriegel, Jorg Sander and Xiaowei Xu in 1996 [10].",
      "startOffset" : 272,
      "endOffset" : 276
    }, {
      "referenceID" : 5,
      "context" : "solution [11, 12] can be supplemented with an additional monitoring layer which monitors a performance of the Quench Protection System.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 6,
      "context" : "solution [11, 12] can be supplemented with an additional monitoring layer which monitors a performance of the Quench Protection System.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "It turns out that there are many fields of applications where no experts exist who can handle feature extraction or the area is simply uncharted and we do not know whether the data contains latent patterns worth exploring [13–15].",
      "startOffset" : 222,
      "endOffset" : 229
    }, {
      "referenceID" : 8,
      "context" : "It turns out that there are many fields of applications where no experts exist who can handle feature extraction or the area is simply uncharted and we do not know whether the data contains latent patterns worth exploring [13–15].",
      "startOffset" : 222,
      "endOffset" : 229
    }, {
      "referenceID" : 9,
      "context" : "It turns out that there are many fields of applications where no experts exist who can handle feature extraction or the area is simply uncharted and we do not know whether the data contains latent patterns worth exploring [13–15].",
      "startOffset" : 222,
      "endOffset" : 229
    }, {
      "referenceID" : 10,
      "context" : "However, standard feed-forward and CNN neural networks are well suited for spatial domain [16–18], in contrast to recurrent neural networks which perform much better in the applications of temporal domain [19–21].",
      "startOffset" : 90,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "However, standard feed-forward and CNN neural networks are well suited for spatial domain [16–18], in contrast to recurrent neural networks which perform much better in the applications of temporal domain [19–21].",
      "startOffset" : 90,
      "endOffset" : 97
    }, {
      "referenceID" : 12,
      "context" : "However, standard feed-forward and CNN neural networks are well suited for spatial domain [16–18], in contrast to recurrent neural networks which perform much better in the applications of temporal domain [19–21].",
      "startOffset" : 90,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "However, standard feed-forward and CNN neural networks are well suited for spatial domain [16–18], in contrast to recurrent neural networks which perform much better in the applications of temporal domain [19–21].",
      "startOffset" : 205,
      "endOffset" : 212
    }, {
      "referenceID" : 14,
      "context" : "However, standard feed-forward and CNN neural networks are well suited for spatial domain [16–18], in contrast to recurrent neural networks which perform much better in the applications of temporal domain [19–21].",
      "startOffset" : 205,
      "endOffset" : 212
    }, {
      "referenceID" : 15,
      "context" : "However, standard feed-forward and CNN neural networks are well suited for spatial domain [16–18], in contrast to recurrent neural networks which perform much better in the applications of temporal domain [19–21].",
      "startOffset" : 205,
      "endOffset" : 212
    }, {
      "referenceID" : 16,
      "context" : "This effect is described as the vanishing gradient problem [22].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "There were many unsuccessful attempts to address the problem before LSTM was eventually introduced by Hochreiter and Schmidhuber [23] which ultimately solved it.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "Originally RNNs were meant to be used with single variable signals but they were also adapted for multiple stream inputs [22].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 18,
      "context" : "Therefore, most of the state-ofthe-art applications use LSTM model [24].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 19,
      "context" : "Modern LSTM architectures may also contain peephole connections [25].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "In terms of the backward pass, so-called constant error carousel enables the gradient to propagate back through many time steps, neither exploding or vanishing [23, 24].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 18,
      "context" : "In terms of the backward pass, so-called constant error carousel enables the gradient to propagate back through many time steps, neither exploding or vanishing [23, 24].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 20,
      "context" : "One day-long record of a single voltage time series for only one magnet occupies roughly 100 MB and there are several voltage time series associated with a single magnet [26] but ultimately we decided to use only one in our experiments, namely Ures.",
      "startOffset" : 170,
      "endOffset" : 174
    } ],
    "year" : 2016,
    "abstractText" : "This article presents a development of web application for quench prediction in Technology Department, Machine Protection and Electrical Integrity group, Electrical Engineering (TE-MPE-EE) at CERN. The authors describe an ELectrical Quality Assurance (ELQA) framework, a platform which was designed for rapid development of web integrated data analysis applications for different analysis needed during the hardware commissioning of the Large Hadron Collider (LHC). In second part the article describes a research carried out with the data collected from Quench Detection System by means of using an LSTM recurrent neural network. The article discusses and presents a conceptual work of implementing quench prediction application for TEMPE-EE based on the ELQA and quench prediction algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}