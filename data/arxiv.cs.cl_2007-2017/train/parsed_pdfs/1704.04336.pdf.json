{
  "name" : "1704.04336.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "AN ENTITY-DRIVEN RECURSIVE NEURAL NETWORK MODEL FOR CHINESE DISCOURSE COHERENCE MODELING",
    "authors" : [ "Fan Xu", "Shujing Du", "Maoxi Li", "Mingwen Wang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "DOI : 10.5121/ijaia.2017.8201 1\nChinese discourse coherence modeling remains a challenge taskin Natural Language Processing field.Existing approaches mostlyfocus on the need for feature engineering, whichadoptthe sophisticated features to capture the logic or syntactic or semantic relationships acrosssentences within a text.In this paper, we present an entity-drivenrecursive deep modelfor the Chinese discourse coherence evaluation based on current English discourse coherenceneural network model. Specifically, to overcome the shortage of identifying the entity(nouns) overlap across sentences in the currentmodel, Our combined modelsuccessfully investigatesthe entities information into the recursive neural network freamework.Evaluation results on both sentence ordering and machine translation coherence rating task show the effectiveness of the proposed model, which significantly outperforms the existing strong baseline.\nKEYWORDS\nEntity, Recursive Neural Network, Chinese Discourse, Coherence"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Discourse Coherence Modeling (DCM) aims to evaluate a degree of coherence among sentences within a discourse or text. It is considered one of the key problems in Natural Language Processing (NLP) due to its wide usage in many NLP applications, such as statistical machine translation[1], discourse generation[2][3][4], text automation summarization[3][5][6], student essay scoring [7][8][9] .\nIn general, a coherent discourse generally has many similar components (lexical overlap or coreference) across sentences within a text, while incoherent discourse is the other one. Therefore, the traditional cohesion theory of Centering[10] driven and entity-based model[11][12][13][14]was proposed to capture the syntactic or semantic distribution of discourse entities (nouns) between two adjacent sentences in a text. Thereafter, many extension works were presented such as Feng and Hirst [15] ’s multiple ranking model, Lin et al. [16] ’s discourse relationbased approach, Louis and Nenkova[17]’s syntactic patterns-based model. However, the potential issue of the existing traditional coherence models need feature engineering, which is a timeconsuming job.\nIn order to overcome the limitation of feature engineering issue, modern research tries to use neural network to extract the syntactic or semantic representation of a sentence automatically. Li et al.[18]proposed neural deep model to deal with English discourse coherence evaluation. However, their discourse coherence model only focuses on the distributed representation for\nsentences, and did not consider the entity (nouns) distribution across sentences. In fact, the entities can be overlapped between two adjacent sentences, and are good insight to capture the coherence between adjacent two sentences as mentioned in traditional entity-based method. Therefore, we successfully integrate this kind of information into current recursive neural network framework. Evaluation results on both sentence ordering and machine translation coherence rating task show the effectiveness of the proposed model, which significantly outperforms the existing strong baseline.\nTherefore, this paper tries to answer the following three questions:\n(1) Can the current English discourse coherence models (traditional or neural method) work for Chinese discourse coherence evaluation task? (2) Can the traditional entity based model be integrated into current deep model? (3) Which kind of word embedding works better for Chinese discourse coherence evaluation?\nThe rest of this paper is organized as follows. Section 2 reviews related work on discourse coherence modeling. Section 3 introduces the framework of our entity-driven recursive neural network based Chinese discourse coherence model. Section 4 describes the experiment results and detailed analysis. Finally, some conclusions are drawn in Section 5."
    }, {
      "heading" : "2. RELATED WORK",
      "text" : "In this section, we describe the related work for discourse coherence modeling from traditional and neural network modes, respectively."
    }, {
      "heading" : "2.1. TRADITIONAL COHERENCE MODEL",
      "text" : "The task of DCM was first introduced by Foltz et al.[19]. They formulated the discourse coherence as a function of semantic relatedness between two adjacent sentences within a text, and employed a vector-based representation of lexical meaning to compute the semantic relatedness. Since then, many supervised approaches to DCM, such as the entity-based model[11][12][13][15], discourse relation-based model [16] , syntactic patterns-based model [17] , co reference resolution-based model [20][21] , content-based model via Hidden Markov Model (HMM) [3][22] and cohesion-driven based model[23] have been proposed in literature.\nTo be more specific, Barzilay and Lapata [11][12] presented an entity-based model to capture the distribution of discourse entities between two adjacent sentences within a text. As an extensive work of entity-based approach, Lin et al.[16] explored the function of discourse relations to revise the entity and to catch the behavior of discourse relation transfer among sentences. In addition, Feng and Hirst[15] showed that multiple ranking instead of pair wise ranking was effective for the DCM.\nDifferently, Louis and Nenkova[17] explored the function of syntactic structure in the DCM. Besides, Iida et al.[20] and Elsner et al.[21] demonstrated the importance of the usage of co reference resolution. In addition, Barzilay et al. [3] and Elsner et al. [22] showed that an Hidden Markov Model (HMM)-based content model can be used to capture the topic’s transfer from the first sentence to the end sentence of a text, where topics were formulated as hidden states and sentences were treated as observations. Still, a potential issue of the HMM model is its domaindependent mechanism. Also, Xu et al.[23] explored the impact of Halliday[24]’s Theme Structure Theory (TST) in English discourse coherence modeling. Their model shows the importance of the theme structure, a cohesion theory of Halliday’s systemic-functional grammar, to DCM, and the appropriateness of theme and co reference based filtering mechanism."
    }, {
      "heading" : "2.2. NEURAL COHERENCE MODEL",
      "text" : "Recently, Li et al.[18]presented a neural deep model for English discourse coherence modeling. They demonstrated the effectiveness of both recurrent and recursive neural network (RNN) model for English situation.\nHowever, as mentioned in the Section 1, their model did not consider the entity (nouns) distribution or entity overlap across sentences within a text. In fact, the entity overlap between two adjacent sentences indicates logical or semantic coherence for para text. Therefore, we successfully integrate these information into their model."
    }, {
      "heading" : "3. ENTITY-DRIVEN RNN COHERENCE MODEL",
      "text" : "In this section, we describe our entity-driven RNN Chinese discourse coherence model."
    }, {
      "heading" : "3.1. FRAMEWORK",
      "text" : "Figure 1 shows the entity-driven recursive deep model for Chinese discourse coherence modeling. Our deep model is based on Li et al. [18] ’s English discourse coherence framework. On comparison, their model doesn’t intensify the effectiveness of entities across each sentences in a text. Therefore, we successfully integrate the entities into current recursive neural network model."
    }, {
      "heading" : "3.2. SENTENCE REPRESENTATION",
      "text" : "For the word-level representation, each word in a sentence can be represented by using a vector representation (or word embedding), and are able to capture the semantic meanings through toolkit, e.g. word2vec 1 or Glove 2 . More specifically, the word of a sentence can be represented using a specific vector embedding ew={ew 1 ,ew 2 ,…,ew K},where K denotes the dimension of the word embedding.\n1http://code.google.com/p/word2vec/ 2http://nlp.stanford.edu/projects/glove/\nFor the sentence-level representation, as shown in Figure 1, the vector representation for the whole sentence is computed as a representation for each parent node based on its immediate children recursively in a bottom-up fashion until reaching the root of the tree. Concretely, for a given parent p in the tree and its two children c1(associated with vector representation hc1) and c2(associated with vector representation hc2), standard recursive network calculates hpfor p as follows:\nhp=f(WRecursive.[hc1, hc2]+bRecursive) (1)\nwhere [hc1, hc2] refers to the concatenating vector for children vector hc1 and hc2;WRecursive is a k*2K matrix and bRecursive is the K*1 bias vector; f(.) is tanh function."
    }, {
      "heading" : "3.3. ENTITY-DRIVEN SENTENCE CONVOLUTION",
      "text" : "The framework treats a window of sentences as a clique C(sliding windows of L sentences)and associates each clique with a tag yc that takes the value 1 if coherent, and 0 otherwise. As shown in Figure 1, each clique C takes as input a (L*K)*1vector hc by concatenating the embedding of all its contained sentences. The hidden layer takes as input hc and performs the convolution using a non-linear tanh function. The concatenating output vector for hidden layers, defined as qc, can therefore be rewritten as:\nqc=f(Wsen*(hc*hentity)+bsen) (2)\nwhere Wsen is a H*(L*K) dimensional matrix and bsen is a H*1 dimensional bias vector; H refers to the number of neurons in the hidden layer."
    }, {
      "heading" : "3.3.1. ENTITY-DRIVEN MECHANISM",
      "text" : "Firstly, we conduct vector summation operation for each nouns’ word embedding to generate hentity formulated as:\nHentity=ewNN1 ⊕ ewNN2⊕……⊕ ewNNk (3)\nThen, we conduct element wise multiplication operation between hc and hentity.\nThe value of the output layer can be formulated as:\nP(yc=1)=sigmod(U T qc+b) (4)\nwhere U is an H*1 vector and b denotes the bias; yc with value 1 means the text is coherent, and 0 otherwise.\nTherefore, the total coherence score for a given document is the probability that all cliques within the document are coherent, which is given by:\nSd=∏ ∈ = dC cyp )1(\n(5)\nFinally, we can determine whether a text is coherent according to the value of their coherence score."
    }, {
      "heading" : "3.4. TRAINING AND OPTIMIZATION",
      "text" : "The cost function for the model is given by:\n2\n0\n1 ( )\n2C trainset\nQ J H\nM M θ θ ∈ ∈Θ Θ = +∑ ∑\n(6)\n0 log[ ( 1)] (1 ) log[1 ( 1)]c c c cH y p y y p y= − = − − − = (7)\nwhere Θ =[WRecursive,Wsen,Usen]; M denotes the number of training samples.\nWe adopt the widely applied optimization diagonal variant of AdaGrad (Duchi et al. [25] ) to optimize the loss function."
    }, {
      "heading" : "4. EXPERIMENTS",
      "text" : "In this section, we demonstrate the effectiveness of our discourse coherence model through both sentence ordering and machine translation coherence rating tasks. The former aims to discern an original text from a permuted ordering of its sentences, while the latter aims to discern a human or reference translation from automatically machine generated translation."
    }, {
      "heading" : "4.1. DATASET",
      "text" : "Sentence Ordering Dataset: We select documents for Chinese Treebank 6.0 from Linguistic Data Consortium (LDC) with catalog number LDC2007T36 and ISBN1-58563-450-6. We select the 100 documents from chtb_2946 to chtb_3045 as our training dataset, and 100 documents from chtb_3046 to chtb_3145 as our testing dataset. The sentences in each source file will be permutated at most 20 times. The total number of testing texts is 1027.The average number of sentence are 10.33 and 13.56 for training set and testing set, respectively. In the evaluation, we\nconsider the original texts are more coherent (positive instances) than the permutated ones (negative instances).\nMachine Translation Dataset: Similarly, we extract documents for NIST Open Machine Translation 2008 Evaluation (MT08) Selected Reference and System Translations from Linguistic Data Consortium (LDC) with catalog number LDC2010T01 and ISBN1-58563-533-2. Therein, the English-to-Chinese language pairs have 127 documents with 1830 segments, output from 11 machine translation systems. The average number of sentence are 13.38 and 13.39 for training set and testing set, respectively. In evaluation, we consider the human or reference translation texts are more coherent than the machine generated one."
    }, {
      "heading" : "4.2. EXPERIMENTAL SETTINGS",
      "text" : "Initialization: Similar to Li et al. [18] , the parameter Wsen, Wrecursive and h0are initialized by randomly drawing from the uniform distribution. The number of hidden layer H is set to 100.Learning rate in the optimization process is set to 0.01, and batch size is set to 20. Differently, word embedding {e} for Chinese are trained using word2vec and Glove respectively. The dimension for word embedding is 50 or 100. The window size L is 3 or 5.\nEvaluation Metric: We report system’s performance using accuracy, which is the ratio of the number of the selected original text/translation document divided by the total number of texts/translation document. Baseline System 1: Entity graph based model[14] which has been demenstrated as a simple but effective implementation of the entity-based coherence model. We re-implement their method in this paper based on publicly available code 3 .\nBaseline System 2: Another baseline, Li et al.[18]’s recursive neural model, which did not consider the entity transition information.We transplant there English discourse coherence framework to Chinese situation. Furthermore, we successfully integrate the entity information into their deep model.\nIn addition, we employ Stanford parser4to generate sentence-level constitute parser tree and generate the part-of-speech to get the entities (nouns) occur in each sentence, and use utility ICTCLAS 5 to conduct Chinese word segmentation."
    }, {
      "heading" : "4.3. EXPERIMENT RESULTS",
      "text" : "In this section, we report the experiment results for the Chinese discourse coherence modeling on both sentence ordering and machine translation coherence rating task."
    }, {
      "heading" : "4.3.1. RESULTS ON SENTENCE ORDERING",
      "text" : "Table 1 shows the performance of our entity-driven deep model using different windows size, different dimension, and with different type of word embedding.\nAs shown in Table 1, it shows that:\n(1)Dimension\nGenerally speaking, the performance increases with the increment of the dimension. In fact, the larger the dimension, the more representative ability it is.\n(2) Window size\nThe performance decreases with the increment of the window size, and the best performance yields at the window size with 3. It is mostly caused by the local entity distribution characteristic demenstrated by Barzilay and Lapata [11][12] ,Guinaudeau and Strube [14] . As the increment of the number of the window size, the entity co-occurance decreases accordingly.\n3http://github.com/karins/CoherenceFramework. 4http://nlp.stanford.edu/software/lex-parser.shtml. 5http://ictclas.nlpir.org/downloads\nTable 2, below, lists the performance comparision among our model and the current baseline model (traditional model and neural network model).\nIt shows that our combined model significantly outperforms the current deep model for Chinese discourse coherence modeling, which demonstrates the effectiveness and importance of the entity distribution across sentences. Interestingly, the traditional entity based model also works for Chinese discourse coherence evaluation, which doesn’t work fine for English situation. This is mostly caused by the entity distribution are obvious in Chinese discourse than in English text."
    }, {
      "heading" : "4.3.2. RESULTS ON MACHINE TRANSLATION COHERENCE RATING",
      "text" : "Table 3, below, lists the performance of our model and the baseline model.\nIn fact, discourse coherence evaluation for the machine translation task is more common than the sentence ordering task evaluation. As the results show in Table 3, again, our model significantly outperforms the current model. Also, our model significantly outperforms the traditional entitybased model. It is mostly caused by the entity distribution is not obvious in the text generated by the machine. But the entity (nouns) information still can be integrated into current recursive neural network model."
    }, {
      "heading" : "5. CONCLUSIONS AND FUTURE WORK",
      "text" : "In this paper, we present an entity-driven recursive deep model for Chinese discourse coherence modeling. We successfully integrate the entities across each sentence into current recursive neural framework. Evaluation results on both sentence ordering and machine translation coherence rating task show the effectiveness of the proposed model. Our future work is to integrate the co reference mechanism into current combined recursive neural network model, together with other coherence evaluation task."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "The authors would like to thank the anonymous reviewers for their comments on this paper. This research was supported bythe National Natural Science Foundation of China under Grant No.61402208, No.61462045, No.61462044, No.61662030, the Natural Science Foundation and Education Department of Jiangxi Province under Grant No. 20151BAB207027 and GJJ150351, and the Research Project of State Language Commission under Grant No.YB125-99."
    } ],
    "references" : [ {
      "title" : "Cohesion and Statistical Machine Translation",
      "author" : [ "J. Heidi" ],
      "venue" : "In Proceedings of EMNLP,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "Generation Using Utility-Trained Coherence Models",
      "author" : [ "Radu Soricut", "Daniel" ],
      "venue" : "In Proceedings of COLING-ACL,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Lee,(2004),“Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization",
      "author" : [ "Regina Barzilay", "Lillian" ],
      "venue" : "In Proceedings of NAACL-HLT,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Hierarchical Neural Autoencoder for Paragraphs and Documents",
      "author" : [ "Jiwei Li", "Minh-Thang Luong", "Dan" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation",
      "author" : [ "Zi-Heng Lin", "Hwee Tou Ng", "Min-Yen" ],
      "venue" : "In Proceedings of ACL,pages",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Bottom-Up Approach to Sentence Ordering for Multi-Document Summarization”,In",
      "author" : [ "Danushka Bollegala", "Naoaki Okazaki", "Mitsuru" ],
      "venue" : "Proceedings of ICCL-ACL,pages",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Briscoe,(2012),“Modeling coherence in ESOL learner texts",
      "author" : [ "Helen Yannakoudakis", "Ted" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Entity-Based Features to Model Coherence in Student Essays",
      "author" : [ "Jill Burstein", "Joel Tetreault", "Slava" ],
      "venue" : "In Proeedings of NAACL-HLT,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Multiple Aspects of Coherence in Student Essays",
      "author" : [ "Derrick Higgins", "Jill Burstin", "Daniel Marcu", "Claudia" ],
      "venue" : "In Proceedings of NAACL-HLT,pages185-192",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2004
    }, {
      "title" : "Framework for Modeling the Local Coherence of Discourse”,Computational",
      "author" : [ "Barbara J. Grosz", "Scott Weinstein", "Aravind K" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1995
    }, {
      "title" : "Local Coherence: An Entity-Based Approach",
      "author" : [ "Regina Barzilay", "Mirella" ],
      "venue" : "In Proceedings of ACL,pages",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "Local Coherence: An Entity-Based Approach”,Computational Linguistics, 34(1):1-34",
      "author" : [ "Regina Barzilay", "Mirella" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "Automatic Evaluation of Text Coherence: Models and Representations",
      "author" : [ "Mirella Lapata", "Regina" ],
      "venue" : "In Proceedings of IJCAI,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2005
    }, {
      "title" : "Graph-based local coherence modeling",
      "author" : [ "Camille Guinaudeau", "Michael Strube" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Hirst,(2012),“Extending the Entity-based Coherence Model with Multiple Ranks",
      "author" : [ "Vanessa Wei Feng", "Graeme" ],
      "venue" : "In Proceedings of EACL,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Evaluating Text Coherence Using Discourse Relations",
      "author" : [ "Zi-Heng Lin", "Hwee Tou Ng", "Min-Yen" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Nenkova,(2012),“A coherence model based on syntactic patterns",
      "author" : [ "Annie Louis", "Ani" ],
      "venue" : "InProceedings of EMNLP-CNLL,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Hovy,(2014),“A Model of Coherence Based on Distributed Sentence Representation",
      "author" : [ "Jiwei Li", "Eduard" ],
      "venue" : "In Proceedings of EMNLP,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2048
    }, {
      "title" : "Landauer,(1998),“The measurement of textual coherence with latent semantic analysis”,Discourse",
      "author" : [ "Peter W. Foltz", "Walter Kintsch", "Thomas K" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1998
    }, {
      "title" : "Metric for Evaluating Discourse Coherence based on Coreference Resolution",
      "author" : [ "Ryu Iida", "Takenobu" ],
      "venue" : "In Proceedings of COLING, Poster,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Unified Local and Global Model for Discourse Coherence",
      "author" : [ "Micha Elsner", "Joseph Austerweil", "Eugene" ],
      "venue" : "In Proceedings of NAACL,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "Discourse Coherence Modeling",
      "author" : [ "Fan Xu", "Qiaoming Zhu", "Guodong Zhou", "Mingwen" ],
      "venue" : "Journal of Chinese Information Processing,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "It is considered one of the key problems in Natural Language Processing (NLP) due to its wide usage in many NLP applications, such as statistical machine translation, discourse generation, text automation summarization, student essay scoring [7][8][9] .",
      "startOffset" : 242,
      "endOffset" : 245
    }, {
      "referenceID" : 7,
      "context" : "It is considered one of the key problems in Natural Language Processing (NLP) due to its wide usage in many NLP applications, such as statistical machine translation, discourse generation, text automation summarization, student essay scoring [7][8][9] .",
      "startOffset" : 245,
      "endOffset" : 248
    }, {
      "referenceID" : 8,
      "context" : "It is considered one of the key problems in Natural Language Processing (NLP) due to its wide usage in many NLP applications, such as statistical machine translation, discourse generation, text automation summarization, student essay scoring [7][8][9] .",
      "startOffset" : 248,
      "endOffset" : 251
    }, {
      "referenceID" : 14,
      "context" : "Thereafter, many extension works were presented such as Feng and Hirst [15] ’s multiple ranking model, Lin et al.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "[16] ’s discourse relationbased approach, Louis and Nenkova’s syntactic patterns-based model.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "Since then, many supervised approaches to DCM, such as the entity-based model, discourse relation-based model [16] , syntactic patterns-based model [17] , co reference resolution-based model [20][21] , content-based model via Hidden Markov Model (HMM) [3][22] and cohesion-driven based model have been proposed in literature.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 16,
      "context" : "Since then, many supervised approaches to DCM, such as the entity-based model, discourse relation-based model [16] , syntactic patterns-based model [17] , co reference resolution-based model [20][21] , content-based model via Hidden Markov Model (HMM) [3][22] and cohesion-driven based model have been proposed in literature.",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 19,
      "context" : "Since then, many supervised approaches to DCM, such as the entity-based model, discourse relation-based model [16] , syntactic patterns-based model [17] , co reference resolution-based model [20][21] , content-based model via Hidden Markov Model (HMM) [3][22] and cohesion-driven based model have been proposed in literature.",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 2,
      "context" : "Since then, many supervised approaches to DCM, such as the entity-based model, discourse relation-based model [16] , syntactic patterns-based model [17] , co reference resolution-based model [20][21] , content-based model via Hidden Markov Model (HMM) [3][22] and cohesion-driven based model have been proposed in literature.",
      "startOffset" : 252,
      "endOffset" : 255
    }, {
      "referenceID" : 20,
      "context" : "Since then, many supervised approaches to DCM, such as the entity-based model, discourse relation-based model [16] , syntactic patterns-based model [17] , co reference resolution-based model [20][21] , content-based model via Hidden Markov Model (HMM) [3][22] and cohesion-driven based model have been proposed in literature.",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 10,
      "context" : "To be more specific, Barzilay and Lapata [11][12] presented an entity-based model to capture the distribution of discourse entities between two adjacent sentences within a text.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "To be more specific, Barzilay and Lapata [11][12] presented an entity-based model to capture the distribution of discourse entities between two adjacent sentences within a text.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "[3] and Elsner et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 20,
      "context" : "[22] showed that an Hidden Markov Model (HMM)-based content model can be used to capture the topic’s transfer from the first sentence to the end sentence of a text, where topics were formulated as hidden states and sentences were treated as observations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] ’s English discourse coherence framework.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] , the parameter Wsen, Wrecursive and h0are initialized by randomly drawing from the uniform distribution.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "It is mostly caused by the local entity distribution characteristic demenstrated by Barzilay and Lapata [11][12] ,Guinaudeau and Strube [14] .",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : "It is mostly caused by the local entity distribution characteristic demenstrated by Barzilay and Lapata [11][12] ,Guinaudeau and Strube [14] .",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : "It is mostly caused by the local entity distribution characteristic demenstrated by Barzilay and Lapata [11][12] ,Guinaudeau and Strube [14] .",
      "startOffset" : 136,
      "endOffset" : 140
    } ],
    "year" : 2017,
    "abstractText" : "Chinese discourse coherence modeling remains a challenge taskin Natural Language Processing field.Existing approaches mostlyfocus on the need for feature engineering, whichadoptthe sophisticated features to capture the logic or syntactic or semantic relationships acrosssentences within a text.In this paper, we present an entity-drivenrecursive deep modelfor the Chinese discourse coherence evaluation based on current English discourse coherenceneural network model. Specifically, to overcome the shortage of identifying the entity(nouns) overlap across sentences in the currentmodel, Our combined modelsuccessfully investigatesthe entities information into the recursive neural network freamework.Evaluation results on both sentence ordering and machine translation coherence rating task show the effectiveness of the proposed model, which significantly outperforms the existing strong baseline.",
    "creator" : "PScript5.dll Version 5.2.2"
  }
}