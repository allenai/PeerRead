{
  "name" : "1609.07701.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "glass}@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 9.\n07 70\n1v 1\n[ cs\n.C L\n] 2\n5 Se"
    }, {
      "heading" : "1 Introduction",
      "text" : "Arabic and Hebrew are Semitic languages spoken by peoples with complicated cultural and political relationships. They share important similar characteristics in all linguistic levels, including orthography, morphology, syntax, and lexicon. Yet there is relatively little previous research on machine translation between the two languages, despite its potential benefit for promoting understanding between their speakers. The main reason for this lacuna is a lack of parallel Arabic-Hebrew texts. This has led researchers to consider alternative approaches, such as pivoting via English (El Kholy and Habash, 2014; El Kholy and Habash, 2015) or developing transferbased systems built with synchronous context free grammars (Shilon et al., 2012). Both approaches are unsatisfactory: the transfer-based system relies on manually-crafted grammars and lexicons, therefore\nsuffering from robustness issues, and pivoting via a morphologically-poor language like English leads to under-specification of potentially useful features.\nRecently, a number of large-scale parallel Arabic-Hebrew corpora have been compiled, mostly from multilingual transcriptions of spoken language available online (Cettolo et al., 2012; Lison and Tiedemann, 2016). These resources finally allow for training full-scale statistical machine translation systems on the Arabic-Hebrew pair. Our first contribution is in evaluating such standard systems on a clearly-defined dataset. We compare phrase-based machine translation (PBMT) with neural machine translation (NMT), using state-of-the-art implementations.\nLike other Semitic languages, Arabic and Hebrew feature rich morphology and frequent cliticization (joining of prepositions, conjunctions, etc. to the main word). These characteristics lead to increased ambiguity and pose a challenge to machine translation. A common solution is to apply tokenization by external tools, shown to help translation between Arabic/Hebrew and English (El Kholy and Habash, 2012; Singh and Habash, 2012). Our second contribution is thus in evaluating tokenization by external tools for the Arabic-Hebrew language pair. We also experiment with character-level neural models that have recently become popular for dealing with morphologically-rich languages (Kim et al., 2016).\nIn this work, we focus on Arabic-to-Hebrew translation. Arabic has relatively more available resources such as tokenizers and morphological analyzers, making this translation direction more ap-\nproachable. We leave the investigation of Hebrewto-Arabic translation for future work.\nOur results show that phrase-based and neural MT systems reach comparable performance, with a small advantage to neural models. We also ascertain the importance of sub-word modeling, where neural character models rival or surpass morphology-aware tokenization by standard tools. We conclude by pointing to potential directions for future research."
    }, {
      "heading" : "2 Related Work",
      "text" : "There is relatively little previous research on machine translation between Arabic and Hebrew, despite cultural and political relations between their speakers, and despite their linguistic similarities. The most relevant work is by Shilon et al. (2012), who built a statistical transfer-based system for translating from Arabic to Hebrew and vice versa. Their work relies on synchronous context free grammars and lexicons in the two languages, an approach that they advocate as being better suited to this pair for two main reasons: (a) a lack of available parallel corpora; and (b) the rich morphology of Arabic and Hebrew that requires linguistic knowledge. Here, we explore an alternative to this approach by exploiting Arabic-Hebrew parallel texts that have recently become available, enabling us to train standard statistical MT systems.1 We further explore methods for handling morphology both by using traditional tools for morphological analysis and tokenization, and by training a character-level neural MT system.\nOther work directly targeting machine translation between Arabic and Hebrew includes (El Kholy and Habash, 2014), which used pivoting via English. They improved translation quality by carefully designing the alignment symmetrization process in a phrase-based system. In later work, El Kholy and Habash (2015) incorporated morphological constraints for pivoting in a phrasebased system, which they augmented with parallel Arabic-Hebrew data (from an earlier version of the corpus we use in this paper). While pivoting is an appealing solution to scarcity in parallel corpora, Shilon et al. (2012) convincingly show how pivoting through a morphologically-poor language like\n1Cettolo (2016) describes the corpus and baseline MT systems in work concurrent with this paper.\nEnglish leads to under-specification of linguistic features and loss of information.\nThere is a fairly decent body of work on translation between Arabic and English, using a variety of methods; see the survey in (Alqudsi et al., 2014). In particular, the importance of morphologyaware tokenization when translating from and to Arabic has been confirmed in phrasebased (Badr et al., 2008; Habash and Sadat, 2006; El Kholy and Habash, 2012) and neural machine translation, in both hybrid (Devlin et al., 2014) and end-to-end systems (Almahairi et al., 2016). Work on Hebrew translation is more limited, but previous studies on translating Hebrew to English also demonstrated the need for morphological analysis and tokenization (Lavie et al., 2004; Lembersky et al., 2012; Singh and Habash, 2012)."
    }, {
      "heading" : "3 Linguistic Description",
      "text" : "We give here a short description of similarities and differences between Arabic and Hebrew, referring to (Shilon et al., 2012) for a comprehensive discussion.\nAs Semitic languages, Arabic and Hebrew share several characteristics. Both orthographies commonly omit vowels and other diacritics in writing, leading to increased ambiguity. The scripts are distinct, but there is substantial overlap in the alphabets. Many clitics (prepositions, conjunctions, definite articles) are prefixed or suffixed to words. Both languages have a rich morphology with a complex system of verbal inflection. Their inflection paradigms partially, but not completely, overlap. Syntactically, the languages have both verbal and verbless sentences. Arabic, in particular, has a more complicated agreement system. Some systematic word order patterns can be noted (SVO for Hebrew, VSO for Arabic), but these have exceptions and depend on genre.\nShilon et al. (2012) discuss the challenges such characteristics pose for machine translation between Arabic and Hebrew. In this work, we mostly address orthographic and morphological challenges, which call for solutions like tokenization and representing sub-word elements."
    }, {
      "heading" : "4 Parallel Corpora",
      "text" : "Until recently, there were not many available parallel corpora of Arabic and Hebrew.\nShilon et al. (2012) prepared a parallel corpus of several hundred sentences from the news domain, too small for training a statistical system but potentially useful for evaluation. Since then, two large resources have become available. First, WIT3 provides multilingual transcriptions of TED talks (Cettolo et al., 2012) and its 2016 release includes about 3 million words of Arabic-Hebrew parallel texts (Cettolo, 2016). As a corpus of TED talks, it has several interesting features: diversity of topics, spoken language transcriptions, and user-generated translations, although the review process ensures a reasonable translation quality. The original transcriptions are segmented at the caption level and WIT3 automatically joins them into sentences.\nSecond, OPUS provides a collection of translation texts from the web. The largest ArabicHebrew parallel corpus is OpenSubtitles, comprising automatically aligned movie and TV subtitles. The 2016 release contains more than 100 million words (Lison and Tiedemann, 2016). In addition, OPUS provides a version with alternative translations, with some 70 million words of Arabic-Hebrew texts (Tiedemann, 2016). Having alternative translations can be valuable for evaluation with multiple references, although many alternatives are simply duplicates. While this is by far the largest available Arabic-Hebrew parallel corpus, it suffers from the usual problems of OpenSubtitles texts: usergenerated content, questionable translation quality, and automatic caption alignment. In addition, the right-to-left scripts cause problems with punctuation marks such as misplacement and wrong tokenization.\nSmaller Arabic-Hebrew corpora in OPUS include localization files (Ubuntu, KDE, GNOME),\neach totaling between 200 thousand to 2 million words, as well as user-contributed translations from Tatoeba, and news stories from GlobalVoices (Tiedemann, 2009; Tiedemann, 2012). Table 1 summarizes statistics about available Arabic-Hebrew corpora."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "5.1 Machine Translation Systems",
      "text" : "Phrase-Based MT We build a standard PBMT system using Moses (Koehn et al., 2007). Word alignment is extracted by fast align (Dyer et al., 2013) and symmetrized with the grow-diag-final-and strategy, and lexical reordering follows the msd-bidirectional-fe configuration. Sentences longer than 80 words are filtered during training. We train a 5-gram language model on the training set target side using KenLM (Heafield et al., 2013) and tune with MERT to optimize BLEU. These are common Moses settings that have also been used in Arabic-English translation (Almahairi et al., 2016).\nNeural MT We train a neural translation system using a Torch (Collobert et al., 2011) implementation of attention sequence-to-sequence learning (Kim, 2016). We keep the default settings and experiment with two architectures: a small 2-layer 500 unit LSTM (on both encoder and decoder sides) and a larger 4-layer 1000 unit LSTM. Sentences are limited to 50 words and the vocabulary size is limited to 50,000 on both source and target sides. The model is trained on a single GPU using SGD. Decoding is done with beam search and a width of 5."
    }, {
      "heading" : "5.2 Tokenization and Sub-Word Models",
      "text" : "Morphological processing and tokenization are considered crucial for machine translation from and to Semitic languages like Arabic and Hebrew (Section 2). This is typically applied as a preprocessing step, requiring language-specific tools. An alternative option is to incorporate language-agnostic sub-word elements inside the training algorithm. We describe the two options next.\nTokenization We experiment with tokenization of the Arabic source side using two tools: MADAMIRA (Pasha et al., 2014), a standard\nmorphological analyzer and disambiguator, and the Farasa segmenter (Abdelali et al., 2016), a much faster ranker that has been shown to perform comparably to MADAMIRA. In both cases we segment the Arabic according to the ATB scheme that tends to perform better than other schemes in translating between Arabic and English (El Kholy and Habash, 2012; Sajjad et al., 2013). This scheme separates all clitics other than the definite article. While it is possible that other schemes will work better for Arabic-Hebrew translation, exploring this option is left for future work. The tokenized text is also normalized with the tools’ default settings. On the Hebrew side, we only separate punctuation marks.\nCharacter-level models Character-level models have been shown to benefit neural MT, especially for languages with large vocabularies. For instance, Sennrich et al. (2016) convert words to sub-word elements using byte-pair encoding and obtain significant gains on English-German/Russian translation. The method was also applied to Arabic-English translation (Abdelali et al., 2016). Here we experiment with a character-level convolutional neural network (charCNN) that replaces input word vectors with learned representations based on character vectors (Kim et al., 2016). We use the default settings in (Kim, 2016)."
    }, {
      "heading" : "5.3 Data and Evaluation",
      "text" : "We mainly experiment with the WIT3 corpus of TED talks (Section 4). It is a fairly large corpus (3 million words), with high-quality translations and diverse topics. We use the designated train.tags files for training, IWSLT16.TED.tst2010-2014 for tuning, and IWSLT16.TED.dev2010 for testing. We keep IWSLT16.TED.tst2015-2016 as a held-out set for future evaluations. Table 2 provides some statistics about the datasets.\nWe also performed initial separate experiments with the OpenSubtitles corpus. However, the trans-\nlation quality was very poor, mostly due to the extremely noisy nature of the dataset. Therefore we leave the exploration of this corpus for future work.\nWe compute BLEU scores using the multi-bleu.perl script included with Moses. Significance testing follows (Koehn, 2004; Riezler and Maxwell, 2005). We also report Meteor scores (version 1.5), using Meteor Universal (Denkowski and Lavie, 2014) to build language resources based on the phrase table learned by the PBMT system."
    }, {
      "heading" : "6 Results",
      "text" : "Table 3 summarizes the results for Arabic-toHebrew translation on the WIT3 corpus of TED talks. As expected, tokenization helps phrase-based MT, although the differences in BLEU scores are not statistically significant. In terms of BLEU, neural MT performs significantly better than phrasebased MT, and char-based models lead to substantial and statistically significant improvement. Another small improvement is gained by replacing generated unknown words with translations of their aligned source words based on the attention weights (Jean et al., 2015). Using a larger and deeper NMT model does not lead to significant improvement, possibly due to the size of the training data.\nWe note that the generally low BLEU scores can be attributed to the single-reference evaluation mode, as well as the challenging nature of the data (spoken language transcripts, automatically aligned\ncaptions, diverse topics). Similar BLEU scores were reported for translating from English into Arabic and Hebrew in previous evaluations of TED talks translation (Cettolo et al., 2014).\nLooking at Meteor scores, we again see that tokenization helps, but this time the basic NMT system is inferior to PBMT. However, as Meteor Universal uses the phrase-table learned by the PBMT system, it might be biased towards PBMT. Using a characterbased model and UNK replacement can close this gap, leading to the best performing system."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "We presented initial experiments in large-scale Arabic-to-Hebrew machine translation, comparing both phrase-based and neural MT. We also evaluated the contribution of tokenization to the PBMT system and of character-level models to the NMT system.\nThis work is a first step that can be extended in a number of ways. First, experimenting with the Hebrew-to-Arabic direction might reveal new insights. Second, other combinations of tokenization and character-level models can be explored (e.g. character-level neural models on tokenized or bytepair encoded text). The parallel corpora can also be cleaned and improved, especially by adding multiple reference translations. Finally, modeling interrelations between the two languages in a more direct manner is an appealing direction, given the similarities across linguistic levels."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Mauro Cettolo for useful suggestions with regards to the ArabicHebrew TED talks corpus, Pierre Lison and Jörg Tiedemann for help with getting access to the OpenSubtitles corpus, and Reshef Shilon for fruitful discussions. This work was supported by the Qatar Computing Research Institute (QCRI). Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations."
    } ],
    "references" : [ {
      "title" : "Nadir Durrani",
      "author" : [ "Ahmed Abdelali", "Kareem Darwish" ],
      "venue" : "and Hamdy Mubarak.",
      "citeRegEx" : "Abdelali et al.2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Nizar Habash",
      "author" : [ "Amjad Almahairi", "Kyunghyun Cho" ],
      "venue" : "and Aaron Courville.",
      "citeRegEx" : "Almahairi et al.2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Nazlia Omar",
      "author" : [ "Arwa Alqudsi" ],
      "venue" : "and Khalid Shaker.",
      "citeRegEx" : "Alqudsi et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Rabih Zbib",
      "author" : [ "Ibrahim Badr" ],
      "venue" : "and James Glass.",
      "citeRegEx" : "Badr et al.2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Christian Girardi",
      "author" : [ "Mauro Cettolo" ],
      "venue" : "and Marcello Federico.",
      "citeRegEx" : "Cettolo et al.2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "2014",
      "author" : [ "Mauro Cettolo", "Jan Niehues", "Sebastian Stüker", "Luisa Bentivogli", "Marcello Federico" ],
      "venue" : "Report on the 11th IWSLT evaluation campaign, IWSLT",
      "citeRegEx" : "Cettolo et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "An Arabic-Hebrew parallel corpus of TED talks",
      "author" : [ "Mauro Cettolo" ],
      "venue" : "In Proceedings of the AMTA 2016 Workshop on Semitic Machine Translation",
      "citeRegEx" : "Cettolo.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cettolo.",
      "year" : 2016
    }, {
      "title" : "Koray Kavukcuoglu",
      "author" : [ "Ronan Collobert" ],
      "venue" : "and Clément Farabet.",
      "citeRegEx" : "Collobert et al.2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Meteor Universal: Language Specific Translation Evaluation for Any Target Language",
      "author" : [ "Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie" ],
      "venue" : "In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation",
      "citeRegEx" : "Denkowski et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Denkowski et al\\.",
      "year" : 2014
    }, {
      "title" : "Richard Schwartz",
      "author" : [ "Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar" ],
      "venue" : "and John Makhoul.",
      "citeRegEx" : "Devlin et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and Noah A",
      "author" : [ "Chris Dyer", "Victor Chahuneau" ],
      "venue" : "Smith.",
      "citeRegEx" : "Dyer et al.2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Orthographic and morphological processing for English–Arabic statistical machine translation",
      "author" : [ "El Kholy", "Habash2012] Ahmed El Kholy", "Nizar Habash" ],
      "venue" : "Machine Translation,",
      "citeRegEx" : "Kholy et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kholy et al\\.",
      "year" : 2012
    }, {
      "title" : "Alignment Symmetrization Optimization Targeting Phrase Pivot Statistical Machine Translation",
      "author" : [ "El Kholy", "Habash2014] Ahmed El Kholy", "Nizae Habash" ],
      "venue" : "In Proceedings of The European Association for Machine Translation (EAMT14)",
      "citeRegEx" : "Kholy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kholy et al\\.",
      "year" : 2014
    }, {
      "title" : "Morphological Constraints for Phrase Pivot Statistical Machine Translation",
      "author" : [ "El Kholy", "Habash2015] Ahmed El Kholy", "Nizar Habash" ],
      "venue" : "Proceedings of MT Summit XV,",
      "citeRegEx" : "Kholy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kholy et al\\.",
      "year" : 2015
    }, {
      "title" : "Arabic Preprocessing Schemes for Statistical Machine Translation",
      "author" : [ "Habash", "Sadat2006] Nizar Habash", "Fatiha Sadat" ],
      "venue" : "In Proceedings of the Human Language Technology Conference of the NAACL,",
      "citeRegEx" : "Habash et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Habash et al\\.",
      "year" : 2006
    }, {
      "title" : "Clark",
      "author" : [ "Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H" ],
      "venue" : "and Philipp Koehn.",
      "citeRegEx" : "Heafield et al.2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Roland Memisevic",
      "author" : [ "Sébastien Jean", "Kyunghyun Cho" ],
      "venue" : "and Yoshua Bengio.",
      "citeRegEx" : "Jean et al.2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "David Sontag",
      "author" : [ "Yoon Kim", "Yacine Jernite" ],
      "venue" : "and Alexander Rush.",
      "citeRegEx" : "Kim et al.2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Alexandra Constantin",
      "author" : [ "Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondřej Bojar" ],
      "venue" : "and Evan Herbst.",
      "citeRegEx" : "Koehn et al.2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Statistical Significance Tests for Machine Translation Evaluation",
      "author" : [ "Philipp Koehn" ],
      "venue" : "In Proceedings of EMNLP",
      "citeRegEx" : "Koehn.,? \\Q2004\\E",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Erik Peterson",
      "author" : [ "Alon Lavie", "Shuly Wintner", "Yaniv Eytani" ],
      "venue" : "and Katharina Probst.",
      "citeRegEx" : "Lavie et al.2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Noam Ordan",
      "author" : [ "Gennadi Lembersky" ],
      "venue" : "and Shuly Wintner.",
      "citeRegEx" : "Lembersky et al.2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles",
      "author" : [ "Lison", "Tiedemann2016] Pierre Lison", "Jörg Tiedemann" ],
      "venue" : "In Proceedings of the Tenth International Conference on Language Resources and Evaluation",
      "citeRegEx" : "Lison et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lison et al\\.",
      "year" : 2016
    }, {
      "title" : "Owen Rambow",
      "author" : [ "Arfath Pasha", "Mohamed AlBadrashiny", "Mona Diab", "Ahmed El Kholy", "Ramy Eskander", "Nizar Habash", "Manoj Pooleery" ],
      "venue" : "and Ryan Roth.",
      "citeRegEx" : "Pasha et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation",
      "author" : [ "Riezler", "Maxwell2005] Stefan Riezler", "John T Maxwell" ],
      "venue" : null,
      "citeRegEx" : "Riezler et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Riezler et al\\.",
      "year" : 2005
    }, {
      "title" : "Fahad Al Obaidli",
      "author" : [ "Hassan Sajjad", "Francisco Guzmán", "Preslav Nakov", "Ahmed Abdelali", "Kenton Murray" ],
      "venue" : "and Stephan Vogel.",
      "citeRegEx" : "Sajjad et al.2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Barry Haddow",
      "author" : [ "Rico Sennrich" ],
      "venue" : "and Alexandra Birch.",
      "citeRegEx" : "Sennrich et al.2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Alon Lavie",
      "author" : [ "Reshef Shilon", "Nizar Habash" ],
      "venue" : "and Shuly Wintner.",
      "citeRegEx" : "Shilon et al.2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Hebrew Morphological Preprocessing for Statistical Machine Translation",
      "author" : [ "Singh", "Habash2012] Nimesh Singh", "Nizar Habash" ],
      "venue" : null,
      "citeRegEx" : "Singh et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2012
    }, {
      "title" : "News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces",
      "author" : [ "Jörg Tiedemann" ],
      "venue" : "In Recent Advances in Natural Language Processing,",
      "citeRegEx" : "Tiedemann.,? \\Q2009\\E",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2009
    }, {
      "title" : "Parallel Data, Tools and Interfaces in OPUS",
      "author" : [ "Jörg Tiedemann" ],
      "venue" : null,
      "citeRegEx" : "Tiedemann.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2012
    }, {
      "title" : "Finding Alternative Translations in a Large Corpus of Movie Subtitle",
      "author" : [ "Jörg Tiedemann" ],
      "venue" : null,
      "citeRegEx" : "Tiedemann.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2016
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Machine translation between Arabic and Hebrew has so far been limited by a lack of parallel corpora, despite the political and cultural importance of this language pair. Previous work relied on manually-crafted grammars or pivoting via English, both of which are unsatisfactory for building a scalable and accurate MT system. In this work, we compare standard phrase-based and neural systems on Arabic-Hebrew translation. We experiment with tokenization by external tools and subword modeling by character-level neural models, and show that both methods lead to improved translation performance, with a small advantage to the neural models.",
    "creator" : "LaTeX with hyperref package"
  }
}