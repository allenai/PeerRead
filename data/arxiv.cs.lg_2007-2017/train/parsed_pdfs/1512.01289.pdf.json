{
  "name" : "1512.01289.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Predicting psychological attributions from face photographs with a deep neural network",
    "authors" : [ "Edward Grant", "Stephan Sahm", "Mariam Zabihi", "Marcel van Gerven" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Facial attributions for intelligence, attractiveness, dominance and trustworthiness have been shown to exhibit a strong effect on social decision making, with far reaching consequences from choosing between presidential candidates to jury decisions in criminal legal cases [1, 2].\nDespite this, reliably predicting how a face will be perceived has proven to be difficult. Some of the best current methods require images hand-annotated with facial landmark features, which is time consuming [3].\nGiven the success of CNNs in image recognition tasks [4], the CNN model is a natural choice for visual attribution of psychological characteristics. In addition to\nar X\niv :1\n51 2.\n01 28\n9v 1\n[ cs\n.C V\n] 4\nD ec\nsuperior performance in vision tasks, CNNs are able to learn visual features from image data and do not require additional human input or hand-crafted features.\nPrevious work on modelling attribute perception was conducted by Khosla et al. who introduced a method for characterising face images using key facial points, histogram information, SIFT features and hand annotated landmark facial features [3]. Using these features it was possible to accurately predict attributions for many psychological and demographic attributes. In contrast, we use a CNN to learn features directly from RGB images without the need for facial landmark annotations, which are time consuming and can introduce bias. Using these learned features the CNN is able to predict attribution labels with high accuracy, surpassing human level performance in some cases. In addition we demonstrate a method to visualize general attribution features learned by the CNN.\nOne important distinction is between the perception of psychological attributes (attributions) and other tests for an attribute. Visual perception has important social consequences but is not always a good indicator of more robust measurements for an attribute. For example, Rezlecu et al. showed that perceived trustworthiness is not significantly correlated with measured trustworthiness [5]. In contrast Kleiser et al. showed that perceived intelligence is associated with measured intelligence in men but not women [6]. In this experiment we focus solely on perception of attributes.\nSeveral methods exist for visualizing the features learned by a neural network. These methods can broadly be divided into approaches that require a target image to be forward propagated through the network before the activity of a target feature detector can be projected back into image space [7, 8] and image free approaches that generate an image that maximises a class score [9, 10].\nThe first type of approach has the benefit of visualizing features from a real example image, the second approach can be more general because it does not rely on a single image example. We use the first approach but visualise the mean of many examples, thus retaining both the benefit of visualizing features from real images and the generality of an image free approach. This is only possible because the images we used contain faces that have roughly the same pose. If this was not the case the features could become obscured by each other.\nTo accomplish feature visualization we use the deconvnet proposed by Zeiler and Fergus [7]. In this approach a target image is forward propagated through the network, all activations except the targets are set to zero. The target activation is projected back into image space by passing the activation back through the network using deconvolution and a special kind of up-sampling. The projected image contains the image features most responsible for the target activation.\nThe resulting visualization represents all of the features important for an at-\ntribute. However, by manipulating the final layer network weights we are able to show that these features can be decomposed into their positive and negative components. Using this method we separately visualize both the positive and negative features for attributions."
    }, {
      "heading" : "2 Methods",
      "text" : ""
    }, {
      "heading" : "2.1 Dataset",
      "text" : "Our example set comprised the annotated subset of the 10K face database collected by Bainbridge et al [11]. This set consists of 2222 face photographs as RGB images annotated with psychological and demographic labels. These data were collected in the form of a rating from 15 participants for psychological features and 13 participants for demographic features."
    }, {
      "heading" : "2.2 Preprocessing",
      "text" : "Each image was labelled as belonging to one of two equally sized classes for each attribute based on its ratings for a target attribute. For gender the ratings were binary already and the classes were balanced by removing images randomly from the larger class. Images were square cropped and resized to 60 × 60 pixels. Images were zero centered by subtracting the mean pixel value from all images."
    }, {
      "heading" : "2.3 Training the Network",
      "text" : "For each attribution each of 11 CNN models were trained within one fold of an 11- fold cross validation setting. Each example was represented exactly once in the test set. The network comprised of three convolutional layers, two fully connected layers and a softmax layer.\nTraining was performed using stochastic gradient descent with a momentum of 0.9, a batch size of 60, a learning rate of 0.005 and weight decay of 0.001. The models were trained using MatConvNet [12] on a Tesla K80 GPU. See Figure 1 for a more detailed description of the network structure."
    }, {
      "heading" : "2.4 Performance Measures",
      "text" : "Two performance measures were computed. The outputs from the CNN are denoted by the probability of an image belonging to the positive or the negative class. These\nvalues were thresholded at 50% and afterwards compared to the true binarization of the image set (individually for each attribution). The fraction of correct predictions was used to determine accuracy. As a baseline a linear support vector machine (SVM) was trained on the same data and corresponding accuracy measurements were obtained. Furthermore, single human accuracy was computed using a leaveone-out strategy over the given (binarized) dataset.\nCorrelation values refer to standard correlation coefficients. They were computed between the CNN output probabilities and the continuous human assessment from the dataset. Again, single human correlation was obtained by a leave-one-out procedure over the (continuous) dataset for a respective attribution. While accuracy values show the correctness of the predictions, the correlations show how the variation within the predictions mimics the true variation in the data."
    }, {
      "heading" : "2.5 Statistics",
      "text" : "CNN performance was tested for significance in two respects: Whether it was significantly different from baseline randomness, and secondly, from human performance.\nThe CNN performance values revealed to be consistent enough over trials in order to reasonably approximate them as being constant. This means, we assume\nthat training the CNN twice will result in exactly the same prediction output – no randomness is involved. Accordingly, the statistical tests only estimate the baseline distributions, or distributions of human performance respectively. Having these distributions, the CNN performance is identical to the quantile of the respective pvalue. As we are interested in significant improvements, all tests are right-sided. In this study, for a significant improvement the p-value must be lower than the alpha level of 5%.\nThe accuracy measure as defined above counts the fraction of correct binary predictions. Hence, the random baseline is the mean of a respective number of independent coin-flips. As there are 2222 images, the baseline distribution is the mean of 2222 independent fair Bernoulli distributed random variables. The distribution of the correlations is less simple to derive. A completely random CNN would output an arbitrary probability prediction between 0 and 1, independently for each image. This corresponds to a vector of 2222 independent standard uniform random variables. The correlation measurement then demands computation of the correlation between the CNN response and each attribution dataset individually. Instead of deriving an analytic expression for such a correlation of a random vector with a given data vector of size 2222, it is easier to simulate these correlation values. For this study, 100000 random samples were generated from a uniform random vector and each correlated with all 22 attribution datasets.\nFor comparison with human performance, the distributions of human performances were approximated. For both accuracy and correlations a bootstrap estimation procedure was used. A short explanation of the bootstrap approach follows. Our dataset was generated by a small number of participants, only a subset of the true population over which we would like to know something in the end. Estimating the distribution of human performance over the whole population would mean taking a new (random) subset of participants and computing the same performance measure again, and again and again, thereby simulating its distribution. As this is obviously not feasible, the method known as bootstrap approximation regards the given subset as the population itself. Instead of taking a new random subset of the whole world population, a random subsubset of the given subset is chosen and the human performance measurement computed. For this study, sampling with replacement was used and again 100000 samples were generated."
    }, {
      "heading" : "2.6 Visualizing Features Using Deconvolution",
      "text" : "Attribution features were visualized using a deconvnet [7]. Using this method the target images was first forward propagated through the network and the location with\nmaximum activation for all pools were stored. The activation for the target attribute was passed back through the network using deconvolution. At each pooling layer an approximate inverse to the pooling operation was performed by up-sampling the image and placing the RGB pixel values at the location with maximum activation previously stored during pooling.\nThe target activation is caused by positive and negative predictions from the previous layer and so deconvolution visualizes an image with both positive and negative features. In order to separately visualize features that positively and negatively contribute to an attribution prediction, we set the negative weights in the final layer to zero to visualize the positive features and set the positive weights to zero to visualize the negative features. This is possible because the final layer nodes prior to the softmax operator output the weighted sum of the previous layer activations. By setting the positive or negative weights to zero in the final layer the effect of the positive or negative features on the prediction are nullified. Using this method we create three visualizations for each attribution. One contains all the features responsible for a prediction, one only negative features and one only positive features.\nThe deconvolution approach is typically used to visualize the features of a single example image. Because most images contain faces at roughly the same pose we visualized the mean of all feature representations for each attribute. This results in an image with the mean features for an attribution. This process was repeated for positive and negative features."
    }, {
      "heading" : "3 Results",
      "text" : ""
    }, {
      "heading" : "3.1 Performance of the CNN, Human and SVM Classifiers",
      "text" : "The CNN accuracy was higher than SVM accuracy in all cases. The averaged CNN accuracy over all attributions is 71.86% with standard deviation 0.080. For correlations the mean is 0.511 and the standard deviation 0.163.\nCNN accuracy as well as correlations between CNN predictions and human assessments are significantly better than chance for all attributions. Compared to human correlations, a significant improvement was found for 14 attributions: attractive, caring, common, confident, egotistic, emotional, emotional stability, friendly, happy, interesting, kind, responsible, sociable and trustworthy. Compared to human accuracies, the CNN is significantly better in predicting 5 attributions, namely emotional, friendly, happy, kind and sociable. See Table 1 for a summary."
    }, {
      "heading" : "3.2 Visualization of CNN Features Involved in Attribution",
      "text" : "Table 2 visualizes the CNN features involved in attribution. Here we identify a number of salient properties of the visualizations.\nAs expected the CNN positive features for a happy attribution look very much like a smile. In contrast the negative features focus around the eyes and a down-turned mouth. These two feature sets compete with each other to produce a prediction for happy.\nSome of the attribution features we visualize have been studied before and we find a general coherence between previous findings and the CNN feature visualizations.\nThe CNN features important for gender discrimination centre around the eyes and lips. This coheres with existing evidence that gender perception can be modulated by subtly changing the color of the lips and eyes in a gender neutral image [13, 14].\nTodorov et al. show that there is a significant correlation between human facial features and perceived trustworthiness. These features are mainly located at the center of the face including the eyebrows, cheekbones, nose and chin. The shape of these features generates a spectrum of trustworthiness impressions. A longer narrower nose indicated increased perceived trustworthiness In addition the shape of the cheekbones was found to be important [15]. The features for trustworthiness shown in Table 2 show that the nose and cheekbones are also important features for CNN attribution of trustworthiness.\nWe further observed striking differences in color hue and saturation between features for many attributes. Important features for age, gender and attractiveness are found mainly in the red and blue channels of the target image. The green channel contains important features for friendly and happy and other attributes have features with a combination of colors. Features for sociable are mainly found in the red channel in the nose area and in the green channel around the eyes and mouth. Although it is interesting to see the color of the features learned by the network it is advisable not to over-consider these findings. Just because the network learns features in a specific color channel does not necessarily mean that similar features cannot be found in other channels. Further work is need to determine the relevance of color in attribute perception, for example by comparing the predictive performance of the present model with that of a model trained only on gray-scale images."
    }, {
      "heading" : "4 Discussion",
      "text" : "This work shows that deep neural networks can be used to accurately predict rated personality traits from face images, even surpassing human level performance in some cases.\nThe coherence between CNN features for attribution and features found to be used by humans is interesting but not unexpected. CNNs and humans both exploit common structures in natural images to perform vision. In addition, CNNs are loosely inspired by real neural structures and similar to CNNs there is strong evidence that the human visual system is organized in a hierarchy of feature detectors of increasing complexity [16].\nBy visualizing positive and negative features separately it can be seen that for some attributes all features are important, whereas for others the positive and negative features are better indicators of the existence of an attribute. For example, the negative features for responsible are much more pronounced whereas for memorable the positive features are more important (see Table 2).\nConsidering the performance of the CNN compared to human performance, it is interesting that the correlation values are far more often significantly outperformed by CNN than accuracy values are. The CNN can replicate the variability within the assessments better than the overall classification. We used classification rather than regression to allow for binary feature representations using deconvolution but a similar network trained using regression may yield superior performance for attribution accuracy.\nMany of the positive and negative features for attributions discriminate between innate features such as face shape or distance between the eyes. Gender and age are good examples of such attributions. In contrast for some attributions positive and negative features appear to be discriminated by expressions. Good examples of these attributions are confident, friendly, happy, kind and emotional. Smiling is a positive feature for each of these attributions suggesting that perception of these attributions can be changed through facial expression unlike gender and age. Other attributes contain a mixture of fixed and expressive features. Attractiveness appears to be influenced by both face shape and the orientation of the lips. A smile is a feature of attractiveness, suggesting that perception of attractiveness can be modulated through expression.\nConcluding, we have shown that CNNs can be used to predict rated personality traits as well as analyze those visual features that contribute to the prediction of these traits. This can have both practical applications as well as provide new insights into the psychological underpinnings of personality ratings.\nTable 2: Deconvolved Features. Per attribution the dataset was binarized into low versus high values (class 0 and 1 respectively) and a deep neural network was trained for classification. Following training two additional network variants were created retaining only the positive(+) or negative(-) final layer weights. This allows for separate visualization of positive and negative features for each attribution. The images displayed are the averaged deconvolution results over the whole test dataset. For instance, for the attribution gender a pronounced mouth region is common for class 0 (female) while nose and eyebrows are important features for class 1 (male).\nAttribution Features Features(-) Features(+)\nage (young/old)\nattractive\ncalm\ncaring\ncommon\nconfident\negotistic\nemotional\nemotStable\nfamiliar\nfriendly\nAttribution Features Features(-) Features(+)\ngender (female/male)\nhappy\nintelligent\ninteresting\nkind\nmemorable\nresponsible\nsociable\ntrustworthy\ntypical\nweird"
    }, {
      "heading" : "5 Acknowledgments",
      "text" : "We would like to thank Wilma Bainbridge for the Human Faces 10k dataset and Umut Güçlü for providing the deconvnet code."
    } ],
    "references" : [ {
      "title" : "Predicting political elections from rapid and unreflective face judgments",
      "author" : [ "Charles C Ballew", "Alexander Todorov" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Looking like a leader–facial shape predicts perceived height and leadership ability",
      "author" : [ "Daniel E Re", "David W Hunter", "Vinet Coetzee", "Bernard P Tiddeman", "Dengke Xiao", "Lisa M DeBruine", "Benedict C Jones", "David I Perrett" ],
      "venue" : "PloS ONE,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Modifying the memorability of face photographs",
      "author" : [ "Aditya Khosla", "Wilma A. Bainbridge", "Antonio Torralba", "Aude Oliva" ],
      "venue" : "In International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Unfakeable facial configurations affect strategic choices in trust games with or without information about past behavior",
      "author" : [ "Constantin Rezlescu", "Brad Duchaine", "Christopher Y Olivola", "Nick Chater" ],
      "venue" : "PloS ONE,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Perceived intelligence is associated with measured intelligence in men but not women",
      "author" : [ "Karel Kleisner", "Veronika Chvátalová", "Jaroslav Flegr" ],
      "venue" : "PloS ONE,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "Matthew D Zeiler", "Rob Fergus" ],
      "venue" : "In Computer Vision–ECCV",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Understanding deep image representations by inverting them",
      "author" : [ "Aravindh Mahendran", "Andrea Vedaldi" ],
      "venue" : "International Journal of Computer Vision (IJCV),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "author" : [ "Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1312.6034,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "author" : [ "Anh Nguyen", "Jason Yosinski", "Jeff Clune" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "The intrinsic memorability of face photographs",
      "author" : [ "Wilma A Bainbridge", "Phillip Isola", "Aude Oliva" ],
      "venue" : "Journal of Experimental Psychology: General,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Matconvnet – convolutional neural networks for matlab",
      "author" : [ "Andrea Vedaldi", "Karel Lenc" ],
      "venue" : "In Proceeding of the International Conference on Multimedia (ACM),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "A sex difference in facial contrast and its exaggeration by cosmetics",
      "author" : [ "Richard Russell" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Lip colour affects perceived sex typicality and attractiveness of human",
      "author" : [ "Ian D Stephen", "Angela M McKeegan" ],
      "venue" : "faces. Perception,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Evaluating face trustworthiness: a model based approach",
      "author" : [ "Alexander Todorov", "Sean G Baron", "Nikolaas N Oosterhof" ],
      "venue" : "Social Cognitive and Affective Neuroscience,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream",
      "author" : [ "Umut Güçlü", "Marcel AJ van Gerven" ],
      "venue" : "The Journal of Neuroscience,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Facial attributions for intelligence, attractiveness, dominance and trustworthiness have been shown to exhibit a strong effect on social decision making, with far reaching consequences from choosing between presidential candidates to jury decisions in criminal legal cases [1, 2].",
      "startOffset" : 273,
      "endOffset" : 279
    }, {
      "referenceID" : 1,
      "context" : "Facial attributions for intelligence, attractiveness, dominance and trustworthiness have been shown to exhibit a strong effect on social decision making, with far reaching consequences from choosing between presidential candidates to jury decisions in criminal legal cases [1, 2].",
      "startOffset" : 273,
      "endOffset" : 279
    }, {
      "referenceID" : 2,
      "context" : "Some of the best current methods require images hand-annotated with facial landmark features, which is time consuming [3].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "Given the success of CNNs in image recognition tasks [4], the CNN model is a natural choice for visual attribution of psychological characteristics.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "who introduced a method for characterising face images using key facial points, histogram information, SIFT features and hand annotated landmark facial features [3].",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 4,
      "context" : "showed that perceived trustworthiness is not significantly correlated with measured trustworthiness [5].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "showed that perceived intelligence is associated with measured intelligence in men but not women [6].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "These methods can broadly be divided into approaches that require a target image to be forward propagated through the network before the activity of a target feature detector can be projected back into image space [7, 8] and image free approaches that generate an image that maximises a class score [9, 10].",
      "startOffset" : 214,
      "endOffset" : 220
    }, {
      "referenceID" : 7,
      "context" : "These methods can broadly be divided into approaches that require a target image to be forward propagated through the network before the activity of a target feature detector can be projected back into image space [7, 8] and image free approaches that generate an image that maximises a class score [9, 10].",
      "startOffset" : 214,
      "endOffset" : 220
    }, {
      "referenceID" : 8,
      "context" : "These methods can broadly be divided into approaches that require a target image to be forward propagated through the network before the activity of a target feature detector can be projected back into image space [7, 8] and image free approaches that generate an image that maximises a class score [9, 10].",
      "startOffset" : 299,
      "endOffset" : 306
    }, {
      "referenceID" : 9,
      "context" : "These methods can broadly be divided into approaches that require a target image to be forward propagated through the network before the activity of a target feature detector can be projected back into image space [7, 8] and image free approaches that generate an image that maximises a class score [9, 10].",
      "startOffset" : 299,
      "endOffset" : 306
    }, {
      "referenceID" : 6,
      "context" : "To accomplish feature visualization we use the deconvnet proposed by Zeiler and Fergus [7].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "Our example set comprised the annotated subset of the 10K face database collected by Bainbridge et al [11].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 11,
      "context" : "The models were trained using MatConvNet [12] on a Tesla K80 GPU.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "Attribution features were visualized using a deconvnet [7].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "This coheres with existing evidence that gender perception can be modulated by subtly changing the color of the lips and eyes in a gender neutral image [13, 14].",
      "startOffset" : 152,
      "endOffset" : 160
    }, {
      "referenceID" : 13,
      "context" : "This coheres with existing evidence that gender perception can be modulated by subtly changing the color of the lips and eyes in a gender neutral image [13, 14].",
      "startOffset" : 152,
      "endOffset" : 160
    }, {
      "referenceID" : 14,
      "context" : "A longer narrower nose indicated increased perceived trustworthiness In addition the shape of the cheekbones was found to be important [15].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 15,
      "context" : "In addition, CNNs are loosely inspired by real neural structures and similar to CNNs there is strong evidence that the human visual system is organized in a hierarchy of feature detectors of increasing complexity [16].",
      "startOffset" : 213,
      "endOffset" : 217
    } ],
    "year" : 2017,
    "abstractText" : "Judgements about personality based on facial appearance are strong effectors in social decision making and are known to impact on areas from presidential elections to jury decisions. Recent work has shown that it is possible to predict perception of memorability, trustworthiness, intelligence and other attributes in human face images. The most successful of these approaches requires face images expertly annotated with key facial landmarks. We demonstrate a Convolutional Neural Network (CNN) model that is able perform the same task without the need for landmark features thereby greatly increasing efficiency. The model has high accuracy, surpassing human level performance in some cases. Furthermore, we use a deconvolutional approach to visualize important features for perception of 22 attributes and show that these can be described as a composites of their positive and negative components by separately visualizing both.",
    "creator" : "LaTeX with hyperref package"
  }
}