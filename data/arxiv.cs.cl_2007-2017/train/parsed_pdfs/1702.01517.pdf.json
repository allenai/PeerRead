{
  "name" : "1702.01517.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Opinion Recommendation using Neural Memory Model",
    "authors" : [ "Zhongqing Wang", "Yue Zhang" ],
    "emails" : [ "zhang}@sutd.edu.sg" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1\nOpinion Recommendation using Neural Memory Model"
    }, {
      "heading" : "1 Introduction",
      "text" : "Offering a channel for customers to share opinions and give scores to products and services, review websites have become a highly influential information source that customers refer to for making purchase decisions. Popular examples include IMDB1 on the movie domain, Epinions2 on the product domain, and Yelp3 on the service domain. Figure 1 shows a screenshot of a restaurant review page on Yelp.com, which offers two main types of\n1http://www.imdb.com/ 2http://epinions.com/ 3https://www.yelp.com/\nFigure 1: A restaurant review on Yelp.com.\ninformation. First, an overall rating score is given under the restaurant name; second, detailed user reviews are listed below the rating.\nThough offering a useful overview and details about a product or service, such information has several limitations. First, the overall rating is general and not necessarily agreeable to the taste of individual customers. Being a simple reflection of all customer scores, it serves an average customer well, but can be rather inaccurate for individuals. For example, the authors themselves often find highly rated movies being tedious. Second, there can be hundreds of reviews for a product or service, which makes it infeasible for exhaustive reading. It would be useful to have a brief summary of all reviews, which ideally should be customized to the reader.\nWe investigate the feasibility of a model that addresses the limitations above. There are two sources of information that the model should collect to achieve its goal, namely information on the target product, and information about the user. The former can be obtained from reviews written by other customers about the target product, and the latter can be obtained from the reviews that the user has written for other products and services. Given the above two sources of information, the model should generate a customized score of the product that the user is likely to give after trying, as well as a customized review that the user would\nar X\niv :1\n70 2.\n01 51\n7v 1\n[ cs\n.C L\n] 6\nF eb\n2 01\n7\n2 have written for the target product. We refer to the task above using the term opinion recommendation, which is a new task, yet closely related to several existing lines of work in NLP. The first is sentiment analysis (Hu and Liu, 2004; Pang and Lee, 2008), which is to give a rating score based on a customer review. Our task is different in that we aim to predict user rating scores of new product, instead of predicting the opinion score of existing reviews. The second is opinion summarization (Nishikawa et al., 2010; Wang and Ling, 2016), which is to generate a summary based on reviews of a product. A major difference between our task and this task is that the summary must be customized to a certain user, and a rating score must additionally be given. The third is recommendation (Su and Khoshgoftaar, 2009; Yang et al., 2014), which is to give a ranking score for a certain product or service based on the purchase history of the user and other customers who have purchased the target product. Our task is different in the source of input, which is textual customer reviews and ratings rather than numerical purchase history. There are three types of inputs for our task, namely the reviews of the target product, the reviews of the user on other products, and other users reviews on other products, and two types of outputs, namely a customized rating score and a customized review. The ideal solution should consider the interaction between all given types of information, jointly predicting the two types of outputs. This poses significant challenges to statistical models, which require manually defined features to capture relevant patterns from training data. Deep learning is a relatively more feasible choice, offering viabilities of information fusion by fully connected hidden layers (Collobert et al., 2011; Henderson et al., 2013). We leverage this advantage in building our model. In particular, we use a recurrent neural network to model the semantic content of each review. A neural network is used to consolidate existing reviews for the target product, serving the role of a product model. In addition, a user model is built by consolidating the reviews of the given user into a single vector form. Third, to address potential sparsity of a user’s history reviews, neighbor users are identified by collaborative filtering (Ding et al., 2006), and a vector representation is learned by using a neural neighborhood model, which consolidates their history reviews. Finally, a deep memory network is utilized to find the association between the user and target product, jointly yielding the rating score and customised review. Experiments on a Yelp dataset show that the model outperforms several pipelined baselines using state-of-the-art techniques. In particular, review scores given by the opinion recordation system are closer to real user review scores compared to the review scores which Yelp assigns to target products. Our code is released at http: //github.com/anonymous."
    }, {
      "heading" : "2 Related Work",
      "text" : "Sentiment Analysis. Our task is related to document-level sentiment classification (Pang and Lee, 2008), which is to infer the sentiment polarity of a given document. Recently, various neural network models are used to capture the sentimental information automatically, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015), which have been shown to achieve competitive results across different benchmarks. Different from binary classification, review rating prediction aims to predict the numeric rating of a given review. Pang and Lee (2005) pioneered this task by regarding it as a classification/regression problem. Most subsequent work focuses on designing effective textural features of reviews (Qu et al., 2010; Li et al., 2011; Wan, 2013). Recently, Tang et al. (2015) proposed a neural network model to predict the rating score by using both lexical semantic and user model. Beyond textural features, user information is also investigated in the literature of sentiment analysis. For example, Gao et al. (2013) developed user-specific features to capture user leniency, and Li et al. (2014) incorporated textual topic and userword factors through topic modeling. For integrating user information into neural network models, Tang et al. (2015) predicted the rating score given a review by using both lexical semantic information and a user embedding model. Chen et al. (2016b) proposed a neural network to incorporate global user and product information for sentiment classification via an attention mechanism. Different from the above research on sentiment analysis, which focuses on predicting the opinion on existing reviews. Our task is to recommend the\n3 score that a user would give to a new product without knowing his review text. The difference originates from the object, previous research aims to predict opinions on reviewed products, while our task is to recommend opinion on new products, which the user has not reviewed. Opinion Summarization. Our work also overlaps with to the area of opinion summarization, which constructs natural language summaries for multiple product reviews (Hu and Liu, 2004). Most previous work extracts opinion words and aspect terms. Typical approaches include association mining of frequent candidate aspects (Hu and Liu, 2004; Qiu et al., 2011), sequence labeling based methods (Jakob and Gurevych, 2010; Yang and Cardie, 2013), as well as topic modeling techniques (Lin and He, 2009). Recently, word embeddings and recurrent neural networks are also used to extract aspect terms (Irsoy and Cardie, 2014; Liu et al., 2015). Aspect term extraction approaches lack critical information for a user to understand how an aspect receives a particular rating. To address this, Nishikawa et al. (2010) generated summaries by selecting and ordering sentences taken from multiple review texts according to affirmativeness and readability of the sentence order. Wang and Liu (2011) adopted both sentence-ranking and graphbased methods to extract summaries on an opinion conversation dataset. While all the methods above are extractive, Ganesan et al. (2010) presented a graph-based summarization framework to generate concise abstractive summaries of highly redundant opinions, and Wang and Ling (2016) used an attention-based neural network model to absorb information from multiple text units and generate summaries of movie reviews. Different from the above research on opinion summarization, we generate a customized review to a certain user, and a rating score must be additionally given. Recommendation. Recommendation systems suggest to a user new products and services that might be of their interest. There are two main approaches, which are content-based and collaborative-filtering (CF) based (Adomavicius and Tuzhilin, 2005; Yang et al., 2014), respectively. Most existing social recommendation systems are CF-based, and can be further grouped into model-based CF and neighborhood-based CF (Kantor et al., 2011; Su and Khoshgoftaar, 2009). Matrix Factorization (MF) is one of the most popular models for CF. In recent MF-based social recommendation works, user-user social trust information is integrated with user-item feedback history (e.g., ratings, clicks, purchases) to improve the accuracy of traditional recommendation systems, which only factorize user-item feedback data (Ding et al., 2006; Koren, 2008; He et al., 2016). There has been work integrating sentiment analysis and recommendation systems, which use recommendation strategies such as matrix factorization to improve the performance of sentiment analysis (Leung et al., 2006; Singh et al., 2011). These methods typically use ensemble learning (Singh et al., 2011) or probabilistic graph models (Wu and Ester, 2015). For example, Zhang et al. (2014) who proposed a factor graph model to recommend opinion rating scores by using explicit product features as hidden variables. Different from the above research on recommendation systems, which utilize numerical purchase history between users and products, we work with textual information. In addition, recommendation systems only predict a rating score, while our system generates also a customized review, which is more informative. Neural Network Models. Multi-task learning has been recognised as a strength of neural network models for natural language processing (Collobert et al., 2011; Henderson et al., 2013; Zhang and Weiss, 2016; Chen et al., 2016a), where hidden feature layers are shared between different tasks that have common basis. Our work can be regarded as an instance of such multi-tasks learning via shared parameters, which has been widely used in the research community recently. Dynamic memory network models are inspired by neural turing machines (Graves et al., 2014), and have been applied for NLP tasks such as question answering (Sukhbaatar et al., 2015; Kumar et al., 2016), language modeling (Tran et al., 2016) and machine translation (Wang et al., 2016). It is typically used to find abstract semantic representations of texts towards certain tasks, which are consistent with our main need, namely abstracting the representation of a product that is biased towards the taste of a certain user.\n4"
    }, {
      "heading" : "3 Model",
      "text" : "Formally, the input to our model is a tuple 〈RT , RU , RN 〉, where RT = {rT1 , rT2 , ..., rTnt} is the set of existing reviews of a target product, RU = {rU1 , rU2 , ..., rUnu} is the set of user’s history reviews, and RN = {rN1 , rN2 , ..., rNnn} is the set of the user’s neighborhood reviews. All the reviews are sorted with temporal order. The output is a pair 〈YS , YR〉, where YS is a real number between 0 and 5 representing the rating score of the target product, and YR is a customised review.\nFor capturing both general and personalized information, we first build a product model, a user model, and a neighborhood model, respectively, and then use a memory network model to integrate these three types of information, constructing a customized product model. Finally, we predict a customized rating score and a review collectively using neural stacking. The overall architecture of the model is shown in Figure 2."
    }, {
      "heading" : "3.1 Review Model",
      "text" : "A customer review is the foundation of our model, based on which we derive representations of both a user and a target product. In particular, a user profile can be achieved by modeling all the reviews of the user RU , and a target product profile can be obtained by using all existing reviews of the product RT . We use the average of word embeddings to model a review. Formally, given a review r = {x1, x2, ..., xm}, where m is the length of the review, each word xk is represented with a Kdimensional embedding ewk (Mikolov et al., 2013). We use the ∑ k(e w k )/m for the representation of the review edr ."
    }, {
      "heading" : "3.2 User Model",
      "text" : "A standard LSTM (Hochreiter and Schmidhuber, 1997) without coupled input and forget gates or\npeephole connections is used to learn the hidden states of the reviews. Denoting the recurrent function at step t as LSTM(xt, ht−1), we obtain a sequence of hidden state vectors {hU1 , hU2 , ..., hUnu} recurrently by feeding {ed(rU1), ed(rU2), ..., edrUnu} as inputs, where hUi = LSTM(e\nd(rUi), hUi−1). The initial state and all stand LSTM parameters are randomly initialized and tuned during training.\nNot all reviews contribute equally to the representation of a user. We introduce an attention mechanism (Bahdanau et al., 2014; Yang et al., 2016) to extract the reviews that are relatively more important, and aggregate the representation of reviews to form a vector. Taking the hidden state {hU1 , ...hU2 , ..., hUnu} of user model as input, the attention model outputs, a continuous vector vU ∈ Rd×1, which is computed as a weighted sum of each hidden state hUi , namely\nvU = nu∑ i αihUi (1)\nwhere nu is the hidden variable size, αi ∈ [0, 1] is the weight of hUi , and ∑ i αi = 1.\nFor each piece of hidden state hUi , the scoring function is calculated by\nui = tanh(WUhUi + bU ) (2)\nαi = exp(ui)∑ j exp(uj)\n(3)\nwhere WU and bU are model parameters. The attention vector vU is used to represent the User Model."
    }, {
      "heading" : "3.3 Finding Neighbor Users",
      "text" : "We use neighborhood reviews to improve the user model, since a user may not have sufficient reviews to construct a reliable model. Here a neighbor refers to a user that has similar tastes to the target user (Koren, 2008; Desrosiers and Karypis, 2011). The same as the user model, we construct the neighborhood model vN using the neighborhood reviews RN = {rN1 , rN2 , ..., rNnn} with an attention recurrent network.\nA key issue in building the neighborhood model is how to find neighbors of a certain user. In this study, we use matrix factorization (Koren, 2008) to detect neighbors, which is a standard approach for recommendation (Ding et al., 2006; Li et al., 2009;\n5 He et al., 2016). In particular, users’ rating scores of products are used to build a product-users matrix M ∈ Rnt×nu with nt products and nu users. We approximate it using three factors, which specify soft membership of products and users (Ding et al., 2006) by finding: min F,S,T ||M − FST T || s.t.S ≥ 0, F ≥ 0, T ≥ 0 (4) where F ∈ Rnt×K represents the posterior probability of K topic clusters for each product; S ∈ RK×K encodes the distribution of each topic k; and T ∈ RK×nu indicates the posterior probability of K topic clusters for each user. As a result of matrix factorization, we directly obtain the probability of each user on each topic from the person-topic matrix T . To infer T , the optimization problem in Eq.4 can be solved using the following updating rule: Tjk ← Tjk (MTFS)jk (TT TMTFS)jk (5) Obtaining the user-topic matrix T , we measure the implicit connection between two users using: sim(i, j) = k∑ k=1 TikTjk (6) where sim(i, j) measure the implicit connection degree between users i and j. If sim(i, j) is higher than a threshold η, we consider user j as the neighbor of user i."
    }, {
      "heading" : "3.4 Product Model",
      "text" : "Given the representations of existing reviews {e(rT1), e(rT2), ..., erTnt} of the product, we use a LSTM to model their temporal orders, obtaining a sequence of hidden state vectors hT = {hT1 , hT2 , ..., hTnt} by recurrently feeding {e(rT1), e(rT2), ..., erTnt} as inputs. The hidden state vectors hT are used to represent the product."
    }, {
      "heading" : "3.5 Customized Product Model",
      "text" : "We use the user representation vU and the neighbour representation vN to transform the target product representation hT = {hT1 , hT2 , ..., hTnt} into a customised product representation vC , which is tailored to the taste of the user. In particular, a dynamic memory network (Sukhbaatar et al., 2015; Xiong et al., 2016) is utilized to iteratively find increasingly abstract representations of ht, by injecting vU and vN information. The memory model consists of multiple dynamic computational layers (hops), each of which contains an attention layer and a linear layer. In the first computational layer (hop 1), we take the hidden variables hTi (0 ≤ i ≤ nt) of product model as input, adaptively selecting important evidences through one attention layer using vU and vN . The output of the attention layer gives a linear interpolation of hT , and the result is considered as input to the next layer (hop 2). In the same way, we stack multiple hops and run the steps multiple times, so that more abstract representations of the target product can be derived. The attention model outputs a continuous vector vC ∈ Rd×1, which is computed as a weighted sum of hTi (0 ≤ i ≤ nt), namely vC = nt∑ i βihTi (7) where nt is the hidden variable size, βi ∈ [0, 1] is the weight of hTi , and ∑ i βi = 1. For each piece of hidden state hTi , we use a feed forward neural network to compute its semantic relatedness with the abstract representation vC . The scoring function is calculated as follows at hop t: uti = tanh(WThTi +WCv t−1 C +WUvU +WNvN + b) (8) βti = exp(uti)∑ j exp(u t j) (9) The vector vC is used to represent the customized product model. At the first hop, we define V 0C =∑ n hTi/n. The product model hTi (0 ≤ i ≤ nt) represents salient information of existing reviews in their temporal order, they do not reflect the taste of a particular user. We use the customised product model to integrate user information and product information (as reflected by the product model), resulting in a single vector that represents a customised product. From this vector we are able to synthesis both a customised review and a customised rating score."
    }, {
      "heading" : "3.6 Customized Review Generation",
      "text" : "The goal of customized review generation is to generate a review YR from the customized product representation vC , composed by a sequence of\n6 words yR1 , ..., yRnr . We decompose the prediction of YR into a sequence of word-level predictions: logP (YR|vC) =∑ j P (yRj |yR1 , ..., yRj−1 , vC) (10) where each word yRj is predicted conditional on the previously generated yR1 , ..., yRj−1 and the input vC . The probability is estimated by using standard word softmax: P (yRj |yR1 , ..., yRj−1 , vC) = softmax(hRj ) (11) where hRj is the hidden state variable at timestamp j, which is modeled as LSTM(uj−1, hRj). Here a LSTM is used to generate a new state hRj from the representation of the previous state hRj−1 and uj−1. uj−1 is the concatenation of previously generated word yRj−1 and the input representation of customized model vC ."
    }, {
      "heading" : "3.7 Customized Opinion Rating Prediction",
      "text" : "We consider two factors for customised opinion rating, namely existing review scores and the customised product representation vC . A baseline rating system such as Yelp.com uses only the former information, typically by taking the average of existing review scores. Such a baseline gives an empirical square error of 1.28 (out of 5) in our experiments, when compared with a test set of individual user ratings, which reflects the variance in user tastes. In order to integrate user preferences into the rating, we instead take a weighted average of existing ratings cores, so that the scores of reviews that are closer to the user preference are given higher weights. As a second factor, we calculate a review score independently according to the customised representation vc of existing reviews, without considering review scores. The motivation is two fold. First, existing reviews can be relatively few, and hence using their scores alone might not be sufficient for a confident score. Second, existing ratings can be all different from a users personal rating, if the existing reviews do not come from the user’s neighbours. As a result, using the average or weighted average of existing reviews, the personalised user rating might not be reached. Formally, given the rating scores s1, s2, ..., sn of existing reviews, and the the customized product representation vC , we calculate: YS = n∑ i αi · si + µ tanh(WSvC + bS) (12) In the left term ∑n i αi·si, we use attention weights αi to measure the important of each rating score si. The right term tanh(WSvC + bS) is a reviewbased shift, weighted by µ. Since the result of customized review generation can be helpful for rating score prediction, we use neural stacking additionally feeding the last hidden state hRn of review generation model as input for YS prediction, resulting in YS = n∑ i αi · si+ + µ tanh(WS(vC ⊕ hRn) + bS) (13) where ⊕ denotes vector concatenation."
    }, {
      "heading" : "3.8 Training",
      "text" : "For our task, there are two joint training objectives, for review scoring and review summarisation, respectively. The loss function for the former is defined as: L(Θ) = N∑ i=1 (Y ∗Si − YSi) 2 + λ 2 ||Θ||2 (14) where Y ∗Si is the predicted rating score, YSi is the rating score in the training data, Θ is the set of model parameters and λ is a parameter for L2 regularization. We train the customized review generation model by maximizing the log probability of Eq.10 (Sutskever et al., 2014; Rush et al., 2015). Standard back propagation is performed to optimize parameters, where gradients also propagate from the scoring objective to the review generation objective due to neural stacking (Eq.13). We apply online training, where model parameters are optimized by using Adagrad (Duchi et al., 2011). For all LSTM models, we empirically set the size of the hidden layers to 128. We train word embeddings using the Skip-gram algorithm (Mikolov et al., 2013)4, using a window size of 5 and vector size of 128. In order to avoid over-fitting, dropout (Hinton et al., 2012) is used for word embedding with a ratio of 0.2. The neighbor similarity threshold η is set to 0.25. 4 https://code.google.com/p/word2vec/\n7\nAmount Business 15,584 Review 334,997 User 303,032\nTable 1: Statistics of the dataset."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : "Our data are collected from the yelp academic dataset5, provided by Yelp.com, a popular restaurant review website. The data set contains three types of objects: business, user, and review, where business objects contain basic information about local businesses (i.e. restaurants), review objects contain review texts and star rating, and user objects contain aggregate information about a single user across all of Yelp. Table 1 illustrates the general statistics of the dataset. For evaluating our model, we choose 4,755 user-product pairs from the dataset. For each pair, the existing reviews of the target service (restaurant) are used for the product model. The rating score given by each user to the target service is considered as the gold customized rating score, and the review of the target service given by each user is used as the gold-standard customized review for the user. The remaining reviews of each user are used for training the user model. We use 3,000 user-product pairs to train the model, 1,000 pairs as testing data, and remaining data for development. We use the ROUGE-1.5.5 (Lin, 2004) toolkit for evaluating the performance of customized review generation, and report unigram overlap (ROUGE-1) as a means of assessing informativeness. We use Mean Square Error (MSE) (Wan, 2013; Tang et al., 2015) is used as the evaluation metric for measuring the performance of customized rating score prediction. MSE penalizes more severe errors more heavily."
    }, {
      "heading" : "4.2 Development Experiments",
      "text" : ""
    }, {
      "heading" : "4.2.1 Ablation Test",
      "text" : "Effects of various configurations of our model, are shown on Table 2, where Joint is the full model of this paper, -user ablates the user model, -neighbor ablates the neighbor model, -rating is a single-task model that generates a review without the rating 5https://www.yelp.com/academic dataset\nRating Generation Joint 0.904 0.267 -user 1.254 0.220 -neighbor 1.162 0.245 -user,-neighbor 1.342 0.205 -rating - 0.254 -generation 1.042 -\nTable 2: Feature ablation tests.\nHOP Bais 0 1.342 0 1.102 1 1.102 1 0.904 2 1.046 2 1.067 3 0.904 3 1.136 4 0.987 4 1.206 5 1.102 5 1.227 6 1.045 7 1.126 8 1.172 9 1.152 10 1.167\n0.90\n0.95\n1.00\n1.05\n1.10\n1.15\n1.20\n1.25\n0 1 2 3 4 5\nM S\nE\nμ\n0.90\n0.95\n1.00\n1.05\n1.10\n1.15\n1.20\n1.25\n1.30\n1.35\n1.40\n0 1 2 3 4 5 6 7 8 9 10\nM S\nE\nhop\nFigure 3: Influence of hops.\nscore, and -generation generates only the rating score.\nBy comparing “Joint” and “-user,-neighbor”, we can find that customized information have significant influence on both the rating and review generation results (p − value < 0.01 using ttest). In addition, comparison between “-Joint” and “-user”, and between “-user” and “-user, - neighbor” shows that both the user information and the neighbour user information of the user are effective for improving the results. A users neighbours can indeed alleviate scarcity of user reviews.\nFinally, comparison between “Joint” and “- generation”, and between “Joint” and “-rating” shows that multi-task learning by parameter sharing is highly useful."
    }, {
      "heading" : "4.2.2 Influence of Hops",
      "text" : "We show the influence of hops of memory network for rating prediction on Figure 3. Note that, the model would only consider the general product reviews (−user,−neighbor), when hop = 0. From the figure we can find that, when hop = 3, the performance is the best. It indicates that multiple hops can capture more abstract evidences from external memory to improve the performance. However, too many hops leads to over-fitting, thereby harms the performance. As a result, we choose 3 as the number of hops in our final test.\n8\nHOP Bais 0 1.342 0 1.102 1 1.102 1 0.904 2 1.046 2 1.067 3 0.904 3 1.136 4 0.987 4 1.206 5 1.102 5 1.227 6 1.045 7 1.126 8 1.172 9 1.152 10 1.167 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 0 1 2 3 4 5 M S E μ\n0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 0 1 2 3 4 5 6 7 8 9 10 M S E hop\nFigure 4: Influence of bias score."
    }, {
      "heading" : "4.2.3 Influence of µ",
      "text" : "We show the influence of the bias weight parameter µ for rating prediction in Figure 4. With µ being 0, the model uses the weighted sum of existing reviews to score the product. When µ is very large, the system tends to use only the customized product representation vc to score the product, hence ignoring existing review scores, which are a useful source of information. Our results show that when µ is 1, the performance is optimal, thus indicating both existing review scores and review contents are equally useful."
    }, {
      "heading" : "4.3 Final Results",
      "text" : "We show the final results for opinion recommendation, comparing our proposed model with the following state-of-the-art baseline systems: • RS-Average is the widely-adopted baseline (e.g., by Yelp.com), using the averaged review scores as the final score. • RS-Linear estimates the rating score that a user would give by sui = sall+su+si (Ricci et al., 2011), where su and si are the the training deviations of the user u and the product i, respectively. • RS-Item applies kNN to estimate the rating score (Sarwar et al., 2001). We choose the cosine similarity between vc to measure the distance between product. • RS-MF is a state-of-the-art recommendation model, which uses matrix factorisation to predict rating score (Ding et al., 2006; Li et al., 2009; He et al., 2016). • Sum-Opinosis uses a graph-based framework to generate abstractive summarisation given redundant opinions (Ganesan et al., 2010).\nRating Generation RS-Average 1.280 - RS-Linear 1.234 - RS-Item 1.364 - RS-MF 1.143 - Sum-Opinosis - 0.183 Sum-LSTM-Att - 0.196 Joint 1.023 0.250\nAll the baseline models are single-task models, without considering rating and summarisation prediction jointly. The results are shown in Table 3. Our model (“ Joint”) significantly outperforms both “RS-Average” and “RS-Linear” (p − value < 0.01 using t-test), which demonstrates the strength of opinion recommendation, which leverages user characteristics for calculating a rating score for the user.\nOur proposed model also significantly outperforms state-of-the-art recommendation systems (RS-Item and RS-MF) (p− value < 0.01 using ttest), indicating that textual information are a useful addition to the rating scores themselves for recommending a product.\nFinally, comparison between our proposed model and state-of-the-art summarisation techniques (Sum-Opinosis and Sum-LSTM-Att) shows the advantage of leveraging user information to enhance customised review generation, and also the strength of joint learning."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We presented a dynamic memory model for opinion recommendation, a novel task of jointly predicting the review and rating score that a certain user would give to a certain product or service. In particular, a deep memory network was utilized to find the association between the user and the product, jointly yielding the rating score and customised review. Results show that our methods are better results compared to several pipelines baselines using state-of-the-art sentiment rating and summarisation systems.\n9"
    } ],
    "references" : [ {
      "title" : "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions",
      "author" : [ "Gediminas Adomavicius", "Alexander Tuzhilin." ],
      "venue" : "IEEE transactions on knowledge and data engineering 17(6):734–749.",
      "citeRegEx" : "Adomavicius and Tuzhilin.,? 2005",
      "shortCiteRegEx" : "Adomavicius and Tuzhilin.",
      "year" : 2005
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Neural network for heterogeneous annotations",
      "author" : [ "Hongshen Chen", "Yue Zhang", "Qun Liu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas,",
      "citeRegEx" : "Chen et al\\.,? 2016a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural sentiment classification with user and product attention",
      "author" : [ "Huimin Chen", "Maosong Sun", "Cunchao Tu", "Yankai Lin", "Zhiyuan Liu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Chen et al\\.,? 2016b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa." ],
      "venue" : "Journal of Machine Learning Research 12:2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "A comprehensive survey of neighborhood-based recommendation methods",
      "author" : [ "Christian Desrosiers", "George Karypis." ],
      "venue" : "Recommender Systems Handbook, pages 107–144.",
      "citeRegEx" : "Desrosiers and Karypis.,? 2011",
      "shortCiteRegEx" : "Desrosiers and Karypis.",
      "year" : 2011
    }, {
      "title" : "Orthogonal nonnegative matrix t-factorizations for clustering",
      "author" : [ "Chris Ding", "Tao Li", "Wei Peng", "Haesun Park." ],
      "venue" : "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, pages 126–135.",
      "citeRegEx" : "Ding et al\\.,? 2006",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2006
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John C. Duchi", "Elad Hazan", "Yoram Singer." ],
      "venue" : "Journal of Machine Learning Research 12:2121–2159. http://dl.acm.org/citation.cfm?id=2021068.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions",
      "author" : [ "Kavita Ganesan", "ChengXiang Zhai", "Jiawei Han." ],
      "venue" : "Proceedings of the 23rd international conference on computational linguistics. Association for Compu-",
      "citeRegEx" : "Ganesan et al\\.,? 2010",
      "shortCiteRegEx" : "Ganesan et al\\.",
      "year" : 2010
    }, {
      "title" : "Modeling user leniency and product popularity for sentiment classification",
      "author" : [ "Wenliang Gao", "Naoki Yoshinaga", "Nobuhiro Kaji", "Masaru Kitsuregawa." ],
      "venue" : "IJCNLP. pages 1107–1111.",
      "citeRegEx" : "Gao et al\\.,? 2013",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2013
    }, {
      "title" : "Neural turing machines",
      "author" : [ "Alex Graves", "Greg Wayne", "Ivo Danihelka." ],
      "venue" : "CoRR abs/1410.5401. http://arxiv.org/abs/1410.5401.",
      "citeRegEx" : "Graves et al\\.,? 2014",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast matrix factorization for online recommendation with implicit feedback",
      "author" : [ "Xiangnan He", "Hanwang Zhang", "Min-Yen Kan", "Tat-Seng Chua." ],
      "venue" : "Proceedings of the 39th International ACM SIGIR conference on Research and De-",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Multilingual joint parsing of syntactic and semantic dependencies with a latent variable model",
      "author" : [ "James Henderson", "Paola Merlo", "Ivan Titov", "Gabriele Musillo." ],
      "venue" : "Computational Linguistics 39(4):949–998.",
      "citeRegEx" : "Henderson et al\\.,? 2013",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "CoRR abs/1207.0580. http://arxiv.org/abs/1207.0580.",
      "citeRegEx" : "Hinton et al\\.,? 2012",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation 9(8):1735–1780. https://doi.org/10.1162/neco.1997.9.8.1735.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Seattle, Washington, USA, August 22-25, 2004. pages 168–177.",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "Opinion mining with deep recurrent neural networks",
      "author" : [ "Ozan Irsoy", "Claire Cardie." ],
      "venue" : "EMNLP. pages 720–728.",
      "citeRegEx" : "Irsoy and Cardie.,? 2014",
      "shortCiteRegEx" : "Irsoy and Cardie.",
      "year" : 2014
    }, {
      "title" : "Extracting opinion targets in a single and cross-domain setting with conditional random fields",
      "author" : [ "Niklas Jakob", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP",
      "citeRegEx" : "Jakob and Gurevych.,? 2010",
      "shortCiteRegEx" : "Jakob and Gurevych.",
      "year" : 2010
    }, {
      "title" : "Recommender systems handbook",
      "author" : [ "Paul B Kantor", "Lior Rokach", "Francesco Ricci", "Bracha Shapira" ],
      "venue" : null,
      "citeRegEx" : "Kantor et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kantor et al\\.",
      "year" : 2011
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Spe-",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Factorization meets the neighborhood: a multifaceted collaborative filtering model",
      "author" : [ "Yehuda Koren." ],
      "venue" : "Proceedings of the 14th ACM SIGKDD",
      "citeRegEx" : "Koren.,? 2008",
      "shortCiteRegEx" : "Koren.",
      "year" : 2008
    }, {
      "title" : "Ask me anything: Dynamic memory networks for natural language processing",
      "author" : [ "Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher." ],
      "venue" : "In",
      "citeRegEx" : "Kumar et al\\.,? 2016",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2016
    }, {
      "title" : "Integrating collaborative filtering and sentiment analysis: A rating inference approach",
      "author" : [ "Cane WK Leung", "Stephen CF Chan", "Fu-lai Chung." ],
      "venue" : "Proceedings of the ECAI 2006 workshop on recommender systems. pages 62–66.",
      "citeRegEx" : "Leung et al\\.,? 2006",
      "shortCiteRegEx" : "Leung et al\\.",
      "year" : 2006
    }, {
      "title" : "Incorporating reviewer and product information for review rating prediction",
      "author" : [ "Fangtao Li", "Nathan Nan Liu", "Hongwei Jin", "Kai Zhao", "Qiang Yang", "Xiaoyan Zhu." ],
      "venue" : "IJCAI 2011, Proceedings of the 22nd International Joint Confer-",
      "citeRegEx" : "Li et al\\.,? 2011",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2011
    }, {
      "title" : "Suit: A supervised user-item based topic model for sentiment analysis",
      "author" : [ "Fangtao Li", "Sheng Wang", "Shenghua Liu", "Ming Zhang." ],
      "venue" : "AAAI. pages 1636–1642.",
      "citeRegEx" : "Li et al\\.,? 2014",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "A nonnegative matrix tri-factorization approach to sentiment classification with lexical prior knowledge",
      "author" : [ "Tao Li", "Yi Zhang", "Vikas Sindhwani." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International",
      "citeRegEx" : "Li et al\\.,? 2009",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2009
    }, {
      "title" : "Joint sentiment/topic model for sentiment analysis",
      "author" : [ "Chenghua Lin", "Yulan He." ],
      "venue" : "Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 2009, Hong Kong,",
      "citeRegEx" : "Lin and He.,? 2009",
      "shortCiteRegEx" : "Lin and He.",
      "year" : 2009
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out: Proceedings of the ACL-04 workshop. Barcelona, Spain, volume 8.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Fine-grained opinion mining with recurrent neural networks and word embeddings",
      "author" : [ "Pengfei Liu", "Shafiq R. Joty", "Helen M. Meng." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon,",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26: 27th Annual Conference",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Optimizing informativeness and readability for sentiment summarization",
      "author" : [ "Hitoshi Nishikawa", "Takaaki Hasegawa", "Yoshihiro Matsuo", "Gen-ichiro Kikui." ],
      "venue" : "ACL 2010, Proceedings of the 48th Annual Meeting of the Association",
      "citeRegEx" : "Nishikawa et al\\.,? 2010",
      "shortCiteRegEx" : "Nishikawa et al\\.",
      "year" : 2010
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 25-30",
      "citeRegEx" : "Pang and Lee.,? 2005",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2005
    }, {
      "title" : "Opinion mining and sentiment analysis",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Foundations and trends in information retrieval 2(1-2):1–135.",
      "citeRegEx" : "Pang and Lee.,? 2008",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2008
    }, {
      "title" : "Opinion word expansion and target extraction through double propagation",
      "author" : [ "Guang Qiu", "Bing Liu", "Jiajun Bu", "Chun Chen." ],
      "venue" : "Computational Linguistics 37(1):9–27.",
      "citeRegEx" : "Qiu et al\\.,? 2011",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2011
    }, {
      "title" : "The bag-of-opinions method for review rating prediction from sparse text patterns",
      "author" : [ "Lizhen Qu", "Georgiana Ifrim", "Gerhard Weikum." ],
      "venue" : "COLING 2010, 23rd International Conference on Computational Linguistics, Proceedings of the Conference,",
      "citeRegEx" : "Qu et al\\.,? 2010",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2010
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon,",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Item-based collaborative filtering recommendation algorithms",
      "author" : [ "Badrul M. Sarwar", "George Karypis", "Joseph A. Konstan", "John Riedl." ],
      "venue" : "Proceedings of the Tenth International",
      "citeRegEx" : "Sarwar et al\\.,? 2001",
      "shortCiteRegEx" : "Sarwar et al\\.",
      "year" : 2001
    }, {
      "title" : "Context-sensitive lexicon features for neural sentiment analysis",
      "author" : [ "Zhiyang Teng", "Duy-Tin Vo", "Yue Zhang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas,",
      "citeRegEx" : "Teng et al\\.,? 2016",
      "shortCiteRegEx" : "Teng et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent memory networks for language modeling",
      "author" : [ "Ke M. Tran", "Arianna Bisazza", "Christof Monz." ],
      "venue" : "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Tran et al\\.,? 2016",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2016
    }, {
      "title" : "Co-regression for cross-language review rating prediction",
      "author" : [ "Xiaojun Wan." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia, Bulgaria, Volume 2: Short Papers. pages",
      "citeRegEx" : "Wan.,? 2013",
      "shortCiteRegEx" : "Wan.",
      "year" : 2013
    }, {
      "title" : "A pilot study of opinion summarization in conversations",
      "author" : [ "Dong Wang", "Yang Liu." ],
      "venue" : "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24",
      "citeRegEx" : "Wang and Liu.,? 2011",
      "shortCiteRegEx" : "Wang and Liu.",
      "year" : 2011
    }, {
      "title" : "Neural networkbased abstract generation for opinions and arguments",
      "author" : [ "Lu Wang", "Wang Ling." ],
      "venue" : "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Wang and Ling.,? 2016",
      "shortCiteRegEx" : "Wang and Ling.",
      "year" : 2016
    }, {
      "title" : "Memory-enhanced decoder for neural machine translation",
      "author" : [ "Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Qun Liu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin,",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "FLAME: A probabilistic model combining aspect based opinion mining and collaborative filtering",
      "author" : [ "Yao Wu", "Martin Ester." ],
      "venue" : "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM 2015, Shang-",
      "citeRegEx" : "Wu and Ester.,? 2015",
      "shortCiteRegEx" : "Wu and Ester.",
      "year" : 2015
    }, {
      "title" : "Dynamic memory networks for visual and textual question answering",
      "author" : [ "Caiming Xiong", "Stephen Merity", "Richard Socher." ],
      "venue" : "Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York",
      "citeRegEx" : "Xiong et al\\.,? 2016",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2016
    }, {
      "title" : "Joint inference for fine-grained opinion extraction",
      "author" : [ "Bishan Yang", "Claire Cardie." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia, Bulgaria,",
      "citeRegEx" : "Yang and Cardie.,? 2013",
      "shortCiteRegEx" : "Yang and Cardie.",
      "year" : 2013
    }, {
      "title" : "A survey of collaborative filtering based social recommender systems",
      "author" : [ "Xiwang Yang", "Yang Guo", "Yong Liu", "Harald Steck." ],
      "venue" : "Computer Communications 41:1–10.",
      "citeRegEx" : "Yang et al\\.,? 2014",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical attention networks for document classification",
      "author" : [ "Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy." ],
      "venue" : "NAACL 2016, 15th Annual Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Explicit factor models for explainable recommendation based on phrase-level sentiment analysis",
      "author" : [ "Yongfeng Zhang", "Guokun Lai", "Min Zhang", "Yi Zhang", "Yiqun Liu", "Shaoping Ma." ],
      "venue" : "The 37th International ACM SIGIR Confer-",
      "citeRegEx" : "Zhang et al\\.,? 2014",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    }, {
      "title" : "Stackpropagation: Improved representation learning for syntax",
      "author" : [ "Yuan Zhang", "David Weiss." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,",
      "citeRegEx" : "Zhang and Weiss.,? 2016",
      "shortCiteRegEx" : "Zhang and Weiss.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "The first is sentiment analysis (Hu and Liu, 2004; Pang and Lee, 2008), which is to give a rating score based on a customer review.",
      "startOffset" : 32,
      "endOffset" : 70
    }, {
      "referenceID" : 32,
      "context" : "The first is sentiment analysis (Hu and Liu, 2004; Pang and Lee, 2008), which is to give a rating score based on a customer review.",
      "startOffset" : 32,
      "endOffset" : 70
    }, {
      "referenceID" : 30,
      "context" : "The second is opinion summarization (Nishikawa et al., 2010; Wang and Ling, 2016), which is to generate a summary based on reviews of a product.",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 41,
      "context" : "The second is opinion summarization (Nishikawa et al., 2010; Wang and Ling, 2016), which is to generate a summary based on reviews of a product.",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 46,
      "context" : "The third is recommendation (Su and Khoshgoftaar, 2009; Yang et al., 2014), which is to give a ranking score for a certain product or service based on the purchase history of the user and other customers who have purchased the target product.",
      "startOffset" : 28,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "Deep learning is a relatively more feasible choice, offering viabilities of information fusion by fully connected hidden layers (Collobert et al., 2011; Henderson et al., 2013).",
      "startOffset" : 128,
      "endOffset" : 176
    }, {
      "referenceID" : 12,
      "context" : "Deep learning is a relatively more feasible choice, offering viabilities of information fusion by fully connected hidden layers (Collobert et al., 2011; Henderson et al., 2013).",
      "startOffset" : 128,
      "endOffset" : 176
    }, {
      "referenceID" : 6,
      "context" : "Third, to address potential sparsity of a user’s history reviews, neighbor users are identified by collaborative filtering (Ding et al., 2006), and a vector representation is learned by using a neural neighborhood model, which consolidates their history reviews.",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 32,
      "context" : "Our task is related to document-level sentiment classification (Pang and Lee, 2008), which is to infer the sentiment polarity of a given document.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 19,
      "context" : "Recently, various neural network models are used to capture the sentimental information automatically, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al.",
      "startOffset" : 143,
      "endOffset" : 154
    }, {
      "referenceID" : 37,
      "context" : ", 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015), which have been shown to achieve competitive results across different benchmarks.",
      "startOffset" : 37,
      "endOffset" : 74
    }, {
      "referenceID" : 34,
      "context" : "Most subsequent work focuses on designing effective textural features of reviews (Qu et al., 2010; Li et al., 2011; Wan, 2013).",
      "startOffset" : 81,
      "endOffset" : 126
    }, {
      "referenceID" : 23,
      "context" : "Most subsequent work focuses on designing effective textural features of reviews (Qu et al., 2010; Li et al., 2011; Wan, 2013).",
      "startOffset" : 81,
      "endOffset" : 126
    }, {
      "referenceID" : 39,
      "context" : "Most subsequent work focuses on designing effective textural features of reviews (Qu et al., 2010; Li et al., 2011; Wan, 2013).",
      "startOffset" : 81,
      "endOffset" : 126
    }, {
      "referenceID" : 19,
      "context" : "Recently, various neural network models are used to capture the sentimental information automatically, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015), which have been shown to achieve competitive results across different benchmarks. Different from binary classification, review rating prediction aims to predict the numeric rating of a given review. Pang and Lee (2005) pioneered this task by regarding it as a classification/regression problem.",
      "startOffset" : 144,
      "endOffset" : 490
    }, {
      "referenceID" : 19,
      "context" : "Recently, various neural network models are used to capture the sentimental information automatically, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015), which have been shown to achieve competitive results across different benchmarks. Different from binary classification, review rating prediction aims to predict the numeric rating of a given review. Pang and Lee (2005) pioneered this task by regarding it as a classification/regression problem. Most subsequent work focuses on designing effective textural features of reviews (Qu et al., 2010; Li et al., 2011; Wan, 2013). Recently, Tang et al. (2015) proposed a neural network model to predict the rating score by using both lexical semantic and user model.",
      "startOffset" : 144,
      "endOffset" : 723
    }, {
      "referenceID" : 7,
      "context" : "For example, Gao et al. (2013) developed user-specific features to capture user leniency, and Li et al.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 7,
      "context" : "For example, Gao et al. (2013) developed user-specific features to capture user leniency, and Li et al. (2014) incorporated textual topic and userword factors through topic modeling.",
      "startOffset" : 13,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "For example, Gao et al. (2013) developed user-specific features to capture user leniency, and Li et al. (2014) incorporated textual topic and userword factors through topic modeling. For integrating user information into neural network models, Tang et al. (2015) predicted the rating score given a review by using both lexical semantic information and a user embedding model.",
      "startOffset" : 13,
      "endOffset" : 263
    }, {
      "referenceID" : 2,
      "context" : "Chen et al. (2016b) proposed a neural network to incorporate global user and product information for sentiment classification via an attention mechanism.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "Our work also overlaps with to the area of opinion summarization, which constructs natural language summaries for multiple product reviews (Hu and Liu, 2004).",
      "startOffset" : 139,
      "endOffset" : 157
    }, {
      "referenceID" : 15,
      "context" : "Typical approaches include association mining of frequent candidate aspects (Hu and Liu, 2004; Qiu et al., 2011), sequence labeling based methods (Jakob and Gurevych, 2010; Yang and Cardie, 2013), as well as topic modeling techniques (Lin and He, 2009).",
      "startOffset" : 76,
      "endOffset" : 112
    }, {
      "referenceID" : 33,
      "context" : "Typical approaches include association mining of frequent candidate aspects (Hu and Liu, 2004; Qiu et al., 2011), sequence labeling based methods (Jakob and Gurevych, 2010; Yang and Cardie, 2013), as well as topic modeling techniques (Lin and He, 2009).",
      "startOffset" : 76,
      "endOffset" : 112
    }, {
      "referenceID" : 17,
      "context" : ", 2011), sequence labeling based methods (Jakob and Gurevych, 2010; Yang and Cardie, 2013), as well as topic modeling techniques (Lin and He, 2009).",
      "startOffset" : 41,
      "endOffset" : 90
    }, {
      "referenceID" : 45,
      "context" : ", 2011), sequence labeling based methods (Jakob and Gurevych, 2010; Yang and Cardie, 2013), as well as topic modeling techniques (Lin and He, 2009).",
      "startOffset" : 41,
      "endOffset" : 90
    }, {
      "referenceID" : 26,
      "context" : ", 2011), sequence labeling based methods (Jakob and Gurevych, 2010; Yang and Cardie, 2013), as well as topic modeling techniques (Lin and He, 2009).",
      "startOffset" : 129,
      "endOffset" : 147
    }, {
      "referenceID" : 16,
      "context" : "Recently, word embeddings and recurrent neural networks are also used to extract aspect terms (Irsoy and Cardie, 2014; Liu et al., 2015).",
      "startOffset" : 94,
      "endOffset" : 136
    }, {
      "referenceID" : 28,
      "context" : "Recently, word embeddings and recurrent neural networks are also used to extract aspect terms (Irsoy and Cardie, 2014; Liu et al., 2015).",
      "startOffset" : 94,
      "endOffset" : 136
    }, {
      "referenceID" : 28,
      "context" : "To address this, Nishikawa et al. (2010) generated summaries by selecting and ordering sentences taken from multiple review texts according to affirmativeness and readability of the sentence order.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 28,
      "context" : "To address this, Nishikawa et al. (2010) generated summaries by selecting and ordering sentences taken from multiple review texts according to affirmativeness and readability of the sentence order. Wang and Liu (2011) adopted both sentence-ranking and graphbased methods to extract summaries on an opinion conversation dataset.",
      "startOffset" : 17,
      "endOffset" : 218
    }, {
      "referenceID" : 8,
      "context" : "While all the methods above are extractive, Ganesan et al. (2010) presented a graph-based summarization framework to generate concise abstractive summaries of highly redundant opinions, and Wang and Ling (2016) used an attention-based neural network model to absorb information from multiple text units and generate summaries of movie reviews.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "While all the methods above are extractive, Ganesan et al. (2010) presented a graph-based summarization framework to generate concise abstractive summaries of highly redundant opinions, and Wang and Ling (2016) used an attention-based neural network model to absorb information from multiple text units and generate summaries of movie reviews.",
      "startOffset" : 44,
      "endOffset" : 211
    }, {
      "referenceID" : 0,
      "context" : "There are two main approaches, which are content-based and collaborative-filtering (CF) based (Adomavicius and Tuzhilin, 2005; Yang et al., 2014), respectively.",
      "startOffset" : 94,
      "endOffset" : 145
    }, {
      "referenceID" : 46,
      "context" : "There are two main approaches, which are content-based and collaborative-filtering (CF) based (Adomavicius and Tuzhilin, 2005; Yang et al., 2014), respectively.",
      "startOffset" : 94,
      "endOffset" : 145
    }, {
      "referenceID" : 18,
      "context" : "Most existing social recommendation systems are CF-based, and can be further grouped into model-based CF and neighborhood-based CF (Kantor et al., 2011; Su and Khoshgoftaar, 2009).",
      "startOffset" : 131,
      "endOffset" : 179
    }, {
      "referenceID" : 6,
      "context" : ", ratings, clicks, purchases) to improve the accuracy of traditional recommendation systems, which only factorize user-item feedback data (Ding et al., 2006; Koren, 2008; He et al., 2016).",
      "startOffset" : 138,
      "endOffset" : 187
    }, {
      "referenceID" : 20,
      "context" : ", ratings, clicks, purchases) to improve the accuracy of traditional recommendation systems, which only factorize user-item feedback data (Ding et al., 2006; Koren, 2008; He et al., 2016).",
      "startOffset" : 138,
      "endOffset" : 187
    }, {
      "referenceID" : 11,
      "context" : ", ratings, clicks, purchases) to improve the accuracy of traditional recommendation systems, which only factorize user-item feedback data (Ding et al., 2006; Koren, 2008; He et al., 2016).",
      "startOffset" : 138,
      "endOffset" : 187
    }, {
      "referenceID" : 22,
      "context" : "There has been work integrating sentiment analysis and recommendation systems, which use recommendation strategies such as matrix factorization to improve the performance of sentiment analysis (Leung et al., 2006; Singh et al., 2011).",
      "startOffset" : 193,
      "endOffset" : 233
    }, {
      "referenceID" : 43,
      "context" : ", 2011) or probabilistic graph models (Wu and Ester, 2015).",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 22,
      "context" : "There has been work integrating sentiment analysis and recommendation systems, which use recommendation strategies such as matrix factorization to improve the performance of sentiment analysis (Leung et al., 2006; Singh et al., 2011). These methods typically use ensemble learning (Singh et al., 2011) or probabilistic graph models (Wu and Ester, 2015). For example, Zhang et al. (2014) who proposed a factor graph model to recommend opinion rating scores by using explicit product features as hidden variables.",
      "startOffset" : 194,
      "endOffset" : 387
    }, {
      "referenceID" : 4,
      "context" : "Multi-task learning has been recognised as a strength of neural network models for natural language processing (Collobert et al., 2011; Henderson et al., 2013; Zhang and Weiss, 2016; Chen et al., 2016a), where hidden feature layers are shared between different tasks that have common basis.",
      "startOffset" : 111,
      "endOffset" : 202
    }, {
      "referenceID" : 12,
      "context" : "Multi-task learning has been recognised as a strength of neural network models for natural language processing (Collobert et al., 2011; Henderson et al., 2013; Zhang and Weiss, 2016; Chen et al., 2016a), where hidden feature layers are shared between different tasks that have common basis.",
      "startOffset" : 111,
      "endOffset" : 202
    }, {
      "referenceID" : 49,
      "context" : "Multi-task learning has been recognised as a strength of neural network models for natural language processing (Collobert et al., 2011; Henderson et al., 2013; Zhang and Weiss, 2016; Chen et al., 2016a), where hidden feature layers are shared between different tasks that have common basis.",
      "startOffset" : 111,
      "endOffset" : 202
    }, {
      "referenceID" : 2,
      "context" : "Multi-task learning has been recognised as a strength of neural network models for natural language processing (Collobert et al., 2011; Henderson et al., 2013; Zhang and Weiss, 2016; Chen et al., 2016a), where hidden feature layers are shared between different tasks that have common basis.",
      "startOffset" : 111,
      "endOffset" : 202
    }, {
      "referenceID" : 10,
      "context" : "Dynamic memory network models are inspired by neural turing machines (Graves et al., 2014), and have been applied for NLP tasks such as question answering (Sukhbaatar et al.",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 21,
      "context" : ", 2014), and have been applied for NLP tasks such as question answering (Sukhbaatar et al., 2015; Kumar et al., 2016), language modeling (Tran et al.",
      "startOffset" : 72,
      "endOffset" : 117
    }, {
      "referenceID" : 38,
      "context" : ", 2016), language modeling (Tran et al., 2016) and machine translation (Wang et al.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 42,
      "context" : ", 2016) and machine translation (Wang et al., 2016).",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 29,
      "context" : ", xm}, where m is the length of the review, each word xk is represented with a Kdimensional embedding ek (Mikolov et al., 2013).",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "A standard LSTM (Hochreiter and Schmidhuber, 1997) without coupled input and forget gates or peephole connections is used to learn the hidden states of the reviews.",
      "startOffset" : 16,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "We introduce an attention mechanism (Bahdanau et al., 2014; Yang et al., 2016) to extract the reviews that are relatively more important, and aggregate the representation of reviews to form a vector.",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 47,
      "context" : "We introduce an attention mechanism (Bahdanau et al., 2014; Yang et al., 2016) to extract the reviews that are relatively more important, and aggregate the representation of reviews to form a vector.",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 20,
      "context" : "Here a neighbor refers to a user that has similar tastes to the target user (Koren, 2008; Desrosiers and Karypis, 2011).",
      "startOffset" : 76,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "Here a neighbor refers to a user that has similar tastes to the target user (Koren, 2008; Desrosiers and Karypis, 2011).",
      "startOffset" : 76,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "In this study, we use matrix factorization (Koren, 2008) to detect neighbors, which is a standard approach for recommendation (Ding et al.",
      "startOffset" : 43,
      "endOffset" : 56
    }, {
      "referenceID" : 6,
      "context" : "We approximate it using three factors, which specify soft membership of products and users (Ding et al., 2006) by finding:",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 44,
      "context" : "In particular, a dynamic memory network (Sukhbaatar et al., 2015; Xiong et al., 2016) is utilized to iteratively find increasingly abstract representations of ht, by injecting vU and vN information.",
      "startOffset" : 40,
      "endOffset" : 85
    }, {
      "referenceID" : 35,
      "context" : "10 (Sutskever et al., 2014; Rush et al., 2015).",
      "startOffset" : 3,
      "endOffset" : 46
    }, {
      "referenceID" : 7,
      "context" : "We apply online training, where model parameters are optimized by using Adagrad (Duchi et al., 2011).",
      "startOffset" : 80,
      "endOffset" : 100
    }, {
      "referenceID" : 29,
      "context" : "We train word embeddings using the Skip-gram algorithm (Mikolov et al., 2013)4, using a window size of 5 and vector size of 128.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : "In order to avoid over-fitting, dropout (Hinton et al., 2012) is used for word embedding with a ratio of 0.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 27,
      "context" : "5 (Lin, 2004) toolkit for evaluating the performance of customized review generation, and report unigram overlap (ROUGE-1) as a means of assessing informativeness.",
      "startOffset" : 2,
      "endOffset" : 13
    }, {
      "referenceID" : 39,
      "context" : "We use Mean Square Error (MSE) (Wan, 2013; Tang et al., 2015) is used as the evaluation metric for measuring the performance of customized rating score prediction.",
      "startOffset" : 31,
      "endOffset" : 61
    }, {
      "referenceID" : 36,
      "context" : "• RS-Item applies kNN to estimate the rating score (Sarwar et al., 2001).",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "• RS-MF is a state-of-the-art recommendation model, which uses matrix factorisation to predict rating score (Ding et al., 2006; Li et al., 2009; He et al., 2016).",
      "startOffset" : 108,
      "endOffset" : 161
    }, {
      "referenceID" : 25,
      "context" : "• RS-MF is a state-of-the-art recommendation model, which uses matrix factorisation to predict rating score (Ding et al., 2006; Li et al., 2009; He et al., 2016).",
      "startOffset" : 108,
      "endOffset" : 161
    }, {
      "referenceID" : 11,
      "context" : "• RS-MF is a state-of-the-art recommendation model, which uses matrix factorisation to predict rating score (Ding et al., 2006; Li et al., 2009; He et al., 2016).",
      "startOffset" : 108,
      "endOffset" : 161
    }, {
      "referenceID" : 8,
      "context" : "• Sum-Opinosis uses a graph-based framework to generate abstractive summarisation given redundant opinions (Ganesan et al., 2010).",
      "startOffset" : 107,
      "endOffset" : 129
    }, {
      "referenceID" : 35,
      "context" : "• Sum-LSTM-Att is a state-of-the-art neural abstractive summariser, which uses an attentional neural model to consolidate information from multiple text sources, generating summaries using LSTM decoding (Rush et al., 2015; Wang and Ling, 2016).",
      "startOffset" : 203,
      "endOffset" : 243
    }, {
      "referenceID" : 41,
      "context" : "• Sum-LSTM-Att is a state-of-the-art neural abstractive summariser, which uses an attentional neural model to consolidate information from multiple text sources, generating summaries using LSTM decoding (Rush et al., 2015; Wang and Ling, 2016).",
      "startOffset" : 203,
      "endOffset" : 243
    } ],
    "year" : 2017,
    "abstractText" : "We present opinion recommendation, a novel task of jointly predicting a custom review with a rating score that a certain user would give to a certain product or service, given existing reviews and rating scores to the product or service by other users, and the reviews that the user has given to other products and services. A characteristic of opinion recommendation is the reliance of multiple data sources for multi-task joint learning, which is the strength of neural models. We use a single neural network to model users and products, capturing their correlation and generating customised product representations using a deep memory network, from which customised ratings and reviews are constructed jointly. Results show that our opinion recommendation system gives ratings that are closer to real user ratings on Yelp.com data compared with Yelp’s own ratings, and our methods give better results compared to several pipelines baselines using state-of-the-art sentiment rating and summarization systems.",
    "creator" : "LaTeX with hyperref package"
  }
}