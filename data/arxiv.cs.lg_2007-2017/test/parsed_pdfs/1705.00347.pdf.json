{
  "name" : "1705.00347.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Scalable Twin Neural Networks for Classification of Unbalanced Data",
    "authors" : [ "Himanshu Pant", "Mayank Sharma" ],
    "emails" : [ "eez142368}@ee.iitd.ac.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Learning from large datasets has been a challenge irrespective of the Machine Learning approach used. Twin Support Vector Machines (TWSVMs) have proved to be an efficient alternative to Support Vector Machine (SVM) for learning from imbalanced datasets. However, the TWSVM is unsuitable for large datasets due to the matrix inversion operations required. In this paper, we discuss a Twin Neural Network for learning from large datasets that are unbalanced, while optimizing the feature map at the same time. Our results clearly demonstrate the generalization ability and scalability obtained by the Twin Neural Network on large unbalanced datasets.\nKeywords. Twin SVM, Neural Network, Unbalanced Datasets, Large Datasets"
    }, {
      "heading" : "1. Introduction",
      "text" : "Support Vector Machines (SVMs) have been widely used as a machine learning technique for a wide variety of applications. In principle, the SVM aims at finding a maximum-margin hyperplane that best separates the categories of samples to be classified. The hyperplane is found mid-way between two parallel planes which pass through points that constitute the support vectors. However, the SVM may not be the optimal choice for a classifier as its complexity in terms of the VapnikChevronenkis (VC) dimension may be unbounded or infinite, as stated by Burges [4]. This poses a bottleneck for the SVM to generalize well on large datasets. Further, it may also be noted that the SVM hyperplane is based on finding a single hyperplane lying between two parallel planes, and it might not often be the best choice based on the distribution of the dataset, particularly in the case of unbalanced datasets.\nThe Twin SVM [8] (TWSVM) has been a derivative of the SVM (motivated by the Generalized Eigenvalue Proximal SVM - GEPSVM) which addresses the problem of class imbalance that arises in many practical applications, including multi-class scenarios. It finds two non-parallel hyperplanes, each of which as as close as possible to points of one class, while being at a distance of at least unity from points of the other class. In terms of generalization, this is a better approach for unbalanced datasets than finding a single maximum-margin separating hyperplane as in the case of the conventional SVM. This is because such a hyperplane would tend to be biased towards the class having larger number of samples in the dataset. Further, the TWSVM solves two smaller sized QPPs, each of which has as many constraints in the Linear Programming (LP) formulation as the number of points in one class. This is in contrast with the SVM QPP which has as many\npreprint submitted to arXiv May 2, 2017\nar X\niv :1\n70 5.\n00 34\n7v 1\n[ cs\n.L G\n] 3\n0 A\npr 2\n01 7\nconstraints as the number of points in the whole dataset. Thus, in principle, the TWSVM QPPs converge faster to a solution as opposed to the SVM.\nHowever, the TWSVM dual formulation which is conventionally used requires matrix operations such as inversion [8, Eqn (28)-(29)], which is not practical for large datasets due to the size of the data matrix. Further, these operations on large matrices may have issues of numerical stability, which tend to make formulations such as the Twin SVM unsuitable for large datasets. This motivates the need for a neural-network based training algorithm based on the principles of the Twin SVM, that enables effective generalization on large datasets. Another advantage obtained by developing such a framework is that as opposed to the conventional formulation which uses the same kernel function for samples of the two classes, the neural network based approach allows the kernel map to be separately optimized for samples of the two classes, which allows for better generalization.\nFormally, a neural network consists of a set of neurons stacked in an input and output layer, along with single or multiple hidden layers. These neurons are initialized with random weights. As the neural network learns from training samples, the weights are updated using an update rule derived on the basis of error minimization approaches such as gradient descent. Over iterations, these weights converge to their optimal values, allowing efficient prediction on test samples which need to be simply run through the network to obtain class predictions. Intermediate layer neuronal activations are computed using the optimal weights and the final layer arrives at a prediction for the test point. This makes neural networks scalable for large datasets, however it has been found that neural networks often suffer from overfitting and poor generalization ability.\nIn this paper, we propose a neural network formulation motivated by spirit of the TWSVM, whose architecture is summarized in Fig. 1. The objective is to be able to harness the generalization ability of the TWSVM formulation for a neural network, thereby making it scalable for large unbalanced datasets. The added advantage is the inherent optimization of the kernel map separately for the two classes by the layers of the Twin NN. We illustrate this claim by computing the classification accuracy and training time for several large unbalanced datasets.\nThe rest of the paper is organized as follows. Section 2 discusses the formulation for the TWSVM and mentions the dual formulation which is commonly used in practice. Section 3 introduces the Twin Neural Network formulation, followed by Section 4 which shows experimental results to benchmark the performance of the proposed network. Finally, Section 5 presents the conclusions and future work."
    }, {
      "heading" : "2. The Twin Support Vector Machine",
      "text" : "We begin by motivating the TWSVM from the classical SVM formulation, based on the inability of SVMs to classify unbalanced datasets efficiently. Consider a binary classification dataset of N samples in M dimensions X = {x(1), x(2), ..., x(N)|x(i) ∈ RM ,∀i = 1, 2, ..., N}, with corresponding labels Y = {y(1), y(2), ..., y(N)|y(i) ∈ {−1,+1},∀i = 1, 2, ..., N}. We can determine the number of points in each class from the this dataset. Let these be denoted as A = {x(i)|y(i) = +1} for samples of class (+1) and B = {x(i)|y(i) = −1} for samples of class (−1). The soft-margin Support Vector Machine (SVM) classifier has the following formulation.\nmin w,b,ξ\n1 2 ||w||2 + C\nl∑\ni=1\nξi (1)\nsubject to, y(i)(wT (x(i) + b)) ≥ 1− ξi, ξi ≥ 0, i = 1, . . . , l.\nwhere the separating hyperplane is given by the vector [w, b], C is the upper bound, ξi represents the slack variables for the soft-margin formulation and αi represents the Lagrange multipliers.\nThe SVM finds a maximum margin separating hyperplane for the binary classification dataset. This tends to be biased in cases when the dataset is unbalanced towards the class which has more number of samples. This is undesirable for better generalization on such unbalanced datasets. In order to address this situation, the TWSVM evolved as a viable alternative.\nThe TWSVM determines two non-parallel separating hyperplanes, each of which is as close as possible to points of one class while being at a distance of at least unity from points of the other class. Let these hyperplanes be denoted by the coefficient vectors w(+1), w(−1) and bias terms\nb(+1), b(−1) corresponding to classes (+1) and (−1) respectively. The TWSVM formulation thus involves solving the set of QPPs given by Eqns. (2)-(7).\nmin w(+1),b(+1),q\n1 2 ‖Aw(+1) + e+1b(+1)‖2 + C(+1)eT−1q, (2) s.t.− (Bw(+1) + e−1b(+1)) + q ≥ e−1 (3) q ≥ 0 (4)\nmin w(−1),b(−1),q\n1 2 ‖Bw(−1) + e−1b(−1)‖2 + C(−1)eT+1q, (5)\ns.t.− (Aw(−1) + e+1b(−1)) + q ≥ e+1 (6) q ≥ 0 (7)\nHere, data belonging to classes (+1) and (−1) are represented by matrices A[NA×M ] and B[NB×M ] respectively, where NA and NB are the number of points in each class such that NA+NB = N . e(+1) and e(−1) are vectors of ones of appropriate dimensions, q represents the error variable associated with the data point and C(+1), C(−1)(> 0) are hyperparameters. It may be noted here that each of the optimization problems solved for the TWSVM has constraints of only the points belonging to one class, as opposed to the conventional SVM which has points of both classes in the constraints. This makes the TWSVM faster than the conventional SVM.\nFor a test point x, the prediction y is computed by a distance measure based on these two hyperplanes. Essentially, the closer the test point is to one of these hyperplanes, the corresponding class is assigned to it. This is summarized by Eqn. (8).\ny = { +1 : wT(+1)x+ b(+1) ≤ wT(−1)x+ b(−1) −1 : otherwise (8)\nIn practice, it is more efficient to solve the dual formulation for the TWSVM, which is given by the Eqns. (9)-(10).\nmax α\neT2 α− 1\n2 αTG(HTH)\n−1 GTα (9)\ns.t. 0 ≤ α ≤ c1 max β eT1 β − 1 2 βTP (QTQ) −1 PTβ (10)\ns.t. 0 ≤ β ≤ c2\nHere, H = P = [A, e(+1)], G = Q = [B, e(−1)], u = [w(+1) b(+1)]T , v = [w(−1) b(−1)]T . As in the case of SVMs, the kernel formulation for the TWSVM is also possible, where the input dataset X is mapped to another feature space (often infinite dimensional) to introduce linear separability in the dataset samples in the mapped feature space. This is typically accomplished using a mapping function denoted by φ(·). The separating hyperplanes are then determined in this mapped space, which is often called as the kernel space.\nHowever, it can be seen from the dual formulation of the TWSVM in Eqns. 9-10 that there is a requirement for matrix inversion. This makes it unfeasible for large datasets, where such\ncomputations require large memory and are often intractable. Hence, we look for an approach for using the TWSVM within a neural network framework, that would not require such computations thereby making it scalable for large datasets."
    }, {
      "heading" : "3. The Twin Neural Network Formulation",
      "text" : "We consider a three-layer neural network to motivate the formulation of the Twin Neural Network, as shown in Fig. 1, whose structure is described as follows. The input layer takes the training samples x(i), i = 1, 2, ..., N and transforms it to a space φ(·) by the neurons of the hidden layer. The final or output layer of this network learns a classifier in the feature space denoted by φ(·), and the classifier hyperplane coefficients (weight vector w and bias b are used to arrive at the prediction for a test sample.\nFor the case of an unbalanced dataset, we propose the formulation of a Twin Neural Network, where two such networks are trained, whose error functions are denoted by E(+1) and E(−1). These are defined as shown in Eqns. (11)-(12).\nE(+1) = 1\n2×NB\nNB∑\ni=1\n(t(i) − y(i))2\n+ C(+1)\n2×NA\nNA∑\ni=1\n(wT(+1)φ(x (i) (+1)) + b(+1)) 2 (11)\nE(−1) = 1\n2×NA\nNA∑\ni=1\n(t(i) − y(i))2\n+ C(−1)\n2×NB\nNB∑\ni=1\n(wT(−1)φ(x (i) (−1)) + b(−1)) 2 (12)\nTo minimize the error, we set the corresponding derivatives to zero to obtain update rules for the weight vector w and bias b. The derivatives w.r.t. w(+1) and w(−1) are shown in Eqns. (13)-(14), which correspond to the weight update rules for our Twin Neural Network.\n∂E(+1) ∂w(+1) = 1 NB\nNB∑\ni=1\n(t(i) − y(i))(1− y2(i))φ(x (i) (−1))\n+ C(+1)\nNA\nNA∑\ni=1\n(wT(+1)φ(x (i) (+1)) + b(+1))φ(x (i) (+1)) (13)\n∂E(−1) ∂w(−1) = 1 NA\nNA∑\ni=1\n(t(i) − y(i))(1− y2(i))φ(x (i) (−1))\n+ C(−1) NB\nNB∑\ni=1\n(wT(−1)φ(x (i) (+1)) + b(−1))φ(x (i) (−1)) (14)\nDerivatives w.r.t b(+1) and b(−1) are shown in Eqns. (15)-(16), which are used as the update rule for the bias terms in our Twin Neural Network.\n∂E(+1) ∂b(+1) = 1 NB\nNB∑\ni=1\n(t(i) − y(i))(1− y2(i))\n+ C(+1)\nNA\nNA∑\ni=1\n(wT(+1)φ(x (i) (+1)) + b(+1)) (15)\n∂E(−1) ∂b(−1) = 1 NA\nNA∑\ni=1\n(t(i) − y(i))(1− y2(i))\n+ C(−1) NB\nNB∑\ni=1\n(wT(−1)φ(x (i) (−1)) + b(−1)) (16)\nBased on these, the weights and bias of the hyperplane are updated across the iterations until these hyperplane parameters coverge. The prediction on a test point x is obtained as follows. First, the point is mapped to the space φ(·) by the hidden layer. Then for the output layer, the label y is predicted using Eqn. 17.\ny =\n{ +1 :\nwT(+1)φ(x)+b(+1) ||w(+1)|| ≤ wT(−1)φ(x)+b(−1) ||w(−1)||\n−1 : otherwise (17)\nIn terms of machine complexity, it can be shown that the Twin NN formulation minimizes the VC dimension, which results in lower test set errors and hence better generalization. This is in addition to the benefit of scalability obtained as a result of using a neural-network based architecture. Drawing from Burges’s tutorial [4], the VC dimension γ of a classifier with margin d ≥ dmin is bounded by [10]\nγ ≤ 1 + min( R 2\nd2min , n) (18)\nwhere R denotes the radius of the smallest sphere enclosing all the training samples. The first Twin NN hyperplane wT(+1)φ(x)+b(+1) is similar to Eqn. 19 with the summation taken over samples of class (+1). Similarly, the second hyperplane wT(−1)φ(x) + b(−1) is identical to (19) with the summation taken over samples of class (−1).\nh = Max i=1,2,...M\n‖u‖ ‖xi‖ ≤ ∑\ni\n(uTxi)2 (19)\nAlternatively, the Twin NN objective function is a measure of the mis-classification error over samples that do not lie in the margin, and is summed over all the training samples of the other class. Hence, the Twin NN objective function tries to minimize a sum of a term that is related to the structural risk, and another that depends on the empirical error.\nTheorem 1. The Twin NN reduces the total risk by enforcing that an upper bound on the VC dimension is minimized. In effect, note that this is an approximation to the total risk. Vapnik and Chervonenkis [11] deduced, that with probability (1− η),\nRtotal(λ) ≤ Remp(λ) +\n√ h(ln 2Nh + 1)− ln η 4\nN (20)\nwhere, Remp(λ) = 1\nN\nN∑\ni=1\n|fλ(xi)− yi|, (21)\nand fλ is a function having VC-dimension h with the smallest empirical risk on a dataset {xi, i = 1, 2, ..., N} of N data points with corresponding labels {yi, i = 1, 2, ..., N}."
    }, {
      "heading" : "4. Experiments and Discussion",
      "text" : ""
    }, {
      "heading" : "4.1. Results on UCI datasets",
      "text" : "The Twin NN was tested on 20 benchmark datasets drawn from the UCI machine learning repository [1], which included two class and multi-class datasets. Input features were scaled to lie between -1 and 1. Target set values were kept at +1 and -1 for the respective classes. For multiclass problems, a one versus rest approach [3, pp. 182, 338] was used, with as many output layer neurons as the number of classes. A Twin NN with one hidden layer was employed for obtaining these results. The number of neurons in the hidden layer and the hyperparameters were optimized by using a grid search. The K-Nearest Neighbor (KNN) imputation method was used for handling missing attribute values, as it is robust to bias between classes in the dataset. Accuracies were obtained using a standard 5-fold cross validation methodology. This process was repeated 10 times to remove the effect of randomization. The accuracies were compared with the standard SVM [5], Twin SVM and Regularized Feed-Forward Neural Networks (RFNN) [2]. The results are shown in Table 1, which clearly indicates the superior performance of the Twin NN compared to the SVM, Twin SVM and and RFNN for 15 of the 20 datasets.\nIn addition, we also present a comparative analysis of the performance of the Twin NN on UCI benchmark datasets w.r.t. other approaches in terms of p-values determined using Wilcoxon’s signed ranks test [12]. The values for the Wilcoxon signed ranks test. The Wilcoxon Signed-Ranks Test is a measure of the extent of statistical deviation in the results obtained by using an approach. A p-value less than 0.05 indicates that the results have a significant statistical difference with the results obtained using the reference approach, whereas p-values greater than 0.05 indicate nonsignificant statistical difference. The p-values for the approaches considered are shown in Table 2, which clearly indicates that the Twin NN works better than the reference approaches.\nTo verify that the results on the UCI datasets are independent of the randomization due to the data distribution across folds, we also performed the Friedman’s test [7]. The p-value is 3.77×10−14 (< 0.05), implying that the superior results of the Twin NN are not due to chance, and are clearly a consequence of the approach used rather than the datasets."
    }, {
      "heading" : "4.2. Results on highly unbalanced datasets",
      "text" : "The key benefit obtained by using the Twin NN is its better generalization for unbalanced datasets. To establish this, we evaluate the Twin NN on several unbalanced datasets [9, 13, 6], which\nare summarized in Table 3. It may be noted here that the class imbalance has been introduced in these datasets by considering the multi-class datasets as separate binary datasets using one-v/srest approach. Thus for a dataset having N classes and M samples per class, we can in principle generate N datasets, each of which has a class ratio of M : (N − 1) ·M . Each of these possible datasets has been denoted as “GenN” suffixed to the dataset name in Table 3, where N represents the corresponding class w.r.t. which imbalance has been induced in the dataset. The procedure to compute the accuracies is illustrated in Fig. 3.\nFor evaluating the performance of the Twin NN on unbalanced datasets, measures other than the classification accuracy are often used in practice. These provide a better understanding of the performance of the learning algorithm taking into consideration the skewness in the dataset. To define these measures, we use four basic quantities, True Positives (TP ), True Negatives (TN), False Positives (FP ) and the False Negatives (FN). TP is the number of samples whose class label and predicted label are “true” and TN is the number of samples whose class and predicted labels are “false”. Correspondingly, we use FP for the samples whose class label is “false” and prediced label is “true”, and vice-versa for FN . These are schematically illustrated in the confusion matrix as shown in Fig. 4.\nBased on these, the basic measures are defined as given by Eqns. (22)-(26). Also, PC = TP + FN , NC = FP + TN , PR = TP + FP and NR = FN + TN .\nAccuracy (Acc):= TP + TN\nTP + TN + FP + FN (22)\nTrue Positive Rate (TPR):= TP TP + FN = TP PC (23)\nTrue Negative Rate (TNR):= TN TN + FP = TN NC (24)\nPositive Prediction Value (PPV):= TP TP + FP = TP PR (25)\nNegative Prediction Value (NPV):= TN TN + FN = TN NR (26)\nThe performance measure in terms of geometric mean between accuracy of the two classes is called the G-means. This is computed as the square-root of the product of the true positive and true negative rates, or as √ TPR ∗ TNR. The G-means for the unbalanced datasets is shown in Table 4. One can observe that the Twin NN has higher G-means value for all except one dataset when compared to the competing approaches.\nThe harmonic mean between the positive prediction value and true positive rate is called the F-measure, which is computed as given by Eqn. (27).\nF-measure := 2\n1 TPR + 1 PPV\n(27)\nThe f-measure for the unbalanced datasets is shown in Table 5. From the results, it is evident that the Twin NN has higher f-measure values for all except one dataset, which demonstrates its superior performance.\nIn addition, we also compute the Mathew Correlation Coefficient (MCC) for evaluating the performance of the Twin NN on unbalanced datasets. High MCC values imply that the classifier has high classification accuracy for both classes and also less mis-classification on both classes. The MCC is computed as given by Eqn. 28. The MCC values for the unbalanced datasets is given in Table 6, and the values are higher for the Twin NN for all except two datasets.\nMCC := TP ∗ TN − FP ∗ FN√ PC ∗NC ∗ PR ∗NR\n(28)\nThe p-values for the measures computed on the unbalanced datasets is shown in Table 7. As these values are less that 0.05, they do not indicate significant statistical difference.\nWe also show the average values of the measures computed on the unbalanced datasets in Table 8 for the various approaches. It can clearly be seen that the average values are highest for the Twin NN, indicating its superior performance across the unbalanced datasets. These are also graphically illustrated in Fig. 5.\nFinally, to illustrate the scalability of the Twin NN on the unbalanced datasets, we present the training time for the Twin NN for these datasets in Table 9. It can be seen that the Twin NN can be trained in tractable time for the unbalanced datasets."
    }, {
      "heading" : "4.3. Experiments on scalability",
      "text" : "We analyze the scalability of the Twin NN on the Forest Covertype dataset described in Table 10. The dataset with imbalance generated has an imbalance ratio of 1 : 211, with 44 numeric and 10 categorical attributes.\nThe performance parameters for this case have been shown in Table 11 for various number of training samples, and the corresponding training time has been shown in Table 12. These are also graphically illustrated in Fig. 6. It can be seen that in comparison to the other methods, the Twin NN scales well and also trains in tractable time with increase in number of training samples. This establishes the scalability of the Twin NN for imbalanced datasets."
    }, {
      "heading" : "5. Conclusion and Future Work",
      "text" : "This paper presented the Twin Neural Network, which provides a neural network training algorithm which minimizes machine complexity in terms of VC dimension and overcomes the issue of scalability of the Twin SVM. The Twin NN has been shown to have superior performance on several datasets, particularly for the case of unbalanced datasets."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The corresponding author would like to acknowledge the support of the Microsoft Chair Professor Project Grant (MI01158, IIT Delhi)."
    } ],
    "references" : [ {
      "title" : "Feed-forward neural networks",
      "author" : [ "George Bebis", "Michael Georgiopoulos" ],
      "venue" : "Potentials, IEEE,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1994
    }, {
      "title" : "Pattern recognition and machine learning",
      "author" : [ "Christopher M Bishop" ],
      "venue" : "springer,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "A tutorial on support vector machines for pattern recognition",
      "author" : [ "Christopher JC Burges" ],
      "venue" : "Data mining and knowledge discovery,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1998
    }, {
      "title" : "Libsvm: A library for support vector machines",
      "author" : [ "Chih-Chung Chang", "Chih-Jen Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Diversified ensemble classifiers for highly imbalanced data learning and their application in bioinformatics",
      "author" : [ "Zejin Ding" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "The use of ranks to avoid the assumption of normality implicit in the analysis of variance",
      "author" : [ "Milton Friedman" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1937
    }, {
      "title" : "Twin Support Vector Machines for pattern classification",
      "author" : [ "Jayadeva", "R. Khemchandani", "S. Chandra" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Machine learning for the detection of oil spills in satellite radar images",
      "author" : [ "Miroslav Kubat", "Robert C Holte", "Stan Matwin" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "Statistical learning theory",
      "author" : [ "Vladimir N Vapnik" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1998
    }, {
      "title" : "Individual comparisons by ranking methods",
      "author" : [ "Frank Wilcoxon" ],
      "venue" : "Biometrics bulletin,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1945
    }, {
      "title" : "Comparative evaluation of pattern recognition techniques for detection of microcalcifications in mammography",
      "author" : [ "Kevin S Woods", "Chiristopher C Doss", "Kevin W Bowyer", "Jeffrey L Solka", "Carey E Priebe", "W Philip Kegelmeyer JR" ],
      "venue" : "International Journal of Pattern Recognition and Artificial Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "However, the SVM may not be the optimal choice for a classifier as its complexity in terms of the VapnikChevronenkis (VC) dimension may be unbounded or infinite, as stated by Burges [4].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 6,
      "context" : "The Twin SVM [8] (TWSVM) has been a derivative of the SVM (motivated by the Generalized Eigenvalue Proximal SVM - GEPSVM) which addresses the problem of class imbalance that arises in many practical applications, including multi-class scenarios.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 2,
      "context" : "Drawing from Burges’s tutorial [4], the VC dimension γ of a classifier with margin d ≥ dmin is bounded by [10] γ ≤ 1 + min( R 2 dmin , n) (18)",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "Drawing from Burges’s tutorial [4], the VC dimension γ of a classifier with margin d ≥ dmin is bounded by [10] γ ≤ 1 + min( R 2 dmin , n) (18)",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "The accuracies were compared with the standard SVM [5], Twin SVM and Regularized Feed-Forward Neural Networks (RFNN) [2].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "The accuracies were compared with the standard SVM [5], Twin SVM and Regularized Feed-Forward Neural Networks (RFNN) [2].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "other approaches in terms of p-values determined using Wilcoxon’s signed ranks test [12].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "To verify that the results on the UCI datasets are independent of the randomization due to the data distribution across folds, we also performed the Friedman’s test [7].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 7,
      "context" : "To establish this, we evaluate the Twin NN on several unbalanced datasets [9, 13, 6], which",
      "startOffset" : 74,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "To establish this, we evaluate the Twin NN on several unbalanced datasets [9, 13, 6], which",
      "startOffset" : 74,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "To establish this, we evaluate the Twin NN on several unbalanced datasets [9, 13, 6], which",
      "startOffset" : 74,
      "endOffset" : 84
    } ],
    "year" : 2017,
    "abstractText" : "Learning from large datasets has been a challenge irrespective of the Machine Learning approach used. Twin Support Vector Machines (TWSVMs) have proved to be an efficient alternative to Support Vector Machine (SVM) for learning from imbalanced datasets. However, the TWSVM is unsuitable for large datasets due to the matrix inversion operations required. In this paper, we discuss a Twin Neural Network for learning from large datasets that are unbalanced, while optimizing the feature map at the same time. Our results clearly demonstrate the generalization ability and scalability obtained by the Twin Neural Network on large unbalanced datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}