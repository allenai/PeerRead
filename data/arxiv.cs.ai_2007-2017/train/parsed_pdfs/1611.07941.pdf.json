{
  "name" : "1611.07941.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-Modal Mean-Fields via Cardinality-Based Clamping",
    "authors" : [ "Pierre Baqué", "François Fleuret", "Pascal Fua" ],
    "emails" : [ "firstname.lastname@epfl.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We therefore replace the fully factorized distribution of Mean Field by a weighted mixture of such distributions, that similarly minimizes the KL-Divergence to the true posterior. By introducing two new ideas, namely, conditioning on groups of variables instead of single ones and using a parameter of the conditional random field potentials, that we identify to the temperature in the sense of statistical physics to select such groups, we can perform this minimization efficiently. Our extension of the clamping method proposed in previous works allows us to both produce a more descriptive approximation of the true posterior and, inspired by the diverse MAP paradigms, fit a mixture of Mean Field approximations. We demonstrate that this positively impacts real-world algorithms that initially relied on mean fields."
    }, {
      "heading" : "1 Introduction",
      "text" : "Mean Field (MF) is a modeling technique that has been central to statistical physics for a century. Its ability to handle stochastic models involving millions of variables and dense graphs has attracted much attention in our community. It is routinely used for tasks as diverse as detection [13, 2], segmentation [31, 23, 9, 41], denoising [10, 27, 25], depth from stereo [14, 23] and pose-estimation [34].\nMF approximates a “true” probability distribution by a fully-factorized one that is easy to encode and manipulate [22]. The true distribution is usually defined in practice through a Conditional Random Field (CRF), and may not be representable explicitly, as it involves complex inter-dependencies between variables. In such a case the MF approximation is an extremely useful tool.\nar X\niv :1\n61 1.\n07 94\n1v 1\n[ cs\n.C V\n] 2\nWhile this drastic approximation often conveys the information of interest, the true distribution may concentrate on configurations that are very different, equally likely, and that cannot be jointly encoded by a product law. Section 3 depicts such a case where groups of variables are correlated and may take one among many values with equal probability. In this situation, MF will simply pick one valid configuration, which we call a mode, and ignore the others. So-called structured Mean Field methods [32, 7] can help overcome this limitation. This can be effective but requires arbitrary choices in the design of a simplified sub-graph for each new problem, which can be impractical especially if the initial CRF is very densely connected.\nHere we introduce a novel way to automatically add structure to the MF approximation and show how it can be used to return several potentially valid answers in ambiguous situations. Instead of relying on a single fully factorized probability distribution, we introduce a mixture of such distributions, which we will refer to as Multi-Modal Mean Field (MMMF).\nWe compute this MMMF by partitioning the state space into subsets in which a standard MF approximation suffices. This is similar in spirit to the approach of [37] but a key difference is that our clamping acts simultaneously on arbitrarily sized groups of variables, as opposed to one at a time. We will show that when dealing with large CRFs with strong correlations, this is essential. The key to the efficiency of MMMF is how we choose these groups. To this end, we introduce a temperature parameter that controls how much we smooth the original probability distribution before the MF approximation. By doing so for several temperatures, we spot groups of variables that may take different labels in different modes of the distribution. We then force the optimizer to explore alternative solutions by clamping them, that is, forcing them to take different values. Our temperature-based approach, unlike the one of [37], does not require a priori knowledge of the CRF structure and is therefore compatible with “black box” models.\nIn the remainder of the paper, we will describe both MF and MMMF in more details. We will then demonstrate that MMMF outperforms both MF and the clamping method of [37] on a range of tasks."
    }, {
      "heading" : "2 Background and Related Work",
      "text" : "Conditional Random Fields (CRFs) are often used to represent correlations between variables [36]. Mean Field inference is a means to approximate them in a computationally efficient way. We briefly review both techniques below."
    }, {
      "heading" : "2.1 Conditional Random Fields",
      "text" : "Let X = (X1, . . . , XN ) represent hidden variables and I an image evidence. A CRF relates the ones to the others via a posterior probability distribution\nP (X | I) = exp (−E(X | I)− log(Z(I))) , (1)\nwhere E(X | I) is an energy function that is the sum of terms known as potentials φc(·) defined on a set of graph cliques c ∈ C, log(Z(I)) is the log-partition function\nthat normalizes the distribution. From now on, we will omit the dependency with respect to I."
    }, {
      "heading" : "2.2 Mean Field Inference",
      "text" : "The set of all possible configurations of X, that we denote by X , is exponentially large, which makes the explicit computation of marginals, Maximum-A-Posteriori (MAP) or Z intractable and a wide range of variational methods have been proposed to approximate P (X) [19]. Among those, Mean Field (MF) inference is one of the most popular. It involves introducing a distribution Q written as\nQ(X = (x1, . . . , xN )) = N∏ i=1 qi(xi) , (2)\nwhere qi( . ) is a categorical discrete distribution defined for xi in a possible labels space L. The qi are estimated by minimizing the KL-divergence\nKL(Q||P ) = ∑ x∈X Q(X = x) log Q(X = x) P (X = x) . (3)\nSinceQ is fully factorized, the terms of the KL-divergence can be recombined as a sum of an expected energy, containing as many terms as there are potentials and a convex negative entropy containing one term per variable. Optimization can then be performed using a provably convergent gradient-descent scheme [3].\nAs will be shown in Section 3, this simplification sometimes comes at the cost of downplaying the dependencies between variables. The DivMBest method [29, 4] addresses this issue starting from the following observation: When looking for an assignment in a graphical model, the resulting MAP is not necessarily the best because the probabilistic model may not capture all that is known about the problem. Furthermore, optimizers can get stuck in local minima. The proposed solution is to sequentially find several local optima and force them to be different from each other by introducing diversity constraints in the objective function. It has recently been shown that it is provably more effective to solve for diverse MAPs jointly but under the same set of constraints [20]. However, none of these methods provide a generic and practical way to choose local constraints to be enforced over variable sub-groups. Furthermore, they only return a set of MAPs. By contrast, our approach yields a multi-modal approximation of the posterior distribution, which is a much richer description and which we will show to be useful.\nAnother approach to improving the MF approximation is to decompose it into a mixture of product laws by “clamping” some of the variables to fixed values, and finding for each set of values the best factorized distribution under the resulting deterministic conditioning. By summing the resulting approximations of the partition function, one can provably improve the approximation of the true partition function [37]. This procedure can then be repeated iteratively by clamping successive variables but is only practical for relatively small CRFs. At each iteration, the variable to be clamped is chosen on the basis of the graphical model weights, which requires intimate knowledge about its internals, which is not always available.\nOur own approach is in the same spirit but can clamp multiple variables at a time without requiring any knowledge of the graph structure or weights.\nFinally, DivMBest approaches do not provide a way to choose the best solution without looking at the ground-truth, except for the one of [39] that relies on training a new classifier for that purpose. By contrast, we will show that the multi modal Bayesian nature of our output induces a principled way to use temporal consistency to solve directly practical problems."
    }, {
      "heading" : "3 Motivation",
      "text" : "To motivate our approach, we present here a toy example that illustrates a typical failure mode of the standard MF technique, which ours is designed to prevent. Fig. 1 depicts a CRF where each pixel represents a binary variable connected to its neighbors by attractive pairwise potentials.\nFor the sake of illustration, we split the grid into four zones as follows. The attractive terms are weak on left side but strong on the right. Similarly, in the top part, the unary terms favor value of 1 while being completely random in the bottom part.\nThe unary potentials are depicted at the top left of Fig. 1 and the result of the standard MF approximation at the bottom in terms of the probability of the pixels being assigned the label 1. In the bottom right corner of the grid, because the interaction potentials are strong, all pixels end up being assigned high probabilities of being 1 by MF, where they could just as well have all been assigned high probabilities to be zero. We explain below how our MMMF algorithm can produce two equally likely modes, one with all pixels being zero with high probability and the other with all pixel being one with high probability."
    }, {
      "heading" : "4 Multi-Modal Mean Fields",
      "text" : "Given a CRF defined with respect to a graphical model and the probability P (X = x) for all states in X , the state space introduced in Section 2.1, the standard MF approximation only models a single mode of the P , as discussed in Section 2.2. We therefore propose to create a richer representation that accounts for potential multiple modes by replacing the fully factorized distribution of Eq. 2 by a weighted mixture of such distributions that better minimizes the KL-divergence to P .\nThe potential roadblock is the increased difficulty of the minimization problem. In this section, we present an overview of our approach to solving it, and discuss its key aspects in the following two.\nFormally, let us assume that we have partitioned X into disjoint subsets Xk for 1 ≤ k ≤ K. We replace the original Mean Field (MF) approximation by one of the form\nP (X = x) ≈ QMM (X = x) = ∑ k mkQk(x) , (4)\nQk(x) = ∏ i qki (xi) ,\nwhere Qk is a MF approximation for the states x ∈ Xk with individual probabilities qki that variable i can take value xi in a set of labels L, and mk is the probability that a state belongs to Xk.\nWe can evaluate the mk and qki values by minimizing the KL-divergence between QMM and P . The key to making this computation tractable is to guarantee that we can evaluate the qki parameters on each subset separately by performing a standard MF approximation for each. One way to achieve that is to constrain the support of the Qk distributions to be disjoint, that is,\n∀k 6= k′, Qk′ (Xk) = 0 . (5)\nIn other words, each MF approximation is specialized on a subset Xk of the state space and is computed to minimize the KL-Divergence there. In practice, we enrich our approximation by recursively splitting a set of states Xk among our partition X1, . . . ,XK into two subsetsX 1k andX 2k to obtain the new partitionX1, . . . ,Xk−1,X 1k ,X 2k ,Xk+1, . . . ,XK , which is then reindexed from 1 to K+1. Initially, Xk represents the whole state space. Then we take it to be the newly created subset in a breadth-first order until a preset number of subsets has been reached. Each time, the algorithm proceeds through the following steps:\n• It finds groups of variables likely to have different values in different modes of the distribution using an entropy-based criterion for the qki .\n• It partitions the set into two disjoint subsets according to a clause that sets a threshold on the number of variables in this group that take a specific label. X 1k will contain the states among Xk that meet this clause and X 2k the others.\n• It performs an MF approximation within each subset independently to compute parameters qk,1i and q k,2 i for each of them. This is done by a standard MF approxima-\ntion, to which we add the disjointness constraint 5.\nThis yields a binary tree whose leaves are the Xk subsets forming the desired statespace partition. Given this partition, we can finally evaluate the mk. In Section 5, we introduce our cardinality based criterion and show that it makes minimization of the KL-divergence possible. In Section 6, we show how our entropy-based criterion selects, at each iteration, the groups of variables on which the clauses depend."
    }, {
      "heading" : "5 Partitioning the State Space",
      "text" : "In this section, we describe the cardinality-based criterion we use to recursively split state spaces and explain why it allows efficient optimization of the KL-divergence KL(QMM‖P ), where QMM is the mixture of Eq. 4."
    }, {
      "heading" : "5.1 Cardinality Based Clamping",
      "text" : "The state space partitionXk , 1≤k≤K introduced above is at the heart of our approximation and its quality and tractability critically depend on how well chosen it is. In [37], each split is obtained by clamping to zero or one the value of a single binary variable. In other words, given a set of states Xk to be split, it is broken into subsets X 1k = {x ∈ Xk|xi = 0} and X 2k = {x ∈ Xk|xi = 1}, where i is the index of a specific variable. To compute a Mean Field approximation to P on each of these subspaces, one only needs to perform a standard Mean Field approximation while constraining the qi probability assigned to the clamped variable to be either zero or one. However, this is limiting for the large and dense CRFs used in practice because clamping only one variable among many at a time may have very little influence overall. Pushing the solution towards a qualitatively different minimum that corresponds to a distinct mode may require simultaneously clamping many variables.\nTo remedy this, we retain the clamping idea but apply it to groups of variables instead of individual ones so as to find new modes of the posterior while keeping the estimation of the parameters mk and qki computationally tractable. More specifically, given a set of states Xk to be split, we will say that the split into X 1k and X 2k is cardinality-based if\nX 1k = {x ∈ Xk s.t. ∑\nu=1...L\n1(xiu = vu) ≥ C} , (6)\nX 2k = {x ∈ Xk s.t. ∑\nu=1...L\n1(xiu = vu) < C} , (7)\nwhere the i1, . . . , iL denote groups of variables that are chosen by the entropy-based criterion and v1, . . . , vL is a set of labels in L. In other words, in one of the splits, more than C of the variables have the assigned values and in the other less than C do. For example, for semantic segmentation X 1k would be the set of all segmentations in\nXk for which at least C pixels in a region take a given label, and X 2k the set of all segmentations for which less than C pixels do.\nWe will refer to this approach as cardinality clamping and will propose a practical way to select appropriate i1, . . . , iL and v1, . . . , vL for each split in Section 6."
    }, {
      "heading" : "5.2 Instantiating the Multi-Modal Approximation",
      "text" : "The cardinality clamping scheme introduced above yields a state space partitionXk , 1≤k≤K . We now show that given such a partition, minimizing the KL-divergence KL(QMM‖P ) using the multi-modal approximation of Eq. 4 under the disjointness constraint, becomes tractable.\nIn practice, we relax the constraint 5 to near disjointness\n∀k 6= k′, Qk′ (Xk) ≤ , (8)\nwhere is a small constant. It makes the optimization problem better behaved and removes the need to tightly constrain any individual variable, while retaining the ability to compute the KL divergence up to O( log( )).\nLet m̂ and q̂ stand for all the mk and qki parameters that appear in Eq. 4. We compute them as\nmin m̂,q̂ KL(QMM‖P )= min m̂,q̂ ∑ x∈X ∑ k≤K mkQk(x) log ( QMM (x) P (x) ) ≡ min\nm̂ ∑ k≤K mk log(mk)− ∑ k≤K mkAk , (9)\nwhere Ak = max qki ,i=1...N ∑ x∈X Qk(x) log ( e−E(x) Qk(x) ) (10)\nwhere Ak is maximized under the near-disjointness constraint of Eq. 16. As proved formally in the supplementary material, the second equality of Eq. 9 is\nvalid up to a constant and after neglecting a term of order O( log ) which appears under the near disjointness assumption of the supports. Given the Ak terms of Eq. 10 and under the constraints that the mixture probabilities m̂ sum to one, we must have\nmk = eAk∑\nk′≤K eAk′\n, (11)\nand we now turn to the computation of these Ak terms. We formulate it in terms of a constrained optimization problem as follows."
    }, {
      "heading" : "5.2.1 Handling Two Modes",
      "text" : "Let us first consider the case where we generate only two modes modeled by Q1(x) =∏ q1i (xi) and Q2(x) = ∏ q2i (xi) and we seek to estimate the q 1 i probabilities. The q 2 i probabilities are evaluated similarly.\nRecall from Section 5.2 that the q1i must be such that the A1 term of Eq. 10 is maximized subject to the near disjointness constraint of Eq. 16, which becomes\nQ1 ( ∑ u=1...L 1(Xiu = vu) < C ) ≤ , (12)\nunder our cardinality-based clamping scheme defined by Eq. 7. Performing this maximization using a standard Lagrangian Dual procedure [8] requires evaluating the constraint and its derivatives. Despite the potentially exponentially large number of terms involved, we can do this in one of two ways. In both cases, the Lagrangian Dual procedure reduces to a series of unconstrained Mean Field minimizations with well known additional potentials.\n1. When C is close to 0 or to L, the Lagrangian term can be treated as a specific form of pattern-based higher-order potentials, as in [35, 13, 21, 1]. 2. When C is both substantially greater than zero and smaller than L, we treat∑ u=1...L 1(Xiu = vu) as a large sum of independent random variables un-\nder Q1. We therefore use a Gaussian approximation to replace the cardinality constraint by a simpler linear one, and finally add unary potentials to the MF problem. Details are provided in the supplementary material.\nWe will encounter the first situation when tracking pedestrians and the second when performing semantic segmentation, as will be discussed in the results section."
    }, {
      "heading" : "5.2.2 Handling an Arbitrary Number of Nodes",
      "text" : "Recall from Section 5 that, in the general case, there can be an arbitrary number of modes. They correspond to the leaves of a binary tree created by a succession of cardinality-based splits. Let us therefore consider mode k for 1 ≤ k ≤ K. Let B be the set of branching points on the path leading to it. The near disjointness 16, can be enforced with only |B| constraints. For each b ∈ B, there is a list of variables ib1, . . . , i b Lb , a list of values v b 1, . . . , v b Lb , a cardinality threshold C\nb, and a sign for the inequality ≥b that define a constraint\nQk ( ∑ u=1...Lb 1(Xibu = v b u) ≥b Cb ) ≤ (13)\nof the same form as that of Eq. 12. It ensures disjointness with all the modes in the subtree on the side of b that mode k does not belong to. Therefore, we can solve the constrained maximization problem of Eq. 10, as in Section 5.2.1, but with |B| constraints instead of only one."
    }, {
      "heading" : "6 Selecting Variables to Clamp",
      "text" : "We now present an approach to choosing the variables i1, . . . , iL and the values v1, . . . , vL, which define the cardinality splits of Eqs. 6 and 7, that relies on phase transitions in the graphical model.\nTo this end, we first introduce a temperature parameter in our model that lets us smooth the probability distribution we want to approximate. This well known parameter for physicists [18] was used in a different context in vision by [28]. We study its influence on the corresponding MF approximation and how we can exploit the resulting behavior to select appropriate values for our variables."
    }, {
      "heading" : "6.1 Temperature and its Influence on Convexity",
      "text" : "We take the temperature T to be a number that we use to redefine the probability distribution of Eq. 1 as\nPT (x) = 1 ZT e −\n1 T E(x) , (14)\nwhere ZT is the partition function that normalizes PT so that its integral is one. For T = 1, PT reduces to P . As T goes to infinity, it always yields the same Maximum-APosteriori value but becomes increasingly smooth. When performing the MF approximation at high T , the first term of the KL-Divergence, the convex negative entropy, dominates and makes the problem convex. As T decreases, the second term of the KLDivergence, the expected energy, becomes dominant, the function stops being convex, and local minima can start to appear. In the supplementary material, we introduce a physics-inspired proof that, in the case of a dense Gaussian CRF [23], we can approximate and upper-bound, in closed-form, the critical temperature Tc at which the KL divergence stops being convex. We validate experimentally this prediction, using directly the denseCRF code from [23]. This makes it easy to define a temperature range [1, Tmax] within which to look for Tc. For a generic CRF, no such computation may be possible and the range must be determined empirically."
    }, {
      "heading" : "6.2 Entropy-Based Splitting",
      "text" : "We describe here our approach to splitting X into X1 and X2 at the root node of the tree. The subsequent splits are done in exactly the same way. The variables to be clamped are those whose value change from one local minimum to another so that we can force the exploration of both minima.\nTo find them, we start at Tmax, a temperature high enough for the KL divergence to be convex and progressively reduce it. For each successive temperature, we perform the MF approximation starting with the estimate for the previous one to speed up the computation. When looking at the resulting set of approximations starting from the lowest temperature ones T = 1, a telltale sign of increasing convexity is that the assignment of some variables that were very definite suddenly becomes uncertain. Intuitively, this happens when the CRF terms that bind variables is overcome by the entropy terms that encourage uncertainty. In physical terms, this can be viewed as a local phase-transition [18].\nLet T be a temperature greater than 1 and letQT andQ1 be the corresponding Mean Field approximations, with their marginal probabilities qTi and q 1 i for each variable i. To detect such phase transitions, we compute\nδi(T ) = 1[H(qTi ) > hhigh]1[H(q1i ) < hlow] , (15)\nfor all i, whereH denotes the individual entropy. All variables and labels with positive δi become candidates for clamping. If there\nare none, we increase the temperature. If there are several, we can either pick one at random or use domain knowledge to pick the most suitable subset and values as will be discussed in the Results Section."
    }, {
      "heading" : "7 Results",
      "text" : "We first use synthetic data to demonstrate that MMMF can approximate a multi-modal probability density function better than both standard MF and the recent approach of [37], which also relies on clamping to explore multiple modes. We then demonstrate that this translates to an actual performance gain for two real-world algorithms—one for people detection [13] and the other for segmentation [9, 40]—both relying on a traditional Mean Field approach. We will make all our code and test datasets publicly available.\nThe parameters that control MMMF are the number of modes we use, the cardinality threshold C at each split, the value of Eq. 16, the entropy thresholds hlow and hhigh of Eq. 15, and the temperature Tmax introduced in Section 6. In all our experiments, we use = 10−4, hlow = 0.3, and hhigh = 0.7. As discussed in Section 6, when the CRF is a dense Gaussian CRF, we can approximate and upper bound the critical temperature Tc in closed-form and we simply take Tmax to be this upper bound to guarantee that Tmax > Tc. Otherwise, we choose Tmax empirically on a small validation-set and fix it during testing."
    }, {
      "heading" : "7.1 Synthetic Data",
      "text" : "To demonstrate that our approach minimizes the KL-Divergence better than both standard MF and the clamping one of [37], we use the same experimental protocol to generate conditional random fields with random weights as in [12, 38, 37]. Our task is then to find the MMMF approximation with lowest KL-Divergence for any given number of nodes. When that number is one, it reduces to MF. Note that the authors of [37] look for an approximation of the log-partition function, which is strictly the same as minimizing the KL-Divergence, as demonstrated in the supplementary material. Because it involves randomly chosen positive and negative weights, this problem effectively mimics difficult real-world ones with repulsive terms, uncontrolled loops, and strong correlations.\nIn Fig. 2, we plot the KL-Divergence as a function of the number of modes used to approximate the distribution on the standard benchmarks. These modes are obtained using either our entropy-based criterion as described in Section 6, or the MaxW one of [37], which we will refer to as BASELINE-MAXW. It involves sequentially clamping the variable having the largest sum of absolute values of pairwise potentials for edges linking it to its neighbors. It was shown to be one of the best methods among several others, which all performed roughly similarly. In our experiments, we used the phase-transition criterion of Section 6 to select candidate variables to clamp. We then either randomly chose the group of L variables to clamp or used the MaxW\ncriterion of [37] to select the best L variables. We will refer to the first as OURSRANDOM and to the second as OURS-MAXW. Finally, in all cases, C = L and the values vu correspond to the ones taken by the MAP of the mode split.\nIn Fig. 2, we plot the resulting curves for L = 1 and L = 3, evaluated on 100 instances. OURS-RANDOM performs better than the method BASELINE-MAXW in most cases, even though it does not use any knowledge of the CRF internals, and OURS-MAXW, which does, performs even better. The results on the 13 × 13 grid demonstrate the advantage of clamping variables by groups when the CRF gets larger."
    }, {
      "heading" : "7.2 Multi-modal Probabilistic Occupancy Maps",
      "text" : "The Probabilistic Occupancy Map (POM) method [13] relies on Mean Field inference for pedestrian detection. More specifically, given several cameras with overlapping fields of view of a discretized ground plane, the algorithm first performs background subtraction. It then estimates the probabilities of occupancy at every discrete location as the marginals of a product law minimizing the KL divergence from the “true” conditional posterior distribution, formulated as in Eq. 1 by defining an energy function. Its value is computed by using a generative model: It represents humans as simple cylinders projecting to rectangles in the various images. Given the probability of presence or absence of people at different locations and known camera models, this produces synthetic images whose proximity to the corresponding background subtraction images is measured and used to define the energy.\nThis algorithm is usually very robust but can fail when multiple interpretations of a background subtraction image are possible. This stems from the limited modeling power of the standard MF approximation, as illustrated in the supplementary material. We show here that, in such cases, replacing MF by MMMF while retaining the rest of\nthe framework yields multiple interpretations, among which the correct one is usually to be found.\nFig. 3 depicts what happens when we replace MF by MMMF to approximate the true posterior, while changing nothing else to the algorithm. To generate new branches of the binary tree of Section 5, we find potential variables to clamp as described in Section 6. Among those, we clamp the one with the largest entropy gap—H(qTi ) − H(q1i ), using the notations of Eq. 15—and its neighbors on the grid. When evaluating our cardinality constraint, we take C to be 1, meaning that one branch of the tree corresponds to no one in the neighborhood of the selected location and the other to at least one person being present in this neighborhood. Since we typically create those locations by discretizing the ground plane into 10cm× 10cm grid cells, this forces the two newly instantiated modes to be significantly different as opposed to featuring the same detection shifted by a few centimeters. In Fig. 3, we plot the results as dotted curves representing the MODA scores as functions of the distance threshold used to compute them [6]. In all cases, we used 4 modes for the MMMF approximation and followed the DivMBest evaluation metric [4] to produce a score by selecting among the 4 detection maps corresponding to each mode the one yielding the highest MODA score. This produces red dotted MMMF curves that are systematically above the blue dotted MF.\nHowever, to turn this improvement into a practical technique, we need a way to choose among the 4 possible interpretations without using the ground truth. We use temporal consistency to jointly find the best sequence of modes, and reconstruct trajectories from this sequence. In the original algorithm, the POMs computed at successive instants were used to produce consistent trajectories using the a K-Shortest Path (KSP) algorithm [5]. This involves building a graph in which each ground location at each time step corresponds to a node and neighboring locations at consecutive time steps are connected. KSP then finds a set of node-disjoint shortest paths in this graph where the cost of going through a location is proportional to the negative log-probability of the location in the POM [33]. Since MMMF produces multiple POMs, we then solve a multiple shortest-path problem in this new graph, with the additional constraint that at each time step all the paths have to go through copies of the nodes corresponding to the same mode, as described in more details in the supplementary material.\nThe solid blue lines in Fig. 3 depict the MODA scores when using KSP and the red ones the multi-modal version, which we label as KSP∗. The MMMF curves are again above the MF ones. This makes sense because ambiguous situations rarely persist for more than a few a frames. As a result, enforcing temporal consistency eliminates them."
    }, {
      "heading" : "7.3 Multi-Modal Semantic Segmentation",
      "text" : "CRF-based semantic segmentation is one of best known application of MF inference in Computer Vision and many recent algorithms rely on dense CRF’s [23] for this purpose. We demonstrate here that our MMMF approximation can enhance the inference component of two such recent algorithms [9, 40] on the Pascal VOC 2012 segmentation dataset and the MPI video segmentation one [15].\nIndividual VOC Images We write the posterior in terms of the CRF of [9], which we try to approximate. To create a branch of the binary tree of Section 5, we first find the potential variables to clamp as described in Section 6. As in 7.2, we select the ones in the sliding window with the largest entropy gap,H(qTi )−H(q1i ). We then take C to be L/2 when evaluating our cardinality constraint, meaning that we seek the dominant label among the selected variables and split the state space into those for which more than half these variables take this value and those in which less than half do.\nFig. 4 illustrates the results on an image of the VOC dataset. To evaluate such results quantitatively, we first use the DivMBest metric [4], as we did in Section 7.2. We assume we have an oracle that can select the best mode of our multi-modal approximation by looking at the ground truth. Fig. 5 depicts the results on the validation set of the VOC 2012 Pascal dataset in terms of the average intersection over union (IU) score as a function of the number of modes. When only 1 mode is used, the result boils down to standard MF inference as in [9]. Using 32 yields a 2.5% improvement over the MF approximation. This may seem small until one considers that we only modify the\nalgorithm’s inference engine and leave the unary terms unchanged. In [9, 41], this engine has been shown to contribute approximately 3% to the overall performance, which means that we almost double its effectiveness. For analysis purposes, we implemented two baselines:\n• Instead of clamping groups of variables, we only clamp the variable with the maximum entropy gap at each step. As depicted by the red curve in Fig. 5, this has absolutely no effect and illustrates the importance of clamping groups of variable instead of single ones as in [37].\n• The DivMBest approach [4] first computes a MAP and then adds a penalty term to the energy function to find another MAP that is different from the first. It then repeats the process. We adapted this approach for MF inference. The green curve in Fig. 5 depicts the result, which MMMF outperforms by 1.5%.\nSemantic Video Segmentation. We ran the same experiment on the images of the MPI video segmentation dataset [15] using the CRF of [40]. In this case, we can exploit temporal consistency to avoid having to use an oracle and nevertheless get an exploitable result, as we did in Section 7.2. Furthermore, we can do this in spite of the relatively low frame-rate of about 1Hz.\nMore specifically, we first define a compatibility measure between consecutive modes based on label probabilities of matching key-points, which we compute using a key-point matching algorithm [30]. We then compute a shortest path over the sequence of modes, taking into account individual mode probabilities given by Eq. 11. Finally, we use only the MAP corresponding to the mode chosen by the shortest path algorithm to produce the segmentation. In Fig. 1, we again report the results in terms of IU score. This time the improvement is around 2.4%, which indicates that imposing temporal consistency very substantially improves the quality of the inference. To the best of our knowledge, other state of the art video semantic segmentation methods are not applicable for such image sequences. [17] requires non-moving scenes and a super-pixel decomposition, which prevents using all the dense CRF-based image segmentors. [24] was only applied to street scenes and requires a much higher frame rate to provide an accurate flow estimation."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We have shown that our MMMF aproach makes it possible to add structure to the standard MF approximation of CRFs and to increase the performance of algorithms that depend on it. In effect, our algorithm creates several alternative MF approximations with probabilities assigned to them, which effectively models complex situations in which more than one interpretation is possible.\nSince MF has recently been integrated into structured learning architectures through the Back Mean-Field procedure [11, 25, 41, 1], future work will aim to replace MF by MMMF in this context as well."
    }, {
      "heading" : "A Proofs for Multi-Modal Mean-Fields via CardinalityBased Clamping",
      "text" : "This document provides technical details and proofs related to Section 5. We first prove the approximation of the KL-Divergence used in Eq. 9. Then, we show that the problem that we are trying to solve in Eq. 9, the minimization of the KL-Divergence, is actually equivalent to the one solved by [37], namely, finding an approximation to the log-partition function. It eventually justifies the benchmark experiments ran in 7.1. Finally, we justify the Gaussian approximation used in the case of large clamping groups in 5.2.1-(2).\nA.1 Minimising the KL-Divergence Let us see how the KL-Divergence between QMM and P of Eq. 3 can be minimised with respect to the parameters mk and to the distributions Qk, leading to Eq. 9. We reformulate the minimisation problem up to a constant approximation factor of order log( ).\nFirst, remember that our minimisation problem enforces the near-disjointness condition, ∀k 6= k′ ∑ x∈X ′k Qk(x) ≤ , (16) between the elements of the mixture. Let us then prove the following useful Lemma.\nLemma A.1 For all mixture element k ≤ K,\n∑ x∈X Qk(x) log ∑ k′≤K mk′Qk′(x)  = ∑ x∈X Qk(x) log (mkQk(x)) +O( log ) .\n(17)\nProof Let k be the index of a mixture component k ≤ K, and let us denote the approximation error\nδk = ∑ x∈X Qk(x) log ∑ k′≤K mk′Qk′(x) −∑ x∈X Qk(x) log (mkQk(x)) . (18)\nThen, we use the near-disjointness condition to bound δk, δk ≤ ∑ x∈Xk Qk(x) log 1 + ∑ k′ 6=k mk′Qk′(x) Qk(x)  ︸ ︷︷ ︸\nI\n+ ∑\nx∈X\\Xk\nQk(x) log 1 + ∑ k′ 6=k mk′Qk′(x)\nQk(x)  ︸ ︷︷ ︸\nJ\n(19)\nWe first use the well known inequality log(1 + x) ≤ x in order to upper bound I ,\nI ≤ ∑ x∈Xk Qk(x)\n∑ k′ 6=k mk′Qk′(x)\nQk(x) (20)\n≤ ∑ k′ 6=k ∑ x∈Xk mk′Qk′(x) (21)\n≤ ∑ k′ 6=k\n(22)\n≤ O( ) . (23)\nThe second term, J , can then be upper-bounded using the fact that the mk′ and Qk′ are mixture weights and probabilities and hence ∑ k′ 6=k mk′Qk′(x) ≤ 1 for all x. Therefore,\nJ ≤ ∑\nx∈X\\Xk\nQk(x) log\n( 1 + 1\nQk(x)\n) (24)\n≤ ∑\nx∈X\\Xk\n−Qk(x) log (Qk(x)) (25)\n≤ ∑ k′ 6=k ∑ x∈Xk′ −Qk(x) log (Qk(x)) . (26)\nFurthermore, for all k′ 6= k, the near-disjointness condition enforces that ∑\nx∈Xk′ Qk(x) ≤\n. Under this constraint, on each of the subsets Xk′ , the maximal entropy is reached if Qk(x) =\n| Xk′ | for all x in Xk′ . And, therefore\n∑ x∈Xk′ −Qk(x) log (Qk(x)) ≤ log ( | X ′k | ) (27)\n≤ O( log ) +O( ) , (28)\nwhere the factor log(| Xk |), which is of the order of the number of variables, has been integrated in the constant.\nHence,\nJ ≤ ∑ k′ 6=k ∑ x∈Xk′ −Qk(x) log (Qk(x)) (29)\n≤ O( log ) +O( ) , (30) (31)\nwhich terminates the proof.\nWe can then move on to the minimisation of the KL-Divergence\nmin m̂,q̂ KL(QMM‖P ) = min m̂,q̂ ∑ x∈X ∑ k≤K QMM (x) log ( QMM (x) P (x) ) (32)\n= min m̂,q̂ ∑ x∈X ∑ k≤K QMM (x) log ( QMM (x) e−E(x) ) + log(Z) (33)\n= min m̂,q̂ ∑ k≤K ∑ x∈X mkQk(x) log\n ∑ k′≤K mk′Qk′(x)\ne−E(x)\n+ log(Z) (34)\n= min m̂,q̂ ∑ k≤K ∑ x∈X mkQk(x) log ( mkQk(x) e−E(x) ) + log(Z) +O( log )\n(35)\n= min m̂ ∑ k≤K mk logmk + ∑ k≤K min qk ∑ x∈X mkQk(x) log ( Qk(x) e−E(x) )+ log(Z) +O( log ) (36)\n= min m̂ ∑ k≤K mk log(mk)− ∑ k≤K mkAk + log(Z) +O( log ) ,\n(37)\nwhere,\nAk = max qki ,i=1...N ∑ x∈X Qk(x) log ( e−E(x) Qk(x) ) .\nEquation 35 is obtained using Lemma A.1. Assuming that we are able to compute Ak, for all k, the minimisation of this KL-\nDivergence with respect to parameters mk, under the nomalisation constraint∑ k≤K mk = 1 , (38)\nis then straightforward and leads to\nmk = eAk∑\nk′≤K eAk′\n. (39)\nA.2 Equivalence with the approximation of the partition function. The recent work of [37], that we use as a baseline, looks for the best heuristic to choose the clamping variables. They measure the quality of the approximation through the closeness of the estimated partition function, which they compute as the sum of MF\napproximated partition functions for each component of the mixture, to the true one. We will now see that this problem is strictly equivalent to the minimisation of the KLDivergence of Eq. 9.\nIndeed, replacing 38 in 37, we directly obtain that KL(QMM‖P ) = log( ∑ k′≤K eAk′ ) + log(Z) +O( log ) (40)\n= log(Z)− log(Z̃) +O( log ) , (41)\nwhere, Z̃ = ∑ k′≤K eAk′ , (42)\nis precisely the approximation of the partition function Z proposed by [37]. In other terms, it is the sum of local variational lower-bounds on clamped subsets of the state space.\nA.3 Gaussian approximation to the cardinality constraint. In the following, we explain the Gaussian approximation of the cardinality constraint used in 5.2.1-(2) and in our application to Semantic Segmentation. Let us consider the case where we generate only two modes modelled by Q1(x) = ∏ q1i (xi) and\nQ2(x) = ∏ q2i (xi) and we seek to estimate the q 1 i probabilities. The q 2 i probabilities are evaluated similarly. Recall that, each Ak is obtained through the constrained MF optimisation problem\nmax qki ,i=1...N ∑ x∈X Qk(x) log ( e−E(x) Qk(x) )\ns.t. Q1 ( ∑ u=1...L 1(Xiu = vu) < C ) ≤ .\n(43)\nUnder the probabilityQ1, ∑\nu=1...L\n1(Xiu = vu) is a sum of independent binary ran-\ndom variables that are non identically distributed, in other words, a Poisson Binomial Distribution. In the general case, there is no closed-form formula for computing the Cumulative Distribution Function of such a distribution from the individual marginals parametrising Q1. However, when L is large (≥ 10), the Gaussian approximation is good enough.\nTherefore, we use a Gaussian approximation to replace the cardinality constraint by ∑\nu∈{1...L}\nq1iu(vu) < C + σF −1(1− ) , (44)\nwhere F is the Gaussian cumulative distribution fonction and σ2 the variance, which, in theory should be\nσ2 = ∑\nu∈{1...L}\nq1iu(vu)(1− q 1 iu(vu)) , (45)\nbut which can be either upper-bounded by L\n4 or re-estimated at the beginning of each\nLagrangian iteration. In short, we replace the untractable higher order constraint 43, by a simple one involving only the sum of the MF parameters q1iu(vu)."
    }, {
      "heading" : "B Computing the Critical Temperature for the Dense Gaussian CRFs",
      "text" : "We first compute analytically the phase transition temperature parameter Tc of 6.2 where the KL-Divergence stops being convex. In the first part Analytical Derivation, we make strong assumptions in order to be able to obtain a closed form estimation of Tc. We then explain how this result helps understanding real cases. In the second part Experimental Analysis, in order to justify our assumptions, we run experiments under three regimes, one where our assumptions are strictly verified, one which corresponds to a real-life scenario and an intermediate one. This set of experiments shows that our strong assumptions provide a valuable insight for practical applications.\nB.1 Analytical derivation Let us take probability distribution P to be defined by a dense Gaussian CRF [23]. In order to make computation tractable, we assume that the RGB distance between pixels is uniform and equal to drgb. Therefore the RGB Kernel is constant with value\nθrgb = e\n−d2rgb 2σrgb . (46)\nWe consider the case where we have only two possible labels and the same unary potential on all the variables. Even if this assumption sounds strong, we can expect them to be locally valid. Formally, on a N × N dense grid, the energy function is defined as\nE(x) = Γθrgb 2πσ2 ∑ (i,j),(i′,j′) 1[x(i,j) 6= x(i′,j′)]e − ‖(i, j)− (i′, j′)‖2 2σ\n+ ∑ (i,j) U(i,j)1[x(i,j) = 0] ,\nwhere σ controls the range of the correlations and U(i,j) is a unary potential. Since that we assumed that all the variables receive the same unary U , all the variables are undiscernibles. Furthermore, the pairwise potentials are attractive, we therefore expect all the mean-field parameters qi,j = Q(xi,j = 0) to have the same value at the fixed point solution of the Mean-Field. Therefore, we designate this common parameter qT and we can try to find analytically the Mean-Field fixed point for qT corresponding to a temperature T .\nAt convergence, the parameter qT will have to satisfy\nlog(qT ) = EQ(E(x)|xi = 0)\n= − Γθrgb 2πσ2T ∑ (i,j)∈Z×Z (1− qT )e − ‖(i, j)‖2 2σ − U T\n= − (1− q T )Γθrgb + U\nT\nHence, we obtain the fixed point equation\nq̃T = 1\n2 tanh\n( q̃T Γθrgb − U\nT\n) , (47)\nwhere q̃T = qT − 0.5. As depicted in Figure 6, when unaries are 0 (on the left) there are two distinct regimes for the solutions of this equation. For high T , there is only one stable solution at q̃ = 0. For low T , there are two distinct stable solutions where q̃ is close to −0.5 or 0.5. The temperature threshold Tc where the transition happens, corresponds to the solution of\n1\n2\nd tanh( q̃Γθrgb T )\ndq̃ |q̃=0 = 1 , (48)\nand hence Tc = Γθrgb\n2 . For real images, we have θrgb ≤ 1, and therefore, Tc =\nΓ 2 can\nbe used to upper-bound the true critical temperature. When unaries are non-zero, there is no closed form solution for Tc, however, from Equation 47, we can show that the smaller the unaries (U ), the lower the critical temperature will be. This is intuitively justified in Fig. 6.\nThe authors of [37], use several heuristics which basically consist in looking for high correlations and low unaries directly in the potentials of the graphical model, in order to find good variables to clamp. We, instead use a criterium based on the critical temperature in order to spot these.\nB.2 Experimental analysis We use the dense CRF implementation of [23] to verify the phase transition experimentally for Γ = 10. In our experiments, we used the three following settings, which range from the stylised example used for calculation to real semantic segmentation problems:\n• Model 1: We use a uniform rgb image drgb = 0. Two classes without unary potentials. This is exactly the model used for the derivations with θrgb = 1 and U = 0.\n• Model 2: Gaussian potentials defined over image coordinates distance + RGB distance. Two classes without unary potentials. In other words, θrgb ≤ 1.\n• Model 3: Gaussian potentials defined over image coordinates distance + RGB distance. Two classes with unary potentials produced by a CNN. This is a reallife scenario.\nFig. 7 shows that, as expected, two regimes appear for Model 1, before and after T = 5. We see that our prediction remains completely valid for Model 2, some nonuniform regions fall under the regime θrgb ≤ 1 and therefore the 10 % highest entropy percentile transitions slightly earlier. For Model 3, however, we see that the minimal and average entropy remain low even for T > 5. This is well explained by the fact that large regions of the image receive strong unary potentials from one class or the other, and therefore fall under the case ”with unaries” of Fig. 6 where the U parameter cannot be ignored. However, some uncertain regions receive unary potentials of same value for both labels, and therefore undergo a phase transition as predicted by our calculation. That is why the maximal entropy behaves similarly to Model 2. Our algorithm precisely targets these uncertain regions.\nInterestingly, we see that in practice, the users of DenseCRF choose the Γ and T parameters in order to be in a Multi-Modal regime, but close to the phase transition. For instance in the public releases of [9] and [41], the Gaussian kernel is set with T = 1 and Γ = 3."
    }, {
      "heading" : "C K-Shortest Path algorithm for the Multi-Modal Probabilistic Occupancy Maps",
      "text" : "We present here the algorithm we use to reconstruct tracks from the Multi-Modal Probabilistic Occupancy Maps (MMPOMs) of Section 7.2.\nKSP In the original algorithm of [5], the POMs computed at successive instants were used to produce consistent trajectories using the a K-Shortest Path (KSP) algorithm [33]. This involves building a graph in which each ground location at each\ntime step corresponds to a node and neighboring locations at consecutive time steps are connected. KSP then finds a set of node-disjoint shortest paths in this graph where the cost of going through a location is proportional to the negative log-probability of the location in the POM [5]. The KSP problem can be solved in linear time and an efficient implementation is available online.\nKSP for Multi-Modal POM Since MMMF produces multiple POMs, one for each mode, at each time-step, we duplicate the KSP graph nodes, once for each mode as well. Each node is then connected to each copy of neighboring locations from previous and following time steps. We then solve a multiple shortest-path problem in this new graph, with the additional constraint that at each time step all the paths have to go through copies of the nodes corresponding to the same mode. This larger problem is\nNP-Hard and cannot be solved by a polynomial algorithm such as KSP. We therefore use the Gurobi Mixed-Integer Linear Program solver [16].\nMore precisely, let us assume that we have a sequence of Multi-Modal POMs Qtk and mode probabilities m t k for t ∈ {1, . . . , T} representing time-steps and k ∈ {1, . . . ,K} representing different modes. Each Qtk is materialized through a vector of probabilities of presence qtk,i, where each i ≤ N is indexes a location on the tracking grid.\nUsing the grid topology, we define a neighborhood around each variable, which corresponds to the maximal distance a walking person can make on a grid in one time step. Let us denote by Ni the set of indices corresponding to locations in the neighbourhood of i. The topology is fixed and hence Ni does not depend on the time steps. We define the following log-likelihood costs.\nUsing a Log-Likelihood penalty, we define the following costs:\n• Ctk,i = log ( 1− qtk,i qtk,i ) , representing the cost of going through variable i at time\nt if mode k is chosen. • Ctk = log (\n1−mtk mtk\n) , representing the cost of choosing mode k at time t.\nWe solve for an optimization problem involving the following variables:\n• xtk,i,l,j is a binary flow variable that should be 1 if a person was located in i at t and moved to j at t+ 1, while modes k and l were respectively chosen at time t and t+ 1.\n• ytk is a binary variable that indicates whether mode k is selected at time t.\nWe can then rewrite the Multi-Modal K-Shortest Path problem as the following program, were we always assume that t ≤ T stands for a time step, k ≤ K and l ≤ K stand for mode indices, and i ≤ N and j ≤ N stand for grid locations:\nmin ∑ t,k Ctky t k + ∑ t,k,l≤K ∑ i,j∈Ni Ctk,ix t k,i,l,j\ns.t. ∀(t, k, i) , ∑\nl,j∈Ni\nxt−1l,j,k,i = ∑\nl,j∈Ni\nxtk,i,l,j flow conservation\n∀(t, k, i) , ∑\nl,j∈Ni\nxtk,i,l,j ≤ ytk disjoint paths + selected mode\n∀t , ∑ k ytk = 1 selecting one mode ∀t, k, i, l, j , 0 ≤ xtk,i,l,j ≤ 1 ∀t, k , ytk ∈ {0, 1}\n(49)\nKSP prunning However, the problem as written above, may involve several tens millions of flow variables and therefore becomes intractable, even for the best MILP solvers. We therefore first prune the graph to drastically reduce its size.\nThe obvious strategy would be by thresholding the POMs and removing all the outgoing and incoming edges from locations which have probabilities below qthresh. However, this would be self-defeating as one of the main strengths of the KSP formulation is to be very robust to missing-detections and be able to reconstruct a track even if a detection is completely lost for several frames.\nWe therefore resort to a different strategy. More precisely, we initially relax the constraint disjoint paths + selected mode, to a simple disjoint path constraint, and remove the constraint selecting one mode. We therefore obtain a relaxed problem\nmin ∑ t,k ∑ t,k,l≤K ∑ i,j∈Ni Ctk,ix t k,i,l,j\ns.t. ∀(t, k, i) , ∑\nl,j∈Ni\nxt−1l,j,k,i = ∑\nl,j∈Ni\nxtk,i,l,j flow conservation\n∀(t, k, i) , ∑\nl,j∈Ni\nxtk,i,l,j ≤ 1 disjoint paths\n∀t, k, i, l, j , 0 ≤ xtk,i,l,j ≤ 1 (50)\nwhich is nothing but a vanilla K-Shortest Path Problem. It can be solved using our linear-time KSP algorithm. This KSP problem will output a very large number of paths, going through all the different modes simultaneously. From, this output, we extract the set of grid locations which are used, in any mode, at each time step, and select them as our potential locations in the final program. In our current implementation, we add to\nthese locations, the ones for which qtk,i ≥ qthresh for any mode at time-step t. We can finally solve Program 49, where non-selected locations are pruned from the flow graph. We don’t know if our strategy, based on a relaxation and pruning, provides a guaranteed optimal solution to 49, but this is an interesting question."
    }, {
      "heading" : "D Pseudo-code for the Multi-Modal Mean-Fields algorithm",
      "text" : "Algorithm D summarises the operations to split one mode into two, or, in other words, to obtain the two additional constraints which are used to define the two newly created subsets. Algorithm 2 summarises the operations to obtain the Multi-Modal Mean Field Distribution by constructing the whole Tree.\nIn Algorithm 2, ConstraintTree, is taken to be a Tree in the form of a list of constraints, one for each branching-point, or leaf,—except for the root—, in a breadth first order. The function pathto(nNode), returns the set of indices corresponding to the branching points on the path to the branching point, or leaf with index nNode, including index nNode itself.\nAlgorithm 1 Function:Split(ConstraintList) Input: E(x): An Energy function defined by a CRF; SolveMF(E,ConstraintList): A Mean Field solver with cardinality constraint.; Temperatures: A list of temperatures in increasing order; Hlow,Hhigh: Entropy thresholds for the phase transition. 0.3 and 0.6 here. C: A cardinality threshold Output: LeftConstraints: A triplet containing a list of variables, clamped to value, -C RightConstraints: A triplet containing a list of variables, clamped to value, C\nQT0 ← SolveMF(E) for T in Temperatures do\nQT ← SolveMF(E T ,ConstraintList) ilist ← [.] vlist ← [.] for index in 1 . . .len(Qt), v in labels do\nif 1[H(qTindex) > 0.6]1[H(q T0 index) < 0.3]1[q T0 index,v > 0.5] = 1 then\nilist.append(index),vlist.append(v) end if\nend for if len(ilist) > 0 then exit for loop\nend if end for LeftConstraints = ilist, vlist,−C RightConstraints = ilist, vlist, C return LeftConstraints,RightConstraints\nAlgorithm 2 Compute Multi-Modal Mean Field Input: E(x): An Energy function defined on a CRF; SolveMF(E,ConstraintList): A Mean Field solver with cardinality constraint; Split(ConstraintList): Alg. D. A function that computes the new constraints. NModes: A target for the number of modes in the Multi-Modal Mean Field Output: Qlist: A list of Mean Field distributions in the form of a table of marginals mlist: A list of probabilities, one for each mode\nConstraintTree = [.] We first build the tree by adding constraints. while nNode < NModes do ConstraintList = [.] for p in pathto(nNode) do ConstraintList.append(ConstraintTree[p])\nend for LeftConstraints,RightConstraints← Split(ConstraintList) ConstraintTree.append(LeftConstraints) ConstraintTree.append(RightConstraints)\nend while We now turn to the computation of on MF distribution per leaf. Qlist = [.], Zlist = [.],mlist = [.] for mode in 0 . . . NModes do ConstraintList = [.] for p in pathto(mode+NModes− 1) do ConstraintList.append(ConstraintTree[p])\nend for Q,Z← SolveMF(E,ConstraintList) Qlist.append(Q) Zlist.append(Z)\nend for Finally, we compute the mode probabilities. for mode in 0 . . . NModes do mlist.append(\nZlist[mode]∑ Zlist )\nend for return Qlist, mlist"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "<lb>Mean Field inference is central to statistical physics. It has attracted much in-<lb>terest in the Computer Vision community to efficiently solve problems expressible<lb>in terms of large Conditional Random Fields. However, since it models the pos-<lb>terior probability distribution as a product of marginal probabilities, it may fail to<lb>properly account for important dependencies between variables.<lb>We therefore replace the fully factorized distribution of Mean Field by a weighted<lb>mixture of such distributions, that similarly minimizes the KL-Divergence to the<lb>true posterior. By introducing two new ideas, namely, conditioning on groups of<lb>variables instead of single ones and using a parameter of the conditional random<lb>field potentials, that we identify to the temperature in the sense of statistical physics<lb>to select such groups, we can perform this minimization efficiently. Our extension<lb>of the clamping method proposed in previous works allows us to both produce a<lb>more descriptive approximation of the true posterior and, inspired by the diverse<lb>MAP paradigms, fit a mixture of Mean Field approximations. We demonstrate that<lb>this positively impacts real-world algorithms that initially relied on mean fields.",
    "creator" : "LaTeX with hyperref package"
  }
}