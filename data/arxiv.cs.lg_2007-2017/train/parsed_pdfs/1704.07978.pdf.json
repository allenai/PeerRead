{
  "name" : "1704.07978.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Improving Deep Reinforcement Learning for POMDPs",
    "authors" : [ "Pengfei Zhu", "Xin Li", "Pascal Poupart" ],
    "emails" : [ "pengfei0408@163.com,", "xinli@bit.edu.cn,", "ppoupart@uwaterloo.ca" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Research in automated learning techniques that interact with the environment span a period of almost 50 years. Such form of learning is often referred to as reinforcement learning. In the past 3 decades, many reinforcement learning techniques based on fully and partially observable Markov decision processes (MDPs) have been successfully applied to a variety of problems including spoken dialog management [Levin et al., 2000], robotic control [Simmons and Koenig, 1995] and automated assistance in health care [Boger et al., 2005]. However, many of those applications were small scale. In contrast, the environment of many real-world applications is best modeled with a large number of states, which induces the “curse of dimensionality” haunting many reinforcement learning techniques. During the last decade, researchers improved the scalability of reinforcement learning approaches, however further advances are needed. In 2015, the success of Google’s AlphaGo caught the attention of many researchers and the public by demonstrating the potential of deep reinforcement learning [Silver et al., 2016]. Deep reinforcement learning combines reinforcement learning and deep learning, and has shown great success in solving a number of very challenging\ntasks that can be easily modeled as conventional reinforcement learning problems, but cannot be solved by conventional reinforcement learning approaches due to the high dimensionality of the state space, e.g., continuous control [Lillicrap et al., 2015], high-dimensional robot control [Stadie et al., 2015], and Atari Learning Environment benchmarks (ALE) [Bellemare et al., 2013].\nSpecifically, Deep Q-Networks (DQNs) can be used to effectively and efficiently play many Atari 2600 games [Mnih et al., 2015]. The idea is to extract convolved features of the frames as states and to approximate the Q-function over the states with a deep neural network. For instance, a DQN is trained to estimate the expected value of a policy based on the the past four frames as input in Atari 2600 games. For most Atari 2600 games, four frames is sufficient to approximate the current state of the game. Generally speaking, the Atari games are treated as MDP problems, and DQNs are used to approximate the value functions of these MDPs. However, for some other real-world tasks including some Atari 2600 games, the problem is really a partially observable Markov decision process (POMDP) where the state of the environment may be partially observable or even unobservable, to the point where arbitrarily long histories of observations are needed to extract sufficient features for optimal action selection. Unfortunately, DQN is not suitable for those problems due to the assumption of complete observability of the state.\nWhile POMDPs can naturally model planning tasks with uncertain action effects and partial state observability, finding an optimal policy is notoriously difficult. Since the state of the environment is not directly observable, a common approach consists of maintaining a distribution over the underlying state, known as a belief. The belief can be revised after\nar X\niv :1\n70 4.\n07 97\n8v 2\n[ cs\n.L G\n] 8\nM ay\n2 01\n7\neach action and observation according to Bayes’ theorem. In POMDPs, beliefs are equivalent to states in MDPs. However since the space of beliefs is infinite and continuous, many previous POMDP techniques focus on identifying a finite subset of beliefs that are sufficient to approximate all reachable beliefs in order to decrease computational complexity [Kurniawati et al., 2008; Li et al., 2007].\nDeep neural networks have the capability of extracting high-level features from raw sensory data, like raw image pixels, and have strong generalization abilities. Recent advances in deep learning suggest a new way of thinking for solving POMDP problems. However, very little work leverages deep reinforcement learning in partially observable environments. Among this work, [Egorov, 2015] adopted DQNs to solve conventional POMDP problems. A policy is obtained with a DQN that maps concatenated observation-belief vector pairs to an optimal action. Their work (we call it DBQN) is designed for model-based representations of the environment where the transition, observation and reward functions are already known. Thus, the belief can be estimated precisely with Bayes’ theorem and can serve as input to the neural network. However, in most real-world POMDP problems, the environment dynamics are unknown. To address this, [Hausknecht and Stone, 2015] adapted the fully connected structure of DQN with a recurrent network [Hochreiter and Schmidhuber, 1997], and called the new architecture Deep Recurrent Q-Network (DRQN). The proposed model recurrently integrates arbitrarily long histories of observations to find an optimal policy that is robust to partial observability. However, DRQNs consider only observation histories without explicitly including actions as part of the histories. This impacts negatively the performance of the approach as demonstrated in Sec. 4. [Lample and Chaplot, 2016] combined DRQN with handcrafted features to jointly supervise the learning process of 3D games in partially observable environments, however the approach suffers from the same problem as DRQN since it overlooks action histories. [Foerster et al., 2016] extended DRQN to handle partially observable multi-agent reinforcement learning problems by proposing a deep distributed recurrent Q-networks (DDRQN). The action history is explicitly processed by an LSTM layer and fed as input to a Q-network. In DDRQN, each action is forcibly decoupled from its associated observation despite the fact that action-observation pairs are the key to belief updating. As a result, the decoupling of actions and observations in DDRQN impacts negatively belief inference.\nIn this paper, we propose a new architecture called Actionbased Deep Recurrent Q-Network (ADRQN) to improve learning performance in partially observable domains. Actions are encoded via a fully connected layer and coupled with their associated observations to form action-observation pairs. The time series of action-observation pairs is processed by an LSTM layer that learns latent states based on which a fully connected layer computes Q-values as in conventional DQNs. We demonstrate the effectiveness of our new architecture in several Atari 2600 games. Table 1 summarizes the main differences between ADRQN and other state-of-the-art deep Q-learning techniques."
    }, {
      "heading" : "2 Background",
      "text" : "In this section, we give a brief review of Deep Q-Networks (DQNs), Partially Observable Markov Decision Processes (POMDPs) and Deep Recurrent Q-networks (DRQNs)."
    }, {
      "heading" : "2.1 Deep Q-Networks",
      "text" : "A sequential decision problem with known environment dynamics is usually formalized as a Markov Decision Process (MDP), which is characterized by a 4-tuple 〈S,A, P,R〉. At each step, an agent selects an action at ∈ A to execute with respect to its fully observable current state st ∈ S and based on its policy π. It receives an immediate reward rt ∼ R(st, at) and transitions to a new state st+1. The objective of reinforcement learning is to find the policy that maximizes the expected discounted rewards Rt\nRt = rt + γrt+1 + γ 2rt+2 + · · · (1)\nwhere γ ∈ [0, 1] is the discount factor. In MDPs, an optimal policy can be computed by value iteration [Sutton and Barto, 1998].\nQ-Learning [Watkins and Dayan, 1992] was proposed as a model-free technique for reinforcement learning problems with unknown dynamics. It estimates the value of executing an action in a given state followed by an optimal policy. This value is called the state-action value, or simply Q-value as defined below:\nQπ(s, a) = E(Rt|st = s, at = a) (2)\nThe Q-values can be learned iteratively according to the following rule while the agent is interacting with the environment:\nQ(s, a) = Q(s, a) + α(r + γmax a′\nQ(s′, a′)−Q(s, a)) (3)\nIn tasks with a large number of states, a common trick is to use a function approximator to estimate the Q-function. For instance, DQN [Mnih et al., 2015] uses a neural network parameterized by θ to represent Q(s, a; θ). Neural networks with at least one (non-linear) hidden layer and sufficiently many nodes can approximate any function arbitrarily closely. DQN is optimized by minimizing the following loss function:\nLi(θi) = Es,a,r,s′ [( (ytargeti −Q(s, a; θi) )2]\n(4)\nwhere ytargeti = r + γmax a′ Q(s′, a′|θ−i ) denotes the target value of the action at given state st. Here θ−i is cloned from θi every fixed numbers of iterations. DQN uses experience replay [Lin, 1993] to store previous samples et = (st, at, rt, st+1) up to a fixed size memory Dt. The Qnetwork is then trained by uniformly sampling mini-batches of past experiences from the replay memory. An important factor for the efficiency of DQN in AlphaGo and the Atari games is the assumption of full state observability that allows the neural network to use only one (or a few) observation(s) as input. Thus, DQN suffers inaccuracy in tasks with partially observable states."
    }, {
      "heading" : "2.2 Partially Observable Markov Decision Processes (POMDPs)",
      "text" : "POMDPs generalize MDPs for planning under partial observability. A POMDP is mathematically defined as a tuple 〈S,A,Z, T,O,R〉, consisting of a finite set of states S, a finite set of actionsA, a transition function T : S×A → Π(S), a reward function depending on the state and the action just performed R : S × A → R, a finite set of observations Z and an observation function O : S × A → Π(Z). As the true states are not fully observable any more, a belief is used to estimate the current state, defined as the probability mass function over the states and denoted as b = (b(s1), b(s2), ...b(s|S|)), where si ∈ S, b(si) ≥ 0, and∑ si∈S b(si) = 1. Given the current belief b\nt, the next belief bt+1 = SE(bt, a, z) is estimated as follows:\nbt+1(sj) = O(sj , a, z)\n∑ si∈S T (si, a, sj)b t(si)\nP (z|a, bt) P (z|a, bt) = ∑ sj∈S O(sj , a, z) ∑ si∈S T (si, a, sj)b t(si) (5)\nThe expected immediate reward for an agent performing action a at the belief state b is computed as ρ(b, a) =∑ si∈S b(si)R(si, a). The transition function among beliefs\nbecomes τ(b, a, b′) = ∑ z∈Z p(b\n′|b, a, z)P (z|b, a) where p(b′|b, a, z) = 1 if b′ = SE(b, a, z), and 0 otherwise. An optimal policy π : R|S| → A can be computed by value iteration:\nV (b) = max a [ρ(b, a) + γ ∑ b′ τ(b, a, b′)V (b′)] (6)\nwhere γ is a discount factor for the past history. In practice, it is common to have the optimal policy represented by a set of linear functions (so called α-vectors) over the belief space, with the maximum “envelop” of the α-vectors forming the value function [Shani et al., 2013]."
    }, {
      "heading" : "2.3 DRQN",
      "text" : "DQN assumes that the agent has full knowledge of the state of the environment, i.e., the agent’s observation is equivalent to the state of environment. However, in practice, this is rarely true, even in some Atari games. For example, one frame of Pong does not reveal the ball’s velocity and its moving direction as shown in Fig. 1(a). In the game of Asteroids (Fig. 1(b)), parts of the image is concealed for several consecutive frames to challenge the player. Hence, these games are naturally POMDP problems. While the strategy of DQN is to\nutilize several consecutive frames as the input in the hope of deducing the full state, this works well only when a short limited history is sufficient to characterize the state of the game. For more complex games, the performance of DQN decreases sharply.\nTo tackle problems with partially observable environments by deep reinforcement learning, [Hausknecht and Stone, 2015] proposed a framework called Deep Recurrent QLearning (DRQN) in which an LSTM layer was used to replace the first post-convolutional fully-connected layer of the conventional DQN. The recurrent structure is able to integrate an arbitrarily long history to better estimate the current state instead of utilizing a fixed-length history as in DQNs. Thus, DRQNs estimate the function Q(ot, ht−1|Θ) instead of Q(st, at|Θ), where Θ denotes the parameters of entire network, ht−1 denotes the output of the LSTM layer at the previous step, i.e., ht = LSTM(ht−1, ot). DRQN matches DQN’s performance on standard MDP problems and outperforms DQN in partially observable domains. Regarding the training process, DRQN only considers the convolutional features of the history of observations instead of explicitly incorporating the actions. However as we argued in Sec. 1 and Eq. 5 in Sec. 2.2, the action performed is crucial for belief estimation. To that effect, we propose a new architecture for deep reinforcement learning with actions incorporated in histories to further improve the performance of deep RL in POMDPs. In the sequel, we will refer to our proposed architecture as ADRQN (action-specific deep recurrent Q-learning Network)."
    }, {
      "heading" : "3 ADRQN - Action-specific Deep Recurrent Q-Network",
      "text" : "Inspired by the aforementioned works, our goal is to propose a model-free deep RL approach that incorporates the influence of the performed action through time. More specifically, we couple the performed action and the obtained observation as the input to the Q-network. The architecture of our model is shown in Fig. 2.\nOur first attempt to couple the action and observation was to concatenate a fixed representation of the action with the observation vector directly, i.e., we utilized one-hot vectors to represent each action a ∈ A. However, such concatenation did not yield good performance because the lengths of the action and observation vectors differ largely, which lead to numerical instability. In Atari games as well as many real-world POMDPs, the number of actions is far less than the dimensionality of the state representation. In conventional DQNs\nand DRQNs, the suggested dimensionality of the convolved features encoding the current state or observation is 3136 (after passing several convolutional and reshaping layers), while the number of actions is only 18 for the Atari games. To address this imbalance, we embed the one-hot action vectors by a fully connected layer into a higher dimensional vector (512- D is our experiments). Thus the representations of the actions and the observations are now more balanced.\nCompared with DQN and DRQN, our model is able to remember the past actions, particularly the last performed action. Thus, we modified the transition (st, at, rt, st+1) in the experience replay mechanism of the conventional DQN to 〈{at−1, ot}, at, rt, ot+1〉 in order to allow the framework to fetch the action-observation pair more conveniently. During the decision process for a given frame within training or the updating process of the neural network, the LSTM layer requires a sequence of action-observation pairs as its input. Thus, we store the transitions sequentially 〈{at−1, ot}, at, rt, ot+1〉 within each episode in the replay memory.\nIdeally, the best strategy to estimate precisely the current state for a model-free POMDP problem is to integrate the entire transition history of each episode, which usually includes thousands of transitions for an Atari game. This also means that the LSTM layer needs to be unrolled for a large number of time steps which will increase significantly the training cost. In our experiments, the LSTM layer is unrolled for 10 time steps during training.\nThe entire process of our proposed approach is presented in Algorithm 1. First, we initialize the parameters of the Qnetwork and the Target network with θ and θ− respectively. For each episode, the first selected action is initialized to nooperation, the first hidden layer’s input is initialized with a\nzero vector and the first observation of each episode is initialized with the preprocessed first frame. At each time step, if the observation does not indicate “game over” (the end of the episode), we select an action based on the -greedy strategy and execute the action. Accordingly, the immediate reward and the next observation of the screen will be obtained. The transition, once obtained, will be sequentially stored in the history of the current episode. To update the Q-network, we randomly sample a sequence of transitions 〈aj−1, oj , aj , rj , oj+1〉 to fit the unrolled LSTM layer. Then, the hidden layer hj−1 of the Q-network and the hidden layer hj of the target network are obtained. The difference between these two network Q-values (i.e., Q-value yj and Q-network value Q(hj−1, aj−1, oj , aj ; θ)) is used as the loss function to update the network parameters θ via back propagation."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we evaluate the training and testing performance of ADRQN with several Atari games and their flickering version."
    }, {
      "heading" : "4.1 Experiments setup",
      "text" : "• Flickering Atari game: [Hausknecht and Stone, 2015]\nintroduced a flickering version of the Atari games, which modified the Atari games by obscuring the entire screen with a certain probability at each time step, which introduces partial observability and therefore yields a POMDP. In their framework, before a frame is sent to the neural network as input, each raw screen is either fully observable or fully obscured with black pixels. • Frame skip Scheme. We adopted the frame skip tech-\nnique [Bellemare et al., 2013]. This mechanism is commonly used in most previous works of deep reinforce-\nAlgorithm 1 ADRQN 1: Initialize the replay memory D, # of iterations M 2: Initialize Q-Network and Target-Network with θ and θ−\nrespectively 3: for episode = 1 to M do 4: Initialize the first action a0 = no operation, 5: h0 = 0 6: Initialize the first observation o1 with the 7: preprocessed first screen 8: while ot 6= terminal do 9: select a random action at with the probability\n10: else select at = argmax a Q(ht−1, at−1, ot, a; θ) 11: execute action at 12: obtain reward rt and resulting observation ot+1 13: store transition 〈{at−1, ot}, at, rt, ot+1〉 as one 14: record of the current episode in D 15: randomly sample a minibatch of transition 16: sequences 〈aj−1, oj , aj , rj , oj+1〉 from D 17: compute Q-value of target network\nyj= { rj , oj+1= terminal rj+γmax\na Q(hj , aj , oj+1, a; θ\n−), oj+1 6= terminal\n18: compute the gradient of 19: (yj −Q(hj−1, aj−1, oj , aj ; θ))2 to update θ 20: end while 21: end for\nment learning to efficiently simulate the environment. In this mechanism, an agent performs the selected action ai for k + 1 consecutive frames and treats the transition from the first frame f0 to frame fk+1 as the effect of action ai, i.e. 〈f0, ai, f1, ..., ai, fk+1〉=〈f0, ai, fk+1〉. Thus, a large number of frames can be skipped to accelerate the training process, but with a tolerable performance loss. In our experiments, k is set to 4. • Hyperparameters. In our experiments, we also adopt\nexperience replay and set the replay memory size to D = 400, 000 (i.e., storing 400,000 transitions). When selecting an action, we follow the −greedy policy with = 1 − 0.9∗iterexplore , where iter is the current number of iterations performed and explore is the number of iterations that epsilon reaches to a given value. In our setting, will reach 0.1 and explore was set to 1, 000, 000. The discount factor γ was set to 0.99. The target network is updated by cloning the weights of the Q-network every 10, 000 iterations. • Random Updates. As suggested in [Hausknecht and\nStone, 2015], random updates can achieve the similar performance as conventional sequential updates of the entire episode, but with much lower training cost. Random updates consist of selecting randomly a series of transitions from one episode as the input. In our framework, this corresponds to utilizing a sequence of actionobservation pairs to perform the updates. The initial hidden input h0 of the RNN is set to the zero vector at the beginning of the update."
    }, {
      "heading" : "4.2 Training",
      "text" : "Compared to the other approaches, the key idea of our framework is to construct action-observation pairs as input to the LSTM layer to retain more representative features for the Qnetwork to learn recurrently. The actions are first encoded with one-hot vectors,\nthen processed by a fully connected layer to construct a higher-dimensional vector that is concatenated with the output of the third convolutional layer for better numerical stability. As the LSTM layer will be unrolled for 10 time steps, we need to ensure that there are enough transitions to be stored in the replay memory D so that we can sample a minibatch of the transition sequences of length 10 each time we update the entire network. In our experiments, we update the entire network until the replay memory is full. Moreover, the scores obtained by playing the games are not always stable since small changes of the weights may have a significant impact on the outcome [Mnih et al., 2013]. Thus, we adopt an adaptive setting as done in most previous works by replacing negative rewards by -1 and positive rewards by +1.\nWhen training the flickering versions of Atari games, we set the probability of obscuring a frame to 0.5 as a compromise. A lower probability may prevent learning due to a large loss of information, and a higher probability may be less convincing that the transformed version is a POMDP. Besides, it is a fair setting to evaluate generalization performance since it divides the test interval into two subintervals evenly. The games Pong and Chopper Command are both trained under the setting of full observation and a 0.5 probability of obscuring a frame. The training performances of Pong and Chopper Command are shown in Fig. 3 and 4 respectively. The reported scores are based on averages of 10 and 100 episodes respectively.\nFig. 3 shows that ADRQN matches the performance obtained by DRQN and DDRQN, and even performs slightly better than DRQN and DDRQN in the conventional setting (full observation), while the improvements of ADRQN over DRQN and DDRQN in partially observable settings are very obvious. Additionally, although the results shown in Fig. 4 indicate that all of the three models can not obtain significant improvements when the frames are always observed (over the partially observable case) in Chopper Command, we can still observe that ADRQN performs well when the environment is partially observable. In general, our proposed model can lead to higher scores in partially observable settings during the training process, which supports the argument that actionobservation pairs are important in POMDP problems."
    }, {
      "heading" : "4.3 Testing Evaluation",
      "text" : "We also evaluate the well-learned DRQN and ADRQN models on the three games and their flickering version. We replay each game based on the learned model 10 times to obtain the average scores as our final results. The epsilon value used in testing evaluation for −greedy is set to 0.05, which is less than that used in the training process since we consider the model to be well trained. Table 2 summarizes the results obtained with full observations. ADRQN outperforms DRQN in general. While Table 3 demonstrates that ADRQN significantly outperforms DRQN when trained with half of the\nframes obscured, an interesting observation is that the testing results are generally better than the results obtained during the training process for both DRQN and ADRQN. This may be explained by the fact that different distribution of the concealed frames may render the problem more or less observable, to the extent that better results may be obtained in some cases."
    }, {
      "heading" : "4.4 Generalization Performance",
      "text" : "To further demonstrate the advantages of ADRQN in dealing with environmental changes, we evaluate the generalization performance of DRQN and ADRQN on the flicking versions of the games. POMDP to MDP Generalization: After being trained with observation probability of 0.5, we test the learned policy in settings with observation probabilities (0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0). The results are shown in Fig. 5. We observe that both DRQN and ADRQN consistently perform better with the increase of observation probability. When the observable probability reaches 1.0, the results obtained are close to the performance obtained by training with full observations, which further demonstrates that both models can effectively respond to environmental changes. ADRQN consistently outperforms DRQN. MDP to POMDP Generalization: After being trained as an MDP by setting the observation probability to 1, we test\nthe learned policy with different settings of the observation probability. As shown in Fig. 6, both the performances achieved by DRQN and ADRQN drop sharply as we increase the amount of missing information. Fig. 6(a) clearly demonstrates that ADRQN is more robust than DRQN."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose an action-specific deep recurrent Q-Network (ADRQN) to enhance the learning performance in partially observable domains. The actions are encoded via a fully connected layer and coupled with convolutional observations to form action-observation pairs. The time series of action-observation pairs are then integrated by an LSTM layer to infer the latent states, based on which Q-values are computed by a fully connected layer similar to conventional DQNs. We demonstrated the effectiveness of the approach in several POMDP problems, including flickering Atari games."
    } ],
    "references" : [ {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "Marc G. Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling" ],
      "venue" : "J. Artif. Intell. Res. (JAIR), 47:253–279,",
      "citeRegEx" : "Bellemare et al.. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A decision-theoretic approach to task assistance for persons with dementia",
      "author" : [ "Boger et al", "2005] Jennifer Boger", "Pascal Poupart", "Jesse Hoey", "Craig Boutilier", "Geoff Fernie", "Alex Mihailidis" ],
      "venue" : "Proceedings of the Nineteenth International Joint Conference on Artificial Intelli-",
      "citeRegEx" : "al. et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2005
    }, {
      "title" : "CoRR",
      "author" : [ "Jakob N. Foerster", "Yannis M. Assael", "Nando de Freitas", "Shimon Whiteson. Learning to communicate to solve riddles with deep distributed recurrent q-networks" ],
      "venue" : "abs/1602.02672,",
      "citeRegEx" : "Foerster et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "CoRR",
      "author" : [ "Matthew J. Hausknecht", "Peter Stone. Deep recurrent q-learning for partially observable mdps" ],
      "venue" : "abs/1507.06527,",
      "citeRegEx" : "Hausknecht and Stone. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Neural Computation",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber. Long short-term memory" ],
      "venue" : "9(8):1735–1780,",
      "citeRegEx" : "Hochreiter and Schmidhuber. 1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "SARSOP: efficient point-based POMDP planning by approximating optimally reachable belief spaces",
      "author" : [ "Kurniawati et al", "2008] Hanna Kurniawati", "David Hsu", "Wee Sun Lee" ],
      "venue" : "In Robotics: Science and Systems IV,",
      "citeRegEx" : "al. et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2008
    }, {
      "title" : "CoRR",
      "author" : [ "Guillaume Lample", "Devendra Singh Chaplot. Playing FPS games with deep reinforcement learning" ],
      "venue" : "abs/1609.05521,",
      "citeRegEx" : "Lample and Chaplot. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Speech and Audio Processing",
      "author" : [ "Esther Levin", "Roberto Pieraccini", "Wieland Eckert. A stochastic model of human-machine interaction for learning dialog strategies. IEEE Trans" ],
      "venue" : "8(1):11–23,",
      "citeRegEx" : "Levin et al.. 2000",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A novel orthogonal nmf-based belief compression for pomdps",
      "author" : [ "Li et al", "2007] Xin Li", "William Kwok-Wai Cheung", "Jiming Liu", "Zhili Wu" ],
      "venue" : "In Machine Learning, Proceedings of the Twenty-Fourth International Conference (ICML",
      "citeRegEx" : "al. et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2007
    }, {
      "title" : "CoRR",
      "author" : [ "Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra. Continuous control with deep reinforcement learning" ],
      "venue" : "abs/1509.02971,",
      "citeRegEx" : "Lillicrap et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Technical report",
      "author" : [ "Long-Ji Lin. Reinforcement learning for robots using neural networks" ],
      "venue" : "DTIC Document,",
      "citeRegEx" : "Lin. 1993",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "CoRR",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin A. Riedmiller. Playing atari with deep reinforcement learning" ],
      "venue" : "abs/1312.5602,",
      "citeRegEx" : "Mnih et al.. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Mnih et al", "2015] Volodymyr Mnih", "Kavukcuoglu Koray", "Silver David", "Rusu Andrei A", "Veness Joel", "Bellemare Marc G", "Graves Alex", "Riedmiller Martin", "Fidjeland Andreas K", "Ostrovski Georg" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Autonomous Agents and Multi-Agent Systems",
      "author" : [ "Guy Shani", "Joelle Pineau", "Robert Kaplow. A survey of point-based pomdp solvers" ],
      "venue" : "pages 1–51,",
      "citeRegEx" : "Shani et al.. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree search",
      "author" : [ "Silver et al", "2016] David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    }, {
      "title" : "Probabilistic robot navigation in partially observable environments",
      "author" : [ "Simmons", "Koenig", "1995] Reid G. Simmons", "Sven Koenig" ],
      "venue" : "In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Simmons et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Simmons et al\\.",
      "year" : 1995
    }, {
      "title" : "CoRR",
      "author" : [ "Bradly C. Stadie", "Sergey Levine", "Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models" ],
      "venue" : "abs/1507.00814,",
      "citeRegEx" : "Stadie et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto" ],
      "venue" : "IEEE Trans. Neural Networks, 9(5):1054–1054,",
      "citeRegEx" : "Sutton and Barto. 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Machine Learning",
      "author" : [ "Christopher J.C.H. Watkins", "Peter Dayan. Technical note q-learning" ],
      "venue" : "8:279–292,",
      "citeRegEx" : "Watkins and Dayan. 1992",
      "shortCiteRegEx" : null,
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "In the past 3 decades, many reinforcement learning techniques based on fully and partially observable Markov decision processes (MDPs) have been successfully applied to a variety of problems including spoken dialog management [Levin et al., 2000], robotic control [Simmons and Koenig, 1995] and automated assistance in health care [Boger et al.",
      "startOffset" : 226,
      "endOffset" : 246
    }, {
      "referenceID" : 9,
      "context" : ", continuous control [Lillicrap et al., 2015], high-dimensional robot control [Stadie et al.",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 16,
      "context" : ", 2015], high-dimensional robot control [Stadie et al., 2015], and Atari Learning Environment benchmarks (ALE) [Bellemare et al.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : ", 2015], and Atari Learning Environment benchmarks (ALE) [Bellemare et al., 2013].",
      "startOffset" : 57,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "To address this, [Hausknecht and Stone, 2015] adapted the fully connected structure of DQN with a recurrent network [Hochreiter and Schmidhuber, 1997], and called the new architecture Deep Recurrent Q-Network (DRQN).",
      "startOffset" : 17,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "To address this, [Hausknecht and Stone, 2015] adapted the fully connected structure of DQN with a recurrent network [Hochreiter and Schmidhuber, 1997], and called the new architecture Deep Recurrent Q-Network (DRQN).",
      "startOffset" : 116,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : "[Lample and Chaplot, 2016] combined DRQN with handcrafted features to jointly supervise the learning process of 3D games in partially observable environments, however the approach suffers from the same problem as DRQN since it overlooks action histories.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "[Foerster et al., 2016] extended DRQN to handle partially observable multi-agent reinforcement learning problems by proposing a deep distributed recurrent Q-networks (DDRQN).",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : "In MDPs, an optimal policy can be computed by value iteration [Sutton and Barto, 1998].",
      "startOffset" : 62,
      "endOffset" : 86
    }, {
      "referenceID" : 18,
      "context" : "Q-Learning [Watkins and Dayan, 1992] was proposed as a model-free technique for reinforcement learning problems with unknown dynamics.",
      "startOffset" : 11,
      "endOffset" : 36
    }, {
      "referenceID" : 10,
      "context" : "DQN uses experience replay [Lin, 1993] to store previous samples et = (st, at, rt, st+1) up to a fixed size memory Dt.",
      "startOffset" : 27,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : "In practice, it is common to have the optimal policy represented by a set of linear functions (so called α-vectors) over the belief space, with the maximum “envelop” of the α-vectors forming the value function [Shani et al., 2013].",
      "startOffset" : 210,
      "endOffset" : 230
    }, {
      "referenceID" : 3,
      "context" : "To tackle problems with partially observable environments by deep reinforcement learning, [Hausknecht and Stone, 2015] proposed a framework called Deep Recurrent QLearning (DRQN) in which an LSTM layer was used to replace the first post-convolutional fully-connected layer of the conventional DQN.",
      "startOffset" : 90,
      "endOffset" : 118
    }, {
      "referenceID" : 3,
      "context" : "• Flickering Atari game: [Hausknecht and Stone, 2015] introduced a flickering version of the Atari games, which modified the Atari games by obscuring the entire screen with a certain probability at each time step, which introduces partial observability and therefore yields a POMDP.",
      "startOffset" : 25,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "We adopted the frame skip technique [Bellemare et al., 2013].",
      "startOffset" : 36,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "As suggested in [Hausknecht and Stone, 2015], random updates can achieve the similar performance as conventional sequential updates of the entire episode, but with much lower training cost.",
      "startOffset" : 16,
      "endOffset" : 44
    }, {
      "referenceID" : 11,
      "context" : "Moreover, the scores obtained by playing the games are not always stable since small changes of the weights may have a significant impact on the outcome [Mnih et al., 2013].",
      "startOffset" : 153,
      "endOffset" : 172
    } ],
    "year" : 2017,
    "abstractText" : "Deep Reinforcement Learning (RL) recently emerged as one of the most competitive approaches for learning in sequential decision making problems with fully observable environments, e.g., computer Go. However, very little work has been done in deep RL to handle partially observable environments. We propose a new architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to enhance learning performance in partially observable domains. Actions are encoded by a fully connected layer and coupled with a convolutional observation to form an action-observation pair. The time series of action-observation pairs are then integrated by an LSTM layer that learns latent states based on which a fully connected layer computes Q-values as in conventional Deep QNetworks (DQNs). We demonstrate the effectiveness of our new architecture in several partially observable domains, including flickering Atari games.",
    "creator" : "LaTeX with hyperref package"
  }
}