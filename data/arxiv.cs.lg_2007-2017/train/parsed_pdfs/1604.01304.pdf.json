{
  "name" : "1604.01304.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Towards Label Imbalance in Multi-label Classification with Many Labels",
    "authors" : [ "Li Li", "Houfeng Wang" ],
    "emails" : [ "wanghf}@pku.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Multi-label classification is supervised learning, where an instance may be associated with multiple labels simultaneously. Multi-label classification attracted increasing attention from various domains (Vens et al., 2008; Nicolas et al., 2013; Sun et al., 2014; Li et al., 2014) in these years. Due to several motivating real-life applications, such as image/video annotation (Weston et al., 2011; Kong et al., 2012) and query/keyword suggestions (Agrawal et al., 2013), the recent research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large (Chen and Lin, 2012; Agrawal et al., 2013; Bi and Kwok, 2013; Lin et al., 2014).\nMulti-label classification with many labels encounters the scalability challenge: how to design\nscalable algorithms that offer fast training procedures and have a small memory footprint. The standard multi-label classification approaches are computationally infeasible, when the number of labels is extremely large. For example, the simplest standard multi-label classification approach Binary Relevance (BR) is not applicable for a multi-label classification problem with 104 labels. BR trains a classifier for each label so that it need train 104 classifiers. The high training time complexity makes it computationally infeasible. BR is not applicable, not to mention the more sophisticated and computationally demanding approaches. There exists some works for multilabel classification with many labels. The mainstream approaches are called Label Space Dimension Reduction (LSDR) (Hsu et al., 2009; Tai and Lin, 2012; Chen and Lin, 2012; Lin et al., 2014; Bi and Kwok, 2013). LSDR encodes the high-dimensional label vectors into low dimensional code vectors. Then predictive models are trained from instances to code vectors. To predict an unseen instance, a low-dimensional code vector is firstly obtained with the predictive models, and then be decoded for the label vector. Besides LSDR, there are another approaches with different style, and we call them RepresentationBased Learning (RBL) (Weston et al., 2011; Yu et al., 2014; Rai et al., 2015) approaches. RBL learns representations for the instances and labels, and produces the predictions with these representations.\nHowever the above-mentioned approaches ignore and even compound an important problem: the label imbalance problem. The label imbalance problem is that the irrelevant labels of an instance are much more than relevant labels, and that some labels are irrelevant to more instances than other labels. As the papers (Spyromitros-Xioufis, 2011; Charte et al., 2013; Zhang et al., 2015) pointed out, the label imbalance problem exists in\nar X\niv :1\n60 4.\n01 30\n4v 1\n[ cs\n.L G\n] 5\nA pr\n2 01\n6\nthe standard multi-label classification, and harms the performance. The label imbalance problem becomes more serious in multi-label classification with many labels. Because more labels are irrelevant to an instance when the number of labels is large. To show the phenomenon, we can use the imbalance ratio defined in (Zhang et al., 2015) to evaluate the label imbalance degree. For a label, the imbalance ratio is the ratio of the number of irrelevant instances to the number of relevant instances.\nImRj = num of irrelevant instances\nnum of relevant instances\nImR = 1\nm m∑ j=1 ImRj\nwhere ImRj denotes the imbalance ratio for the j-th label, ImR denotes the average of the imbalance ratios. The high imbalance ratio indicates the serious label imbalance problem. The Enron dataset (Goldstein et al., 2006) has 45 labels and its average imbalance ratio is 3. 34. TheEurlex desc dataset (Mencia and Fürnkranz, 2008) has 3993 labels and its average imbalance ratio is 1,378.58, much larger than the Enron dataset’s. The label imbalance problem in the Eurlex desc dataset is more serious than that in the Enron dataset. Hence we need attach more importance to the label imbalance problem in multi-label classification with many labels. However the existing approaches ignore the label imbalance problem. Even LSDR compounds this problem. The labels with very little relevant instances contain little information. So the lossy compression in LSDR may consider these labels as noisy and drop information about them.\nTo address this drawback, we propose a novel Representation-based Multi-label Learning with Sampling (RMLS) approach, which can tackles the label imbalance problem in multi-label classification with many labels. To the best of our knowledge, we are the first to tackle the imbalance problem in multi-label classification with many labels. RMLS is a RBL approach and employs a representation learning framework with a sampling strategy."
    }, {
      "heading" : "2 Related Works",
      "text" : ""
    }, {
      "heading" : "2.1 Multi-label Classification with Many Labels",
      "text" : "We categorize the existing approaches for multilabel classification with many labels into two types: Label Space Dimension Reduction (LSDR) and Representation-Based Learning (RBL). Figure 1 (it is from (Lin et al., 2014)) is the schematic diagram of LSDR. LSDR encodes the high-dimensional label vectors into low dimensional code vectors. Then predictive models are trained from instances to code vectors. To predict an unseen instance, a low-dimensional code vector is firstly obtained with the learnt predictive models, and then be decoded for the label vector.\nCompressive Sensing (CS) (Hsu et al., 2009) is the first LSDR approach. Specifically, CS linearly encodes the original label space as compressed sensing and uses standard recovery algorithms for decoding. Principle Label Space Transformation (PLST) (Tai and Lin, 2012) performs PCA on the label matrix Y to get the compressing matrix V\nV ∗ = argmax V TV =I Tr(V TY TY V ) (1)\nUsing the compressing matrix, we can obtain code vector c = yV . CS and PLST aim to find the compressing matrix with high recoverability. However they don’t consider the predictability of the code vector. With high predictability, it will is easy to train the model to predict the code vector. Conditional Principal Label Space Transformation (CPLST) (Chen and Lin, 2012) considers the predictability, and optimizes the following problem to\nget the compressing matrix.\nV ∗ = argmax V TV =I Tr(V TY TXX+Y V ) (2)\nCPLST argues that the compressing matrix obtained by this way can balances the predictability with recoverability. Feature-aware Implicit label space Encoding (FaIE) (Lin et al., 2014) balances predictability with recoverability, and optimize the following problem.\nV ∗ = argmax V TV =I Tr(V T (Y Y T + αXX+))V ) (3)\nwhere α denotes a parameter specified by users. Column Subset Selection for Multi-Label (CSS ML) (Bi and Kwok, 2013) seeks to select exactly k representative labels so as to span all labels as much as possible. Then CSS ML learns k classifiers for these selected labels. For unseen instance, CSS ML predicts k selected labels and spans the predictions for all labels. CSS ML can be considered as a special LSDR approach.\nRBL learns representations for instances and labels, and produces the predictions with these representations. Web Scale Annotation by Image Embedding (WSABIE) (Weston et al., 2011) trains the representation model by minimizing the Weighted Approximate-Rank Pairwise (WARP) loss function. Low rank Empirical risk minimization for Multi-Label Learning (LEML) (Yu et al., 2014) develops a fast optimization scheme for the representation model with different loss functions, and analyses the representation model’s generalization error. Bayesian Multi-label Learning via Positive Labels (BMLPL) (Rai et al., 2015) uses the topic model to represent instance, and learns the model with only relevant labels"
    }, {
      "heading" : "2.2 Label Imbalance Problem",
      "text" : "The label imbalance problem has attracted some attention from the multi-label classification community. One solution to label-imbalance multilabel learning is to train a classifier for a label and deal with the skewness in each classifier via popular binary imbalance techniques such as random or synthetic undersampling/oversampling (Spyromitros-Xioufis, 2011; Tahir et al., 2012; Charte et al., 2013; Charte et al., 2015). The paper (Zhang et al., 2015) improves this approach by aggregating one binary-class imbalance learner corresponding to the current label and several multi-class imbalance learners coupling with other\nlabels for prediction. Besides integrating binary decomposition, Petterson et al (Petterson and Caetano, 2010) and Dembczynski et al (Dembczynski et al., 2013) address the label imbalance problem by directly optimizing imbalance-specific metric.\nAll of the above-mentioned approach solve the label imbalance problem by incorporating more correlations or designing more complex algorithms. These approaches are so complex that they are only applicable to the multi-label learning with the number of labels assumed to be small. In this paper, we aim to addressing the label imbalance problem in multi-label classification with many labels."
    }, {
      "heading" : "3 Models",
      "text" : ""
    }, {
      "heading" : "3.1 Preliminaries",
      "text" : "Let X denote the instance feature space, and Y = {0, 1}m denote label space with m labels. A instance x ∈ X is associated with a label vector y = (y1, y2, ..., ym), where yj = 1 denotes the j-th label is relevant to the instance and yj = 0 otherwise. The goal of multi-label learning is to learn a function f : X → Y . In general, the function f consists of m functions, one for a label, i.e., f(x) = [f1(x), f2(x), ..., fm(x)], where f j(x) is the prediction of the relevance between the instance x and the j-th label."
    }, {
      "heading" : "3.2 Representation Learning",
      "text" : "The architecture of RMLS is as shown in figure 2. The feature vectorx is mapped to a low-dimension feature representation vector h with a mapping matrixW\nh = θ(xW ) (4)\nwhere θ is an activation function. Each label corresponds a low-dimension label representation vector, denoted by l1, l2, ..., lm. The dimension of the\nfeature representation vector and the label representation vector are identical. The prediction for the j-th label, denoted by f j(x), is produced by the inner dot of the feature representation vector h and the j-th label representation vector lj . We add an activation function σ to the inner dot, for example, the logistic function.\nf j(x) = σ(hT lj) (5)\nwhere σ is an activation function. our family of models have constrained norms:\n||W ||2F ≤ C1 ||lj ||22 ≤ C2, j = 1, ...,m (6)\nThe constrained norms acts as a regularizer in the same way as is used in lasso (Tibshirani, 1996)\nIf we set the dimension of the feature representation vector to be k, the number of parameters mapping an instance x to an feature representation vector h is d × k, the number of parameters of all label representation vectors is k × m. So the total number of parameters of RMLS is d × k + k × m. The simplest standard multilabel classification model BR trains m classifiers. If the classifier in BR is a linear model with d parameters, the total number of parameters of BR is d × m. Generally speaking, k is much less than min(d,m), so that the number of parameters of RMLS is much less than that of BR. The less parameters mean less training cost.\nBoth of LSDR and RBL reduce the number of parameters by this way. LSDR encodes the label vectors into the code vectors and generates a recovery matrix Rk×m 1. Then LSDR learns regression models Gd×k mapping from the instances to the code vectors. The total number of parameters of LSDR is d × k + k × m. RBL learns a model W d×k mapping instances to lowdimension instance representation vectors. The number of parameters of label representation vectors is k × m. The total number of parameters of RBL is d × k + k × m too. The number of parameters of LSDR and RBL are the same, since the architecture of them are identical. For an unseen instance x, LSDR produces the code vectors with c = xGd×k, and then produces the prediction p with p = cGd×k = xGd×kRk×m.\n1In this paragraph, we show the size of matrixes with subscripts\nRBL produces the instance representation vector with h = xW d×k. If we treat the label representation vectors as the columns of a label matrix Lk×m, and set the activation function to the linear function, the prediction is produced with p = hLk×m = xW d×kLk×m. Hence the linear regression modelGd×k in LSDR is the equal of the mapping matrix W d×k in RBL, and the recovery matrix Rk×m in LSDR is equal of the label matrixLk×m in RBL. The difference between LSDR and RBL is how to obtain the parameters. LDSR obtains parameters by linear algebra approaches, and RBL learns the model by gradient descent approaches. RMLS is not the first RBL approach, however we are the first to point out that LSDR and RBL have identical architectures and the same number of parameters."
    }, {
      "heading" : "3.3 Sampling Strategy",
      "text" : "Our goal is to identify relevant labels from irrelevant labels. To obtain this goal, we minimize a loss function over the training set to get the model parameters W and lj . The loss function L is as shown in the following formula.\nL = m∑ j=1 `(f j(x), yj) (7)\n` denotes the classification loss function. Different classification loss functions can be used as `, for example, cross entropy loss, least square loss and L2 hinge loss.\nWith the serious label imbalance problem, the cost of classifying relevant labels as irrelevant is higher than that of classifying irrelevant labels as relevant. Incorporating this consideration into the loss function, the loss function becomes as follows.\nL = ∑ j∈P `(f j(x), yj) + 1 C ∑ j∈N `(f j(x), yj) (8)\nWhereP = {j|yj = 1} is the set of relevant labels andN = {j|yj = 0} is the set of irrelevant labels.\nWith the loss function, the overall risk we want to minimize is\nR(f ) = ∫ ∑ j∈P `(f j(x), yj) + 1 C ∑ j∈N `(f j(x), yj)dp(x,y) (9)\nAn unbiased estimator of this risk can be obtained by stochastically sampling |N |C irrelevant labels with the uniform distribution, and minimizing the\nloss function over the relevant labels and the chosen irrelevant labels. Then the loss function becomes as follows.\nL = ∑ j∈P `(f j(x), yj) + ∑ j∈S `(f j(x), yj) (10)\nWhere S denotes the set of the chosen irrelevant labels. Minimization the formula 10 approximates to obtain the minimizer of risk 9.\nWe think, an instance with more relevant labels contains more information so that C in the formula 8 should be less. The number of an instance’s relevant labels is denoted by |P |, and the number of an instance’s irrelevant labels is denoted by |N |. We set C = 1α |N | |P | and get the number of the chosen irrelevant labels |S | = α × |P |. We sample α × |P | irrelevant labels with the uniform distribution, where α is the sampling coefficient. The sampling coefficient α is an important parameter specified by the user, and we suggest to set it to be 5.\nOur family of models have constrained norm so that the `2 norm is added to the minimization objective. The final minimization problem becomes as follows."
    }, {
      "heading" : "W ,l = argmin",
      "text" : "W ,l { ∑ j∈P `(f j(x), yj) + ∑ j∈S `(f j(x), yj)\n+λ||W ||2F + λ m∑ j=1 ||lj ||22}\n|S | = α× |P | (11)\nwhere λ denotes the regularization coefficient."
    }, {
      "heading" : "3.4 Training Our Models",
      "text" : "The mini-batch Stochastic Gradient Descent (SGD) is performed to the above-mentioned optimization problem. We use the Adagrad (Duchi et al., 2011) to adapt the learning rate.\nThe sampling labels may be biased and unstable. The straightforward approach to this problem is to train different models with different sampling results and employ the ensemble strategy. However, it is very expensive to train different models for the large scale multi-label classification with many labels. A practical solution to this problem is to sample different labels in every batch of minibatch SGD. Let B denote the index of the labelled training data in a batch, the pseudocode for training RLML with a batch of labelled data is given in Algorithm 1.\nAlgorithm 1 Mini-batch SGD with a batch of labelled data Input: (xi, y i) where i ∈ B , α\ndataset n d m Enron 1702 1001 53 Delicious 16105 500 983 Eurlex desc 19348 5000 3993 Wiki 28596 23495 50341"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We perform experiments on four real world datasets. These datasets are available online 2. To reduce the time cost, we only use the accessible labelled training part of the Wiki dataset and select the labels with at least 5 relevant instances. Table 1 shows these multi-label datasets and associated statistics where n denotes the number of instances, d denotes the number of features, m denotes the number of labels."
    }, {
      "heading" : "4.2 Evaluation Criteria",
      "text" : "Compared with the single-label classification, the multi-label setting introduces the additional degrees of freedom, so that we need various multilabel evaluation metrics. We use three common evaluation metrics in our experiments. Let p denotes the prediction vector. The Hammingloss is defined as the percentage of the wrong labels to the total number of labels.\nHammingloss = 1\nm |p∆y| (12)\n2http://mulan.sourceforge.net/ datasets.html and http://mlkd.csd.auth. gr/multilabel.html and https://www.kaggle. com/c/lshtc/data\nwhere ∆ denotes the symmetric difference of two sets, equivalent to XOR operator in Boolean logic.\nLet pi and ri denote the precision and recall for the i-th instance, which means that pi =\n|pi ⋂ yi|\n|p|\nand that ri = |p\n⋂ yi|\n|yi| . The Fscore is defined as follows.\nFscore = 1\nn n∑ i=1 2piri pi + ri\n(13)\nThe Fscore is a harmonic mean between precision and recall, and the higher F score means the better performance.\nThe Accuracy in multi-label classification is the size of the intersection of predicted label set p and true label set y divided by the size of the union of this two set. The Accuracy in multi-label classification is defined as follows:\nAccuracy = |y ∩ p| |y ∪ p|\n(14)"
    }, {
      "heading" : "4.3 Experimentation Results",
      "text" : ""
    }, {
      "heading" : "4.3.1 Performance Comparison",
      "text" : "We compare RMLS to some state-of-the-art appoaches and a baseline approach.\n- Principle Label Space Transformation (PLST) (Tai and Lin, 2012). PLST performs PCA on the label matrix to get the compressing matrix.\n- Feature-aware Implicit label space Encoding (FaiE) (Lin et al., 2014). FaiE balances predictability with recoverability.\n- Column Subset Selection for Multi-Label (CSS ML) (Bi and Kwok, 2013). CSS ML seeks to select exactly k representative labels so as to span all labels as much as possible.\n- Web Scale Annotation by Image Embedding (WSABIE) (Weston et al., 2011). WSABIE trains the representation model by minimizing the Weighted Approximate-Rank Pairwise (WARP) loss function.\n- Low rank Empirical risk minimization for Multi-Label Learning (LEML) (Yu et al., 2014). LEML develops a fast optimization scheme for the representation model with different loss functions.\n- Baseline. The baseline classifies all labels as irrelevant labels.\nPLST, FaiE and ML CSSP are the LSDR approaches. We use the open-source code mlc lsdr 3 for them. The project mlc lsdr is developed by the author of PLST and CPLST. In experiments, we use mlc lsdr’s default settings. WSABIE, LEML and our RMLS are the RBL approaches. We implement the code for WSABIE and LEML. When implementing LEML, we replace the gradient optimization scheme by the least square minimization scheme. Since the least square minimization is more effective in the linear model. For our RMLS, we set the sampling coefficient α to 5, as suggested above. The regularization coefficients for WSABIE, LEML and our RMLS are set to 0.001. The dimension of the latent vectors k (the dimension of the code vectors in LSDR and the dimension of the representation vectors in RBL) is an important parameter. We perform all algorithms on the Enron dataset with k = 25 and k = 50, and other datasets with k = 250 and k = 500. The experiments are done in five-fold cross validation.\nTable 2, table 3 and table 4 show detail comparison results and we can draw two conclusions: 1) RMLS shows clear majorities of winning over the state-of-the-art approaches in terms of Fscore and Accuracy, which demonstrates its effectiveness. In terms of Hammingloss, RMLS doesn’t show superiorities. However, the winner in terms of Hammingloss is the baseline, which predicts all labels as irrelevant labels. This implies that Hammingloss is not a reasonable evaluation criteria for multi-label classification with the label imbalance, just like the predictive accuracy isn’t a good evaluation criteria for imbalance classification. 2) RBL approaches outperform LSDR approaches. The reason for it may be that the LSDR approaches make assumptions about compressing label space, and that RBL approaches learn the label representations without making any assumptions."
    }, {
      "heading" : "4.3.2 Time Cost",
      "text" : "We also record the training time of each approach in table 5. We have some conclusions about the training time: 1) The training time of RBL approaches on small datasets (Enron, Delicious, Eurlex desc) are similar. But WSABIE and LEML spend much more time training on the Wiki dataset than RMLS. Because the\n3https://github.com/hsuantien/mlc_lsdr\nsampling scheme in RMLS will reduce the time cost dramatically, when the number of labels is very large. 2) PLST and FaiE spend little time training on the small datasets (Enron,Delicious, Eurlex desc). But they run out of the memory and consume too much time on the Wiki dataset, since both of them perform a partial SVD on the dense 50k × 50k matrix. 3) ML CSSP is the only LSDR approach that is applicable on the Wiki dataset. But the performance is very poor."
    }, {
      "heading" : "4.3.3 Influence of the Sampling Ratio",
      "text" : "To examine the influence of the sampling ratio , i.e., the parameterα, we run RMLS withα varying from 1 to 10 with step size of 1. Due to the page limit, we only report results on the Eurlex desc dataset, whereas experiments on other datasets get similar results. The detail results are shown in thefigure 3\nThe Fscore and Accuracy are poor when the sampling ratio is small. As the sampling ratio grows large, these two evaluation criteria go up first and then down. When the sampling ratio is small, too many irrelevant labels are dropped, resulting the poor performance. When the sampling ratio is large, the number of irrelevant labels is much larger than the number of relevant labels, the label imbalance problem results in the poor performance. This implies that the sampling scheme with the proper sampling ratio α can handle the label imbalance problem and improve the performance.\nThe Hammingloss goes down when the sampling ratio grows up. When the sampling ratio is large, we achieve good performance in terms of Hammingloss with the serious label imbalance problem. The reason for it may be that Hammingloss is not a reasonable evaluation criteria, which has been uncovered in the performance comparison experiments."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In multi-label classification, an instance is associated with with a set of labels simultaneously. Recently, the researchers on multi-label classification focused on the multi-label learning with many labels. The existing approaches for multi-label learning with many labels ignore and even compound the label imbalance problem. To address this problem, we propose a novel Representation-based Multi-label Learning with Sampling (RMLS) approach. Our experimentations demonstrate the effectiveness of the proposed approach."
    } ],
    "references" : [ {
      "title" : "Multilabel learning with millions of labels: Recommending advertiser bid phrases for web pages",
      "author" : [ "Archit Gupta", "Yashoteja Prabhu", "Manik Varma" ],
      "venue" : "In Proceedings of the 22nd international conference on",
      "citeRegEx" : "Agrawal et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2013
    }, {
      "title" : "A first approach to deal with imbalance in multilabel datasets",
      "author" : [ "Antonio Rivera", "Marı́a José del Jesus", "Francisco Herrera" ],
      "venue" : "In Hybrid Artificial Intelligent Systems,",
      "citeRegEx" : "Charte et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Charte et al\\.",
      "year" : 2013
    }, {
      "title" : "Addressing imbalance in multilabel classification: Measures and random resampling algorithms",
      "author" : [ "Antonio J Rivera", "Marı́a J del Jesus", "Francisco Herrera" ],
      "venue" : null,
      "citeRegEx" : "Charte et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Charte et al\\.",
      "year" : 2015
    }, {
      "title" : "Feature-aware label space dimension reduction for multi-label classification",
      "author" : [ "Chen", "Lin2012] Yao-Nan Chen", "Hsuan-Tien Lin" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Chen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2012
    }, {
      "title" : "Optimizing the f-measure in multi-label classification: Plug-in rule approach versus structured loss",
      "author" : [ "Arkadiusz Jachnik", "Wojciech Kotlowski", "Willem Waegeman", "Eyke Hüllermeier" ],
      "venue" : null,
      "citeRegEx" : "Dembczynski et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dembczynski et al\\.",
      "year" : 2013
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Annotating subsets of the enron email corpus. In CEAS",
      "author" : [ "Andres Kwasinksi", "Paul Kingsbury", "Roberta Evans Sabin", "Albert McDowell" ],
      "venue" : null,
      "citeRegEx" : "Goldstein et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Goldstein et al\\.",
      "year" : 2006
    }, {
      "title" : "Multi-label prediction via compressed sensing",
      "author" : [ "Hsu et al.2009] Daniel Hsu", "Sham Kakade", "John Langford", "Tong Zhang" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Hsu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2009
    }, {
      "title" : "Multi-label relieff and f-statistic feature selections for image annotation",
      "author" : [ "Kong et al.2012] Deguang Kong", "Chris Ding", "Heng Huang", "Haifeng Zhao" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Kong et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2012
    }, {
      "title" : "Muli-label text categorization with hidden components",
      "author" : [ "Li Li", "Longkai Zhang", "Houfeng Wang" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-label classification via feature-aware implicit label space encoding",
      "author" : [ "Lin et al.2014] Zijia Lin", "Guiguang Ding", "Mingqing Hu", "Jianmin Wang" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient pairwise multilabel classification for large-scale problems in the legal domain",
      "author" : [ "Mencia", "Johannes Fürnkranz" ],
      "venue" : "In Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "Mencia et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mencia et al\\.",
      "year" : 2008
    }, {
      "title" : "Multi-label classification based on analog reasoning",
      "author" : [ "Andreu SanchoAsensio", "Elisabet Golobardes", "Albert Fornells", "Albert Orriols-Puig" ],
      "venue" : "Expert Systems with Applications,",
      "citeRegEx" : "Nicolas et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nicolas et al\\.",
      "year" : 2013
    }, {
      "title" : "Reverse multi-label learning",
      "author" : [ "Petterson", "Caetano2010] James Petterson", "Tibério S Caetano" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Petterson et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Petterson et al\\.",
      "year" : 2010
    }, {
      "title" : "Large-scale bayesian multi-label learning via topic-based label embeddings",
      "author" : [ "Rai et al.2015] Piyush Rai", "Changwei Hu", "Ricardo Henao", "Lawrence Carin" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Rai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rai et al\\.",
      "year" : 2015
    }, {
      "title" : "Multilabel image categorization with sparse factor representation",
      "author" : [ "Sun et al.2014] Fuming Sun", "Jinhui Tang", "Haojie Li", "Guo-Jun Qi", "Thomas S Huang" ],
      "venue" : "Image Processing, IEEE Transactions",
      "citeRegEx" : "Sun et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2014
    }, {
      "title" : "Inverse random under sampling for class imbalance problem and its application to multi-label classification",
      "author" : [ "Josef Kittler", "Fei Yan" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "Tahir et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tahir et al\\.",
      "year" : 2012
    }, {
      "title" : "Multilabel classification with principal label space transformation",
      "author" : [ "Tai", "Lin2012] Farbound Tai", "Hsuan-Tien Lin" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Tai et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2012
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Tibshirani.,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani.",
      "year" : 1996
    }, {
      "title" : "Decision trees for hierarchical multi-label classification",
      "author" : [ "Vens et al.2008] Celine Vens", "Jan Struyf", "Leander Schietgat", "Sašo Džeroski", "Hendrik Blockeel" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Vens et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vens et al\\.",
      "year" : 2008
    }, {
      "title" : "Wsabie: Scaling up to large vocabulary image annotation",
      "author" : [ "Weston et al.2011] Jason Weston", "Samy Bengio", "Nicolas Usunier" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Weston et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2011
    }, {
      "title" : "Large-scale multi-label learning with missing labels",
      "author" : [ "Yu et al.2014] Hsiang-fu Yu", "Prateek Jain", "Purushottam Kar", "Inderjit Dhillon" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Yu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards class-imbalance aware multi-label learning",
      "author" : [ "Zhang et al.2015] Min-Ling Zhang", "Yu-Kun Li", "Xu-Ying Liu" ],
      "venue" : "In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI’15)",
      "citeRegEx" : "Zhang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Multi-label classification attracted increasing attention from various domains (Vens et al., 2008; Nicolas et al., 2013; Sun et al., 2014; Li et al., 2014) in these years.",
      "startOffset" : 79,
      "endOffset" : 155
    }, {
      "referenceID" : 12,
      "context" : "Multi-label classification attracted increasing attention from various domains (Vens et al., 2008; Nicolas et al., 2013; Sun et al., 2014; Li et al., 2014) in these years.",
      "startOffset" : 79,
      "endOffset" : 155
    }, {
      "referenceID" : 15,
      "context" : "Multi-label classification attracted increasing attention from various domains (Vens et al., 2008; Nicolas et al., 2013; Sun et al., 2014; Li et al., 2014) in these years.",
      "startOffset" : 79,
      "endOffset" : 155
    }, {
      "referenceID" : 9,
      "context" : "Multi-label classification attracted increasing attention from various domains (Vens et al., 2008; Nicolas et al., 2013; Sun et al., 2014; Li et al., 2014) in these years.",
      "startOffset" : 79,
      "endOffset" : 155
    }, {
      "referenceID" : 20,
      "context" : "Due to several motivating real-life applications, such as image/video annotation (Weston et al., 2011; Kong et al., 2012) and query/keyword suggestions (Agrawal et al.",
      "startOffset" : 81,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : "Due to several motivating real-life applications, such as image/video annotation (Weston et al., 2011; Kong et al., 2012) and query/keyword suggestions (Agrawal et al.",
      "startOffset" : 81,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : ", 2012) and query/keyword suggestions (Agrawal et al., 2013), the recent research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large (Chen and Lin, 2012; Agrawal et al.",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : ", 2013), the recent research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large (Chen and Lin, 2012; Agrawal et al., 2013; Bi and Kwok, 2013; Lin et al., 2014).",
      "startOffset" : 182,
      "endOffset" : 261
    }, {
      "referenceID" : 10,
      "context" : ", 2013), the recent research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large (Chen and Lin, 2012; Agrawal et al., 2013; Bi and Kwok, 2013; Lin et al., 2014).",
      "startOffset" : 182,
      "endOffset" : 261
    }, {
      "referenceID" : 7,
      "context" : "The mainstream approaches are called Label Space Dimension Reduction (LSDR) (Hsu et al., 2009; Tai and Lin, 2012; Chen and Lin, 2012; Lin et al., 2014; Bi and Kwok, 2013).",
      "startOffset" : 76,
      "endOffset" : 170
    }, {
      "referenceID" : 10,
      "context" : "The mainstream approaches are called Label Space Dimension Reduction (LSDR) (Hsu et al., 2009; Tai and Lin, 2012; Chen and Lin, 2012; Lin et al., 2014; Bi and Kwok, 2013).",
      "startOffset" : 76,
      "endOffset" : 170
    }, {
      "referenceID" : 20,
      "context" : "Besides LSDR, there are another approaches with different style, and we call them RepresentationBased Learning (RBL) (Weston et al., 2011; Yu et al., 2014; Rai et al., 2015) approaches.",
      "startOffset" : 117,
      "endOffset" : 173
    }, {
      "referenceID" : 21,
      "context" : "Besides LSDR, there are another approaches with different style, and we call them RepresentationBased Learning (RBL) (Weston et al., 2011; Yu et al., 2014; Rai et al., 2015) approaches.",
      "startOffset" : 117,
      "endOffset" : 173
    }, {
      "referenceID" : 14,
      "context" : "Besides LSDR, there are another approaches with different style, and we call them RepresentationBased Learning (RBL) (Weston et al., 2011; Yu et al., 2014; Rai et al., 2015) approaches.",
      "startOffset" : 117,
      "endOffset" : 173
    }, {
      "referenceID" : 1,
      "context" : "As the papers (Spyromitros-Xioufis, 2011; Charte et al., 2013; Zhang et al., 2015) pointed out, the label imbalance problem exists in ar X iv :1 60 4.",
      "startOffset" : 14,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "As the papers (Spyromitros-Xioufis, 2011; Charte et al., 2013; Zhang et al., 2015) pointed out, the label imbalance problem exists in ar X iv :1 60 4.",
      "startOffset" : 14,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "To show the phenomenon, we can use the imbalance ratio defined in (Zhang et al., 2015) to evaluate the label imbalance degree.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "The Enron dataset (Goldstein et al., 2006) has 45 labels and its average imbalance ratio is 3.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "Figure 1 (it is from (Lin et al., 2014)) is the schematic diagram of LSDR.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "Compressive Sensing (CS) (Hsu et al., 2009) is the first LSDR approach.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "Feature-aware Implicit label space Encoding (FaIE) (Lin et al., 2014) balances predictability with recoverability, and optimize the following problem.",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 20,
      "context" : "Web Scale Annotation by Image Embedding (WSABIE) (Weston et al., 2011) trains the representation model by minimizing the Weighted Approximate-Rank Pairwise (WARP) loss function.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 21,
      "context" : "Low rank Empirical risk minimization for Multi-Label Learning (LEML) (Yu et al., 2014) develops a fast optimization scheme for the representation model with different loss functions, and analyses the representation model’s generalization error.",
      "startOffset" : 69,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "Bayesian Multi-label Learning via Positive Labels (BMLPL) (Rai et al., 2015) uses the topic model to represent instance, and learns the model with only relevant labels",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "One solution to label-imbalance multilabel learning is to train a classifier for a label and deal with the skewness in each classifier via popular binary imbalance techniques such as random or synthetic undersampling/oversampling (Spyromitros-Xioufis, 2011; Tahir et al., 2012; Charte et al., 2013; Charte et al., 2015).",
      "startOffset" : 230,
      "endOffset" : 319
    }, {
      "referenceID" : 1,
      "context" : "One solution to label-imbalance multilabel learning is to train a classifier for a label and deal with the skewness in each classifier via popular binary imbalance techniques such as random or synthetic undersampling/oversampling (Spyromitros-Xioufis, 2011; Tahir et al., 2012; Charte et al., 2013; Charte et al., 2015).",
      "startOffset" : 230,
      "endOffset" : 319
    }, {
      "referenceID" : 2,
      "context" : "One solution to label-imbalance multilabel learning is to train a classifier for a label and deal with the skewness in each classifier via popular binary imbalance techniques such as random or synthetic undersampling/oversampling (Spyromitros-Xioufis, 2011; Tahir et al., 2012; Charte et al., 2013; Charte et al., 2015).",
      "startOffset" : 230,
      "endOffset" : 319
    }, {
      "referenceID" : 22,
      "context" : "The paper (Zhang et al., 2015) improves this approach by aggregating one binary-class imbalance learner corresponding to the current label and several multi-class imbalance learners coupling with other Figure 2: An illustration of the representations learning framwork.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "Besides integrating binary decomposition, Petterson et al (Petterson and Caetano, 2010) and Dembczynski et al (Dembczynski et al., 2013) address the label imbalance problem by directly optimizing imbalance-specific metric.",
      "startOffset" : 110,
      "endOffset" : 136
    }, {
      "referenceID" : 18,
      "context" : "The constrained norms acts as a regularizer in the same way as is used in lasso (Tibshirani, 1996)",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "We use the Adagrad (Duchi et al., 2011) to adapt the learning rate.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : "- Feature-aware Implicit label space Encoding (FaiE) (Lin et al., 2014).",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 20,
      "context" : "- Web Scale Annotation by Image Embedding (WSABIE) (Weston et al., 2011).",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : "- Low rank Empirical risk minimization for Multi-Label Learning (LEML) (Yu et al., 2014).",
      "startOffset" : 71,
      "endOffset" : 88
    } ],
    "year" : 2016,
    "abstractText" : "In multi-label classification, an instance may be associated with a set of labels simultaneously. Recently, the research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large. The existing works focus on how to design scalable algorithms that offer fast training procedures and have a small memory footprint. However they ignore and even compound another challenge the label imbalance problem. To address this drawback, we propose a novel Representation-based Multilabel Learning with Sampling (RMLS) approach. To the best of our knowledge, we are the first to tackle the imbalance problem in multi-label classification with many labels. Our experimentations with realworld datasets demonstrate the effectiveness of the proposed approach.",
    "creator" : "LaTeX with hyperref package"
  }
}