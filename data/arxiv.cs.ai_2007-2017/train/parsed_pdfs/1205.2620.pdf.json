{
  "name" : "1205.2620.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Exact Structure Discovery in Bayesian Networks with Less Space",
    "authors" : [ "Pekka Parviainen" ],
    "emails" : [ "mikko.koivisto}@cs.helsinki.fi" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The fastest known exact algorithms for scorebased structure discovery in Bayesian networks on n nodes run in time and space 2nnO(1). The usage of these algorithms is limited to networks on at most around 25 nodes mainly due to the space requirement. Here, we study space–time tradeoffs for finding an optimal network structure. When little space is available, we apply the Gurevich– Shelah recurrence—originally proposed for the Hamiltonian path problem—and obtain time 22n−snO(1) in space 2snO(1) for any s = n/2, n/4, n/8, . . .; we assume the indegree of each node is bounded by a constant. For the more practical setting with moderate amounts of space, we present a novel scheme. It yields running time 2n(3/2)pnO(1) in space 2n(3/4)pnO(1) for any p = 0, 1, . . . , n/2; these bounds hold as long as the indegrees are at most 0.238n. Furthermore, the latter scheme allows easy and efficient parallelization beyond previous algorithms. We also explore empirically the potential of the presented techniques."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "There has been relatively recent interest in devising exact algorithms for score-based structure learning in Bayesian networks (Eaton and Murphy, 2007; Koivisto and Sood, 2004; Koivisto, 2006; Perrier et al., 2008; Silander and Myllymäki, 2006; Singh and Moore, 2005). The research is motivated not only by theoretical curiosity but also by applications: an exact algorithm— that is guaranteed to produce an optimal solution or the exact quantity of interest—allows the user to concentrate on modelling issues and direct interpretation of the learning results, with no (extra) uncertainty on\nthe quality of the algorithm’s output per se.\nThe fastest known exact algorithms for Bayesian networks on n nodes (i.e, attributes or variables) compute and store intermediate results for all the possible 2n node subsets, running in time and space 2nnO(1), assuming the score obeys certain usual modularity properties (Koivisto and Sood, 2004; Ott and Miyano, 2003; Silander and Myllymäki, 2006; Singh and Moore, 2005). While both the time and the space requirement become soon prohibitive as n gets larger, it is particularly the space requirement that determines the feasibility limit in practice. Indeed, on typical modern desktop computers with a few gigabytes of memory the algorithms can handle networks on up to around 25 nodes before they run out of space, the running time being still in only some minutes or hours. The current record of 29 nodes was achieved by the streamlined Silander–Myllymäki implementation (Silander and Myllymäki, 2006) using nearly 100 gigabytes of hard disk. For making exact algorithms practically feasible in larger networks, reducing the space complexity is the main concern.\nTo understand the nature of the algorithmic challenges at hand, it is useful to think the structure discovery problem as a “permutation problem”: one seeks a linear order of the n nodes that maximizes a sum of local scores, one per node; the local score for a node only depends on the set of nodes that precede it in the order (the potential endpoints of the incoming arcs). Note that here we restrict ourselves to the problem of finding an optimal network structure. For a number of similar permutation problems—like the Travelling Salesman problem, the Feedback Arcset problem, and Treewidth, to name a few—dynamic programming algorithms running in time and space 2nnO(1) have been known for decades (Bellman, 1962; Held and Karp, 1962; Lawler, 1964; Arnborg et al., 1987), with only negligible progress since; more recent work (Bodlaender et al., 2006) has shown that if only polynomial space is allowed, then many permutation problems can\nbe solved in time 4nnO(1) using a divide and conquer technique we outline in the next two paragraphs. We are not aware of any previous work on interpolating between the two extremes of space complexity.\nPerhaps the most straightforward approach to solve a permutation problem in space less than 2nnO(1) is to divide the node set into two buckets: N0 containing the first s nodes in the order, and N1 containing the remaining n−s nodes. Both N0 and N1 specify subproblems that can be solved by an exact algorithm in time and space 2snO(1) and 2n−snO(1), respectively; here we assume the indegrees of the network, i.e. the number of parents per node, are at most some constant. Assuming w.l.o.g. that s ≥ n/2, the algorithm that tries out all possible partitions {N0, N1} runs in time (\nn s\n)\n2snO(1) and space 2snO(1). For instance, putting s = 4/5n yields time O(2.872n) and space O(1.742n). This simple scheme is the starting point of the present study of time–space tradeoffs for exact structure discovery in Bayesian networks, in two respects.\nFirst (in Section 3), we notice that the above scheme with balanced partitioning (into about equally sized parts) and recursive application yields time 22n−snO(1) in space 2snO(1) for any s = n/2, n/4, n/8, . . .. In particular, we get a polynomial-space algorithm with running time 4nnO(1). This divide and conquer technique is known as the Gurevich–Shelah recurrence, originally presented for the Hamiltonian path problem (Gurevich and Shelah, 1987) and later applied also elsewhere (Björklund and Husfeldt, 2008; Bodlaender et al., 2006). Unfortunately, the Gurevich–Shelah recurrence falls short when one is allowed to use more space, say 24n/5nO(1).\nOur second, main contribution (in Section 4) addresses the practically more relevant range where the Gurevich–Shelah recurrence does not apply. By contrast, the above outlined simple scheme (unbalanced and without recursion) applies, but—perhaps somewhat surprisingly—turns out to be suboptimal. Indeed, we present a completely different approach, where the idea is to cover the linear orders by a class of suitably specified partial orders, namely, partial orders consisting of p node pairs. This pairwise scheme runs in time 2n(3/2)pnO(1) in space 2n(3/4)pnO(1) for any p = 0, 1, . . . , n/2. For instance, with p = n/2 we get time O(2.4495n) in space O(1.733n), thus improving upon the simple scheme at s = 4/5n in both time and space (cf. the calculated bounds above).\nSo far we have assumed that the indegrees are at most some constant k. A straightforward implementation of the ideas described above yields time bounds where the number of possible parent sets appears as a multiplicative factor (absorbed into nO(1)). Ideally, one\nwould have that term affecting the running time only additively. The issue becomes very significant in practice when k is large, and particularly so if the indegree is unbounded. With the Gurevich–Shelah recurrence we, unfortunately, have not found a way to reach this goal. In sharp contrast, our novel scheme is amenable to such an implementation. Specifically, we show in Section 5 that even if the maximum indegree k is let to grow linearly with the number of nodes (with a decent slope), the indegree bound k will play no role in the dominating part of the running time bound.\nFinally, the pairwise scheme provides a desirable feature not possessed by previous exact algorithms: easy and efficient parallelization. We note that while the Silander–Myllymäki implementation is easy to run in parallel on n processors, the time requirement per processor remains 2n, up to a polynomial factor (Silander and Myllymäki, 2006). The pairwise scheme goes beyond this. We show in Section 6 that the time requirement per processor decreases with p as 2n(3/4)p, thus yielding savings in both time and space, provided that sufficiently many (2p) processors are available.\nWhile the present work is mostly theoretical, we expect the techniques also to have practical significance. In Section 7 we present a preliminary empirical exploration of the potential of the pairwise scheme. With the current implementation we are not able to beat the record of 29 nodes, as parallelization remains to be implemented and certain key operations have not been optimized to the level of the Silander–Myllymäki implementation. However, we are currently working on a more streamlined implementation and expect networks of 34 nodes to be within reach, if employing some hundreds of processors in parallel."
    }, {
      "heading" : "2 PRELIMINARIES",
      "text" : "In this section, we first formulate the problem of structure discovery in Bayesian networks. Then we present one of the possible variants of existing exact algorithms. Finally, we tune the problem formulation to accomodate for limited space."
    }, {
      "heading" : "2.1 The Structure Discovery Problem",
      "text" : "A Bayesian network is a multivariate probability distribution that obeys a structural representation in terms of a directed acyclic graph (DAG) and a corresponding collection of univariate conditional probability distributions. For our purposes, it is crucial to treat the DAG, i.e., the network structure, explicitly, whereas the conditional probabilities will enter our formalism only implicitly.\nA DAG on a set N is an acyclic graph (N,A) with\nnode set N and arc set A. A node u is said to be a parent of v if the arc uv is in A. We denote by Av the set of parents of v. We associate the DAG with the edge set A when there is no ambiguity about the node set. Throughout the paper we denote the cardinality of N by n.\nThe problem of Bayesian network structure discovery is as follows. For each node v ∈ V and a possible parent set Av ⊆ N \\ {v} one specifies a local score fv(Av) that gauges the fit of a class of conditional probability distributions to a given data set on the involved nodes under some statistical principle (e.g., Bayesian, maximum likelihood, minimum description length). Given these local scores, the task is to find a DAG A that maximizes the sum of the local scores (Cooper and Herskovits, 1992; Heckerman et al., 1995),\nf(A) . = ∑\nv∈N\nfv(Av) .\nWe note that this formulation does not directly apply to the so-called Bayesian approach to structure discovery (Friedman and Koller, 2003; Koivisto and Sood, 2004)."
    }, {
      "heading" : "2.2 A Dynamic Programming Algorithm",
      "text" : "While the existing exact algorithms for finding an optimal network structure exhibit some variability in the details, the key ideas are the same. We now review one of the variants, an algorithm the computes the maximum score—an actual DAG that achieves the optimal score can then be constructed using standard techniques. The algorithm can be described in two phases.\nIn the first phase, it computes for each node v ∈ N and node subset Y ⊆ N \\ {v}, the maximum of the local scores over the subsets of Y , defined as\nf̂v(Y ) . = max\nX⊆Y fv(X) . (1)\nIn words, f̂v(Y ) is the maximum local score for v given that the parents of v must be selected from Y .\nIn the second phase, the algorithm effectively goes through all permutations of the nodes, however, tabulating only intermediate results for the sets of the first i nodes in the order, for i = 0, 1, . . . , n. Formally, we define g(∅) .= 0 and for nonempty subsets Y ⊆ N recursively:\ng(Y ) . = max\nv∈Y\n{ g(Y \\ {v}) + f̂v(Y \\ {v}) } . (2)\nIn words, g(Y ) is the maximum score over DAGs on the node subset Y , obtained by selecting a node v that, when being the last node in the order among the nodes\nin Y and thus having the possibility to have parents from Y \\ {v}, yields the largest score. In particular, g(N) gives the maximum score over all DAGs on N .\nThe straighforward computation of the values f̂v(Y ) requires 2|Y | steps for fixed v and Y , and hence n3n−1 steps in total. However, this can be significantly improved by dynamic programming, observed first in Ott and Miyano (2003):\nLemma 1 (Ott and Miyano 2003)\nf̂v(Y ) = max {\nfv(Y ),max u∈Y\nf̂v(Y \\ {u}) } .\nThis recurrence can be solved in O(n22n) steps.\nGiven the values f̂v(Y ), the recurrence for the values g(Y ) can then be computed in O(n2n) steps in a straightforward manner. Thus, the algorithm takes a total of O(n22n) steps, which is nearly optimal, since the input, as formulated above, may already consist of n2n−1 (real) numbers.\nThe space requirement of the above algorithm is O(n2n) due to the space requirement of storing f̂v(Y ) for each v and Y . This is possible to improve slightly to O( √ n2n) by noticing that the two phases can be merged into a single algorithm that visits every set Y only once, proceeding level-wise, that is, in increasing cardinality of Y . This amounts to a truly significant saving in the space usage, provided that the input, the local scores fv(Av), are given implicitly, that is, each score fv(Av), once needed, is computed from (polynomial-space) input data; we next elaborate on this issue."
    }, {
      "heading" : "2.3 Limited Space: The Setting",
      "text" : "To study the structure discovery problem with limited space usage, we need to be explicit about the input of the problem. To this end, it is convenient to let the local scores fv(Av), for any node v, be available only for parent sets Av that belong to a given family of possible parent sets, denoted as Fv; elsewhere we define fv(Av) . = −∞. Whether the families Fv are represented implicitly or explicitly affects the space requirement of the structure discovery problem: In the former case, we assume each local score is evaluated based on input data once needed in time and space polynomial in n. In the latter case, the local scores are treated as explicit input, taking already space Ω( ∑\nv |Fv|). We will present our results under both approaches.\nWe are particularly interested in families Fv that are downward closed, that is, closed with respect to inclusion: if Y ∈ Fv and X ⊆ Y , then X ∈ Fv. Important examples of such downward closed families are (a) the\nparent sets of cardinality at most k, for some fixed k, and (b) the parent sets that are contained in a given set of candidate parents (Perrier et al., 2008). A useful property of a downward closed family is that its members can be listed in essentially linear time. In fact, in Section 5 we will make use of the following slightly stronger observation; the proof (omitted) is, e.g., by visiting the members in increasing order of cardinality or in lexicographic order.\nProposition 2 Given a set N , a downward closed family F ⊆ 2N , and sets X and Y with X ⊆ Y ⊆ N , the members Z ∈ F with X ⊆ Z ⊆ Y can be listed in time |F||N |O(1)."
    }, {
      "heading" : "3 A DIVIDE & CONQUER SCHEME",
      "text" : "We next consider in more detail the partitioning based approach outlined in the Introduction.\nLet Â be an optimal DAG on the node set N . Fix an integer s with n/2 ≤ s ≤ n. Since Â is acyclic there exists a partition of N into two sets N0 and N1 of size s and n−s, respectively, such that every arc between N0 and N1 in Â is directed from N0 to N1; in other words, the parents of any node in N0 are from N0, while a node in N1 may have parents also from N0. That said, one can find Â—strictly speaking, the associated score f(Â)—by trying out all possible partitions {N0, N1} and solving the recurrences\ng0(Y ) . = max\nv∈Y\n{ g0(Y \\ {v}) + f̂v(Y \\ {v}) } ,\nfor ∅ ⊂ Y ⊆ N0 with g0(∅) = 0, and\ng1(Y ) . = max\nv∈Y\n{ g1(Y \\ {v}) + f̂v(N0 ∪ Y \\ {v}) } ,\nfor ∅ ⊂ Y ⊆ N1 with g1(∅) = 0; the score of Â is obtained as the maximum of g0(N0) + g1(N1) over all partitions {N0, N1}. We notice that the two subproblems are independent of each other given the partition {N0, N1}, and thus can be solved separately. Applying the exact algorithm given in the previous section, the computation of g0 takes time and space 2\nsnO(1). Computing g1 can be more expensive, since evaluating the term f̂v(N0 ∪ Y \\ {v}) requires the consideration of all the 2s possible subsets of N0 as parents of v, in addition to a subset from Y \\ {v}. To simplify the analysis, we assume that the number of possible parent sets, |Fv|, is polynomial in n, in which case g1 can be computed in time and space 2n−snO(1). Because there are (\nn s\n)\npossible partitions, we have the following result.\nProposition 3 The structure discovery problem in Bayesian networks can be solved in time (\nn s\n)\n2snO(1)\nin space 2snO(1) for any s = n/2, n/2 + 1, . . . , n, provided that each node has nO(1) possible, predetermined parent sets.\nThe above approach yields a smooth time–space tradeoff for space bounds between 2n/2nO(1) and 2nnO(1). However, this is all but the end of the story: Firstly, within this range a more efficient scheme exists, as we will show in the next section. Secondly, with space less than 2n/2nO(1) the above scheme is not applicable, unless executed recursively, as we show next.\nTo solve the subproblems, namely computing g0(N0) and g1(N1), with less space we may apply the partitioning technique again. The problem of computing g0(N0) being of the same form as the original problem, let us turn to look at the problem of computing g1(N1). As before, we see that there exists a partitioning of the node set N1 into subsets N10 and N11 such that every arc between N10 and N11 in the optimal DAG Â is directed from N10 to N11. So, one can compute g1(N1) by trying out all possible partitions {N10, N11} and solving the recurrences\ng10(Y ) . = max\nv∈Y\n{ g10(Y \\{v}) + f̂v((N0 ∪ Y \\ {v}) } ,\nfor ∅ ⊂ Y ⊆ N10 with g10(∅) = 0, and\ng11(Y ) . = max\nv∈Y\n{ g11(Y \\{v}) + f̂v(N0 ∪N10 ∪Y \\{v}) } ,\nfor ∅ ⊂ Y ⊆ N11 with g11(∅) = 0; the score g1(N1) is obtained as the maximum of g10(N10)+ g11(N11) over all partitions {N10, N11}. In general, one can apply partitioning recursively, say d times, and then solve the remaining subproblems by dynamic programming. For an analysis of the time and space requirements, it is convenient to assume a balanced scheme: in every step of the recursion, the node set in question is partitioned into two sets of about equal sizes; for simplicity, assume n is a power of 2. Then, at depth d ≥ 0 of the recurrence, the node set in each subproblem in question is of size s . = n/2d; hence, each subproblem can be solved in time and space 2snO(1). Because each subproblem of size 2s is divided into 2 (\n2s s\n)\n≤ 22s subproblems of size s, the total number of subproblems of size s that need to be solved is at most 2n2n/22n/4 · · · 22s = 22n−2s.\nTheorem 4 The structure discovery problem in Bayesian networks can be solved in time 22n−snO(1) in space 2snO(1) for any s = n/2, n/4, n/8, . . ., provided that each node has nO(1) possible, predetermined parent sets.\nAs a theoretically interesting special case we have the following.\nCorollary 5 The structure discovery problem in Bayesian networks can be solved in time 4nnO(1) in space polynomial in n, provided that each node has nO(1) possible, predetermined parent sets."
    }, {
      "heading" : "4 PARTIAL ORDERS: THE PAIRWISE SCHEME",
      "text" : "We next present a scheme for trading space against time when the exponential term in the space complexity ranges between 2n and 2n/2.\nIn general terms, the idea is to fix a class of partial orders on the node set N such that any linear order on N can be realized as a linear extension of at least one of the partial orders in the class. Each partial order in the class corresponds to a restricted instance of the original task, which is to be solved by a suitably tailored variant of the basic dynamic programming algorithm. The bucket orders specified by the partitions of N into two subsets of fixed sizes, described in the previous section, is a simple example of such a class. Below we introduce a more “efficient” class.\nIn what follows, we focus on partial orders that are specified by p ordered pairs of nodes from N . More precisely, for a fixed integer p, with 0 ≤ p ≤ n/2, we pick arbitrarily 2p distinct nodes u1, v1, . . . , up, vp ∈ N and let Cp denote the set of all partial orders {µ1, . . . , µp} such that each µq is either uqvq (i.e., uq precedes vq) or vquq (i.e., vq precedes uq). Thus the cardinality of Cp is 2p. The following lemma tells us that the class Cp “covers” the linear orders on N ; the proof is by picking up ordered pairs from a linear order.\nLemma 6 Let p be an integer with 0 ≤ p ≤ n/2, and let ≺ be a linear order on N . Then there exists a member R ∈ Cp such that ≺ is a linear extension of R, that is, uv ∈ R implies u ≺ v.\nBecause of this coverage property an optimal DAG can be found by trying out every R ∈ Cp and searching for an optimal DAG compatible with R. A DAG A is said to be compatible with a partial order R if any topological ordering of A is a linear extension of R.\nIn the dynamic programming algorithm (2) the restriction to linear extensions of R amounts to simple constraints on the node subsets that need to be visited. We see that a set Y ⊆ N can form the |Y | first elements in a linear extension of R if and only if Y belongs to the family NR defined as follows.\nDefinition 1 Let R ∈ Cp. Denote by NR the family of sets Y ⊆ N satisfying\nv ∈ Y implies u ∈ Y for every uv ∈ R .\nSince R ∈ Cp contains p disjoint pairs of nodes, and for each pair one of the four possibilities is excluded, we have the following.\nLemma 7 Let R ∈ Cp. Then the cardinality of NR is 3p2n−2p.\nThe restricted dynamic programming algorithm evaluates the function gR defined by gR(∅) .= 0 and for nonempty Y ∈ NR recursively:\ngR(Y ) . = max\nv∈Y Y \\{v}∈NR\n{ gR(Y \\ {v}) + f̂v(Y \\ {v}) } . (3)\nIt follows that gR(N) equals the optimum score over DAGs compatible with R.\nWe summarize the above findings:\nLemma 8 Let p be an integer with 0 ≤ p ≤ n. Then for any partial order R ∈ Cp it holds that gR(N) = max{f(A) : A is compatible with R} ,\nand, furthermore,\nmax R∈Cp gR(N) = max A f(A) ,\nwhere A runs through all DAGs on the n-node set N .\nWe see that gR can be computed in time and space |NR|nO(1), provided that the number of possible parents for each node is polynomial in n. Thus we have obtained the following time–space tradeoff result, referred to as the pairwise scheme.\nTheorem 9 The structure discovery problem in Bayesian networks can be solved in time 2n(3/2)pnO(1) in space 2n(3/4)pnO(1) for any p = 0, 1, 2, . . . , n/2, provided that each node has nO(1) possible, predetermined parent sets.\nProof: The space complexity is obtained by rewriting |NR| = 3p2n−2p as 2n(3/4)p. The time complexity is obtained by multiplying |NR|nO(1) = 3p2n−2pnO(1) by |Cp| = 2p. The bounds in Theorem 9 should be compared to bounds that arise from the basic partitioning scheme (Proposition 3). For a comparison we set the space complexities equal (up to polynomial factors) and investigate the resulting running time bounds. To this end, we first solve p from 2n(3/4)p = 2s, giving p = (n − s)/ log2(4/3). We also set s = rn, yielding the running time bound\n2na(r) , a(r) . = r − r log2 r − (1− r) log2(1− r) (4)\nfor the partitioning method and the bound\n2nb(r) , b(r) . = 1 + (1− r) log2\n2\n3\n/ log2 3\n4 (5)\nfor the pairwise scheme. Note that the pairwise scheme only applies for 0 ≤ p ≤ n/2, which in terms of r amounts to 0.7924 . . . ≤ r ≤ 1. Within this range, b(r) appears to be strictly less than a(r), except at r = 1, in which case both schemes yield the running time bound of 2n (up to polynomial factors); see Figure 1."
    }, {
      "heading" : "5 UNBOUNDED INDEGREE",
      "text" : "So far we have assumed that the number of possible parent sets per node is polynomial in the number of nodes n. This is a reasonable assumption, e.g., when, each node is allowed to have at most some constant number k of parents. However, the polynomial factors in the running time bounds of Theorems 4 and 9 grow directly with the number of possible parent sets and thus heavily affect the performance in practice. In what follows, we aim at an implementation of the pairwise scheme (Theorem 9) such that the number of possible parent sets affects the running time additively rather than multiplicatively.\nThe idea is to arrange the computation of the recurrence (3) in such a way that some of the computations for f̂v(Y \\ {v}), the maximum local score among the parent sets contained in Y \\ {v}, are reused. Perhaps the most immediate attempt to do this would be dynamic programming according to the recurrence in Lemma 1; unfortunately, this seems to require space proportional to 2n/ √ n, for each v, when proceeding level-wise (Ott and Miyano, 2003): to compute the\nscores f̂v(Y ) for sets Y of size ℓ one needs to access the scores f̂v(X) of sets X of size ℓ − 1. Luckily, it turns out that a sparse dynamic programming variant solves the task with less space.\nOur key insight is the following simple observation,\nwhich we state in plain terms; the proof is omitted.\nLemma 10 Let X and Y be sets with X ⊆ Y . Let A .= {Z ⊆ Y : X ⊆ Z} , B .= {Z ⊆ Y : x 6∈ Z for some x ∈ X} .\nThen 2Y = A ∪ B and B = ⋃x∈X 2Y \\{x}.\nIn terms of the set functions fv and f̂v, for an arbitary v, the above lemma amounts to the following generalization of Lemma 1; the proof (omitted) is immediate\nby Lemma 10 and the definition of f̂v.\nLemma 11 Let X and Y be subsets of N \\ {v} with X ⊆ Y . Then f̂v(Y ) = max {\nmax X⊆Z⊆Y fv(Z),max u∈X\nf̂v(Y \\ {u}) } .\nNote that Lemma 1 is obtained as a special case of Lemma 11, with X = Y .\nLemma 11 leaves us the freedom to choose a suitable node subset X for each set of interest Y . How we make this choice is guided by the fact that, in the evaluation of gR by dynamic programming, we need the values f̂v(Y ) only for sets Y that belong to NR; in what follows, we consider R ∈ Cp fixed. By storing the values f̂v(Y ) only for Y ∈ NR we adhere to the space requirement (up to a polynomial factor) already needed for storing gR(Y ) for each Y ∈ NR. Thus our goal is to choose X such that Y \\ {u} ∈ NR for all u ∈ X. To this end, we let X consist of all such nodes in Y that have no larger node in Y (w.r.t. R). Accordingly, for Y ∈ NR define\nXY . = {u ∈ Y : uv 6∈ R for all v ∈ Y } .\nFurthemore, define the tail of Y as\nTY .= {Z ⊆ Y : XY ⊆ Z} .\nIn the next two lemmas we first show that XY indeed has the desired property (in a maximal sense), and then that the tails for different sets Y are pairwise disjoint, and thus optimally cover the subsets of N .\nLemma 12 Let Y ∈ NR and u ∈ Y . Then Y \\ {u} ∈ NR if and only if u ∈ XY .\nProof: “If”: Let u ∈ XY . Let st ∈ R. By the definition of NR we need to show that t ∈ Y \\ {u} implies s ∈ Y \\ {u}. So, suppose t ∈ Y \\ {u}, hence t ∈ Y . Now, since Y ∈ NR, we must have s ∈ Y . It remains to show that s 6= u. But this holds because ut 6∈ R by the definition of XY .\n“Only if”: Let u 6∈ XY . Then we have uv ∈ R for some v ∈ Y . But u 6∈ Y \\ {u} and v ∈ Y \\ {u}, implying Y \\ {u} 6∈ NR by the definition of NR.\nLemma 13 Let Y and Y ′ be two distinct sets in NR. Then the tails of Y and Y ′ are disjoint.\nProof: Suppose the contrary that Z ∈ TY ∩ TY ′ . W.l.o.g. let w ∈ Y \\ Y ′. Thus w 6∈ Z, for Z ⊆ Y and Z ⊆ Y ′. Therefore, w 6∈ XY and w 6∈ XY ′ , for XY ⊆ Z and XY ′ ⊆ Z. We conclude that wv ∈ R for some v ∈ Y and wv′ ∈ R for some v′ ∈ Y ′. But by the definition of R we must have v = v′. Thus we have arrived at wv ∈ R, w 6∈ Y ′, and v ∈ Y ′, which is a contradiction given that Y ′ ∈ NR. We now merge the ingredients given above into an algorithm for evaluating gR using the recurrence (3), for a fixed R ∈ Cp. In Algorithm 1 below, gR[Y ] and f̂v[Y ] denote program variables that correspond to the respective target values gR(Y ) and f̂v(Y ) to be computed. Also, recall that Fv denotes the family of possible parent sets for node v.\nAlgorithm 1\n1. Let\ngR[∅] ← 0 .\n2. For each v ∈ N , if ∅ ∈ Fv, then let\nf̂v[∅] ← fv(∅) ; else let f̂v[∅] ← −∞ .\n3. For each nonempty Y ∈ NR, in increasing order of cardinality:\n(a) let\ngR[Y ] ← max v∈XY\n{ gR[Y \\ {v}] + f̂v[Y \\ {v}] } ;\n(b) for each v ∈ Y let f̂v[Y ] be the larger of\nmax Z∈TY ∩Fv fv(Z) and max u∈XY\nf̂v[Y \\ {u}] .\nLemma 14 Algorithm 1 correctly computes gR, that is, gR[Y ] = gR(Y ) for all Y ∈ NR.\nProof: By the definition of gR in (3) and by Lemma 11 it suffices to notice that, by Lemma 12, the condition “v ∈ Y and Y \\ {v} ∈ NR” is equivalent to v ∈ XY , given that Y ∈ NR. Note also that maximizing over TY ∩Fv is equivalent to maximizing over TY , since, by convention, fv(Z) = −∞ for Z 6∈ Fv. We are ready prove the main result of this paper.\nTheorem 15 The structure discovery problem in Bayesian networks can be solved in time [ 2n(3/2)p + 2pF ]\nnO(1) in space 2n(3/4)pnO(1) for any p = 0, 1, 2, . . . , ⌊n/2⌋, provided that for each node v the family of possible parent sets Fv is downward closed and of size at most F .\nProof: By Lemmas 8 and 14 it suffices to run Algorithm 1 for every R ∈ Cp, that is, |Cp| = 2p times. The time requirement of Algorithm 1 is dominated by steps 3(a) and 3(b). Given Y , the set XY can clearly be constructed in time nO(1). Thus the contribution of step 3(a) in the total time requirement is |NR|nO(1) = 3p2n−2pnO(1) (Lemma 7).\nWe then analyze the time requirement of step 3(b), for fixed v. By Proposition 2 the maximization of the local scores over TY ∩ Fv can be done in time polynomial in |TY ∩ Fv|. Since, by Lemma 13, these families are disjoint for different Y ∈ NR, the total contribution to the time requirement is polynomial in |Fv| ≤ F , for each v. Because step 3(b) is executed |NR| times, the total time requirement of step 3(a) is |NR|nO(1) + FnO(1).\nCombining the time bounds of step 3(a) and (b) and multiplying by 2p yields the claimed bound 2p [ 3p2n−2p + F ] nO(1) = [ 2n(3/2)p + 2pF ] nO(1).\nThe space requirement is |NR|nO(1) = 3p2n−2pnO(1), since, by Lemma 12, the values gR[Y ] and f̂v[Y ] are needed only for Y ∈ NR. When there is no restrictions on the possible parent sets (i.e., each node has 2n−1 possible parent sets), we get the following.\nCorollary 16 The structure discovery problem in Bayesian networks can be solved in time 2n+pnO(1) in space 2n(3/4)pnO(1) for any p = 0, 1, 2, . . . , ⌊n/2⌋.\nOn the other hand, if each node is allowed to have at most k parents, we get a significantly better running time bound, even if k is let to grow linearly in the number of nodes n; the proof (omitted) is by direct comparison of the binomial coefficient (\nn−1 k\n)\nand the bound 3p2n−2p in the stringest case, at p = n/2.\nCorollary 17 The structure discovery problem in Bayesian networks can be solved in time 2n(3/2)pnO(1) in space 2n(3/4)pnO(1) for any p = 0, 1, 2, . . . , ⌊n/2⌋, provided that each node is allowed to have at most 0.238n parents."
    }, {
      "heading" : "6 ON PARALLELIZATION",
      "text" : "We note that the pairwise scheme described in the previous sections allows for efficient parallelization. Obviously, each partial order R ∈ Cp can be treated in parallel. Furthermore, as in the Silander–Myllymäki implementation, the optimal local scores over given sets of possible parents in step 3(b) of Algorithm 1 can be precomputed—that is, not merging with step 3(a)—in parallel for each of the n nodes. In total, this\namounts to parallelization onto 2pn processors (each with own memory); this is efficient in the sense that the running time per processor is scaled down by the same factor. So, if ignoring factors polynomial in n, the running time per processor becomes 2n(3/4)p (under the conditions of Corollary 17), thus exponentially less than 2n when p grows."
    }, {
      "heading" : "7 EMPIRICAL RESULTS",
      "text" : "We have implemented the pairwise scheme in the C++ language. We examined the running time for the limit of 16 gigabytes of memory, letting the number of nodes n vary from 25 to 31, with maximum indegree set to 3 (the local scores were taken as given, so computing them is not included in the running time estimates). First we estimated the minimum number of node pairs p that yields a memory requirement of at most 16 gigabytes. Then we ran Algorithm 1 for a single partial order R ∈ Cp; the resulting running time was multiplied by 2p to get the total running time, see Table 7. The experiments were run on a 3.66-GHz Intel Xeon with 32 GB of RAM.\nTable 1: The implemented pairwise scheme given 16 gigabytes of memory. Reported are CPU hours in total, T , and as divided to 2p processors.\nn p T T/2p"
    }, {
      "heading" : "25 0 2 2.12",
      "text" : ""
    }, {
      "heading" : "26 2 9 2.27",
      "text" : ""
    }, {
      "heading" : "27 4 41 2.56",
      "text" : ""
    }, {
      "heading" : "28 7 331 2.59",
      "text" : ""
    }, {
      "heading" : "29 9 1660 3.24",
      "text" : ""
    }, {
      "heading" : "30 12 13322 3.25",
      "text" : ""
    }, {
      "heading" : "31 14 67748 4.14",
      "text" : "We see that the current implementation is feasible up to around 31 nodes (4 weeks using 100 processors, 3 days using 1000 processors). However, we believe that by a more careful implementation both the time and the space requirement can be reduced to about one tenth, which should bring networks on 34 nodes to within reach (with massive parallelization)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors wish to thank Petteri Kaski, Fedor Fomin, Saket Saurabh, and Yngve Villanger for useful discussions on the Gurevich–Shelah recurrence. The research was supported in part by the Academy of Finland, Grant 125637."
    }, {
      "heading" : "A. Björklund and T. Husfeldt. Exact algorithms for exact",
      "text" : "satisfiability and number of perfect matchings. Algorithmica, 52:226–249, 2008."
    }, {
      "heading" : "H. L. Bodlaender, F. V. Fomin, A. M. C. A. Koster,",
      "text" : "D. Kratsch, and D. M. Thilikos. On exact algorithms for treewidth. In ESA, pages 672–683, 2006."
    }, {
      "heading" : "G. F. Cooper and E. Herskovits. A Bayesian method for the",
      "text" : "induction of probabilistic networks from data. Machine Learning, 9:309–347, 1992.\nD. Eaton and K. Murphy. Exact Bayesian structure learning from uncertain interventions. In Proc. of the 23rd Conference on Uncertainty in Artificial Intelligence and Statistics (AISTAT). Omnipress, 2007. Electronic only."
    }, {
      "heading" : "N. Friedman and D. Koller. Being Bayesian about network structure: A Bayesian approach to structure discovery in",
      "text" : "Bayesian networks. Machine Learning, 50(1–2):95–125, 2003."
    }, {
      "heading" : "Y. Gurevich and S. Shelah. Expected computation time",
      "text" : "for Hamiltonian path problem. SIAM J. Comput., 16: 486–502, 1987."
    }, {
      "heading" : "D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The combination of knowledge and",
      "text" : "statistical data. Machine Learning, 20:197–243, 1995."
    }, {
      "heading" : "M. Held and R. Karp. A dynamic programming approach",
      "text" : "to sequencing problems. J. Soc. Indust. Appl. Math., (10):196–210, 1962.\nM. Koivisto. Advances in exact Bayesian structure discovery in Bayesian networks. In Proc. of the 22nd Conference on Uncertainty in Artificial Intelligence (UAI), pages 241–248. AUAI Press, 2006.\nM. Koivisto and K. Sood. Exact Bayesian structure discovery in Bayesian networks. Journal of Machine Learning Research, 5:549–573, 2004."
    }, {
      "heading" : "E. Lawler. A comment on minimum feedback arc sets.",
      "text" : "IEEE Trans. on Circuit Theory, pages 296–297, 1964."
    }, {
      "heading" : "S. Ott and S. Miyano. Finding optimal gene networks using",
      "text" : "biological constraints. Genome Informatics, (14):124– 133, 2003."
    }, {
      "heading" : "E. Perrier, S. Imoto, and S. Miyano. Finding optimal",
      "text" : "Bayesian network given a super-structure. Journal of Machine Learning Research, 9:2251–2286, 2008."
    }, {
      "heading" : "T. Silander and P. Myllymäki. A simple approach for finding the globally optimal Bayesian network structure. In",
      "text" : "Proc. of the 22nd Conference on Uncertainty in Artificial Intelligence (UAI), pages 445–452. AUAI Press, 2006."
    }, {
      "heading" : "A. Singh and A. Moore. Finding optimal Bayesian networks by dynamic programming. Technical report,",
      "text" : "Carnegie Mellon University, June 2005."
    } ],
    "references" : [ {
      "title" : "Complexity of finding embeddings in a k-tree",
      "author" : [ "S. Arnborg", "D.G. Corneil", "A. Proskurowski" ],
      "venue" : "SIAM J. Alg. Disc. Meth.,",
      "citeRegEx" : "Arnborg et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Arnborg et al\\.",
      "year" : 1987
    }, {
      "title" : "Dynamic programming treatment of the travelling salesman problem",
      "author" : [ "R. Bellman" ],
      "venue" : "J. Assoc. Comput. Mach.,",
      "citeRegEx" : "Bellman.,? \\Q1962\\E",
      "shortCiteRegEx" : "Bellman.",
      "year" : 1962
    }, {
      "title" : "Exact algorithms for exact satisfiability and number of perfect matchings",
      "author" : [ "A. Björklund", "T. Husfeldt" ],
      "venue" : null,
      "citeRegEx" : "Björklund and Husfeldt.,? \\Q2008\\E",
      "shortCiteRegEx" : "Björklund and Husfeldt.",
      "year" : 2008
    }, {
      "title" : "On exact algorithms for treewidth",
      "author" : [ "H.L. Bodlaender", "F.V. Fomin", "A.M.C.A. Koster", "D. Kratsch", "D.M. Thilikos" ],
      "venue" : "In ESA,",
      "citeRegEx" : "Bodlaender et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bodlaender et al\\.",
      "year" : 2006
    }, {
      "title" : "A Bayesian method for the induction of probabilistic networks from data",
      "author" : [ "G.F. Cooper", "E. Herskovits" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cooper and Herskovits.,? \\Q1992\\E",
      "shortCiteRegEx" : "Cooper and Herskovits.",
      "year" : 1992
    }, {
      "title" : "Exact Bayesian structure learning from uncertain interventions",
      "author" : [ "D. Eaton", "K. Murphy" ],
      "venue" : "In Proc. of the 23rd Conference on Uncertainty in Artificial Intelligence and Statistics (AISTAT). Omnipress,",
      "citeRegEx" : "Eaton and Murphy.,? \\Q2007\\E",
      "shortCiteRegEx" : "Eaton and Murphy.",
      "year" : 2007
    }, {
      "title" : "Being Bayesian about network structure: A Bayesian approach to structure discovery in Bayesian networks",
      "author" : [ "N. Friedman", "D. Koller" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Friedman and Koller.,? \\Q2003\\E",
      "shortCiteRegEx" : "Friedman and Koller.",
      "year" : 2003
    }, {
      "title" : "Expected computation time for Hamiltonian path problem",
      "author" : [ "Y. Gurevich", "S. Shelah" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Gurevich and Shelah.,? \\Q1987\\E",
      "shortCiteRegEx" : "Gurevich and Shelah.",
      "year" : 1987
    }, {
      "title" : "Learning Bayesian networks: The combination of knowledge and statistical data",
      "author" : [ "D. Heckerman", "D. Geiger", "D.M. Chickering" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Heckerman et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 1995
    }, {
      "title" : "A dynamic programming approach to sequencing problems",
      "author" : [ "M. Held", "R. Karp" ],
      "venue" : "J. Soc. Indust. Appl. Math.,",
      "citeRegEx" : "Held and Karp.,? \\Q1962\\E",
      "shortCiteRegEx" : "Held and Karp.",
      "year" : 1962
    }, {
      "title" : "Advances in exact Bayesian structure discovery in Bayesian networks",
      "author" : [ "M. Koivisto" ],
      "venue" : "In Proc. of the 22nd Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Koivisto.,? \\Q2006\\E",
      "shortCiteRegEx" : "Koivisto.",
      "year" : 2006
    }, {
      "title" : "Exact Bayesian structure discovery in Bayesian networks",
      "author" : [ "M. Koivisto", "K. Sood" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Koivisto and Sood.,? \\Q2004\\E",
      "shortCiteRegEx" : "Koivisto and Sood.",
      "year" : 2004
    }, {
      "title" : "A comment on minimum feedback arc sets",
      "author" : [ "E. Lawler" ],
      "venue" : "IEEE Trans. on Circuit Theory,",
      "citeRegEx" : "Lawler.,? \\Q1964\\E",
      "shortCiteRegEx" : "Lawler.",
      "year" : 1964
    }, {
      "title" : "Finding optimal gene networks using biological constraints",
      "author" : [ "S. Ott", "S. Miyano" ],
      "venue" : "Genome Informatics,",
      "citeRegEx" : "Ott and Miyano.,? \\Q2003\\E",
      "shortCiteRegEx" : "Ott and Miyano.",
      "year" : 2003
    }, {
      "title" : "Finding optimal Bayesian network given a super-structure",
      "author" : [ "E. Perrier", "S. Imoto", "S. Miyano" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Perrier et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Perrier et al\\.",
      "year" : 2008
    }, {
      "title" : "A simple approach for finding the globally optimal Bayesian network structure",
      "author" : [ "T. Silander", "P. Myllymäki" ],
      "venue" : "In Proc. of the 22nd Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Silander and Myllymäki.,? \\Q2006\\E",
      "shortCiteRegEx" : "Silander and Myllymäki.",
      "year" : 2006
    }, {
      "title" : "Finding optimal Bayesian networks by dynamic programming",
      "author" : [ "A. Singh", "A. Moore" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Singh and Moore.,? \\Q2005\\E",
      "shortCiteRegEx" : "Singh and Moore.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "There has been relatively recent interest in devising exact algorithms for score-based structure learning in Bayesian networks (Eaton and Murphy, 2007; Koivisto and Sood, 2004; Koivisto, 2006; Perrier et al., 2008; Silander and Myllymäki, 2006; Singh and Moore, 2005).",
      "startOffset" : 127,
      "endOffset" : 267
    }, {
      "referenceID" : 11,
      "context" : "There has been relatively recent interest in devising exact algorithms for score-based structure learning in Bayesian networks (Eaton and Murphy, 2007; Koivisto and Sood, 2004; Koivisto, 2006; Perrier et al., 2008; Silander and Myllymäki, 2006; Singh and Moore, 2005).",
      "startOffset" : 127,
      "endOffset" : 267
    }, {
      "referenceID" : 10,
      "context" : "There has been relatively recent interest in devising exact algorithms for score-based structure learning in Bayesian networks (Eaton and Murphy, 2007; Koivisto and Sood, 2004; Koivisto, 2006; Perrier et al., 2008; Silander and Myllymäki, 2006; Singh and Moore, 2005).",
      "startOffset" : 127,
      "endOffset" : 267
    }, {
      "referenceID" : 14,
      "context" : "There has been relatively recent interest in devising exact algorithms for score-based structure learning in Bayesian networks (Eaton and Murphy, 2007; Koivisto and Sood, 2004; Koivisto, 2006; Perrier et al., 2008; Silander and Myllymäki, 2006; Singh and Moore, 2005).",
      "startOffset" : 127,
      "endOffset" : 267
    }, {
      "referenceID" : 15,
      "context" : "There has been relatively recent interest in devising exact algorithms for score-based structure learning in Bayesian networks (Eaton and Murphy, 2007; Koivisto and Sood, 2004; Koivisto, 2006; Perrier et al., 2008; Silander and Myllymäki, 2006; Singh and Moore, 2005).",
      "startOffset" : 127,
      "endOffset" : 267
    }, {
      "referenceID" : 16,
      "context" : "There has been relatively recent interest in devising exact algorithms for score-based structure learning in Bayesian networks (Eaton and Murphy, 2007; Koivisto and Sood, 2004; Koivisto, 2006; Perrier et al., 2008; Silander and Myllymäki, 2006; Singh and Moore, 2005).",
      "startOffset" : 127,
      "endOffset" : 267
    }, {
      "referenceID" : 11,
      "context" : "e, attributes or variables) compute and store intermediate results for all the possible 2 node subsets, running in time and space 2n, assuming the score obeys certain usual modularity properties (Koivisto and Sood, 2004; Ott and Miyano, 2003; Silander and Myllymäki, 2006; Singh and Moore, 2005).",
      "startOffset" : 195,
      "endOffset" : 295
    }, {
      "referenceID" : 13,
      "context" : "e, attributes or variables) compute and store intermediate results for all the possible 2 node subsets, running in time and space 2n, assuming the score obeys certain usual modularity properties (Koivisto and Sood, 2004; Ott and Miyano, 2003; Silander and Myllymäki, 2006; Singh and Moore, 2005).",
      "startOffset" : 195,
      "endOffset" : 295
    }, {
      "referenceID" : 15,
      "context" : "e, attributes or variables) compute and store intermediate results for all the possible 2 node subsets, running in time and space 2n, assuming the score obeys certain usual modularity properties (Koivisto and Sood, 2004; Ott and Miyano, 2003; Silander and Myllymäki, 2006; Singh and Moore, 2005).",
      "startOffset" : 195,
      "endOffset" : 295
    }, {
      "referenceID" : 16,
      "context" : "e, attributes or variables) compute and store intermediate results for all the possible 2 node subsets, running in time and space 2n, assuming the score obeys certain usual modularity properties (Koivisto and Sood, 2004; Ott and Miyano, 2003; Silander and Myllymäki, 2006; Singh and Moore, 2005).",
      "startOffset" : 195,
      "endOffset" : 295
    }, {
      "referenceID" : 15,
      "context" : "The current record of 29 nodes was achieved by the streamlined Silander–Myllymäki implementation (Silander and Myllymäki, 2006) using nearly 100 gigabytes of hard disk.",
      "startOffset" : 97,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "For a number of similar permutation problems—like the Travelling Salesman problem, the Feedback Arcset problem, and Treewidth, to name a few—dynamic programming algorithms running in time and space 2n have been known for decades (Bellman, 1962; Held and Karp, 1962; Lawler, 1964; Arnborg et al., 1987), with only negligible progress since; more recent work (Bodlaender et al.",
      "startOffset" : 229,
      "endOffset" : 301
    }, {
      "referenceID" : 9,
      "context" : "For a number of similar permutation problems—like the Travelling Salesman problem, the Feedback Arcset problem, and Treewidth, to name a few—dynamic programming algorithms running in time and space 2n have been known for decades (Bellman, 1962; Held and Karp, 1962; Lawler, 1964; Arnborg et al., 1987), with only negligible progress since; more recent work (Bodlaender et al.",
      "startOffset" : 229,
      "endOffset" : 301
    }, {
      "referenceID" : 12,
      "context" : "For a number of similar permutation problems—like the Travelling Salesman problem, the Feedback Arcset problem, and Treewidth, to name a few—dynamic programming algorithms running in time and space 2n have been known for decades (Bellman, 1962; Held and Karp, 1962; Lawler, 1964; Arnborg et al., 1987), with only negligible progress since; more recent work (Bodlaender et al.",
      "startOffset" : 229,
      "endOffset" : 301
    }, {
      "referenceID" : 0,
      "context" : "For a number of similar permutation problems—like the Travelling Salesman problem, the Feedback Arcset problem, and Treewidth, to name a few—dynamic programming algorithms running in time and space 2n have been known for decades (Bellman, 1962; Held and Karp, 1962; Lawler, 1964; Arnborg et al., 1987), with only negligible progress since; more recent work (Bodlaender et al.",
      "startOffset" : 229,
      "endOffset" : 301
    }, {
      "referenceID" : 3,
      "context" : ", 1987), with only negligible progress since; more recent work (Bodlaender et al., 2006) has shown that if only polynomial space is allowed, then many permutation problems can PARVIAINEN & KOIVISTO 436 UAI 2009",
      "startOffset" : 63,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "This divide and conquer technique is known as the Gurevich–Shelah recurrence, originally presented for the Hamiltonian path problem (Gurevich and Shelah, 1987) and later applied also elsewhere (Björklund and Husfeldt, 2008; Bodlaender et al.",
      "startOffset" : 132,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "This divide and conquer technique is known as the Gurevich–Shelah recurrence, originally presented for the Hamiltonian path problem (Gurevich and Shelah, 1987) and later applied also elsewhere (Björklund and Husfeldt, 2008; Bodlaender et al., 2006).",
      "startOffset" : 193,
      "endOffset" : 248
    }, {
      "referenceID" : 3,
      "context" : "This divide and conquer technique is known as the Gurevich–Shelah recurrence, originally presented for the Hamiltonian path problem (Gurevich and Shelah, 1987) and later applied also elsewhere (Björklund and Husfeldt, 2008; Bodlaender et al., 2006).",
      "startOffset" : 193,
      "endOffset" : 248
    }, {
      "referenceID" : 15,
      "context" : "We note that while the Silander–Myllymäki implementation is easy to run in parallel on n processors, the time requirement per processor remains 2, up to a polynomial factor (Silander and Myllymäki, 2006).",
      "startOffset" : 173,
      "endOffset" : 203
    }, {
      "referenceID" : 4,
      "context" : "Given these local scores, the task is to find a DAG A that maximizes the sum of the local scores (Cooper and Herskovits, 1992; Heckerman et al., 1995),",
      "startOffset" : 97,
      "endOffset" : 150
    }, {
      "referenceID" : 8,
      "context" : "Given these local scores, the task is to find a DAG A that maximizes the sum of the local scores (Cooper and Herskovits, 1992; Heckerman et al., 1995),",
      "startOffset" : 97,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : "We note that this formulation does not directly apply to the so-called Bayesian approach to structure discovery (Friedman and Koller, 2003; Koivisto and Sood, 2004).",
      "startOffset" : 112,
      "endOffset" : 164
    }, {
      "referenceID" : 11,
      "context" : "We note that this formulation does not directly apply to the so-called Bayesian approach to structure discovery (Friedman and Koller, 2003; Koivisto and Sood, 2004).",
      "startOffset" : 112,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "However, this can be significantly improved by dynamic programming, observed first in Ott and Miyano (2003):",
      "startOffset" : 86,
      "endOffset" : 108
    }, {
      "referenceID" : 14,
      "context" : "parent sets of cardinality at most k, for some fixed k, and (b) the parent sets that are contained in a given set of candidate parents (Perrier et al., 2008).",
      "startOffset" : 135,
      "endOffset" : 157
    }, {
      "referenceID" : 13,
      "context" : "Perhaps the most immediate attempt to do this would be dynamic programming according to the recurrence in Lemma 1; unfortunately, this seems to require space proportional to 2/ √ n, for each v, when proceeding level-wise (Ott and Miyano, 2003): to compute the scores f̂v(Y ) for sets Y of size l one needs to access the scores f̂v(X) of sets X of size l − 1.",
      "startOffset" : 221,
      "endOffset" : 243
    } ],
    "year" : 2009,
    "abstractText" : "The fastest known exact algorithms for scorebased structure discovery in Bayesian networks on n nodes run in time and space 2n. The usage of these algorithms is limited to networks on at most around 25 nodes mainly due to the space requirement. Here, we study space–time tradeoffs for finding an optimal network structure. When little space is available, we apply the Gurevich– Shelah recurrence—originally proposed for the Hamiltonian path problem—and obtain time 2n in space 2n for any s = n/2, n/4, n/8, . . .; we assume the indegree of each node is bounded by a constant. For the more practical setting with moderate amounts of space, we present a novel scheme. It yields running time 2(3/2)n in space 2(3/4)n for any p = 0, 1, . . . , n/2; these bounds hold as long as the indegrees are at most 0.238n. Furthermore, the latter scheme allows easy and efficient parallelization beyond previous algorithms. We also explore empirically the potential of the presented techniques.",
    "creator" : null
  }
}