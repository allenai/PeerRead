{
  "name" : "1605.04809.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The AMU-UEDIN Submission to the WMT16 News Translation Task: Attention-based NMT Models as Feature Functions in Phrase-based SMT",
    "authors" : [ "Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Rico Sennrich" ],
    "emails" : [ "junczys@amu.edu.pl", "t.dwojak@amu.edu.pl", "rico.sennrich@ed.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 5.\n04 80\n9v 3\n[ cs\n.C L\n] 2\n3 Ju\nn 20\n16"
    }, {
      "heading" : "1 Introduction",
      "text" : "This paper describes the AMU-UEDIN submissions to the WMT 2016 shared task on news translation. We explore methods of decode-time integration of attention-based neural translation models with phrase-based decoding. Experiments have been conducted for the English-Russian language pair in both translation directions.\nFor these experiments we re-implemented the inference step of the models described in Bahdanau et al. (2015) (more exactly the DL4MT1 variant also present in Nematus2) in efficient C++/CUDA code that can be directly\n1https://github.com/nyu-dl/dl4mt-tutorial 2https://github.com/rsennrich/nematus\ncompiled as a Moses feature function. The GPU-based computations come with their own peculiarities which we reconcile with the two most popular phrase-based decoding algorithms — stack-decoding and cube-pruning.\nWhile it seems at first that for English-Russian our phrase-based system is holding back the neural models in terms of BLEU, the manual evaluation reveals that our systems is tied with the pure neural systems, occupying the same top cluster for restricted systems with an even slightly higher TrueSkill score. We achieve the top BLEU result for the Russian-English task, outperforming the best pure neural system by 1.1 BLEU points and our own phrase-based baseline by 1.6 BLEU. After manual evaluation, this system is the best restricted system in its own cluster.\nOur implementation is available as a Moses fork from https://github.com/emjotde/mosesdecoder_nmt"
    }, {
      "heading" : "2 Preprocessing",
      "text" : "As we reuse the neural systems from Sennrich et al. (2016), we follow their preprocessing scheme for the phrase-based systems as well. All data is tokenized with the Moses tokenizer, for English the Penn-format tokenization scheme has been used. Tokenized text is true-cased.\nSennrich et al. (2016) use byte-pair-encoding (BPE) to achieve open-vocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2015b). For English, the vocabulary size is limited to 50,000 units, for Russian to 100,000. This has the interesting consequence of using subword units for phrase-based SMT. Although SMT seems to be better equipped to handle large vocabularies, the case of Russian still poses problems which are usually solved with\ntransliteration mechanisms (Durrani et al., 2014). Resorting to subword units eliminates the need for these.3"
    }, {
      "heading" : "3 Neural translation systems",
      "text" : "As mentioned before, we reuse the EnglishRussian and Russian-English NMT models from Sennrich et al. (2016) and refer the reader to that paper for a more detailed description of these systems. In this section we give a short summarization for the sake of completeness.\nThe neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which has been trained with Nematus. Additional parallel training data has been produced by automatically translating a random sample (2 million sentences) of the monolingual Russian News Crawl 2015 corpus into English (Sennrich et al., 2015a), which has been combined with the original parallel data in a 1-to-1 ratio.4 The same has been done for the other direction. We used mini-batches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We clip the gradient norm to 1.0 (Pascanu et al., 2013). Models were trained with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs. The models have been trained model for approximately 2 weeks, saving every 30000 mini-batches.\nFor our experiments with PB-SMT integration, we chose the same four models that constituted the best-scoring ensemble from Sennrich et al. (2016). If less than four models were used, we chose the models with the highest BLEU scores among these four models as measured on a development set."
    }, {
      "heading" : "4 Phrase-Based baseline systems",
      "text" : "We base our set-up on a Moses system (Koehn et al., 2007) with a number of additional feature functions. Apart from the default configuration with a lexical reordering model,\n3In experiments not described in this paper, we tried BPE encoding for the English-German language pair and found subword units to cope well with German compound nouns when used for phrase-based SMT.\n4This artificial data has not been used for the creation of the phrase-based system, but it might be worthwhile to explore this possibility in the future. It might enable the phrasebased system to produce translation that are more similar to the neural output.\nwe add a 5-gram operation sequence model (Durrani et al., 2013).\nWe perform no language-specific adaptations or modifications. The two systems differ only with respect to translation direction and the available (monolingual) training data. For domainadaptation, we rely solely on parameter tuning with Batch-Mira (Cherry and Foster, 2012) and on-line log-linear interpolation. Binary domainindicators for each separate parallel corpus are introduced to the phrase-tables (four indicators) and a separate language model per parallel and monolingual resource is trained (en:16 and ru:12). All language models are 5-gram models with Modified Kneser-Ney smoothing and without pruning thresholds (Heafield et al., 2013). We treat different years of the News Crawl data as different domains to take advantage of possible recency-based effects. During parameter tuning on the newstest2014 test set, we can unsurprisingly observe that weights for the last three LMs (2013, 2014, 2015) are much higher than for the remaining years.\nAfter concatenating all resources, a large 5- gram background language model is trained, with 3-grams or higher n-gram orders being pruned if they occur only once. The same concatenated files and pruning settings are used to create a 9-gram word-class language model with 200 word-classes produced by word2vec (Mikolov et al., 2013)."
    }, {
      "heading" : "5 NMT as Moses feature functions",
      "text" : "As mentioned in the introduction, we implemented a C++/CUDA version of the inference step for the neural models trained with DL4MT or Nematus, which can be used directly with our code. One or multiple models can be added to the Moses log-linear model as different instances of the same feature, which during tuning can be separately weighted. Adding multiple models as separate features becomes thus similar to ensemble translation with pure neural models.\nIn this section we give algorithmic details about integrating GPU-based soft-attention neural translation models into Moses as part of the feature function framework. Our work differs from Alkhouli et al. (2015) in the following aspects:\n1. While Alkhouli et al. (2015) integrate RNNbased translation models in phrase-based decoding, this work is to our knowledge the first to integrate soft-attention models.\n2. Our implementation is GPU-based and our algorithms being tailored towards GPU computations require very different caching strategies from those proposed in Alkhouli et al. (2015). Our implementation seems to be about 10 times faster on one GPU, 30 times faster when three GPUs are used."
    }, {
      "heading" : "5.1 Scoring hypotheses and their expansions",
      "text" : "We assume through-out this section that the neural model has already been initialized with the source sentence and that the source sentence context is available at all time.\nIn phrase-based machine translation, a pair consisting of a translation hypothesis and a chosen possible target phrase that expands this hypothesis to form a new hypothesis can be seen as the smallest unit of computation. In the typical case they are processed independently from other hypothesis-expansion pairs until they are put on a stack and potentially recombined. Our aim is to run the computations on one or more GPUs. This makes the calculation of scores per hypothesisexpansion pair (as done for instance during n-gram language model querying) unfeasible as repeated GPU-access with very small units of computation comes with a very high overhead.\nIn neural machine translation, we treat neural states to be equivalent to hypotheses, but they are extended only by single words, not phrases, by performing computations over the whole target vocabulary. In this section, we present a batching and querying scheme that aims at taking advantage\nof the capabilities of GPUs to perform batched calculations efficiently, by combining the approaches from phrase-based and neural decoding.\nGiven is a set of pairs (h, t) where h is a decoding hypothesis and t a target phrase expanding the hypothesis. In a naive approach (corresponding to unmodified stack decoding) the number of queries to the GPU would be equal to the total number of words in all expansions. A better algorithm might take advantage of common target phrase prefixes per hypothesis. The number of queries would be reduced to the number of collapsed edges in the per-hypothesis prefix-tree forest.\nBy explicitly constructing this forest of prefix trees where a single prefix tree encodes all target phrases that expand the same hypothesis, we can actually reduce the number of queries to the neural model to the maximum depth of any of the trees (i.e. the maximum target phrase length) as illustrated in Figures 1 and 2.\nTarget phrases t are treated as sequences of words w. Rectangles at tree nodes should be imagined to be empty before the preceding step has been performed. The first embedding matrix E1 is constructed by concatenating embedding vectors ei ← LOOKUP(wi) as rows of the matrix, for all wi marked in the first dashed rectangle. The initial state matrix H0 is a row-wise concatenation of the neural hypothesis states, repeated for each outgoing edge. Thus, the embedding matrix and state matrix have the same number of corresponding rows. Example matrices for the first step take the following form:\nE1 =\n\n  \ne0\ne1 e1\ne2\n\n   H0 =\n\n  \nh0\nh0 h1\nh1\n\n  \nGiven the precomputed source context state, we can now perform one forward step in the neural network which yields a matrix of output states and a matrix of probabilities, both corresponding rowwise to the input state matrix and embedding matrix we constructed earlier. The target nodes for each edge pointed to after the first step are filled. Probabilities will be queried later during phrasebased scoring, neural hypothesis states are reused to construct the state matrix of the next step and potentially as initial states when scoring another batch of hypotheses at later time."
    }, {
      "heading" : "5.2 Two-pass stack decoding",
      "text" : "Standard stack decoding still scores hypotheses one-by-one. In order to limit the number of modifications of Moses to a minimum, we propose two-pass stack decoding where the first pass is a hypothesis and expansions collection step and the second pass is the original expansion and scoring step. Between the two steps we pre-calculate perhypothesis scores with the procedure described above. The data structure introduced in Figure 1 is then reused for probability look-up during the scoring phrase of stack decoding as if individual hypotheses where scored on-the-fly.\nFigure 3 contains our complete proposal for two-pass stack decoding, a modification of the original stack decoding algorithm described in Koehn (2010). We dissect stack decoding into smaller reusable pieces that can be passed func-\n1: procedure TWOPASSSTACKDECODING 2: Place empty hypothesis h0 into stack S0 3: for stack S in stacks do 4: L← ∅ 5: PROCESSSTACK(S, GATHER{L}) 6: C ← SCOREBATCH(L, NMT) 7: PROCESSSTACK(S, EXPAND{C})\n8:\n9: procedure PROCESSSTACK(S, f ) 10: for hypothesis h in S do 11: for target phrase t do 12: if applicable then 13: Apply functor f (h, t)\n14: 15: procedure GATHER(h, t) 16: L← L ∪ {(h, t)}\n17: 18: procedure EXPAND(h, t) 19: Look-up p for (h, t) in C 20: Create new hypothesis ĥ from (h, t, p) 21: Place ĥ on appropriate stack s 22: if possible then 23: Recombine hypothesis ĥ with other\ntors to perform different tasks for the same sets of hypotheses. The main reason for this is the small word “applicable” in line 12, which hides a complicated set of target phrase choices based on reordering limits and coverage vectors which should not be discussed here. This allows our algorithm to collect exactly the set of hypotheses and expansions for score pre-calculation that will be used during the second expansion step.\nAs already mentioned, the number of forward steps for the NMT network per stack is equal to the greatest phrase length among all expansions. The total number of GPU queries increases therefore linearly with respect to the sentence length. Branching factors or stack sizes affect the matrix sizes, not the number of steps.5\nFor this method we do not provide results due to a lack of time. We confirmed for other experiments that improvements are smaller than for the next method. A comparison will be provided in an\n5Large matrix sizes, however, do slow-down translation speed significantly.\nextended version of this work."
    }, {
      "heading" : "5.3 Stack rescoring",
      "text" : "The previous approach cannot be used with lazy decoding algorithms — like cube pruning — which has also been implemented in Moses. Apart from that, due to the large number of expansions even small stack sizes of around 30 or 50 quickly result in large matrices in the middle steps of BATCHSCORE where the prefix trees have the greatest number of edges at the same depth level. In the worst case, matrix size will increase by a factor bd, where b is the branching factor and d is the current depth. In practice, however, the maximum is reached at the third or fourth step, as only few target phrases contain five or more words.\nTo address both shortcomings we propose a second algorithm: stack rescoring. This algorithm (Figure 4) relies on two ideas:\n1. During hypothesis expansion the NMT feature is being ignored, only probabilities of 0 are assigned for this feature to all newly created hypotheses. Hypothesis recombination and pruning take place without NMT scores for the current expansions (NMT scores for all previous expansions are included). Any stack-based decoding algorithm, also cubepruning, can be used in this step.\n2. The BATCHSCORE procedure is applied to all direct predecessors of hypotheses on the currently expanded stack. Predecessors consist of the parent hypothesis and the expansion that resulted in the current hypothesis.\nThe previously assigned 0-probabilities are replaced with the actual NMT scores.\nThis procedure results in a number of changes when compared to standard stack decoding approaches and the previous method:\n• The maximum matrix row count is equal to the stack size, and often much smaller due to prefix collapsing. Branching factors are irrelevant and stack sizes of 2,000 or greater are possible. By contrast, for two-pass stack decoding stack sizes of around 50 could already result in row counts of 7,000 and more.\n• With cube pruning, by setting cube pruning pop-limits much larger than the stack size many more hypotheses can be scored with all remaining feature functions before the survivors are passed to BATCHSCORE.\n• Scoring with the NMT-feature is delayed until the next stack is processed. This may result in missing good translations due to recombination. However, the much larger stack sizes may counter this effect.\n• N-best list extraction is more difficult, as hypotheses that have been recombined do not display correct cumulative sums for the NMT-feature scores. The one-best translation is always correctly scored as it has never been discarded during recombination, so there is no problem at test time. For tuning, where a correctly scored n-best list is required, we simply rescore the final n-best list with the same neural feature functions as during decoding. The resulting scores are the same as if they were produced at decodetime. Final n-best list rescoring can thus be seen as an integral part of stack-rescoring."
    }, {
      "heading" : "6 Experiments and results",
      "text" : "For decoding, we use the cube-pruning algorithm with stack size of 1,000 and cube-pruning pop limit of 2,000 during tuning. At test time, a stacksize of 1,000 is kept, but the cube-pruning pop limit is increased to 5,000. We set a distortion limit of 12. We run 10 iterations of Batch-Mira (Cherry and Foster, 2012) and choose the best set of weights based on the development set. Our development set is a subset of 2,000 sentences from the newstest-2014 test set. Sentences have been\nselected to be shorter than 40 words to avoid GPUmemory problems. Our GPUs are three Nvidia GeForce GTX-970 cards with 4GB RAM each.\nIn this paper, similar as Alkhouli et al. (2015), we ignore the implications of the infinite neural state and hypothesis recombination in the face of infinite state. We rely on the hypothesis recombination controlled by the states of the other feature functions. It is worth mentioning again that our phrase-based baseline features a 9-gram wordclass language model which should be rather prohibitive of recombinations. If recombination was only allowed for hypotheses with the same partial translations, results were considerably worse."
    }, {
      "heading" : "6.1 Speed",
      "text" : "Translation speed is difficult to compare across systems (Table 2). Even with three GPUs our system is ten times slower than than a pure PB-SMT system running with 24 CPU-threads. It is however unclear at this moment if the large stack sizes we use are really necessary. Significant speed-up might be achieved for smaller stacks."
    }, {
      "heading" : "6.2 Submitted results",
      "text" : "Table 1 summarizes the results for our experiments. BLEU scores are reported for the newstest2015 and newstest-2016 test sets.\nOur baseline phrase-based systems (PB) are quite competitive when comparing to the best results of last year’s WMT (24.4 and 27.9 for English-Russian and Russian-English, respectively). NMT-4 is the best pure neural ensemble from Sennrich et al. (2016) for both translation directions. Due to memory restrictions, we were not able to use all four models as separate feature functions and limit ourselves to the best two models for English-Russian and best three for RussianEnglish. The pure neural ensembles are NMT-2 (en-ru) and NMT-3 (ru-en), respectively.\nFor English-Russian, our results stay behind the pure-neural 4-ensemble NMT-4 in terms of BLEU. In a direct comparison between ensembles of 2 models (PB+NMT-2 and NMT-2), we actually reach similar BLEU scores. However, in the manual evaluation our system is best restricted system, tied with the neural system. Absolute TrueSkill scores are even slightly higher for our system.\nFor Russian-English the best-performing pure neural system NMT-4 and the phrase-based baseline are only 0.5% BLEU apart. Adding three NMT models as feature functions to Moses results in a 1.1% BLEU improvement over the neural model and 1.6% over the phrase-based system. The systems PB-NMT-2 (en-ru) and PB-NMT3 (ru-en) are our submissions to the WMT-2016 news translation task. PB-NMT-3 scores the top BLEU results for Russian-English. In the manual\nevaluation, our system is the best restricted system in its own cluster."
    }, {
      "heading" : "6.3 Follow-up experiments",
      "text" : "Frustrated by the limited memory of our GPU cards and against better knowledge6, we computed the element-wise average of all model weights in the NMT ensembles and saved the resulting model. Interestingly, the performance of these new models (NMT-4-Avg) is not much worse than the actual ensemble (NMT-4), while being four times smaller and four times faster at decodetime. The average models outperforms any single model or the smaller 2-ensembles. All models taking part in the average are parameter dumps saved at different points in time during the same training run. This seem to be an interesting results for model compression and deployment settings. We can also average more models: for the Russian-English direction we experiment with the parameter-wise average of ten models (NMT10-Avg) which even slightly outperforms the real four-model ensemble NMT-4.\nWith this smaller model it is easier to tune and deploy our feature function. The performance of our combined setup improves for both translation directions. For English-Russian, however, the pure NMT system (NMT-4) remains ahead of our WMT 2016 submission. For Russian-English we get another improvement of 0.8 BLEU, which sets the new state-of-the-art for this direction."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 644333 (TraMOOC) and 688139 (SUMMA) and was partially funded by the Amazon Academic Research Awards programme."
    } ],
    "references" : [ {
      "title" : "Investigations on phrasebased decoding with recurrent neural network language and translation models",
      "author" : [ "Felix Rietig", "Hermann Ney" ],
      "venue" : "In Proceedings of the Tenth Workshop on Statistical Machine Translation,",
      "citeRegEx" : "Alkhouli et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Alkhouli et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural Machine Translation by Jointly Learning to Align and Translate",
      "author" : [ "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations (ICLR)",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Batch tuning strategies for statistical machine translation",
      "author" : [ "Cherry", "Foster2012] Colin Cherry", "George Foster" ],
      "venue" : "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Cherry et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cherry et al\\.",
      "year" : 2012
    }, {
      "title" : "Can Markov models over minimal translation units help phrase-based SMT",
      "author" : [ "Alexander Fraser", "Helmut Schmid", "Hieu Hoang", "Philipp Koehn" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Durrani et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Durrani et al\\.",
      "year" : 2013
    }, {
      "title" : "Integrating an unsupervised transliteration model into statistical machine translation",
      "author" : [ "Hassan Sajjad", "Hieu Hoang", "Philipp Koehn" ],
      "venue" : "In Proceedings of the 14th Conference of the European Chapter of the Asso-",
      "citeRegEx" : "Durrani et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Durrani et al\\.",
      "year" : 2014
    }, {
      "title" : "Scalable modified Kneser-Ney language model estimation",
      "author" : [ "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn" ],
      "venue" : "In Proceedings of the 51st Annual Meeting of the ACL,",
      "citeRegEx" : "Heafield et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Heafield et al\\.",
      "year" : 2013
    }, {
      "title" : "Moses: Open source toolkit for statistical machine translation",
      "author" : [ "Herbst." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the ACL, pages 177–180. ACL.",
      "citeRegEx" : "Herbst.,? 2007",
      "shortCiteRegEx" : "Herbst.",
      "year" : 2007
    }, {
      "title" : "Statistical Machine Translation",
      "author" : [ "Philipp Koehn" ],
      "venue" : null,
      "citeRegEx" : "Koehn.,? \\Q2010\\E",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2010
    }, {
      "title" : "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781",
      "author" : [ "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Tomas Mikolov", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "Pascanu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving Neural Machine Translation Models with Monolingual Data",
      "author" : [ "Barry Haddow", "Alexandra Birch" ],
      "venue" : null,
      "citeRegEx" : "Sennrich et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural Machine Translation of Rare Words with Subword Units. CoRR, abs/1508.07909",
      "author" : [ "Barry Haddow", "Alexandra Birch" ],
      "venue" : null,
      "citeRegEx" : "Sennrich et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Edinburgh Neural Machine Translation Systems for WMT 16",
      "author" : [ "Barry Haddow", "Alexandra Birch" ],
      "venue" : "In Proc. of the Conference on Machine Translation",
      "citeRegEx" : "Sennrich et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Weight averaging for neural networks and local resampling schemes",
      "author" : [ "Joachim Utans" ],
      "venue" : "In Proc. AAAI-96 Workshop on Integrating Multiple Learned Models,",
      "citeRegEx" : "Utans.,? \\Q1996\\E",
      "shortCiteRegEx" : "Utans.",
      "year" : 1996
    }, {
      "title" : "ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701",
      "author" : [ "Matthew D. Zeiler" ],
      "venue" : null,
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "For these experiments we re-implemented the inference step of the models described in Bahdanau et al. (2015) (more exactly the DL4MT1 variant also present in Nematus2) in efficient C++/CUDA code that can be directly",
      "startOffset" : 86,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "As we reuse the neural systems from Sennrich et al. (2016), we follow their preprocessing scheme for the phrase-based systems as well.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "transliteration mechanisms (Durrani et al., 2014).",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "As mentioned before, we reuse the EnglishRussian and Russian-English NMT models from Sennrich et al. (2016) and refer the reader to that paper for a more detailed description of these systems.",
      "startOffset" : 85,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "The neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which has been trained with Nematus.",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "0 (Pascanu et al., 2013).",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 14,
      "context" : "Models were trained with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs.",
      "startOffset" : 34,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "For our experiments with PB-SMT integration, we chose the same four models that constituted the best-scoring ensemble from Sennrich et al. (2016). If less than four models were used, we chose the models with the highest BLEU scores among these four models as measured on a development set.",
      "startOffset" : 123,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "we add a 5-gram operation sequence model (Durrani et al., 2013).",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "All language models are 5-gram models with Modified Kneser-Ney smoothing and without pruning thresholds (Heafield et al., 2013).",
      "startOffset" : 104,
      "endOffset" : 127
    }, {
      "referenceID" : 8,
      "context" : "The same concatenated files and pruning settings are used to create a 9-gram word-class language model with 200 word-classes produced by word2vec (Mikolov et al., 2013).",
      "startOffset" : 146,
      "endOffset" : 168
    }, {
      "referenceID" : 0,
      "context" : "Our work differs from Alkhouli et al. (2015) in the following aspects:",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "While Alkhouli et al. (2015) integrate RNNbased translation models in phrase-based decoding, this work is to our knowledge the first to integrate soft-attention models.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "Our implementation is GPU-based and our algorithms being tailored towards GPU computations require very different caching strategies from those proposed in Alkhouli et al. (2015). Our implementation seems to be about 10 times faster on one GPU, 30 times faster when three GPUs are used.",
      "startOffset" : 156,
      "endOffset" : 179
    }, {
      "referenceID" : 7,
      "context" : "Figure 3 contains our complete proposal for two-pass stack decoding, a modification of the original stack decoding algorithm described in Koehn (2010). We dissect stack decoding into smaller reusable pieces that can be passed func1: procedure TWOPASSSTACKDECODING 2: Place empty hypothesis h0 into stack S0 3: for stack S in stacks do",
      "startOffset" : 138,
      "endOffset" : 151
    }, {
      "referenceID" : 12,
      "context" : "3 NMT-4 (Sennrich et al., 2016) 27.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 12,
      "context" : "8 NMT-4 (Sennrich et al., 2016) 28.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "In this paper, similar as Alkhouli et al. (2015), we ignore the implications of the infinite neural state and hypothesis recombination in the face of infinite state.",
      "startOffset" : 26,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "NMT-4 is the best pure neural ensemble from Sennrich et al. (2016) for both translation directions.",
      "startOffset" : 44,
      "endOffset" : 67
    } ],
    "year" : 2016,
    "abstractText" : "This paper describes the AMU-UEDIN submissions to the WMT 2016 shared task on news translation. We explore methods of decode-time integration of attention-based neural translation models with phrase-based statistical machine translation. Efficient batch-algorithms for GPU-querying are proposed and implemented. For English-Russian, our system stays behind the state-of-the-art pure neural models in terms of BLEU. Among restricted systems, manual evaluation places it in the first cluster tied with the pure neural model. For the Russian-English task, our submission achieves the top BLEU result, outperforming the best pure neural system by 1.1 BLEU points and our own phrase-based baseline by 1.6 BLEU. After manual evaluation, this system is the best restricted system in its own cluster. In follow-up experiments we improve results by additional 0.8 BLEU.",
    "creator" : "LaTeX with hyperref package"
  }
}