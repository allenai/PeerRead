{
  "name" : "1605.02592.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "GLEU Without Tuning",
    "authors" : [ "Courtney Napoles", "Keisuke Sakaguchi", "Matt Post", "Joel Tetreault" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "GLEU (Generalized Language Understanding Evaluation)2 was designed and developed using two sets of annotations as references, with a tunable weight to penalize n-grams that should have been changed in the system output but were left unchanged (Napoles et al., 2015). After publication, it was observed that the weight needed to be re-tuned as the number of references changed. With more references, more variations of the sentence are seen which results in a larger set of reference n-grams. Larger sets of reference n-grams tend to have higher overlap with the source ngrams, which decreases the number of n-grams that were seen in the source but not the reference. Because of this, the penalty term decreases and a large weight is needed for the penalty term to have the same magnitude as the penalty when there are fewer references.\nAs re-tuning the weight for different sized reference sets is undesirable, we simplified GLEU so that there is no tuning needed and the metric is\n1Download available at https://github.com/ cnap/gec-ranking/.\n2Not to be confused with the method of the same name presented in Mutton et al. (2007).\nportable across comparisons against any number of references."
    }, {
      "heading" : "2 Modifications to GLEU",
      "text" : "Our GLEU implementation differs from that of Napoles et al. (2015). As originally presented, in computing n-gram precision, GLEU doublecounts n-grams in the reference that do not appear in the source, and it subtracts a weighted count of n-grams that appear in the source (S) and not the reference (R). We use a modified version, GLEU+, that simplifies this. Precision is simply the number of reference n-gram matches, minus the counts of n-grams found more often in the source than the reference (Equation 1). GLEU+ follows the same intuition as the original GLEU: overlap between S and R should be rewarded and n-grams that should have been changed in S but were not should be penalized.\nThe precision term in Equation 1 is then used in the standard BLEU equation (Papineni et al., 2002) to get the GLEU+ score. Because the number of possible reference n-grams increases as more reference sets are used, we calculate an intermediate GLEU+ by randomly sample from one of the references for each sentence, and report the mean score over 500 iterations. It takes less than 30 seconds to evaluate 1,000 sentences using 500 iterations."
    }, {
      "heading" : "3 Results",
      "text" : "Using this revised version of GLEU, we calculated the scores for each system submitted to the CoNLL 2014–Shared Task on Grammatical Error Correction3 to update the results reported in Tables 4 and 5 of Napoles et al. (2015). The system ranking by GLEU+ is compared to the originally reported GLEU (GLEU0), M2, and the human ranking (Table 1).\n3http://www.comp.nus.edu.sg/˜nlp/ conll14st.html\nar X\niv :1\n60 5.\n02 59\n2v 1\n[ cs\n.C L\n] 9\nM ay\n2 01\n6\nOn average, M2 ranks systems within 3.4 places of the human ranking. Both GLEU scores have closer rankings on average: GLEU0 within 2.6 and GLEU+ within 2.9 places of the human ranking.\nThe correlation between the system scores and the human ranking is shown in Table 2. GLEU+ has slightly stronger correlation with the human ranking than GLEU0, which is significantly greater than the human correlation with M2, however the rank correlation of GLEU+ is weaker than GLEU0 and M2."
    }, {
      "heading" : "4 Conclusion",
      "text" : "We recommend that the originally presented GLEU no longer be used due to the issues we identified in Section 1. The updated version of GLEU that does not require tuning (GLEU+) should be used instead. The code is available at https://github.com/cnap/ gec-ranking."
    } ],
    "references" : [ {
      "title" : "Gleu: Automatic evaluation of sentence-level fluency",
      "author" : [ "Andrew Mutton", "Mark Dras", "Stephen Wan", "Robert Dale." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 344–351, Prague, Czech Repub-",
      "citeRegEx" : "Mutton et al\\.,? 2007",
      "shortCiteRegEx" : "Mutton et al\\.",
      "year" : 2007
    }, {
      "title" : "Ground truth for grammatical error correction metrics",
      "author" : [ "Courtney Napoles", "Keisuke Sakaguchi", "Matt Post", "Joel Tetreault." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Napoles et al\\.,? 2015",
      "shortCiteRegEx" : "Napoles et al\\.",
      "year" : 2015
    }, {
      "title" : "BLEU: A method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "specific annotated errors (Napoles et al., 2015).",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "unchanged (Napoles et al., 2015).",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Not to be confused with the method of the same name presented in Mutton et al. (2007). portable across comparisons against any number of references.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "Our GLEU implementation differs from that of Napoles et al. (2015). As originally presented, in computing n-gram precision, GLEU doublecounts n-grams in the reference that do not appear",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "The precision term in Equation 1 is then used in the standard BLEU equation (Papineni et al., 2002) to get the GLEU+ score.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 1,
      "context" : "Using this revised version of GLEU, we calculated the scores for each system submitted to the CoNLL 2014–Shared Task on Grammatical Error Correction3 to update the results reported in Tables 4 and 5 of Napoles et al. (2015). The system ranking by GLEU+ is compared to the originally reported GLEU (GLEU0), M2, and the human ranking (Table 1).",
      "startOffset" : 202,
      "endOffset" : 224
    } ],
    "year" : 2016,
    "abstractText" : "The GLEU metric was proposed for evaluating grammatical error corrections using n-gram overlap with a set of reference sentences, as opposed to precision/recall of specific annotated errors (Napoles et al., 2015). This paper describes improvements made to the GLEU metric that address problems that arise when using an increasing number of reference sets. Unlike the originally presented metric, the modified metric does not require tuning. We recommend that this version be used instead of the original version.1",
    "creator" : "LaTeX with hyperref package"
  }
}