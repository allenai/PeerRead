{
  "name" : "1611.04361.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Attending to Characters in Neural Sequence Labeling Models",
    "authors" : [ "Marek Rei", "Gamal K.O. Crichton", "Sampo Pyysalo" ],
    "emails" : [ "marek.rei@cl.cam.ac.uk", "gkoc2@cam.ac.uk", "smp66@cam.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word- or character-level component. We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters."
    }, {
      "heading" : "1 Introduction",
      "text" : "Many NLP tasks, including named entity recognition (NER), part-of-speech (POS) tagging and shallow parsing can be framed as types of sequence labeling. The development of accurate and efficient sequence labeling models is thereby useful for a wide range of downstream applications. Work in this area has traditionally involved task-specific feature engineering – for example, integrating gazetteers for named entity recognition, or using features from a morphological analyser in POS-tagging. Recent developments in neural architectures and representation learning have opened the door to models that can discover useful features automatically from the data. Such sequence labeling systems are applicable to many tasks, using only the surface text as input, yet are able to achieve competitive results (Collobert et al., 2011; Irsoy and Cardie, 2014).\nCurrent neural models generally make use of word embeddings, which allow them to learn similar representations for semantically or functionally similar words. While this is an important improvement over count-based models, they still have weaknesses that should be addressed. The most obvious problem arises when dealing with out-of-vocabulary (OOV) words – if a token has never been seen before, then it does not have an embedding and the model needs to back-off to a generic OOV representation. Words that have been seen very infrequently have embeddings, but they will likely have low quality due to lack of training data. The approach can also be sub-optimal in terms of parameter usage – for example, certain suffixes indicate more likely POS tags for these words, but this information gets encoded into each individual embedding as opposed to being shared between the whole vocabulary.\nIn this paper, we construct a task-independent neural network architecture for sequence labeling, and then extend it with two different approaches for integrating character-level information. By operating on individual characters, the model is able to infer representations for previously unseen words and share information about morpheme-level regularities. We propose a novel architecture for combining character-level representations with word embeddings using a gating mechanism, also referred to as attention, which allows the model to dynamically decide which source of information to use for each word. In addition, we describe a new objective for model training where the character-level representations are optimised to mimic the current state of word embeddings.\nThis work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/\nar X\niv :1\n61 1.\n04 36\n1v 1\n[ cs\n.C L\n] 1\n4 N\nov 2\n01 6\nWe evaluate the neural models on 8 datasets from the fields of NER, POS-tagging, chunking and error detection in learner texts. Our experiments show that including a character-based component in the sequence labeling model provides substantial performance improvements on all the benchmarks. In addition, the attention-based architecture achieves the best results on all evaluations, while requiring a smaller number of parameters."
    }, {
      "heading" : "2 Bidirectional LSTM for sequence labeling",
      "text" : "We first describe a basic word-level neural network for sequence labeling, following the models described by Lample et al. (2016) and Rei and Yannakoudakis (2016), and then propose two alternative methods for incorporating character-level information.\nFigure 1 shows the general architecture of the sequence labeling network. The model receives a sequence of tokens (w1, ..., wT ) as input, and predicts a label corresponding to each of the input tokens. The tokens are first mapped to a distributed vector space, resulting in a sequence of word embeddings (x1, ..., xT ). Next, the embeddings are given as input to two LSTM (Hochreiter and Schmidhuber, 1997) components moving in opposite directions through the text, creating context-specific representations. The respective forward- and backward-conditioned representations are concatenated for each word position, resulting in representations that are conditioned on the whole sequence:\n−→ ht = LSTM(xt, −−→ ht−1)\n←− ht = LSTM(xt, ←−− ht+1) ht = [ −→ ht ; ←− ht ] (1)\nWe include an extra narrow hidden layer on top of the LSTM, which proved to be a useful modification based on development experiments. An additional hidden layer allows the model to detect higher-level feature combinations, while constraining it to be small forces it to focus on more generalisable patterns:\ndt = tanh(Wdht) (2)\nwhere Wd is a weight matrix between the layers, and the size of dt is intentionally kept small. Finally, to produce label predictions, we use either a softmax layer or a conditional random field (CRF, Lafferty et al. (2001)). The softmax calculates a normalised probability distribution over all the possible labels for each word:\nP (yt = k|dt) = eWo,kdt∑\nk̃∈K e Wo,k̃dt\n(3)\nwhere P (yt = k|dt) is the probability of the label of the t-th word (yt) being k, K is the set of all possible labels, and Wo,k is the k-th row of output weight matrix Wo. To optimise this model, we minimise categorical crossentropy, which is equivalent to minimising the negative log-probability of the correct labels:\nE = − T∑ t=1 log(P (yt|dt)) (4)\nFollowing Huang et al. (2015), we can also use a CRF as the output layer, which conditions each prediction on the previously predicted label. In this architecture, the last hidden layer is used to predict confidence scores for the word having each of the possible labels. A separate weight matrix is used to learn transition probabilities between different labels, and the Viterbi algorithm is used to find an optimal sequence of weights. Given that y is a sequence of labels [y1, ..., yT ], then the CRF score for this sequence can be calculated as:\ns(y) = T∑ t=1 At,yt + T∑ t=0 Byt,yt+1 (5)\nAt,yt =Wo,ytdt (6)\nwhere At,yt shows how confident the network is that the label on the t-th word is yt. Byt,yt+1 shows the likelihood of transitioning from label yt to label yt+1, and these values are optimised during training. The output from the model is the sequence of labels with the largest score s(y), which can be found efficiently using the Viterbi algorithm. In order to optimise the CRF model, the loss function maximises the score for the correct label sequence, while minimising the scores for all other sequences:\nE = −s(y) + log ∑ ỹ∈Ỹ es(ỹ) (7)\nwhere Ỹ is the set of all possible label sequences."
    }, {
      "heading" : "3 Character-level sequence labeling",
      "text" : "Distributed embeddings map words into a space where semantically similar words have similar vector representations, allowing the models to generalise better. However, they still treat words as atomic units and ignore any surface- or morphological similarities between different words. By constructing models that operate over individual characters in each word, we can take advantage of these regularities. This can be particularly useful for handling unseen words – for example, if we have never seen the word cabinets before, a character-level model could still infer a representation for this word if it has previously seen the word cabinet and other words with the suffix -s. In contrast, a word-level model can only represent this word with a generic out-of-vocabulary representation, which is shared between all other unseen words.\nResearch into character-level models is still in fairly early stages, and models that operate exclusively on characters are not yet competitive to word-level models on most tasks. However, instead of fully replacing word embeddings, we are interested in combining the two approaches, thereby allowing the model to take advantage of information at both granularity levels. The general outline of our approach is shown in Figure 2. Each word is broken down into individual characters, these are then mapped to a sequence of character embeddings (c1, ..., cR), which are passed through a bidirectional LSTM:\n−→ h∗i = LSTM(ci, −−→ h∗i−1)\n←− h∗i = LSTM(ci, ←−− h∗i+1) (8)\nWe then use the last hidden vectors from each of the LSTM components, concatenate them together, and pass the result through a separate non-linear layer.\nh∗ = [ −→ h∗R; ←− h∗1] m = tanh(Wmh ∗) (9)\nwhere Wm is a weight matrix mapping the concatenated hidden vectors from both LSTMs into a joint word representation m, built from individual characters.\nWe now have two alternative feature representations for each word – xt from Section 2 is an embedding learned on the word level, and m(t) is a representation dynamically built from individual characters in\nthe t-th word of the input text. Following Lample et al. (2016), one possible approach is to concatenate the two vectors and use this as the new word-level representation for the sequence labeling model:\nx̃ = [x;m] (10)\nThis approach, also illustrated in Figure 2, assumes that the word-level and character-level components learn somewhat disjoint information, and it is beneficial to give them separately as input to the sequence labeler."
    }, {
      "heading" : "4 Attention over character features",
      "text" : "Alternatively, we can have the word embedding and the character-level component learn the same semantic features for each word. Instead of concatenating them as alternative feature sets, we specifically construct the network so that they would learn the same representations, and then allow the model to decide how to combine the information for each specific word.\nWe first construct the word representation from characters using the same architecture – a bidirectional LSTM operates over characters, and the last hidden states are used to create vector m for the input word. Instead of concatenating this with the word embedding, the two vectors are added together using a weighted sum, where the weights are predicted by a two-layer network:\nz = σ(W (3)z tanh(W (1) z x+W (2) z m)) x̃ = z · x+ (1− z) ·m (11)\nwhere W (1)z , W (2) z and W (3) z are weight matrices for calculating z, and σ() is the logistic function with values in the range [0, 1]. The vector z has the same dimensions as x or m, acting as the weight between the two vectors. It allows the model to dynamically decide how much information to use from the character-level component or from the word embedding. This decision is done for each feature separately, which adds extra flexiblity – for example, words with regular suffixes can share some character-level features, whereas irregular words can store exceptions into word embeddings. Furthermore, previously unknown words are able to use character-level regularities whenever possible, and are still able to revert to using the generic OOV token when necessary.\nThe main benefits of character-level modeling are expected to come from improved handling of rare and unseen words, whereas frequent words are likely able to learn high-quality word-level embeddings directly. We would like to take advantage of this, and train the character component to predict these word embeddings. Our attention-based architecture requires the learned features in both word representations to align, and we can add in an extra constraint to encourage this. During training, we add a term to the loss function that optimises the vector m to be similar to the word embedding x:\nẼ = E + T∑ t=1 gt(1− cos(m(t), xt)) gt = { 0, if wt = OOV 1, otherwise\n(12)\nEquation 12 maximises the cosine similarity between m(t) and xt. Importantly, this is done only for words that are not out-of-vocabulary – we want the character-level component to learn from the word embeddings, but this should exclude the OOV embedding, as it is shared between many words. We use gt to set this cost component to 0 for any OOV tokens.\nWhile the character component learns general regularities that are shared between all the words, individual word embeddings provide a way for the model to store word-specific information and any exceptions. Therefore, while we want the character-based model to shift towards predicting high-quality word embeddings, it is not desireable to optimise the word embeddings towards the character-level representations. This can be achieved by making sure that the optimisation is performed only in one direction; in Theano (Bergstra et al., 2010), the disconnected grad function gives the desired effect."
    }, {
      "heading" : "5 Datasets",
      "text" : "We evaluate the sequence labeling models and character architectures on 8 different datasets. Table 1 contains information about the number of labels and dataset sizes for each of them.\n• CoNLL00: The CoNLL-2000 dataset (Tjong Kim Sang and Buchholz, 2000) is a frequently used benchmark for the task of chunking. Wall Street Journal Sections 15-18 from the Penn Treebank are used for training, and Section 20 as the test data. As there is no official development set, we separated some of the training set for this purpose.\n• CoNLL03: The CoNLL-2003 corpus (Tjong Kim Sang and De Meulder, 2003) was created for the shared task on language-independent NER. We use the English section of the dataset, containing news stories from the Reuters Corpus1.\n• PTB-POS: The Penn Treebank POS-tag corpus (Marcus et al., 1993) contains texts from the Wall Street Journal, annotated for part-of-speech tags. The PTB label set includes 36 main tags and an additional 12 tags covering items such as punctuation.\n• FCEPUBLIC: The publicly released subset of the First Certificate in English (FCE) dataset contains short essays written by language learners and manual corrections by examiners (Yannakoudakis et al., 2011). We use a version of this corpus converted into a binary error detection task, where each token is labeled as being correct or incorrect in the given context.\n• BC2GM: The BioCreative II Gene Mention corpus (Smith et al., 2008) consists of 20,000 sentences from biomedical publication abstracts and is annotated for mentions of the names of genes, proteins and related entities using a single NE class.\n1http://about.reuters.com/researchandstandards/corpus/\n• CHEMDNER: The BioCreative IV Chemical and Drug (Krallinger et al., 2015) NER corpus consists of 10,000 abstracts annotated for mentions of chemical and drug names using a single class. We make use of the official splits provided by the shared task organizers.\n• JNLPBA: The JNLPBA corpus (Kim et al., 2004) consists of 2,404 biomedical abstracts and is annotated for mentions of five entity types: CELL LINE, CELL TYPE, DNA, RNA, and PROTEIN. The corpus was derived from GENIA corpus entity annotations for use in the shared task organized in conjuction with the BioNLP 2004 workshop.\n• GENIA-POS: The GENIA corpus (Ohta et al., 2002) is one of the most widely used resources for biomedical NLP and has a rich set of annotations including parts of speech, phrase structure syntax, entity mentions, and events. Here, we make use of the GENIA POS annotations, which cover 2,000 PubMed abstracts (approx. 20,000 sentences). We use the same 210-document test set as Tsuruoka et al. (2005), and additionally split off a sample of 210 from the remaining documents as a development set."
    }, {
      "heading" : "6 Experiment settings",
      "text" : "For data prepocessing, all digits were replaced with the character ’0’. Any words that occurred only once in the training data were replaced by the generic OOV token for word embeddings, but were still used in the character-level components. The word embeddings were initialised with publicly available pretrained vectors, created using word2vec (Mikolov et al., 2013), and then fine-tuned during model training. For the general-domain datasets we used 300-dimensional vectors trained on Google News2; for the biomedical datasets we used 200-dimensional vectors trained on PubMed and PMC3. The embeddings for characters were set to length 50 and initialised randomly.\nThe LSTM layer size was set to 200 in each direction for both word- and character-level components. The hidden layer d has size 50, and the combined representation m has the same length as the word embeddings. CRF was used as the output layer for all the experiments – we found that this gave most benefits to tasks with larger numbers of possible labels. Parameters were optimised using AdaDelta (Zeiler, 2012) with default learning rate 1.0 and sentences were grouped into batches of size 64. Performance on the development set was measured at every epoch and training was stopped if performance had not improved for 7 epochs; the best-performing model on the development set was then used for evalua-\n2https://code.google.com/archive/p/word2vec/ 3http://bio.nlplab.org/\ntion on the test set. In order to avoid any outlier results due to randomness in the model initialisation, we trained each configuration with 10 different random seeds and present here the averaged results.\nWhen evaluating on each dataset, we report the measures established in previous work. Token-level accuracy is used for PTB-POS and GENIA-POS; F0.5 score over the erroneous words for FCEPUBLIC; the official evaluation script for BC2GM which allows for alternative correct entity spans; and microaveraged mention-level F1 score for the remaining datasets."
    }, {
      "heading" : "7 Results",
      "text" : "While optimising the hyperparameters for each dataset separately would likely improve individual performance, we conduct more controlled experiments on a task-independent model. Therefore, we use the same hyperparameters from Section 6 on all datasets, and the development set is only used for the stopping condition. With these experiments, we wish to determine 1) on which sequence labeling tasks do character-based models offer an advantange, and 2) which character-based architecture performs better.\nResults for the different model architectures on all 8 datasets are shown in Table 2. As can be seen, including a character-based component in the sequence labeling architecture improves performance on every benchmark. The NER datasets have the largest absolute improvement – the model is able to learn character-level patterns for names, and also improve the handling of any previously unseen tokens.\nCompared to concatenating the word- and character-level representations, the attention-based character model outperforms the former on all evaluations. The mechanism for dynamically deciding how much character-level information to use allows the model to better handle individual word representations, giving it an advantage in the experiments. Visualisation of the attention values in Figure 3 shows that the model is actively using character-based features, and the attention areas vary between different words.\nThe results of this general tagging architecture are competitive, even when compared to previous work using hand-crafted features. The network achieves 97.27% on PTB-POS compared to 97.55% by Huang et al. (2015), and 72.70% on JNLPBA compared to 72.55% by Zhou and Su (2004). In some cases, we are also able to beat the previous best results – 87.99% on BC2GM compared to 87.48% by Campos et al. (2015), and 41.88% on FCEPUBLIC compared to 41.1% by Rei and Yannakoudakis (2016). Lample et al. (2016) report a considerably higher result of 90.94% on CoNLL03, indicating that the chosen hyperparameters for the baseline system are suboptimal for this specific task. Compared to the experiments presented here, their model used the IOBES tagging scheme instead of the original IOB, and embeddings pretrained with a more specialised method that accounts for word order.\nIt is important to also compare the parameter counts of alternative neural architectures, as this shows their learning capacity and indicates their time requirements in practice. Table 3 contains the parameter counts on three representative datasets. While keeping the model hyperparameters constant, the character-level models require additional parameters for the character composition and character embeddings. However, the attention-based model uses fewer parameters compared to the concatenation approach. When the two representations are concatenated, the overall word representation size is increased, which in turn increases the number of parameters required for the word-level bidirectional LSTM. Therefore, the attention-based character architecture achieves improved results even with a smaller parameter footprint."
    }, {
      "heading" : "8 Related work",
      "text" : "There is a wide range of previous work on constructing and optimising neural architectures applicable to sequence labeling. Collobert et al. (2011) described one of the first task-independent neural tagging models using convolutional neural networks. They were able to achieve good results on POS tagging, chunking, NER and semantic role labeling, without relying on hand-engineered features. Irsoy and Cardie (2014) experimented with multi-layer bidirectional Elman-style recurrent networks, and found that the deep models outperformed conditional random fields on the task of opinion mining. Huang et al. (2015) described a bidirectional LSTM model with a CRF layer, which included hand-crafted features specialised for the task of named entity recognition. Rei and Yannakoudakis (2016) evaluated a range of neural architectures, including convolutional and recurrent networks, on the task of error detection in learner writing. The word-level sequence labeling model described in this paper follows the previous work, combining useful design choices from each of them. In addition, we extended the model with two alternative character-level architectures, and evaluated its performance on 8 different datasets.\nCharacter-level models have the potential of capturing morpheme patterns, thereby improving generalisation on both frequent and unseen words. In recent years, there has been an increase in research into these models, resulting in several interesting applications. Ling et al. (2015b) described a character-level neural model for machine translation, performing both encoding and decoding on individual characters. Kim et al. (2016) implemented a language model where encoding is performed by a convolutional network and LSTM over characters, whereas predictions are given on the word-level. Cao and Rei (2016) proposed a method for learning both word embeddings and morphological segmentation with a bidirectional recurrent network over characters. There is also research on performing parsing (Ballesteros et al., 2015) and text classification (Zhang et al., 2015) with character-level neural models. Ling et al. (2015a) proposed a neural architecture that replaces word embeddings with dynamically-constructed characterbased representations. We applied a similar method for operating over characters, but combined them with word embeddings instead of replacing them, as this allows the model to benefit from both approaches. Lample et al. (2016) described a model where the character-level representation is combined with word embeddings through concatenation. In this work, we proposed an alternative architecture, where the representations are combined using an attention mechanism, and evaluated both approaches on a range of tasks and datasets. Recently, Miyamoto and Cho (2016) have also described a related method for the task of language modelling, combining characters and word embeddings using gating."
    }, {
      "heading" : "9 Conclusion",
      "text" : "Developments in neural network research allow for model architectures that work well on a wide range of sequence labeling datasets without requiring hand-crafted data. While word-level representation learning is a powerful tool for automatically discovering useful features, these models still come with certain weaknesses – rare words have low-quality representations, previously unseen words cannot be modeled at all, and morpheme-level information is not shared with the whole vocabulary.\nIn this paper, we investigated character-level model components for a sequence labeling architecture, which allow the system to learn useful patterns from sub-word units. In addition to a bidirectional LSTM operating over words, a separate bidirectional LSTM is used to construct word representations from\nindividual characters. We proposed a novel architecture for combining the character-based representation with the word embedding by using an attention mechanism, allowing the model to dynamically choose which information to use from each information source. In addition, the character-level composition function is augmented with a novel training objective, optimising it to predict representations that are similar to the word embeddings in the model.\nThe evaluation was performed on 8 different sequence labeling datasets, covering a range of tasks and domains. We found that incorporating character-level information into the model improved performance on every benchmark, indicating that capturing features regarding characters and morphmes is indeed useful in a general-purpose tagging system. In addition, the attention-based model for combining character representations outperformed the concatenation method used in previous work in all evaluations. Even though the proposed method requires fewer parameters, the added ability of controlling how much character-level information is used for each word has led to improved performance on a range of different tasks."
    } ],
    "references" : [ {
      "title" : "Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs",
      "author" : [ "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Ballesteros et al\\.,? 2015",
      "shortCiteRegEx" : "Ballesteros et al\\.",
      "year" : 2015
    }, {
      "title" : "Theano: a CPU and GPU math compiler in Python",
      "author" : [ "James Bergstra", "Olivier Breuleux", "Frederic Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Python for Scientific Computing Conference (SciPy).",
      "citeRegEx" : "Bergstra et al\\.,? 2010",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2010
    }, {
      "title" : "A document processing pipeline for annotating chemical entities in scientific documents",
      "author" : [ "David Campos", "Sergio Matos", "Jose L. Oliveira." ],
      "venue" : "Journal of Cheminformatics, 7.",
      "citeRegEx" : "Campos et al\\.,? 2015",
      "shortCiteRegEx" : "Campos et al\\.",
      "year" : 2015
    }, {
      "title" : "A Joint Model for Word Embedding and Word Morphology",
      "author" : [ "Kris Cao", "Marek Rei." ],
      "venue" : "Proceedings of the 1st Workshop on Representation Learning for NLP (RepL4NLP-2016).",
      "citeRegEx" : "Cao and Rei.,? 2016",
      "shortCiteRegEx" : "Cao and Rei.",
      "year" : 2016
    }, {
      "title" : "Natural Language Processing (Almost) from Scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of Machine Learning Research, 12.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Long Short-term Memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Bidirectional LSTM-CRF Models for Sequence Tagging",
      "author" : [ "Zhiheng Huang", "Wei Xu", "Kai Yu." ],
      "venue" : "arXiv:1508.01991.",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Opinion Mining with Deep Recurrent Neural Networks",
      "author" : [ "Ozan Irsoy", "Claire Cardie." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Irsoy and Cardie.,? 2014",
      "shortCiteRegEx" : "Irsoy and Cardie.",
      "year" : 2014
    }, {
      "title" : "Introduction to the Bio-entity Recognition Task at JNLPBA",
      "author" : [ "Jin-Dong Kim", "Tomoko Ohta", "Yoshimasa Tsuruoka", "Yuka Tateisi", "Nigel Collier." ],
      "venue" : "Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and Its Applications.",
      "citeRegEx" : "Kim et al\\.,? 2004",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2004
    }, {
      "title" : "Character-Aware Neural Language Models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI16).",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "CHEMDNER: The drugs and chemical names extraction challenge",
      "author" : [ "Martin Krallinger", "Florian Leitner", "Obdulia Rabal", "Miguel Vazquez", "Julen Oyarzabal", "Alfonso Valencia." ],
      "venue" : "Journal of Cheminformatics, 7(Suppl 1).",
      "citeRegEx" : "Krallinger et al\\.,? 2015",
      "shortCiteRegEx" : "Krallinger et al\\.",
      "year" : 2015
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando Pereira." ],
      "venue" : "Proceedings of the 18th International Conference on Machine Learning.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Neural Architectures for Named Entity Recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of NAACL-HLT 2016.",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation",
      "author" : [ "Wang Ling", "Tiago Luı́s", "Luı́s Marujo", "Ramón Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W. Black", "Isabel Trancoso" ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Ling et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Character-based Neural Machine Translation",
      "author" : [ "Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black." ],
      "venue" : "arXiv preprint arXiv:1511.04586.",
      "citeRegEx" : "Ling et al\\.,? 2015b",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Building a large annotated corpus of English: The Penn Treebank",
      "author" : [ "Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz." ],
      "venue" : "Computational Linguistics, 19.",
      "citeRegEx" : "Marcus et al\\.,? 1993",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "Tomáš Mikolov", "Greg Corrado", "Kai Chen", "Jeffrey Dean." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR 2013).",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Gated Word-Character Recurrent Language Model",
      "author" : [ "Yasumasa Miyamoto", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:1606.01700.",
      "citeRegEx" : "Miyamoto and Cho.,? 2016",
      "shortCiteRegEx" : "Miyamoto and Cho.",
      "year" : 2016
    }, {
      "title" : "The GENIA corpus: An annotated research abstract corpus in molecular biology domain",
      "author" : [ "Tomoko Ohta", "Yuka Tateisi", "Jin-Dong Kim." ],
      "venue" : "Proceedings of the second international conference on Human Language Technology Research.",
      "citeRegEx" : "Ohta et al\\.,? 2002",
      "shortCiteRegEx" : "Ohta et al\\.",
      "year" : 2002
    }, {
      "title" : "Compositional Sequence Labeling Models for Error Detection in Learner Writing",
      "author" : [ "Marek Rei", "Helen Yannakoudakis." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Rei and Yannakoudakis.,? 2016",
      "shortCiteRegEx" : "Rei and Yannakoudakis.",
      "year" : 2016
    }, {
      "title" : "Overview of BioCreative II gene mention recognition",
      "author" : [ "Jacinto Mata", "W John Wilbur." ],
      "venue" : "Genome biology, 9 Suppl 2.",
      "citeRegEx" : "Mata and Wilbur.,? 2008",
      "shortCiteRegEx" : "Mata and Wilbur.",
      "year" : 2008
    }, {
      "title" : "Introduction to the CoNLL-2000 shared task: Chunking",
      "author" : [ "Erik F. Tjong Kim Sang", "Sabine Buchholz." ],
      "venue" : "Proceedings of the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language Learning, 7.",
      "citeRegEx" : "Sang and Buchholz.,? 2000",
      "shortCiteRegEx" : "Sang and Buchholz.",
      "year" : 2000
    }, {
      "title" : "Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003.",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Developing a robust part-of-speech tagger for biomedical text",
      "author" : [ "Yoshimasa Tsuruoka", "Yuka Tateishi", "Jin Dong Kim", "Tomoko Ohta", "John McNaught", "Sophia Ananiadou", "Jun’ichi Tsujii" ],
      "venue" : "In Proceedings of Panhellenic Conference on Informatics",
      "citeRegEx" : "Tsuruoka et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Tsuruoka et al\\.",
      "year" : 2005
    }, {
      "title" : "A New Dataset and Method for Automatically Grading ESOL Texts",
      "author" : [ "Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.",
      "citeRegEx" : "Yannakoudakis et al\\.,? 2011",
      "shortCiteRegEx" : "Yannakoudakis et al\\.",
      "year" : 2011
    }, {
      "title" : "ADADELTA: An Adaptive Learning Rate Method",
      "author" : [ "Matthew D. Zeiler." ],
      "venue" : "arXiv preprint arXiv:1212.5701.",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "Character-level Convolutional Networks for Text Classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploring Deep Knowledge Resources in Biomedical Name Recognition",
      "author" : [ "GuoDong Zhou", "Jian Su." ],
      "venue" : "Workshop on Natural Language Processing in Biomedicine and Its Applications at COLING.",
      "citeRegEx" : "Zhou and Su.,? 2004",
      "shortCiteRegEx" : "Zhou and Su.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Such sequence labeling systems are applicable to many tasks, using only the surface text as input, yet are able to achieve competitive results (Collobert et al., 2011; Irsoy and Cardie, 2014).",
      "startOffset" : 143,
      "endOffset" : 191
    }, {
      "referenceID" : 7,
      "context" : "Such sequence labeling systems are applicable to many tasks, using only the surface text as input, yet are able to achieve competitive results (Collobert et al., 2011; Irsoy and Cardie, 2014).",
      "startOffset" : 143,
      "endOffset" : 191
    }, {
      "referenceID" : 5,
      "context" : "Next, the embeddings are given as input to two LSTM (Hochreiter and Schmidhuber, 1997) components moving in opposite directions through the text, creating context-specific representations.",
      "startOffset" : 52,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "We first describe a basic word-level neural network for sequence labeling, following the models described by Lample et al. (2016) and Rei and Yannakoudakis (2016), and then propose two alternative methods for incorporating character-level information.",
      "startOffset" : 109,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : "We first describe a basic word-level neural network for sequence labeling, following the models described by Lample et al. (2016) and Rei and Yannakoudakis (2016), and then propose two alternative methods for incorporating character-level information.",
      "startOffset" : 109,
      "endOffset" : 163
    }, {
      "referenceID" : 11,
      "context" : "Finally, to produce label predictions, we use either a softmax layer or a conditional random field (CRF, Lafferty et al. (2001)).",
      "startOffset" : 105,
      "endOffset" : 128
    }, {
      "referenceID" : 6,
      "context" : "Following Huang et al. (2015), we can also use a CRF as the output layer, which conditions each prediction on the previously predicted label.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "Following Lample et al. (2016), one possible approach is to concatenate the two vectors and use this as the new word-level representation for the sequence labeling model:",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "This can be achieved by making sure that the optimisation is performed only in one direction; in Theano (Bergstra et al., 2010), the disconnected grad function gives the desired effect.",
      "startOffset" : 104,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "• PTB-POS: The Penn Treebank POS-tag corpus (Marcus et al., 1993) contains texts from the Wall Street Journal, annotated for part-of-speech tags.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 24,
      "context" : "• FCEPUBLIC: The publicly released subset of the First Certificate in English (FCE) dataset contains short essays written by language learners and manual corrections by examiners (Yannakoudakis et al., 2011).",
      "startOffset" : 179,
      "endOffset" : 207
    }, {
      "referenceID" : 10,
      "context" : "• CHEMDNER: The BioCreative IV Chemical and Drug (Krallinger et al., 2015) NER corpus consists of 10,000 abstracts annotated for mentions of chemical and drug names using a single class.",
      "startOffset" : 49,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "• JNLPBA: The JNLPBA corpus (Kim et al., 2004) consists of 2,404 biomedical abstracts and is annotated for mentions of five entity types: CELL LINE, CELL TYPE, DNA, RNA, and PROTEIN.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 18,
      "context" : "• GENIA-POS: The GENIA corpus (Ohta et al., 2002) is one of the most widely used resources for biomedical NLP and has a rich set of annotations including parts of speech, phrase structure syntax, entity mentions, and events.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "• GENIA-POS: The GENIA corpus (Ohta et al., 2002) is one of the most widely used resources for biomedical NLP and has a rich set of annotations including parts of speech, phrase structure syntax, entity mentions, and events. Here, we make use of the GENIA POS annotations, which cover 2,000 PubMed abstracts (approx. 20,000 sentences). We use the same 210-document test set as Tsuruoka et al. (2005), and additionally split off a sample of 210 from the remaining documents as a development set.",
      "startOffset" : 31,
      "endOffset" : 400
    }, {
      "referenceID" : 16,
      "context" : "The word embeddings were initialised with publicly available pretrained vectors, created using word2vec (Mikolov et al., 2013), and then fine-tuned during model training.",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 25,
      "context" : "Parameters were optimised using AdaDelta (Zeiler, 2012) with default learning rate 1.",
      "startOffset" : 41,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "55% by Huang et al. (2015), and 72.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "55% by Huang et al. (2015), and 72.70% on JNLPBA compared to 72.55% by Zhou and Su (2004). In some cases, we are also able to beat the previous best results – 87.",
      "startOffset" : 7,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "48% by Campos et al. (2015), and 41.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "48% by Campos et al. (2015), and 41.88% on FCEPUBLIC compared to 41.1% by Rei and Yannakoudakis (2016). Lample et al.",
      "startOffset" : 7,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "48% by Campos et al. (2015), and 41.88% on FCEPUBLIC compared to 41.1% by Rei and Yannakoudakis (2016). Lample et al. (2016) report a considerably higher result of 90.",
      "startOffset" : 7,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "There is also research on performing parsing (Ballesteros et al., 2015) and text classification (Zhang et al.",
      "startOffset" : 45,
      "endOffset" : 71
    }, {
      "referenceID" : 26,
      "context" : ", 2015) and text classification (Zhang et al., 2015) with character-level neural models.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "Collobert et al. (2011) described one of the first task-independent neural tagging models using convolutional neural networks.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "Collobert et al. (2011) described one of the first task-independent neural tagging models using convolutional neural networks. They were able to achieve good results on POS tagging, chunking, NER and semantic role labeling, without relying on hand-engineered features. Irsoy and Cardie (2014) experimented with multi-layer bidirectional Elman-style recurrent networks, and found that the deep models outperformed conditional random fields on the task of opinion mining.",
      "startOffset" : 0,
      "endOffset" : 293
    }, {
      "referenceID" : 2,
      "context" : "Collobert et al. (2011) described one of the first task-independent neural tagging models using convolutional neural networks. They were able to achieve good results on POS tagging, chunking, NER and semantic role labeling, without relying on hand-engineered features. Irsoy and Cardie (2014) experimented with multi-layer bidirectional Elman-style recurrent networks, and found that the deep models outperformed conditional random fields on the task of opinion mining. Huang et al. (2015) described a bidirectional LSTM model with a CRF layer, which included hand-crafted features specialised for the task of named entity recognition.",
      "startOffset" : 0,
      "endOffset" : 490
    }, {
      "referenceID" : 2,
      "context" : "Collobert et al. (2011) described one of the first task-independent neural tagging models using convolutional neural networks. They were able to achieve good results on POS tagging, chunking, NER and semantic role labeling, without relying on hand-engineered features. Irsoy and Cardie (2014) experimented with multi-layer bidirectional Elman-style recurrent networks, and found that the deep models outperformed conditional random fields on the task of opinion mining. Huang et al. (2015) described a bidirectional LSTM model with a CRF layer, which included hand-crafted features specialised for the task of named entity recognition. Rei and Yannakoudakis (2016) evaluated a range of neural architectures, including convolutional and recurrent networks, on the task of error detection in learner writing.",
      "startOffset" : 0,
      "endOffset" : 665
    }, {
      "referenceID" : 2,
      "context" : "Collobert et al. (2011) described one of the first task-independent neural tagging models using convolutional neural networks. They were able to achieve good results on POS tagging, chunking, NER and semantic role labeling, without relying on hand-engineered features. Irsoy and Cardie (2014) experimented with multi-layer bidirectional Elman-style recurrent networks, and found that the deep models outperformed conditional random fields on the task of opinion mining. Huang et al. (2015) described a bidirectional LSTM model with a CRF layer, which included hand-crafted features specialised for the task of named entity recognition. Rei and Yannakoudakis (2016) evaluated a range of neural architectures, including convolutional and recurrent networks, on the task of error detection in learner writing. The word-level sequence labeling model described in this paper follows the previous work, combining useful design choices from each of them. In addition, we extended the model with two alternative character-level architectures, and evaluated its performance on 8 different datasets. Character-level models have the potential of capturing morpheme patterns, thereby improving generalisation on both frequent and unseen words. In recent years, there has been an increase in research into these models, resulting in several interesting applications. Ling et al. (2015b) described a character-level neural model for machine translation, performing both encoding and decoding on individual characters.",
      "startOffset" : 0,
      "endOffset" : 1374
    }, {
      "referenceID" : 2,
      "context" : "Collobert et al. (2011) described one of the first task-independent neural tagging models using convolutional neural networks. They were able to achieve good results on POS tagging, chunking, NER and semantic role labeling, without relying on hand-engineered features. Irsoy and Cardie (2014) experimented with multi-layer bidirectional Elman-style recurrent networks, and found that the deep models outperformed conditional random fields on the task of opinion mining. Huang et al. (2015) described a bidirectional LSTM model with a CRF layer, which included hand-crafted features specialised for the task of named entity recognition. Rei and Yannakoudakis (2016) evaluated a range of neural architectures, including convolutional and recurrent networks, on the task of error detection in learner writing. The word-level sequence labeling model described in this paper follows the previous work, combining useful design choices from each of them. In addition, we extended the model with two alternative character-level architectures, and evaluated its performance on 8 different datasets. Character-level models have the potential of capturing morpheme patterns, thereby improving generalisation on both frequent and unseen words. In recent years, there has been an increase in research into these models, resulting in several interesting applications. Ling et al. (2015b) described a character-level neural model for machine translation, performing both encoding and decoding on individual characters. Kim et al. (2016) implemented a language model where encoding is performed by a convolutional network and LSTM over characters, whereas predictions are given on the word-level.",
      "startOffset" : 0,
      "endOffset" : 1522
    }, {
      "referenceID" : 2,
      "context" : "Cao and Rei (2016) proposed a method for learning both word embeddings and morphological segmentation with a bidirectional recurrent network over characters.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "There is also research on performing parsing (Ballesteros et al., 2015) and text classification (Zhang et al., 2015) with character-level neural models. Ling et al. (2015a) proposed a neural architecture that replaces word embeddings with dynamically-constructed characterbased representations.",
      "startOffset" : 46,
      "endOffset" : 173
    }, {
      "referenceID" : 0,
      "context" : "There is also research on performing parsing (Ballesteros et al., 2015) and text classification (Zhang et al., 2015) with character-level neural models. Ling et al. (2015a) proposed a neural architecture that replaces word embeddings with dynamically-constructed characterbased representations. We applied a similar method for operating over characters, but combined them with word embeddings instead of replacing them, as this allows the model to benefit from both approaches. Lample et al. (2016) described a model where the character-level representation is combined with word embeddings through concatenation.",
      "startOffset" : 46,
      "endOffset" : 499
    }, {
      "referenceID" : 0,
      "context" : "There is also research on performing parsing (Ballesteros et al., 2015) and text classification (Zhang et al., 2015) with character-level neural models. Ling et al. (2015a) proposed a neural architecture that replaces word embeddings with dynamically-constructed characterbased representations. We applied a similar method for operating over characters, but combined them with word embeddings instead of replacing them, as this allows the model to benefit from both approaches. Lample et al. (2016) described a model where the character-level representation is combined with word embeddings through concatenation. In this work, we proposed an alternative architecture, where the representations are combined using an attention mechanism, and evaluated both approaches on a range of tasks and datasets. Recently, Miyamoto and Cho (2016) have also described a related method for the task of language modelling, combining characters and word embeddings using gating.",
      "startOffset" : 46,
      "endOffset" : 836
    } ],
    "year" : 2016,
    "abstractText" : "Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a wordor character-level component. We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters.",
    "creator" : "TeX"
  }
}