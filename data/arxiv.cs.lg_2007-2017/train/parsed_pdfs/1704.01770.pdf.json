{
  "name" : "1704.01770.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Enabling Smart Data: Noise filtering in Big Data classification",
    "authors" : [ "Diego Garćıa-Gila", "Julián Luengo", "Salvador Garćıa", "Francisco Herrera" ],
    "emails" : [ "djgarcia@decsai.ugr.es", "julianlm@decsai.ugr.es", "salvagl@decsai.ugr.es", "herrera@decsai.ugr.es" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.\nKeywords: Big Data, Smart Data, Classification, Class Noise, Label Noise."
    }, {
      "heading" : "1. Introduction",
      "text" : "Vast amounts of information surround us today. Technologies such as the Internet generate data at an exponential rate thanks to the affordability and great development of storage and network resources. It is predicted that by 2020, the digital universe will be 10 times as big as it was in 2013, totaling an astonishing 44 zettabytes [22]. The current volume of data has exceeded the processing capabilities of classical data mining systems [50] and have created a need for new frameworks for storing and processing this data. It is widely\n∗Corresponding author Email addresses: djgarcia@decsai.ugr.es (Diego Garćıa-Gil),\njulianlm@decsai.ugr.es (Julián Luengo), salvagl@decsai.ugr.es (Salvador Garćıa), herrera@decsai.ugr.es (Francisco Herrera)\nPreprint submitted to Journal of LATEX Templates July 31, 2017\nar X\niv :1\n70 4.\n01 77\n0v 2\n[ cs\n.D B\n] 2\n8 Ju\nl 2 01\naccepted that we have entered the Big Data era [31]. Big Data is the set of technologies that make processing such large amounts of data possible [7], while most of the classic knowledge extraction methods cannot work in a Big Data environment because they were not conceived for it.\nBig Data as concept is defined around five aspects: data volume, data velocity, data variety, data veracity and data value [24]. While the volume, variety and velocity aspects refer to the data generation process and how to capture and store the data, veracity and value aspects deal with the quality and the usefulness of the data. These two last aspects become crucial in any Big Data process, where the extraction of useful and valuable knowledge is strongly influenced by the quality of the used data.\nIn Big Data, the usage of traditional preprocessing techniques [16, 34, 18] to enhance the data is even more time consuming and resource demanding, being unfeasible in most cases. The lack of efficient and affordable preprocessing techniques implies that the problems in the data will affect the models extracted. Among all the problems that may appear in the data, the presence of noise in the dataset is one of the most frequent. Noise can be defined as the partial or complete alteration of the information gathered for a data item, caused by an exogenous factor not related to the distribution that generates the data. Learning from noisy data is an important topic in machine learning, data mining and pattern recognition, as real world data sets may suffer from imperfections in data acquisition, transmission, storage, integration and categorization. Noise will lead to excessively complex models with deteriorated performance [49], resulting in even larger computing times for less value.\nThe impact of noise in Big Data, among other pernicious traits, has not been disregarded. Recently, Smart Data (focusing on veracity and value) has been introduced, aiming to filter out the noise and to highlight the valuable data, which can be effectively used by companies and governments for planning, operation, monitoring, control, and intelligent decision making. Three key attributes are needed for data to be smart, it must be accurate, actionable and agile:\n• Accurate: data must be what it says it is with enough precision to drive value. Data quality matters.\n• Actionable: data must drive an immediate scalable action in a way that maximizes a business objective like media reach across platforms. Scalable action matters.\n• Agile: data must be available in real-time and ready to adapt to the changing business environment. Flexibility matters.\nAdvanced Big Data modeling and analytics are indispensable for discovering the underlying structure from retrieved data in order to acquire Smart Data. In this paper we provide several preprocessing techniques for Big Data, transforming raw, corrupted datasets into Smart Data. We focus our interest on classification tasks, where two types of noise are distinguished: class noise, when it affects the class label of the instances, and attribute noise, when it affects\nthe rest of attributes. The former is known to be the most disruptive [39, 54]. Consequently, many recent works, including this contribution, have been devoted to resolving this problem or at least to minimize its effects (see [15] for a comprehensive and updated survey). While some architectural designs are already proposed in the literature[52], there is no particular algorithm which deals with noise in Big Data classification, nor a comparison of its effect on model generalization abilities or computing times.\nThereby we propose a framework for Big Data under Apache Spark for removing noisy examples composed of two algorithms based on ensembles of classifiers. The first one is an homogeneous ensemble, named Homogeneous Ensembe for Big Data (HME-BD), which uses a single base classifier (Random Forest [4]) over a partitioning of the training set. The second ensemble is an heterogeneous ensemble, namely Heterogeneous Ensembe for BigData (HTEBD), that uses different classifiers to identify noisy instances: Random Forest, Logistic Regression and K-Nearest Neighbors (KNN) as base classifiers. For the sake of a more complete comparison, we have also considered a simple filtering approach based on similarities between instances, named Edited Nearest Neighbor for Big Data (ENN-BD). ENN-BD examines the nearest neighbors of every example in the training set and eliminates those whose majority of neighbors belong to a different class. All these techniques have been implemented under the Apache Spark framework [20, 40] and can be downloaded from the Spark’s community repository 1.\nTo show the performance of the three proposed algorithms, we have carried out an experimental evaluation with four large datasets, namely SUSY, HIGGS, Epsilon and ECBDL14. We have induced several levels of class noise to evaluate the effects of applying such framework and the improvements obtained in terms of classification accuracy for two classifiers: a decision tree and the KNN technique. Decision trees with pruning are known to be tolerant to noise, while KNN is a noise sensitive algorithm when the number of selected neighbors is low. These differences allow us to better compare the effect of the framework in classifiers which behave differently towards noise. We also show that, for the Big Data problems considered, the classifiers also benefit from applying the noise treatment even when no additional noise is induced, since Big Data problems contain implicit noise due to incidental homogeneity, spurious correlations and the accumulation of noisy examples [12]. The results obtained indicate that the framework proposed can successfully deal with noise. In particular, the homogeneous ensemble is a suitable technique for dealing with noise in Big Data problems, with low computing times and enabling the classifier to achieve better accuracy.\nThe remainder of this paper is organized as follows: Section 2 presents the concepts of noise, MapReduce and Smart Data. Section 3 explains the proposed framework. Section 4 describes the experiments carried out to check the performance of the framework. Finally, Section 5 concludes the paper.\n1https://spark-packages.org/package/djgarcia/NoiseFramework"
    }, {
      "heading" : "2. Related work",
      "text" : "In this section we first present the problem of noise in classification tasks in Section 2.1. Then we introduce the MapReduce framework commonly used in Big Data solutions in Section 2.2. Finally, we provide an insight into Smart Data in 2.3."
    }, {
      "heading" : "2.1. Class noise vs. attribute noise",
      "text" : "In a classification problem, several effects of this noise can be observed by analyzing its spatial characteristics: noise may create small clusters of instances of a particular class in the instance space corresponding to another class, displace or remove instances located in key areas within a concrete class, or disrupt the boundaries of the classes resulting in an increased boundaries overlap. All these imperfections may harm data interpretation, the design, size, building time, interpretability and accuracy of models, as well as decision making [53, 54].\nAs described by Wang et al. [48], from the large number of components that comprise a dataset, class labels and attribute values are two essential elements in classification datasets. Thus, two types of noise are commonly differentiated in the literature [54, 48]:\n• Class noise, also known as label noise, takes place when an example is wrongly labeled. Class noise includes contradictory examples [42, 39] (examples with identical input attribute values having different class labels) and misclassifications [54] (examples which are incorrectly labeled).\n• Attribute noise refers to corruptions in the values of the input attributes. It includes erroneous attribute values, missing values and incomplete attributes or “do not care” values. Missing values are usually considered independently in the literature, so attribute noise is mainly used for erroneous values [54].\nClass noise is generally considered more harmful to the learning process, and methods for dealing with class noise are more frequent in the literature [54]. Class noise may have many reasons, such as errors or subjectivity in the data labeling process, as well as the use of inadequate information for labeling. Data labeling by domain experts is generally costly, and automatic taggers are used (e.g., sentiment analysis polarization [29]), increasing the probability of class noise.\nDue to the increasing attention from researchers and practitioners, numerous techniques have been developed to tackle it [15, 54, 16]. These techniques include learning algorithms robust to noise as well as data preprocessing techniques that remove or “repair” noisy instances. In [15] the mechanisms that generate label noise are examined, relating them to the appropriate treatment procedures that can be safely applied:\n• On the one hand, algorithm level approaches attempt to create robust classification algorithms that are little influenced by the presence of noise.\nThis includes approaches where existing algorithms are modified to cope with label noise by either being modeled in the classifier construction [25, 27], by applying pruning strategies to avoid overfiting as in [35] or by diminishing the importance of noisy instances with respect to clean ones [33]. Recent proposals exist which that combine these two approaches, which model the noise and give less relevance to potentially noisy instances in the classifier building process [3].\n• On the other hand, data level approaches (also called filters) try to develop strategies to cleanse the dataset as a previous step to the fit of the classifier, by either creating ensembles of classifiers [5], iteratively filtering noisy instances [23], computing metrics on the data or even hybrid approaches that combine several of these strategies.\nIn the Big Data environment there is a special need for noise filter methods. It is well known that the high dimensionality and example size generate accumulated noise in Big Data problems [12]. Noise filters reduce the size of the datasets and improve the quality of the data by removing noisy instances, but most of the classic algorithms for noisy data, noise filters in particular, are not prepared for working with huge volumes of data."
    }, {
      "heading" : "2.2. Big Data. MapReduce and Apache Spark",
      "text" : "The globalization of the Big Data paradigm is generating a large response in terms of technologies that must deal with the rapidly growing rates of generated data [45]. Among all of them, MapReduce is the seminal framework designed by Google in 2003 [9]. It follows a divide and conquer approach to process and generate large datasets with parallel and distributed algorithms on a cluster. The MapReduce model is composed of two phases: Map and Reduce. The Map phase performs a transformation of the data, and the Reduce phase performs a summary operation. Briefly explained, first the master node splits the input data and distributes it across the cluster. Then the Map transformation is applied to each key-value pair in the local data. Once that process is finished the data is redistributed based on the key-value pairs generated in the Map phase. Once all pairs belonging to one key are in the same node, it is processed in parallel. Apache Hadoop [46] [1] is the most popular open-source framework based on the MapReduce model.\nApache Spark [20, 40] is an open-source framework for Big Data processing built around speed, ease of use and sophisticated analytics. Its main feature is its ability to use in-memory primitives. Users can load their data into memory and iterate over it repeatedly, making it a suitable tool for ML algorithms. The motivation for developing Spark came from the limitations in the MapReduce/Hadoop model [28, 13]:\n• Intensive disk usage\n• Insufficiency for in-memory computation\n• Poor performance on online and iterative computing.\n• Low inter-communication capacity.\nSpark is built on top of a distributed data structure called Resilient Distributed Datasets (RDDs) [51]. Operations on RDDs are applied to each partition of the node local data. RDDs support two types of operations: transformations, which are not evaluated when defined and produce a new RDD, and actions, which evaluate all the previous transformations and return a new value. The RDD structure allows programmers to persist them into memory or disk for re-usability purposes. RDDs are immutable and fault-tolerant by nature. All operations are tracked using a ”lineage”, so that each partition can be recalculated in case of failure.\nAlthough new promising frameworks for Big Data are emerging, like Apache Flink [14], Apache Spark is becoming the reference in performance [19]."
    }, {
      "heading" : "2.3. From Big Data to Smart Data",
      "text" : "Big Data is an appealing discipline that presents an immense potential for global economic growth and promises to enhance competitiveness of high technological countries. Such as occurs in any knowledge extraction process, vast amounts of data are analyzed, processed, and interpreted in order to generate profits in terms of either economic or advantages for society. Once the Big Data has been analyzed, processed, interpreted and cleaned, it is possible to access it in a structured way. This transformation is the difference between “Big” and “Smart” Data [26].\nThe first step in this transformation is to perform an integration process, where the semantics and domains from several large sources are unified under a common structure. The usage of ontologies to support the integration is a recent approach [10, 8], but graph databases are also an option where the data is stored in a relational form, as in healthcare domains [36]. Even when the integration phase ends, the data is still far from being “smart”: the accumulated noise in Big Data problems creates problems in classical Data Mining techniques, specially when the dimensionality is large [11]. Thus, in order to be “smart”, the data still needs to be cleaned even after its integration, and data preprocessing is the set of techniques utilized to encompass this task [16, 17].\nOnce the data is “smart”, it can hold the valuable data and allows interactions in “real time”, like transactional activities and other Business Intelligence applications. The goal is to evolve from a data-centered organization to a learning organization, where the focus is set on the knowledge extracted instead of struggling with the data management [21]. However, Big Data generates great challenges to achieve this since its high dimensionality and large example size imply noise accumulation, algorithmic instability and the massive sample pool is often aggregated from heterogeneous sources [12]. While feature selection, discretization or imbalanced algorithms to cope with the high dimensionality have drawn the attention of current Big Data frameworks (such as Spark’s MLlib [32]) and researchers [38, 41, 37, 43], algorithms to clean noise are still a\nchallenge. In summary, challenges are still present to fully operate a transition between Big Data to Smart Data. In this paper we provide an automated preprocessing framework to deal with class noise, enabling the practitioner to reach Smart Data."
    }, {
      "heading" : "3. Towards Smart Data: Noise filtering for Big Data",
      "text" : "In this section, we present the framework for Big Data under Apache Spark for removing noisy examples based on the MapReduce paradigm, proving its performance over real-world large problems. It is a MapReduce design where all the noise filter processes are performed in a distributed way.\nIn Section 3.1 we describe the Spark primitives used for the implementation of the framework. We have designed two algorithms based on ensembles. Both perform a k-fold on the training data, learn a model on the training partition and clean noisy instances in the test partition. The first one is an homogeneous ensemble using Random Forest as a classifier, named HME-BD (Section 3.2). The second one, named HTE-BD (Section 3.3) is a heterogeneous ensemble based on the use of three different classifiers: Random Forest, Logistic Regression and KNN. We have also implemented a simple filter based on the similarity between the instances, named ENN-BD (Section 3.4)."
    }, {
      "heading" : "3.1. Spark Primitives",
      "text" : "For the implementation of the framework, we have used some basic Spark primitives from Spark API. These primitives offer much complex operations by extending the MapReduce paradigm. Here, we outline those more relevant to the algorithms 2:\n• map: Applies a transformation to each element of a RDD. Once the operation has been performed to each element, the resulting RDD is returned.\n• zipWithIndex: for each element of a RDD, a pair consisting in the element and its index is created, starting at 0. The resulting RDD is then returned.\n• join: Return a RDD containing all pairs of elements with matching keys between two RDDs.\n• filter: Return a new RDD containing only the elements that satisfy a predicate.\n• union: Return a RDD of pairs as result of the union of two RDDs.\n• kFold: Returns a list of k pairs of RDDs with the first element of each pair containing the train data, a complement of the test data, and the second element containing the test data, being a unique 1/kth of the data. Where k is the number of folds.\n2For a complete description of Spark’s operations, please refer to Spark’s API: http:// spark.apache.org/docs/latest/api/scala/index.html\nAlgorithm 1 HME-BD Algorithm\n1: Input: data a RDD of tuples (label, features) 2: Input: P the number of partitions 3: Input: nTrees the number of trees for Random Forest 4: Output: the filtered RDD without noise 5: partitions← kFold(data, P ) 6: filteredData← ∅ 7: for all train, test in partitions do 8: rfModel← randomForest(train, nTrees) 9: rfPred← predict(rfModel, test)\n10: joinedData← join(zipWithIndex(test), zipWithIndex(rfPred)) 11: markedData← 12: map original, prediction ∈ joinedData 13: if label(original) = label(prediction) then 14: original 15: else 16: (label = ∅, features(original)) 17: end if 18: end map 19: filteredData← union(filteredData,markedData) 20: end for 21: return(filter(filteredData, label 6= ∅))\n• randomForest: Method to learn a Random Forest model for classification problems.\n• predict: Returns a RDD containing the features and the predicted labels for a given dataset using the learned model.\n• learnClassifiers: Although its not a pure Spark primitive, we use it to simplify the description of the algorithms. This primitive learns a Random Forest, Logistic Regression and 1NN models from the input data.\nThese Spark primitives from Spark API are used in the following sections where HME-BD, HTE-BD and ENN-BD algorithms are described."
    }, {
      "heading" : "3.2. Homogeneous Ensemble: HME-BD",
      "text" : "The homogeneous ensemble is inspired by Cross-Validated Committees Filter (CVCF) [44]. This filter removes noisy examples by partitioning the data in P subsets of equal size. Then, a decision tree, such as C4.5, is learned P times, each time leaving out one of the subsets of the training data. This results in P classifiers which are used to predict all the training data P times. Then, using a voting strategy, misclassified instances are removed.\nHME-BD is also based on a partitioning scheme of the training data. There is an important difference with respect to CVCF: the use of Spark’s implementation of Random Forest instead a of a decision tree as a classifier. CVCF creates\nan ensemble from partitioning of the training data. HME-BD also partitions the training data, but the use of Random Forest allows us to improve the voting step:\n• CVCF predicts the whole dataset P times. We only predict the instances of the partition that Random Forest has not seen while learning the model. This step is repeated P times. With this change we not only improve the performance, but also the computing time of the algorithm since it only has to predict a small part of the training data each iteration.\n• We don’t need to implement a voting strategy, the decision of whether an instance is noisy is associated with the Random Forest prediction.\nAlgorithm 1 describes the noise filtering process in HME-BD:\n• The algorithm filters the noise in a dataset by performing a kFold on the training data. As stated previously, Spark’s kFold function returns a list of (train, test) for a given P , where test is a unique 1/kth of the data, and train is a complement of the test data.\n• We iterate through each partition, learning a Random Forest model using the train as input data and predicting the test using the learned model.\n• In order to join the test data and the predicted data for comparing the classes, we use the zipWithIndex operation in both RDDs. With this operation, we add an index to each element of both RDDs. This index is used as key for the join operation.\n• The next step is to apply a Map function to the previous RDD in order to check for each instance the original class and the predicted one. If the predicted class and the original are different, the instance is marked as noise.\n• The result of the previous Map function is a RDD where noisy instances are marked. These instances are finally removed using a filter function and the resulting dataset is returned.\nThe following are required as input parameters: the dataset (data), the number of partitions (P ) and the number of trees for the Random Forest (nTrees).\nIn Figure 1 we can see a flowchart of the HME-BD noise filtering process."
    }, {
      "heading" : "3.3. Heterogeneous Ensemble: HTE-BD",
      "text" : "Heterogeneous Ensemble is inspired by Ensemble Filter (EF) [5]. This noise filter uses a set of three learning algorithms for identifying mislabeled instances in a dataset: a univariate decision tree (C4.5), KNN and a linear machine. It performs a k-fold cross validation over the training data. For each one of the k parts, three algorithms are trained on the other k − 1 parts. Each of the classifiers is used to tag each of the test examples as noisy or clean. At the end\nFigure 1: HME-BD noise filtering process flowchart\nof the k-fold, each example of the input data has been tagged. Finally, using a voting strategy, a decision is made and noisy examples are removed.\nHTE-BD follows the same working scheme as EF. The main difference is the choice of the three learning algorithms:\n• Instead of a decision tree, we use Spark’s implementation of Random Forest.\n• We use an exact implementation of KNN with the euclidean distance present in Spark’s community repository, kNN-IS3 [30]\n• The linear machine has been replaced by Spark’s implementation of Logistic Regression, which is another linear classifier.\nThe noise filtering process in HTE-BD is shown in Algorithm 2:\n• For each train and test partition of the k-fold performed to the input data, it learns three classification algorithms: Random Forest, Logistic Regression and 1NN using the train as input data.\n• Then it predicts the test data using the three learned models. This creates a RDD of triplets (rf, lr, knn) with the prediction of each algorithm for each instance.\n• The predictions and the test data are joined by index in order to compare the predictions and the original label.\n3https://spark-packages.org/package/JMailloH/kNN_IS\nAlgorithm 2 HTE-BD Algorithm\n1: Input: data a RDD of tuples (label, features) 2: Input: P the number of partitions 3: Input: nTrees the number of trees for Random Forest 4: Input: vote the voting strategy (majority or consensus) 5: Output: the filtered RDD without noise 6: partitions← kFold(data, P ) 7: filteredData← ∅ 8: for all train, test in partitions do 9: classifiersModel← learnClassifiers(train, nTrees)\n10: predictions← predict(classifiersModel, test) 11: joinedData← join(zipWithIndex(predictions), zipWithIndex(test)) 12: markedData← 13: map rf, lr, knn, orig ∈ joinedData 14: count← 0 15: if rf 6= label(orig) then count← count + 1 end if 16: if lr 6= label(orig) then count← count + 1 end if 17: if knn 6= label(orig) then count← count + 1 end if 18: if vote = majority then 19: if count ≥ 2 then (label = ∅, features(orig)) end if 20: if count < 2 then orig end if 21: else 22: if count = 3 then (label = ∅, features(orig)) end if 23: if count 6= 2 then orig end if 24: end if 25: end map 26: filteredData← union(filteredData,markedData) 27: end for 28: return(filter(filteredData, label 6= ∅))\n• It compares the three predictions of each instance in the test data with the original label using a Map function and, depending upon the voting strategy, the instance is marked as noise or clean.\n• Once the Map function has been applied to each instance, noisy data is removed using a filter function and the dataset is returned.\nThe following are required as input parameters: the dataset (data), the number of partitions (P ), the number of trees for the Random Forest (nTrees) and the voting strategy (vote).\nIn Figure 2 we show a flowchart of the HTE-BD noise filtering process."
    }, {
      "heading" : "3.4. Similarity: ENN-BD",
      "text" : "ENN-BD is a simple filtering algorithm that works as a baseline for comparison purposes. It has been designed based on the Edited Nearest Neighbor algorithm (ENN) [47] and follows a similarity between instances approach. ENN\nFigure 2: HTE-BD noise filtering process flowchart\nAlgorithm 3 ENN-BD Algorithm\n1: Input: data a RDD of tuples (label, features) 2: Output: the filtered RDD without noise 3: knnModel← KNN(1, ”euclidean”, data) 4: knnPred← zipWithIndex(predict(knnModel, data)) 5: joinedData← join(zipWithIndex(data), knnPred) 6: filteredData← 7: map original, prediction ∈ joinedData 8: if label(original) = label(prediction) then 9: original\n10: else 11: (noise, features(original)) 12: end if 13: end map 14: return(filter(filteredData, label 6= noise))\nremoves noisy instances in a dataset by comparing the label of each example with its closest neighbor. If the labels are different, the instance is considered as noisy and removed.\nENN-BD performs a 1NN using Spark’s community repository kNN-IS with the euclidean distance. It checks for each instance if its closest neighbor belongs to the same class. In case the classes are different, the instance is marked as noise. Finally, marked instances are removed from the training data. This process is described in Algorithm 3. The only input parameter required is the dataset (data)."
    }, {
      "heading" : "4. Experimental Results",
      "text" : "This section describes the experimental details and the analysis carried out to show the performance of the three noise filter methods over four huge problems. In Section 4.1, we present the details of the datasets and the parameters used in the methods. We analyze the accuracy improvements generated by the proposed framework and the study of instances removed in Section 4.2. Finally, Section 4.3 is devoted to the computing times of the proposals."
    }, {
      "heading" : "4.1. Experimental Framework",
      "text" : "Four classification datasets are used in our experiments:\n• SUSY dataset, which consists of 5,000,000 instances and 18 attributes. The first eight features are kinematic properties measured by the particle detectors at the Large Hadron Collider. The last ten are functions of the first eight features. The task is to distinguish between a signal process which produces supersymmetric (SUSY) particles and a background process which does not [2].\n• HIGGS dataset, which has 11,000,000 instances and 28 attributes. This dataset is a classification problem to distinguish between a signal process which produces Higgs bosons and a background process which does not.\n• Epsilon dataset, which consists of 500,000 instances with 2,000 numerical features. This dataset was artificially created for the Pascal Large Scale Learning Challenge in 2008. It was further pre-processed and included in the LibSVM dataset repository [6].\n• ECBDL14 dataset, which has 32 million instances and 631 attributes (including both numerical and categorical). This dataset was used as a reference at the ML competition of the Evolutionary Computation for Big Data and Big Learning held on July 14, 2014, under the international conference GECCO-2014. It is a binary classification problem where the class distribution is highly imbalanced: 98% of negative instances. For this problem, we use a reduced version with 1,000,000 instances and 30% of positive instances.\nTable 1 provides a brief summary of these datasets, showing the number of examples (Instances), the total number of attributes (Atts.), the total number of training data (Total), and the number classes (CL).\nWe carried out experiments on five levels of uniform class noise [42]: for each level of noise, a percentage of the training instances are altered by replacing their actual label by another label from the available classes. The selected noise levels are 0%, 5%, 10%, 15% and 20%. In this case, a 0% noise level indicates that the dataset was unaltered. We have conducted a hold-out validation due to the time limitations of the KNN algorithm.\nIn Table 2 we can see the complete list of parameters used for the noise treatment algorithms. In order to evaluate the effect of the number of partitions on the behavior of the filters, we have selected 4 and 5 training partitions for HME-BD and HTE-BD. For the heterogeneous filter, HTE-BD, we also use two voting strategies: consensus (same result for all classifiers) and majority (same result for at least half the classifiers).\nTwo classifiers, one MLlib classifier, a decission tree, and one algorithm present in Spark’s community repository, KNN, are used to evaluate the effectiveness of the filtering carried out by the two ensemble proposals and the similarity filter. The decision tree can adapt its depth to avoid overfitting to noisy instances, while KNN is known to be sensitive to noise when the number of selected neighbors is low. Prediction accuracy is used to evaluate the model’s performance produced by the classifiers (number of examples correctly labeled as belonging to a given class divided by the total number of elements). The parameters used for the classifiers can be seen in Table 3. Default parameters are used, except for the decision tree, in which we have tuned the depth of the tree for a better detection of noisy instances.\nFor all experiments we have used a cluster composed of 20 computing nodes and one master node. The computing nodes hold the following characteristics: 2 processors x Intel(R) Xeon(R) CPU E5-2620, 6 cores per processor, 2.00 GHz, 2 TB HDD, 64 GB RAM. Regarding software, we have used the following configuration: Hadoop 2.6.0-cdh5.4.3 from Cloudera’s open source Apache Hadoop distribution, Apache Spark and MLlib 1.6.0, 460 cores (23 cores/node), 960 RAM GB (48 GB/node)."
    }, {
      "heading" : "4.2. Analysis of accuracy performance and removed instances",
      "text" : "In this section, we present the analysis on the performance results obtained by the selected classifiers after applying the proposed framework. We denote with Original the application of the classifier without using any noise treatment techniques, in order to evaluate the impact of the increasing noise level in the quality of the models extracted by the classification algorithms.\nTable 4 shows the test accuracy values for the four datasets and the five levels of noise using the KNN algorithm for classification. From these results we can point out that:\n• It is important to remark that the usage of any noise treatment technique always improves the Original accuracy value at the same noise level. Please note that the usage of the noise treatment technique allows KNN to obtain better performance at any noise level, even at the highest ones, than Original at 0% level for every dataset. Since Big Datasets tend to accumulate noise, the proposed noise framework is able to improve the behavior and performance of the KNN classifier in every case.\n• If we attend the best noise treatment strategy for KNN, we must point out that the homogeneous filter, HME-BD, enables KNN to obtain the highest accuracy values.\n• The different number of partitions used for HME-BD has little impact in the accuracy values, which, in this respect, makes it a robust method.\n• The heterogeneous ensemble filter, HTE-BD, is also robust to the number of partitions chosen, but its performance is lower than HME-BD. However, the voting scheme is crucial for HTE-BD, as the consensus strategy will\nTable 5 gathers the test accuracy values for the three noise filter methods using a deep decision tree. From these results we can point out that:\n• Again, avoiding the treatment of noise is never the best option and using the appropriate noise filtering technique will provide a significant improvement in accuracy. However, since the decision tree is more robust against noise than KNN, not all the filters are better than avoiding filtering noise (Original). When the filters remove too many instances, both noisy and clean, the decision tree is more affected since it is able to withstand small amounts of noise while exploiting the clean instances. KNN was very affected by the noisy instances left, in a higher degree than the decision tree. Thus, a wrong filtering strategy will penalize the performance of the decision tree. We will elaborate more on this later.\n• In terms of the best filtering technique for the decision tree, for low levels of noise, the heterogeneous ensemble HTE-BD can perform slightly better than the homogeneous HME-BD for some datasets. Nevertheless, from\nThe results presented have shown the importance of applying a noise treatment strategy, no matter how much noise is present in the dataset, and the best strategy overall: HME-BD. To better explain why HME-BD is the best filtering strategy in the framework, we must study the amount of instances removed.\nIn Table 6 we present the average number of instances left after the application of the three noise filtering methods for the four datasets. In Figure 3 we can see a graphic representation of the number of instances for the sake of a better depiction. As we can expect, the higher the percentage of noise, the lower the number of instances that remain in the dataset after applying the filtering technique. However, there are different patterns depending on the filtering technique used:\n• For the homogeneous ensemble HME-BD, there is no effect in the number of partitions P chosen with respect to the amount of removed instances. On average, HME-BD removes around 20% of the instances at a 0% noise\nlevel. At each noise level increment an average of 3% of the instances are removed.\n• For the Epsilon dataset, at 20% nosie, HME-BD does not remove as many instances as expected, but it is still the best option out of the two classifiers. A high instance redundancy in this dataset may cause homogeneous voting to not discard as many instances as the other filters.\n• Like HME-BD, HTE-BD is not affected by the number of partitions, but the voting scheme does have a great impact on its behavior. While the majority voting strategy achieves almost the same number of removed instances as HME-BD, the consensus voting strategy is more conservative. Consensus voting removes 10% of the instances for 0% level of noise, and it is increasing a 3% on average as the level of noise increases, the same rate as HME-BD.\n• ENN-BD is the filter that removes more instances. On average it removes half the instances of the datasets for 0% level of noise and then increases around 1% at each increment of noise level. This aggresive filtering hinders the performance of noise tolerant classifiers, such as the decision tree.\n• In general, HME-BD is the most balanced technique in terms of instances\nWe have performed a deeper analysis of the removed instances, analyzing the amount of correctly removed instances for each method in the framework.\nIn Table 7 we present the average percentage of correctly removed instances after the application of the three noise filtering methods for the four datasets. In Figure 4 we can see a graphic representation of these percentages of correctly removed instances. As we can see, the consensus voting strategy is much more conservative removing noisy instances than the rest of the methods. We can also outline some patterns depending on the filtering method used:\n• While ENN-BD is the filter that more instances removes, it is also the one that less noise removes from the datasets, averaging a 50% of noisy instances removed.\n• Similarly to the number of instances removed, HME-BD and HTE-BD are not affected by the number of partitions, while the voting strategy does influence the percentage of correctly removed instances. As we could expect, the consensus voting strategy is the one that less noisy instances clean. Consensus voting removes only 25% of noisy instances in HIGGS dataset, and only increases to 45% in ECBDL14 dataset.\n• HME-BD and HTE-BD with majority voting, are removing aroung 65% and 80% of noisy instances. Both methods outperform the other in two out of four datasets.\n• In Epsilon dataset, HTE-BD is cleaning 10% more noisy instances than HME-BD, but HME-BD performs better in test accuracy. This can be explained by the accumulated noise of this particular dataset.\nIn view of the results, we can conclude that HME-BD is the most suitable ensemble option in the proposed framework to deal with noise in Big Data problems. Even when we did not introduce any additional noise, the usage of noise treatment methods has proven to be very beneficial. As previously mentioned, Big Data problems tend to accumulate noise and the proposed noise framework is a suitable tool to clean and proceed from Big to Smart Datasets."
    }, {
      "heading" : "4.3. Computing times",
      "text" : "In the previous section we have shown the suitability of the proposed framework in terms of accuracy. In order to constitute a valid proposal in Big Data, this framework has to be scalable as well. This section is devoted to present the computing times for the two prosposed ensemble techniques, HME-BD and HTE-BD, and the simple similarity method, ENN-BD, used as a baseline.\nIn Table 8 we can see the average run times of the three methods for the four datasets in seconds. As the level of noise is not a factor that affects the run time, we show the average of the five executions performed for each dataset. In Figure 5 we can see a graphic representation of these times.\nThe measured times show that the homogeneous ensemble, HME-BD, is not only the best performing option in terms of accuracy, but also the most efficient one in terms of computing time. HME-BD is about ten times faster than the heterogeneous filter HTE-BD and the similarity filter ENN-BD. This is caused by the usage of the KNN classifier by HTE-BD and ENN-BD, which is very demanding in computing terms. As a result, HME-BD does not need to compute any distance measures, saving computing time and being the most recommended option to deal with noise in Big Data problems."
    }, {
      "heading" : "5. Conclusions",
      "text" : "In this paper, we have tackled the problem of noise in Big Data classification, which is a crucial step in transforming such raw data into Smart Data. We have proposed several noise filtering algorithms, implemented in a Big Data framework: Spark. These filtering techniques are based on the creation of ensembles of classifiers that are executed in the different maps, enabling the practitioner to deal with huge datasets. Different strategies of data partitioning and ensemble classifier combination have led to three different approaches.\nThe suitability of these proposed techniques has been analyzed using several data sets, in order to study the accuracy improvement, running times and data reduction rates. The homogeneous ensemble has shown to be the most suitable approach in most cases, both in accuracy improvement and better running times.\nIt also shows the best balance between removing and keeping sufficient instances, being among the most balanced filter in terms of preprocessed training sets.\nThis work presents the first suitable noise filter in Big Data domains, where the high redundancy of the instances and high dimensional problems pose new challenges to classic noise preprocessing algorithms. Thus, the presented framework is a valuable tool for achieving the goal of Smart Data. It also opens promising research lines in this topic, where the presence of iterative algorithms and the usage of noise measures are also known as viable alternatives for dealing with noise."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This work is supported by the Spanish National Research Project TIN201457251-P and the Foundation BBVA project 75/2016 BigDaPTOOLS."
    } ],
    "references" : [ {
      "title" : "Searching for Exotic Particles in High- Energy Physics with Deep Learning",
      "author" : [ "P. Baldi", "P. Sadowski", "D. Whiteson" ],
      "venue" : "Nature Communications 5 ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Robust supervised classification with mixture models: Learning from data with uncertain labels",
      "author" : [ "C. Bouveyron", "S. Girard" ],
      "venue" : "Pattern Recognition 42 (11) ",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Random forests",
      "author" : [ "L. Breiman" ],
      "venue" : "Machine Learning 45 (1) ",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Identifying Mislabeled Training Data",
      "author" : [ "C.E. Brodley", "M.A. Friedl" ],
      "venue" : "Journal of Artificial Intelligence Research 11 ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Libsvm: A library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST) 2 (3) ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Data-intensive applications",
      "author" : [ "C.P. Chen", "C.-Y. Zhang" ],
      "venue" : "challenges, techniques and technologies: A survey on Big Data, Information Sciences 275 ",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Smart data integration by goal driven ontology learning",
      "author" : [ "J. Chen", "D. Dosyn", "V. Lytvyn", "A. Sachenko" ],
      "venue" : "in: Advances in Intelligent Systems and Computing, vol. 529",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Mapreduce: Simplified data processing on large clusters",
      "author" : [ "J. Dean", "S. Ghemawat" ],
      "venue" : "Communications of the ACM 51 (1) ",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Towards an automatic analyze and standardization of unstructured data in the context of big and linked data",
      "author" : [ "H. Fadili", "C. Jouis" ],
      "venue" : "in: 8th International Conference on Management of Digital EcoSystems, MEDES 2016",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "High dimensional classification using features annealed independence rules",
      "author" : [ "J. Fan", "Y. Fan" ],
      "venue" : "Annals of statistics 36 (6) ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Challenges of big data analysis",
      "author" : [ "J. Fan", "F. Han", "H. Liu" ],
      "venue" : "National Science Review 1 (2) ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "S",
      "author" : [ "A. Fernández" ],
      "venue" : "del Ŕıo, V. López, A. Bawakid, M. J. del Jesús, J. M. Beńıtez, F. Herrera, Big data with cloud computing: an insight on the computing environment, mapreduce, and programming frameworks, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 4 (5) ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Classification in the presence of label noise: A survey",
      "author" : [ "B. Frénay", "M. Verleysen" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems 25 (5) ",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Data Preprocessing in Data Mining",
      "author" : [ "S. Garćıa", "J. Luengo", "F. Herrera" ],
      "venue" : "Springer",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Tutorial on practical tips of the most influential data preprocessing algorithms in data mining",
      "author" : [ "S. Garćıa", "J. Luengo", "F. Herrera" ],
      "venue" : "Knowledge-Based Systems 98 ",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "S",
      "author" : [ "S. Garćıa" ],
      "venue" : "Ramı́rez-Gallego, J. Luengo, J. M. Beńıtez, F. Herrera, Big data preprocessing: methods and prospects, Big Data Analytics 1 (1) ",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "S",
      "author" : [ "D. Garćıa-Gil" ],
      "venue" : "Ramı́rez-Gallego, S. Garćıa, F. Herrera, A comparison on scalability for batch big data processing on Apache Spark and Apache Flink, Big Data Analytics 2 (1) ",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Learning Spark: Lightning-Fast Big Data Analytics",
      "author" : [ "M. Hamstra", "H. Karau", "M. Zaharia", "A. Konwinski", "P. Wendell" ],
      "venue" : "OReilly Media",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A journey from big data to smart data",
      "author" : [ "F. Iafrate" ],
      "venue" : "Advances in Intelligent Systems and Computing 261 ",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Improving software quality prediction by noise filtering techniques",
      "author" : [ "T.M. Khoshgoftaar", "P. Rebours" ],
      "venue" : "Journal of Computer Science and Technology 22 ",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "3D data management: Controlling data volume",
      "author" : [ "D. Laney" ],
      "venue" : "velocity, and variety, Tech. rep., META Group ",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Estimating a kernel fisher discriminant in the presence of label noise",
      "author" : [ "N.D. Lawrence", "B. Schölkopf" ],
      "venue" : "in: Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Towards a taxonomy of standards in smart data",
      "author" : [ "A. Lenk", "L. Bonorden", "A. Hellmanns", "N. Roedder", "S. Jaehnichen" ],
      "venue" : "in: Proceedings - 2015 IEEE International Conference on Big Data, IEEE Big Data 2015",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "D",
      "author" : [ "Y. Li", "L.F. Wessels" ],
      "venue" : "de Ridder, M. J. Reinders, Classification in the presence of class noise using a probabilistic Kernel Fisher method, Pattern Recognition 40 (12) ",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Mapreduce is good enough? if all you have is a hammer",
      "author" : [ "J. Lin" ],
      "venue" : "throw away everything that’s not a nail!, Big Data 1 (1) ",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Sentiment analysis: Mining opinions",
      "author" : [ "B. Liu" ],
      "venue" : "sentiments, and emotions, Cambridge University Press",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "S",
      "author" : [ "J. Máıllo" ],
      "venue" : "Ramı́rez, I. Triguero, F. Herrera, kNN-IS: An Iterative Sparkbased design of the k-Nearest Neighbors classifier for big data, Knowledge- Based Systems 117 ",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Big Data: A Revolution That Will Transform How We Live",
      "author" : [ "V. Mayer-Schonberger", "K. Cukier" ],
      "venue" : "Work, and Think, Houghton Mifflin Harcourt",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Mllib: Machine learning in apache spark",
      "author" : [ "X. Meng", "J. Bradley", "B. Yavuz", "E. Sparks", "S. Venkataraman", "D. Liu", "J. Freeman", "D. Tsai", "M. Amde", "S. Owen", "D. Xin", "R. Xin", "M.J. Franklin", "R. Zadeh", "M. Zaharia", "A. Talwalkar" ],
      "venue" : "Journal of Machine Learning Research 17 (34) ",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Rboost: Label noiserobust boosting algorithm based on a nonconvex loss function and the numerically stable base learners",
      "author" : [ "Q. Miao", "Y. Cao", "G. Xia", "M. Gong", "J. Liu", "J. Song" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems 27 (11) ",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Data preparation for data mining",
      "author" : [ "D. Pyle" ],
      "venue" : "Morgan Kaufmann, Los Altos",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "C4.5: programs for machine",
      "author" : [ "J.R. Quinlan" ],
      "venue" : null,
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1993
    }, {
      "title" : "Framework for smart health: Toward connected data from big data",
      "author" : [ "P. Raja", "E. Sivasankar", "R. Pitchiah" ],
      "venue" : "Advances in Intelligent Systems and Computing 343 ",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Data discretization: taxonomy and big data challenge, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",
      "author" : [ "S. Ramı́rez-Gallego", "S. Garćıa", "H. Mouriño-Taĺın", "D. Mart́ınez-Rego", "V. Bolón-Canedo", "A. Alonso-Betanzos", "J.M. Beńıtez", "F. Herrera" ],
      "venue" : null,
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2016
    }, {
      "title" : "Analyzing the presence of noise in multi-class problems: alleviating its influence with the One-vs-One decomposition",
      "author" : [ "J.A. Sáez", "M. Galar", "J. Luengo", "F. Herrera" ],
      "venue" : "Knowledge and Information Systems 38 (1) ",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Apache Spark: Lightning-fast cluster computing",
      "author" : [ "A. Spark" ],
      "venue" : "http://spark. apache.org/ ",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Towards ultrahigh dimensional feature selection for big data",
      "author" : [ "M. Tan", "I.W. Tsang", "L. Wang" ],
      "venue" : "Journal of Machine Learning Research 15 ",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Correcting Noisy Data",
      "author" : [ "C.-M. Teng" ],
      "venue" : "in: Proceedings of the Sixteenth International Conference on Machine Learning, Morgan Kaufmann Publishers, San Francisco, CA, USA",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "S",
      "author" : [ "I. Triguero" ],
      "venue" : "del Ŕıo, V. López, J. Bacardit, J. M. Beńıtez, F. Herrera, Rosefw-rf: the winner algorithm for the ecbdl14 big data competition: an extremely imbalanced big data bioinformatics problem, Knowledge-Based Systems 87 ",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Ensemble methods for noise elimination in classification problems",
      "author" : [ "S. Verbaeten", "A. Assche" ],
      "venue" : "in: 4th International Workshop on Multiple Classifier Systems, vol. 2709 of Lecture Notes on Computer Science, Springer",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Towards felicitous decision making: An overview on challenges and trends of Big Data",
      "author" : [ "H. Wang", "Z. Xu", "H. Fujita", "S. Liu" ],
      "venue" : "Information Sciences 367 ",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Hadoop: The Definitive Guide",
      "author" : [ "T. White" ],
      "venue" : "O’Reilly Media, Inc.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Asymptotic properties of nearest neighbor rules using edited data",
      "author" : [ "D.L. Wilson" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics 2 (3) ",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "Knowledge acquisition from databases",
      "author" : [ "X. Wu" ],
      "venue" : "Ablex Publishing Corp., Norwood, NJ, USA",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Mining with noise knowledge: Error-aware data mining",
      "author" : [ "X. Wu", "X. Zhu" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics 38 ",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Data mining with big data",
      "author" : [ "X. Wu", "X. Zhu", "G.-Q. Wu", "W. Ding" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering 26 (1) ",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Resilient distributed datasets: A faulttolerant abstraction for in-memory cluster computing",
      "author" : [ "M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M.J. Franklin", "S. Shenker", "I. Stoica" ],
      "venue" : "in: Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation, NSDI’12, USENIX Association, Berkeley, CA, USA",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Class noise elimination approach for large datasets based on a combination of classifiers",
      "author" : [ "B. Zerhari" ],
      "venue" : "in: Cloud Computing Technologies and Applications (CloudTech), 2016 2nd International Conference on, IEEE",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Analyzing Software Measurement Data with Clustering Techniques",
      "author" : [ "S. Zhong", "T.M. Khoshgoftaar", "N. Seliya" ],
      "venue" : "IEEE Intelligent Systems 19 (2) ",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Class Noise vs",
      "author" : [ "X. Zhu", "X. Wu" ],
      "venue" : "Attribute Noise: A Quantitative Study, Artificial Intelligence Review 22 ",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 45,
      "context" : "The current volume of data has exceeded the processing capabilities of classical data mining systems [50] and have created a need for new frameworks for storing and processing this data.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 27,
      "context" : "accepted that we have entered the Big Data era [31].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "Big Data is the set of technologies that make processing such large amounts of data possible [7], while most of the classic knowledge extraction methods cannot work in a Big Data environment because they were not conceived for it.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 20,
      "context" : "Big Data as concept is defined around five aspects: data volume, data velocity, data variety, data veracity and data value [24].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 13,
      "context" : "In Big Data, the usage of traditional preprocessing techniques [16, 34, 18] to enhance the data is even more time consuming and resource demanding, being unfeasible in most cases.",
      "startOffset" : 63,
      "endOffset" : 75
    }, {
      "referenceID" : 30,
      "context" : "In Big Data, the usage of traditional preprocessing techniques [16, 34, 18] to enhance the data is even more time consuming and resource demanding, being unfeasible in most cases.",
      "startOffset" : 63,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "In Big Data, the usage of traditional preprocessing techniques [16, 34, 18] to enhance the data is even more time consuming and resource demanding, being unfeasible in most cases.",
      "startOffset" : 63,
      "endOffset" : 75
    }, {
      "referenceID" : 44,
      "context" : "Noise will lead to excessively complex models with deteriorated performance [49], resulting in even larger computing times for less value.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 34,
      "context" : "The former is known to be the most disruptive [39, 54].",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 49,
      "context" : "The former is known to be the most disruptive [39, 54].",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "Consequently, many recent works, including this contribution, have been devoted to resolving this problem or at least to minimize its effects (see [15] for a comprehensive and updated survey).",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 47,
      "context" : "While some architectural designs are already proposed in the literature[52], there is no particular algorithm which deals with noise in Big Data classification, nor a comparison of its effect on model generalization abilities or computing times.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "The first one is an homogeneous ensemble, named Homogeneous Ensembe for Big Data (HME-BD), which uses a single base classifier (Random Forest [4]) over a partitioning of the training set.",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "All these techniques have been implemented under the Apache Spark framework [20, 40] and can be downloaded from the Spark’s community repository .",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 35,
      "context" : "All these techniques have been implemented under the Apache Spark framework [20, 40] and can be downloaded from the Spark’s community repository .",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "We also show that, for the Big Data problems considered, the classifiers also benefit from applying the noise treatment even when no additional noise is induced, since Big Data problems contain implicit noise due to incidental homogeneity, spurious correlations and the accumulation of noisy examples [12].",
      "startOffset" : 301,
      "endOffset" : 305
    }, {
      "referenceID" : 48,
      "context" : "All these imperfections may harm data interpretation, the design, size, building time, interpretability and accuracy of models, as well as decision making [53, 54].",
      "startOffset" : 155,
      "endOffset" : 163
    }, {
      "referenceID" : 49,
      "context" : "All these imperfections may harm data interpretation, the design, size, building time, interpretability and accuracy of models, as well as decision making [53, 54].",
      "startOffset" : 155,
      "endOffset" : 163
    }, {
      "referenceID" : 43,
      "context" : "[48], from the large number of components that comprise a dataset, class labels and attribute values are two essential elements in classification datasets.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 49,
      "context" : "Thus, two types of noise are commonly differentiated in the literature [54, 48]:",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 43,
      "context" : "Thus, two types of noise are commonly differentiated in the literature [54, 48]:",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 37,
      "context" : "Class noise includes contradictory examples [42, 39] (examples with identical input attribute values having different class labels) and misclassifications [54] (examples which are incorrectly labeled).",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 34,
      "context" : "Class noise includes contradictory examples [42, 39] (examples with identical input attribute values having different class labels) and misclassifications [54] (examples which are incorrectly labeled).",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 49,
      "context" : "Class noise includes contradictory examples [42, 39] (examples with identical input attribute values having different class labels) and misclassifications [54] (examples which are incorrectly labeled).",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 49,
      "context" : "Missing values are usually considered independently in the literature, so attribute noise is mainly used for erroneous values [54].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 49,
      "context" : "Class noise is generally considered more harmful to the learning process, and methods for dealing with class noise are more frequent in the literature [54].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 25,
      "context" : ", sentiment analysis polarization [29]), increasing the probability of class noise.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "Due to the increasing attention from researchers and practitioners, numerous techniques have been developed to tackle it [15, 54, 16].",
      "startOffset" : 121,
      "endOffset" : 133
    }, {
      "referenceID" : 49,
      "context" : "Due to the increasing attention from researchers and practitioners, numerous techniques have been developed to tackle it [15, 54, 16].",
      "startOffset" : 121,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : "Due to the increasing attention from researchers and practitioners, numerous techniques have been developed to tackle it [15, 54, 16].",
      "startOffset" : 121,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : "In [15] the mechanisms that generate label noise are examined, relating them to the appropriate treatment procedures that can be safely applied:",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 21,
      "context" : "This includes approaches where existing algorithms are modified to cope with label noise by either being modeled in the classifier construction [25, 27], by applying pruning strategies to avoid overfiting as in [35] or by diminishing the importance of noisy instances with respect to clean ones [33].",
      "startOffset" : 144,
      "endOffset" : 152
    }, {
      "referenceID" : 23,
      "context" : "This includes approaches where existing algorithms are modified to cope with label noise by either being modeled in the classifier construction [25, 27], by applying pruning strategies to avoid overfiting as in [35] or by diminishing the importance of noisy instances with respect to clean ones [33].",
      "startOffset" : 144,
      "endOffset" : 152
    }, {
      "referenceID" : 31,
      "context" : "This includes approaches where existing algorithms are modified to cope with label noise by either being modeled in the classifier construction [25, 27], by applying pruning strategies to avoid overfiting as in [35] or by diminishing the importance of noisy instances with respect to clean ones [33].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 29,
      "context" : "This includes approaches where existing algorithms are modified to cope with label noise by either being modeled in the classifier construction [25, 27], by applying pruning strategies to avoid overfiting as in [35] or by diminishing the importance of noisy instances with respect to clean ones [33].",
      "startOffset" : 295,
      "endOffset" : 299
    }, {
      "referenceID" : 1,
      "context" : "Recent proposals exist which that combine these two approaches, which model the noise and give less relevance to potentially noisy instances in the classifier building process [3].",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 3,
      "context" : "• On the other hand, data level approaches (also called filters) try to develop strategies to cleanse the dataset as a previous step to the fit of the classifier, by either creating ensembles of classifiers [5], iteratively filtering noisy instances [23], computing metrics on the data or even hybrid approaches that combine several of these strategies.",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 19,
      "context" : "• On the other hand, data level approaches (also called filters) try to develop strategies to cleanse the dataset as a previous step to the fit of the classifier, by either creating ensembles of classifiers [5], iteratively filtering noisy instances [23], computing metrics on the data or even hybrid approaches that combine several of these strategies.",
      "startOffset" : 250,
      "endOffset" : 254
    }, {
      "referenceID" : 10,
      "context" : "It is well known that the high dimensionality and example size generate accumulated noise in Big Data problems [12].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 40,
      "context" : "MapReduce and Apache Spark The globalization of the Big Data paradigm is generating a large response in terms of technologies that must deal with the rapidly growing rates of generated data [45].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 7,
      "context" : "Among all of them, MapReduce is the seminal framework designed by Google in 2003 [9].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 41,
      "context" : "Apache Hadoop [46] [1] is the most popular open-source framework based on the MapReduce model.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 17,
      "context" : "Apache Spark [20, 40] is an open-source framework for Big Data processing built around speed, ease of use and sophisticated analytics.",
      "startOffset" : 13,
      "endOffset" : 21
    }, {
      "referenceID" : 35,
      "context" : "Apache Spark [20, 40] is an open-source framework for Big Data processing built around speed, ease of use and sophisticated analytics.",
      "startOffset" : 13,
      "endOffset" : 21
    }, {
      "referenceID" : 24,
      "context" : "The motivation for developing Spark came from the limitations in the MapReduce/Hadoop model [28, 13]:",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "The motivation for developing Spark came from the limitations in the MapReduce/Hadoop model [28, 13]:",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 46,
      "context" : "Spark is built on top of a distributed data structure called Resilient Distributed Datasets (RDDs) [51].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "Although new promising frameworks for Big Data are emerging, like Apache Flink [14], Apache Spark is becoming the reference in performance [19].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 22,
      "context" : "This transformation is the difference between “Big” and “Smart” Data [26].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "The usage of ontologies to support the integration is a recent approach [10, 8], but graph databases are also an option where the data is stored in a relational form, as in healthcare domains [36].",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "The usage of ontologies to support the integration is a recent approach [10, 8], but graph databases are also an option where the data is stored in a relational form, as in healthcare domains [36].",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 32,
      "context" : "The usage of ontologies to support the integration is a recent approach [10, 8], but graph databases are also an option where the data is stored in a relational form, as in healthcare domains [36].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 9,
      "context" : "Even when the integration phase ends, the data is still far from being “smart”: the accumulated noise in Big Data problems creates problems in classical Data Mining techniques, specially when the dimensionality is large [11].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 13,
      "context" : "Thus, in order to be “smart”, the data still needs to be cleaned even after its integration, and data preprocessing is the set of techniques utilized to encompass this task [16, 17].",
      "startOffset" : 173,
      "endOffset" : 181
    }, {
      "referenceID" : 14,
      "context" : "Thus, in order to be “smart”, the data still needs to be cleaned even after its integration, and data preprocessing is the set of techniques utilized to encompass this task [16, 17].",
      "startOffset" : 173,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : "The goal is to evolve from a data-centered organization to a learning organization, where the focus is set on the knowledge extracted instead of struggling with the data management [21].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 10,
      "context" : "However, Big Data generates great challenges to achieve this since its high dimensionality and large example size imply noise accumulation, algorithmic instability and the massive sample pool is often aggregated from heterogeneous sources [12].",
      "startOffset" : 239,
      "endOffset" : 243
    }, {
      "referenceID" : 28,
      "context" : "While feature selection, discretization or imbalanced algorithms to cope with the high dimensionality have drawn the attention of current Big Data frameworks (such as Spark’s MLlib [32]) and researchers [38, 41, 37, 43], algorithms to clean noise are still a",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 36,
      "context" : "While feature selection, discretization or imbalanced algorithms to cope with the high dimensionality have drawn the attention of current Big Data frameworks (such as Spark’s MLlib [32]) and researchers [38, 41, 37, 43], algorithms to clean noise are still a",
      "startOffset" : 203,
      "endOffset" : 219
    }, {
      "referenceID" : 33,
      "context" : "While feature selection, discretization or imbalanced algorithms to cope with the high dimensionality have drawn the attention of current Big Data frameworks (such as Spark’s MLlib [32]) and researchers [38, 41, 37, 43], algorithms to clean noise are still a",
      "startOffset" : 203,
      "endOffset" : 219
    }, {
      "referenceID" : 38,
      "context" : "While feature selection, discretization or imbalanced algorithms to cope with the high dimensionality have drawn the attention of current Big Data frameworks (such as Spark’s MLlib [32]) and researchers [38, 41, 37, 43], algorithms to clean noise are still a",
      "startOffset" : 203,
      "endOffset" : 219
    }, {
      "referenceID" : 39,
      "context" : "Homogeneous Ensemble: HME-BD The homogeneous ensemble is inspired by Cross-Validated Committees Filter (CVCF) [44].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : "Heterogeneous Ensemble: HTE-BD Heterogeneous Ensemble is inspired by Ensemble Filter (EF) [5].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 26,
      "context" : "• We use an exact implementation of KNN with the euclidean distance present in Spark’s community repository, kNN-IS [30] • The linear machine has been replaced by Spark’s implementation of Logistic Regression, which is another linear classifier.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 42,
      "context" : "It has been designed based on the Edited Nearest Neighbor algorithm (ENN) [47] and follows a similarity between instances approach.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "The task is to distinguish between a signal process which produces supersymmetric (SUSY) particles and a background process which does not [2].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : "It was further pre-processed and included in the LibSVM dataset repository [6].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 37,
      "context" : "We carried out experiments on five levels of uniform class noise [42]: for each level of noise, a percentage of the training instances are altered by replacing their actual label by another label from the available classes.",
      "startOffset" : 65,
      "endOffset" : 69
    } ],
    "year" : 2017,
    "abstractText" : "In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.",
    "creator" : "LaTeX with hyperref package"
  }
}