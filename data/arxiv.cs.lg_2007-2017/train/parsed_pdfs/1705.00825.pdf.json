{
  "name" : "1705.00825.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-view Unsupervised Feature Selection by Cross-diffused Matrix Alignment",
    "authors" : [ "Xiaokai Wei", "Bokai Cao" ],
    "emails" : [ "psyu}@uic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Multi-view Unsupervised Feature Selection by Cross-diffused Matrix Alignment\nXiaokai Wei, Bokai Cao and Philip S. Yu Department of Computer Science\nUniversity of Illinois at Chicago, Chicago, IL Email: {xwei2, caobokai, psyu}@uic.edu\nAbstract—Multi-view high-dimensional data become increasingly popular in the big data era. Feature selection is a useful technique for alleviating the curse of dimensionality in multi-view learning. In this paper, we study unsupervised feature selection for multi-view data, as class labels are usually expensive to obtain. Traditional feature selection methods are mostly designed for single-view data and cannot fully exploit the rich information from multi-view data. Existing multi-view feature selection methods are usually based on noisy cluster labels which might not preserve sufficient information from multi-view data. To better utilize multi-view information, we propose a method, CDMAFS, to select features for each view by performing alignment on a cross diffused matrix. We formulate it as a constrained optimization problem and solve it using Quasi-Newton based method. Experiments results on four real-world datasets show that the proposed method is more effective than the state-of-theart methods in multi-view setting.\nI. INTRODUCTION\nData obtained from different sources or feature subsets usually provide complementary information for machine learning tasks, and conventionally they are named as multi-view data. We can observe multi-view data in a wide range of application domains (Figure 1). For example, news about the same event can often be reported in different languages and by different agencies. In the video domain, in addition to features extracted from visual signals, videos are often equipped with textual descriptions and related tags. In medical science, many different diagnosis tools have been developed to obtain a large number of measurements from various laboratory tests, including clinical, imaging, immunologic, serologic features.\nCapability for simultaneous consideration of data coming from multiple views/sources is important for many learning tasks, which is referred to as multi-view learning. Multiple views together depict an enriched picture about the entities of interest and thereby provide an effective way of heterogeneous data fusion. How to effectively incorporate the abundant information from multiple views is critical for different application domains [7] [17]. It has been shown that incorporating information from multiples views can improve the performance of various machine learning tasks. For example, co-regularized spectral clustering [7], by enforcing consensus learning on latent factors, outperforms single-view clustering significantly.\nThe curse of dimensionality is an inevitable problem in the era of big data, which is also one of the major challenges in many multi-view learning scenarios. For example, the\nvocabulary of news articles can contain more than 100, 000 words in each language. Also, the user generated content in social media (such as blog websites) tends to be highly noisy. Such high-dimensional noisy data can hamper the performance and efficiency of many machine learning/data mining tasks. Feature selection is potentially a useful technique for alleviating such issue. Traditional feature selection methods mainly focus on a single view which could be insufficient considering the existence of other views being available. It is desirable to utilize information from other complementary views, when selecting features for each view.\nSince class labels are usually expensive to obtain, unsupervised feature selection usually has wider applicability than its supervised counterpart. The key challenge of unsupervised multi-view feature selection is twofold: (1) how to effectively represent the fused information from multiple views, and (2) how to effectively exploit the fused information representation to select high-quality features. State-of-the-art unsupervised multi-view feature selection approaches [17] [13] fuse information by generating intermediate cluster labels. However, summarizing the information for each instance with a cluster label tends to lose too much information, since the cluster labels are usually noisy and inaccurate. In this paper, we propose a new method, CDMA-FS (Cross Diffused Matrix Alignment based Feature Selection), to address the challenges of multiview feature selection in unsupervised setting. The advantages of our method compared to state-of-the-art approaches [17] [13] can be summarized as follows.\n• We employ a cross diffusion-based approach to learn a consensus similarity graph from multiple views, which retains more information than the cluster labels (Figure 2). • Rather than relying on cluster-label guided sparse regression, we directly exploit the information from the crossdiffused matrix by matrix alignment. • Existing approaches typically have a few parameters which are difficult to set in unsupervised setting. This makes them less practical for real-world applications. In contrast, we provide guidelines for setting the parameter in the proposed method. • Our objective function is not based on linear regression and hence can evaluate the non-linear usefulness of features.\nar X\niv :1\n70 5.\n00 82\n5v 1\n[ cs\n.L G\n] 2\nM ay\n2 01\n7\nThe rest of paper is organized as follows. In section II,\nwe review related work on unsupervised feature selection on single-view or multi-view data. In section III ∼ V, we present the approach of CDMA-FS by aligning with cross-diffusd matrix. Experimental results are discussed in section VI and we conclude our work in section VII."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "Earlier unsupervised feature selection methods [6] [27] usually assign scores to each feature based on certain heuristics and neglect the correlation among features. However, such heuristic based methods usually ignore the correlation among the features and redundancy may exist in the selected features. In recent years, different methods [26] [12] [24] [23] have been proposed to evaluate feature quality jointly. Linear projection based methods [26] [8] [4] [19] with sparsity-inducing L2,1 norm have become prevalent among others. Compared to the heuristic-based methods [6] [27], the major advantage of L2,1-based approaches is that they can evaluate features jointly. Different L2,1 norm-based methods usually differ in the ways they generate pseudo labels and the loss functions on the projection. Unsupervised Discriminative Feature Selection (UDFS) [26] introduces pseudo-label based regression to better capture the information from the local structure. Non-negative Discriminative Feature Selection (NDFS) [8] derives the cluster/pseudo labels from non-negative spectral analysis. Robust Unsupervised Feature Selection (RUFS) [12] and Embedded Unsupervised Feature Selection (EUFS) [19] generate pseudo labels from non-negative matrix factorization. Robust Spectral Feature Selection (RSFS) [16] employs local kernel regression for the cluster indicators and Huber loss for the projection. These methods are only able to evaluate the. To address this issue, Stochastic Neighbor-preserving Feature Selection (SNFS) [25] and Nonlinear Joint Feature Selection (NJFS) [22] are proposed, which can evaluate the non-linear usefulness of features.\nRecently, several pseudo label-based methods have been extended to multi-view setting [17] [13] [15] via cluster consensus learning. In these approaches, pseudo-labels derived from certain clustering algorithms are required to be the same across different views in order to incorporate multi-view information. For example, adaptive Unsupervised Multi-view Feature Selection (AUMFS) [5] rely on spectral clustering on the combined similarity graphs obtained from different views. Multi-View Feature Selection (MVFS) [17] and MVUFS [13] can be seen as extention of NDFS [8] and RUFS [12] to multiview feature selection by enforcing consensus on the cluster indicators from different views, respectively. However, they rely on the cluster labels to guide feature selection, and the noisy cluster labels may lead to suboptimal feature selection results. Also, they evaluate features based on linear regression and hence cannot select high-quality features if they are nonlinearly correlated with the class labels."
    }, {
      "heading" : "III. FUSING DIFFERENT VIEWS BY CROSS DIFFUSION",
      "text" : "We denote n data samples with m views as {X(v)|v = 1, . . . ,m}, X(v) = [x(v)1 ,x (v) 2 , . . . ,x (v) n ] and the number of features in the v-th view as D(v). So x(v)i ∈ RD (v)\nand x(v)ip denotes the value of p-th (p = 1, . . . , D(v)) feature of x(v)i .\nThe proposed CDMA-FS framework is a two-step approach. First, we fuse different kernels into one robust similarity matrix through cross diffusion. Second, we perform matrix alignment for the features from each view so that the kernel constructed from the selected features can best align with the fused matrix (Figure 2). In this manner, feature selection on each view can benefit from the consensus information fused from multiple views.\nWith the features from the v-th view, one can construct a kernel/similarity matrix for this view. There are different types of similarity matrices: • Gaussian Kernel Weighting: Wij = e−(xi−xj) 2/σ2\n• Dot-product Kernel Weighting: Wij = xTi · xj • 0-1 Weighting: Wij = 1 if and only if xi is within xj’s\nk Nearest Neighbors. A similarity matrix can then be used to define the transition\nprobability as follows.\nP(v)ij = W (v) ij∑n\nk=1W (v) ik\n(1)\nwhere ∑n j=1 P (v) ij = 1 (∀i = 1, . . . , n) and we let P (v) ii = 0 for convenience. For a probability vector u (i.e., uT1 = 1), uTP(v) is a Markov random walk of u w.r.t. P(v). P(v)u can be viewed as a local averaging operation with W(v) measuring the locality. It can also be interpreted as a generalization of Parzen window estimators to functions on the local manifold [18]. Both uTP(v) and P(v)u can be viewed as a diffusion process."
    }, {
      "heading" : "A. Cross Diffusion",
      "text" : "Cross diffusion [18] aims to exploit mutual enhancement of different views inspired by co-training [1]. The main idea of\ncross diffusion is to perform random walk using the transition probability from different views in an alternating manner. In the case of m = 2, the cross diffusion process can be defined as follows.\nP (1) t+1 = P (1) ·P(2)t · (P(1))T (2) P\n(2) t+1 = P (2) ·P(1)t · (P(2))T (3)\nwhere P(1)t and P (2) t are the status matrices at the t-th iteration for view 1 and view 2, respectively. For the initial values, we set P(1)1 = P (1) and P(2)1 = P (2). Since the distances between data points are usually unreliable in high-dimensional space, it is usually preferable to use the k nearest neighbors as P(1) and P(2). Under mild conditions that P(1) and P(2) are irreducible and aperiodic, the convergence of this process can be proved using Perron-Frobenius Theorem [11]. The final status matrix can be computed as the average of status matrices from two views: P∗ = (P(1)e +P (2) e )/2, where e is the number of iterations at which the cross diffusion terminates. We refer to this final status matrix P∗ as cross diffused matrix.\nLet us denote the connected components in the crossdiffused matrix as {θ1, θ2, . . . , θQ}, where Q is the total number of connected components. We also denote the groundtruth class label of x as c(x). We define the purity of the q-th connected component as the percentage of majority class of instances. If purity(θq) ≥ 1 − for all 1 ≤ q ≤ Q, we say that P is an -good graph. At the (2t+1)-th iteration, P(1)2t+1 and P(2)2t+1 can be written as the following.\nP (1) 2t+1 ∝ (P (1)P(2))t ·P(2) · ((P(2))T (P(1))T )t (4) P\n(2) 2t+1 ∝ (P (2)P(1))t ·P(1) · ((P(1))T (P(2))T )t (5)\nIn order to effectively guide subsequent feature selection, it is desirable that the connected components in P(1)2t+1 and P\n(2) 2t+1 obtained from the cross-diffusion process have large purity. The following theorem provides guarantee on the purity of components in the cross-diffused matrix [18].\nTheorem 1: If the K-nearest-neighbors is good to measure local affinity [20], P(1)2t+1 and P (2) 2t+1 are -good graphs. The number of connected components in graph P(1)2t+1 is equal to that of graph P(2)2t+1, which is no larger than that in graphs P(1) and P(2).\nMoreover, it is usually helpful to add regularization at each iteration of the diffusion process to make the probability matrix more robust.\nP (1) t+1 = P (1) ·P(2)t · (P(1))T + αI (6) P\n(2) t+1 = P (2) ·P(1)t · (P(2))T + αI (7)\nwhere I is an identity matrix and α is the parameter that controls the regularization. We remark that CDMA-FS can perform reasonably well for a wide range of α (e.g., 10−4 ∼ 10)."
    }, {
      "heading" : "B. Extension to more than two views",
      "text" : "Similar to the case of m = 2, P(v)t+1 for m > 2 can be calculated as follows.\nP (v) t+1 = P (v) · 1 m− 1 ∑ i 6=v P (i) t · (P(v))T (8)\nThe final status matrix is the average of m matrices:\nP∗ = 1\nm m∑ v=1 P(v)e (9)\nSince the transition probability might be not reliable for nonnearest neighbors, we create a kNN graph G from P∗ after obtaining P∗. In the following section, we present how to use G to guide the feature selection for each view."
    }, {
      "heading" : "IV. ALIGNING WITH CROSS-DIFFUSED MATRIX",
      "text" : "Our goal is to select d(v) (d(v) D(v)) high-quality features for each view. We denote the selection indicator vector as s(v) ∈ {0, 1}D(v) , where s(v)p = 1 indicates that the p-th feature is selected and s(v)p = 0 otherwise.\nTo directly exploit the information from the cross-diffused matrix for feature selection in each view, we propose to perform matrix alignment towards the cross-diffused matrix. We assume that a kernel matrix can be constructed from each view based on the selected features diag(s)X(v) with Gaussian kernels (i.e., Radial Basis Function):\nK (v) ij = exp ( − 1 σ2 ‖diag(s(v))x(v)i − diag(s (v))x (v) j ‖ 2 ) (10)\nThe intuitive idea of CDMA-FS is to make the kernel constructed from selected features imitate the cross-diffused matrix G. We achieve this by employing the matrix alignment technique [3] [21] as follows.\nDefinition 1: Matrix Alignment For two symmetric matrices K1 ∈ Rn×n and K2 ∈ Rn×n, the alignment between K1 and K2 is defined as\nρ(K1,K2) = Tr(K1K2)\n||K1||F · ||K2||F (11)\nwhere Tr(·) is the trace of a matrix. Matrix alignment can be viewed as calculating the cosine similarity between two vectorized matrices. However, the normalization term ||K1||F · ||K2||F makes the optimization problem more difficult to solve. In this paper, we employ the unnormalized version of matrix alignment as in [3], which can be considered as the inner product between two vectorized matrices.\nDefinition 2: Unnormalized Matrix Alignment For two symmetric matrices K1 ∈ Rn×n and K2 ∈ Rn×n, the alignment between K1 and K2 is defined as\nρ(K1,K2) = Tr(K1K2) (12)\nIt is usually helpful to center the matrix for better matrix alignment performance as in observed in [2]. For a symmetric\nmatrix K, centering K can be achieved by HKH, where the centering matrix H = I− 1n11\nT . Definition 3: Centered Matrix Alignment For two real matrices K1 ∈ Rn×n and K2 ∈ Rn×n, the centered alignment between K1 and K2 is defined as\nρ(K1,K2) =Tr(HK1HHK2H)\n=Tr(HK1HK2)\nwhere the second equation can be obtained by noting HH = H and Tr(AB) = Tr(BA) for arbitrary matrices A,B ∈ Rn×n.\nAfter a high-quality cross-diffused matrix is obtained, we select features for each view under the guidance of this matrix. To achieve this, we aim to maximize the correlation between the cross-diffused matrix and the kernel matrix computed from selected features. To select d(v) features for the v-th view, we formulate it as a constrained optimization problem and find s(v) to minimize the following objective function:\nmin s(v)\nf = −Tr(HGHK(v))\ns.t. D(v)∑ p=1 s(v)p = d (v)\ns(v)p ∈ {0, 1},∀p = 1, . . . , D(v)\n(13)\nDiscussion Traditional sparse regression based methods [17] [13] rely on generating intermediate cluster labels and rank features by their linear regression coefficients. In contrast, CDMA-FS framework utilizes the cross-diffused matrix, which preserves more information than cluster labels. Also, the connected components in the cross diffused matrix tend to have good purity as shown in Theorem 1, which means the connected data points are likely from the same class. The objective, through matrix alignment, aims to select the features that make connected instances close and unconnected instances far apart. By optimizing the objective above, we directly infer the selection vector s which can achieve the following desirable effects: features that make data points from the same class similar would be rewarded and features that make data points from different classes similar would shrink sp to zero. Hence, different classes would be more separable in the space of selected features."
    }, {
      "heading" : "V. OPTIMIZATION",
      "text" : ""
    }, {
      "heading" : "A. Gradient Derivation with Relaxed Constraint",
      "text" : "The ‘0/1’ integer programming problem in Eq (13) is computationally intensive to optimize. We relax the ‘0/1’ constraint on s(v)p (p = 1, . . . , D(v)) to real values in range of [0, 1] to make the optimization tractable as in [22]. We further rewrite the summation constraint ∑D(v) p=1 s (v) p = d(v) in the form of Lagrange multiplier:\nmin s(v)\nf = −Tr(HGHK(v)) + λ||s(v)||1\ns.t. 0 ≤ s(v)p ≤ 1,∀p = 1, . . . , D(v) (14)\nwhere || · ||1 denotes the l1 norm on vector (·) and λ controls the sparsity of s(v). Note that in our case ||s(v)||1 = ∑D p=1 sp since we have non-negative constraints on s(v). We can derive the following gradient w.r.t. the objective function, since K(v) (v = 1, . . . ,m) is a symmetric matrix.\n∂f\n∂s (v) p\n= − n∑\ni,j=1\n((HGH)ij · ∂K\n(v) ij\n∂s (v) p\n) + λ\n= n∑ i,j=1 (((HGH) K(v))ij ( x (v) ip − x (v) jp )2 ) 2sp σ2 + λ\n(15)\nwhere is element-wise product. To solve this constrained optimization problem efficiently, we use Projected QuasiNewton Method as shown in the next subsection."
    }, {
      "heading" : "B. Projected Quasi-Newton Method",
      "text" : "Traditional Newton method optimizes the following secondorder approximation at the t-th iteration.\nqt(s) = f(st)+(s−st)T∇f(st)+ 1\n2 (s−st)TBt(s−st) (16)\nwhere Bt = ∇2f(st) is the Hessian matrix. Newton method enjoys good convergence rate but the Hessian matrix requires O(D2) storage and it is time-consuming to compute. So QuasiNewton methods (e.g., L-BFGS [9]) use a positive definite approximation to the Hessian matrix∇2f(st). For example, LBFGS [9] uses the gradients in previous iterations to compute an approximate Hessian matrix.\nBt+1 = Bt − Btutu\nT t Bt\nuTt Btut +\nyty T t yTt ut (17)\nwhere ut = st+1 − st and yt = ∇f(st+1)−∇f(st). To address the constraints on s in Eq (14), projected Newton method can be used to solve the following constrained quadratic approximation:\nmin s qt(s)\ns.t. s ∈ C (18)\nIn our case, C is the [0, 1] box constraint on s(v). A projection operator for this constraint can be defined as follows.\n[Proj[0,1](s (v))]p = min(1,max(0, s(v)p )), ∀p = 1, 2, . . . , D(v) (19) To make the optimization more efficient, we use a variant of the L-BFGS method which employs spectral projected gradient method as subroutine to solve the constrained problem in Eq (18). The optimization method [14] is two-level approach: at the outer level, L-BFGS updates are used to construct a sequence of quadratic approximations (with constraints) to the problem; at the inner level, a spectral projected gradient method optimizes the constrained subproblem approximately to generate a feasible direction. The number of iterations in this algorithm remains linear in dimensionality of feature vector, but with a higher constant factor than the L-BFGS method. Nevertheless, the method can lead to significant gain\nwhen the cost of the projection is much lower than evaluating the function, which is the case in our problem setting.\nAlthough we could use spectral projected gradient method to exactly solve problem Eq (18), it is expensive to do so in practice. Therefore, we terminate the spectral gradient descent subroutine before the exact solution is found, since our goal is only to obtain a feasible descent direction for L-BFGS. One might be concerned about the early termination of the spectral gradient descent subroutine, but in [14] it has been shown that the spectral gradient descent subroutine, even when terminated early, can give a descent direction, if we initialize it with st and we perform at least one spectral gradient descent iteration. In the implementation, we can parametrize the maximum number of the spectral gradient descent iterations by tp, the cost of one iteration is O(mtpD) for the inexact Newton method, given that our projection operation requires O(D) time and L-BFGS stores m most recent gradients. The projected Quasi-Newton algorithm is shown in Algorithm 1.\nAlgorithm 1 Solve CDMA-FS with Projected Quasi-Newton Algorithm\nInitialize: s0 ← 1, t = 0. while not converged do\nCompute the gradient by Eq (15) Compute the approximate Hessian Solve Eq (18) for s∗t using projected spectral gradient\nalgorithm. dt = s ∗ t − st\nPerform line search on the direction of dt to satisfy the Armijo condition.\nt = t+ 1 end while Select the features with corresponding entry in s equal to 1."
    }, {
      "heading" : "VI. PARAMETER SELECTION",
      "text" : "Existing multi-view feature selection methods typically have 2 ∼ 3 regularization parameters and it is difficult to choose appropriate values for these parameters when class labels are not available. In the original papers of these psuedolabel approaches [26] [12] [16], only the best performance is reported, the parameters of which are tuned using all the class labels. However, such way of setting parameters violates the assumption of no supervision. In practice, it is impossible to know the best parameter values and this makes them less useful for real world applications.\nFor CDMA-FS, we provide guidelines for choosing the value of parameter λ. Let us denote the number of features with s(v)p = 1 as N (v) 1 , which is influenced by the value of λ. By noting that N (v)1 is a monotonically non-increasing function of λ, we can choose the value of λ for each view that makes N (v)1 equal to (or within a small range of) the feature size one wants to retain."
    }, {
      "heading" : "VII. EXPERIMENTS",
      "text" : "In this section, we compare the proposed method with stateof-the-art baseline methods on four real world datasets."
    }, {
      "heading" : "A. Datasets",
      "text" : "We use four publicly available real-world datasets in our experiments. • Reuters Multilingual dataset 1: News articles in English\nand German on six topics. Each language can be considered a view for the same article. • BBC Sport dataset 2: BBC news articles from 5 topics: athletics, cricket, football, rugby, tennis. Paragraphs in the news articles are used to construct two views. • CNN dataset 3: It consists of news articles from CNN with two views: news text and images in the news. • Blogcatalog dataset 4: A subset of blog posts from Blogcatalog website in the categories of {Autos, Software, Crafts, Football, Career&Jobs}. Two views are the text in posts and the tags associated with the posts, respectively.\nThe statistics of four real-world datasets is summarized in Table I."
    }, {
      "heading" : "B. Baselines",
      "text" : "We compare CDMA-FS with using all features and five other unsupervised feature selection methods as follows: • All Features: It uses all original features without selection\nfor evaluation. • LS: Laplacian Score [6] selects the features that preserve\nthe local manifold structure. • UDFS: Unsupervised Discriminative Feature Selection\n[26] is a pseudo-label based approach with L2,1 regularization to exploit the local structure. • RSFS: Robust Spectral Feature Selection [16] selects features by robust spectral analysis framework with sparse regression. • MVFS: Multi-view Feature Selection [17] is unsupervised feature selection for multi-view data based on pseudo labels, which are generated as the consensus of spectral clustering on two views. • MVUFS: Multi-view Unsupervised Feature Selection [13] generates pseudo-labels by Non-negative Matrix Factorization and local kernel learning.\n1https://archive.ics.uci.edu/ml/datasets/Reuters+RCV1+RCV2+ Multilingual,+Multiview+Text+Categorization+Test+collection\n2http://mlg.ucd.ie/datasets/segment.html 3https://sites.google.com/site/qianmingjie/home/datasets/cnn-and-fox-news 4http://dmml.asu.edu/users/xufei/datasets.html"
    }, {
      "heading" : "C. Experiment setup",
      "text" : "In this section, we evaluate the quality of selected features by their clustering performance. We use the the popular coregularized spectral clustering [7] for clustering multi-view data 5. We set their σ as the median of pairwise Euclidean distances between data points and λ = 0.1 as suggested in the paper. KMeans is then used on these latent factors. We repeat the KMeans experiment for 20 times (since it is initilization) and report the average performance. We vary the number of features d in the range of {100, 200, 300, 400}. For each feature size d, we choose appropriate λ in our method via binary search to let the number of selected features (with score sp = 1) within d± 10.\nFollowing the typical experimental setting for unsupervised feature selection [26] [8] [25], we use Accuracy and Normalized Mutual Information (NMI) to evaluate the result of clustering. Accuracy is defined as follows.\nAccuracy = 1\nn n∑ i=1 I(ci = map(pi)) (20)\nwhere pi is the clustering result of instance i and ci is its real class label. map(·) is a mapping function that maps each cluster label to a ground-truth label using Kuhn-Munkres Algorithm [10].\nNormalized Mutual Information (NMI) is another popular metric for evaluating clustering performance. Let C be the set of clusters from the ground truth and C ′ obtained from a clustering algorithm. Their mutual information MI(C,C ′) can be defined as follows:\nMI(C,C ′) = ∑\nci∈C,c′j∈C′ p(ci, c\n′ j) log\np(ci, c ′ j)\np(ci)p(c′j) (21)\nwhere p(ci) and p(c′j) are the probabilities that a random instance from the data set belongs to ci and c′j , respectively, and p(ci, c′j) is the joint probability that the instance belongs to the cluster ci and c′j at the same time. In our experiments, we use the normalized mutual information as in previous work [8] [16].\nNMI(C,C ′) = MI(C,C ′)\nmax(H(C), H(C ′)) (22)\nwhere H(C) and H(C ′) are the entropy of C and C ′. Higher value of and Accuracy and NMI indicates better quality of clustering.\n5We use the code at http://www.umiacs.umd.edu/∼abhishek/code coregspectral.zip\nWe set k = 5 for the kNN neighbor size in the baseline methods and our approach following previous convention [8]. For the number of pseudo-classes in UDFS, RSFS, MVFS and MVUFS, we use the ground-truth number of classes. Also, we perform grid search in the range of {0.1, 1, 10} for the regularization parameters in these baseline methods. Besides their best performance, we also report the median performance for them. For CDMA-FS proposed in this paper, we use ‘0/1’ weighting in the W and we fix σ2 = 1 and α = 0.01 for all the datasets after normalizing each data point to unit length. We set the maximum number of iterations for the cross-diffusion process as 20."
    }, {
      "heading" : "D. Results",
      "text" : "The clustering accuracy and NMI on four datasets are shown in Table II and III. It can be observed that feature selection is a useful technique for improving the multi-view clustering performance. For example, compared with using all the features, CDMA-FS with 400 features improves the accuracy on BBCSport and BlogCatalog datasets by 26% and 15%, respectively. When comparing with other feature selection methods, we can observe that CDMA-FS performs\nfavorably or comparable to the best performance of baseline methods, the parameters of which are tuned using all the class labels. Considering that in practice one cannot know the best parameters for these baseline methods (since we assume no supervision), their median performance is a better reflection of these methods’ practical power, which is far inferior to CDMA-FS."
    }, {
      "heading" : "E. Parameter Sensitivity",
      "text" : "In this subsection, we study how the regularization α in the cross diffusion process affects the quality of selected features. The performance w.r.t different α on BlogCatalog and CNN is shown in Figure 3. We can observe that the performance is not very sensitive to α, and CDMA-FS can perform reasonably well when α > 10−5. In contrast, the baseline methods in Table II and III tend to be more sensitive w.r.t. the parameter values, as the their median performance differs significantly with best performance."
    }, {
      "heading" : "VIII. CONCLUSION",
      "text" : "High-dimensional multi-view data pose challenges for many machine learning tasks. While feature selection methods can\nbe useful for alleviating the curse of dimensionality, existing approaches either cannot exploit information from multiple views simultaneously or rely on cluster labels for this task. In this paper, we aim to preserve more accurate information from multi-view data by learning a cross-diffused matrix and directly utilize the information by matrix alignment. Experimental results show that CDMA-FS is able to select highquality features on real-world datasets and outperforms the baseline methods significantly."
    } ],
    "references" : [ {
      "title" : "Combining labeled and unlabeled data with co-training",
      "author" : [ "A. Blum", "T. Mitchell" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1998
    }, {
      "title" : "Algorithms for learning kernels based on centered alignment",
      "author" : [ "C. Cortes", "M. Mohri", "A. Rostamizadeh" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "On kernel-target alignment",
      "author" : [ "N. Cristianini", "J. Shawe-Taylor", "A. Elisseeff", "J.S. Kandola" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "Unsupervised feature selection with adaptive structure learning",
      "author" : [ "L. Du", "Y.-D. Shen" ],
      "venue" : "In KDD,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Adaptive unsupervised multi-view feature selection for visual concept recognition",
      "author" : [ "Y. Feng", "J. Xiao", "Y. Zhuang", "X. L" ],
      "venue" : "In ACCV (1),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Laplacian score for feature selection",
      "author" : [ "X. He", "D. Cai", "P. Niyogi" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "Co-regularized multi-view spectral clustering",
      "author" : [ "A. Kumar", "P. Rai", "H.D. III" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Unsupervised feature selection using nonnegative spectral analysis",
      "author" : [ "Z. Li", "Y. Yang", "J. Liu", "X. Zhou", "H. Lu" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "On the limited memory bfgs method for large scale optimization",
      "author" : [ "D.C. Liu", "J. Nocedal" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1989
    }, {
      "title" : "Algorithms for the assignment and transportation problems",
      "author" : [ "J. Munkres" ],
      "venue" : "Journal of the Society of Industrial and Applied Mathematics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1957
    }, {
      "title" : "Zur Theorie der Matrices",
      "author" : [ "O. Perron" ],
      "venue" : "Mathematische Annalen,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1907
    }, {
      "title" : "Robust unsupervised feature selection",
      "author" : [ "M. Qian", "C. Zhai" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Unsupervised feature selection for multi-view clustering on text-image web news data",
      "author" : [ "M. Qian", "C. Zhai" ],
      "venue" : "In CIKM,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Optimizing costly functions with simple constraints: A limited-memory projected quasi-newton algorithm",
      "author" : [ "M. Schmidt", "E.V.D. Berg", "M.P. Friedl", "K. Murphy" ],
      "venue" : "In In AI & Statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Online unsupervised multi-view feature selection",
      "author" : [ "W. Shao", "L. He", "C.-T. Lu", "X. Wei", "P.S. Yu" ],
      "venue" : "In ICDM,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Robust spectral learning for unsupervised feature selection",
      "author" : [ "L. Shi", "L. Du", "Y.-D. Shen" ],
      "venue" : "In ICDM,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Unsupervised feature selection for multi-view data in social media",
      "author" : [ "J. Tang", "X. Hu", "H. Gao", "H. Liu" ],
      "venue" : "In SDM,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Unsupervised metric fusion by cross diffusion",
      "author" : [ "B. Wang", "J. Jiang", "W. Wang", "Z.-H. Zhou", "Z. Tu" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Embedded unsupervised feature selection",
      "author" : [ "S. Wang", "J. Tang", "H. Liu" ],
      "venue" : "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "A new analysis of co-training",
      "author" : [ "W. Wang", "Z.-H. Zhou" ],
      "venue" : "In ICML, pages 1135–1142,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "Community detection with partially observable links and node attributes",
      "author" : [ "X. Wei", "B. Cao", "W. Shao", "C.-T. Lu", "P.S. Yu" ],
      "venue" : "In IEEE International Conference on Big Data,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Nonlinear joint unsupervised feature selection",
      "author" : [ "X. Wei", "B. Cao", "P.S. Yu" ],
      "venue" : "In SDM,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Unsupervised feature selection on networks: A generative view",
      "author" : [ "X. Wei", "B. Cao", "P.S. Yu" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Efficient partial order preserving unsupervised feature selection on networks",
      "author" : [ "X. Wei", "S. Xie", "P.S. Yu" ],
      "venue" : "In SDM,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Unsupervised feature selection by preserving stochastic neighbors",
      "author" : [ "X. Wei", "P.S. Yu" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    }, {
      "title" : "l2, 1-norm regularized discriminative feature selection for unsupervised learning",
      "author" : [ "Y. Yang", "H.T. Shen", "Z. Ma", "Z. Huang", "X. Zhou" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "Spectral feature selection for supervised and unsupervised learning",
      "author" : [ "Z. Zhao", "H. Liu" ],
      "venue" : "In ICML,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "How to effectively incorporate the abundant information from multiple views is critical for different application domains [7] [17].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "How to effectively incorporate the abundant information from multiple views is critical for different application domains [7] [17].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "For example, co-regularized spectral clustering [7], by enforcing consensus learning on latent factors, outperforms single-view clustering significantly.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 16,
      "context" : "State-of-the-art unsupervised multi-view feature selection approaches [17] [13] fuse information by generating intermediate cluster labels.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 12,
      "context" : "State-of-the-art unsupervised multi-view feature selection approaches [17] [13] fuse information by generating intermediate cluster labels.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 16,
      "context" : "The advantages of our method compared to state-of-the-art approaches [17] [13] can be summarized as follows.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 12,
      "context" : "The advantages of our method compared to state-of-the-art approaches [17] [13] can be summarized as follows.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "Earlier unsupervised feature selection methods [6] [27] usually assign scores to each feature based on certain heuristics and neglect the correlation among features.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 26,
      "context" : "Earlier unsupervised feature selection methods [6] [27] usually assign scores to each feature based on certain heuristics and neglect the correlation among features.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 25,
      "context" : "In recent years, different methods [26] [12] [24] [23] have been proposed to evaluate feature quality jointly.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "In recent years, different methods [26] [12] [24] [23] have been proposed to evaluate feature quality jointly.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 23,
      "context" : "In recent years, different methods [26] [12] [24] [23] have been proposed to evaluate feature quality jointly.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 22,
      "context" : "In recent years, different methods [26] [12] [24] [23] have been proposed to evaluate feature quality jointly.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 25,
      "context" : "Linear projection based methods [26] [8] [4] [19] with sparsity-inducing L2,1 norm have become prevalent among others.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 7,
      "context" : "Linear projection based methods [26] [8] [4] [19] with sparsity-inducing L2,1 norm have become prevalent among others.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "Linear projection based methods [26] [8] [4] [19] with sparsity-inducing L2,1 norm have become prevalent among others.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 18,
      "context" : "Linear projection based methods [26] [8] [4] [19] with sparsity-inducing L2,1 norm have become prevalent among others.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 5,
      "context" : "Compared to the heuristic-based methods [6] [27], the major advantage of L2,1-based approaches is that they can evaluate features jointly.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 26,
      "context" : "Compared to the heuristic-based methods [6] [27], the major advantage of L2,1-based approaches is that they can evaluate features jointly.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 25,
      "context" : "Unsupervised Discriminative Feature Selection (UDFS) [26] introduces pseudo-label based regression to better capture the information from the local structure.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "Non-negative Discriminative Feature Selection (NDFS) [8] derives the cluster/pseudo labels from non-negative spectral analysis.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 11,
      "context" : "Robust Unsupervised Feature Selection (RUFS) [12] and Embedded Unsupervised Feature Selection (EUFS) [19] generate pseudo labels from non-negative matrix factorization.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "Robust Unsupervised Feature Selection (RUFS) [12] and Embedded Unsupervised Feature Selection (EUFS) [19] generate pseudo labels from non-negative matrix factorization.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "Robust Spectral Feature Selection (RSFS) [16] employs local kernel regression for the cluster indicators and Huber loss for the projection.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 24,
      "context" : "To address this issue, Stochastic Neighbor-preserving Feature Selection (SNFS) [25] and Nonlinear Joint Feature Selection (NJFS) [22] are proposed, which can evaluate the non-linear usefulness of features.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 21,
      "context" : "To address this issue, Stochastic Neighbor-preserving Feature Selection (SNFS) [25] and Nonlinear Joint Feature Selection (NJFS) [22] are proposed, which can evaluate the non-linear usefulness of features.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "Recently, several pseudo label-based methods have been extended to multi-view setting [17] [13] [15] via cluster consensus learning.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "Recently, several pseudo label-based methods have been extended to multi-view setting [17] [13] [15] via cluster consensus learning.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "Recently, several pseudo label-based methods have been extended to multi-view setting [17] [13] [15] via cluster consensus learning.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "For example, adaptive Unsupervised Multi-view Feature Selection (AUMFS) [5] rely on spectral clustering on the combined similarity graphs obtained from different views.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "Multi-View Feature Selection (MVFS) [17] and MVUFS [13] can be seen as extention of NDFS [8] and RUFS [12] to multiview feature selection by enforcing consensus on the cluster indicators from different views, respectively.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : "Multi-View Feature Selection (MVFS) [17] and MVUFS [13] can be seen as extention of NDFS [8] and RUFS [12] to multiview feature selection by enforcing consensus on the cluster indicators from different views, respectively.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "Multi-View Feature Selection (MVFS) [17] and MVUFS [13] can be seen as extention of NDFS [8] and RUFS [12] to multiview feature selection by enforcing consensus on the cluster indicators from different views, respectively.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "Multi-View Feature Selection (MVFS) [17] and MVUFS [13] can be seen as extention of NDFS [8] and RUFS [12] to multiview feature selection by enforcing consensus on the cluster indicators from different views, respectively.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "It can also be interpreted as a generalization of Parzen window estimators to functions on the local manifold [18].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 17,
      "context" : "Cross diffusion [18] aims to exploit mutual enhancement of different views inspired by co-training [1].",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "Cross diffusion [18] aims to exploit mutual enhancement of different views inspired by co-training [1].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "Under mild conditions that P and P are irreducible and aperiodic, the convergence of this process can be proved using Perron-Frobenius Theorem [11].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 17,
      "context" : "The following theorem provides guarantee on the purity of components in the cross-diffused matrix [18].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 19,
      "context" : "Theorem 1: If the K-nearest-neighbors is good to measure local affinity [20], P 2t+1 and P (2) 2t+1 are -good graphs.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "We achieve this by employing the matrix alignment technique [3] [21] as follows.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 20,
      "context" : "We achieve this by employing the matrix alignment technique [3] [21] as follows.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "In this paper, we employ the unnormalized version of matrix alignment as in [3], which can be considered as the inner product between two vectorized matrices.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "It is usually helpful to center the matrix for better matrix alignment performance as in observed in [2].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "Discussion Traditional sparse regression based methods [17] [13] rely on generating intermediate cluster labels and rank features by their linear regression coefficients.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "Discussion Traditional sparse regression based methods [17] [13] rely on generating intermediate cluster labels and rank features by their linear regression coefficients.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : ", D) to real values in range of [0, 1] to make the optimization tractable as in [22].",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : ", D) to real values in range of [0, 1] to make the optimization tractable as in [22].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : ", L-BFGS [9]) use a positive definite approximation to the Hessian matrix∇f(st).",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 8,
      "context" : "For example, LBFGS [9] uses the gradients in previous iterations to compute an approximate Hessian matrix.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "In our case, C is the [0, 1] box constraint on s.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "[Proj[0,1](s )]p = min(1,max(0, s p )), ∀p = 1, 2, .",
      "startOffset" : 5,
      "endOffset" : 10
    }, {
      "referenceID" : 13,
      "context" : "The optimization method [14] is two-level approach: at the outer level, L-BFGS updates are used to construct a sequence of quadratic approximations (with constraints) to the problem; at the inner level, a spectral projected gradient method optimizes the constrained subproblem approximately to generate a feasible direction.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 13,
      "context" : "One might be concerned about the early termination of the spectral gradient descent subroutine, but in [14] it has been shown that the spectral gradient descent subroutine, even when terminated early, can give a descent direction, if we initialize it with st and we perform at least one spectral gradient descent iteration.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 25,
      "context" : "In the original papers of these psuedolabel approaches [26] [12] [16], only the best performance is reported, the parameters of which are tuned using all the class labels.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "In the original papers of these psuedolabel approaches [26] [12] [16], only the best performance is reported, the parameters of which are tuned using all the class labels.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 15,
      "context" : "In the original papers of these psuedolabel approaches [26] [12] [16], only the best performance is reported, the parameters of which are tuned using all the class labels.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "• LS: Laplacian Score [6] selects the features that preserve the local manifold structure.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 25,
      "context" : "• UDFS: Unsupervised Discriminative Feature Selection [26] is a pseudo-label based approach with L2,1 regularization to exploit the local structure.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : "• RSFS: Robust Spectral Feature Selection [16] selects features by robust spectral analysis framework with sparse regression.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 16,
      "context" : "• MVFS: Multi-view Feature Selection [17] is unsupervised feature selection for multi-view data based on pseudo labels, which are generated as the consensus of spectral clustering on two views.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 12,
      "context" : "• MVUFS: Multi-view Unsupervised Feature Selection [13] generates pseudo-labels by Non-negative Matrix Factorization and local kernel learning.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "We use the the popular coregularized spectral clustering [7] for clustering multi-view data 5.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 25,
      "context" : "Following the typical experimental setting for unsupervised feature selection [26] [8] [25], we use Accuracy and Normalized Mutual Information (NMI) to evaluate the result of clustering.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "Following the typical experimental setting for unsupervised feature selection [26] [8] [25], we use Accuracy and Normalized Mutual Information (NMI) to evaluate the result of clustering.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 24,
      "context" : "Following the typical experimental setting for unsupervised feature selection [26] [8] [25], we use Accuracy and Normalized Mutual Information (NMI) to evaluate the result of clustering.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "map(·) is a mapping function that maps each cluster label to a ground-truth label using Kuhn-Munkres Algorithm [10].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "In our experiments, we use the normalized mutual information as in previous work [8] [16].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "In our experiments, we use the normalized mutual information as in previous work [8] [16].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "We set k = 5 for the kNN neighbor size in the baseline methods and our approach following previous convention [8].",
      "startOffset" : 110,
      "endOffset" : 113
    } ],
    "year" : 2017,
    "abstractText" : "Multi-view high-dimensional data become increasingly popular in the big data era. Feature selection is a useful technique for alleviating the curse of dimensionality in multi-view learning. In this paper, we study unsupervised feature selection for multi-view data, as class labels are usually expensive to obtain. Traditional feature selection methods are mostly designed for single-view data and cannot fully exploit the rich information from multi-view data. Existing multi-view feature selection methods are usually based on noisy cluster labels which might not preserve sufficient information from multi-view data. To better utilize multi-view information, we propose a method, CDMAFS, to select features for each view by performing alignment on a cross diffused matrix. We formulate it as a constrained optimization problem and solve it using Quasi-Newton based method. Experiments results on four real-world datasets show that the proposed method is more effective than the state-of-theart methods in multi-view setting.",
    "creator" : "LaTeX with hyperref package"
  }
}