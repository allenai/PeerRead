{
  "name" : "1302.6794.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Estimation of the Value of Information in Monte Carlo Models",
    "authors" : [ "Tom Chavez", "Max Henrion" ],
    "emails" : [ ],
    "sections" : null,
    "references" : [ {
      "title" : "Recovering Value oflnformation from Pairwise Peeks,",
      "author" : [ "Chavez", "Tom" ],
      "venue" : "Rockwell Palo Alto Science Lab. Technical Memorandum",
      "citeRegEx" : "Chavez and Tom.,? \\Q1994\\E",
      "shortCiteRegEx" : "Chavez and Tom.",
      "year" : 1994
    }, {
      "title" : "Uncertainty: A Guilk ro Dealing with Uncertainry in Quantitative Risk and Policy Analysis",
      "author" : [ "Hennon. Max", "Morgan", "Granger" ],
      "venue" : null,
      "citeRegEx" : "Max et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Max et al\\.",
      "year" : 1990
    }, {
      "title" : "Proximal Decision Analysis.",
      "author" : [ "A. Howard. R" ],
      "venue" : "Management Science,",
      "citeRegEx" : "R.,? \\Q1970\\E",
      "shortCiteRegEx" : "R.",
      "year" : 1970
    }, {
      "title" : "Applied Statistical Decision Th�ory",
      "author" : [ "Raiffa", "Howard", "Schlaifer", "Raben" ],
      "venue" : null,
      "citeRegEx" : "Raiffa et al\\.,? \\Q1961\\E",
      "shortCiteRegEx" : "Raiffa et al\\.",
      "year" : 1961
    }, {
      "title" : "Statistical Reasoning for IM Behavioral Sciences",
      "author" : [ "Shavelson", "Richard" ],
      "venue" : null,
      "citeRegEx" : "Shavelson and Richard.,? \\Q1988\\E",
      "shortCiteRegEx" : "Shavelson and Richard.",
      "year" : 1988
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "The expected value of information (EVI) is the most powerful measure of sensitivity to uncer­ tainty in a decision model: it measures the potential of information to improve the decision, and hence measures the expected value of the outcome. Stan­ dard methods for computing EVI use discrete vari­ ables and are computationally intractable for models that contain more than a few variables. Monte Carlo simulation provides the basis for more tractable evaluation of large predictive models with continu­ ous and discrete variables, but so far computation of EVI in a Monte Carlo setting also has appeared impractical. We introduce an approximate approach based on preposterior analysis for estimating EVI in Monte Carlo models. Our method uses a linear approximation to the value function and multiple linear regression to estimate the linear model from the samples. The approach is efficient and practical for extremely large models. It allows easy estima­ tion of EVI for perfect or panial information on individual variables or on combinations of variables. We illustrate its implementation within Demos (a decision modeling system), and its application to a large model for crisis transponation planning. 1.0 EVI: What's so, and What's New A ny model is inevitably a simplification of reality, and most of its input quantities are invariably uncertain. Sensi­ tivity analysis identifies which sources of uncertainty in a model affect its outputs most significantly. In this way, it helps a decision maker focus attention on what assump­ tions really matter. It also helps a decision modeler to assign priorities to his efforts to improve, refine, or extend his model by identifying those variables for which it will be most valuable to find more complete data, to interview more knowledgeable experts, or to build more elaborate submodels. The expected value of information (EVI) on a variable xi measures the expected increase in value y if we learn new information about xi and make a decision with higher­ expected value in light of that information. It is the most powerful method of sensitivity analysis because it ana­ lyzes a variable's importance in terms of the overall pre­ scription for action, and it expresses that importance in the utility or value units of the problem. Other methods, such as rank-order correlation, express importance in terms of the correlation between an uncertain variable and the out­ put of the decision model. There are many cases where a variable can show high sensitivity in this way, yet still have no effect on the selection of an optimal decision. Deterministic perturbation measures importance in utility or value units, but it ignores nonlinearities and interactions among variables, and also fails to measure a variable's importance in terms of that variable's ability to change the recommended decision. 120 Chavez and Henrion One calculates EVPI (Expected Value of Perfect Infor· mation) in discrete models by rolling back the decision tree. The computation itself is straightforward in the sense that, to compute EVI, one simply places at the front of the tree the chance variables to be observed. The EVPI is computed as the difference between the expected value computed for this scenario and the expected value for the regular tree, without observations. Computing EVI with continuous variables is less intuitive, because we have no tidy way of reversing the uncertainty, unlike the discrete case. Yet continuous models are increasingly the norm for risk and decision analysis, first because discretizing inherently continuous variables intro­ duces unnecessary approximation, and second because Monte Carlo methods and their variants (e.g., Latin hyper­ cube) generate tractable, highly efficient solutions to pre­ dictive models that contain thousands of variables. An especially useful feature of the Monte Carlo method is that, for a specified error, the computational complexity increases linearly in the number of uncertain variables [Morgan and Henrion, 1990]. Exact methods require com­ putation time that is exponential in the number of vari­ ables. There is thus a need to develop flexible, efficient methods for computing EVI on continuous variables in a Monte Carlo setting. A ftexible method has (I) the ability to com­ pute EVI on single variables or on any combination of variables, and (2) the ability to compute both perfect and partial values of information. Perfect information removes uncertainty entirely. Partial information reduces uncertainty. We present a general framework for calculating EVI based on preposterior analysis. Using that framework, we develop a technique for computing EVI that depends on a linear approximation to the value function and on multiple linear regression to estimate the constants for the linear function. We also discuss a heuristic method for measuring the value of partial information in terms of what we call the relative information multiple (RIM). We have implemented these methods in detachable computational modules using Demos, a decision modeling system from Lumina, Inc., Palo Alto, CA. We demonstrate their use on a large model to aid in military transportation crisis plan­ ning. 2.0 Framework A decision model consists of a set of n state variables x1, •• ,xn, which we will denote by X. The decision maker has control of a decision variable D, which can assume one of m possible values d1 , .. ,dm. The value or utility func­ tion v(X,di) expresses the payoff to the decision maker when X obtains and decision di is chosen. In a typical decision model, the state variables are uncer­ tain. We express prior knowledge about X in the form of a probability distribution, denoted {XI �},where �denotes a prior state of information. The optimal Bayes' decision maximizing the expected value1 is given by l = Arg max ( X d I l' ) d v ( ' ) .., . The optimal decision given perfect information on state variable x , denoted d* _., is d• x __ Arg max d (v(X,d)Jx,�) We define EVPI on x as EVPI (x) = (v (X, l x) I �){v (X, d*) I �). In a similar fashion, we define the optimal expected­ value decision given the revelation of evidence e, d* e• as le= Arg d max (v(X,d)Je.�) Then the EVI for evidence e is EVI(e) = (v(X,d.e)l�)-(v(X,l)l�) 2.1 Binary decisions and Function z Let us consider a simplified decision problem with two decision alternatives: one of them is the optimal Bayes' decision d*; the other we denote tr\". In view of the uncertainty in the state variables, there must exist uncertainty in the outputs as well. Thus, for each 1. We use Howard's inferential notation (see, for example, Howard, 1970). {XIS) denotes the probability density of X condi­ tional on S; (XIS) denotes the expectation of X conditional on S. Efficient Estimation of the Value of Information in Monte Carlo Models 121 decision d;, there exists a unique probability distribution on value { v(X,d;)l �} (see Figure 1). For notational conve­ nience we let v(d) = (v(X, d) I�). We now define z = v(X,l) -v(X,tf). Function z is the pivotal element in our framework for computing EVI because it describes the difference in value between the best and second-best decisions. In Figure 2, we have graphed the probability distribution of z. The shaded area represents the total probability of making a bad decision, i.e., doing d' when cY would yield higher value. Exploiting information encoded in the shaded, neg­ ative portion of the z distribution's curve will provide the necessary clues to compute EVPI and EVI. FIGURE 1. Probability �istributions on value for the two decisions d and cr-. Pro density {v(X,tf)[�} {v(X,l)[s} FIGURE 2. Function z: the difference in value between the best and second-best decisions.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}