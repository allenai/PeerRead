{
  "name" : "1604.08120.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Extracting Temporal and Causal Relations between Events",
    "authors" : [ "Paramita Mirza", "Sara Tonelli" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "PhD Dissertation",
      "text" : ""
    }, {
      "heading" : "International Doctorate School in Information and Communication Technologies",
      "text" : "DISI - University of Trento\nExtracting Temporal and Causal Relations between Events\nParamita Mirza\nAdvisor:\nDr. Sara Tonelli\nUniversità degli Studi di Trento\nApril 2016\nar X\niv :1\n60 4.\n08 12\n0v 1\n[ cs\n.C L\n] 2\n7 A\npr 2\n01 6\nParamita Mirza: Extracting Temporal and Causal Relations between Events © April 2016\nPost hoc ergo propter hoc. Post ‘doc’ ergo propter ‘doc’.\nA B S T R A C T\nStructured information resulting from temporal information processing is crucial for a variety of natural language processing tasks, for instance to generate timeline summarization of events from news documents, or to answer temporal/causal-related questions about some events. In this thesis we present a framework for an integrated temporal and causal relation extraction system. We first develop a robust extraction component for each type of relations, i.e. temporal order and causality. We then combine the two extraction components into an integrated relation extraction system, CATENA—CAusal and Temporal relation Extraction from NAtural language texts—, by utilizing the presumption about event precedence in causality, that causing events must happened BEFORE resulting events. Several resources and techniques to improve our relation extraction systems are also discussed, including word embeddings and training data expansion. Finally, we report our adaptation efforts of temporal information processing for languages other than English, namely Italian and Indonesian."
    }, {
      "heading" : "Keywords",
      "text" : "[Temporal information extraction, Temporal ordering of events, Event causality]\nv\nP U B L I C AT I O N S\nMirza, Paramita and Sara Tonelli (2016). “CATENA: CAusal and Temporal relation Extraction from NAtural language texts.” Submitted to the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016).\nMirza, Paramita (2016). “Computational Linguistics: 14th International Conference of the Pacific Association for Computational Linguistics, PACLING 2015, Bali, Indonesia, May 19-21, 2015, Revised Selected Papers.” In: Singapore: Springer Singapore. Chap. Recognizing and Normalizing Temporal Expressions in Indonesian Texts, pp. 135– 147. url: http://dx.doi.org/10.1007/978-981-10-0515-2_10.\nMirza, Paramita and Anne-Lyse Minard (2015). “HLT-FBK: a Complete Temporal Processing System for QA TempEval.” In: Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015). Denver, Colorado: Association for Computational Linguistics, pp. 801–805. url: http : / / www . aclweb . org / anthology / S15 - 2135.\nMirza, Paramita and Anne-Lyse Minard (2014). “FBK-HLT-time: a complete Italian Temporal Processing system for EVENTI-Evalita 2014.” In: Proceedings of the 4th International Workshop EVALITA-2014. Pisa, Italy, pp. 44–49. url: http://clic.humnet.unipi.it/ proceedings/Proceedings-EVALITA-2014.pdf.\nMirza, Paramita and Sara Tonelli (2014a). “An Analysis of Causality between Events and its Relation to Temporal Information.” In: Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin, Ireland: Dublin City University and Association for Computational Linguistics, pp. 2097–2106. url: http://www.aclweb.org/anthology/C14-1198.\nMirza, Paramita (2014). “Extracting Temporal and Causal Relations between Events.” In: Proceedings of the ACL 2014 Student Research Workshop. Baltimore, Maryland, USA: Association for Computational Linguistics, pp. 10–17. url: http://www.aclweb.org/ anthology/P/P14/P14-3002.\nMirza, Paramita and Sara Tonelli (2014b). “Classifying Temporal Relations with Simple Features.” In: Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Gothenburg, Sweden: Association for Computational Linguistics, pp. 308–317. url: http : / / www . aclweb . org / anthology / E14 - 1033.\nMirza, Paramita, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza (2014). “Annotating Causality in the TempEval-3 Corpus.” In: Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL). Gothenburg, Sweden: Association for Computational Linguistics, pp. 10–19. url: http://www.aclweb.org/ anthology/W14-0702.\nvii\nResearch is what I’m doing when I don’t know what I’m doing.\n— Wernher von Braun\nA C K N O W L E D G M E N T S\nFirst and foremost, I would like to express my sincere gratitude to my advisor, Sara Tonelli, for the continuous support during my PhD journey, for her patience, motivation, and immense knowledge. I could not have imagined having a better advisor and mentor. Thank you for helping me find out what I should do, when I don’t know what I’m doing.\nI would like to thank the members of my thesis examination committee: Alessandro Lenci, German Rigau and Steven Bethard, for their insightful comments, feedback and suggestions to widen my research taking into account various perspectives.\nI thank the NewsReader Project1 and the people involved in it: Anne-Lyse Minard, Luciano Serafini, Manuela Speranza and Rachele Sprugnoli, for fruitful research collaborations. I also thank Rosella Gennari and Pierpaolo Vittorini for the ‘remote’ collaboration; Ilija Ilievski, Min-Yen Kan and Hwee Tou Ng for the enlightening ideas and discussions in Singapore.\nThanks to “Algorithm for Temporal Information Processing of Text and their Applications” by Oleksandr Kolomiyets, and “Finding Event, Temporal and Causal Structure in Text: A Machine Learning Approach” by Steven Bethard, which served as “The Hitchhiker’s Guide to the Thesiswriting Galaxy” for this particular topic of PhD research.\nThanks to my fellow officemates—FBK Crew—for the stimulating and fun discussions during our lunches and coffee-breaks for the last three and a half years. Thanks to friends in Trento and Bolzano for their supports and cheers.\nThanks to my family—my mum, dad and my sister—for your endless support all these years; my dearest cats, for your warmth and purrs. Thanks to Simon, for everything.\nSpecial thanks to ‘trees’ for providing countless papers, for printing research papers and thesis drafts.\n1 This work has been supported by the European Union’s 7th Framework Programme via the NewsReader Project (ICT- 316404) http://www.newsreader-project.eu/.\nix\nC O N T E N T S"
    }, {
      "heading" : "1 introduction 1",
      "text" : "1.1 Motivations and Goals 3 1.2 Contributions 5 1.3 Structure of the Thesis 6"
    }, {
      "heading" : "2 background 7",
      "text" : "2.1 Natural Language Processing 7\n2.1.1 Morphological Analysis 7 2.1.2 Syntactic Analysis 8 2.1.3 Information Extraction 10\n2.2 Techniques for Information Extraction 11 2.2.1 Rule-based Methods 11 2.2.2 Supervised Machine Learning 12 2.2.3 Hybrid Approaches 17 2.2.4 Semi-supervised Machine Learning 17 2.2.5 Word Embeddings 19"
    }, {
      "heading" : "3 temporal information processing 23",
      "text" : "3.1 Modelling Temporal Information 23 3.2 TimeML Annotation Standard 25\n3.2.1 TimeML Tags 25 3.2.2 ISO Related Standards 29\n3.3 TimeML Annotated Corpora 31 3.4 TempEval Evaluation Campaigns 33 3.5 State-of-the-art Methods 35\n3.5.1 Timex Extraction 35 3.5.2 Event Extraction 37 3.5.3 Temporal Relation Extraction 37 3.5.4 Temporal Information Processing 38\n3.6 Conclusions 38 4 temporal relation extraction 39\n4.1 Introduction 39 4.2 Related Work 41 4.3 Related Publications 41 4.4 Formal Task Definition 42 4.5 Method 42\n4.5.1 Temporal Relation Identification 43 4.5.2 Temporal Relation Type Classification 43\n4.6 Evaluation 50 4.6.1 TempEval-3 Evaluation 50 4.6.2 TimeBank-Dense Evaluation 52 4.6.3 QA TempEval Evaluation 54\n4.7 Conclusions 56 5 annotating causality between events 59\n5.1 Introduction 59\nxi\nxii contents\n5.2 Related Work 60 5.3 Related Publications 61 5.4 TimeML-based Causality Annotation Guidelines 61 5.5 Causal-TimeBank 65 5.5.1 Corpus Statistics 65 5.6 Conclusions 67\n6 causal relation extraction 69 6.1 Introduction 69 6.2 Related Work 70 6.3 Related Publications 71 6.4 Formal Task Definition 71 6.5 Method 71\n6.5.1 Candidate Event Pairs 72 6.5.2 Causal Verb Rules 72 6.5.3 CLINK Classifier 74\n6.6 Evaluation 76 6.7 Conclusions 78\n7 integrated system for temporal and causal relations 79 7.1 Introduction 79 7.2 Related Work 80 7.3 Related Publications 80 7.4 Temporal and Causal Links in Causal-TimeBank 80 7.5 Temporal and Causal Links as Features 81\n7.5.1 TLINKs for Causal Relation Extraction 81 7.5.2 CLINKs for Temporal Relation Extraction 82\n7.6 Integrated System - CATENA 82 7.7 Evaluation 83 7.8 Conclusions 85\n8 word embeddings for temporal relations 87 8.1 Introduction 87 8.2 Related Work 88 8.3 Experiments 88 8.4 Evaluation 91 8.5 Conclusions 92\n9 training data expansion 93 9.1 Introduction 93 9.2 Related Work 93 9.3 Temporal Reasoning on Demand 94\n9.3.1 Predicting Number of Deduced TLINKs 96 9.3.2 Experiments 96\n9.4 Semi-supervised Learning 99 9.4.1 CLINK Extraction System 99 9.4.2 Experiments 101\n9.5 Conclusions 105 10 multilinguality in temporal processing 107\n10.1 Introduction 107 10.2 Related Work 108\ncontents xiii\n10.3 Related Publications 108 10.4 Italian 108\n10.4.1 Temporal Information Extraction System 109 10.4.2 EVENTI (Evalita 2014) Evaluation 114\n10.5 Indonesian 117 10.5.1 Challenges in Indonesian 117 10.5.2 Timex Extraction System 118 10.5.3 Temporal Tagging 122 10.5.4 Evaluation 122\n10.6 Conclusions 124 11 conclusions and outlook 127\n11.1 Conclusions 127 11.2 Ongoing and Future Work 130\ni appendix 131 a appendix 133\na.1 Temporal Signal Lists 133 a.2 Causal Verb & Signal Lists 136 a.3 Temporal Tagging Algorithm 138\nbibliography 141\nL I S T O F F I G U R E S\nFigure 1.1 The LOF caption 2 Figure 2.1 Two variants of syntactic parse trees for “I saw a boy with a dog”. The interpretation represented by (a) is the most likely semantic representation and means that “the boy” was “with a dog”. 9\nFigure 2.2 Dependency trees for a sentence “I saw a boy with a dog”, using (a) Stanford CoreNLP tool suite and (b) Mate tools. 9 Figure 2.3 Regular expressions for extracting time and date in the POSIX Extended Regular Expression (ERE) syntax. 12 Figure 2.4 Support Vector Machines with two characteristics hyperplanes H1 and H2 (Burges, 1998). The data points x that lie on the hyperplanes H1 and H2 are called support vectors (circled), satisfying w · x + b = 0, where w is normal to the hyperplane, |b|‖w‖ is the perpendicular distance from the hyperplane to the origin, and ‖w‖ is the Euclidean norm of w. 13 Figure 2.5 Form of logistic function. In Logistic Regression, P(Y|X) is assumed to follow this form. 15 Figure 2.6 Examples of hybrid architecture for information processing. 17 Figure 3.1 Text excerpt annotated with temporal entities and temporal relations in TimeML standard. 30 Figure 4.1 Our proposed temporal relation extraction system, TempRelPro 43 Figure 5.1 Text excerpt annotated with temporal entities and temporal rela-\ntions in TimeML standard, as well as causal signals and causal relations following our annotation guidelines. 64\nFigure 6.1 Our proposed causal relation extraction system, CauseRelPro 72 Figure 7.1 CATENA, CAusal and Temporal relation Extraction from NAtural\nlanguage texts 83 Figure 9.1 Classifier performances using TBAQ and TBAQc as training set\nFigure 10.3 Part of the FST’s transition diagram used to recognize tiga hari yang lalu [three days ago] as a temporal expression of date type. 121\nxiv\nList of Tables xv\nL I S T O F TA B L E S\nTable 3.1 Allen’s atomic relations, their semantics when interpreted over the real line, and their corresponding TimeML TLINK type 27 Table 3.2 State-of-the-art temporal information processing systems according to TempEval-3 36 Table 4.1 CAEVO’s classifiers vs TempRelPro’s classifiers 44 Table 4.2 Feature set for event-DCT (E-D), event-timex (E-T) and event-event (E-E) classification models, along with each feature representation (Rep.) in the feature vector and feature descriptions. 49\nTable 4.3 The distribution of each relation type in the datasets for each type of temporal entity pairs: timex-timex (T-T), event-DCT (E-D), eventtimex (E-T) and event-event (E-E). 50 Table 4.4 Tempeval-3 evaluation on temporal relation extraction tasks 51 Table 4.5 TempRelPro performances per module on temporal relation identification and type classification, evaluated on the TempEval-3 evaluation corpus. RB = rule-based sieve, ML = machine-learned sieve and TR = temporal reasoner. 52\nTable 4.6 TempRelPro performances evaluated on the TimeBank-Dense test set and compared with CAEVO. 53 Table 4.7 TempRelPro performances per module on temporal relation type classification, evaluated on the TimeBank-Dense test set, and compared with CAEVO. RB = rule-based sieve, ML = machine-learned sieve and TR = temporal reasoner. 53 Table 4.8 TempRelPro performances in terms of coverage (Cov), precision (P), recall (R) and F1-score (F1), compared with HLT-FBK. 55 Table 4.9 TempRelPro performances in terms of coverage (Cov), precision (P), recall (R) and F1-score (F1) for all domains, compared with systems in QA TempEval augmented with TREFL. 55 Table 5.1 Statistics of causal markers found in Causal-TimeBank, including causal signals and causative verbs, and the corresponding numbers of CLINKs. 66 Table 6.1 Possible dependency relations between v and e1/e2, their corresponding paths according to Surdeanu et al. (2008) and examples in texts. 73 Table 6.2 Feature set for the CLINK classification model, along with each feature representation (Rep.) in the feature vector and feature descriptions. 75 Table 6.3 CauseRelPro micro-averaged performances per module on causal relation extraction, evaluated with stratified 10-fold cross-validation on the Causal-TimeBank corpus. #Cand. = number of candidate event pairs, TP = number of true positives and FP = number of false positives. 76\nxvi List of Tables\nTable 7.1 Statistics of CLINKs overlapping with TLINKs. TR = temporal reasoner. 81 Table 7.2 CauseRelPro micro-averaged performances on causal relation extraction, evaluated with stratified 10-fold cross-validation on the Causal-TimeBank corpus. + TLINK = with TLINK types as features, TP = number of true positives and FP = number of false positives. 82 Table 7.3 TempRelPro E-E Classifier micro-averaged performances on temporal relation type classification, evaluated with stratified 10-fold cross-validation on the Causal-TimeBank corpus. + CLINK = with CLINK directions as features and TP = number of true positives. 82 Table 7.4 CATENA performances per sieve and sieve combination. RB: rule-\nbased sieve, ML: machine-learned sieve and TR: temporal reasoner. 84 Table 8.1 Classifier performances (F1-scores) in different experimental settings S1 and S2, compared with using only traditional features. TP: true positives and FP: false positives. 90 Table 8.2 F1-scores per TLINK type with different feature vectors. Pairs of word vectors (~w1, ~w2) are retrieved from Word2Vec pre-trained vectors. 90 Table 8.3 Classifier performance per TLINK type with different feature vectors, evaluated on TempEval-3-platinum. Pairs of word vectors (~w1, ~w2) are retrieved from Word2Vec pre-trained vectors. 91 Table 9.1 Regression analysis (p = 0.000, R2 = 0.633) 96 Table 9.2 Classifier performances for event-event (ee) and event-timex (et) pairs with different training data 97 Table 9.3 Classifier performances with TBAQd and TBAQcd training data and their best deduction threshold, i.e., 160 and 100 respectively 99 Table 9.4 CauseRelPro-beta’s micro-averaged scores. 102 Table 9.5 CauseRelPro-beta’s micro-averaged scores using different degrees of polynomial kernel. 103 Table 9.6 Impact of increased training data, using EMM-clusters and prop-\n1 I N T R O D U C T I O N\nWhen the Greek government missed its 1.6 billion euro payment to the IMF as its bailout expired on 30 June 2015, people started to look for information such as, What is going on? Why did it happen and what will happen next? While trying to relate current events to past events, news readers may ask themselves more questions, such as When did the crisis start? How did Greece get to this point? A compact summary that represents the development of a story over time, be it over the course of one day, several months or even years, would be very beneficial for providing information that the readers need.\nTimeline summarization has become a widely adopted, natural way to present news stories in a compact manner. An example of a timeline is illustrated in Figure 1.1. News agencies often manually construct and maintain timelines for major events, but constructing such visual summaries requires a considerable amount of human effort and does not scale well, especially given enormous and expanding news data in the web, with millions of documents added daily. This is where information extraction comes into play. Information extraction (IE) is part of natural language processing (NLP), and aims to automatically extract information from unstructured text into predefined structures.\nNewspaper articles are often used to describe events occurring in a certain time, and specify the temporal order of these events. Consider, for example, the following excerpt from a news article in Figure 1.1 published on July 16, 2015:\nPrime Minister Tsipras bends to European creditors and presses parliament to approve new austerity measures, despite a July 5 referendum in which Greeks overwhelmingly rejected these terms. The agreement comes after a weekend of talks in which a Greek eurozone exit was only narrowly averted and opens the way to a possible third bailout program worth up to 86 billion euros ($94 billion). The ECB resumes some support for Greek banks, but the compromise splits the ruling Syriza party and sets the stage for new elections in the coming months.\nHuman readers can easily comprehend that: there was a referendum on July 5, there were talks that last for a weekend, there was an agreement that comes after the talks and there will be new elections in the coming months; and order these facts in chronological order. They can also infer that the agreement and the compromise refer to the same entity, and that there will be new elections because the ruling Syriza party is split. This kind of text comprehension— building structured information about events and their temporal-causal relations—is an ultimate goal of temporal information processing, in which the main task is extracting temporal information from texts.\nStructured information resulting from temporal information processing is in fact crucial for a variety of natural language tasks, particularly summarization and question answering. In summarization tasks, given a large set of texts, a system is required to generate a much smaller text still containing all important contents of the original. For texts describing events, knowing which events are important and linking them in a temporal-causal structure would allow an automatic generation of a timeline-style summary. In question answering, a system is asked questions in natural language and expected to return the\n1\n2 introduction\nanswers by looking for the appropriate information in a large set of documents. For example, having the temporal-causal structure about Greece’s debt crisis would allow questions such as When did the talks resulting in the agreement take place? or What is the reason for new elections in the coming months? to be answered. To answer the first question it is necessary to infer that the talks happened during the weekend (most probably) before the news article is published. Meanwhile, answering the second question requires knowledge of the causing event, which is the splitting of the Syriza party.\nFurthermore, domain-specific structured temporal-causal information, e.g., about events involving a specific company extracted from financial news, or about chains of symptoms and diagnosis extracted from clinical reports, could be exploited in decision making support systems.\nBuilding a system for extracting from text such temporal-causal information, specifically the temporal and causal relations between events found in the text, is the main focus of this thesis. Temporal and causal relations are closely related, as by common sense, a cause must precede its effect. In our research, we aim to exploit this presumption to improve the performance of our integrated temporal and causal relation extraction system.\nThere are several annotation frameworks for modelling temporal information, i.e. temporal entities and relations, in a text. TimeML (Pustejovsky et al., 2003), being one of the prominent ones, is a language specification for events and temporal expressions, which was\n1 Source: http://www.cfr.org/greece/timeline-greeces-debt-crisis/p36451\n1.1 motivations and goals 3\ndeveloped in the context of the TERQAS workshop2. An event is defined as something that happens/occurs or a state that holds true, which can be expressed by a verb (e.g. killed, acquire), a noun (e.g. earthquake, merger), an adjective (e.g. injured, retired), as well as a nominalization either from verbs or adjectives (e.g. investigation, bankruptcy).\nThe distinctive feature of TimeML is the separation of the representation of temporal entities, i.e. events and temporal expressions, from the anchoring or ordering dependencies. Instead of treating a temporal expression as an event argument, TimeML introduces temporal link annotations to establish dependencies (temporal relations) between events and temporal expressions. Moreover, in TimeML, all types of events are annotated because every event takes part in the temporal network.3 These are the main reason why we adopted the definitions of temporal entities and temporal relations from TimeML for this research.\nAs an illustration, consider our previous news excerpt, now annotated with temporal entities according to TimeML definitions:\nPrime Minister Tsipras [bends] to European creditors and [presses] parliament to [approve] new austerity measures, despite a [July 5] [referendum] in which Greeks overwhelmingly [rejected] these terms. The [agreement] [comes] after [a weekend] of [talks] in which a Greek eurozone [exit] was only narrowly [averted] and [opens] the way to a possible third bailout [program] worth up to 86 billion euros ($94 billion). The ECB [resumes] some [support] for Greek banks, but the [compromise] [splits] the ruling Syriza party and [sets] the stage for new [elections] in [the coming months].\nGiven such annotated texts, a relation extraction system should be able to identify, for example: IS_INCLUDED (referendum, July 5), DURING (talks, a weekend), AFTER (agreement, talks), IS_INCLUDED (elections, the coming months) and CAUSE (splits, elections).\n1.1 motivations and goals\ntemporal relations\nTimeML is the annotation framework used in a series of evaluation campaigns for temporal information processing called TempEval (UzZaman et al., 2013; Verhagen et al., 2007, 2010), in which the ultimate goal is the automatic identification of temporal expressions, events and temporal relations within a text. In TempEval, the temporal information processing task is divided into several sub-problems. Given a text, the extraction task basically includes: (i) identifying temporal entities mentioned in the text and (ii) identifying the temporal relations between them. In this research, we take the best performing systems in TempEval as our baseline.\nThe best performing extraction system for complete temporal information extraction achieves 30.98% F1-score. According to the results reported in TempEval, the main limiting factor seems to be the low performance of temporal relation extraction systems (36.26% F1-score). This is the main reason why we focus our research on temporal relation extraction. Meanwhile, the extraction systems for temporal entities already achieve\n2 http://www.timeml.org/site/terqas/index.html 3 Except for generics as in “Use of corporate jets for political travel is legal.” (Saurí et al., 2006)\n4 introduction\nquite good results (>80% F1-scores). Therefore, and to limit the scope of our thesis, we assume that the annotation of temporal entities is already given.\nIn our attempt to improve the performance of the extraction system for temporal relations, we explore several research directions, which will be explained in the following paragraphs.\ncausal relations\nA cause should always precede its effect. — Anonymous\nThe first research direction for improving the performance of temporal relation extraction is related to the connection between temporal and causal relations, based on the assumption that there is a temporal constraint in causality regarding event precedence. We aimed to investigate whether extracting causal relations between events can benefit temporal relation extraction. Apart from the efforts to improve the temporal relation extraction system, the recognition of causality between events is also crucial to reconstruct a causal chain of events in a story. This could be exploited, for example, in question answering systems, decision making support systems and for predicting future events given a chain of events. Having an integrated extraction system for both temporal and causal relations is one of the goals of this research.\nUnfortunately, unlike for temporal relations, there was no corpus available for building (and evaluating) an automatic extraction system for event causality, specifically the one that provides comprehensive account of how causality can be expressed in a text without limiting the effort to specific connectives. This motivated us to build annotation guidelines for explicit causality in text, and to annotate the TimeBank corpus, in which gold annotated events and temporal relations were already present. The resulting causality corpus, which we called Causal-TimeBank, enabled the adaptation of existing temporal processing systems to the extraction of causal information, and made it easier for us to investigate the relation between temporal and causal information.\nword embeddings\nYou shall know a word by the company it keeps. — Firth (1957)\nWord embeddings and deep learning techniques are gaining momentum in the NLP research, as they are seen as powerful tools to solve several NLP tasks, such as language modelling, relation extraction and sentiment analysis. Word embedding is a way to capture the semantics of a word via a low-dimensional vector, based on the distribution of other words around this word.\nIn this research, we explored the effect of using lexical semantic information about event words, based on word embeddings, on temporal relation extraction between events. For example, whether the word embeddings can capture that attack often happens BEFORE injured.\ntraining data expansion\nWe don’t have better algorithms. We just have more data. — Google’s Research Director Peter Norvig\n1.2 contributions 5\nThe scarcity of annotated data is often an issue in building extraction systems with supervised learning approach. One widely known approach to gain more training examples is semi-supervised learning, as for some NLP tasks it was shown that unlabelled data, when used in conjunction with a small amount of labelled data, can produce considerable improvement in learning accuracy.\nWe investigated two approaches to expand the training data for temporal and causal relation extraction, namely (i) temporal reasoning on demand for temporal relation type classification and (ii) self-training, a wrapper method for semi-supervised learning, for causal relation extraction.\nFinally,\nTo have another language is to possess a second soul. — Charlemagne\nResearch on temporal information processing has been gaining a lot of attention from the NLP community, but most research efforts have focused only on English. In this research we explore the adaptation of our temporal information processing system for two languages other than English, i.e. Italian and Indonesian.\n1.2 contributions\nThe following contributions are presented in this thesis:\n• A hybrid approach for building an improved temporal relation extraction system, partly inspired by the sieve-based architecture of CAEVO (Chambers et al., 2014). Our approach is arguably more efficient than CAEVO, because (i) the temporal closure inference over extracted temporal relations is run only once and (ii) we use less classifiers in general.\n• Annotation guidelines for annotating explicit causality between events, strongly inspired by TimeML. Compared with existing attempts for annotating causality in text, we aim to provide a more comprehensive account of how causality can be expressed in a text, without limiting the effort to specific connectives.\n• An event causality corpus, Causal-TimeBank, is made available to the research community, to support evaluations or developments of supervised learning systems for extracting causal relations between events.\n• A hybrid approach for building an improved causal relation extraction system, making use of the constructed event causality corpus.\n• An integrated extraction system for temporal and causal relations, which exploits the assumption about event precedence when two events are connected by causality.\n• Preliminary results on how word embeddings can be exploited for temporal relation extraction.\n• An investigation into the impact of training data expansion for temporal and causal relation extraction.\n• A summary of our adaptation efforts of temporal information processing for Italian and Indonesian languages.\n6 introduction\n1.3 structure of the thesis\nThis thesis is organized as follows. In Chapter 2, we provide background information about natural language processing and information extraction, and discuss approaches widely used for information extraction tasks. Chapter 3 introduces the task of temporal information processing that comprises the TimeML annotation standard, annotated corpora and related evaluation campaigns. We also give a brief overview of state-of-the-art methods for extracting temporal information from text.\nChapter 4 focuses on our hybrid approach for building an improved temporal relation extraction system. In Chapter 5 we present annotation guidelines for explicit causality between events. We also provide some statistics from the resulting causality-annotated corpus, Causal-TimeBank, on the behaviour of causal cues in a text. Chapter 6 provides details on the hybrid approach for extracting causal relations between events from a text. In Chapter 7 we describe our approach for building an integrated system for both temporal and causal relations, making use of the assumption about the temporal constraint of causality.\nChapter 8 provides preliminary results on the effects of using word embeddings for extracting temporal relations between events. Chapter 9 discusses the impacts of our training data expansion approaches for temporal relation type classification and causal relation extraction. In Chapter 10 we address the multilinguality issue, by providing a review of our adaptation efforts of the temporal information processing task for Italian and Indonesian.\nFinally, Chapter 11 discusses the lesson learned from this research work, and possible fruitful directions for future research.\n2 B A C K G R O U N D\n2.1 Natural Language Processing 7\n2.1.1 Morphological Analysis 7\n2.1.2 Syntactic Analysis 8\n2.1.3 Information Extraction 10\n2.2 Techniques for Information Extraction 11\n2.2.1 Rule-based Methods 11\n2.2.2 Supervised Machine Learning 12\n2.2.3 Hybrid Approaches 17\n2.2.4 Semi-supervised Machine Learning 17\n2.2.5 Word Embeddings 19\nIn this chapter, we provide background information about natural language processing and information extraction, as well as methods and techniques widely used in the experiments described in this thesis.\n2.1 natural language processing\n2.1.1 Morphological Analysis\nMorphological analysis refers to the identification, analysis and description of the structure and formation of a given languages’s morphemes and other linguistic units, such as stems, affixes, part-of-speech, intonations and stresses, or implied context. A morpheme is defined as the smallest meaningful unit of a language. Consider a word like unhappiness containing three morphemes, each carrying a certain amount of meaning: un means “not”, ness means “being in a state or condition” and happy. Happy is a free morpheme, and considered as a root, because it can appear on its own. Bound morphemes, typically affixes, have to be attached to a free morpheme, thus, we cannot have sentences in English such as “Jason feels very un ness today”. Morphological analysis is a very important step for natural language processing, especially when dealing with morphologically complex languages.\nstemming and lemmatization A stem may be a root (e.g. run) or a word with derivational morphemes (e.g. the derived verbs standard-ize). For instance, the root of destabilized is stabil- (i.e. a form of stable that does not occur alone), and the stem is destabil-ize, which includes the derivational affixes de- and -ize but not the inflectional past tense suffix -(e)d. In other words, a stem is a part of a word that inflectional affixes attach to. A lemma refers to a dictionary form of a word. A typical example of this are the words see, sees, seeing and saw, which all have the same see-lemma.\npart-of-speech tagging In natural language, words are divided into two broad categories: open and closed classes. Open classes do not have a fixed word membership,\n7\n8 background\nand encompass nouns, verbs, adjectives and adverbs. Closed classes, contrastingly, have a relatively fixed word membership. They include function words, such as articles, prepositions, auxiliary verbs and pronouns, which have a high occurrence frequency in linguistic expressions. Part-of-speech (PoS) tagging is the problem of assigning each word in a sentence the part of speech that it assumes in that sentence, according to their different lexical categories (noun, verb, adjective, adverb, preposition, pronoun, etc.).\nA PoS tagset specifies the set of PoS categories being distinguished and provides a list of tags used to denote each of those categories. The commonly used PoS tagsets include Penn Treebank PoS Tagset1 (Marcus et al., 1993; Santorini, 1990), the British National Corpus (BNC) Tagset and the BNC Enriched Tagset2 (Leech et al., 1994). The difference between the text annotated with Penn Treebank PoS Tagset and BNC (Basic) Tagset is exemplified in the following sentence (i) and (ii), respectively.\n(i) I/PRP saw/VBD a/DT boy/NN with/IN a/DT dog/NN ./.\n(ii) I/PNP saw/VVD a/AT0 boy/NN1 with/PRP a/AT0 dog/NN1 ./PUN\nThere are a total of 48 tags in the Penn Treebank PoS Tagset, while the BNC Basic Tagset, also known as the C5 Tagset, distinguishes a total of 61 categories. Notably, the C5 Tagset includes separate categories for the various forms of the verbs be, do and have.\nThe Penn Treebank PoS Tagset is used in the Stanford CoreNLP tool suite3 (Manning et al., 2014). Meanwhile, the TextPro tool suite4 (Pianta et al., 2008), which is the one mainly used in our research, employs the BNC Basic Tagset.\n2.1.2 Syntactic Analysis\nsyntactic parsing\nParsing means taking an input and producing some sort of linguistic structure for it. — Jurafsky and Martin (2000)\nA syntactic parser takes a sentence as input and produces a syntactic structure that corresponds to a semantic interpretation of the sentence. For example, the sentence “I saw a boy with a dog” can be parsed in two different ways (Figure 2.1). This divergence is caused by two possible interpretations of the sentence: (a) and (b). While both are grammatically correct, they reflect two different meanings: (a) the phrase “a dog” is attached to “a boy” which means accompanied; (b) the phrase “a dog” is attached to “saw” which means a tool used to make the observation. The major challenge for a syntactic parser is to find the correct parse(s) from an exponential number of possible parses.\nIn terms of its overall structure, the parse tree is always rooted at a node ROOT, with the terminal elements that relate to actual words in the sentence. Each of its sub-parses, or internal nodes, spans over several tokens, and is characterized by a set of syntactic types (e.g., NP and VP, which denote noun and verb phrases, resp.). The most important word in that span is called the head word. In this work we will also refer to syntactically dominant and governing verbs. Syntactically dominant verbs are the verbs that are located closer\n1 http://www.comp.leeds.ac.uk/amalgam/tagsets/upenn.html 2 http://www.natcorp.ox.ac.uk/docs/gramtag.html 3 http://stanfordnlp.github.io/CoreNLP/ 4 http://hlt-services2.fbk.eu/textpro/\nto the root of the entire parse tree. For a number of words in a textual span, governing verbs are the verbs in verb phrases that are the roots of the corresponding sub-trees. For example, for the sentence in Figure 2.1, the verb “saw” is the syntactically dominant verb of the sentence, and the governing verb for the textual span “a boy with a dog”.\ndependency parsing In contrast to syntactic parsing, where the linguistic structure is formulated by the grammar that organizes the sentences’ words into phrases, word dependency formalism orders them according to binary dependency relations between the words (as between a head and a dependent). Word dependency formalism is often referenced as an effective mean to represent the linguistic structures of languages with a relatively free word-order.\n10 background\nExamples of dependency parses for a sentence “I saw a boy with a dog” are presented in Figure 2.2. There are several dependency representations, such as (a) Stanford (Typed) Dependencies (Marneffe and Manning, 2008) used in Stanford CoreNLP, and (b) CoNLL2008 Shared Task Syntactic Dependencies (Surdeanu et al., 2008) used in Mate tools5 (Bjorkelund et al., 2010).\n2.1.3 Information Extraction\nInformation extraction is a broad research field that uses computer algorithms to extract predefined structured information from natural language text, where elements of the structure relate to textual spans in the input. With the exception of temporal information processing, which will be explained further in Chapter 3, the different tasks of information extraction are listed in the following sections.\nnamed-entity recognition Named-entity recognition is a task of information extraction that categorizes single textual elements in text in terms of a set of common criterion (persons, organizations, locations, times, numbers, etc.).\nThe violent clashes between the security forces and protesters have lasted [two days Date] in [Cairo Location] and other cities.\nIn the example, the textual span “two days” is identified and classified as an instance of Date, while the span of “Cairo” is identified and classified as an instance of Location.\nword-sense disambiguation The task of word-sense disambiguation is to assign a label to every noun phrase, (non-auxiliary) verb phrase, adverb and adjective in a text. This label indicates the meaning of its attached word, and is chosen from a dictionary of meanings for a large number of phrases.\nThe [violent violent.01] [clashes clash.04] between the [security security.03] [forces force.01] and [protesters protester.02] have [lasted last.01] [two two.01] [days day.04] in [Cairo Cairo.02] and [other other.01] [cities city.01].\nIn the example, the meanings are assigned the labels of synsets in the WordNet lexical database (Fellbaum, 1998), e.g., the word “clashes” receives the label clash.04 which means “fight” or “fighting”, whereas the most common sense clash.01 stands for “clang” or “noise”.\nsemantic role labelling Semantic Role Labelling (SRL) consists of the detection of the semantic arguments associated with the predicate or verb of a sentence, and their classification into their specific roles. For example, given a sentence like “Mary sold the car to John”, the task would be to recognize the verb “to sell” as the predicate, “Mary” as the seller (agent), “the car” as the goods (theme) and “John” as the recipient. The task is seen as an important step towards making sense of the meaning of a sentence, which is at a higher-level of abstraction than a syntactic tree. For instance, “The car has been sold by Mary to John” has a different syntactic form, but the same semantic roles.\n5 https://code.google.com/archive/p/mate-tools/\n2.2 techniques for information extraction 11\nThe FrameNet project (Baker et al., 1998) produced the first major computational lexicon that systematically described many predicates and their corresponding roles. Gildea and Jurafsky (2002) developed the first automatic semantic role labeling system based on FrameNet. FrameNet additionally captures relationships between different frames, including among others: Precedes, which captures a temporal order that holds between subframes of a complex scenario, and Causative_of, which expresses causality between frames.\nAnother project related to semantic role labelling is the PropBank project (Palmer et al., 2005), which added semantic role—or predicate-argument relations—annotations to the syntactic tree of the Penn Treebank corpus (Prasad et al., 2008). The PropBank annotation is exemplified in the following sentence:\n[The violent clashes Arg1] between the security forces and protesters have [lasted last.01] [two days Arg2] in [Cairo and other cities Arg-Loc].\nHere the verb “lasted” has a predicate label last.01, which means “extend for some period of time”. The related words have semantic roles:\n• Arg1 for “The violent clashes”, denoting thing that lasts\n• Arg2 for “two days”, denoting period of time\n• Arg-Loc for “Cairo and other cities”, denoting location\ncoreference resolution Given a sentence or larger chunk of text, the task is to determine which words—mentions—refer to the same objects—entities. Anaphora resolution is a special case of this task, which is concerned with matching up pronouns with the nouns or names that they refer to.\nAnother typical coreference problem is to find links between previously-extracted named entities. For example, “International Business Machines” and “IBM” might refer to the same real-world entity. If we take the two sentences “M. Smith likes fishing. But he doesn’t like biking”, it would be beneficial to detect that “he” is referring to the previously detected person “M. Smith”.\nrelationship extraction This task basically deals with the identification of relations between entities, including:\n• Compound noun relations: recognition of relations between two nouns.\n• (Geo)spatial analysis: recognition of trajectors, landmarks, frames of reference, paths, regions, directions and motions, and relations between them.\n• Discourse analysis: recognition of non-overlapping text spans and discourse relations between them.\n2.2 techniques for information extraction\n2.2.1 Rule-based Methods\nRule-based methods are the earliest ones used in information extraction. A rule-based system makes use of a database of predefined and hand-crafted rules that specify knowledge\ntypically in form of regular expressions. Regular expressions are a linguistic formalism that is based on a regular grammar—one of the simplest classes of formal language grammars (Chomsky, 1959).\nRegular expressions are a declarative mechanism for specifying declarative languages based on regular grammars. Regular grammars are recognized by a computation device, called finite state automaton (FSA). A finite state automaton (Hopcroft and Ullman, 1969) is a five-tuple (Θ, θ0,Σ, δ, F), where Θ is a finite set of states, θ0 is the initial state, Σ is a finite set of alphabet symbols, δ : Θ× Σ××Θ is a relation from states and alphabet symbols to states, and F ⊆ Θ is a set of final states. The extension of δ to handle input strings is standard and denoted by δ∗. δ∗(θ,a) denotes the state reached from θ on reading the string a. A string a is said to be accepted by an FSA if δ∗(θ,a) ∈ F. The language AL is the set of all strings accepted by AL’s FSA. Strings that are not accepted by AL’s FSA are outside of the language AL.\nSystems based on regular expressions are considered as rule-based systems in which knowledge about the domain is encoded in regular expressions. If the input string is accepted, i.e., it matches one of the regular expressions, it is labelled with a class label associated with that particular rule. In natural language processing, rule-based approaches were applied for, among others, tokenization—identifying the spans of single tokens in a text, stemming—finding the stem of a token, and Part-of-Speech (PoS) tagging.\nFigure 2.3 shows regular expression examples in the POSIX Extended Regular Expressions (ERE) syntax to extract time and date from a text.\nAlways traditionally popular, rule-based techniques have long been utilized for smallsize applications and applications for new domains. However, with the development of large annotated corpora, machine learning techniques have grown increasingly popular, with users beginning to compare their performance to rule-based methods. These comparative studies have found that rule-based systems are very difficult to maintain, and that such systems are not well-scalable. Nevertheless, there are problems which can only be solved by the rule-based approach. Main reasons to still employ rule-based systems are:\n• New, small or restricted application domains.\n• Short development time for a set of generally applicable and observable rules.\n• Absence of annotated training data.\n• Poor quality of training data.\n2.2.2 Supervised Machine Learning\nSince rule-writing requires enormous human effort, an easier approach would be to utilize existing examples, i.e. annotations, to extract the rules automatically; or to use statistics, which can predict the labels of words, phrases, sentences or even the entire document. In the following sections we describe a number of state-of-the-art supervised machine\n2.2 techniques for information extraction 13\nlearning methods that are currently used in the field of natural language processing and information extraction. The focus of supervised approaches in NLP has hitherto been limited to feature extraction (how an object under consideration is represented in a numerical way as a vector of features), and selecting appropriate machine learning methods.\nformal definitions In terms of supervised machine learning, the labelling task can be defined as: given a set of n observations x1, x2, ..., xn with their corresponding target class value y1,y2, ...,yn, the goal is to predict the value of y for an unseen instance x. More formally, it can be defined as Func(x) : x→ y, where each instance x is represented as a vector of feature values, i.e., x = [f1, f2, ..., fm], with m being the total number of features used in the representation. Depending on the number of distinct target values of y, one distinguishes between binary (with two target values) and multi-class classifications. In the following sections we describe the commonly used machine learning methods to model the prediction function Func(x).\nsupport vector machines Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) is a well-known discriminative machine learning classifier that models the data as points in a high-dimensional space, and spatially separates them as far as possible. Technically, an SVM constructs a hyperplane, which can be used for classification, regression or other tasks. The best separation of data points is achieved by the hyperplane that has the largest distance to the nearest data point of any class.\nFormally, an SVM is defined as: given a set of observations x1, x2, . . . , xn with a corresponding set of labels y1,y2, . . . ,yn, where yi ∈ −1,+1, the separating hyperplane H0 that divides the data points in space can be defined as:\nw · x + b = 0 (2.1)\nwhere w is the normal vector to the hyperplane, x is a set of points xi that lie on the hyperplane, and · denotes the dot product (see Figure 2.5).\n14 background\nWe can select two others hyperplanes H1 and H2 which also separate the data and defined as:\nw · xi + b > 1 for yi = +1 (2.2)\nand\nw · xi + b 6 −1 for yi = −1 (2.3)\nso that H0 is equidistant from H1 and H2, and taking into consideration the constraint that there is no data point between the two hyperplanes. Equation ( 2.2) and (2.3) can be combined into a single constraint:\nyi (w · xi + b) > 1 for all 1 6 i 6 n (2.4)\nThe optimal hyperplane w0 · x + b0 = 0 is the unique one that separates the training data with a maximal margin, i.e., the distance 2‖w0‖ between the two hyperplanes H1 and H2 is maximal. This means that the optimal hyperplane is the unique one that minimizes w ·w under the constraint (2.4).\nConsider the case where the training data cannot be separated without error. In this case one may want to separate the training set with a minimal number of errors. To express this formally some non-negative variables ξi > 0, i = 1 . . . l, are introduced. The problem of finding the optimal soft-margin hyperplane is then defined as:\nmin w,b,ξ\n1 2 w ·w +C l∑ i=1 ξi\nsubject to yi (w ·φ(xi) + b) > 1− ξi, ξi > 0, i = 1, . . . , l\n(2.5)\nwhere the training vectors xi are mapped to a higher dimensional space by the function φ. C > 0 is the penalty parameter of the error term. Furthermore, K(xi, xj) ≡ φ(xi) ·φ(xj) is called the kernel function. Though new kernels are being proposed by researchers, the basic kernels include:\n• linear: K(xi, xj) = xi · xj\n• polynomial: K(xi, xj) = (γxi · xj + r)d,γ > 0\n• radial basis function (RBF): K(xi, xj) = exp(−γ‖xi − xj‖2),γ > 0\n• sigmoid: tanh(γxi · xj + r)\nThe earliest used implementation for SVM multi-class classification is probably the oneagainst-all method. It construct k SVM models where k is the number of classes. The mth SVM is trained with all of the examples in the mth class with positive labels, and all other examples with negative labels. Thus, given l training data (x1,y1), . . . (xl,yl), where xi ∈ Rn, i = 1, . . . , l and yi ∈ 1, . . . ,k is the class of xi, the mth SVM solves the following problem:\nmin wm,bm,ξm\n1 2 wm ·wm +C l∑ i=1 ξmi\nsubject to (wm ·φ(xi) + bm) > 1− ξmi , if yi = m, (wm ·φ(xi) + bm) 6 −1+ ξmi , if yi 6= m, ξmi > 0.\n(2.6)\n2.2 techniques for information extraction 15\nAfter solving (2.6), there are k decision functions: w1 ·φ(x) + b1, . . . , wk ·φ(x) + bk. We say x is in the class which has the largest value of the decision function:\nclass of x ≡ arg maxm=1,...,k(wm ·φ(x) + bm) (2.7)\nAnother major method is called the one-against-one method, introduced by Knerr et al. (1990). This method constructs k(k − 1)/2 classifiers where each one is trained on data from two classes. For the training data from the ith and the jth classes, we solve the following binary classification problem:\nmin wij,bij,ξij\n1 2 wij ·wij +C ∑ t ξ ij t\nsubject to ( wij ·φ(xt) + bij ) > 1− ξijt , if yt = i,(\nwij ·φ(xt) + bij ) 6 −1+ ξijt , if yt = j,\nξ ij t > 0.\n(2.8)\nThere are different methods for doing the future testing after all k(k − 1)/2 classifiers are constructed. For instance, the following voting strategy suggested by Friedman (1996) may be used: if sign(wij ·φ(xt) + bij) says x is in the ith class, then the vote for the ith class is added by one. Otherwise, the jth is increased by one. Then we predict x is in the class with the largest vote. This voting approach is also called the “Max Wins” strategy. In case that two classes have identical votes, the one with the smaller index is usually selected, though it may not be a good strategy.\nlogistic regression Logistic regression6 is an approach to learning functions of the form f : X → Y, or P(Y|X) in the case where Y is discrete-valued, and X = 〈X1 . . . Xn〉 is any vector containing discrete or continuous variables.\nLogistic Regression assumes a parametric form for the distribution P(Y|X), then directly estimates its parameters from the training data. The parametric model assumed by Logistic Regression in the case where Y is boolean is:\nP(Y = 1|X) = 1 1+ exp(w0 + ∑n i=1wiXi)\n(2.9)\n6 We took the explanations about Logistic Regression from http://www.cs.cmu.edu/~tom/NewChapters.html by Tom Mitchell.\n16 background\nand\nP(Y = 0|X) = exp(w0 +\n∑n i=1wiXi)\n1+ exp(w0 + ∑n i=1wiXi)\n(2.10)\nNote that equation (2.10) follows directly from equation (2.9), because the sum of these two probabilities must equal to 1.\nOne highly convenient property of this form for P(Y|X) is that it leads to a simple linear expression for classification. To classify any given X we generally want to assign the value yk maximizing P(Y = yk|X). Put another way, we assign the label Y = 0 if the following condition holds:\n1 < P(Y = 0|X)\nP(Y = 1|X) (2.11)\nsubstituting from equations (2.9) and (2.10), this becomes\n1 < exp(w0 + n∑ i=1 wiXi) (2.12)\nand taking the natural log of both sides we have a linear classification rule that assigns label Y = 0 if X satisfies\n0 < w0 + n∑ i=1 wiXi (2.13)\nand assigns Y = 1 otherwise. One reasonable approach to train a logistic regression model is to choose parameter values that maximize the conditional data likelihood. The conditional data likelihood is the probability of the observed Y values in the training data, conditioned on their corresponding X values. We choose parameters W that satisfy\nW ← arg max W ∏ l P(Yl|Xl,W) (2.14)\nwhere W = 〈w0,w1 . . . wn〉 is the vector of parameters to be estimated, Yl denotes the observed value of Y in the lth training example, and Xl denotes the observed value of X in the lth training example. The expression to the right of the arg max is the conditional data likelihood. Equivalently, we can work with the log of the conditional likelihood:\nW ← arg max W ∑ l lnP(Yl|Xl,W) (2.15)\nAbove we considered using Logistic Regression to learn P(Y|X) only for the case where Y is a boolean variable, i.e. binary classification. If Y can take on any of the discrete values y1, . . . yK, then the form of P(Y = yk|X) for Y = y1, Y = y2, . . . Y = yK−1 is:\nP(Y = yk|X) = exp(wk0 +\n∑n i=1wkiXi)\n1+ ∑K−1 j=1 exp(wj0 + ∑n i=1wjiXi)\n(2.16)\nWhen Y = yK, it is\nP(Y = yK|X) = 1 1+ ∑K−1 j=1 exp(wj0 + ∑n i=1wjiXi)\n(2.17)\n2.2 techniques for information extraction 17\nHere wji denotes the weight associated with the jth class Y = yj and with input Xi. It is easy to see that our earlier expressions for the case where Y is boolean (equation (2.9) and (2.10)) are a special case of the above expressions. Note also that the form of the expression for P(Y = yk|X) assures that [∑K k=1 P(Y = yk|X) [ = 1.\nThe primary difference between these expressions and those for boolean Y is that when Y takes on K possible values, we construct K− 1 different linear expressions to capture the distributions for for the different values of Y. The distribution for the final, Kth, value of Y is simply one minus the probabilities of the first K− 1 values.\n2.2.3 Hybrid Approaches\nHybrid approaches are another kind of method employed in natural language processing, which combine rule-based with machine learning methods. Hybrid approaches are considered as a reasonable solution for a number of problems for which the training data exhibit irregularities and exceptions.\nFigure 2.6 exemplifies two hybrid architectures: (a) a concurrent information processing pipeline in which different tasks are performed by either rule-based or statistical methods, and (b) an information processing pipeline in which the output of the one family of methods is used as input for the other. Hybrid approaches are very popular in NLP applications such as machine translation, parsing, information extraction, etc. Schäfer (2007) provides a good overview of integrating deep and shallow NLP components into hybrid architectures.\n2.2.4 Semi-supervised Machine Learning\nAs the name suggests, semi-supervised learning7 is somewhere between unsupervised and supervised learning. In fact, most semi-supervised learning strategies are based on extending either unsupervised or supervised learning to include additional information typical of the other learning paradigm. Specifically, semi-supervised learning encompasses several different settings, including:\n• Semi-supervised classification. Also known as classification with labelled and unlabelled data (or partially labelled data), this is an extension to the supervised classification problem. The training data consists of both l labelled instances {(xi,yi)} l i=1\n7 We took the explanations about Semi-supervised Learning from Zhu et al. (2009).\n18 background\nand u unlabelled instances {xj} l+u j=l+1 . One typically assumes that there is much more unlabelled data than labelled data, i.e., u l. The goal of semi-supervised classification is to train a classifier f from both the labelled and unlabelled data, such that it is better than the supervised classifier trained on the labelled data alone.\n• Constrained clustering. This is an extension to unsupervised clustering. The training data consists of unlabelled instances {xi} n j=1, as well as some “supervised informa-\ntion” about the clusters. For example, such information can be so-called must-link constraints, that two instances xi, xj must be in the same cluster; and cannot-link constraints, that xi, xj cannot be in the same cluster. One can also constrain the size of the clusters. The goal of constrained clustering is to obtain better clustering than the clustering from unlabelled data alone.\nSemi-supervised learning has tremendous practical value. In many tasks, there is a paucity of labelled data. The labels y may be difficult to obtain because they require human annotators, special devices, or expensive and slow experiments. In this thesis, we will focus on a simple semi-supervised classification model: self-training.\nself-training Self-training is characterized by the fact that the learning process uses its own predictions to teach itself. For this reason, it is also called self-teaching or bootstrapping (not to be confused with the statistical procedure with the same name). Selftraining can be either inductive or transductive, depending on the nature of the predictor f. The algorithm for self-training is as follows: Input: labelled data {(xi,yi)} l i=1, unlabelled instances {xj} l+u j=l+1 .\n1: Initially, let L = {(xi,yi)} l i=1 and U = {xj} l+u j=l+1 . 2: repeat 3: Train f from L using supervised learning. 4: Apply f to the unlabelled instances in U. 5: Remove a subset S from U; add {(x, f(x))|x ∈ S} to L. 6: until U is empty. The main idea is to first train f on labelled data. The function f is then used to predict the labels for the unlabelled data. A subset S of the unlabelled data, together with their predicted labels, are then selected to augment the labelled data. Typically, S consists of the few unlabelled instances with the most confident f predictions. The function f is re-trained on the now larger set of labelled data, and the procedure repeats. It is also possible for S to be the whole unlabelled data set. In this case, L and U remain the whole training sample, but the assigned labels on unlabelled instances might vary from iteration to iteration.\nSelf-Training Assumption The assumption of self-training is that its own predictions, at least the high confidence ones, tend to be correct.\nThe major advantages of self-training are its simplicity and the fact that it is a wrapper method. This means that the choice of learner for f in step 3 is left completely open. The self-training procedure “wraps” around the learner without changing its inner workings. This is important for many real world tasks related to natural language processing, where the learners can be complicated black boxes not amenable to changes.\nOn the other hand, it is conceivable that an early mistake made by f (which is not perfect to start with, due to a small initial L) can reinforce itself by generating incorrectly labelled data. Re-training with this data will lead to an even worse f in the next iteration.\n2.2 techniques for information extraction 19\n2.2.5 Word Embeddings\nImage and audio processing systems typically work with rich, high-dimensional datasets encoded as vectors, e.g., the individual raw pixel-intensities for image data, or power spectral density coefficients for audio data. For tasks like object or speech recognition we know that all the information required to successfully perform the task is encoded in the data. However, natural language processing systems traditionally treat words as discrete atomic symbols, and therefore, provide no useful information to the system regarding the relationships that may exist between the individual symbols. This means that a model can leverage very little of what it has learned about cat when it is processing data about dog, for instance, that they are both animals, four-legged, pets, and so on. This kind of representations could lead to data sparsity, and usually means that we may need more data in order to successfully train statistical models. Vector representations of words can overcome these obstacles.\nIt has been shown that for words in the same language, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be (Miller and Charles, 1991). This phenomenon that words that occur in similar contexts tend to have similar meanings has been widely known as Distributional Hypothesis (Harris, 1954), which can be stated in the following way:\nDistributional Hypothesis The degree of semantic similarity between two linguistic expressions A and B is a function of the similarity of the linguistic contexts in which A and B can appear.\nThis hypothesis is the core behind the application of vector-based models for semantic representation of words, which are variously known as word space (Sahlgren, 2006), semantic spaces (Mitchell and Lapata, 2010), vector space models (VSMs) (Turney and Pantel, 2010) or distributional semantic models (DSMs) (Baroni and Lenci, 2010).\nTo have better illustration about distributional hypothesis, consider a foreign word such as wampimuk, occurring in these two sentences: (1) He filled the wampimuk, passed it around and we all drunk some, and (2) We found a little, hairy wampimuk sleeping behind the tree. We could infer that the meaning of wampimuk is either ’cup’ or ’animal’, heavily depends on its context which is either sentence (1) or (2) respectively.\nThe different approaches that leverage this principle can be divided into two categories (Baroni et al., 2014): (i) count-based models, e.g. Latent Semantic Analysis (LSA) (Deerwester et al., 1990), and (ii) predictive models, e.g. neural probabilistic language models (Mikolov et al., 2013).\ncount-based models Count-based models compute the statistics of how often some word co-occurs with its neighbouring words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word (Agirre et al., 2009; Baroni and Lenci, 2010; Bullinaria and Levy, 2007; Padó and Lapata, 2007; Sahlgren, 2006).\nOne widely known algorithm falls under this category is GloVe8 (Pennington et al., 2014). GloVe is essentially a log-bilinear model with a weighted least-squares objective. The main intuition underlying the model is the simple observation that ratios of wordword co-occurrence probabilities have the potential for encoding some form of meaning. For example, consider the co-occurrence probabilities for target words ice and steam with\n8 We took the explanations about GloVe from http://nlp.stanford.edu/projects/glove/.\n20 background\nvarious probe words from the vocabulary. Here are some actual probabilities from a 6 billion word corpus:\nProbability and Ratio k = solid k = gas k = water k = fashion\nP(k|ice) 1.9× 10−4 6.6× 10−5 3.0× 10−3 1.7× 10−5 P(k|steam) 2.2× 10−5 7.8× 10−4 2.2× 10−3 1.8× 10−5 P(k|ice)/P(k|steam) 8.9 8.5× 10−2 1.36 0.96\nAs one might expect, ice co-occurs more frequently with solid than it does with gas, whereas steam co-occurs more frequently with gas than it does with solid. Both words co-occur with their shared property water frequently, and both co-occur with the unrelated word fashion infrequently. Only in the ratio of probabilities does noise from nondiscriminative words like water and fashion cancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and small values (much less than 1) correlate well with properties specific of steam. In this way, the ratio of probabilities encodes some crude form of meaning associated with the abstract concept of thermodynamic phase.\nThe training objective of GloVe is to learn word vectors such that their dot product equals the logarithm of the words’ probability of co-occurrence. Owing to the fact that the logarithm of a ratio equals the difference of logarithms, this objective associates (the logarithm of) ratios of co-occurrence probabilities with vector differences in the word vector space. Because these ratios can encode some form of meaning, this information gets encoded as vector differences as well.\npredictive models In predictive models, instead of first collecting context vectors and then re-weighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013; Turian et al., 2010).\nWord2Vec9 (Mikolov et al., 2013) is a particularly computationally-efficient predictive model for learning word embeddings from raw text. It comes in two flavours, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. Algorithmically, these models are similar, except that CBOW predicts target words (‘mat’) from source context words (‘the cat sits on the’), whereas the skip-gram does the inverse and predict source context words from the target words. This inversion might seem like an arbitrary choice, but statistically it has the effect that CBOW smoothes over a lot of the distributional information, by treating an entire context as one observation. For the most part, this turns out to be a useful feature for smaller datasets. On the other hand, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets.\nNeural probabilistic language models are traditionally trained using the maximum likelihood (ML) principle to maximize the probability of the target word wt given the previous words (history) h in terms of a softmax function:\nP(wt|h) = softmax(score(wt,h))\n= exp{score(wt,h)}∑\nWord w ′ in Vocab exp{score(w ′,h)} (2.18)\n9 We took the explanations about Word2Vec from http://www.tensorflow.org/versions/r0.7/tutorials/ word2vec/.\n2.2 techniques for information extraction 21\nwhere score(wt,h) computes the compatibility of word wt with the context h, typically using a dot product. The model is trained by maximizing its log-likelihood on the training set, i.e. by optimizing:\nJML = logP(wt|h)\n= score(wt,h) − log ( ∑ Word w ′ in Vocab exp{score(w ′,h)} ) (2.19)\nThis yields a properly normalized probabilistic model for language modelling. However, this is very expensive, because we need to compute and normalize each probability using the score for all other words w ′ in the current context h, at every training step.\nIn Word2Vec, a full probabilistic model is not needed. Instead, the CBOW and skipgram models are trained using a binary classification objective (logistic regression) to discriminate the real target words wt from k imaginary (noise) words w̃, in the same context. Mathematically, the objective is to maximize:\nJNEG = logQθ(D = 1|wt,h) + k E w̃∼Pnoise [logQθ(D = 0|w̃,h)] (2.20)\nwhere Qθ(D = 1|w,h) is the binary logistic regression probability under the model of seeing the word w in the context h in the dataset D, calculated in terms of the learned embedding vectors θ. The expectation is approximated by drawing k contrastive words from the noise distribution.\nThis objective is maximized when the model assigns high probabilities to the real words, and low probabilities to noise words. Technically, this is called Negative Sampling, and there is a good mathematical motivation to use this loss function, i.e., the updates it proposes approximate the updates of the softmax function in the limit. But computationally it is especially appealing because computing the loss function now scales only with the number of noise words that are selected (k) instead of the size of the vocabulary.\n3 T E M P O R A L I N F O R M AT I O N P R O C E S S I N G\n3.1 Modelling Temporal Information 23\n3.2 TimeML Annotation Standard 25\n3.2.1 TimeML Tags 25\n3.2.2 ISO Related Standards 29\n3.3 TimeML Annotated Corpora 31\n3.4 TempEval Evaluation Campaigns 33\n3.5 State-of-the-art Methods 35\n3.5.1 Timex Extraction 35\n3.5.2 Event Extraction 37\n3.5.3 Temporal Relation Extraction 37\n3.5.4 Temporal Information Processing 38\n3.6 Conclusions 38\nNewspapers and narrative texts are often used to describe events that occur in a certain time and specify the temporal order of these events. To comprehend such texts, the capability to extract temporally relevant information is clearly required, which includes: (i) identifying events and (ii) temporally linking them to build event timelines. This capability is crucial to a wide range of NLP applications, such as personalized news systems, question answering and document summarization. In this chapter we provide an introduction to temporal information processing that comprises annotation standards, annotated corpora and related evaluation campaigns. We also give a brief overview of the state-ofthe-art methods for extracting temporal information from text.\n3.1 modelling temporal information\nIn NLP, the definition of an event can be varied depending on the target application. In topic detection and tracking (Allan, 2002), the term event is used interchangeably with topic, which describes something that happens and is usually used to identify a cluster of documents, e.g., Olympics, wars. On the other hand, information extraction provides finer granularity of event definitions, in which events are entities that happen/occur within the scope of a document.\nEvents, especially within a narrative text, are naturally anchored to temporal attributes, which are often expressed with time expressions such as ‘two days ago’ or ‘Friday the 13th’. However, an event can also have non-temporal attributes such as event participants and the location where the event took place. Here is where event modelling plays its part in automatic event extraction, to define the structure of events one wants to extract from a text.\nThere are several annotation frameworks for events and time expressions that can be viewed as event models, TimeML (Pustejovsky et al., 2003) and ACE (Consortium, 2005) being the prominent ones. There are other event models based on web ontology\n23\n24 temporal information processing\n(RDFS+OWL) such as LODE (Shaw et al., 2009), SEM (Hage et al., 2011) and DOLCE (Gangemi et al., 2002), which encode knowledge about events as triples. While event triples can be seen as ways to store the extracted knowledge to perform reasoning on, event annotations and the corresponding annotated corpora are geared towards automatic event extraction from texts in natural language.\ntimeml TimeML is a language specification for events and time expressions, which was developed in the context of the TERQAS workshop1 supported by the AQUAINT program. The main purpose is to identify and extract events and their temporal anchoring from a text, such that it can be used to support a question answering system in answering temporally-based questions like “In which year did Iraq finally pull out of Kuwait during the war in the 1990s?”.\nace The ACE annotation framework was introduced by the ACE program, which provides annotated data, evaluation tools, and periodic evaluation exercises for a variety of information extraction tasks. It covers the annotation of five basic kinds of extraction targets including entities, values, time expressions, relations (between entities) and events (ACE, 2005).\ntimeml vs ace Both TimeML and ACE define an event as something that happens/occurs or a state that holds true, which can be expressed by a verb, a noun, an adjective, as well as a nominalization either from verbs or adjectives. However, both event models are designed for different purposes, hence, resulting in different annotation of events. In addition to basic features of events existing in both models (tense, aspect, polarity and modality), ACE events have more complex structures involving event arguments, which can either be event participants (entities participating in the corresponding events) or event attributes (place and time of the corresponding events) (Consortium, 2005).\nWhile in TimeML all events are annotated, because every event takes part in the temporal network2, in ACE only ’interesting’ events falling into a set of particular types and subtypes are annotated.\nIn annotating temporal expressions, ACE and TimeML use similar temporal annotations. ACE uses TIMEX2 (Ferro et al., 2001) model, which was developed under DARPA’s Translingual Information Detection, Extraction and Summarization (TIDES) program, whereas TimeML introduces TIMEX3 annotation modelled on TIMEX (Setzer, 2001) as well as TIMEX2.\nThe most important attribute of TimeML that differs from ACE is the separation of the representation of events and time expressions from the anchoring or ordering dependencies. Instead of treating a time expression as an event argument, TimeML introduces temporal link annotations to establish dependencies (temporal relations) between events and time expressions (Pustejovsky et al., 2003). This annotation is important in (i) anchoring an event to a time expression (event time-stamping) and (ii) determining the temporal order between events. This distinctive feature was the main reason why we chose TimeML as the event model for our research.\n1 http://www.timeml.org/site/terqas/index.html 2 Except for generics as in “Use of corporate jets for political travel is legal.” (Saurí et al., 2006)\n3.2 timeml annotation standard 25\ndefinitions According to TimeML, we can formalize the definitions of temporal information as follows:\n• Events are expressions in text denoting situations that happen or occur, or predicates describing states or circumstances in which something obtains or holds true. They can be punctual or last for a period of time.\n• Time expressions, temporal expressions, or simply timexes are expressions in text denoting time “when” something happens, how often it happens, or how long it lasts.\n• Temporal relations represent the temporal order holding between two arguments, i.e., event and event, event and timex, or timex and timex.\n• Temporal signals are specific types of word indicating or providing a cue of an explicit temporal relation between two arguments.\n3.2 timeml annotation standard\nTimeML introduces 4 major data structures: EVENT for events, TIMEX3 for time expressions, SIGNAL for temporal signals, and LINK for relations among EVENTs and TIMEX3s (Pustejovsky et al., 2003; Saurí et al., 2006). There are three types of LINK tags: TLINK, SLINK and ALINK, which will be further explained in the following section. Note that TimeML EVENTs never participate in a link. Instead, their corresponding event instance IDs, which are realized through the MAKEINSTANCE tag, are used.\nFor the clarity purposes, henceforth, snippets of text annotated with events, timexes and temporal signals serving as examples will be in the respective forms. For example, “John drove for 5 hours.”\n3.2.1 TimeML Tags\nEVENT Events in a text can be expressed by tensed or untensed (phrasal) verbs (1), nominalizations (2), adjectives (3), predicative clauses (4), or prepositional phrases (5).\n1. Foreign ministers of member-states has agreed to set up a seven-member panel to investigate who shot down Rwandan President Juvenal Habyarimana’s plane.\n2. The financial assistance from the World Bank and the International Monetary Fund are not helping.\n3. Philippine volcano, dormant for six centuries, began exploding with searing gases, thick ash and deadly debris.\n4. Those observers looking for a battle between uncompromising representatives and very different ideologies will, in all likelihood, be disappointed.\n5. All 75 people on board the Aeroflot Airbus died.\nNote that some events may be sequentially discontinuous in some context as exhibited in (4). In order to simplify the annotation process, only the word considered as the syntactic head is annotated, shown with bold letters in the examples, except for prepositional phrases.\n26 temporal information processing\nThe attributes for the EVENT tag includes:\n• eid – unique ID number.\n• class – class of the event: REPORTING, PERCEPTION, ASPECTUAL, I_ACTION, I_STATE, STATE or OCCURRENCE.\n• stem – stem of the event’s head.\nMAKEINSTANCE The MAKEINSTANCE tag is an auxiliary tag used to distinguish event tokens from event instances. The typical example of its usage is: to annotate the markable ‘taught’ in “He taught on Monday and Tuesday.” as two event instances happened in different time. The attributes for this tag include:\n• eiid – unique ID number.\n• eventID – unique ID to the referenced EVENT found in the text.\n• tense – tense of the event: PAST, PRESENT, FUTURE, INFINITIVE, PRESPART, PASTPART or NONE.\n• aspect – aspect of the event: PROGRESSIVE, PERFECTIVE, PERFECTIVE_PROGRESSIVE or NONE.\n• pos – part-of-speech tag of the event: ADJECTIVE, NOUN, VERB, PREPOSITION or OTHER.\n• polarity – polarity of the event: POS or NEG.\n• modality – the modal word modifying the event (if exists).\nTIMEX3 The TIMEX3 tag is used to mark up explicit temporal expressions, including dates, times, durations, and sets of dates and times. There are three major types of TIMEX3 expressions: (i) fully specified timexes, e.g., June 11 1989, summer 2002; (ii) underspecified timexes, e.g. Monday, next month, two days ago; (iii) durations, e.g., three months.\nThis tag allows specification of a temporal anchor, which facilitates the use of temporal functions to calculate the value of an underspecified timex. For example, within an article with a document creation time such as ‘January 3, 2006’, the temporal expression ‘today’ may occur. By anchoring the TIMEX3 for ‘today’ to the document creation time, we can determine the exact value of the TIMEX3.\nThe attributes of the TIMEX3 tag, which are of particular interest in the scope of this work, include:3\n• tid – unique ID number.\n• type – type of timex: DATE, TIME, DURATION or SET.\n• value – normalized temporal value of the annotated timex represented in an extended ISO 8601 format.\n• functionInDocument – function of a TIMEX3 in providing a temporal anchor for other temporal expressions in the document: CREATION_TIME, MODIFICATION_TIME, PUBLICATION_TIME, RELEASE_TIME, RECEPTION_TIME, EXPIRATION_TIME or NONE.\n3 The full set of attributes with their descriptions can be found in (Saurí et al., 2006)\n• anchorTimeID – (optional) the timex ID of the timex to which the TIMEX3 markable is temporally anchored.\nSIGNAL The SIGNAL tag is used to mark up textual elements that make relations holding between two temporal elements explicit, which are generally:\n• Temporal prepositions: on, in, at, from, to, during, etc.\n• Temporal conjunctions: before, after, while, when, etc.\n• Prepositions signaling modality: to.\n• Special characters: ‘-’ and ‘/’, in temporal expressions denoting ranges, e.g., September 4-6 or Apr. 1999/Jul. 1999.\nThe only attribute for the SIGNAL tag is sid, corresponding to the unique ID number.\nTLINK The TLINK, Temporal Link, tag is used to (i) establish a temporal order between two events (event-event pair), (ii) anchor an event to a time expression (event-timex pair), and (iii) establish a temporal order between two time expressions (timex-timex pair). Each temporal link has a temporal relation type assigned to it. The temporal relation types are modelled based on Allen’s interval algebra between two intervals (Allen, 1983). Table 3.1 shows the TimeML temporal relation types corresponding to relation types existing in Allen’s interval logic.\nThe Allen’s overlap relation is not represented in TimeML. However, TimeML introduces three more types of temporal relations (IDENTITY, INCLUDES and IS_INCLUDED), resulting in a set of 14 relation types. IDENTITY relation is used to encode event co-reference as\n28 temporal information processing\nexhibited in the following sentence, “John drove to Boston. During his drive he ate a donut.”\nAccording to TimeML 1.2.1 annotation guidelines (Saurí et al., 2006), the difference between DURING and IS_INCLUDED (also their inverses) is that DURING relation is specified when an event persists throughout a temporal duration (1), while IS_INCLUDED is specified when an event happens within a temporal expression (2). Moreover, INCLUDES and IS_INCLUDED relations are used to specify a set/subset relationship between events (3).\n1. John drove for 5 hours.\n2. John arrived on Tuesday.\n3. The police looked into the slayings of 14 women. In six of the cases suspects have already been arrested.\nThe attributes of the TLINK tag include:\n• lid – unique ID number.\n• eventInstanceID or timeID – unique ID of the annotated MAKEINSTANCE or TIMEX3 involved in the temporal link.\n• relatedToEventInstance or relatedToTime – unique ID of the annotated MAKEINSTANCE or TIMEX3 that is being related to.\n• relType – temporal relation holding between the elements: BEFORE, AFTER, INCLUDES, IS_INCLUDED, DURING, DURING_INV, SIMULTANEOUS, IAFTER, IBEFORE, IDENTITY, BEGINS, ENDS, BEGUN_BY or ENDED_BY.\n• signalID – (optional) the ID of SIGNAL explicitly signalling the temporal relation.\nSLINK The SLINK, Subordination Link, tag is used to introduce a directional relation going from the main to the subordinated verb (indicated with s), which can be in one of the following contexts:\n• Modal, e.g., “Mary wanted John to buy s some wine.”\n• Factive, e.g., “John managed to go s to the supermarket.”\n• Counter-factive, e.g., “John forgot to buy s some wine”.\n• Evidential, e.g., “Mary saw John only carrying s beer.”\n• Negative-evidential, e.g., “John denied he bought s only beer.”\n• Conditional, e.g., “If John brings s only beer, Mary will buy some wine.”\nThe attributes of the SLINK tag include:\n• lid – unique ID number.\n• eventInstanceID – unique ID of the annotated MAKEINSTANCE involved in the subordination link.\n3.2 timeml annotation standard 29\n• subordinatedEventInstance – unique ID of the subordinated MAKEINSTANCE that is being related to.\n• relType – subordination relation holding between the event instances: MODAL, EVIDENTIAL, NEG_EVIDENTIAL, FACTIVE, COUNTER_FACTIVE or CONDITIONAL.\n• signalID – (optional) the ID of SIGNAL explicitly signalling the subordination relation.\nALINK The ALINK, Aspectual Link, tag represents the relationship between an aspectual event (indicated with a) and its argument event, belonging to one of the following:\n• Initiation, e.g., “John started a to read.”\n• Culmination, e.g., “John finished a assembling the table.”\n• Termination, e.g., “John stopped a talking.”\n• Continuation, e.g., “John kept a talking.”\n• Reinitiation, e.g., “John resumed a talking.”\nThe attributes of the ALINK tag include:\n• lid – unique ID number.\n• eventInstanceID – unique ID of the annotated (aspectual) MAKEINSTANCE involved in the aspectual link.\n• relatedToEventInstance – unique ID of the MAKEINSTANCE that is being related to.\n• relType – relation holding between the event instances: INITIATES, CULMINATES, TERMINATES, CONTINUES or REINITIATES.\n• signalID – (optional) the ID of SIGNAL explicitly signalling the relation.\nexample Figure 3.1 shows an excerpt of news text annotated with temporal entities and temporal relations in TimeML annotation standard.\n3.2.2 ISO Related Standards\niso 8601 ISO 8601 is an international standard providing an unambiguous method for representing dates and times, which is used by TimeML to represent the value attribute of the TIMEX3 tag. The standard is based on the following principles:4\n• Date, time and duration unit values are organized from the most to the least significant: year, month (or week), day, hour, minute, second and fraction of second.\n• Calendar dates are represented in the form of YYYY-MM-DD, where YYYY, MM and DD are the place-holders for the year, month and day number values respectively, or YYYY-Www with ww for the week-of-year number value.\n4 Note that in the formulation, the emphasized letters denote place-holders for number or designated letter values.\n30 temporal information processing\n3.3 timeml annotated corpora 31\n• Times are represented with respect to the 24-hour clock system and follow the format of hh:mm:ss.ff, with hh, mm, ss and ff for the hour, minute, second and fraction of second values respectively.\n• The combination of a date and a time is represented in the format of: YYYY-MM-DDThh:mm:ss.ff.\n• Durations follow the format of PnX, where n is the duration number value, and X is the duration unit which can be one of the following units: Y, M, W, D, H, M and S for year, month, week, day, hour, minute and second respectively.\n• The combination of duration units follows the format of: PnYnMnDTnHnMnS or PnW.\nAn extended version of ISO 8601 was proposed in the TIDES annotation standard (Ferro et al., 2001) to address the ambiguity and vagueness of natural language:\n• Parts of day, weekend, seasons, decades and centuries were introduced as new concepts. For example, YYYY-Www-WE (where WE indicates weekend), YYYY-MM-DDTPoD (where PoD takes one of the following values: NI, MO, MI, AF and EV for night, morning, midday, afternoon and evening respectively), etc.\n• Additional temporal values are used for temporal expressions such as ‘nowadays’ to refer to either the past, present or future , i.e., PAST_REF, PRESENT_REF or FUTURE_REF respectively.\niso-timeml Adopting the existing TimeML annotation standard, ISO-TimeML aims to define a mark-up language for annotating documents with information about time and events. Several changes to TimeML have been proposed to address capturing temporal semantics in text: (i) stand-off annotations rather than in-line annotations that do not modify the text being annotated, and (ii) the introduction of a new link for measuring out events (MLINK), which characterizes a temporal expression of DURATION type as a temporal measurement of an event. The resulting standard, ISO 24617-1:2012, SemAF-Time, specifies a formalized XML-based mark-up language facilitating the exchange of temporal information (Pustejovsky et al., 2010).\n3.3 timeml annotated corpora\nWe list below several corpora annotated with either simplified or extended TimeML annotation standard, which are freely available for research purposes5. Most corpora are in English, but few other corpora are in other languages, such as Chinese, French, Italian, Korean and Spanish.\ntimebank 1 .2 The TimeBank 1.2 corpus (Pustejovsky et al., 2006) is an annotated corpus of temporal semantics that was created following the TimeML 1.2.1 specification (Saurí et al., 2006). It contains 183 news articles, with just over 61,000 non-punctuation tokens, coming from a variety of news report, specifically from the ACE program and PropBank. The ones taken from the ACE program are originally transcribed broadcast\n5 To access the Clinical TempEval corpus, users must agree to handle the data appropriately, formalized in the requirement that users must sign a data use agreement with the Mayo Clinic.\n32 temporal information processing\nnews from the following sources: ABC, CNN, PRI, VOA and news-wire from AP and NYT. Meanwhile, PropBank contains articles from the Wall Stree Journal. TimeBank 1.2 is freely distributed by the Linguistic Data Consortium.6\naquaint timeml corpus The AQUAINT corpus contains 73 news report document, and freely available for download.7. It is often referred to as the Opinion corpus.\ntempeval related corpora The corpora released in the context of TempEval evaluation campaigns (see Section 3.4), which serve as development and evaluation datasets, are mostly based on the TimeML annotation standard, with some exception in the early tasks for the purpose of simplification:\n• The corpus created for the first TempEval task (Verhagen et al., 2007) at SemEval2007 employs a simplified version of TimeML. For example, there is no event instance annotation (realized with the MAKEINSTANCE tag), and the TLINK types include only three core relations (BEFORE, AFTER and OVERLAP), two less specific relations (BEFORE-OR-OVERLAP and OVERLAP-OR-AFTER) for ambiguous cases, and VAGUE for where no particular relation can be established.\n• As the TempEval-2 task (Verhagen et al., 2010) at SemEval-2010 attempted to address multilinguality, the corpus released within this task includes texts in Chinese, English, French, Italian, Korean and Spanish. The annotation contains the same set of TLINK types used in the previous TempEval.\n• The TempEval-3 corpus created for the TempEval-3 task (UzZaman et al., 2013) at SemEval-2013, however, is based on the latest TimeML annotation guideline version 1.2.1 (Saurí et al., 2006), with the complete set of 14 TLINK types. The corpus contains (i) the enhanced existing corpora, TimeBank 1.2 and AQUAINT, resulting in TBAQ-cleaned as the development data for the task, (ii) the TempEval-3 silver corpus8, and (iii) the newly released TE3-Platinum as the evaluation corpus.\n• The creation of evaluation corpus for QA-TempEval (Llorens et al., 2015) does not require manual annotation of all TimeML elements in the documents. The annotators created temporal-related questions from the documents, such as, “Will Manchester United and Liverpool play each other after they topped their respective groups?”, provided the correct yes/no answers, then annotated the corresponding entities and relations in the text following the TimeML annotation format. There are 294 questions in total, coming from 28 documents belonging to three different domains: news articles, Wikipedia articles (history, biographical) and informal blog posts (narrative).\n• The Clinical TempEval corpus (Bethard et al., 2015) comprises 600 clinical notes and pathology reports from cancer patients at the Mayo clinic. The documents are annotated using an extended TimeML annotation framework, which includes new temporal expression types (e.g., PrePostOp for post-operative), new EVENT attributes (e.g., degree=LITTLE for slight nausea) and new temporal relation type (CONTAINS).\n6 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T08 7 http://timeml.org/site/timebank/aquaint-timeml/aquaint_timeml_1.0.tar.gz 8 The TempEval-3 silver corpus is obtained by running automatic annotation systems, TIPSem and TIPSem-\nB (Llorens et al., 2010) and TRIOS (UzZaman and Allen, 2010), on 600K word corpus collected from Gigaword.\n3.4 tempeval evaluation campaigns 33\n• The evaluation dataset released for the “TimeLine: Cross-Document Event Ordering” task (Minard et al., 2015) consists of 90 Wikinews articles within specific topics (e.g., Airbus, General Motors, Stock Market) surrounding the target entities for which the event timelines are created. An event timeline is represented as ordered events, which are anchored to time with granularity ranging from DAY to YEAR. There are 37 event timelines for target entities of type PERSON (e.g., Steve Jobs), ORGANISATION (e.g., Apple Inc.), PRODUCT (e.g., Airbus A380) and FINANCIAL (e.g., Nasdaq), with around 24 events and 18 event chains per timeline in average.\nita-timebank The Ita-TimeBank corpus (Caselli et al., 2011a) is composed of two corpora (more than 150K tokens) that have been developed in parallel following the It-TimeML annotation scheme for Italian language. The two corpora are (i) the CELCT corpus containing news articles taken from the Italian Content Annotation Bank (I-CAB) (Magnini et al., 2006), and (ii) the ILC corpus, which consists of 171 news articles collected from the Italian Syntactic-Semantic Treebank, the PAROLE corpus and the web.\ntimebank-dense The TimeBank-Dense corpus (Chambers et al., 2014) is created to address the sparsity issue in the existing TimeML corpora. Using a specialized annotation tool, annotators are prompted to label all pairs of events and time expressions in the same sentence, all pairs of events and time expressions in the immediately following sentence, and all pairs of events and the document creation time. The VAGUE relation introduced at the first TempEval task (Verhagen et al., 2007) is adopted to cope with ambiguous temporal relations, or to indicate pairs for which no clear temporal relation exists. The resulting corpus contains 12,715 temporal relations, under the labels BEFORE, AFTER, INCLUDES, IS_INCLUDED, SIMULTANEOUS and VAGUE, over 36 documents taken from TimeBank.9\n3.4 tempeval evaluation campaigns\nTempEval is a series of evaluation campaigns, which are part of SemEval (Semantic Evaluation), an ongoing series of evaluations of computational semantic analysis systems. The ultimate goal of TempEval is the automatic identification of temporal expressions (timexes), events, and temporal relations within a text as specified in TimeML annotation (Pustejovsky et al., 2003). However, since addressing this aim in a first evaluation challenge was deemed too difficult, a staged approach was employed.\ntempeval-1 The first TempEval (Verhagen et al., 2007) focuses only on the categorization of temporal relations into simplified TimeML TLINK types, and only for English. There were three tasks proposed, each is a task of determining the TLINK type of:\n• pairs of event and timex within the same sentence,\n• pairs of event and DCT (Document Creation Time), and\n• pairs of main events of adjacent sentences, where main event is usually the syntactically dominant verb in a sentence.\n9 This is significantly in contrast with the TimeBank corpus containing only 6,418 temporal relations over 183 documents.\n34 temporal information processing\ntempeval-2 TempEval-2 (Verhagen et al., 2010) extended the first TempEval, growing into a multilingual task, and adding three more tasks:\n• determining the extent of time expressions (TIMEX3 tagging) and the attribute values for type and value,\n• determining the extent of events (EVENT tagging) and the attribute values for class, tense, aspect, polarity and modality,\n• determining the TLINK type of pairs of events where one event syntactically dominate the other event.\ntempeval-3 TempEval-3 (UzZaman et al., 2013) is different from its predecessor in several aspects:\n• Dataset In terms of size, the task provided 100K word gold standard data and 600K word silver standard data for training, compared to 50K word corpus used in TempEval-1 and TempEval-2. A new evaluation dataset was developed, TE3Platinum, based on manual annotations by experts over new text.\n• End-to-end extraction task The temporal information extraction tasks are performed on raw text. Participants need to recognize EVENTs and TIMEX3s first, determine which ones to link, then label the links with TLINK types. In previous TempEvals, gold annotated EVENTs, TIMEX3s and TLINKs (without type) were given.\n• TLINK types The full set of relation types according to TimeML is used, as opposed to the simplified one used in earlier TempEvals.\n• Evaluation A single score, temporal awareness score, was reported to rank the participating systems.\nThere were three main tasks proposed in TempEval-3 focusing on TimeML entities and relations:\n• Task A Determine the extent of timexes in a text as defined by the TIMEX3 tag, and determine the value of their type and value attributes.\n• Task B Determine the extent of events in a text as defined by the EVENT tag, and assign the value of the class attribute.\n• Task ABC The end-to-end task that goes from raw text to TimeML annotation of EVENTs, TIMEX3s and TLINKs, which entails performing tasks A and B.\nIn addition to the main tasks, two extra temporal relation tasks were also included:\n• Task C Given gold annotated EVENTs and TIMEX3s, identify the pairs of entities having temporal link (TLINK) and classify the relation type.\n• Task C relation type only Given gold annotated EVENTs, TIMEX3s and TLINKs (without type), classify the relation type.\n3.5 state-of-the-art methods 35\ntempeval continuation At the last SemEval-2015, there are several tasks related to temporal processing, taking it further into different directions: cross-document event ordering (Minard et al., 2015), temporal-related question answering (Llorens et al., 2015) and clinical domain (Bethard et al., 2015).\nWe are particularly interested in the QA TempEval task (Llorens et al., 2015), which requires the participants to perform end-to-end TimeML annotation from the plain text in the same way as in TempEval-3 (Task ABC), but evaluates the systems in terms of correctly answered questions instead of using common information extraction performance measures. The task focuses on answering yes/no questions in the following format: IS <entity1> <RELATION> <entity2> ?, e.g., Is event1 BEFORE event2 ?. The systems are ranked based on the accuracy in answering the questions.\n3.5 state-of-the-art methods\nThe problem of temporal information processing can be decomposed into several subproblems, as has been defined in TempEval-3. Hence, the best participating systems in TempEval-3 for each task, i.e., timex extraction (Task A), event extraction (Task B) and temporal relation extraction (Task C), can be perceived as the state-of-the-art systems (see Table 3.2).10 Apart from TempEval-3, the efforts towards complete temporal information processing are still ongoing, so we also report systems claiming to be better than the best systems in TempEval-3. For some tasks, TIPSem (Llorens et al., 2010), the best performing system in TempEval-2, also performed best in TempEval-3. However, it was used by the annotators to pre-label the evaluation corpus, so it was excluded from the ranking.\n3.5.1 Timex Extraction\nIn terms of recognizing the extent of timexes in a text, both rule-based and data-driven strategies are equally good. The rule-engineering systems HeidelTime, NavyTime and SUTime performed best at relaxed matching with 90.3%, 90.32% and 90.32% F1-score respectively, while the statistical system ClearTK performed best at strict matching with 82.71% F1-score. Strict match is when there is an exact match between the system entity and gold entity, e.g., sunday morning vs sunday morning, whereas relaxed match is when there is at least an overlap between the system entity and gold entity, e.g., sunday vs sunday morning.\nThe rule-engineering systems commonly rely on regular expression (regex) matching to find a temporal expressions in a text, whereas the data-driven approaches regard the problem as a BIO token-chunking task, building a classifier to decide whether a token is at the B(eginning) of, I(nside) of or O(utside) of a timex.\nIn TempEval-3, the timex recognition task also includes determining the type of a timex (DATE, TIME, DURATION of SET) and normalize its value, e.g., the day before yesterday would be normalized into 2015-12-30 (assuming that today is 2016-01-01). The normalization task is currently (and perhaps inherently) done best by rule-engineered systems, HeidelTime being the best with 77.61% F1-score.11 ClearTK included another classifier to determine\n10 Note that we only report the high-performing participating systems (and their best system runs) in TempEval3. 11 The F1-score for value captures the performance of extracting timex and identifying the attribute value together, i.e., valuef1 = timexf1 ∗ valueaccuracy\n36 temporal information processing\ntimex types, but used TIMEN (Llorens et al., 2012), which is rule-based, to normalize timex values.\nMost rule-based approaches for timex normalization use string-to-string translation approach, i.e., each word in the expression is looked up in a normalization lexicon, then the resulting sequence is mapped directly to the normalized form. Both HeidelTime and TIMEN follow this approach. A drawback of this approach is that there are different rules for each expression, e.g., yesterday, the day before yesterday, regardless of the compositional nature that may hold, that the day before yesterday is one day before yesterday.\nTimeNorm (Bethard, 2013a) exploits a synchronous context free grammar for timex normalization to address these shortcomings. Synchronous rules map the source language to formally defined operators for manipulating times. Time expressions are then parsed using an extended CYK+ algorithm, and converted to a normalized form by applying the operators recursively. UWTime (Lee et al., 2014) uses a Combinatory Categorial Grammar (CCG) to construct compositional meaning representations, while also considering contextual cues (e.g. the document creation time, the governing verb’s tense) to compute the normalized value of a timex.\nEvaluated on the TempEval-3 evaluation corpus, UWTime achieved 82.4% F1-score on the value resolution task, while TimeNorm achieved 81.6% accuracy given gold annotated timex extents, compared with 78.5% and 74.1% accuracies achieved by HeidelTime and TIMEN, respectively.\n3.5 state-of-the-art methods 37\n3.5.2 Event Extraction\nAll high performing systems for event recognition in TempEval-3 used machine learning approaches. Typically a system consists of different classifiers each for recognizing events and determining their class attribute. For event extent recognition, since in TimeML the annotated events are usually single-word events, the problem is often regarded as a binary token-classification.12 Meanwhile, since there are 7 event classes in TimeML, the task of determining the class attribute is modelled as a multi-class classification task.\nThe best performing system is ATT with 81.05% and 71.88%, followed by KUL with 79.32% and 70.17%.13 These systems, also TIPSem, use semantic information obtained through semantic role labelling as features, which proves to play an important role in event recognition.\n3.5.3 Temporal Relation Extraction\nIn TempEval-3, identifying which pair of entities are connected by a temporal relation is a new task in the series of TempEval challenges; in TempEval and TempEval-2, the pair of entities are given and limited to specific syntactic constructs. TempEval-3 participants approached the problem with rule-based, data-driven and also hybrid methods. The rules are typically based on the possible TLINK candidates enumerated in the task description: (i) main events of consecutive sentences, (ii) pairs of events in the same sentence, (iii) event and timex in the same sentence and (iv) event and document creation time.\nFor (ii) candidate pairs, TIPSem only considered pairs of events where one is subordinated by the other. ClearTK included three different multi-class classification models (for (ii), (iii) and (iv) candidate pairs) for temporal relation identification, as well as temporal relation type classification. Given a pair of entities, the classifiers have to predict the temporal relation type (BEFORE, AFTER, SIMULTANEOUS, etc.) or NORELATION if there is no relation exists. UTTime-1 only relied on rules to consider candidate pairs as having temporal relations. However, UTTime-5 used re-trained classifiers with an additional relation type UNKNOWN to filter the candidate pairs, in the same way as ClearTK.\nThe hybrid method, employed by NavyTime, combines candidate-pair-rules with four binary classifiers (for (i), (ii), (iii) and (iv) candidate pairs) that decide whether a candidate pair is having temporal relation or not.\nOn the other hand, for classifying the temporal relation types, all participants resort to data-driven approaches. Both TIPSem and UTTime used sentence-level semantic information as features, obtained via semantic role labelling and deep syntactic parsing, respectively.\nRegarding the classifiers used, ClearTK relied on Mallet14 MaxEnt, OpenNLP15 MaxEnt, and LIBLINEAR (Fan et al., 2008), and picked the final classifiers by running a grid search over models and parameters on the training data. UTTime used two LIBLINEAR (Fan et al., 2008) classifiers (L2-regularized logistic regression); one for event-event pairs, i.e., (i) and (ii) candidate pairs, and another one for event-timex pairs, i.e., (iii) and (iv) candidate\n12 Some systems in TempEval3 also modelled the problem as a BIO token-chunking task, e.g., ClearTK, as in for timex recognition. 13 The F1-score for class captures the performance of extracting event and identifying the attribute class together, i.e., classf1 = eventf1 ∗ classaccuracy 14 http://mallet.cs.umass.edu/ 15 http://opennlp.apache.org/\n38 temporal information processing\npairs. In addition to four binary classifiers for identifying candidate pairs having temporal links, NavyTime trained four MaxEnt classifiers for temporal relation classification.\nFor both temporal relation identification and temporal relation type classification tasks, ClearTK is the best performing system with 36.26% F1-score. The organizers also provided the gold annotated temporal links to measure the performance of systems in classifying the temporal relation types (Task C relation type only). UTTime with semantic features performed best with 56.45% F1-score. Using only rules to determine the candidate pairs, UTTime-1 achieved the highest recall (65.64%) at the expense of precision (15.18%). UTTime-5 can obtain a better F1-score by reducing the recall significantly, but still in the second place after ClearTK with 34.90% F1-score.\n3.5.4 Temporal Information Processing\nFor complete temporal annotation from raw text, which is the Task ABC in TempEval-3, the best performing system is ClearTK, with 30.98% F1-score.\n3.6 conclusions\nWe presented an introduction to temporal information processing, particularly in using TimeML as the annotation framework. The separation of temporal entities and temporal anchoring/dependency representations in TimeML, also the fact that events are not limited to specific types, were the main reason why we chose TimeML over ACE for temporal information modelling in our research. The TimeML annotation standard has been described (Section 3.2), along with several corpora annotated with TimeML (Section 3.3).\nWe have also given an overview of state-of-the-arts methods for extracting temporal information from text (Section 3.5), according to TempEval evaluation campaigns (Section 3.4). TempEval-3 results reported by UzZaman et al. (2013) show that even though the performances of systems for extracting TimeML entities are quite good (>80% F1score), the overall performance of end-to-end temporal information extraction systems suffers due to the low performance on extracting temporal relations. The state-of-the-art performance on the temporal relation extraction task yields only around 36% F1-score. This is the main reason underlying our choice to focus this work on the extraction of temporal relations.\nIdentifying temporal relations in a full discourse is a task that is difficult to define. In general it involves the classification of temporal relations between every possible pair of events and timexes. Hence, without a completely labelled graph of events and timexes, we cannot speak about true extraction, but rather about matching human labelling decisions that were constrained by time and effort. The TimeBank-Dense corpus mentioned in Section 3.3 is created to cope with such problem.\nSeveral tasks in line with TempEval (Section 3.4) approach the problem by changing the evaluation scheme used. In QA TempEval, the task is no longer about annotation accuracy, but rather the accuracy for targeted questions. The “TimeLine: Cross-Document Event Ordering” task limited the extraction of event timelines only to events related to specific target entities.\n4 T E M P O R A L R E L AT I O N E X T R A C T I O N\nOur thoughts have an order, not of themselves, but because the mind generates the spatio-temporal relationships involved in every experience. — Robert Lanza\n4.1 Introduction 39\n4.2 Related Work 41\n4.3 Related Publications 41\n4.4 Formal Task Definition 42\n4.5 Method 42\n4.5.1 Temporal Relation Identification 43\n4.5.2 Temporal Relation Type Classification 43\n4.6 Evaluation 50\n4.6.1 TempEval-3 Evaluation 50\n4.6.2 TimeBank-Dense Evaluation 52\n4.6.3 QA TempEval Evaluation 54\n4.7 Conclusions 56\nIn the previous chapter we have shown that the low performance of state-of-the-art extraction systems for temporal information processing (30.98% F1-score) is mainly due to the low performance of temporal relation extraction (36.26% F1-score). Hence our decision to focus on this particular task. In this chapter, we describe our efforts in building an improved temporal relation extraction system. The system is evaluated with the TempEval-3 evaluation scheme, to be comparable with the reported state-of-the-art systems.\nMoreover, as has been discussed in the previous chapter, given the sparse annotation in the TempEval-3 corpus, it is not trivial to determine whether an unannotated temporal link is actually missed by the annotator or is simply having no relation. TimeBank-Dense (Section 3.3) was created to address the problem by building a complete labelled graph of temporal links, shifting the evaluation scheme from identifying (and classifying) some relations to all relations. On the other hand, QA TempEval (Section 3.4) shifted the evaluation methodology towards a more extrinsic goal of question answering, evaluating systems based on how well the extracted temporal relations can be used to answer questions about the text. In this chapter, we also report our system’s performances evaluated with both evaluation methodologies.\n4.1 introduction\nTemporal relations, or temporal links, are annotations that bring together pieces of markable temporal information in a text, and make formal representation of temporally ordered events possible. In TimeML, temporal relation types have been modelled based on Allen’s interval algebra between two intervals (Allen, 1983). In Table 3.1 in Section 3.2.1 we show the relation types defined in Allen’s interval logic, along with the corresponding TLINK types in TimeML.\n39\n40 temporal relation extraction\nTimeML (Pustejovsky et al., 2003) is the annotation framework used in the TempEval series, evaluation exercises focused on temporal information processing, i.e. the extraction of temporal expressions (timexes), events and temporal relations in a text (see Section 3.2 and Section 3.4). According to TempEval-3 results reported by UzZaman et al. (2013), while systems for timex extraction and event extraction tasks yield quite high performances with over 80% F1-scores, the best performing system achieved very low performance on the temporal relation extraction task, bringing down the overall performance on the end-to-end temporal information processing task to only around 30% F1-score. This is the main reason why we focus our attention on the automatic extraction of temporal relations.\nIdentifying temporal relations in a full discourse is a very difficult task. In general it involves the classification of temporal relations between every possible pair of events and timexes. With n markable elements in a text, the total number of possible temporal links is n2 − n. Most of the research done so far focused on estimating the relation type, given an annotated pair of temporal events and timexes. In TempEval-1 and TempEval2, participants were given gold annotated temporal links, which are missing the type annotation, between temporal entities following predefined syntactic constructs, e.g. pairs of main events of adjacent sentences.\nIn TempEval-3, participants were required to identify pairs of temporal entities connected by a temporal link (TLINK), but possible TLINK candidates were only: (i) main events of consecutive sentences, (ii) pairs of events in the same sentence, (iii) event and timex in the same sentence and (iv) event and document creation time. Moreover, compared to earlier TempEval campaigns, TempEval-3 required the recognition of the full set of temporal relations in TimeML (14 TLINK types) instead of a simplified set, increasing the task complexity.\nIn this chapter, we describe our methods in building an improved temporal relation extraction system (Section 4.5), then evaluate our system following the TempEval-3 evaluation scheme to be able to compare it with the state-of-the-art systems (Section 4.6.1).\nHowever, the sparse annotation of temporal relations in the TempEval corpora makes it difficult to build an automatic extraction system and evaluate the system regarding its performance, particularly on identifying temporal links. The best system in TempEval-3 for labelling the temporal links with 14 temporal relation types (Task C classifying only), UTTime (Laokulrat et al., 2013), achieved around 56% F1-score. When the system is evaluated on the temporal relation extraction task (Task C: identifying + classifying), its performance dropped to 24.65% F1-score, even though it gained a very high recall of 65.64%. The best performing system for Task C in TempEval-3, ClearTK (Bethard, 2013a), optimized only relation classification and intentionally left many pairs unlabelled, balancing the precision and recall into 36.26% F1-score.\nThe TimeBank-Dense corpus (Section 3.3) is created to cope with this sparsity issue. Using a specialized annotation tool, annotators are prompted to label all possible pairs of temporal entities, resulting in a complete graph of temporal relations. On the other hand, one of the continuation of the TempEval series, QA TempEval (Section 3.4), approached the problem by changing the evaluation scheme used. The task is no longer about annotation accuracy, but rather the accuracy for answering targeted questions.\nTherefore, we also evaluate our system following the TimeBank-Dense and QA TempEval evaluation methodologies (Section 4.6.2 and Section 4.6.3, respectively), to give a\n4.2 related work 41\ncomplete overview on how well our system can extract temporal relations between temporal entities in a text.\n4.2 related work\nSupervised learning for temporal relation extraction has already been explored in several earlier works. Most existing models formulate temporal ordering as a pairwise classification task, where each pair of temporal entities is classified into temporal relation types (Chambers et al., 2007; Mani et al., 2007).\nSeveral works have tried to exploit an external temporal reasoning module to improve the supervised learning models for temporal relation extraction, through training data expansion (Mani et al., 2007; Tatu and Srikanth, 2008), or testing data validation, i.e., replacing the inconsistencies in automatically identified relations (if any) with the next best relation types (Tatu and Srikanth, 2008). Some other works tried to take advantage of global information to ensure that the pairwise classifications satisfy temporal logic transitivity constraints, using frameworks like Integer Linear Programming and Markov Logic Networks (Chambers and Jurafsky, 2008b; UzZaman and Allen, 2010; Yoshikawa et al., 2009). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008b).\nIn the context of TempEval evaluation campaigns (Section 3.4), which is a series of evaluations of temporal information processing systems, our research on temporal relation extraction is based on the third instalment of the series, TempEval-3. For the tasks related to temporal relation extraction (Task C and Task C relation type only), there were five participants in total, including ClearTK (Bethard, 2013a), UTTime (Laokulrat et al., 2013) and NavyTime (Chambers, 2013), which are reported in Section 3.5.3. These systems resorted to data-driven approaches for classifying the temporal relation types, using morphosyntactic information (e.g., PoS tags, syntactic parsing information) and lexical semantic information (e.g., WordNet synsets) as features. UTTime additionally used sentence-level semantic information (i.e., predicate-argument structure) as features.\nOur proposed approach for temporal relation type classification is inspired by recent works on hybrid classification models (Chambers et al., 2014; D’Souza and Ng, 2013). D’Souza and Ng (2013) introduce 437 hand-coded rules along with supervised classification models using lexical relation features (extracted from Merriam-Webster dictionary and WordNet), as well as semantic and discourse features. CAEVO, a CAscading EVent Ordering architecture by Chambers et al. (2014), combines rule-based and data-driven classifiers in a sieve-based architecture for temporal ordering. The classifiers are ordered by their individual precision. After each classifier-sieve proposes its labels, the architecture infers transitive links from the new labels, adds them to the temporal label graph and informs the next classifier-sieve about this decision.\n4.3 related publications\nIn Mirza and Tonelli (2014b), we argue that using a simple set of features, avoiding complex pre-processing steps (e.g., discourse parsing, deep syntactic parsing, semantic role labelling), combined with carefully selected contributing features, could result in a better performance compared with the work of D’Souza and Ng (2013) and the best system in TempEval-3 (Task C relation type only), UTTime (Laokulrat et al., 2013).\n42 temporal relation extraction\nFor QA TempEval task, we submitted our temporal information processing system, HLT-FBK (Mirza and Minard, 2015), which ranked 1st in all three domains: News, Wikipedia and Blogs (informal narrative text).\nBoth works serve as the basis of our proposed temporal relation extraction system that will be described in the following sections.\n4.4 formal task definition\nFor temporal relation extraction, we perform two tasks: identification and classification. We first identify pairs of temporal entities having temporal relations, then classify the temporal relation types of these pairs.\ntemporal relation identification Given a text annotated with a document creation time (DCT) and temporal entities, which can be an event or timex, identify which entity pairs are considered as having temporal relations.\ntemporal relation type classification Given an ordered pair of entities (e1, e2), which can be a timex-timex (T-T), event-DCT (E-D), event-timex (E-T) or event-event (E-E) pair, assign a certain label to the pair, which can be one of the 14 TLINK types: BEFORE, AFTER, INCLUDES, IS_INCLUDED, DURING, DURING_INV, SIMULTANEOUS, IAFTER, IBEFORE, IDENTITY, BEGINS, ENDS, BEGUN_BY or ENDED_BY.\nexample Consider the following excerpt taken from the TimeBank corpus, annotated with events and temporal expressions:\nDCT=1989-10-30 t0 According to the filing, Hewlett - Packard acquired E24 730,070 common shares from Octel as a result of an Aug. 10, 1988 T25 , stock purchase agreement E26 . That accord E27 also called E28 for Hewlett - Packard to buy E29 730,070 Octel shares in the open market within 18 months T30 .\nThe temporal relation extraction system should be able to identify, among others:\n• timex-timex: [T0 AFTER T25], [T30 AFTER T25] • event-DCT: [E24 BEFORE T0, [E28 BEFORE T0] • event-timex: [E26 IS_INCLUDED T25], [E29 DURING T30] • event-event: [E24 AFTER T26], [E27 INCLUDES T28]\n4.5 method\nWe propose a hybrid approach for temporal relation extraction, as illustrated in Figure 4.1. Our system, TempRelPro, is composed of two main modules: (i) temporal relation identification, which is based on a simple set of rules, and (ii) temporal relation type classification, which is a combination of rule-based and supervised classification modules, and a temporal reasoner component in between.\n4.5 method 43\n4.5.1 Temporal Relation Identification\nAll possible pairs having temporal relations according to the TempEval-3 task description are extracted using a set of simple rules; pairs of temporal entities satisfying one of the following rules are considered as having temporal links (TLINKs):\n• pairs of main events of consecutive sentences\n• pairs of events in the same sentence\n• pairs of event and timex in the same sentence\n• pairs of event and document creation time\n• pairs of all possible timexes (including document creation time) linked with each other1\nThese pairs are then grouped together into four different groups: timex-timex (T-T), event-DCT (E-D), event-timex (E-T) and event-event (E-E).\n4.5.2 Temporal Relation Type Classification\nOur approach for temporal relation type classification is inspired by CAEVO (Chambers et al., 2014), which combines rule-based and supervised classifiers in a sieve-based architecture. One of the benefits of this architecture is the seamless enforcement of transitivity constraints, by inferring all transitive relations from each classifier-sieve’s output before\n1 Note that this is not included in the enumerated possible TLINKs in the TempEval-3 task description.\n44 temporal relation extraction\nCAEVO TempRelPro Differences\nRules-Verb/Time Adjacent Event-timex Rules\nRules-TimeTime Timex-timex Rules\nRules-Reporting Governor Event-event Rules\nRules-Reichenbach Event-event Rules\nRules-General Governor Event-event Rules\nRules-WordNet -\nRules-Reporting DCT Event-DCT Rules TempRelPro considers all types of events, not only reporting events\nthe graph is passed on to the next one. The classifiers are ordered based on their precision. Hence, the most precise ones based on linguistic motivated rule-based approaches are executed first, followed by machine learned ones.\nWe also follow the idea of a sieve-based architecture. However, our proposed system is different than CAEVO regarding the following:\n• We consider all rule-based classifiers as one sieve component (rule-based sieve), and all Support Vector Machine (SVM) classifiers as another one (machine-learned sieve).\n• Instead of running transitive inference after each classifier, we run our temporal reasoner module (Section 4.5.2.5) on the output of the rule-based sieve, only once.\n• We use the output of the rule-based sieve as features for the machine-learned sieve, specifically:\n– The timex-DCT link label proposed by the timex-timex rules (Section 4.5.2.1) are used as a feature in the event-timex SVM (Section 4.5.2.6)\n– The event-DCT link label proposed by the event-DCT rules (Section 4.5.2.1) are used as a feature in the event-event SVM (Section 4.5.2.6)\n• In Table 4.1 we report the comparison between CAEVO’s sieves and ours. Several sieves are not implemented in our system. Some others are different in terms of generality. Note that the last sieve of CAEVO, which labels all unlabelled pairs with VAGUE, is not implemented in our system, but implicitly embedded in our machinelearned models.\n4.5.2.1 Timex-timex Rules\nOnly temporal expressions of types DATE and TIME are considered in the hand-crafted set of rules, based on their normalized values. For example, 7 PM tonight with value = 2015-12-12T19:00 IS_INCLUDED in today with value = 2015-12-12.\n4.5 method 45\n4.5.2.2 Event-DCT Rules\nThe rules for E-D pairs are based on the tense and/or aspect of the event word:\n• If tenseE = PAST and aspectE = PERFECTIVE then [E BEFORE D]\n• If tenseE = PRESENT and aspectE = PROGRESSIVE then [E INCLUDES D]\n• If tenseE = PRESENT and aspectE = PERFECTIVE_PROGRESSIVE then [E INCLUDES D]\n• If tenseE = FUTURE then [E AFTER D]\n4.5.2.3 Event-timex Rules\nMany prepositions in English have temporal senses, as has been discussed in The Preposition Project (TPP) (Litkowski and Hargraves, 2006) and the Pattern Dictionary of English Prepositions (PDEP) (Litkowski, 2014). We took the list of temporal prepositions2 and built a set of rules for E-T pairs based on their temporal senses (tsense). The rules are only applied whenever a temporal preposition establishes a temporal modifier relationship between an event (E) and a timex (T), based on the existing dependency path:\n• If tsense = TimePoint (e.g., in, at, on) then [E IS_INCLUDED T]\n• If tsense = TimePreceding (e.g., before) then [E BEFORE T]\n• If tsense = TimeFollowing (e.g., after) then [E AFTER T]\n• If tsense = Duration (e.g., during, throughout) then [E DURING T]\n• If tsense = StartTime (e.g., from, since) then [E BEGUN_BY T]\n• If tsense = EndTime (e.g., until) then [E ENDED_BY T]\nIn the absence of a temporal preposition, a timex might simply be a temporal modifier of an event, as exemplified in “Police confirmed E Friday T that the body was found...”. In this case, we assume that [E IS_INCLUDED T].\nMoreover, sometimes events are modified by temporal expressions marking the starting time and ending time in a duration pattern. For example, ‘between tmx_begin and tmx_end’, ‘from tmx_begin to/until tmx_end’ or ‘tmx_begin-tmx_end’. We define the rules as follow:\n• If T matches tmx_begin then [E BEGUN_BY T]\n• If T matches tmx_end then [E ENDED_BY T]\n4.5.2.4 Event-event Rules\nThe first set of rules applied to E-E pairs is based on the existing dependency path (dep)3 between the first event (E1) and the second event (E2):\n• If E2 is the logical subject of E1 (a passive verb), i.e., dep = LGS-PMOD then [E1 AFTER E2], e.g., “The disastrous chain reaction touched E1 off by the collapse E2 of Lehman Brothers...”\n2 http://www.clres.com/db/classes/ClassTemporal.php 3 The dependency path syntax is according to The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and\nSemantic Dependencies (Surdeanu et al., 2008).\n46 temporal relation extraction\n• If E2 is the locative adverb of E1, i.e., dep = LOC-PMOD then [E1 IS_INCLUDED E2], e.g., “China’s current economic policies cause an enormous surge E1 in coal consumption E2 .”\n• If E2 is the predicative complement of E1 (a raising/control verb), i.e., dep = OPRD-IM or dep = OPRD:\n– If E1 is an aspectual verb for initiation (e.g., begin, start) then [E1 BEGINS E2], e.g., “The situation began E1 to relax E2 in the early 1990s.”\n– If E1 is an aspectual verb for culmination/termination (e.g., finish, stop) then [E1 ENDS E2], e.g., “There ’s some price at which we ’d stop E1 bidding E2 .”\n– If E1 is an aspectual verb for continuation (e.g., continue, keep) then [E1 INCLUDES E2], e.g., “The maturing industry ’s growth continues E1 to slow E2 .”\n– If E1 is a general verb and aspectE1 = PERFECTIVE_PROGRESSIVE then [E1 SIMULTANEOUS E2], e.g., “Hewlett-Packard have been working E1 to develop E2 quantum computers.”\n– If E1 is a general verb then [E1 BEFORE E2], e.g., “The AAR consortium attempted E1 to block E2 a drilling joint venture.”\nThe other sets of rules are taken from CAEVO (Chambers et al., 2014), including:\n• Rules for links between a reporting event and another event that is syntactically dominated by the reporting event, based on the tense and aspect of both events.\n• Reichenbach rules based on the analysis of the role played by various tenses of English verbs in conveying temporal discourse (Reichenbach, 1947).\n4.5.2.5 Temporal Reasoner\nConsider as an example the following news excerpt taken from the TimeBank corpus, annotated with events and temporal expressions:\nShe (Magdalene Albright, Ed.) then lavished praise, and the State Department’s award for heroism, on embassy staffers before meeting e116 with bombing victims at the Muhimbili Medical Center and with government officials. [. . . ] (During the meeting, Ed.) Albright announced e45 a gift of 500 pounds (225 kilograms) of medical supplies to Tanzania and Kenya from the Walter Reed Army Medical Center. She also pledged e46 to ask e47 Congress to approve [. . . ]\nThe annotated temporal relations of the documents are the following: [e45 BEFORE e46], [e46 BEFORE e47], [e116 IS_INCLUDED e45]4 and [e116 INCLUDES e46].\nAn annotated TimeML document can be mapped into a constraint problem according to how TLINKs are mapped into Allen relations (Table 3.1). A possible mapping is as follows:\n• < and > for BEFORE and AFTER\n• o and o−1 for DURING and DURING_INV\n4 This is an annotation error that later causes an inconsistency in the temporal graph during the consistency checking.\n4.5 method 47\n• d and d−1 for IS_INCLUDED and INCLUDES\n• s and s−1 for BEGINS and BEGUN_BY\n• f and f−1 for ENDS and ENDED_BY\nFor instance, the TLINKs in the previous excerpt can be mapped as follows: < for BEFORE between announced and pledged; d for IS_INCLUDED between meeting and announced; and d−1 for INCLUDES between meeting and pledge. Other mappings are possible, e.g., by relaxing the mapping of BEFORE and its inverse AFTER into <m and <m−1, respectively, considering vagueness in interpreting temporal annotations. For example, < for BEFORE between announced and pledged could be replaced by < m in case of uncertainty whether one event is before or immediately before the other.\nThese and other mappings are handled by the Service-oriented Qualitative Temporal Reasoner (SQTR), which was developed for reasoning on TimeML documents within the TERENCE FP7 project (GA n. 257410) in a Service-Oriented Architecture context (Erl, 2004). SQTR is used to check consistency and perform deduction, and relies on the Generic Qualitative Reasoner (GQR), a fast solver for generic qualitative constraint problems, such as Allen constraint problems. The rationale of preferring GQR to other solutions, such as fast SAT solvers, is due to its scalability, simplicity of use and efficiency performances (Westphal and Wölfl, 2009).\nSQTR behaves as follows:\n• In case of consistency checking, SQTR maps the TimeML document into a GQR constraint problem, invokes GQR, and returns a true/false value. In case of consistency, it also returns the mapping for which consistency is found for informing the deduction operation. If we consider the previous example, the system will detect an inconsistency, caused by the annotation of IS_INCLUDED between meeting and announced, which should be INCLUDES instead for the set of TLINKs to be consistent.\n• In case of deduction, SQTR maps the TimeML document into a GQR constraint problem, invokes GQR, maps the GQR output to a TimeML document, marks the deduced TLINKs with an attribute deduced set to true, and returns such a document as the result. The system will deduce, for example, a new relation BEFORE between announced and ask, because the same relation holds between announced and pledge and between pledge and ask.\nNote that the temporal reasoner only deduce new TLINKs if the TimeML document is found to be consistent.\n4.5.2.6 SVM Classifiers\nWe built three supervised classification models each for event-DCT (E-D), event-timex (E-T) and event-event (E-E) pairs, using LIBLINEAR (Fan et al., 2008) L2-regularized L2loss linear SVM (dual), with default parameters, and one-vs-rest strategy for multi-class classification.\ntools and resources Several external tools and resources are used to extract features from each temporal entity pair, including:\n48 temporal relation extraction\n• TextPro tool suite5 (Pianta et al., 2008) to get the morphological analysis (PoS tags, shallow phrase chunk) of each token in the text.\n• Mate tools6 (Bjorkelund et al., 2010) to extract the dependency path between tokens in the document.\n• WordNet similarity module7 to compute (Lin) semantic similarity/relatedness (Lin, 1998) between words.\n• Temporal signal lists as described in Mirza and Tonelli (2014b). However, we further expand the lists using the Paraphrase Database (Ganitkevitch et al., 2013), and manually cluster some signals together, e.g. {before, prior to, in advance of }. Finally, we have 50 timex-related and 138 event-related temporal signals in total, which are clustered into 27 and 35 clusters, respectively (Appendix A.1).\nfeature set The implemented features are listed in Table 4.2. Some features are computed independently based on either e1 or e2 of the temporal entity pairs, while some others are pairwise features, which are computed based on both entities. In order to have a feature vector of reasonable size, we simplified the possible values of some features during the one-hot encoding:\n• dependencyPath We only consider several dependency path between the event pairs denoting e.g. coordination, subordination, subject and object relations.\n• signalTokens The clusterID of signal cluster, e.g., {before, prior to, in advance of }, is considered as a feature instead of the signal tokens.\n• signalDependency For each atomic label in a vector of syntactic dependency labels according to Surdeanu et al. (2008),9 if the signal dependency path contains the atomic label, the value in the feature vector is flipped to 1. Hence, TMP-SUB and SUB-TMP will have the same one-hot representations.\n• wnSim The value of WordNet similarity measure is discretized as follows: sim 6 0.0, 0.0 < sim 6 0.5, 0.5 < sim 6 1.0 and sim > 1.0.\nNote that several features from Mirza and Tonelli (2014b) such as string features and temporal discourse connectives10 are not used. String features, i.e., token and lemma of temporal entities, are removed in order to increase the classifiers’ robustness in dealing with completely new texts with different vocabularies. So instead, we include WordNet similarity in the feature set. Temporal discourse connectives are no more included as features because it did not prove to be beneficial.\n5 http://textpro.fbk.eu/ 6 http://code.google.com/archive/p/mate-tools/ 7 http://ws4jdemo.appspot.com/ 8 The order of e1 and e2 in event-event pairs is always according to the appearance order in the text, while in\nevent-timex pairs, e2 is always a timex regardless of the appearance order. 9 We manually selected a subset of such labels that are relevant for temporal modifier.\n10 The information about discourse connectives was acquired using the addDiscourse tool (Pitler and Nenkova, 2009), which identifies connectives based on syntactic constructions, and assigns them to one of four semantic classes: Temporal, Expansion, Contingency and Comparison.\nlabel simplification During the feature extraction process for training the classification models, we collapse some labels, i.e., IBEFORE into BEFORE, IAFTER into AFTER, DURING and DURING_INV into SIMULTANEOUS, in order to simplify the learning process, also considering the sparse annotation of such labels in the datasets.\n50 temporal relation extraction"
    }, {
      "heading" : "T-T E-D E-T E-E T-T E-D E-T E-E",
      "text" : "4.6 evaluation\n4.6.1 TempEval-3 Evaluation\ndataset We use the same training and test data released in the context of Tempeval3. Two types of training data were made available in the challenge: TBAQ-cleaned and TE3-Silver-data. The former includes the cleaned and improved version of the TimeBank 1.2 corpus and the AQUAINT TimeML corpus (see Section 3.3). TE3-Silver-data, instead, is a 600K word corpus annotated by the best performing systems at Tempeval-2, which we do not use because it was proven not so useful for temporal relation extraction task (UzZaman et al., 2013). For evaluation, the newly created TempEval-3-platinum evaluation corpus is used. The distribution of the relation types in training and test datasets is shown in Table 4.3.\nevaluation metrics TempEval-3 introduced an evaluation metric (UzZaman and Allen, 2011) capturing temporal awareness in terms of precision, recall and F1-score. To compute precision and recall, the correctness of annotated temporal links is verified using temporal closure, by checking the existence of the identified relations in the closure graph. However, there is a minor variation of the formula, that the reduced graph of relations is considered instead of all relations of the system and reference.11\nPrecision = |Sys−relation ∩ Ref + relation|\n|Sys−relation|\n11 Details can be found in Chapter 6 of (UzZaman, 2012).\n4.6 evaluation 51\nRecall = |Ref−relation ∩ Sys + relation|\n|Ref−relation|\nPrecision is the proportion of the number of reduced system relations (Sys−relation) that can be verified from the reference annotation temporal closure graph (Ref+relation), out of the number of temporal relations in the reduced system relations (Sys−relation). Similarly, Recall is the proportion of number of reduced reference annotation relations (Ref−relation) that can be verified from the system’s temporal closure graph (Sys+relation), out of the number of temporal relations in reduced reference annotation (Ref−relation). In order to replicate this type of evaluation, we use the scorer made available to the task participants.\nevaluation results We compare in Table 4.4 the performance of TempRelPro to the other systems participating in temporal relation tasks of TempEval-3, Task C and Task C relation type only, according to the figures reported in (UzZaman et al., 2013). We also compare TempRelPro performance with our preliminary results reported in Mirza and Tonelli (2014b) for Task C relation type only.\nFor the temporal relation type classification task (Task C relation type only), TempRelPro achieves the best performance with 61.86% F1-score. For the temporal relation extraction task (Task C), our approach is most similar to UTTime-1 with the highest recall in TempEval-3. In comparison with UTTime-1, we can double the precision without reducing too much the recall. TempRelPro achieves the best F1-score of 40.15%, almost 4% increase compared with the best system in TempEval-3, ClearTK-2.\nWe also report in Table 4.5 the performances of each module included in TempRelPro, evaluated on TempEval-3-platinum. The temporal relation identification module (Section 4.5.1) obtains a very low precision for T-T pairs because the dataset contains very few annotated timex-timex links. If we remove the T-T pairs, we can increase the F1-score for the temporal relation identification task by 12%. Therefore, in our final annotated TimeML documents for the TempEval-3 evaluation, T-T pairs are not included, even though they play a big role in the temporal relation type classification task.\nRegarding the temporal relation type classification modules (Section 4.5.2), there is no significant improvement by combining rule-based and machine-learned sieves (RB + ML), compared with only using machine-learned classifiers (ML), particularly for E-T and E-E pairs. However, introducing the temporal reasoner in between (RB + TR + ML) results in significant improvement especially for E-T pairs, since recall increases from 72% to 81%.\n52 temporal relation extraction\nWe also compare TempRelPro performances for this classification task to a majority class baseline for each temporal entity type according to the distribution of temporal relation types in the training data (Table 4.3), i.e., BEFORE for T-T, BEFORE for E-D, IS_INCLUDED for E-T and BEFORE for E-E pairs.\n4.6.2 TimeBank-Dense Evaluation\ndataset We follow the experimental setup in Chambers et al. (2014), in which the TimeBank-Dense corpus (mentioned in Section 3.3) is split into a 22 document training set, a 5 document development set and a 9 document test set12. All the classification models for the machine-learned sieve are trained using the training set. We evaluate our system performances on the test set.\nadjustments The set of TLINK types used in TimeBank-Dense corpus is different from the one used in TempEval-3. Some relation types are not used, and the VAGUE relation introduced at the first TempEval task (Verhagen et al., 2007) is adopted to cope with ambiguous temporal relations, or to indicate pairs for which no clear temporal relation exists. The final set of TLINK types in TimeBank-Dense includes: BEFORE, AFTER, INCLUDES, IS_INCLUDED, SIMULTANEOUS and VAGUE. Therefore, we map the relation types of TLINKs labelled by TempRelPro as follows:13\n• IBEFORE, BEGINS and ENDED_BY into BEFORE\n• IAFTER, BEGUN_BY and ENDS into AFTER\n• IDENTITY, DURING and DURING_INV into SIMULTANEOUS\n12 Available at http://www.usna.edu/Users/cs/nchamber/caevo/. 13 We tried different mappings in our experiments and found this mapping to be the one giving the best\noutcome.\n4.6 evaluation 53\nMoreover, we introduce some rules for E-D and E-T pairs to recover the VAGUE relations, such as:\n• If the PoS tag of the event in an E-D pair is an adjective then [E VAGUE D]\n• If the timex value in an E-T pair is PAST_REF, PRESENT_REF or FUTURE_REF then [E VAGUE T]\nevaluation results In Table 4.6 we report the performances of TempRelPro compared with CAEVO. We achieve a small improvement in the overall F1-score, i.e., 51.1% vs 50.7%. For each temporal entity pair type, since we label all possible links, precision and recall are the same. TempRelPro is significantly better than CAEVO in labelling T-T and E-T pairs.\nWe also report in Table 4.7 the performances of each module composing TempRelPro, evaluated on the TimeBank-Dense test set. Note that one of differences between TempRelPro and CAEVO is that in TempRelPro machine-learned sieve (ML) is the last sieve, while in CAEVO AllVague is the last sieve. This explains the big difference of F1-score for the RB + TR + ML composition in TempRelPro and CAEVO, i.e., 51.1% vs 39.8%.\nIn general, combining RB and ML modules results in a slight improvement (47.8% to 49.4% F1-score), especially for T-T (since there is no ML classifier for T-T pairs) and ED pairs, but not for E-T and E-E pairs. Introducing the TR module in between (RB + TR + ML) is even more beneficial, resulting in overall 51.1% F1-score, especially for E-T pairs with an increase from 48% to 55.6% F1-score. This is in line with the results of the TempEval-3 evaluation (Section 4.6.1).\nWith only two sieves, TempRelPro is arguably more efficient than CAEVO, because (i) the temporal closure inference over extracted TLINKs is run only once and (ii) we use less classifiers in general (see Table 4.1). Our decision to consider all rule-based classifiers as one sieve is motivated by the hypothesis that entity pairs generated by each rule-based\n54 temporal relation extraction\nclassifier, i.e. E-D, E-T and E-E pairs, are independent of each other. Using the consistency checking module of the temporal reasoner, we found out that all the documents in the test set, annotated by the rule-based classifiers, are consistent, which supports our hypothesis.\n4.6.3 QA TempEval Evaluation\ndataset The training data set is the TimeML annotated data released by the task organizers, which includes TBAQ-cleaned and TE3-Platinum corpora reused from the TempEval-3 task (UzZaman et al., 2013). The test data are 30 plain texts extracted from news, wikipedia and blogs domains (10 documents each). For evaluating the system, 294 temporal-based questions and the test data annotated with entities relevant for the questions are used.\ntemporal entity extraction system We use the same systems reported in Mirza and Minard (2015) for timex and event extraction.\nevaluation system Given the documents labelled by the participating systems, the evaluation process consists of three main steps (Llorens et al., 2015):\n• ID normalization: this step is performed because systems may provide different IDs to the same temporal entities annotated in the gold standard test data.\n• Timegraph generation: Timegraph (Gerevini et al., 1995) is used to compute temporal closure as proposed by Miller and Schubert (1990). Timegraph is first initialized by adding the system’s explicit TLINKs. Then the Timegraph’s reasoning mechanism infers implicit relations through rules such as transitivity.\n• Question processing: queries are converted to point-based queries in order to check the necessary point relations in Timegraph to verify an interval relation. For example, to answer the question “is e1 AFTER e2”, the evaluation system verifies whether start(e1) > end(e2); if it is verified then the answer is true (YES), if it conflicts with the Timegraph then it is false (NO), otherwise it is UNKNOWN.\nevaluation metrics For each question the obtained answer from the Timegraph (created with system annotations) is compared with the expected answer (human annotated).\nPrecision (P) = num_correct\nnum_answered\nRecall (R) = num_correct\nnum_questions\nF1-score (F1) = 2 ∗ P ∗ R P+ R\nRecall (QA accuracy) is used as the main metrics to rank the systems, and F1-score is used in case of the same recall. Coverage is used to measure how many questions can be answered by a system, regardless of the correctness.\n4.6 evaluation 55\nevaluation results We compare TempRelPro with our previous system submitted for QA TempEval, HLT-FBK (Mirza and Minard, 2015), in Table 4.8. HLT-FBK shows a significant improvement by including an event co-reference rule14 (HLT-FBK + coref). The event co-reference information was obtained from the NewsReader pipeline.15 For TempRelPro, we include the event co-reference rule in the rule-based sieve for E-E pairs (Section 4.5.2.4). Using event co-reference, the overall performance of TempRelPro (TempRelPro + coref) is slightly improved, especially for Blogs domain. In general, HLT-FBK + coref is very good in covering the number of questions answered (Cov), but not in answering accurately.\nThe QA TempEval organizers also provide an extra evaluation, augmenting the participating systems with a time expression reasoner (TREFL) as a post-processing step (Llorens et al., 2015). The TREFL component adds TLINKs between timexes based on their resolved values. Note that TempRelPro already includes the T-T links in the final TimeML documents produced, based on the output of the rule-based sieve for T-T pairs (Section 4.5.2.1). In Table 4.9 we report the performance of TempRelPro compared with participating systems in QA TempEval, augmented with TREFL, as reported in Llorens et al. (2015). A comparison with off-the-shelf systems not optimized for the task, i.e., CAEVO (Chambers et al., 2014), which is the same system reported in Section 4.6.2, and TIPSemB and\n14 Whenever two events co-refer, the E-E pair is excluded from the classifier and automatically labelled SIMULTANEOUS. 15 More information about the NewsReader pipeline, as well as a demo, are available on the project website http://www.newsreader-project.eu/results/.\n56 temporal relation extraction\nTIPSem (Llorens et al., 2010), was also provided. TempRelPro + coref achieves the best performance with 35% recall and 46% F1-score.\n4.7 conclusions\nOur decision to focus on temporal relation extraction is driven by the low performance of state-of-the-art systems in the TempEval-3 evaluation campaign for this particular task (36.26% F1-score), compared with system performances for the temporal entity extraction tasks (>80% F1-score). We have described our approach in building an improved temporal relation extraction system, TempRelPro, which is inspired by a sieve-based architecture for temporal ordering introduced by Chambers et al. (2014) with their system, CAEVO. However, our approach is different from CAEVO by adopting simpler architecture, considering all rule-based classifiers as one sieve and all machine-learned classifiers as another one. Hence, we run our temporal reasoner module only once, in between the two sieves we have. Moreover, we also introduced a novel method to include the rule-based sieve output, particularly the labels of timex-DCT and event-DCT links, as features for the supervised event-timex and event-event classifiers.\nWe have evaluated TempRelPro using the TempEval-3 evaluation scheme, which results in a significant improvement of 40.15% F1-score, compared to the best performing system in TempEval-3, ClearTK-2, with 36.26% F1-score. Unfortunately, building and evaluating an automatic temporal relation extraction system is not trivial, given the sparse annotated temporal relations as in TempEval-3. Without a completely labelled graph of temporal entities, we cannot speak of true extraction, but rather of matching human annotation decisions that were constrained by time and effort. This is shown by the low precision achieved by TempRelPro, since it extracts many TLINKs of which the real labels are unknown. Therefore, we also evaluated TempRelPro following the TimeBank-Dense evaluation methodology (Chambers et al., 2014) and QA TempEval (Llorens et al., 2010). In general, TempRelPro performs best in both evaluation methodologies.\nAccording to the TempEval-3 and TimeBank-Dense evaluation schemes, componentwise, combining rule-based and machine-learned sieves (RB + ML) results in a slight improvement. However, introducing the temporal reasoner module in between the sieves (RB + TR + ML) is quite beneficial, especially for E-T pairs.\nIf we look into each type of temporal entity pairs, TempRelPro still performs poorly with E-E pairs. On TempEval-3 evaluation, TempRelPro performances when labelling T-T, E-D and E-T pairs are already above 70% F1-scores, but only around 50% F1-score for E-E pairs. On TimeBank-Dense evaluation, its performance for E-E pairs is still <50% F1score. Our efforts in improving the performance of the supervised classifier for E-E pairs, i.e., combining it with a rule-based classifier, introducing a temporal reasoner module, or including event-DCT labels as features, do not result in a better outcome.\nThere are several directions that we look into regarding this issue. The first one is by building a causal relation extraction system, because there is a temporal constraint in causal relations, so that the causing event always happens BEFORE the resulting event. In the following chapters we will discuss the interaction between these two types of relations, and whether extracting causal relations can help in improving the output of a temporal relation extraction system, especially for pairs of events (Chapter 7).\nThe other direction would be to exploit the lexical semantic information about the event words in building a supervised classifier for E-E pairs, using word embeddings and deep\n4.7 conclusions 57\nlearning techniques (Chapter 8). Currently, the only lexical semantic information used by the event-event SVM classifier in TempRelPro is WordNet similarity measure between pairs of words.\n5 A N N O TAT I N G C A U S A L I T Y B E T W E E N E V E N T S\n5.1 Introduction 59 5.2 Related Work 60 5.3 Related Publications 61 5.4 TimeML-based Causality Annotation Guidelines 61 5.5 Causal-TimeBank 65\n5.5.1 Corpus Statistics 65 5.6 Conclusions 67\nIn the previous chapter we have described our efforts in building an improved temporal relation extraction system, TempRelPro (Section 4.5). However, according to the TempRelPro evaluations (Section 4.6), TempRelPro still performs poorly in labelling the temporal relation types of event-event pairs. One direction to address this issue is to build a causal relation extraction system, since there is a temporal constraint in causality, that the cause happens BEFORE the effect. The system would then be used to support temporal relation type classification between events.\nUnfortunately, unlike for temporal relations, there is no available corpus yet to be used for building (and evaluating) a causal relation extraction system for event-event pairs. Moreover, while there is a wide consensus in the NLP community over the modelling of temporal relations between events, mainly based on Allen’s temporal logic, the question on how to annotate other types of event relations, in particular causal relation, is still open.\nIn this chapter, we present some annotation guidelines to capture explicit causality between event-event pairs, partly inspired by the TimeML annotation standard (Section 3.2). Based on the guidelines, we manually annotated causality in the TimeBank corpus (Section 3.3) taken from the TempEval-3 evaluation campaign (Section 3.4). We chose this corpus because gold annotated events were already present, between which we could add causal links. Finally, we report some statistics from the resulting causality-annotated corpus, Causal-TimeBank, on the behaviour of causal cues in a text.\n5.1 introduction\nWhile there is a wide consensus in the NLP community over the modeling of temporal relations between events, mainly based on Allen’s interval algebra (Allen, 1983), the question on how to model other types of event relations is still open. In particular, linguistic annotation of causal relations, which have been widely investigated from a philosophical and logical point of view, are still under debate. This leads, in turn, to the lack of a standard benchmark to evaluate causal relation extraction systems, making it difficult to compare systems performances, and to identify the state-of-the-art approach for this particular task.\nAlthough several resources exist in which causality has been annotated, they cover only few aspects of causality and do not model it in a global way, comparable to what\n59\n60 annotating causality between events\nhas been proposed for temporal relations in TimeML. See for instance the annotation of causal arguments in PropBank (Bonial et al., 2010b) and of causal discourse relations in the Penn Discourse Treebank (Prasad et al., 2008).\nIn Section 5.4, we propose annotation guidelines for explicit construction of causality inspired by TimeML, trying to take advantage of the clear definition of events, signals and relations proposed by Pustejovsky et al. (2003). This is the first step towards the annotation of a TimeML corpus with causality.\nWe annotated TimeBank, a freely available corpus, with the aim of making it available to the research community for further evaluations. Our annotation effort results in Causal-TimeBank, a TimeML corpus annotated with both temporal and causal information (Section 5.5). We chose TimeBank because it already contains gold annotated temporal information, including temporal entities (events and temporal expressions) and temporal relations. The other reason is because we want to investigate the strict connection between temporal and causal relations. In fact, there is a temporal constraint in causality, i.e. the cause must occur BEFORE the effect. We believe that investigating this precondition on a corpus basis can contribute to improving the performance of temporal and causal relation extraction systems.\n5.2 related work\nUnlike the temporal order that has a clear definition, there is no consensus in the NLP community on how to define causality. Causality is not a linguistic notion, meaning that although language can be used to express causality, causality exists as a psychological tool for understanding the world independently of language (Koot and Neeleman, 2012). In the psychology field, several models have been proposed to model causality, including the counterfactual model (Lewis, 1973), probabilistic contrast model (Cheng and Novick, 1991; Cheng and Novick, 1992) and the dynamics model (Wolff, 2007; Wolff and Song, 2003; Wolff et al., 2005), which is based on Talmy’s force dynamic account of causality (Talmy, 1985, 1988).\nSeveral attempts have been made to annotate causal relations in texts. A common approach is to look for specific cue phrases like because or since or to look for verbs that contain a cause as part of their meaning, such as break (cause to be broken) or kill (cause to die) (Girju et al., 2007; Khoo et al., 2000; Sakaji et al., 2008). In PropBank (Bonial et al., 2010b), causal relations are annotated in the form of predicate-argument relations, where argmcau is used to annotate ‘the reason for an action’, for example: “They [predicate moved] to London [argm-cau because of the baby].”\nAnother scheme annotates causal relations between discourse arguments, in the framework of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). As opposed to PropBank, this kind of relations holds only between clauses, and does not involve predicates and their arguments. In PDTB, the Cause relation type is classified as a subtype of Contingency.\nCausal relations have also been annotated as relations between events in a restricted set of linguistic constructions (Bethard et al., 2008), between clauses in text from novels (Grivaz, 2010), or in noun-noun compounds (Girju et al., 2007).\nSeveral types of annotation guidelines for causal relations have been presented, with varying degrees of reliability. One of the simpler approaches asks annotators to check whether the sentence containing event pairs conjoined by ‘and’ can be paraphrased using\n5.3 related publications 61\na connective phrase such as ‘and as a result’ or ‘and as a consequence’ (Bethard et al., 2008). For example, “Fuel tanks had leaked and contaminated the soil.” could be rephrased as “Fuel tanks had leaked and as a result contaminated the soil.” This approach is relatively simple for annotators, but agreement is only moderate (kappa of 0.556), in part because there are both causal and non-causal readings of such connective phrases. Another approach to annotate causal relations tries to combine linguistic tests with semantic reasoning tests. In the work of Grivaz (2010), the linguistic paraphrasing suggested by Bethard et al. (2008) is augmented with rules that take into account other semantic constraints, for instance if the potential cause occurs before or after the potential effect.\nDo et al. (2011) developed an evaluation corpus by collecting 20 news articles from CNN, allowing the detection of causality between verb-verb, verb-noun, and noun-noun triggered event pairs. The most recent work of Riaz and Girju (2013) focuses on the identification of causal relations between verbal events. They rely on the unambiguous discourse markers because and but to automatically collect training instances of cause and non-cause event pairs, respectively. The result is a knowledge base of causal associations of verbs, which contains three classes of verb pairs: strongly causal, ambiguous and strongly non-causal.\n5.3 related publications\nIn Mirza et al. (2014), we have presented the annotation guidelines to capture causality between event pairs, inspired by TimeML. Based on the automatic annotation performed, we also reported some statistics on the behavior of causal cues in text and perform a preliminary investigation on the interaction between causal and temporal relations.\n5.4 timeml-based causality annotation guidelines\nAs part of a wider annotation effort aimed to annotate texts at the semantic level (Tonelli et al., 2014), within the NewsReader project1, we propose guidelines for the annotation of causal information. In particular, we define causal relations between events based on the TimeML definition of events (Pustejovsky et al., 2010), as including all types of actions (punctual and durative) and states. Syntactically, events can be realized by a wide range of linguistic expressions such as verbs, nouns (which can realize eventualities in different ways, for example through a nominalization process of a verb or by possessing an eventive meaning), adjectives and prepositional constructions.\nFollowing TimeML, our annotation of causal relations is realized with a LINK tag, i.e., CLINK, parallel with the TLINK tag in TimeML for temporal relations. The annotation of CLINK also includes the csignalID attribute, which refers to the ID of the causal signal, realized with a CSIGNAL tag marking a cue for an explicit causal relation.\nFor the sake of clarity, henceforth, snippets of text annotated with events and causal markers (either causative verbs or causal signals) serving as examples will be in the respective forms. For example, “Tsunami caused an financial crisis” or “The financial crisis deepened due to the tsunami.”\n1 http://www.newsreader-project.eu/\n62 annotating causality between events\ncsignal Parallel to the SIGNAL tag in TimeML, which marks a cue for an explicit temporal relations, we introduce the notion of causal signals through the CSIGNAL tag to mark-up textual elements indicating the presence of a causal relations. Such elements include all causal uses of:\n• prepositions, e.g. because of, on account of, as a result of, in response to, due to, from, by;\n• conjunctions, e.g. because, since, so that, as;\n• adverbial connectors, e.g. as a result, so, therefore, thus, hence, thereby, consequently; and\n• clause-integrated expressions, e.g. the result is, the reason why, that’s why.\nThe extent of CSIGNALs corresponds to the whole expression, so multi-token extensions are allowed. The only attribute for the CSIGNAL tag is id, corresponding to a unique ID number.\nclink The CLINK tag is a directional one-to-one relation where the causing event is the source (the first argument, indicated with s in the examples) and the caused event is the target (the second argument, indicated with t).\nA seminal research in cognitive psychology based on the force dynamics theory (Talmy, 1988) has shown that causation covers three main kinds of causal concepts (Wolff, 2007), which are CAUSE, ENABLE, and PREVENT, and that these causal concepts can be lexicalized as verbs (Wolff and Song, 2003):\n• CAUSE-type verbs – bribe, cause, compel, convince, drive, have, impel, incite, induce, influence, inspire, lead, move, persuade, prompt, push, force, get, make, rouse, send, set, spur, start, stimulate;\n• ENABLE-type verbs – aid, allow, enable, help, leave, let, permit;\n• PREVENT-type verbs – bar, block, constrain, deter, discourage, dissuade, hamper, hinder, hold, impede, keep, prevent, protect, restrain, restrict, save, stop.\nCAUSE, ENABLE, and PREVENT categories of causation and the corresponding verbs are taken into account in our guidelines.\nAs causal relations are often not overtly expressed in text (Wolff et al., 2005), we restrict the annotation of CLINKs to the presence of an explicit causal construction linking two events in the same sentence2, as detailed below:\n• Basic constructions for CAUSE, ENABLE and PREVENT categories of causation as shown in the following examples: The purchase s caused the creation t of the current building. The purchase s enabled the diversification t of their business. The purchase s prevented a future transfer t.\n• Expressions containing affect verbs, such as affect, influence, determine, and change. They can be usually rephrased using cause, enable, or prevent: Ogun ACN crisis s affects the launch t of the All Progressives Congress. → Ogun ACN crisis s causes/enables/prevents the launch t of the All Progressives Congress.\n2 A typical example of implicit causal construction is represented by lexical causatives; for example, kill has the embedded meaning of causing someone to die (Huang, 2012). In the present guidelines, these cases are not included.\n5.4 timeml-based causality annotation guidelines 63\n• Expressions containing link verbs, such as link (to/with), lead (to), and depend on. They can usually be replaced only with cause and enable: An earthquake t in North America was linked to a tsunami s in Japan. → An earthquake t in North America was caused/enabled by a tsunami s in Japan. *An earthquake t in North America was prevented by a tsunami s in Japan.\n• Periphrastic causatives are generally composed of a verb that takes an embedded clause or predicate as a complement; for example, in the sentence “The blast s caused the boat to heel t violently,” the verb (i.e. caused) expresses the notion of CAUSE while the embedded verb (i.e. heel) expresses a particular result. Note that the notion of CAUSE can be expressed by verbs belonging to the three categories previously mentioned (which are CAUSE-type verbs, ENABLE-type verbs and PREVENT-type verbs), and the same sets of verbs are taken into consideration.\n• Expressions containing causative conjunctions and prepositions as previously listed in the CSIGNAL section. Causative conjunctions and prepositions are annotated as CSIGNALs and their ID is to be reported in the csignalID attribute of the CLINK.3\nIn some contexts, the coordinating conjunction and, or the temporal conjunctions since and as, can also imply causation. We decided to annotate these ambiguous conjunctions, given the causation context, as CAUSALs. However, even though the temporal conjunctions after and when can also implicitly assert a causal relation, they should not be annotated as CSIGNALs and no CLINKs are to be created (temporal relations have to be created instead).\nThe recognition of ENABLE-type causal relations is not always straightforward. The suggestion is to try rephrasing the sentence using the cause verb:\n(i) The board authorized the purchase of the stocks.\n(ii) The authorization of the board caused the stocks to be purchased.\nThe verb authorize proves to be an ENABLE-type verb. In sentence (i), a CLINK is established between authorize and purchase, while in sentence (ii), a CLINK is annotated between authorization and purchased.\nThe attributes of the CLINK tag include:\n• id – unique ID number.\n• source – unique ID of the annotated EVENT involved in the causal link as the causing event.\n• target – unique ID of the annotated EVENT involved in the causal link as the resulting event.\n• csignalID – (optional) the ID of CSIGNAL explicitly marking the causal link.\nexample Consider our previous example of text excerpt annotated with temporal entities and temporal relations in TimeML annotation standard (Figure 3.1), in Figure 5.1 we present the same text also annotated with causal signals and causal relations following the previously explained annotation guidelines.\n3 The absence of a value for the csignalID attribute means that the causal relation is encoded by a causative verb.\n64 annotating causality between events\n5.5 causal-timebank 65\n5.5 causal-timebank\nBased on the guidelines detailed in Section 5.4, we manually annotated causality in the TimeBank corpus (Section 3.3) taken from TempEval-3 (Section 3.4), containing 183 documents with 6,811 annotated events in total. We chose this corpus because gold events were already present, between which we could add causal links. Besides, one of our research goals is the analysis of the interaction between temporal and causal information, and TimeBank already presents full manual annotation of temporal information according to the TimeML standard. The resulting corpus, Causal-TimeBank, is made available to the research community for further evaluations.4\nHowever, during the annotation process, we noticed that some events involved in causal relations were not annotated, probably because the corpus was originally built focusing on events involved in temporal relations. Therefore, we annotated also 137 new events, which led to around 56% increase in the number of annotated CLINKs.\nAnnotation was performed using the CAT tool (Bartalesi Lenzi et al., 2012), a web-based application with a plugin to import annotated data in TimeML and add information on top of it. The agreement reached by two annotators on a subset of 5 documents is 0.844 Dice’s coefficient on CSIGNALs (micro-average over markables) and 0.73 on CLINKs.\n5.5.1 Corpus Statistics\nIn the Causal-TimeBank corpus, the total number of annotated CSIGNALs is 171 and there are 318 CLINKs, much less than the number of TLINKs—particularly of event-event pairs— found in the corpus, which is 2,519. Besides, not all documents contain causality relations between events. From the total number of documents in TimeBank, only 109 (around 60%) of them contain explicit causal links and only 87 (around 47%) of them contain CSIGNALs.\nIn Table 5.1 we report the statistics of causal signals and causative verbs found in the corpus, along with the corresponding numbers of CLINKs associated with them.\ncausal signals There are 180 CLINKs explicitly cued by causal signals. Note that only 169 CLINKs are actually annotated with the csignalID attribute referring to the annotated CSIGNALs, the rest are missed by the annotators. Several CLINKs are annotated based on the presence of causal markers, but the markers are missed to be annotated as CSIGNALs, i.e., as factors in, responsible for. Some CSIGNALs may correspond to several CLINKs as always the case when the events are in coordinating clauses, e.g. “The company said its shipments declined t as a result of a reduction s in inventories by service centers and increasing s competitive pressures in the construction market.”\nSeveral causal markers can be perceived as variations from basic ones, e.g., due mostly to, thanks in part to, the combined effect of, the main reason for, that, he said, is why, or is a principal reason, mostly by an insertion of adjectives or adverbs. These variations need to be taken into account in building an automatic extraction system for causal signals, or causal relations.\nSome prepositions (e.g., by, from, for, with), conjunctions (e.g., and, as, since) and adverbs (e.g., so) are highly ambiguous (indicated with italic in Table 5.1). They are very abundant in the corpus, but only a few of them that can be regarded as causal signals, depending on the context.\n4 http://hlt.fbk.eu/technologies/causal-timebank\n66 annotating causality between events\ncausative verbs There are 115 CLINKs resulting from the causative verb constructions. Some verbs of CAUSE, ENABLE and PREVENT types do not have to be involved in a periphrastic construction to cue a causal relation, since they already own a strong causation sense, e.g., cause, provoke, trigger, ensure, guarantee, avoid, prevent. Some others occur more often in a text, but they must be in a periphrastic construction to carry a causation meaning, e.g., make, send, allow, help, keep, protect (indicated with italic in Table 5.1).\n5.6 conclusions 67\nFurthermore, even though these verbs are involved in a periphrastic construction, if their subjects are not events, they cannot be considered as causal markers.\nMost of the verbs classified as link verbs need to be followed by a specific prepositions in order to carry a causal meaning, e.g., lead (to), link (to/with), result (from/in), stem (from). The verbs follow, as in “A steep rise t in world oil prices followed the Kuwait invasion s”, do not always assert a causal relation, since it could be that only temporal order is realized by this verb. The (transitive) verb reflect is highly ambiguous, because it could mean5:\n(i) to make manifest or apparent, e.g., “The third-quarter net income fell t 22 %, reflecting the damages s from Hurricane Hugo.”\n(ii) to bring or cast as a result, e.g., “The success s of the project reflected great credit t on all the staff.” (iii) to give back or exhibit as an image, likeness, or outline, e.g., “The water reflects the color of the sky.” which shows that it carries different senses of causal direction in (i) and (ii), depending on its semantic, and carries no causation meaning at all in (iii).\nThe rest of around 23 CLINKs, which are not involved in a construction with causal signals or causative verbs, are related to some adjectives or verbs that can be perceived to carry a causal meaning depending on the context. For example, “The downturn t all across Asia means that people are not spending s here” or “The Bush administration considers the sanctions s essential to keeping t Saddam Hussein under control.”\n5.6 conclusions\nWe have presented our guidelines for annotating causality between events (Section 5.4), strongly inspired by TimeML. In fact, we inherit the concept of events, event relations and signals.\nWe manually annotated TimeBank, a freely available corpus, with causality information according to the presented annotation guidelines, with the aim of making it available to the research community for further evaluations, and supporting supervised learning for automatically extracting causal relations between events.\nDuring the annotation process, we realized that some events that are actually involved in causal relations were not annotated, probably because TimeBank was created focusing only on temporal relations. By annotating additional events, we get around 56% more CLINKs compared with only using original TimeBank’s events.\nThe resulting causality corpus, Causal-TimeBank (Section 5.5), contains 171 CSIGNAL and 318 CLINK annotations, so much less compared with 2,519 TLINKs, particularly of event-event (E-E) pairs, found in the corpus. This shows that causal relations, particularly the explicit ones, appear very rarely in a text.\nFrom the statistics presented in Section 5.5.1, we can observe some ambiguous causal markers, either causal signals or causative verbs, which occur abundantly in a text but do not always carry a causation sense. Some causative verbs can be easily disambiguated based on the construction they appear at, i.e., periphrastic construction, or when both subject and object of such verbs are events.\nOn the other hand, it is not trivial to disambiguate causal signals such as from, and or since. Bethard et al. (2008) attempted to disambiguate the conjunction and by asking the annotators to try paraphrasing it with and as a result or and as a consequence. This approach\n5 Merriam-Webster.com http://www.merriam-webster.com (4 March 2016)\n68 annotating causality between events\nis relatively simple for annotators, but the agreement is only moderate (kappa of 0.556), showing that it is a difficult task.\nHaving an annotated data for causality between events, the next step would be to exploit the corpus to build (and evaluate) an automatic extraction system for causal relations. The statistics obtained regarding the behaviour of causal markers in a text would definitely help us in building our system.\n6 C A U S A L R E L AT I O N E X T R A C T I O N\nI would rather discover a single causal connection than win the throne of Persia. — Democritus\n6.1 Introduction 69\n6.2 Related Work 70\n6.3 Related Publications 71\n6.4 Formal Task Definition 71\n6.5 Method 71\n6.5.1 Candidate Event Pairs 72\n6.5.2 Causal Verb Rules 72\n6.5.3 CLINK Classifier 74\n6.6 Evaluation 76\n6.7 Conclusions 78\nAs explained in Chapter 4, one direction to address the low performance of the eventevent pair classifier for temporal relations is to build a causal relation extraction system, since there is a temporal constraint in causality, that the cause happens BEFORE the effect.\nApart from the efforts to improve the temporal relation extraction system, the recognition of causal relations holding between events in a text is crucial to reconstruct the causal chain of events in a story. This could be exploited in question answering, decision support systems and even predicting future events given a chain of past events.\nIn this chapter we describe an approach for building an automatic system for identifying causal links between events, making use of the annotated causality corpus CausalTimeBank, resulted from our annotation effort for causality between events (Chapter 5). We also use the same corpus to evaluate the performance of the developed causal relation extraction system.\n6.1 introduction\nAn important part of text understanding arises from understanding if and how two events are related semantically. For instance, when given a sentence “The police arrested him because he killed someone,” humans understand that there are two events, triggered by the words arrested and killed, and that there is a causality relationship between these two events.\nBesides being an important component of discourse understanding, automatically identifying causal relations between events is important for various NLP applications such as question answering, decision support systems or predicting future events given a chain of past events.\nIn this chapter, we take advantage of the developed causality corpus, Causal-TimeBank (Section 5.5), to build an automatic extraction system for identifying causal links between events in a text (Section 6.5), and to perform an evaluation on the system’s performance\n69\n70 causal relation extraction\n(Section 6.6). Causal-TimeBank is the TimeBank corpus, containing the annotation of temporal entities and temporal relations, annotated with the formalisation of causality information inspired by TimeML (Section 5.4). This causality corpus provides annotations of a range of expressions for explicit causality which was never considered before, and hence, gives us a broader view over causal relations between events found in a text.\nSince the annotated corpus contains only causal links that are explicitly cued by either causal signals or causal verbs, we focus our effort on extracting causal links that are overtly expressed in the text. The approaches for building the causal relation extraction system is heavily influenced by the hybrid approach in a sieve-based architecture that proved to be beneficial for the temporal relation extraction task (Section 4.5). We combine the rule-base methods presented in Mirza et al. (2014) with the statistical-based methods presented in Mirza and Tonelli (2014a) in a similar fashion as for temporal relation extraction. Moreover, we tried to address one of the issues explained in Mirza and Tonelli (2014a) related to the dependency parser errors, by using another parser which has a better coverage for longrange dependencies.\n6.2 related work\nThe problem of detecting causality between events is as challenging as recognizing their temporal order, but less analyzed from an NLP perspective. Besides, it has mostly focused on specific types of event pairs and causal expressions in text, and has failed to provide a global account of causal phenomena that can be captured with NLP techniques. SemEval2007 Task 4 Classification of Semantic Relations between Nominals (Girju et al., 2007) gives access to a corpus containing nominal causal relations among others, as causality is one of the considered semantic relations in the task.\nBethard et al. (2008) collected 1,000 conjoined event pairs connected by and from the Wall Street Journal corpus. The event pairs were annotated manually with both temporal (before, after, no-rel) and causal relations (cause, no-rel). They use 697 event pairs to train a classification model for causal relations, and use the rest for evaluating the system, which results in 37.4% F-score. Rink et al. (2010) perform textual graph classification using the same corpus, and make use of manually annotated temporal relation types as a feature to build a classification model for causal relations between events. This results in 57.9% F-score, 15% improvement in performance compared with the system without the additional feature of temporal relations.\nDo et al. (2011) developed an evaluation corpus by collecting 20 news articles from CNN, allowing the detection of causality between verb-verb, verb-noun, and noun-noun triggered event pairs. Causality between event pairs is measured by taking into account Pointwise Mutual Information (PMI) between the cause and the effect. They also incorporate discourse information, specifically the connective types extracted from the Penn Discourse TreeBank (PDTB), and achieve a performance of 46.9% F-score. Ittoo and Bouma (2011) presented a minimally-supervised algorithm that extracts explicit and implicit causal relations based on syntactic-structure-based causal patterns.\nThe most recent work of Riaz and Girju (2013) focuses on the identification of causal relations between verbal events. They rely on the unambiguous discourse markers because and but to automatically collect training instances of cause and non-cause event pairs, respectively. The result is a knowledge base of causal associations of verbs, which contains three classes of verb pairs: strongly causal, ambiguous and strongly non-causal.\n6.3 related publications 71\n6.3 related publications\nIn Mirza et al. (2014) we presented a simple rule-based system to extract (explicit) event causality from a text. The rule-based system relies on an algorithm that, given a term t belonging to affect, link, causative verbs or causal signals (as listed in the annotation guidelines presented in Section 5.4), looks for specific dependency constructions where t is connected to the two observed events. If such dependencies are found, a CLINK is automatically set between the two events.\nIn Mirza and Tonelli (2014a) we presented a data-driven approach to extract causal relations between events, making use of the Causal-TimeBank corpus (Section 5.5). The system is a pipeline of two classifiers: (i) CSIGNAL labeller and (ii) CLINK classifier.\n6.4 formal task definition\nWe can formulate the task of recognizing event causality in a text as a classification task. Given an ordered candidate event pair (e1, e2) the classifier has to decide whether there is a causal relation between them or not. However, since we also consider the direction of the causal link, i.e. identifying source and target, an event pair (e1, e2) is classified into 3 classes: (i) CLINK, where e1 is the source and e2 is the target, meaning e1 cause e2; (ii) CLINK-R, with the reverse order of source and target (e2 and e1, resp.), meaning e1 is_caused_by e2; and (iii) NO-REL, for when there is no causal relation.\nexample Consider the following excerpt taken from the TimeBank corpus, annotated with events:\nDCT=1989-10-30 t0 Other market-maker gripes: Program trading E11 also causes E12 the Nasdaq Composite Index to lose E13 ground against other segments of the stock market. Peter DaPuzzo, head of retail equity trading at Shearson Lehman Hutton, acknowledges E14 that he wasn’t troubled E15 by program trading E16 when it began E17 in the pre-crash bull market because it added E18 liquidity and people were pleased E19 to see E20 stock prices rising E21 .\nThe causal relation extraction system should be able to identify: [E11 CLINK E13], [E15 CLINK-R E18] and [E15 CLINK-R E19].\n6.5 method\nFollowing the success of a hybrid approach in a sieve-based architecture for the temporal relation extraction task, we decided to combine in a similar way the rule-based methods in Mirza et al. (2014) with the statistical methods in Mirza and Tonelli (2014a), for identifying explicit causal links (CLINKs) in a text. We can define two main problems included in this task:\n(i) Recognizing CLINKs established by affect, link and causative verbs (CAUSE-, ENABLE- and PREVENT-type verbs), hereinafter simply addressed as causal verbs; and\n(ii) Recognizing CLINKs marked by causal signals.\n72 causal relation extraction\nThe causal constructions containing causal verbs are quite straightforward: assuming verb v belongs to such verbs, the first event must be the subject of v and the second event must be either the object or the predicative complement of v. Considering that such relations between the events and causal verbs are usually embedded in their dependency paths, we can easily approach problem (i) with a rule-based method.\nMeanwhile, some causal signals can be ambiguous, and the dependency paths between causal signals and events can be varied. Moreover, the position of causal signals with respect to the two events is crucial to determine the causal direction, e.g., “The building collapsed t because of the earthquake s” vs “Because of the earthquake s the building collapsed t”. Using the available Causal-TimeBank corpus, we believe that for problem (ii) a classification model can be learned to discover the regularities in which event pairs, connected by causal signals, are identified as having causal links.\nWe propose a hybrid approach for causal relation extraction, as illustrated in Figure 4.1. Our system, CauseRelPro, is a combination of rule-based and supervised classification modules, in a sieve-based architecture.\n6.5.1 Candidate Event Pairs\nGiven a document already annotated with events, we take into account every possible combination of events in a sentence in a forward manner as candidate event pairs. For example, if we have a sentence “e1, triggered by e2, cause them to e3,” the candidate event pairs are (e1,e2), (e1,e3) and (e2,e3). We also include as candidate event pairs the combination of each event in a sentence with events in the following one. This is necessary to account for inter-sentential causality, under the simplifying assumption that causality may occur only between events in two consecutive sentences.\n6.5.2 Causal Verb Rules\ncausal verb list We take lists of affect, link and causative verbs presented in the annotation guidelines (Section 5.4) as the causal verb list. We further expand the list,\nwhich contains 56 verbs in total, using the Paraphrase Database (Ganitkevitch et al., 2013) and original verbs as seeds, resulting in a total of 97 verbs.\nLooking at the statistics of causal verbs in Causal-TimeBank (Section 5.5.1), we can identify ambiguous CAUSAL-, ENABLE- and PREVENT-type verbs, e.g., make, send, allow, help, keep, protect, which must be in a periphrastic construction in order to carry a causation meaning. We make a separate list for such verbs, distinguishing them from CAUSAL-, ENABLEand PREVENT-type verbs with a strong causation sense, e.g., cause, provoke, trigger, ensure, guarantee, avoid, prevent.\nMost of the link verbs need to be followed by a specific prepositions in order to carry a causal meaning. Moreover, result in and result from carry different sense of causal directions, i.e., the causing event is the subject of result in, but instead the object of result from. Therefore, such verb-preposition combinations are distinct items in the list, and each corresponds to the carried sense of causal direction. The verb follow and reflect are excluded from the list because they are highly ambiguous.\nIn the end, our causal verb list (Appendix A.2) contains 96 verbs belonging to 8 verbtypes, including: AFFECT, LINK, CAUSE, CAUSE-AMBIGUOUS, ENABLE, ENABLE-AMBIGUOUS, PREVENT and PREVENT-AMBIGUOUS. Different from all of the AFFECT, CAUSE, ENABLE and PREVENT verbs that have the normal causal direction (e1 CLINK e2), most of the LINK verbs have the inverse causal direction (e1 CLINK-R e2), such as link(ed)-with, stem-from, result-from, etc.\ncandidate event pair filtering We only consider candidate event pairs in which causal verbs v occur between the two events (e1, e2) in the text, e.g., “The blast e1 caused v the boat to heel e2 violently.”\nThe set of rules applied to the filtered candidate event pairs is based on (i) the category of the causal verb v, (ii) the possible existing dependency relations between v and e1 (dep1), and between v and e2 (dep2), as listed in Table 6.1, and (iii) the causal direction sense (dir) embedded in v:\n74 causal relation extraction\n• If v is an AFFECT verb:\n– If dep1 is one of all possible relations and dep2 = OBJ then [e1 CLINK e2]\n• If v is a LINK verb:\n– If dep1 is one of all possible relations and dep2 = OBJ/ADV-PMOD/DIR-PMOD/AMOD-PMOD and dir = CLINK then [e1 CLINK e2]\n– If dep1 is one of all possible relations and dep2 = OBJ/ADV-PMOD/DIR-PMOD/AMOD-PMOD and dir = CLINK-R then [e1 CLINK-R e2]\n• If v is a CAUSE, ENABLE or PREVENT verb\n– If dep1 is one of all possible relations and dep2 = OBJ/OPRD/OPRD-IM/ADV-PMOD then [e1 CLINK e2]\n– If dep1 is one of all possible relations and dep2 = LGS-PMOD then [e1 CLINK-R e2]\n• If v is a CAUSE-AMBIGUOUS, ENABLE-AMBIGUOUS or PREVENT-AMBIGUOUS verb\n– If dep1 is one of all possible relations and dep2 = OPRD/OPRD-IM/ADV-PMOD then [e1 CLINK e2]\n6.5.3 CLINK Classifier\nFor recognizing (and determining the causal direction of) CLINKs in a text, we built a classification model using LIBLINEAR (Fan et al., 2008) L2-regularized L2-loss linear SVM (dual), with default parameters, and one-vs-rest strategy for multi-class classification.\ntools and resources The same external tools and resources for building the classifiers for temporal relation extraction (Section 4.5.2.6) are used to extract features from each event pair, such as PoS tags, shallow phrase chunk, dependency path and WordNet (Lin) semantic similarity/relatedness.\nAdditionally, we take the list of causal signals from the annotation guidelines (Section 5.4) as the causal signal list. Again we expand the list using the Paraphrase Database (Ganitkevitch et al., 2013), resulting in a total of 200 signals. We also manually cluster some signals together, e.g. {therefore, thereby, hence, consequently}, as we did for temporal signals. Note that we exclude and, for and with from the list because they are highly ambiguous.\nFor some causal signals, instead of the signal text we put regular expression patterns in the list to cover possible variations of causal signals. For example,\ndue ([a-z]+\\\\s)?to\nfor {‘due to’, ‘due mainly to’, ‘due mostly to’, ...}; or\nth[ai][st] (, ([a-z]+\\\\s)+, )*[i’]s ([a-z]+\\\\s)*why\nfor {‘this is why’, ‘that ’s exactly why’, ‘that , he said , is why’, ...}. In the end, our causal signal list (Appendix A.2) contains 66 causal signals belonging to 19 clusters. Among the signals in the list, 45 are actually causal signal patterns covering more variety of causal signals.\n6.5 method 75\ncandidate event pair filtering We only take into account a candidate event pair (e1, e2) in which:\nfeature set The implemented features are listed in Table 6.2. All of the features for the event-event (E-E) classification model for temporal relation extraction (Table 4.2) are re-used, along with additional features related to causal signals. As for temporal relation\n76 causal relation extraction\nextraction (Section 4.5.2.6), we also simplified the possible values of causal signal features during the one-hot encoding:\n• causSignalTokens The clusterID of signal cluster, e.g., {therefore, thereby, hence, consequently}, is considered as a feature instead of the signal tokens.\n• causSignalDependency For each atomic label in a vector of syntactic dependency labels according to Surdeanu et al. (2008), if the signal dependency path contains the atomic label, the value in the feature vector is flipped to 1. Hence, PRP-PMOD and PMOD-PRP will have the same one-hot representations.\n6.6 evaluation\nWe evaluate CauseRelPro using the causality corpus, Causal-TimeBank (Section 5.5), in stratified 10-fold cross-validation. The stratified cross-validation scheme is chosen to account for the highly imbalanced dataset as illustrated in Table 6.3, i.e., from the total of 28,058 candidate event pairs, only 318 are under the CLINK/CLINK-R class while the rest are under the NO-REL class. With this scheme, the proportion of CLINK, CLINK-R and NO-REL instances are approximately the same in all 10 folds.\nWe report in Table 6.3 the micro-averaged performances of each module included in CauseRelPro. Out of the 94 pairs under the CLINK/CLINK-R class in the candidate pairs, the causal verb rules can correctly recognize 50 causal links, summing up into 26.39% F1-score.\nFor the CLINK classifier, we compare the performance of the module with and without the candidate event pair filtering rules (Section 6.5.3). Without the filtering rules, the proportion of positive (CLINK/CLINK-R) and negative (NO-REL) instances is around 1:103, hence the classifier’s very low recall of 7.86% and F1-score of 13.89%. If we enforce the filtering rules, the proportion of positive and negative instances is around 1:19, and therefore, the classifier performs much better with 21.39% F1-score.\n6.6 evaluation 77\nWe combine the causal verb rules and the CLINK classifier in a sieve-based architecture. All candidate event pairs that are not identified as having causal links (CLINK/CLINK-R) by the causal verb rules, are passed to the CLINK classifier. If we enforce the filtering rules of the CLINK classifier, we basically ignore 106 annotated causal links in the CausalTimeBank, and only try to recognize the rest of 212 causal links. Out of the 212 considered causal links, our system can correctly identifies 93 event pairs as having causal links.\nFinally, CauseRelPro, which is the combination of the causal verb rules and the CLINK classifier (with candidate event pair filtering rules), achieves 40.17% F1-score in recognizing causality between event pairs in the Causal-TimeBank corpus. We manually evaluated the false positives extracted by the causal verb rules, and we found that 7 out of 11 event pairs are actually having causal links but not annotated. If we consider the corrected numbers of false positives and true positives, CauseRelPro achieves 43.2% F1-score instead. Some false positive examples are reported below:\n(i) The white house yesterday disclosed that Kuwait’s ousted government has formally asked the U.S. to enforce s the total trade embargo the United Nations has imposed on Iraq, allowing the U.S. and other nations to immediately begin t stopping ships carrying Iraqi goods.\n(ii) The Oklahoma City energy and defense concern said it will record a $7.5 million reserve for its defense group, including a $4.7 million charge s related to problems under a fixed-price development contract and $2.8 million overhead costs t that won’t be reimbursed.\nSentences (i) and (ii) contains the causal links that are extracted by the causal verb rules, but not annotated (false positives). In sentence (i), despite of the long sentence and many possibilities for the source event, the dependency parser managed to pick the correct source event, and therefore, establish the correct causal link. Unfortunately, this causal link is missed by the annotators. Meanwhile, in sentence (ii), the dependency parser mistakenly identify cost as the coordinating noun of problems instead of charge, resulting in a wrong causal link.\n(iii) StatesWest Airlines, Phoenix, Ariz., said it withdrew t its offer to acquire Mesa Airlines because the Farmington, N.M., carrier didn’t respond s to its offer...\nMirza and Tonelli (2014a) stated that one of the contributing factors for the low performance is the dependency errors from the parser used, i.e. Stanford CoreNLP dependency parser. One of the reported mistakes is exemplified in sentence (iii), where the causal link is established between acquire and respond, instead of withdrew and respond. Using a different dependency parser, in our case namely the parser of the Mate tools, resolves this problem since the dependency parser correctly connects withdrew and respond, given the causal marker because.\nWe also compare in Table 6.3 the performance of CauseRelPro to the performances of Mirza et al. (2014) rule-based system (18.40% F1-score) and Mirza and Tonelli (2014a) data-driven system (33.88% F1-score), even though they are not directly comparable since the two latter systems used 5-fold cross-validation as the evaluation scheme.\n78 causal relation extraction\n6.7 conclusions\nWe have presented our approach to building an improved causal relation extraction system, CauseRelPro, which is inspired by the sieve-based architecture for the temporal relation extraction task (Section 4.5), which is proven to bring advantages. CauseRelPro is a combination of the rule-based methods presented in Mirza et al. (2014) and the statisticalbased methods presented in Mirza and Tonelli (2014a), with some modifications regarding the tools and resources used, including the dependency parser (Stanford CoreNLP vs Mate tools), the lists of causal signals and verbs (augmented with paraphrases from PPDB, and clustered) and the algorithm for the classifier (YamCha SVM vs LIBLINEAR SVM).\nWe have also evaluated CauseRelPro using the Causal-TimeBank corpus in stratified 10-fold cross-validation, resulting in 40.95% F1-score, much better than the previous two systems reported in Mirza et al. (2014) and Mirza and Tonelli (2014a), even though they are not directly comparable because 5-fold cross-validation was used instead. Furthermore, we manually evaluated the output of the causal verb rules, and found that among the 11 false positives, 7 causal links are actually correct. The wrongly extracted ones are due to the dependency parser errors. However, we found that compared with the previous dependency parser used, i.e. Stanford CoreNLP dependency parser, the dependency parser of Mate tools performs better in connecting the events involved in causal relations.\nAs has been explained in Chapter 5, we intentionally added causality annotation on the TimeBank corpus, which is layered with the annotation of temporal entities and temporal relations, because we want to investigate the strict connection between temporal and causal relations. In fact, there is a temporal constraint in causality, i.e. the cause must occur BEFORE the effect.\nIn the following chapter (Chapter 7) we will discuss the interaction between these two types of relations. Bethard and Martin (2008); Rink et al. (2010) showed that including temporal relation information in detecting causal links results in improved classification performance. We will investigate whether the same will hold for our causal relation extraction system, and whether extracting causal relations can help in improving the output of a temporal relation extraction system, especially for pairs of events.\n7 I N T E G R AT E D S Y S T E M F O R T E M P O R A L A N D C A U S A L R E L AT I O N S\nPost hoc, ergo propter hoc — After this, therefore, because of this.\n7.1 Introduction 79\n7.2 Related Work 80\n7.3 Related Publications 80\n7.4 Temporal and Causal Links in Causal-TimeBank 80\n7.5 Temporal and Causal Links as Features 81\n7.5.1 TLINKs for Causal Relation Extraction 81\n7.5.2 CLINKs for Temporal Relation Extraction 82\n7.6 Integrated System - CATENA 82\n7.7 Evaluation 83\n7.8 Conclusions 85\nGiven a resource annotated with temporal entities, i.e. events and temporal expressions, temporal and causal relations (Chapter 5), a temporal relation extraction system (Chapter 4) and a causal relation extraction system (Chapter 6), our next step is to build an integrated system for extracting both temporal and causal relations between events in texts. We start from the premises that there is a temporal constraint in causality, i.e., the causing event must happen BEFORE the resulting event, and that a system for extracting both temporal and causal relations may benefit from integrating this presumption.\nIn this chapter we first investigate the interaction between temporal and causal relations in the text, by looking at the constructed Causal-TimeBank corpus (Section 5.5). We also investigate the effects of using each type of relations as features for the supervised classifiers used to extract temporal and causal relations. Next, we propose a way to combine the temporal relation and causal relation extraction systems into one integrated system.\n7.1 introduction\nWe have seen in the previous chapters (Chapter 3, Chapter 4 and Chapter 6) the performances of automatic extraction systems for events and event relations, specifically temporal and causal relations. Now, given the temporal constraint of causality, we want to investigate the interaction between temporal and causal relations, which is made possible by the corpus annotated with both relations, Causal-TimeBank (Section 5.5). CausalTimeBank is the result of our causality annotation effort (Chapter 5) on TimeBank, a corpus widely used by the research community working on temporal information processing. Different from several previous works, we aim at providing a more comprehensive account of how causal relations can be explicitly expressed in a text, and we do not limit our analysis to specific connectives. Our investigation into the interaction between temporal and causal relations between events in a text is reported in Section 7.4.\nSeveral works related to temporal and causal relations have shown that temporal information is crucial in identifying causal relations (Bethard and Martin, 2008; Mirza and\n79\n80 integrated system for temporal and causal relations\nTonelli, 2014a; Rink et al., 2010). In this chapter we will investigate the effects of using temporal relation types as features for the causal relation extraction task, and vice versa, causal links as features for the temporal relation extraction task (Section 7.5). However, we expect that given the sparsity of explicit causal links, the reverse impact may not hold.\nEven though the explicit causal information may not be so beneficial for data-driven temporal relation extraction, we want to explore other options for exploiting causal information, namely post-editing rules to correct misclassified temporal relations using the output of the causal relation extraction system. In Section 7.6, we describe our proposed approach for the integrated system for temporal and causal relations. We perform an evaluation of the proposed approach in Section 7.7.\n7.2 related work\nGrivaz (2010) presented an experiment that elicits the intuitive features or tests of causation that are consciously used in causal reasoning. Temporal order is shown to be one of the features that helped to rule out non-causal occurrences.\nBethard et al. (2008) collected 1,000 conjoined event pairs connected by and from the Wall Street Journal corpus. The event pairs were annotated manually with both temporal (BEFORE, AFTER, NO-REL) and causal relations (CAUSAL, NO-REL). A corpus analysis is reported to show the ties between the temporal and causal annotations, which includes the fact that 32% of CAUSAL relations in the corpus did not have an underlying BEFORE relations. Bethard and Martin (2008) trained machine learning models using this corpus of parallel temporal and causal relations, achieving 49% F1-score for temporal relations and 52.4% F1-score for causal relations. The performance of the causal relation classifier is boosted by exploiting gold-standard temporal labels as features.\nRink et al. (2010) performed textual graph classification using the same corpus, and make use of manually annotated temporal relation types as a feature to build a classification model for causal relations between events. This results in 57.9% F-score, 15% improvement in performance compared with the system without the additional feature of temporal relations.\n7.3 related publications\nIn Mirza and Tonelli (2014a) we proposed a data-driven approach for extracting causality between events, using the manually annotated causality corpus, Causal-TimeBank. The evaluation and analysis of the system’s performance provides an insight into explicit causality in texts and the connection between temporal and causal relations.\nWe have submitted a research paper containing the description of our integrated temporal and causal relation extraction system (Mirza and Tonelli, 2016).\n7.4 temporal and causal links in causal-timebank\nWe provide in Table 7.1 some statistics on the overlaps between causal links and temporal relation types in the Causal-TimeBank corpus (Section 5.5). The Others class in the table includes SIMULTANEOUS, IS_INCLUDED, BEGUN_BY and DURING_INV relations. In total, only around 32% of 318 annotated causal links have an underlying temporal relation. Examples\n7.5 temporal and causal links as features 81\nof explicit event causality found in the text are few in comparison with the number of annotated temporal relations, particularly of event-event (E-E) pairs, i.e. 318 CLINKs vs 2,519 (E-E) TLINKs. This means that only around 4% of the total (E-E) TLINKs overlap with CLINKs. Note that the annotators could not see the temporal links already present in the data, therefore they were not biased by TLINKs when assessing causal links.\nWe also run the temporal reasoner module (Section 4.5.2.5) on the Causal-TimeBank corpus to enrich the temporal relation annotation. The number of (E-E) TLINKs greatly increases, around 4 times of the original number. However, the number of causal links overlapping with temporal relation annotation does not improve significantly, i.e. from 32.39% to 39.62%.\nThe data confirm our intuition that temporal information is a strong constraint in causal relations, with the BEFORE class having the most overlaps with CLINK and AFTER with CLINK-R. We found that the few cases where CLINKs overlap with AFTER relation are not due to annotation mistakes, as in the example “But some analysts questioned t how much of an impact the retirement package will have, because few jobs will end s up being eliminated.” This shows that the concept of causality is more abstract than the concept of temporal order of events in a text. Here, the causing event is not the future event ‘jobs will end up being eliminated’, but rather the knowledge that ‘jobs will end up being eliminated’. Annotating causality between events in a text is indeed a much more challenging task.\n7.5 temporal and causal links as features\nIn the following sections we will investigate the effects of using temporal relation types as features for the causal relation extraction task, and vice versa, causal links as features for the temporal relation extraction task.\n7.5.1 TLINKs for Causal Relation Extraction\nWe take the causal relation extraction system described in Section 6.5, CauseRelPro, for the experiment. The feature set for the CLINK classifier (Section 6.5.3) is augmented with temporal order information of the event pairs, which is taken from the gold annotated temporal relations. Specifically, for an event pair (e1, e2), if there exists a temporal link connecting e1 and e2, the TLINK type (e.g. BEFORE, AFTER, SIMULTANEOUS, etc.) is added to the feature set, NONE is added otherwise. Note that we use the enriched temporal relation annotation using the temporal reasoner module, i.e., the Causal-TimeBank + TR corpus in Table 7.1, since it provides more information about the underlying temporal relations.\nThe experiment is done following the evaluation methodology explained in Section 6.6, which is a stratified 10-fold cross-validation on the Causal-TimeBank corpus. The results\n82 integrated system for temporal and causal relations\nare reported in Table 7.2, with a quite significant improvement—particularly in recall—if TLINK types are included in the feature set, resulting in 45.89% F1-score. The reported results are based on the corrected number of false positives, since we found that 7 out of 11 event pairs are actually having causal links but not annotated (Section 6.6).\nThe significant improvement by adding TLINK types as features supports the previous finding by the related works, that temporal information can boost the performance in recognizing causality between events in a text.\n7.5.2 CLINKs for Temporal Relation Extraction\nFor the experiment, we take the supervised classification model for event-event (E-E) pairs included in TempRelPro described in Section 4.5. The feature set for the E-E classifier (Section 4.5.2.6) is augmented with causality information of the event pairs, which is taken from the gold annotated CLINKs in the Causal-TimeBank corpus. Specifically, for an event pair (e1, e2), if there exists a causal link connecting e1 and e2, the causal direction (i.e. CLINK or CLINK-R) is added to the feature set, NONE is added otherwise.\nWe evaluate the E-E classifier on the task of classifying the temporal relation types, with stratified 10-fold cross-validation on the Causal-TimeBank corpus, under the assumption that the temporal links (TLINKs) are already established between events. From the total of 2,519 event-event (E-E) pairs in the corpus, the E-E classifier augmented with CLINK information as features yields one less true positives than the classifier without CLINK. Note that since we classify all TLINKs—of event-event pairs—in the corpus, the precision and recall are the same. This result confirms our intuition, that while temporal order information can benefit the performance of causal relation extraction, the converse does not hold because the explicit causal links found in texts are very sparse.\n7.6 integrated system - catena\nCATENA—CAusal and Temporal relation Extraction from NAtural language texts—is an integrated system for extracting temporal and causal relations from texts. It includes two main extraction modules, TempRelPro (Section 4.5) and CauseRelPro (Section 6.5), for\n7.7 evaluation 83\ntemporal and causal relation extraction, respectively. As shown in Figure 7.1, both modules take as input a document annotated with document creation time (DCT), events and temporal expressions (so-called temporal entities) according to TimeML. The output is the same document with temporal and causal links set between pairs of temporal entities.\nThe modules for temporal and causal relation extraction rely both on a sieve-based architecture, in which the output of a rule-based component is fed into a supervised classifier. Although some steps can be run in parallel, the two modules interact, based on the assumption that the notion of causality is tightly connected with the temporal dimension and that information from one module can be used to improve or check the consistency of the other. In particular, (i) TLINK labels (e.g. BEFORE, AFTER, SIMULTANEOUS, etc.) for event-event (E-E) pairs, as a result of rule-based sieve + temporal reasoner modules in TempRelPro, are used as features for the CLINK classifier in CauseRelPro; and (ii) CLINK labels (i.e. CLINK and CLINK-R) are used as a post-editing method for correcting the wrong labelled event pairs by the E-E classifier in TempRelPro. The post-editing method relies on a set of rules based on the temporal constraint of causality, that the cause must precede the effect:\n• If an event pair (e1, e2) is found to have a causal link with normal direction (CLINK), then [e1 BEFORE e2]\n• If an event pair (e1, e2) is found to have a causal link with reverse direction (CLINK-R), then [e1 AFTER e2]\n7.7 evaluation\nThe purpose of the evaluation is twofold: (i) to evaluate the quality of individual modules for temporal and causal relations; and (ii) to investigate whether the post-editing method improves the performance of the temporal relation extraction system. We perform the same TempEval-3 Evaluation previously presented in Section 4.6.1 for evaluating temporal relation extraction."
    }, {
      "heading" : "Temporal Relation Type Classification",
      "text" : "dataset For TempRelPro, we use the same training data released by the organizers, i.e. the TBAQ-Cleaned corpus. Meanwhile, Causal-TimeBank is used as the training data for CauseRelPro. The evaluation data is the TempEval-3-platinum corpus used as the evaluation corpus in TempEval-3. We manually annotated the 20 evaluation documents with causal links following the annotation guidelines of Causal-TimeBank, described in Chapter 5. Causal relations are much sparser than temporal ones, and we found only 26 clinks. Note that we only annotated causal links where events involved in causality are also overtly annotated in the corpus, even though we found several cases where there is causality but involved events are not annotated, as what was found during the annotation of Causal-TimeBank.\nevaluation results In order to measure the contribution of each component to the overall performance of the system, we evaluate the performance of each sieve both in the temporal and in the causal module. Results are reported in Table 7.4. For temporal relation extraction, we have presented the evaluation in Section 4.6.1.\nFor causal relation extraction, the combination of rule-based and machine-learned sieves (RB + ML) achieves 0.62 F1-score in TempEval-3 evaluation, with the ML component contributing to increase the recall of the highly precise RB component.\n(i) “An Israeli raid s on the ship left nine passengers dead t, ...”\n(ii) “The FAA on Friday announced it will close t 149 regional airport control towers because of forced spending cuts s ...”\n(iii) “The incident s provoked an international outcry t and led to a major deterioration t in relations between Turkey and Israel.”\n(iv) “But the tie-up s with Rosneft will keep s BP in Russia, allowing it to continue t to explore and exploit ...”\n7.8 conclusions 85\nThe system can identify causal links marked by both causal verbs (i) and causal signals (ii). The dependency relations allow the system to recognize causality in coordinating clauses (iii). Moreover, the dependency paths also enable the extraction of a causal chain between tie-up, keep and continue (iv). Unfortunately, the dependencies are also the cause of mistakenly identified causal links, as in “The FAA on Friday announced it will close t 149 regional airport control towers because of forced spending cuts – sparing 40 others that the FAA had been expected s to shutter.” Other mistakes are related to the ambiguous causal signals that the classifier failed to disambiguate, such as from in “But with 20 female senators now in office t women have morphed from the curiosity s they were for much of the 20th century.”\nAs shown in Figure 7.1, E-E labels returned by the temporal reasoner are used by the CLINK classifier as features, whose causal relations are then used to post-edit TLINK labels.\nWe evaluate the impact of the first step through an ablation test, by removing TLINK types from the features used by the TLINK classifier. Without TLINK types, the F1-score drops from 0.62 to 0.57, with a significant recall drop from 0.54 to 0.46. This shows that temporal information is beneficial to the classification of causal relations between events, especially in terms of recall.\nAs for the evaluation of TLINK post-editing using TLINKs, the system identifies in the test set 19 causal links, 4 of which are passed to the temporal module (the others are already consistent with BEFORE/AFTER labels). We manually evaluated the 4 links: 3 of them would add new correct TLINKs that are currently not annotated in the evaluation corpus. The fourth would add a BEFORE label between cloaked and coughing in “A haze akin to volcanic fumes cloaked s the capital, causing convulsive coughing t ...”. This relation is labelled as INCLUDES in the gold standard, but we believe that BEFORE would be correct as well.\n7.8 conclusions\nFollowing the analysis of the connection between temporal and (explicit) causal relations in a text, we have presented our proposed approach for integrating our temporal and causal relation extraction systems. The integrated system, CATENA, is a combination of previously explained TempRelPro (Section 4.5) and CauseRelPro (Section 6.5), which takes an input document annotated with temporal entities, i.e. event and timexes, and produces the same document augmented with temporal and causal links.\nThe integrated system exploits the presumption about event precedence when two events are connected by causality, that the causing event must happen BEFORE the resulting event. With this presumption, the interaction between TempRelPro and CauseRelPro is manifested by (i) using the TLINK labels—resulted from the rule-based sieve + temporal reasoner modules of TempRelPro—as features for the CLINK classifier and (ii) using the output of the CLINK classifier as a post-editing method for correcting wrong temporal labels of event-event pairs.\nFrom the evaluation, we found that the causal relation extraction module achieves 62% F1-score, correctly identified 14 out of the 26 gold annnotated CLINKs in the evaluation corpus. The mistakes are mostly due to dependency parsing mistakes and issues in disambiguating signals such as from. Bethard et al. (2008) attempted to disambiguate the conjunction and by asking the annotators to try paraphrasing it with and as a result or and\n86 integrated system for temporal and causal relations\nas a consequence. This approach could be adopted for other ambiguous causal signals such as from, by, as and since.\nWe also found that the post-editing rules would improve the output of temporal relation labelling, but this phenomenon is not captured by the TempEval-3 evaluation, due to the sparse TLINK annotation in the evaluation corpus.\nNevertheless, explicit causality found in a text is very sparse, and hence, cannot contribute much in improving the performance of the temporal relation extraction system. Extracting implicit causality from texts and investigating the concept of causal-transitivity are two directions that can be explored, so that the amount of causality information will be significant enough to contribute more in boosting the TLINK labeller’s performance.\n8 W O R D E M B E D D I N G S F O R T E M P O R A L R E L AT I O N S\nYou shall know a word by the company it keeps. — Firth (1957)\n8.1 Introduction 87\n8.2 Related Work 88\n8.3 Experiments 88\n8.4 Evaluation 91\n8.5 Conclusions 92\nIt has been discussed in Chapter 4 that our improved temporal relation extraction system, TempRelPro (Section 4.5), performs poorly in labelling the temporal relation types of event-event pairs. While morpho-syntactic and context features are sufficient for classifying timex-timex and event-timex pairs, we believe that exploiting the lexical semantic information about the event words can benefit the supervised classifier for event-event pairs. Currently, the only lexical semantic information used by the event-event SVM classifier in TempRelPro is WordNet similarity measure between the pair of words.\nIn this chapter we explore the possibilities of using word embeddings as lexical semantic features of event words for temporal relation type classification between event pairs.1\n8.1 introduction\nThe identification of discourse relations between events, namely temporal and causal relations, is relatively straightforward when there is an explicit marker (e.g. before, because) connecting the two events. The tense, aspect and modality of event words, as well as specific syntactic constructions, could also play a big role in determining the temporal order of events. It becomes more challenging when such an overt indicator is lacking, which is often the case when two events take place in different sentences, as exemplified in (i), where the label for the event pair (e1, e2) is BEFORE. This type of event pairs can not be disregarded since for example, in the TempEval-3 evaluation corpus, 32.76% of the event pairs do not occur in the same sentences.\n(i) When Wong Kwan spent e1 seventy million dollars for this house, he thought it was a great deal. He sold e2 the property to five buyers and said he’d double his money.\nMost research on implicit relations, tracing back to Marcu and Echihabi (2002), incorporate word-based information in the form of word pair features. Such word pairs are often encoded in a one-hot representation, in which each possible word pair corresponds to a single component of a very high-dimensional vector. From a machine learning point of view, this type of sparse representation makes parameter estimation extremely difficult and prone to over-fitting. It is also very challenging to achieve any interesting semantic\n1 Joint work with Ilija Ilievski, Prof. Min-Yen Kan and Prof. Hwee Tou Ng, National University of Singapore (NUS).\n87\n88 word embeddings for temporal relations\ngeneralization with this representation. Consider for example, (attack, injured) that would be at equal distance from a synonymic pair (raid, hurt) and an antonymic pair (died, shooting).\nRecently there has been an increasing interest in using word embeddings as an alternative to traditional hand-crafted features. Word embeddings represent (embed) the semantic of a word in a continuous vector space, where semantically similar words are mapped to nearby points. The underlying principle is the Distributional Hypothesis (Harris, 1954), which states that words which are similar in meaning occur in similar contexts. Baroni et al. (2014) draws a distinction of approaches leveraging this principle into two categories: (i) count-based models and (ii) predictive models. GloVe (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013) are the two popular word embedding algorithms recently, each represents (i) and (ii), respectively. We have briefly explained the two algorithms in Section 2.2.5.\n8.2 related work\nMost works on implicit discourse relations focused on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), in which relations are annotated at the discourse level. There are five distinct groups of relations: implicit, explicit, alternative lexicalizations, entity relations and no relation; each could carry multiple relation types that are organized into a three-level hierarchy. The top level relations, for example, includes Temporal, Contingency, Comparison and Expansion. Braud and Denis (2015) presented a detailed comparative studies for assessing the benefit of unsupervised word representations, i.e. one-hot word pair representations against low-dimensional ones based on Brown cluster and word embeddings, for identifying implicit discourse relations in PDTB.\nBaroni et al. (2014) provides a systematic comparison between count-based and predictive word embeddings, on a wide range of lexical semantic tasks, including semantic relatedness, synonym detection, concept categorization, selectional preferences and analogy. The main takeaway is that the predictive models are shown to perform better than count-based ones.\n8.3 experiments\nOur objective is to assess the usefulness of different vector representations of word pairs for temporal relation type classification of event-event pairs. Specifically, we want to establish (i) whether predictive models are better than count-based ones for this particular task, (ii) which vector combination schemes are more suitable for the task, and finally, (iii) whether traditional features are still relevant in the presence of dense representations of word pairs.\nThe problem of temporal relation type classification can be formally defined as:\nGiven an ordered pair of events (e1, e2), assign a certain label to the pair, which can be one of the 14 TimeML temporal relation (TLINK) types: BEFORE, AFTER, INCLUDES, IS_INCLUDED, DURING, DURING_INV, SIMULTANEOUS, IAFTER, IBEFORE, IDENTITY, BEGINS, ENDS, BEGUN_BY or ENDED_BY.\n8.3 experiments 89\nHowever, we collapse some labels as explained in Section 4.5.2.6, i.e., IBEFORE into BEFORE, IAFTER into AFTER, DURING and DURING_INV into SIMULTANEOUS, considering the sparse annotation of such labels in the datasets, leaving 10 TLINK types as possible labels.\npre-trained word vectors We take pre-trained word vectors from GloVe2 and Word2Vec3 representing count-based and predictive word embeddings, respectively. The GloVe embeddings are 300-dimensional vectors trained upon 6 billion tokens of Wikipedia articles (2014) and English Gigaword (5th edition) with 400,000 uncased vocabularies. The Word2Vec embeddings are 300-dimensional vectors for 3 million words and phrases trained on part of Google News dataset (about 100 billion words).\nGiven an event pair (e1, e2), we retrieve the pair of word vectors (~w1, ~w2) based on vector look-up for the head words of e1 and e2 in the pre-trained word vectors.\nvector combinations Given a pair of word vectors (~w1, ~w2), we consider (i) concatenation (~w1 ⊕ ~w2), (ii) addition (~w1 + ~w2) and (iii) subtraction (~w1 − ~w2), as vector combination schemes. Note that in (i) the word ordering information is retained, which is not the case in (ii) and (iii).\ntraditional features Considered as the traditional features are the same features described in Section 4.5.2.6 for event-event (E-E) pairs.\nclassifier We use LIBLINEAR (Fan et al., 2008) L2-regularized logistic regression (dual), with default parameters, as the classifier. We have run the same set of experiments using LIBLINEAR L2-regularized L2-loss linear SVM (dual), and found that logistic regression yields better results than SVM.\nexperimental setup Experiments were run using the TBAQ-cleaned corpus (Section 5.5), in stratified 10-fold cross-validation. TBAQ-cleaned is the training corpus released in the context of TempEval-3 challenge, which includes (i) the AQUAINT TimeML corpus, containing 73 news report documents, and (ii) the TimeBank corpus, with 183 news articles. The stratified cross-validation scheme is chosen to account for the imbalanced distribution of relation types as illustrated in Table 4.3, e.g., the event pairs under the majority type BEFORE makes 34% of the data. With this scheme, the proportion of instances under each TLINK type are approximately the same in all 10 folds.\nTwo experimental settings are considered:\nS1 Given a pair of word vectors (~w1, ~w2), which is retrieved from either GloVe or Word2Vec pre-trained vectors, (~w1 ⊕ ~w2), (~w1 + ~w2) or (~w1 − ~w2) is considered as the feature set for the classifier.\nS2 Given a pair of word vectors (~w1, ~w2), which is the best performing embeddings in S1, and the traditional feature set ~f, ((~w1⊕ ~w2)⊕ ~f), ((~w1+ ~w2)⊕ ~f) or ((~w1− ~w2)⊕ ~f) is considered as the feature set for the classifier.\nexperiment results In Table 8.1 we report the performances of the classifier in different experimental settings S1 and S2, compared with the classifier performance using\n2 http://nlp.stanford.edu/data/glove.6B.zip 3 http://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/\n90 word embeddings for temporal relations\nonly traditional features. Since we classify all possible event pairs in the dataset, precision and recall are the same.\nWe found that pre-trained word vectors from Word2Vec perform better than the ones from GloVe. Based on this we may conclude that word embeddings from predictive models are superior than those from count-based models for this particular task. However, we should take into account that the word vectors from GloVe are only partially trained on news texts, while Word2Vec embeddings are fully trained on news dataset. Given that the TBAQ-cleaned corpus used in these experiments contains news articles, the different source of texts used for training the embeddings may contribute to the vary performance. Moreover, we acknowledge the different size of data used to train the embeddings, i.e. 6 billion tokens for GloVe vs 100 billion words for Word2Vec, which may also be the cause of GloVe embeddings’ loss.\nFrom the different vector combinations, concatenation (~w1⊕ ~w2) is shown to be the best combination. Using the concatenated Word2Vec embeddings (~w1⊕ ~w2) as features results in 0.4950 F1-score, almost as good as using only traditional features (0.5155 F1-score). The fact that this representation retain the word order information may be the reason why it\n8.4 evaluation 91\nbeats the other vector combinations. With the exception of IDENTITY and SIMULTANEOUS, all of the other TLINK types are asymmetric, e.g. BEFORE/AFTER, INCLUDES/IS_INCLUDED.\nCombining the word embeddings with traditional features (S2) yields significant improvement. With ((~w1⊕ ~w2)⊕~f) as features, the classifier achieves 0.5760 F1-score, around 6% improvement compared to using only ~f as features and around 8% if compared to using only (~w1 ⊕ ~w2) as features. The rationale behind this improvement would be that in this setting, the classifier could utilize both morpho-syntactic and lexical-semantic features to learn the temporal ordering between events in texts.\nWe detail in Table 8.2, the performances of the classifier (in terms of F1-scores) with different feature vectors. Pairs of word vectors (~w1, ~w2) are retrieved from Word2Vec pre-trained vectors. (~w1 − ~w2) is shown to be the best in identifying IDENTITY, BEGINS, BEGUN_BY and ENDS relation types, while the rest are best identified by (~w1 ⊕ ~w2). Combining (~w1 ⊕ ~w2) and ~f improves the identification of all TLINK types in general, particularly BEGINS/BEGUN_BY and ENDS/ENDED_BY types, which were barely identified when (~w1 ⊕ ~w2) or ~f is used individually as features.\n8.4 evaluation\ndataset We use the same training and evaluation data released in the context of Tempeval-3, i.e., TBAQ-cleaned and TempEval-3-platinum.\nTable 8.3 shows the classifier performances with different feature vectors, evaluated on the TempEval-3-platinum corpus. In general, using only (~w1 ⊕ ~w2) as features does not give any benefit since the performance is significantly worse compared to using only traditional features ~f, i.e. 0.4271 vs 0.5043 F1-scores. Combining the word embedding and traditional features ((~w1 ⊕ ~w2)⊕ ~f) also does not improve the classifier performance in general. However, if we look into each TLINK type, the classifier performance in identi-\n92 word embeddings for temporal relations\nfying IDENTITY, SIMULTANEOUS and IS_INCLUDED is improved, and quite significantly for SIMULTANEOUS.\n8.5 conclusions\nWe have presented a preliminary investigation into the potentiality of exploiting word embeddings for temporal relation type classification between event pairs. We compared two existing pre-trained word vectors from the two popular word embedding algorithms, GloVe and Word2Vec, representing count-based and predictive models, resp. Word2Vec embeddings are found to be the better word representation for this particular task. However, we cannot make a strong judgement towards predictive models being better than count-based models. This is because of the different training criteria, such as source of texts and size of the training set, used to build the two embeddings.\nWe also found that concatenation is the best way to combine the word vectors, although subtraction may also bring advantages for some TLINK types such as IDENTITY and BEGINS/BEGUN_BY.\nIn the 10-fold cross-validation setting, combining word embedding and traditional features ((~w1⊕ ~w2)⊕ ~f) results in significant improvement. However, using the same feature vector evaluated on the TempEval-3 evaluation corpus, the classifier does not improve in general, although for some TLINK types (IDENTITY, SIMULTANEOUS and IS_INCLUDED) we observe a performance gain.\nThese results shed some light on how word embeddings can potentially improve a classifier performance for temporal relation extraction. In the future we would like to explore the impact of ensemble learning, stacking method in particular, in which a super-classifier is trained to combine the predictions of several other classifiers, e.g., classification models with (~w1 ⊕ ~w2), (~w1 + ~w2), (~w1 − ~w2) and ~f as features.\nFurthermore, instead of using general-purpose word embeddings, several works presented methods for building task-specific word embeddings (Boros et al., 2014; Hashimoto et al., 2015; Nguyen and Grishman, 2014; Tang et al., 2014), which may also be beneficial for temporal ordering task.\n9"
    }, {
      "heading" : "T R A I N I N G D ATA E X PA N S I O N",
      "text" : "We don’t have better algorithms. We just have more data. — Google’s Research Director Peter Norvig\n9.1 Introduction 93 9.2 Related Work 93 9.3 Temporal Reasoning on Demand 94\n9.3.1 Predicting Number of Deduced TLINKs 96 9.3.2 Experiments 96\n9.4 Semi-supervised Learning 99 9.4.1 CLINK Extraction System 99 9.4.2 Experiments 101\n9.5 Conclusions 105\n9.1 introduction\nThe scarcity of annotated data is often an issue in building an extraction system with supervised learning approach. Halevy et al. (2009) argue that linear classifiers trained on millions of specific features outperform more elaborate models that try to learn general rules but are trained on less data. One of the widely known approaches to gain more training examples is semi-supervised learning, since many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce considerable improvement in learning accuracy.\nIn this chapter we explore two approaches to expand the training data for temporal and causal relation extraction, namely (i) temporal reasoning on demand for temporal relation type classification1 and (ii) self-training, a wrapper method for semi-supervised learning, for causal relation extraction. Each approach will be discussed in details in Section 9.3 and Section 9.4, respectively.\n9.2 related work\nSeveral systems have been proposed to classify temporal relations between temporal entities according to TimeML specifications, as shown in the TempEval evaluation campaigns (UzZaman et al., 2013; Verhagen et al., 2007, 2010). However, only a few of them dealt with the problem of combining classification and temporal reasoning (Mani et al., 2006). Such a combination can have three main advantages: (i) to detect and possibly discard documents in the training data that are inconsistent, (ii) to automatically analyse and partially complete documents with missing relevant data before running a classifier, (iii) to improve the output of the classifier by adding data or spotting inconsistencies in the classifier output (Tatu and Srikanth, 2008).\n1 Joint work with Rosella Gennari, Free University of Bozen-Bolzano and Pierpaolo Vittorini, Università degli Studi dell’Aquila.\n93\n94 training data expansion\nResearch work has recently focused on (ii) and (iii), using mainly Integer Linear Programming (Chambers and Jurafsky, 2008b; Do et al., 2012) and Markov Logic Networks (Yoshikawa et al., 2009) to optimize the classification output. To our knowledge, however, no studies have so far tackled the problem (i) to improve on the quality of temporal relation classification. The only work partially related to this topic is by Derczynski and Gaizauskas (2010), whose CAVaT tool is able to perform error checking and validation of TimeML documents. However, the impact of error checking and validation on automated classification performances is not experimentally assessed.\nAs for (ii), the annotation of event-event relations following TimeML guidelines has focused on events in the same or close sentences, therefore, most machine-learning based classifiers learn to annotate following this principle. However, deducing additional relevant relations can be beneficial to a classifier performance. Laokulrat et al. (2014) partially complete documents by deducing relations before running a classifier: they extract data by using timegraphs, and apply a stacked learning method to temporal relation classification. However, for performance reasons, they do not deduce all possible relations, e.g., they arbitrarily limit the number of deduced relations for each document to 10,000. More generally concerning the costs of (ii), no past work has taken them into account from a statistical experimental viewpoint.\nSemi-supervised setting for temporal relation extraction were made possible in the TempEval-3 shared task. The task organizers released the TempEval-3 silver corpus, a 600K corpus collected from Gigaword, which is automatically annotated by best performing systems in the preceding TempEval task. In TempEval-3, none of the participants submitted the systems trained on this silver data, most probably because the amount of gold data released within the task was enough to achieve good results in a fully supervised setting.\nFisher and Simmons (2015) presents a semi-supervised spectral model for a sequential relation labelling task for discourse parsing, in which Temporal and Contingency/Causal relations are included in the possible relation types. The empirical evaluation on the Penn Discourse TreeBank (PDTB) (Prasad et al., 2007) dataset yields 0.485 F1-score, around 7- 9 percentage point improvement over approaches that do not utilize unlabelled training data.\n9.3 temporal reasoning on demand\nIn an attempt to improve the performance of temporal relation type classification between event pairs, we introduce a method to enrich the training data via temporal reasoner. However, instead of running the reasoner over the whole training set, we argue that this step can be enhanced by invoking the reasoner only when it is estimated to be effective for improving the classifier’s performance. The estimation is based on document parameters that are easy to calculate.\nMoreover, the gold standard corpora are found to contain inconsistent documents with respect to the temporal graph built from annotated temporal relations using a temporal closure algorithm (Gennari et al., 2015; Verhagen, 2005a). As Verhagen (2005a) pointed out, “it is hard to annotate a one-page document without introducing inconsistencies” and “even trained annotators are liable to introduce relations that clash with previous choices”. Therefore, we also investigate the impact of excluding inconsistent documents\n9.3 temporal reasoning on demand 95\nfrom the training set in order to improve not only the quantity but also the quality of the training data.\nmethod Given a training set with TimeML documents, the following steps are performed:\nStep 1 The temporal reasoner checks the consistency of each document. If the document is found to be consistent, it reports so; else it tries with a more relaxed TLINK mapping into Allen relation (see Section 4.5.2.5) or, if no other mapping is available, it terminates reporting inconsistency.\nStep 2 Documents that are inconsistent with respect to all the considered mappings are discarded from the training set.\nStep 3 For each document, the number of temporal relations (TLINKs) that can be deduced is empirically predicted.\nStep 4 The decision whether deduction should be run or not on a document depends on whether the predicted number of deduced TLINKs falls below a certain threshold. Accordingly, the temporal reasoner runs the deduction (or not).\nStep 5 The initial training set, now enriched with deduced TLINKs, is used to train the classifiers for determining the TLINK types of event-event (E-E) and event-timex (E-T) pairs.\nFor the temporal reasoning process, including inconsistency checking (Step 1) and new TLINK deduction (Step 4), the temporal reasoner (SQTR) explained in Section 4.5.2.5 is used. The method for estimating the number of deduced TLINKs (Step 3) will be further detailed in the following section (Section 9.3.1). The threshold for Step 4 is estimated in the experiments reported in Section 9.3.2.\nThe TLINK type classification system reported in this chapter is a hybrid classification system, which determines the temporal relation type of a given pair of ordered temporal entities (e1, e2). Included are two supervised classifiers for event-event (E-E) and eventtimex (E-T) pairs, and a set of rules for timex-timex (T-T) pairs.\nThe supervised classifiers for E-E and E-T pairs (Step 5) are basically the prior version of the E-E classifier and E-T classifier included in TempRelPro (Section 4.5.2.6). Meanwhile, the set of rules for T-T pairs is the same set of rules explained in Section 4.5.2.1. Henceforth, we will address the TLINK type classification system used in this chapter as TempRelPro-beta. Several features that make TempRelPro-beta different from the current version include:\n• dependencyPath The considered dependency path were not limited to any specific constructions.\n• signalTokens For TempRelPro-beta, temporal signals were not clustered, hence, the tokens of temporal signals or temporal discourse markers appearing around the event pairs are used as features instead of the clusterID of signal cluster. The discourse markers were obtained using addDiscourse (Pitler and Nenkova, 2009), retaining only those labelled as Temporal.\n• signalDependency This feature was not included in TempRelPro-beta.\nFurthermore, in TempRelPro-beta, the label simplification method mentioned in Section 4.5.2.6 is not employed, so the classifiers label the entity pairs with the full set of 14 TLINK types.\nSince it has been shown that the current system for temporal relation extraction performs poorly in classifying the temporal relation types of event-event pairs, in this chapter the automated temporal reasoning is considered only for event-event pairs, albeit the method design is in principle more general. Nevertheless, the classification experiments presented in the following sections cover event-event, event-timex and timex-timex pairs, using TempRelPro-beta, in line with the tasks proposed in the TempEval campaigns.\n9.3.1 Predicting Number of Deduced TLINKs\nAs suggested by Verhagen (2005a), there should be a relation between the size of the maximal connected components (SMCC) of the graph of the document and the number of deducible TLINKs. This could also be related to number of EVENTs and TLINKs of the input document. All such variables were computed for each consistent document in the training corpus of TempEval-3 UzZaman et al., 2013 and a regression analysis was conducted with: (i) the number of deduced TLINKs as dependent variable, and (ii) the number of EVENTs, the number of TLINKs, and SMCC as independent variables. Table 9.1 shows the results of the analysis.\nSince the model proves to fit with good reliability (R2=0.633), one can assume that a linear relation exists between the number of TLINKs, EVENTs, SMCC, and the number of deducible TLINKs. According to the estimates in Table 9.1, SMCC together with the number of TLINKs are likely to have a large impact on the number of deducible TLINKs. On the contrary, increasing the number of EVENTs has an adverse effect.\nSuch results confirm that SMCC, the number of TLINKs and EVENTs of the input document can predict the number of deducible TLINKs (e.g., if #TLINKs = 15, #EVENTs = 10, SMCC = 10, one may expect to deduce ≈ 43,CI95% = [32, 54] TLINKs). Moreover they enable Step 4, which relies on the following research hypothesis: if the estimated number of deducible TLINKs falls below a threshold, deducing TLINKs is likely to be effective for classification performances; else not. The threshold is estimated in the experiments reported in the following section.\n9.3.2 Experiments\nTwo sets of experiments were run. The first experiment focuses on the effect of consistency checking and/or deduction on classification performances. The second experiment\n9.3 temporal reasoning on demand 97"
    }, {
      "heading" : "Accuracy",
      "text" : "instead evaluates the threshold foreseen in Step 4 returning the best classification performances.\ndataset Experiments were run using two widely used corpora as the training set, which were made available during the Tempeval-3 challenge, TBAQ-cleaned, which includes (i) the AQUAINT TimeML corpus, containing 73 news report documents, and (ii) the TimeBank corpus, with 183 news articles. The test data is the newly created TempEval3-platinum evaluation corpus that was annotated and reviewed by the Tempeval-3 task organizers.\nconsistency checking and deduction Classification performances of TempRelProbeta were computed, comparing four different experimental settings. In all settings, Step 4 of the workflow is disabled, since there were no available experimental data concerning the threshold. Several experimental settings are considered:\nS1 The temporal relation classifiers are trained using only the TimeBank and AQUAINT corpora (TBAQ) without running SQTR.\nS2 The training set is obtained by discarding the 9 inconsistent documents detected by SQTR (TBAQc), i.e., removing around 11.61% event-event pairs and 5.1% eventtimex pairs. Note that although consistency checking concerns only event-event pairs, when a document is inconsistent it is discarded as a whole, hence, e.g., also event-timex pairs are removed.\nS3 The training set is obtained after computing the deductive closure on event-event pairs with SQTR (TBAQd−all).\nS4 The training set is obtained with deduction after discarding inconsistent documents, i.e., TBAQc after deduction (TBAQcd−all). After deduction, the number of eventevent pairs is almost three times as many as in annotated input documents, with new 9,750 pairs addition.\nClassification results are reported in Table 9.2. Precision, recall and F1-score are computed using the graph-based evaluation metrics of the official TempEval-3 scorer, while accuracy is given by the predict function of LIBLINEAR.\nResults are in line with the literature and show that consistency checking and deduction are not always beneficial for classification performances. In particular, removing inconsistent documents slightly decreases performances (possibly because less data is given to the classifier), even if this improves the quality of training data for classification.\n98 training data expansion\nTo compare the effect of deduction, the performances are compared based on the same training data, i.e., either all documents (TBAQ vs. TBAQd−all) or only consistent documents (TBAQc vs. TBAQcd−all). Accordingly, deduction improves precision, lowers recall, improves F1-score when the training data is made up only of consistent documents, and lowers F1-score when all documents are included in the training set.\nAs mentioned earlier, consistency checking discards document as a whole, even if only two relations are inconsistent, and thus also discarding consistent annotations in the document that the classifier may use. Such an issue has space for improvements, e.g., by implementing in SQTR a further method that extracts consistent annotations from an inconsistent document. Concerning deduction, the main reason for the lowered recall is that the already skewed data become more unbalanced after computing deduction (e.g., there are 6000 instances of BEFORE type vs <100 instances of IBEFORE type).\n9.3.2.1 Threshold.\nFor each document, the estimated number of deducible TLINKs between events is computed (Step 3). Next, a routine was implemented for finding the lowest threshold which yields the highest F-score, where the threshold corresponds to the number of maximum deducible TLINKs for each document.\nThe routine starts from the threshold value t = 10, going up until t = 200 with interval i = 10. Figure 9.1 shows the best threshold value, before the F1-scores decreases or becomes stagnant. Again, the impact of Step 4 is assessed by applying deduction on eventevent pairs from TBAQ and TBAQc. This analysis shows that classification performance using TBAQd or TBAQcd is sensitive to the threshold.\nThe classification experiment using TBAQd and TBAQcd was then re-run with their best thresholds (160 and 100 respectively). Results are reported in Table 9.3.\nThe outcome of the second set of experiments sheds light on when deduction should be invoked: in a flexible setting, with deduction being called only for specific documents, clas-\n9.4 semi-supervised learning 99\nsification performances achieve state-of-the art results. In other words, deduction proves useful to increase TLINKs between events only for specific documents. Also discarding inconsistent documents helps in this setting. These findings corroborate our initial hypothesis that a flexible system for temporal relation classification may be useful to address consistency and data sparseness problems of the training data, improving both quality of data and classification performances.\n9.4 semi-supervised learning\nGiven the sparse annotated CLINKs in the Causal-TimeBank corpus, we introduce a bootstrap method to get more labelled examples. Specifically, we employ self-training, a wrapper method for semi-supervised learning. We first train a CLINK classifier based on labelled data in the Causal-TimeBank. The classifier is then applied to unlabelled data to generate more examples to be used as input for the CLINK classifier.\nFurthermore, given the nature of news texts, that often describe the same set of events just by rewording the same underlying story, we make the following assumptions:\n(i) for the same two event pairs in different news articles, the causal relations (if any) may be expressed with different causal markers, or be implicit; and\n(ii) if we set a causal relation between two events in a news, the same holds every time the two events are mentioned in similar news.\nTherefore, we can bootstrap new training data by propagating the causal relation through event co-reference. In these new training instances, causality may be expressed differently from the original news.\nIn addition to the labelled data obtained via self-training, we bootstrap further training examples through the so-called CLINK propagation method, following the assumptions above. The workflow of our CLINK extraction system and bootstrapping method is reported in Figure 9.2.\nNote that the CLINK extraction system reported in this chapter is slightly different than our current CauseRelPro (Section 6.5), which will be explained further in Section 9.4.1, and henceforth named as CauseRelPro-beta. We will detail our training data expansion experiments through self-training and CLINK propagation in Section 9.4.2.\n9.4.1 CLINK Extraction System\npreprocessing Several modules and tools are used in the preprocessing steps to leverage syntactic and semantic information needed for the automatic extraction part. In particular, we employ the following systems:\n100 training data expansion\n• NewsReader NLP pipeline, which is an NLP pipeline developed as part of the NewsReader project.2, with the goal to integrate several analysis layers in a single NLP suite. We are particularly interested in the output of the event annotation module, since we want to detect causality between events. However, the other modules of the pipeline such as part-of-speech tagging, constituency parser, dependency parser, event co-reference and temporal relation extraction also play a great role in producing feature vectors for to-be-classified event pairs.\n• SuperSense tagger (Ciaramita and Altun, 2006), which annotates verbs and nouns in a text with 41 semantic categories (WordNet supersense).\n• addDiscourse (Pitler and Nenkova, 2009), which identifies discourse connectives and assigns them to one of four semantic classes: Temporal, Expansion, Contingency and Comparison. Causal connectives are included in the Contingency class.\nCauseRelPro-beta includes a classification model based on Support Vector Machines (SVMs) with a one-vs-one strategy for multi-class classification and polynomial kernel. Considered as candidate event pairs are the same candidate pairs described in Section 6.5.1. The classification model is mainly based on the CLINK classifier introduced in Mirza and Tonelli (2014a), with several novelties:\n(i) We consider WordNet supersenses as token-level features instead of tokens or lemmas to better generalize our model and avoid over-fitting.\n(ii) We avoid a two-step procedure, in which causal signals are first automatically identified as a separate task and then included as features in the classification model for event relations. Instead, causal markers features, which will be detailed below, are included in the set together with event features and temporal information.\n(iii) Co-reference between event pairs is considered, since no causal relation can hold between two co-referring events. This information is provided by the NewsReader\n2 http://www.newsreader-project.eu/results/software/\n9.4 semi-supervised learning 101\npipeline, which includes an implementation of the event coreference tool by Cybulska and Vossen (2013).\ncausal marker features We consider three types of causal markers that can cue a causal relation between events:\n1. Causal signals. We rely on the list of causal signals used to annotate CSIGNALs in the Causal-TimeBank corpus. For some causal signals that are discontinuous, e.g. due (mostly) to, we include their regular expression patterns in the list, e.g. /due .*to/.\n2. Causal connectives, i.e. the discourse connectives under the Contingency class according to the output of the addDiscourse tool.\n3. Causal verbs. The three types of verbs lexicalizing causal concepts as listed in Wolff and Song (2003): i) CAUSE-type verbs, e.g. cause, prompt, force; ii) ENABLE-type verbs, e.g. allow, enable, help; and iii) PREVENT-type verbs, e.g. block, prevent, restrain.\nBased on the existence of causal markers around e1 and e2, exactly in that priority order3, we include as features: • causal marker string; • causal marker position, i.e. between e1 and e2, before e1, or at the beginning of the sen-\ntence where e1/e2 is in; and • dependency path between the causal marker and e1/e2.\nSince the previously mentioned causal marker features are based on non-exhaustive lists of causal signals and causal verbs, not to mention the imperfect output of the addDiscourse tool, we also consider possible causal signals and causal verbs derived from dependency relations.\nWe observed common dependency paths connecting events with causal signals or causal verbs, as illustrated in Figure 9.3. The possible causal signals are identified using dependency path pattern matching, e.g. e→prp→dep (because of), pmod→e (because), and e→adv (thus). Pattern priority rules are employed, e.g. e→prp→dep > e→adv, to avoid extracting partly (via e→adv) in sentence (a). We also consider PoS tags to avoid extracting increasing (via e→adv) in sentence (b). The same method is applied to identify possible causal verbs, e.g. sbj→e (causes) and oprd→im→e (causes).\nIn the end, we include as features: • possible causal signals via dependency path pattern matching for e1/e2; • their positions w.r.t e1/e2 (before or after); • possible causal verbs via dependency path pattern matching for e1/e2; and • their positions w.r.t e1/e2 (before or after);\n9.4.2 Experiments\ndataset Causal-TimeBank (Section 5.5) is used as the initial training set. The unlabelled dataset is collected from the EMM NewsBrief platform4, which performs multilingual\n3 We first look for causal signals. If we do not find any, then we continue looking for causal connectives. And so on. 4 http://emm.newsbrief.eu\n102 training data expansion\nnews aggregation and analysis from news portals world-wide, updated every 10 minutes. The news are automatically clustered according to subjects.\nWe collect 16 RSS feeds of subjects containing more than 15 news articles from the ’top stories’ list in English from January 20 to January 21, 2015. For each subject, we select only the news published within the same hour, to ensure the high similarity between the news in the same cluster. The news articles are automatically fetched from the links listed in the RSS feeds, cleaned, and then converted into the annotation format required by the NewsReader NLP pipeline. In the end, we have 16 clusters of 15 documents each, for a total of 240 documents in the newly created corpus, i.e. EMM-clusters.\ncauserelpro-beta We first run a classification experiment aimed at comparing the performance of CauseRelPro-beta in a supervised setting with the existing system by Mirza and Tonelli (2014a) as the baseline. We adopt the same five-fold cross-validation setting and use only the Causal-TimeBank corpus as training and test set. The goal of this first experiment is to assess the impact of the new features presented in Section 9.4.1.\nTable 9.4 shows the micro-averaged performance of the system, compared with the baseline. The main difference between CauseRelPro-beta and that reported in Mirza and Tonelli (2014a) is the elimination of the middle step in which causal signals are identified. This, together with the use of supersenses, contributes to increasing recall. However, using token-based features and having a specific step to label CSIGNALs yield better precision.\nWe also conduct some experiments by using different degrees of polynomial kernel (Table 9.5). Note that even though the best F1-score is achieved by using the 3rd degree of polynomial kernel, the best precision of 0.6337 is achieved with degree 4.\nThe best performance achieves only 0.3511 F1-score. We argue that this low performance is strongly dependent on the lack of training data. Moreover, the proportion of positive and negative examples in the training data is highly imbalanced. In our previous experiment with a 5-fold cross-validation setting, on average, only 1.14% of event pairs in the training data is labelled with causal relations. In order to tackle this issue, we first implement a self-training approach using the EMM clusters as additional corpus, and then apply CLINK propagation. Each step is evaluated separately in the following paragraphs.\nself-training In order to boost the training data with more CLINKs to learn from, we adopt a basic configuration of the self-training approach. CauserelPro-beta is trained using the whole Causal-TimeBank corpus, which has gold annotated events and TLINKs. Since the model will be used to label new data to be included in the training set, we give preference to a higher precision over recall, therefore we use the 4th degree of polynomial kernel in building the classifier, in line with the findings reported in Table 9.5.\nThen, we run the model on the EMM-clusters dataset, which contains events and TLINKs annotated by the NewsReader pipeline. New labelled event pairs are obtained this way. Note that we only consider the positive examples as additional training data, as we want to reduce the imbalance in the dataset. In the end, we have 324 additional event pairs labelled with CLINK/CLINK-R.\nThe impact of having more training data is evaluated in the same five-fold crossvalidation setting as before. The Causal-TimeBank corpus is used again for evaluation. For each fold, we append the training data with the new labeled event pairs. A new classifier is trained with the new training data using the 3rd degree of polynomial kernel, this time to obtain the best possible F1.\nAs shown in Table 9.6, the appended training corpus (Causal-TimeBank + EMM-clusters) improves micro-averaged F1-scores of the system by 5.57% significantly (p < 0.01).\n104 training data expansion\nclink propagation Given the EMM-clusters dataset annotated with CLINKs, our CLINK propagation algorithm is as follows:\n1: for every cluster C in EMM-clusters corpus do 2: for every news article N in C do 3: clinks← all event pairs labelled with CLINK/CLINK-R above confidence threshold\n4: for every event pair (e1, e2) in clinks do 5: Look for event pair (ec1, ec2) in other news article in C, where ec1 co-refers with e1 and ec2 co-refers with e2 6: if (ec1, ec2) is not labelled with CLINK/CLINK-R then 7: Label (ec1, ec2) with the label of (e1, e2) (establish the propagated CLINK) 8: end if 9: end for\n10: end for 11: end for\nIn order to decide which new CLINKs should be added to the training data, we did some experiments to select the confidence threshold returned by the classifier that maximizes the system’s performance. Figure 9.4 shows how the performance measures change at different cut-off values, ranging from 1 to 2, with 1.75 giving the best outcome. Using this threshold, we add 32 event pairs labelled with CLINK/CLINK-R to the training data.\nWe evaluate the performance of the system trained with the enriched training data from CLINK propagation in the same five-fold cross-validation setting as the previous experiment. As shown in Table 9.6, the micro-averaged F1-score increases by 8.1% with respect to the initial configuration if we append propagated CLINKs as new labelled examples (Causal-TimeBank + EMM-clusters + prop. CLINKs). The improvement is statistically significant with p-value < 0.001. As expected, the overall improvement is caused by the increased recall, even though the system precision drops.\n9.5 conclusions 105\nAs an example, we report below three text passages from different news in the same cluster of the EMM corpus. The CLINK in the first excerpt was correctly propagated to the other two, so that 2 additional event-pair instances were added to the training data.\n(i) U.S. Secretary of State John Kerry arrived in Nigeria on Sunday to urge its rival political camps to respect the outcome of a Feb. 14 presidential election, amid concerns that post-poll violence e1 could undermine the fight e2 against Boko Haram militants.\n(ii) Washington is concerned that post-poll violence e3 could undermine the stability of Africa’s top oil producer and hamper efforts to tackle e4 the Islamist militants of Boko Haram.\n(iii) U.S. Secretary of State John Kerry arrived in the commercial capital Lagos on Sunday to urge the candidates and their supporters to respect the election outcome, underscoring U.S. concerns that post-poll violence e5 could destabilise the country and undermine the fight e6 against Boko Haram.\nThe event pair (e1, e2) in (i) is labelled as having a causal relation with a high confidence score by the classifier trained with the Causal-TimeBank corpus. This is because there is a causal verb undermine, which falls under the PREVENT causal type, connecting the two events.\nEvent pairs (e3, e4) and (e5, e6) in (ii) and (iii), respectively, are labelled with having causal relations through the CLINK propagation method, since e1 co-refers with e3 and e5, and e2 co-refers with e4 and e6.\nIf we apply the classification model learnt only from the Causal-TimeBank, instead, the event pair (e3, e4) is not labelled as having a causal relation, probably because there is no training instance including the causal verb to hamper. Also the event pair (e5, e6) is not recognized as having a causal relation, probably because there is no direct syntactic connection between the causal verb undermine and e5 (violence).\n9.5 conclusions\ntemporal reasoning on demand We have presented an approach to improve temporal relation classification, by activating temporal reasoning on training data to improve not only the quantity but also the quality of the training data. However, the reasoning process, in particular deducing new TLINKs based on existing set of TLINKs, is only run when it is estimated to be effective, i.e. temporal reasoning on demand.\nThe result of a regression analysis showed that the number of TLINKs deducible from an annotated document can be estimated considering easy-to-measure parameters from the document. According to the experiments, deduction may be beneficial when the expected number of deducible TLINKs is inferior to a certain threshold, experimentally assessed. With this setting, removing inconsistent documents prior to deduction can also have a positive impact on classification performances.\nA first possible improvement may concern how to be more precise in estimating the number of deducible TLINKs, which in turns should lead to an improved prediction on when to run deduction. To this aim, one should increase the R2 of the regression analysis, by finding further or different parameters that predict with a greater reliability the number\n106 training data expansion\nof deducible TLINKs, and thus improving on the impact of deduction on classification performances.\nFurthermore, as already mentioned, the current version of the reasoner identifies inconsistent documents as a whole, without providing any judgement at the level of temporal entities. Removing (only) inconsistent pairs is a very challenging task but with potential benefits for classification performances, which should be explored in the future (e.g., see Mitra and Launay, 2006 for a possible approach).\nsemi-supervised learning Since the performance of the CLINK extraction system seems to be negatively affected by the scarcity of training examples, we proposed a selftraining method to deal with this issue. Moreover, we proposed the causal link propagation method to further enrich the labelled data using event co-reference information in the news clusters dataset.\nOur experiments show that self-training and CLINK propagation methods can significantly improve the performance of a CLINK extraction system, even if the semi-supervised learning is simplified (only one iteration is performed) and the unlabelled data contain only 240 documents. Despite the limited number of newly acquired training examples (324 through self-training and 32 through CLINK propagation), they still have a significant impact on the classifier performance, since the original training corpus contains only 318 causal links.\nIn the future, we would like to investigate the impact of our bootstrap method in a standard semi-supervised setting with many more unlabelled data and several iterations.\n10 M U LT I L I N G U A L I T Y I N T E M P O R A L P R O C E S S I N G\nTo have another language is to possess a second soul. — Charlemagne\n10.1 Introduction 107\n10.2 Related Work 108\n10.3 Related Publications 108\n10.4 Italian 108\n10.4.1 Temporal Information Extraction System 109\n10.4.2 EVENTI (Evalita 2014) Evaluation 114\n10.5 Indonesian 117\n10.5.1 Challenges in Indonesian 117\n10.5.2 Timex Extraction System 118\n10.5.3 Temporal Tagging 122\n10.5.4 Evaluation 122\n10.6 Conclusions 124\n10.1 introduction\nResearch on temporal information processing has been gaining a lot of attention from the NLP community in the recent years. However, most research efforts in temporal information processing have focused only on English.\nTempEval-2, one of TempEval evaluation campaigns, attempted to address multilinguality in temporal information processing by releasing annotated TimeML corpora in 6 languages including English. The distribution over the six languages was highly uneven; out of 18 participating systems, only 3 were for Spanish, the rest were for English, and none for the other languages.\nApart from TempEval, HeidelTime1, a multilingual, domain-sensitive temporal tagger currently contains hand-crafted resources for 13 languages. In addition, the most recent version contains automatically created resources for more than 200 languages.\nIn this chapter, we focus on temporal information processing for two languages other than English: Italian and Indonesian. For Italian, there was the EVENTI challenge for temporal information processing of Italian texts, which provides us a framework to evaluate our temporal information processing system for Italian. For Indonesian, our extension effort is only for the temporal expression extraction. This is because this task, especially timex normalization, typically requires a rule-engineering approach unlike event extraction or temporal relation extraction, which are commonly approached with data-driven methods. This will be the first step towards a complete temporal information processing for the Indonesian language.\n1 http://heideltime.ifi.uni-heidelberg.de/heideltime/\n107\n108 multilinguality in temporal processing\n10.2 related work\nThe second instalment of TempEval evaluation campaigns (Section 3.4), TempEval-2 (Verhagen et al., 2010), extended the first TempEval with a multilingual task. In addition to English, the organizers released TimeML annotated corpora in 5 other languages: Chinese, Italian, French, Korean and Spanish. All corpora include timex and event annotation in TimeML standard. However, not all corpora contain data for all subtasks related to temporal relation extraction.\nFrom the eighteen system runs submitted to TempEval-2, sixteen were for English, one for Spanish, i.e. UC3M for timex extraction, and two for both English and Spanish, i.e. TIPSem and TIPSem-B (Llorens et al., 2010).\nFor temporal expression extraction, HeidelTime (Strötgen et al., 2014) is perhaps the temporal expression tagging system covering the most languages. HeidelTime currently understands documents in 11 languages, including English, German, Dutch, Vietnamese, Arabic, Spanish, Italian, French, Chinese, Russian, and Croatian. Even though the most recent work by Strötgen and Gertz (2015) presented an automatic extension approach to cover around 200+ languages in the world, we believe that for low-resource (and lessexplored) languages such as Indonesian, a manual extension effort is still required.\nThe recent work on temporal expression tagging for Indonesian documents (Simamora, 2013) only covers temporal expressions of DATE type. Moreover, the annotated documents are not in the TimeML annotation format, which is the widely used annotation format for temporal expression tagging. As far as we know, we are the first to implement a system for annotating temporal expressions in Indonesian documents with the TimeML format.\n10.3 related publications\nIn Mirza and Minard (2014), we summarized our attempts and approaches in building a complete extraction system for temporal expressions, events, and temporal relations in Italian documents, which participated in the EVENTI challenge.\nIn Mirza (2015), we presented an automatic system for recognizing and normalizing the value of temporal expressions in Indonesian texts.\n10.4 italian\nEVENTI2, one of the new tasks of Evalita 20143, was established to promote research in temporal information processing for Italian texts. Currently, even though there exist some independent modules for temporal expression extraction (e.g. HeidelTime (Strötgen et al., 2014)) and event extraction (e.g. Caselli et al. (2011b)), there is no complete system for temporal information processing for Italian.\nThe main EVENTI task is composed of 4 subtasks for temporal expression (timex), event and temporal relation extraction from newspaper articles. The evaluation scheme follows the existing TempEval evaluation campaign for English (Section 3.4), particularly TempEval-3. Additionally, a pilot task on temporal information processing of historical texts was also proposed.\n2 https://sites.google.com/site/eventievalita2014/ 3 http://www.evalita.it/2014\n10.4 italian 109\nOur temporal information extraction system, FBK-HLT-time, participated in both main task (for news articles) and pilot task (for historical texts), and was the only participant for the event extraction and temporal relation extraction tasks. For timex extraction in historical texts, FBK-HLT-time was the best performing system, showing that our approach is more robust to domain changes.\n10.4.1 Temporal Information Extraction System\nWe developed an end-to-end temporal information extraction system to participate in the EVENTI evaluation campaign. It combines three subsystems: (i) time expression (timex) recognizer and normalizer (Section 10.4.1.1), (ii) event extraction (Section 10.4.1.2) and (iii) temporal relation identification and classification (Section 10.4.1.3). These subsystems have been first developed for English as part of the NewsReader project4 and then adapted to Italian.\ndata We used the Ita-TimeBank released by the task organizers of EVENTI-Evalita 2014, containing 274 documents and around 112,385 tokens in total, for developing purposes. For the final end-to-end system submitted to EVALITA, we use this corpus as the training data for our classification models.\ntools and resources Several tools were used in developing our end-to-end system, especially for timex and temporal relation extraction:\n• TextPro5 (Pianta et al., 2008), a suite of NLP tools for processing English and Italian texts. Among the modules we specifically use: lemmatizer, morphological analyzer, part-of-speech tagger, chunker, named entity tagger and dependency parser.\n• YamCha6, a text chunker which uses SVMs algorithm. YamCha supports the dynamic features that are decided dynamically during the classification. It also supports multi-class classification using either one-vs-rest or one-vs-one strategies.\n• TimeNorm7 (Bethard, 2013a), a library for converting natural language expressions of dates and times into their normalized form, based on synchronous context free grammars.\nMoreover, we also exploit several external resources, such as the list of temporal signals extracted from the annotated Ita-TimeBank corpus. Mirza and Tonelli (2014b) show that the performance of their temporal relation classification system benefits from distinguishing event-related signals (e.g. mentre [while], intanto [in the meantime]) from timex-related signals (e.g. tra [between], entro [within]). Therefore we split the list of signals into two separate lists. Signals that are used in both cases (e.g. già [before], dopo [after], quando [when]) are added to both lists.\nIn the following sections we will explain our efforts in building a complete temporal information extraction system, excluding the event extraction module because we were not directly involved in its development.\n4 http://www.newsreader-project.eu/ 5 http://textpro.fbk.eu/ 6 http://chasen.org/~taku/software/yamcha/ 7 http://github.com/bethard/timenorm\n110 multilinguality in temporal processing\n10.4.1.1 Timex Extraction System\nAn automatic extraction system for temporal expressions typically consists of two modules, i.e., (i) timex extent recognition and type classification, and (ii) timex normalization. As has been discussed in Section 3.5.1, for recognizing the timex extent in English texts, both data-driven and rule-based strategies are equally good; the statistical system ClearTK performed best at strict matching with 82.71% F1-score. Meanwhile, the timex normalization task is currently done best by a rule-engineered system, TimeNorm, which achieves 81.6% F1-score.\ntimex extent and type identification We decided to adopt the data-driven approach for recognizing the extent of a timex. However, unlike ClearTK, we combined both timex extent recognition and type classification8 tasks as one text chunking task, using only one classification model. Since the extent of a timex can be expressed by a multi-word expression, we employ the BIO tagging to annotate the data. In the end, the classifier has to classify a token into 9 classes, including B-DATE, I-DATE, B-TIME, I-TIME, B-DURATION, I-DURATION, B-SET, I-SET and O (for other).\nThe classification model is built using the Support Vector Machine (SVM) implementation provided by YamCha. The following features were defined to characterize a token:\n• Token’s text, lemma, part-of-speech (PoS) tags, flat constituent (noun phrase or verbal phrase), and the entity’s type if the token is part of a named entity;\n• Whether a token matches regular expression patterns for unit (e.g. secondo [second]), part of a day (e.g. mattina [morning]), name of days, name of months, name of seasons, ordinal and cardinal numbers (e.g. 31, quindici [fifteen], primo [first]), year (e.g. ’80, 2014), time (e.g. 08:30), duration (e.g. 1h3’, 50\"), adverbs (e.g. passato [past], ieri [yesterday]), names (e.g. Natale [Christmas], Pasqua [Easter]), or set (e.g. ogni [every], mensile [monthly]);\n• Whether a token matches regular expression patterns for SIGNAL (e.g. per [for], dalle [from]);\n• All of the above features for the preceding 2 and following 2 tokens, except the token’s text;\n• The preceding 2 labels tagged by the classifier.\nIn Table 10.1 we report the classifier performance with 5-fold cross-validation scheme and a strict-match evaluation, comparing the one-vs-one method with one-vs-rest for multi-class classification. The one-vs-rest method gives better performance, especially for recognizing timex under the SET class.\ntimex value normalization For timex normalization, we decided to extend TimeNorm (Bethard, 2013a) to cover Italian time expressions. For English, it is shown to have a better accuracy compared with other systems such as HeidelTime (Strötgen et al., 2013) and TIMEN (Llorens et al., 2012).\nWe translated and modified some of the existing rules of the English grammar into Italian. Figure 10.1 presents some examples of the grammar for Italian. We also modified the TimeNorm code in order to support Italian language specificity:\n8 Temporal expressions are classified into 4 timex types in TimeML, i.e., DATE, TIME, DURATION and SET.\n10.4 italian 111\n[Int:1Digit] ||| due ||| 2 ||| 1.0 [Int:Hundred2Digit] ||| cento ||| 0 0 ||| 1.0 [Int:3Digit] ||| [Int:1Digit] [Int:Hundred2Digit] ||| [Int:1Digit] [Int:Hundred2Digit] ||| 1.0 [Period:Amount] ||| [Int] [Unit] ||| Simple [Int] [Unit] ||| 1.0 [Period] ||| [Period:Amount,1] e [Period:Amount,2] ||| Sum [Period:Amount,1] [Period:Amount,2] ||| 1.0 [TimeSpan:Regular] ||| oggi ||| FindEnclosing PRESENT DAYS ||| 1.0 [TimeSpan:Regular] ||| ieri ||| EndAtStartOf ( TimeSpan FindEnclosing PRESENT DAYS ) ( Period Simple 1 DAYS ) ||| 1.0\nTo process the annotated timex in TimeML format, some pre-processing and postprocessing steps are needed before and after the normalizing process by TimeNorm.\nThe pre-processing rules treat time expressions composed by only one number of one or two digits, and append a unit or name of month, which is inferred from closed timex (e.g. [ore 17timex] - [23timex]→ [ore 23timex]) or the document creation time (e.g. Siamo partiti il [7timex] (DCT=2014-09-23 tid=\"t0\")→ [7 settembretimex]).\nTimeNorm returns a list of all possible values for given timex and anchor time. We defined a set of post-processing rules in order to select one of the returned values, that is most consistent with the timex type. For example, if the timex is of type DURATION, the system selects the value starting with P (for Period of time).\nDuring the development of the grammar for Italian we noticed that the TimeNorm grammar does not support the normalization of the semester or half-year unit (e.g. il primo semestre [the first semester]). We developed another set of post-processing rules in order to cope with this issue. Despite all that, some expressions still cannot be normalized because\n112 multilinguality in temporal processing\nthey are too complex, e.g. ultimo trimestre dell’anno precedente [last quarter of the previous year] or primi mesi dell’anno prossimo [first months of next year].\nIf we always assume that the anchor time used for the timex normalization task is the document creation time (DCT), the system yields 0.753 accuracy on the training data. Note that the result is heavily biased by the fact that the system is tailored to improve its performance using the same corpus.\nempty timex identification The It-TimeML annotation guidelines adopted for the EVENTI task allow the creation of empty TIMEX3 tags, whenever a temporal expression can be inferred from a text-consuming one. For example, for the expression un mese fa [one month ago], two TIMEX3 tags are annotated: (i) one of type DURATION that strictly corresponds to the duration of one month (P1M) and (ii) one of type DATE that is not text consuming, referring to the date of one month ago (with the DCT as the anchor time).\nAs these timexes are not overtly expressed they cannot be discovered by the text chunking approach. We performed the recognition of the empty timexes using a set of simple post-processing rules and the output of the timex normalization module.\n10.4.1.2 Event Extraction System\nThe subsystem used for the event extraction task is reported in Mirza and Minard (2014), and was mainly developed by Anne-Lyse Minard from the Human Language Technology Group at the Fondazione Bruno Kessler, Trento, Italy.\n10.4.1.3 Temporal Relation Extraction System\nAn automatic extraction system for temporal relations is typically composed of two modules for (i) temporal relation identification and (ii) temporal relation type classification. As has been discussed in Section 3.5.3, for English language, the TempEval-3 participants approached the task (i) with rule-based (e.g., UTTime), data-driven (e.g., ClearTK) and also hybrid methods (e.g., NavyTime). Meanwhile, for (ii), all participating systems resort to data-driven approaches.\ntemporal relation identification In the EVENTI task, the task of temporal link identification is restricted to event-event (E-E) and event-timex (E-T) pairs within the same sentence. We decided to adopt the hybrid approach for this task. First, we considered all combinations of E-E and E-T pairs within the same sentence (in a forward manner) as candidate temporal links. For example, if we have a sentence with entity order such as “...ev1...ev2...tmx1...ev3...”, the candidate pairs are (ev1, ev2), (ev1, tmx1), (ev1, ev3), (ev2, tmx1), (ev2, ev3) and (ev3, tmx1).\nNext, in order to filter the candidate temporal links, we trained a classifier to decide whether a given E-E or E-T pair is considered as having a temporal link (REL) or not (O). The classification models are built in the same way as in classifying the temporal relation types, using the same set of features, which will be explained in the following section.\ntemporal relation type classification A classification model is trained for each type of entity pair (E-E and E-T), as suggested in several previous works (Chambers, 2013; Mani et al., 2006). Again, YamCha is used to build the classifiers. However, this time,\na feature vector is built for each pair of entities (e1, e2) and not for each token as in the previously mentioned temporal entity extraction tasks.\nGiven an ordered pair of entities (e1, e2) that could be either event/event or event/timex pair, the classifier has to assign a certain label, i.e., one of the 13 TimeML temporal relation types: BEFORE, AFTER, IBEFORE, IAFTER, INCLUDES, IS_INCLUDED, MEASURE, SIMULTANEOUS, BEGINS, BEGUN_BY, ENDS, ENDED_BY and IDENTITY. The overall approach is largely inspired by an existing framework for the classification of temporal relations in English documents (Mirza and Tonelli, 2014b). The implemented features are as follows:\n• String and grammatical features. Tokens, lemmas, PoS tags and NP-chunks of e1 and e2, along with a binary feature indicating whether e1 and e2 have the same PoS tags (only for event/event pairs).\n• Textual context. Pair order (only for event/timex pairs, i.e. event/timex or timex/event), textual order (i.e. the appearance order of e1 and e2 in the text) and entity distance (i.e. the number of entities occurring between e1 and e2).\n• Entity attributes. Event attributes (class, tense, aspect and polarity) 9, and timex type attribute 10 of e1 and e2 as specified in TimeML annotation. Four binary features are used to represent whether e1 and e2 have the same event attributes or not (only for event/event pairs).\n• Dependency information. Dependency relation type existing between e1 and e2, dependency order (i.e. governor-dependent or dependent-governor), and binary features indicating whether e1/e2 is the root of the sentence.\n• Temporal signals. We take into account the list of temporal signals mentioned in Section 10.4.1. Tokens of temporal signals occurring around e1 and e2 and their positions with respect to e1 and e2 (i.e. between e1 and e2, before e1, or at the beginning of the sentence) are used as features.\nIn order to provide the classifier with more data to learn from, we bootstrap the training data with inverse relations (e.g., BEFORE/AFTER). By switching the order of the entities in a given pair and labelling the pair with the inverse relation type, we roughly double the size of the training corpus.\nWe evaluate the system’s performance in classifying the temporal relation types with 5-fold cross-validation scheme on the training data. The evaluation scores are computed\n9 The event attributes tense, aspect and polarity have been annotated using rules based on the EVENTI guidelines and using the morphological analyses of each token. 10 The value attribute tends to decrease the classifier performance as shown in Mirza and Tonelli (2014b), and therefore, it is excluded from the feature set.\n114 multilinguality in temporal processing\nusing the scorer11 provided by EVENTI organizers. Table 10.2 shows the evaluation results of each following system run:\n• Run1: Two classifiers are used to determine the temporal relation types of E-E and E-T pairs.\n• Run2: The same as Run1, but we only consider the frequent relation types as classes for E-E pairs, meaning that we discarded E-E pairs under IBEFORE/IAFTER, BEGINS/ BEGUN_BY, and ENDS/ENDED_BY classes in building the classifier for E-E pairs.\n• Run3: The same as Run2, but we tried to incorporate the TLINK rules for E-T pairs which conforms to specific signal patterns as explained in the task guidelines12. For example, EVENT + da/dalle/dal/dai/dall’ + type=DATE → relType=BEGUN_BY. The E-T pairs matching the patterns are automatically assigned with relation types according to the rules, and do not need to be classified.\nThe number of training instances of E-E pairs under the IBEFORE/IAFTER, BEGINS/ BEGUN_BY, and ENDS/ENDED_BY classes is so few that removing them from the training corpus resulted in a slightly improved performance. Even when these classes are included in the training corpus, the classifier will still fail to classify the E-E pairs into these classes due to the heavily skewed dataset.\nSimilar phenomenon happens with E-T pairs. The classifier tends to classify an E-T pair into only three classes: IS_INCLUDED, INCLUDES (when the pair order is timex/event) or MEASURE. We try to address this issue by incorporating the TLINK rules based on EVENTsignal-TIMEX3 patterns listed in the task guidelines. Unfortunately, this solution does not help improving the system, perhaps because the rules were not strictly followed in the annotation process.\nThese phenomena of imbalanced dataset can also be observed in English TimeML corpora. For English, in the case of E-E pairs, we collapsed IBEFORE/IAFTER and DURING/ DURING_INV relations into BEFORE/AFTER and SIMULTANEOUS, resp; and built a rule-based module to extract possible BEGINS/BEGUN_BY and ENDS/ENDED_BY relations. Note that compared with English, there is a slight difference in It-TimeML annotation guidelines, namely the introduction of the TLINK type MEASURE for event-timex pairs when the timex is of DURATION type.\n10.4.2 EVENTI (Evalita 2014) Evaluation\nFollowing the TempEval-3 evaluation scheme, the EVENTI task included 4 subtasks:\n• Task A: Determine the extent and the normalization of temporal expressions according to the TimeML TIMEX3 tag. Empty TIMEX3 tags, as specified in It-TimeML annotation guidelines, will be taken into account as well.\n• Task B: Determine the extent and the class attribute value of events according to the TimeML EVENT tag.\n• Task C: Determine temporal relations from raw text. This involves performing Task A and Task B, and subsequently identifying pairs of temporal entities connected by a temporal relation (TLINK) and classifying their relation types.\n11 http://sites.google.com/site/eventievalita2014/data-tools 12 http://sites.google.com/site/eventievalita2014/file-cabinet/specificheEvalita_v2.pdf\n10.4 italian 115\n• Task D: Determine the temporal relation types given gold annotated pairs of temporal entities.\ndataset The training data is the Ita-TimeBank released by the task organizers, containing 274 documents and around 112,385 tokens in total. For the evaluation stage, the organizers released two test corpora:\n• Main task corpus, containing 92 documents from Ita-TimeBank.\n• Pilot task corpus, containing 10 documents of historical texts published in “Il Trentino” newspaper by the Italian statesman A. De Gasperi in 1914.\nevaluation results Table 10.3 shows the results of our system, FBK-HLT-time, on the two tasks of the EVENTI challenge, i.e. the main task (MT) and the pilot task (PT), and on the 4 subtasks. For the pilot task we report only the results obtained with the best system runs. For Task A, there were 3 participants and 6 unique runs in total; we also compare our system with HT 1.8, the extended version of HeidelTime (Strötgen et al., 2014), which achieved the highest score for the timex normalization (for the main task). For Task B, C and D, FBK-HLT-time was the only participant.\ntask a : timex extraction For recognizing the extent of timex in news articles, the system achieves 0.827 F1-score using strict-match scheme. The performances for deter-\n116 multilinguality in temporal processing\nmining the timex type and determining the timex value (timex normalization) are 0.8 F1-score and 0.665 F1-score, respectively. The performance for timex normalization is still considerably lower than the state-of-the-art system for English (TimeNorm), with 81.6% F1-score. This suggests that the TimeNorm adaptation for Italian can still be improved, for example by including semester or half-year as a unit.\nFor the pilot task, in recognizing the extent of timex, the system achieves comparable scores with the main task. However, in determining the timex type and value, the accuracies drop considerably. With the assumption that the articles written with a gap of one century differ more at the lexical level than at the syntactic level, our take on this phenomena is that in recognizing the extent of timex, the system depends more on the syntactic features. Meanwhile, in determining the timex type and value, the system relies more on the lexical/semantic features and so the performances of the system decrease when it is applied to historical texts.\nCompared with other participants, for the main task, our system performed best in strict matching and in classifying the timex types. A rule-engineering system, HT 1.8, which extended HeidelTime, performed better in relaxed matching13 with 0.893 F1-score, and in timex normalization with 0.709 F1-score. However, our system performed best for the pilot task, showing that our approach is more capable of domain adaptation.\ntask b : event extraction We observed that event classification performed better with the one-vs-one multi-class strategy (Run1), with a strict F1-score of 0.867 for event detection and an F1-score of 0.671 for event classification, than with the one-vs-rest one (Run2). Looking at the number of predicted events with both classifiers, the second classifier did not classify all the events found (1036 events were not classified). For this reason the precision is slightly better but the recall is much lower.\nOn the pilot task data the results are a bit lower, with a strict F1-score of 0.834 for event detection and an F1-score of 0.604 for event classification. Note that for Run 3 we re-trained the model only on 80% of the data due to a problem while training the model on all the training data.\ntask d : temporal relation extraction The two runs submitted to EVALITA for this subtask, Run1 and Run2, corresponds to Run2 and Run3 explained in Section 10.4.1.3, respectively. For the main task, there was a slight error in the format conversion for Run 2. Hence, we recomputed the scores of Run 2* independently, which results in a slightly better performance compared with Run 1. The system (Run 2*) yields 0.738 F1-score using TempEval-3 evaluation scheme.\nFor the pilot task (post-submission evaluation), both Run 1 and Run 2 have exactly the same F1-scores, i.e. 0.588. This suggests that in the pilot data there is no E-T pair matching the EVENT-signal-TIMEX3 pattern rules listed in the task guidelines. Similar to the classification of timex types, the classifiers tend to rely more on lexical/semantic features, hence, the system performances decrease when they are applied on historical texts.\nAs the dataset is heavily skewed, we have decided to reduce the set of temporal relation types. It would be interesting to see if using patterns or trigger lists as a post-processing step can improve the system in the detection of the under-represented relations. For ex-\n13 This is in line with the reported results for the timex extraction task for English texts in TempEval-3 (Section 3.5.1).\n10.5 indonesian 117\nample, the relation type IAFTER (as a special case of the relation AFTER) can be recognized through the adjective immediato [immediate].\ntask c : temporal awareness This task involves performing Task A and Task B, and subsequently identifying pairs of temporal entities having a TLINK and classifying their temporal relation types (Task D). For this task, we combine the timex extraction system, the 3 system runs for event extraction (Ev), the system for identifying temporal links, and the 2 system runs for classifying temporal relation types (Tr).\nWe found that for both main task and pilot task, the best performing system is the combination of the best run of task B (Ev Run 1) and the best run of task D (Tr Run 1), with 0.341 F1-score and 0.232 F1-score respectively (strict-match evaluation).\n10.5 indonesian\nWe propose a rule-based system for recognizing and normalizing temporal expressions for Indonesian documents. For normalizing temporal expressions, we extend an existing normalizer for English, TimeNorm (Bethard, 2013a). We report some modifications of the tool required for Indonesian language, with respect to the different characteristics of Indonesian temporal expressions compared with English.\nFor recognizing (and determining the types of) temporal expressions, we build a finite state transducer heavily influenced by the crafted TimeNorm’s time grammar for Indonesian. Even though it is shown that the machine learning approach can be as good as ruleengineering for recognizing temporal expressions, since there is no available Indonesian TimeML corpus yet, we resort to the rule-based approach. We believe that annotating sufficient data for the machine learning approach is more time-consuming than hand-crafting a transducer.\nThe evaluation is done on 25 news articles, containing 9,549 tokens (comparable with the TempEval-3 evaluation corpus with 9,833 tokens). The system yields 92.87% F1-score in recognizing temporal expressions and 85.26% F1-score in normalizing them.\n10.5.1 Challenges in Indonesian\nBahasa Indonesia (or simply Indonesian) is the official language of Indonesia, which is the fourth most populous nation in the world. Of its large population, the majority speaks Indonesian, making it one of the most widely spoken languages in the world. Nevertheless, it is still highly under-represented in terms of NLP research. The lack of available annotated corpora makes it difficult to build (and evaluate) automatic NLP systems using data-driven approaches.\nOne of many characteristics that makes Indonesian different from other languages such as English or Italian is that the form of the verb does not change to indicate tense or aspect. A sentence “Saya pergi ke kantor [I go to office]” carries no indication of whether the verb refers to a regular occurrence or to a single occurrence and, if the latter, when it happens in relation to the present. This is inferred from the context within which the utterance is made, by looking at either aspect markers (e.g., sudah [already], sedang [in the process of], akan [will]), or temporal expressions.\nRegarding temporal expressions, there are several differences compared to English including, among others, the order of numbers in dates (e.g. 3/21/2015 vs 21/3/2015), the\n118 multilinguality in temporal processing\npunctuations used (e.g. 5:30 vs 05.30, 2.5 vs 2,5) and the fact that there are only two seasons (rainy or dry) known in Indonesia. Furthermore, since Indonesian is an agglutinative language, some of the temporal expressions of DURATION type contain affixes.\n10.5.2 Timex Extraction System\nThe actual steps in the temporal expression (timex) extraction task are (i) recognizing the extent of a timex, (ii) determining its type, then (iii) normalizing the timex (resolving its value). However, during the development phase, we first develop the system to normalize temporal expressions based on an existing system for English. Then, based on the created time grammar, we develop a finite state transducer to do both recognizing temporal expressions’ extents and determining their types in one step. In the following sections, we will organize the explanation of each module composing our timex extraction system, i.e., timex normalization, timex extent recognition and timex type classification, in such order.\nThe complete system, called IndoTimex14, is implemented in Python and made available for download15. The system takes as input a TimeML document (or a collection of TimeML documents) and gives as output a TimeML document (or a collection of TimeML documents) annotated with temporal expressions (TIMEX3 tags).\n10.5.2.1 Timex Normalization\nTemporal expressions in Indonesian language are quite similar with the ones in English. Therefore, for normalization, we decided to extend TimeNorm (Bethard, 2013a) to cover Indonesian temporal expressions. TimeNorm is a tool for normalizing temporal expressions based on a synchronous context free grammar, developed in Scala. Given an anchor time, TimeNorm parses time expressions and returns all possible normalized values of the expressions. A temporal expressions is parsed with an extended CYK+ algorithm, then converted to its normalized form by applying the operators recursively. The normalization value is determined as specified in TIDES(02).\n14 http://paramitamirza.ml/indotimex/ 15 http://github.com/paramitamirza/IndoTimex\nThe time grammar in TimeNorm, based on a synchronous context free grammar formalism, allows two trees (one in the source language and one in the target language) to be constructed simultaneously. Figure 10.2 shows a synchronous parse for tiga hari yang lalu [three days ago], where Figure 10.2a is the source side (an Indonesian expression), Figure 10.2b is the target side (a temporal operator expression), and the alignment is shown via subscripts.\nExtending TimeNorm for a new language is very straightforward, we just need to translate the existing time grammar for English into Indonesian. However, there are some differences on expressing time in American English and Indonesian, as shown in Table 10.4. Therefore, several adjustments are required to cope with those differences, as well as to comply with the TIDES(02) standard:\n• Dates are always in the Day-Month-Year order.\n• Roman numerals are added since they are used in describing century (e.g. abad XVII [17th century]).\n• The expression for time is written with dot (.) instead of colon (:), and the same applies for time duration.\n• The ‘am/pm’ expression is not used since hours range from 0 to 24.\n• Comma (,) is used as the decimal separator instead of dot (.).\n• There is no distinction between plural and singular time units following quantifiers (e.g. tahun [year] denotes both year and years).\n120 multilinguality in temporal processing\n• There are three time zones in Indonesia, namely WIB (UTC+07:00), WITA (UTC+08:00) and WIT (UTC+09:00). In normalizing the temporal expression, we decided to ignore the time zones even though they are included in the extents.\n• Indonesia has only two seasons, musim hujan [rainy season] and musim kemarau [dry season], which are not available in the standard. Hence, we normalize musim hujan and musim kemarau as Winter and Summer respectively.\n• Sore and petang could mean both ‘afternoon’ and ‘evening’. We decided to normalize sore as Afternoon, while petang as Evening.\n• The ‘DayOfWeek malam’ [DayOfWeek night] expression, can also be expressed with ‘malam DayOfWeek-after’, e.g. malam Minggu [night (of) Sunday] means Sabtu malam [Saturday night]. A special rule is needed to handle this case, which is quite similar with the rule for ‘Christmas Eve’ or ‘New Year’s Eve’.\nApart from the grammar, there are several modifications of the TimeNorm code in order to support Indonesian temporal expressions:\n• In Indonesian language, being an agglutinative language, some temporal expressions contain affixes. In the numerals, the prefix se- when attached to a Unit (e.g. tahun [year]) or a PartOfDay (e.g. pagi [morning]) means one. Hence, setahun denotes a year and sepagian (with suffix -an) a whole morning. Moreover, to make a Unit become plural, the prefix ber- is added to the reduplicated Unit, e.g. berjam-jam [hours]. In order to have a concise grammar, we need to isolate the affixes from the root expressions before giving the temporal expressions to the parser.\n• The term minggu is ambiguous, which could mean ‘week’ (a Unit) or ‘Sunday’ (a DayOfWeek). However, as in English, a DayOfWeek is always capitalised. Therefore, we disambiguate the term according to this rule before giving it to the parser.\n10.5.2.2 Recognizing Temporal Expressions and Determining The Types\nBased on the time grammar for TimeNorm, we construct regular expression rules to label tokens with [Int], [Unit] or [Field], e.g. hari→ Unit. The defined labels are as follows: • [Int:Numeral], e.g. satu [one], puluh [(times) ten] • [Int:Digit], e.g. 12, 1,5, XVII [17] • [Int:Ordinal], e.g. ke-2 [2nd], ketiga [third], ke XVII [17th] • [Unit], e.g. hari [day], musim [season] • [Unit:Duration], e.g. setahun [a year], berjam-jam [hours] • [Field:Year], e.g. ’86, 2015 • [Field:Decade], e.g. 70-an [70’s], limapuluhan [fifties] • [Field:Time], e.g. 08.30, WIB • [Field:Date], e.g. 10/01/2015 • [Field:PartOfDay], e.g. pagi [morning] • [Field:DayOfWeek], e.g. Selasa [Tuesday] • [Field:MonthOfYear], e.g. Januari [January] • [Field:SeasonOfYear], e.g. kemarau [dry], gugur [autumn]\n10.5 indonesian 121\n• [Field:NamedDay], e.g. Natal [Christmas] In expressing Time and Date, some tokens are commonly used before the temporal expression, which by themselves cannot be considered a temporal expression (e.g. pukul 08.30 [08:30], tanggal 10 Januari [January 10]). Hence, we define labels for those tokens as follows: • [Pre:Time], i.e. pukul • [Pre:Date], i.e. tanggal\nApart from [Int], [Unit] and [Field], some tokens can be considered a single temporal expression. Moreover, some tokens preceding or following [Int], [Unit] and [Field] can be included in the temporal expression extent to further define the expression. Such tokens are labelled as follows: • [Date:Solo], e.g. dulu [in the past], kini [now] • [Date:Begin], e.g. masa [period], zaman [times] (they are usually combined with other\ntokens, e.g. masa lalu [the past], zaman sekarang [nowadays]) • [Duration:Solo], e.g. sebentar [for a while] • [Quantifier], e.g. beberapa [a few] • [Modifier], e.g. sekitar [around], penghujung [the end of] • [Current], e.g. ini [this], sekarang [now] • [Earlier], e.g. kemarin [yesterday], lalu [last] • [Later], e.g. besok [tomorrow], mendatang [next] • [Set], e.g. setiap [each], sehari-hari [daily]\nWe then build a deterministic finite state transducer (FST) to recognize a temporal expression and to label it with one of the TIMEX3 types, i.e. date, duration, time and set. We define the FST T = (Q,Σ, Γ , δ,ω,q0, F) such that:\n• Q is a finite set of states;\n• Σ as the input alphabet is a finite set of previously defined token labels {[Int:Numeral], [Int:Digit], ..., [Later], [Set]} ∪ {yang, ke, akan, dan} (i.e. function words that are often used in temporal expressions);\n• Γ as the output alphabet is a finite set of temporal expression types {date, duration, time, set};\n• δ : Q× Σ→ Q is the transition function;\n• ω : Q× Σ→ Γ is the output function;\nWe used OpenFST16 to minimize the built FST, resulting in a deterministic FST with 26 states (of which 8 are final states) and 177 arcs. The complete FST is specified in a text file using the AT&T FSM format17, and visualized as a transition diagram using OpenFST18.\n10.5.3 Temporal Tagging\nGiven a document in the TimeML annotation format, we first parse the document creation time (DCT) inside the DCT tag and the document content inside the TEXT tag. The content is further tokenized following a simple splitting rule with white-spaces and punctuations as delimiters, except for tokens containing digits (e.g. 08.30, ’86, ke-2, 70-an, 10/01/2015).\nGiven a list of tokens and the document creation time, the tagging algorithm goes as described in Appendix A.3.\n10.5.4 Evaluation\ndataset The dataset comprises 75 news articles taken from www.kompas.com, and is made available for download19. The preparation of the dataset includes cleaning the HTML files and converting the text into the TimeML document format. As shown in Table 10.5, during the development phase only 50 news articles are used to develop the time grammar and the transducer. The rest 25 articles are manually annotated with temporal expressions and used for the evaluation phase.\nevaluation results Table 10.6 shows the performance results of each task in temporal expression tagging, including temporal expression recognition and normalization.\nThere are 211 temporal expressions identified by our method in the evaluation data. With 189 correctly identified entities, 22 false positives and 7 false negatives, the system yields 89.57% precision, 96.43% recall and 92.87% F1-score.\nAmong the false positives, 11 entities which are actually flight numbers (e.g. 8501) are tagged as date, while 5 entities which are part of a geographic coordinate (e.g. 08\n16 http://www.openfst.org 17 http://github.com/paramitamirza/IndoTimex/blob/master/lib/fst/timex.fst 18 http://github.com/paramitamirza/IndoTimex/blob/master/lib/fst/timex.pdf 19 http://github.com/paramitamirza/IndoTimex/tree/master/dataset\n10.5 indonesian 123\nderajat 50 menit 43 detik selatan [08 degrees 50 minutes 43 seconds south] or 03.22.46 Lintang Selatan [3◦ 22’ 46” South]) are tagged as duration or time. There are 6 entities identified incorrectly due to the ambiguous nature of dulu/dahulu, which could mean ‘in the past’ or ‘first’ (as in \"John wants to say goodbye first before leaving\") depending on the context.\nIntroducing a threshold for the reasonable maximum year number that could appear in a text (e.g. year 3000) will decrease the number of falsely extracted flight numbers (e.g. 8501) because it is in the same format as a year. It might also help to include temporal signals such as pada [on/at] or selama [during] in the transducer, to ensure that the following tokens are indeed temporal expressions.\nWe could also include in the transducer the expressions that can rule out the following tokens to be part of temporal expressions. For example, if we find derajat [degree], we can make sure that even though the following tokens are usually part of temporal expressions (i.e. menit [minutes] and detik [seconds]), the transducer will end up in a non-final state. The same strategy could be applied if the following tokens denote geographical directions such as Lintang Selatan [South] or Bujur Timur [East].\nThe false negatives include esok hari [tomorrow] and setengah hari [half a day], which are due to the incomplete transducer. Another cases are jauh-jauh hari sebelumnya [many days before] and 2-3 menit [2-3 minutes], which are due to the incomplete regular expressions to recognize indefinite quantifiers for expressing durations (i.e. jauh-jauh [many] and 2-3).\nIn determining the temporal expression types (i.e. date, time, duration and set), the system achieves a perfect accuracy. Meanwhile, for normalizing the correctly identified temporal expressions, the system achieves 85.71% accuracy, resulting in 85.26% F1-score.\nMost incorrect cases in the normalization task are because of the wrong anchor time, since we always use the document creation time as the anchor time in resolving the values. For example, in the documents, the expressions saat itu [that moment] mostly refer to the previously mentioned temporal expressions of time type.\nThere are 3 temporal expressions of which TimeNorm failed to normalize, including saat yang sama [the same moment], tanggal 24 kemarin [24th yesterday] and pukul 13.25 kemudian [13:25 later].\nAs a future improvement, we consider including temporal signals (e.g. pada [on/at], selama [during]) in the transducer to make sure that the following tokens are indeed part of temporal expressions, as well as including expressions that rule out the following or preceding tokens to be part of temporal expressions (e.g. derajat [degree], Lintang Selatan [South]). This strategy might be useful to reduce the number of false positives.\n124 multilinguality in temporal processing\n10.6 conclusions\ntimex recognition In the EVENTI task, for recognizing temporal expressions in Italian texts, our statistical approach performed best in strict matching and in classifying the timex types, with 82.7% F1-score and 80% F1-score respectively. Furthermore, our system performed best in the pilot task, i.e. on historical texts, showing that our approach is robust with respect to domain changes.\nWe have also developed a rule-based system for recognizing temporal expressions in Indonesian documents. Even though the system could still be improved, particularly by completing the regular expression rules and the finite state transducer, the system achieves good results of 92.87% F1-score. The built framework can be easily extended to accommodate other low resource languages, requiring only modifications of the regular expression rules and the finite state transducer.\ntimex normalization For timex normalization, we have extended an existing tool for English, TimeNorm, for both Italian and Indonesian languages. The adaptation is quite straightforward, because how time is expressed is more or less the same in all languages, involving time units (e.g. week, hour and relative time functions (e.g. two days ago, now)). There are subtle differences depending on local conventions that need to be addressed such as punctuations used, day-month order, how to tell the time (e.g. half past eight vs otto e mezzo [eight and a half] for Italian vs setengah sembilan [half (to) nine] for Indonesian), etc. Few modifications of the TimeNorm grammar and code are required in order to deal with the characteristics of Italian and Indonesian temporal expressions.\nFor Italian, the performance for timex normalization is still considerably lower than the state-of-the-art system for English, UWTime (Lee et al., 2014), i.e. 66.5% vs 82.4% F1scores. This suggests that the TimeNorm adaptation for Italian can still be improved, for example by including semester or half-year as a unit. For Indonesian, the system can achieve 85.26% F1-score in normalizing temporal expressions. However, some improvements are required to cope with superfluous temporal expressions such as tanggal 24 kemarin [(on) 24th yesterday] and pukul 13.25 kemudian [(at) 13:25 later].\nFurthermore, for both Italian and Indonesian, we could implement different strategies to select the correct anchor time for some temporal expressions, instead of always using the document creation time. The best approximation would be to use the preceding temporal expressions of the same type (if any) as the anchor time.\nevent and temporal relation extraction In EVENTI, the individual performances of our Italian event extraction system and temporal relation extraction system were quite good, with 86.7% F1-score and 73.3% F1-score resp. However, for the complete end-to-end temporal information processing the temporal awareness score was only 34.1%. This result is quite similar to the TempEval-3 results for English, most probably related to the sparse annotation of temporal relations in the dataset. Without any specific adaptation to historical text, our system yields comparable results. In a close future, our system for temporal information processing of Italian texts will be included in the TextPro tools suite.\nIn general, the work confirms that statistical approaches for temporal information processing are robust across languages, given the availability of annotated texts and natural\n10.6 conclusions 125\nlanguage processing tools (e.g. PoS tagger, dependency parser) for the language of interest. However, some tasks are still best solved with rule-based methods, e.g. timex normalization. Furthermore, for low-resource languages such as Indonesian, statistical approaches are more time consuming to implement, since we should first develop annotated data and basic NLP tools.\nCross-lingual annotation or cross-lingual model transfer approaches are often proposed to solve NLP tasks for low-resource languages. If the necessary resources are already available for a closely related language, they can be utilized to facilitate the construction of a model or annotation for the target language. For example, Nakov and Ng (2009) utilized Malay language to improve statistical machine translation for Indonesian→ English, considering that more resources are available for Malay, and that Malay and Indonesian are closely related.\n11 C O N C L U S I O N S A N D O U T L O O K\n11.1 conclusions\nThe goal of temporal information processing is to construct structured information about events and temporal-causal relations between them, given the fact that news and narrative texts often describe dynamic information of events that occur in a particular temporal order or causal structure. In this thesis, building an integrated system for extracting such temporal-causal information from text has been our main focus. Furthermore, since temporal and causal relations are closely related, given the presumed constraint of event precedence in causality, we explored ways to exploit this presumption to improve the performance of our integrated temporal and causal relation extraction system.\nIn Chapter 2, besides some natural language processing foundations, we have provided background information about machine learning approaches that are used in this thesis. Chapter 3 introduced the task of temporal information processing, and outlined the state-of-the-art methods for extracting temporal information from text. Based on results reported on an evaluation campaign related to temporal information processing, i.e. TempEval-3, we highlighted the fact that the overall performance of end-to-end temporal information processing systems from raw text suffers due to the lacking temporal relation extraction systems, with 36% F1-score. This was the main reason underlying our choice to focus our attention on the extraction of relations between events.\ntemporal relations In Chapter 4 we have described our approach to build a hybrid temporal relation extraction system, TempRelPro, which combines rule-based and machine learning modules in a sieve-based architecture inspired by CAEVO (Chambers et al., 2014). However, our architecture is arguably simpler and more efficient than CAEVO since (i) the temporal closure inference module is run only once and (ii) we use less classifiers in general. We have evaluated TempRelPro in three different evaluation settings, i.e. TempEval-3, TimeBank-Dense and QA-TempEval, in which TempRelPro is shown to achieve state-of-the-art performances. However, TempRelPro still performs poorly in labelling the temporal relation types of event-event pairs, compared to its performance for pairs of temporal expressions (timex-timex) and event-timex.\ncausal relations One direction to address this issue is to build a causal relation extraction system, considering the temporal constraint of event precedence in causality. Apart from being an effort to improve the temporal relation extraction system, the extraction of causal chains of events in a story can also benefit question answering and decision support systems, among others.\nLooking at the existing resources for causality annotation, we could not find one that provides a comprehensive account of how causality can be expressed in a text. Therefore, we have presented in Chapter 5 our guidelines for annotating explicit causality between events, inheriting the concept of events, event relations and signals in TimeML, without limiting our effort to specific connectives. Our annotation effort on TimeBank—a freely available TimeML corpus that already contains temporal entity and temporal rela-\n127\n128 conclusions and outlook\ntion annotation—resulted in Causal-TimeBank. Causal-TimeBank contains 318 causal links, much less than the 2,519 temporal links between events found in the corpus. This shows that causal relations, particularly explicit ones, appear relatively rarely in texts. Our analysis on the corpus statistics sheds light on the behaviour of causal markers in texts. For instance, there are several ambiguous causal signals and causative verbs, which occur abundantly but do not always carry a causation sense.\nOur next step was to exploit this corpus and the obtained corpus statistics for building (and evaluating) a causal relation extraction system. Chapter 6 provided details on our hybrid approach for building a system for identifying causal links between events, CauseRelPro, making use of the previously mentioned Causal-TimeBank. Again, we adopted a sieve-based architecture for combining the rule-based and machine-learned modules, which is proven to benefit temporal relation extraction. An evaluation of CauseRelPro using the Causal-TimeBank corpus in stratified 10-fold cross-validation resulted in 40.95% F1-score, much better than our previous data-driven system for causal relations reported in Mirza and Tonelli (2014a) with 33.88% F1-score.\nintegrated temporal and causal relation extraction system In Chapter 7, following the analysis of the interaction between temporal and (explicit) causal relations in texts, we presented our approach for integrating our temporal and causal relation extraction systems. The integrated system, CATENA—CAusal and Temporal relation Extraction from NAtural language texts—, is a combination of TempRelPro and CauseRelPro, exploiting the presumption about event precedence when two events are connected by causality. The interaction between TempRelPro and CauseRelPro in the integrated architecture is realized by (i) using the output of TempRelPro (temporal link labels) as features for CauseRelPro, and (ii) using the output of CauseRelPro as a post-editing method for correcting the mislabelled output of TempRelPro.\nConfirming the finding of several previous works (Bethard and Martin, 2008; Mirza and Tonelli, 2014a; Rink et al., 2010), using temporal information as features boosted the performance of our causal relation extraction system. Through an ablation test, we found that without temporal link labels as features, the F1-score drops from 62% to 57%, with a significant recall drop from 54% to 46%. We also found that the post-editing rules would improve the output of temporal relation labelling, even though this phenomenon is not captured statistically in the TempEval-3 evaluation due to the sparse annotation of the evaluation corpus. Nevertheless, explicit causality found in a text is very infrequent, and hence, cannot contribute much in improving the performance of the temporal relation extraction system.\nword embeddings While morpho-syntactic, context and time-value information features are sufficient for determining the temporal order of timex-timex and event-timex pairs, the lack of lexical-semantic information about event words may contribute to TempRelPro’s low performance on event-event pairs. Chapter 8 discusses our preliminary investigation into the potentiality of exploiting word embeddings for alleviating this issue, specifically in using word embeddings as lexical-semantic features for the supervised temporal relation type classifier included in TempRelPro.\nWe have compared two pre-trained word vectors from GloVe and Word2Vec, and found that Word2Vec embeddings yield better results. We also found that concatenating the two\n11.1 conclusions 129\nhead word vectors of event pairs is the best combination method, although subtraction may also bring advantages for some relation types such as IDENTITY or BEGINS/BEGUN_BY.\nIn a 10-fold cross-validation setting, we found that combining word embeddings and traditional features results in significant improvement. However, using the same feature vector evaluated on the TempEval-3 evaluation corpus, the classifier’s performance does not improve in general, despite of performance gains in identifying several relation types, i.e. IDENTITY, SIMULTANEOUS and IS_INCLUDED.\ntraining data expansion In Chapter 9 we have presented our investigation into the effect of training data expansion for temporal and causal relation extraction. In particular, we investigated the impact of (i) temporal reasoning on demand for temporal relation type classification and (ii) self-training for causal relation extraction.\nIn (i), our objective is to improve not only the quantity but also the quality of training data for temporal relation type classification. We made use of a temporal reasoner module that checks the temporal graph consistency and infers new temporal links, based on temporal closure inference on the initial set of annotated temporal links in a document. However, the temporal reasoner is only run when it is estimated to be effective, hence the term ‘temporal reasoning on demand’. According to our experiments, deduction may be beneficial when the estimated number of deducible temporal links falls below a certain threshold, which is experimentally assessed. With this setting, removing inconsistent documents prior to deduction can also have a positive impact on classification performances.\nIn (ii), we employed self-training to bootstrap the training data, along with a causallink propagation method. The propagation method relies on an assumption that news texts often describe the same set of events by rewording the underlying story. Thus, if we found a causal relation between two events in a news, the same relation holds every time the two events are mentioned in similar news, in which the causality may be expressed differently than in the original news. Our experiments show that self-training and causallink propagation can boost the performance of a causal relation extraction system, albeit our simplified implementation of self-training (only one iteration is performed), and the size of the unlabelled dataset being not significantly larger than the original training set. Despite the limited number of newly acquired training examples (324 through selftraining and 32 through causal link propagation), they still have a significant impact on the classifier performance, since the original training corpus contains only 318 causal links.\nmultilinguality Finally, Chapter 10 summarizes our efforts in the adaptation of temporal information processing for texts in languages other than English, i.e. Italian and Indonesian. In general, the work confirms that statistical approaches for temporal information processing are robust across languages, given the availability of annotated texts and natural language processing tools for the language of interest. However, some tasks are still best solved with rule-based methods, e.g. timex normalization. Furthermore, for low-resource languages such as Indonesian, statistical approaches require more efforts since we should first construct annotated data and basic NLP tools.\n130 conclusions and outlook\n11.2 ongoing and future work\nimplicit temporal and causal relations The identification of temporal and causal relations between two events is relatively straightforward given an explicit marker (e.g. before, because) connecting the two events, tense-aspect-modality information embedded in the event words or specific syntactic construction involving the two events. It becomes more challenging when such an overt indicator is lacking, which is often the case when two events take place in different sentences. In the TempEval-3 evaluation corpus, 32.76% of the event pairs do not occur in the same sentences. Furthermore, our CausalTimeBank corpus only contains 318 causal links; more could be found if we do not limit our annotation to overtly expressed causal links (via causal signals and causal verbs) and also consider the implicit ones.\nOur preliminary work with word embeddings is motivated by this issue, since most research on implicit relations incorporate word-based information in the form of word pair features. The results of our experiments in Chapter 8 shed some light on how word embeddings can potentially improve a classifier performance for temporal ordering of events. We have seen different advantages brought by different ways of combining word vectors, i.e., concatenation works well for identifying relations such as BEFORE/AFTER and INCLUDES/IS_INCLUDED, whereas subtraction may benefit relations such as IDENTITY and BEGINS/BEGUN_BY. We would like to take advantage of ensemble learning, particularly stacking, to learn a super-classifier that decides the best label for an event pair, given different predictions by other classifiers. In this case, the other classifiers could be classification models trained on traditional feature vector, concatenated word vectors and/or subtracted word vectors.\nWe would also like to apply the same method to extract implicit causality between events. However, given the limited amount of training data for causal relations in CausalTimeBank, it may be difficult to obtain a robust classification model based on word pair features. There are several resources that we can use to expand our training data, for instance, causality annotated between nominals (Girju et al., 2007), the parallel temporalcausal corpus by Bethard et al. (2008), and causality annotated between verbal event pairs (Do et al., 2011; Riaz and Girju, 2013). Another way to expand the training data would be to run the rule-based module in CauseRelPro, considering its high precision, on boundless unlabelled data to retrieve significant amount of event pairs connected by causal links for training the classification models with word embeddings as features.\nFurthermore, instead of using general-purpose word embeddings, several works presented methods for building task-specific word embeddings (Boros et al., 2014; Hashimoto et al., 2015; Nguyen and Grishman, 2014; Tang et al., 2014), which may also be beneficial for temporal ordering and causality extraction task.\nPart I\nA P P E N D I X\nA A P P E N D I X\na.1 temporal signal lists\nEvent-related Signals Timex-related Signals\nText Cluster Text Cluster\njust as soon as as soon as at at\njust as long as as long as by by\nat the very moment at the same time in in\nat the same time at the same time on on\nso far as as long as for for\nread out by followed by by by\nprior to making prior to from from\nin advance of prior to to to\nimmediately followed by followed by during during\nbeing pursued by followed by between between\nbefore proceeding with prior to after after\nbefore proceeding to prior to before before\nat one time at the same time up to a maximum of up to\nas swiftly as as soon as to a maximum of up to\nas speedily as as soon as up to up to\nas soon as as soon as up till up to\nas rapidly as as soon as within within\nas quickly as as soon as upon after\nas quick as as soon as until until\nas promptly as as soon as under within\nas much as as long as till until\nas long as as long as since since\nas fast as as soon as still still\nas far as as long as throughout during\nas expeditiously as as soon as through during\nas early as as soon as recently recently\nread by followed by previously formerly\npursued by followed by previous former\nprior to prior to preliminary early\nmonitoring of followed by preceding former\nmonitored by followed by over over\nin parallel at the same time next next\nin conjunction at the same time latterly recently\nfollowed by followed by later later\nfollow-up on followed by lately lately\nfollow-up of followed by just immediately\nfirst of prior to initial early\nattended by followed by further later\napplied by followed by formerly formerly\nahead of prior to former former\nContinued on next page\n133\n134 appendix\nA.1 temporal signal lists 135\n136 appendix\na.2 causal verb & signal lists\nCausal Verbs Causal Signals\nText Cluster Text Cluster\nbribe CAUSE Pattern\ncause CAUSE on ([athe]+\\\\s)?([a-z]+\\\\s)?grounds? of because of\ncompel CAUSE on ([athe]+\\\\s)?([a-z]+\\\\s)?basis of because of\nconvince CAUSE in ([a-z]+\\\\s)?light of because of\ndrive CAUSE in ([a-z]+\\\\s)?pursuance of because of\nimpel CAUSE on ([athe]+\\\\s)?([a-z]+\\\\s)?background of because of\nincite CAUSE on ([a-z]+\\\\s)?account of because of\ninduce CAUSE for ([athe]+\\\\s)?sake of because of\ninfluence CAUSE by ([a-z]+\\\\s)?virtue of because of\ninspire CAUSE by ([a-z]+\\\\s)?reason of because of\npersuade CAUSE by ([a-z]+\\\\s)?cause of because of\nprompt CAUSE because of the ([a-z]+\\\\s)?need to due to\npush CAUSE due to the ([a-z]+\\\\s)?need to due to\nforce CAUSE due ([a-z]+\\\\s)?to due to\nenforce CAUSE owing ([a-z]+\\\\s)?to due to\nrouse CAUSE thanks ([in]+\\\\s)?([a-z]+\\\\s)?to due to\nset CAUSE thanks ([a-z]+\\\\s)?to due to\nspur CAUSE under the ([a-z]+\\\\s)?influence of in consequence of\nstart CAUSE in ([athe]+\\\\s)?([a-z]+\\\\s)?wake of in consequence of\nstimulate CAUSE in ([anthe]+\\\\s)?([a-z]+\\\\s)?aftermath of in consequence of\nentail CAUSE in ([athe]+\\\\s)?([a-z]+\\\\s)?wake in consequence\ngenerate CAUSE in ([athe]+\\\\s)?([a-z]+\\\\s)?aftermath in consequence\ntrigger CAUSE in ([a-z]+\\\\s)?answer to in response to\nspark CAUSE in ([a-z]+\\\\s)?response to in response to\nfuel CAUSE in ([a-z]+\\\\s)?responding to in response to\nignite CAUSE in ([a-z]+\\\\s)?replying to in response to\nreignite CAUSE in ([a-z]+\\\\s)?reaction to in response to\ninflict CAUSE in ([a-z]+\\\\s)?retaliation for in exchange for\nprovoke CAUSE in ([a-z]+\\\\s)?exchange for in exchange for\nhave CAUSE-AMBIGUOUS in ([a-z]+\\\\s)?order to in order to\nmove CAUSE-AMBIGUOUS as ([athe]+\\\\s)?([a-z]+\\\\s)?result of as a result of\nget CAUSE-AMBIGUOUS as ([athe]+\\\\s)?([a-z]+\\\\s)?reaction to as a result of\nmake CAUSE-AMBIGUOUS as ([anthe]+\\\\s)?([a-z]+\\\\s)?outcome of as a result of\nsend CAUSE-AMBIGUOUS as ([athe]+\\\\s)?([a-z]+\\\\s)?follow-up to as a result of\naid ENABLE as ([anthe]+\\\\s)?([a-z]+\\\\s)?effect of as a result of\nallow ENABLE as ([athe]+\\\\s)?([a-z]+\\\\s)?consequence of as a result of\nauthorize ENABLE as ([athe]+\\\\s)?([a-z]+\\\\s)?result as a result\nauthorise ENABLE as ([athe]+\\\\s)?([a-z]+\\\\s)?reaction as a result\nempower ENABLE as ([anthe]+\\\\s)?([a-z]+\\\\s)?outcome as a result\nenable ENABLE as ([athe]+\\\\s)?([a-z]+\\\\s)?follow-up as a result\nensure ENABLE as ([anthe]+\\\\s)?([a-z]+\\\\s)?effect as a result\nfacilitate ENABLE as ([athe]+\\\\s)?([a-z]+\\\\s)?consequence as a result\nguarantee ENABLE for th[eioa][st]e* ([a-z]+\\\\s)?reasons? for reason permit ENABLE it [i’]s ([a-z]+\\\\s)*why is why provide ENABLE th[ai][st] (, ([a-z]+\\\\s)+, )*[i’]s ([a-z]+\\\\s)*why is why\nContinued on next page\nA.2 causal verb & signal lists 137\n138 appendix\na.3 temporal tagging algorithm\nInput: list of tokens tok, DCT dct, FST T Output: string out of text annotated with TIMEX3 tags\nRecognizing temporal expressions 1: starts← empty dictionary 2: ends← empty list 3: timex← empty dictionary 4: start← −1 5: end← −1 6: tmx_type← O 7: i← 0 8: while i < length of tok do 9: tlabel← token label of tok[i] based on regex 10: if start is −1 then 11: if tlabel is in input labels of T .initial then 12: start← i 13: (q, type)← T .transition(q0, tlabel) 14: if q is in T .final then 15: end← i 16: tmx_type← type 17: end if 18: end if 19: else 20: if T .transition(q, tlabel) is not null then 21: (q, type)← T .transition(q, tlabel) 22: if q is in T .final then 23: end← i 24: tmx_type← type 25: end if 26: else 27: if start > −1 and end > −1 then 28: starts[start]← tmx_type 29: Add end to ends 30: timex[start]← tok[start...end] 31: end if 32: start← −1 33: end← −1 34: tmx_type← O 35: end if 36: end if 37: i← i+ 1 38: end while\nA.3 temporal tagging algorithm 139\nNormalizing temporal expressions 39: timex_norm← empty dictionary 40: for key in timex do 41: timex_norm[key]← normalize(timex[key],dct) 42: end for\nTIMEX3 tagging 43: out← empty string 44: tid← 1 45: for i = 0 to length of tok do 46: if i in keys of starts then 47: timex_id← tid 48: timex_type← starts[i] 49: timex_value← timex_norm[i] 50: out← out+ TIMEX3 opening tag (with timex_id, timex_type and timex_value) + tok[i] + space 51: tid← tid+ 1 52: else if i in ends then 53: out← out + tok[i] + TIMEX3 closing tag + space 54: else 55: out← out + tok[i] + space 56: end if 57: end for 58: return out\nB I B L I O G R A P H Y\nACE (2005). The ACE 2005 (ACE05) Evaluation Plan. url: http://www.itl.nist.gov/iad/ mig/tests/ace/2005/doc/ace05-evalplan.v3.pdf. Agirre, Eneko, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa (2009). “A Study on Similarity and Relatedness Using Distributional and WordNetbased Approaches.” In: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Boulder, Colorado: Association for Computational Linguistics, pp. 19–27. url: http: //www.aclweb.org/anthology/N/N09/N09-1003. Allan, James, ed. (2002). Topic Detection and Tracking: Event-based Information Organization. Norwell, MA, USA: Kluwer Academic Publishers. isbn: 0-7923-7664-1. Allen, James F. (Nov. 1983). “Maintaining Knowledge About Temporal Intervals.” In: Commun. ACM 26.11, pp. 832–843. issn: 0001-0782. doi: 10.1145/182.358434. url: http: //doi.acm.org/10.1145/182.358434. Baker, Collin F., Charles J. Fillmore, and John B. Lowe (1998). “The Berkeley FrameNet Project.” In: Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1. Montreal, Quebec, Canada: Association for Computational Linguistics, pp. 86–90. doi: 10.3115/980845.980860. url: http://www.aclweb.org/anthology/P98-1013. Baroni, Marco and Alessandro Lenci (Oct. 2010). “Distributional memory: A general framework for corpus-based semantics.” In: Computational Linguistics 36.4, pp. 673– 721. issn: 0891-2017. doi: 10.1162/coli\\_a\\_00016. url: http://dx.doi.org/10. 1162/coli\\_a\\_00016. Baroni, Marco, Georgiana Dinu, and Germán Kruszewski (2014). “Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.” In: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Baltimore, Maryland: Association for Computational Linguistics, pp. 238–247. url: http://www.aclweb.org/anthology/P14-1023. Bartalesi Lenzi, Valentina, Giovanni Moretti, and Rachele Sprugnoli (2012). “CAT: the CELCT Annotation Tool.” In: Proceedings of LREC 2012. Baum, Leonard E. (1972). “An Inequality and Associated Maximization Technique in Statistical Estimation for Probabilistic Functions of Markov Processes.” In: Inequalities III: Proceedings of the Third Symposium on Inequalities. Ed. by Oved Shisha. University of California, Los Angeles: Academic Press, pp. 1–8. Bengio, Yoshua, Réjean Ducharme, Pascal Vincent, and Christian Janvin (Mar. 2003). “A Neural Probabilistic Language Model.” In: J. Mach. Learn. Res. 3, pp. 1137–1155. issn: 1532-4435. url: http://dl.acm.org/citation.cfm?id=944919.944966. Berger, Adam L., Vincent J. Della Pietra, and Stephen A. Della Pietra (Mar. 1996). “A Maximum Entropy Approach to Natural Language Processing.” In: Comput. Linguist. 22.1, pp. 39–71. issn: 0891-2017. Bethard, Steven (2013a). “A Synchronous Context Free Grammar for Time Normalization.” In: Proceedings of the 2013 Conference on Empirical Methods in Natural Language\n141\n142 Bibliography\nProcessing. Seattle, Washington, USA, pp. 821–826. url: http://www.aclweb.org/ anthology/D13-1078.\nBethard, Steven (2013b). “ClearTK-TimeML: A minimalist approach to TempEval 2013.” In: Proceedings of the Seventh International Workshop on Semantic Evaluation. SemEval ’13. Atlanta, Georgia, USA: Association for Computational Linguistics. url: http://www. aclweb.org/anthology/S13-2002. Bethard, Steven and James H. Martin (2008). “Learning Semantic Links from a Corpus of Parallel Temporal and Causal Relations.” In: Proceedings of ACL-08: HLT, Short Papers. Columbus, Ohio: Association for Computational Linguistics, pp. 177–180. url: http: //www.aclweb.org/anthology/P/P08/P08-2045. Bethard, Steven, William Corvey, Sara Klingenstein, and James H. Martin (2008). “Building a Corpus of Temporal-Causal Structure.” In: Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08). Ed. by Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, and Daniel Tapias. http://www.lrec-conf.org/proceedings/lrec2008/. Marrakech, Morocco: European Language Resources Association (ELRA). isbn: 2-9517408- 4-0. Bethard, Steven, Leon Derczynski, Guergana Savova, James Pustejovsky, and Marc Verhagen (2015). “SemEval-2015 Task 6: Clinical TempEval.” In: Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015). Denver, Colorado: Association for Computational Linguistics, pp. 806–814. url: http://www.aclweb.org/ anthology/S15-2136. Bjorkelund, Anders, Bernd Bohnet, Love Hafdell, and Pierre Nugues (2010). “A HighPerformance Syntactic and Semantic Dependency Parser.” In: Coling 2010: Demonstrations. Beijing, China: Coling 2010 Organizing Committee, pp. 33–36. url: http: //www.aclweb.org/anthology/C10-3009. Bonial, Claire, Olga Babko-Malaya, Jinho D. Choi, Jena Hwang, and Martha Palmer (2010b). PropBank Annotation Guidelines, Version 3.0. Tech. rep. http://clear.colorado.edu/ compsem/documents/propbank_guidelines.pdf. Pisa, Italy: Center for Computational Language and Education Research, Institute of Cognitive Science, University of Colorado at Boulder. Bonial, Claire, Olga Babko-Malaya, Jinho D. Choi, Jena Hwang, and Martha Palmer (2010a). PropBank Annotation Guidelines. http://www.ldc.upenn.edu/Catalog/docs/LDC2011T03/ propbank/english-propbank.pdf. Boros, Emanuela, Romaric Besançon, Olivier Ferret, and Brigitte Grau (2014). “Event Role Extraction using Domain-Relevant Word Representations.” In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar: Association for Computational Linguistics, pp. 1852–1857. url: http://www.aclweb. org/anthology/D14-1199. Braud, Chloé and Pascal Denis (2015). “Comparing Word Representations for Implicit Discourse Relation Classification.” In: Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal: Association for Computational Linguistics, pp. 2201–2211. url: http://aclweb.org/anthology/D15-1262. Bullinaria, J.A. and J.P. Levy (2007). “Extracting semantic representations from word cooccurrence statistics: A computational study.” In: Behavior Research Methods 3, p. 510.\nBibliography 143\nBurges, Christopher J. C. (June 1998). “A Tutorial on Support Vector Machines for Pattern Recognition.” In: Data Min. Knowl. Discov. 2.2, pp. 121–167. issn: 1384-5810. doi: 10. 1023/A:1009715923555. Byrd, Richard H., Peihuang Lu, Jorge Nocedal, and Ciyou Zhu (Sept. 1995). “A Limited Memory Algorithm for Bound Constrained Optimization.” In: SIAM J. Sci. Comput. 16.5, pp. 1190–1208. issn: 1064-8275. doi: 10.1137/0916069. Caselli, Tommaso, Valentina Bartalesi Lenzi, Rachele Sprugnoli, Emanuele Pianta, and Irina Prodanof (2011a). “Annotating Events, Temporal Expressions and Relations in Italian: the It-Timeml Experience for the Ita-TimeBank.” In: Proceedings of the 5th Linguistic Annotation Workshop. Portland, Oregon, USA: Association for Computational Linguistics, pp. 143–151. url: http://www.aclweb.org/anthology/W11-0418. Caselli, Tommaso, Hector Llorens, Borja Navarro-Colorado, and Estela Saquete (2011b). “Data-Driven Approach Using Semantics for Recognizing and Classifying TimeML Events in Italian.” In: Proceedings of the International Conference Recent Advances in Natural Language Processing 2011. Hissar, Bulgaria, pp. 533–538. url: http://aclweb.org/ anthology/R11-1074. Chambers, Nate (2013). “NavyTime: Event and Time Ordering from Raw Text.” In: Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013). Atlanta, Georgia, USA: Association for Computational Linguistics, pp. 73–77. url: http:// www.aclweb.org/anthology/S13-2012. Chambers, Nathanael and Dan Jurafsky (2008a). “Jointly Combining Implicit Constraints Improves Temporal Ordering.” In: Proceedings of the Conference on Empirical Methods in Natural Language Processing. EMNLP ’08. Honolulu, Hawaii: Association for Computational Linguistics, pp. 698–706. url: http://dl.acm.org/citation.cfm?id=1613715. 1613803. Chambers, Nathanael and Daniel Jurafsky (2008b). “Jointly Combining Implicit Constraints Improves Temporal Ordering.” In: Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Honolulu, Hawaii: Association for Computational Linguistics, pp. 698–706. url: http://www.aclweb.org/anthology/D08-1073. Chambers, Nathanael, Shan Wang, and Dan Jurafsky (2007). “Classifying Temporal Relations Between Events.” In: Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions. Prague, Czech Republic: Association for Computational Linguistics, pp. 173–176. url: http://www.aclweb.org/anthology/P07-2044. Chambers, Nathanael, Taylor Cassidy, Bill McDowell, and Steven Bethard (Jan. 1, 2014). “Dense Event Ordering with a Multi-Pass Architecture.” In: Transactions of the Association for Computational Linguistics 2, pp. 273–284. url: https://tacl2013.cs.columbia. edu/ojs/index.php/tacl/article/view/255. published. Chang, Angel X. and Christopher D. Manning (2012). “SUTIME: A Library for Recognizing and Normalizing Time Expressions.” In: 8th International Conference on Language Resources and Evaluation (LREC 2012). url: http://nlp.stanford.edu/pubs/lrec2012sutime.pdf. Cheng, Patricia W. and Laura R. Novick (1991). “Causes versus enabling conditions.” In: Cognition 40.1-2, pp. 83 –120. issn: 0010-0277. doi: http://dx.doi.org/10.1016/00100277(91)90047- 8. url: http://www.sciencedirect.com/science/article/pii/ 0010027791900478.\n144 Bibliography\nCheng, Patricia W and Laura R Novick (1992). “Covariation in Natural Causal Induction.” In: Psychological Review 99(2), pp. 365–382. Chklovski, Timothy and Patrick Pantel. “VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations.” In: Proceedings of EMNLP 2004. Ed. by Dekang Lin and Dekai Wu. Barcelona, Spain: Association for Computational Linguistics, pp. 33–40. Chomsky, Noam (1959). “On Certain Formal Properties of Grammars.” In: Information and Control 2.2, pp. 137–167. Ciaramita, Massimiliano and Yasemin Altun (2006). “Broad-coverage Sense Disambiguation and Information Extraction with a Supersense Sequence Tagger.” In: Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. EMNLP ’06. Sydney, Australia: Association for Computational Linguistics, pp. 594–602. isbn: 1-932432-73-6. url: http://dl.acm.org/citation.cfm?id=1610075.1610158. Collobert, Ronan and Jason Weston (2008). “A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.” In: Proceedings of the 25th International Conference on Machine Learning. ICML ’08. Helsinki, Finland: ACM, pp. 160–167. isbn: 978-1-60558-205-4. doi: 10 . 1145 / 1390156 . 1390177. url: http : //doi.acm.org/10.1145/1390156.1390177. Collobert, Ronan, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa (Nov. 2011). “Natural Language Processing (Almost) from Scratch.” In: J. Mach. Learn. Res. 12, pp. 2493–2537. issn: 1532-4435. url: http://dl.acm.org/ citation.cfm?id=1953048.2078186. Consortium, Linguistic Data (2005). ACE (Automatic Content Extraction) English Annotation Guidelines for Events. Cortes, Corinna and Vladimir Vapnik (Sept. 1995). “Support-Vector Networks.” In: Mach. Learn. 20.3, pp. 273–297. issn: 0885-6125. doi: 10.1023/A:1022627411411. Cybulska, Agata and Piek Vossen (2013). “Semantic Relations between Events and their Time, Locations and Participants for Event Coreference Resolution.” In: Proceedings of Recent Advances in Natural Language Processing (RANLP-2013). Ed. by G. Angelova, K. Bontcheva, and R. Mitkov. ISSN 1313-8502. Hissar, Bulgaria. url: http://aclweb. org/anthology//R/R13/R13-1021.pdf. D’Souza, Jennifer and Vincent Ng (2013). “Classifying Temporal Relations with Rich Linguistic Knowledge.” In: Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Atlanta, Georgia: Association for Computational Linguistics, pp. 918–927. url: http://www. aclweb.org/anthology/N13-1112. Darroch, J. N. and D. Ratcliff (1972). “Generalized iterative scaling for log-linear models.” In: vol. 43, pp. 1470–1480. Deerwester, Scott C., Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman (1990). “Indexing by Latent Semantic Analysis.” In: Journal of the American Society of Information Science 41.6, pp. 391–407. url: citeseer.ist.psu. edu/deerwester90indexing.html. Della Pietra, Stephen, Vincent Della Pietra, and John Lafferty (Apr. 1997). “Inducing Features of Random Fields.” In: IEEE Trans. Pattern Anal. Mach. Intell. 19.4, pp. 380–393. issn: 0162-8828. doi: 10.1109/34.588021. Derczynski, Leon and Robert J. Gaizauskas (2012). “Using Signals to Improve Automatic Classification of Temporal Relations.” In: CoRR abs/1203.5055.\nBibliography 145\nDerczynski, Leon and Robert Gaizauskas (2010). “Analysing Temporally Annotated Corpora with CAVaT.” In: Proceedings of the Seventh Conference on International Language Resources and Evaluation. Valletta, Malta: European Language Resources Association (ELRA), pp. 398–404. isbn: 2-9517408-6-7. Do, Quang Xuan, Wei Lu, and Dan Roth (2012). “Joint Inference for Event Timeline Construction.” In: Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. EMNLP-CoNLL ’12. Jeju Island, Korea: Association for Computational Linguistics, pp. 677–687. url: http: //dl.acm.org/citation.cfm?id=2390948.2391023. Do, Quang, Yee Seng Chan, and Dan Roth (2011). “Minimally Supervised Event Causality Identification.” In: Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Edinburgh, Scotland, UK.: Association for Computational Linguistics, pp. 294–303. url: http://www.aclweb.org/anthology/D11-1027. Erl, Thomas (2004). Service-oriented Architecture. Prentice Hall Englewood Cliffs. Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin (2008).\n“LIBLINEAR: A Library for Large Linear Classification.” In: Journal of Machine Learning Research 9, pp. 1871–1874. Fellbaum, Christiane, ed. (1998). WordNet: An Electronic Lexical Database. MIT Press. Ferro, Lisa, Inderjeet Mani, Beth Sundheim, and George Wilson (2001). TIDES Temporal An-\nnotation Guidelines. Version 1.0.2. Tech. rep. MITRE Technical Report, MTR 01W0000041. Firth, J. R. (1957). “A synopsis of linguistic theory 1930-55.” In: 1952-59, pp. 1–32. Fischer, Michael J. and Albert R. Meyer (1971). “Boolean Matrix Multiplication and Tran-\nsitive Closure.” In: SWAT (FOCS). IEEE Computer Society, pp. 129–131. url: http: //dblp.uni-trier.de/db/conf/focs/focs71.html#FischerM71. Fisher, Robert and Reid Simmons (2015). “Spectral Semi-Supervised Discourse Relation Classification.” In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Beijing, China: Association for Computational Linguistics, pp. 89–93. url: http://www.aclweb.org/anthology/P15-2015. Friedman, Jerome H. (1996). Another approach to polychotomous classification. Tech. rep. url: http://www-stat.stanford.edu/~jhf/ftp/poly.ps.Z. Gangemi, Aldo, Nicola Guarino, Claudio Masolo, Alessandro Oltramari, and Luc Schneider (2002). “Sweetening Ontologies with DOLCE.” In: Proceedings of the 13th International Conference on Knowledge Engineering and Knowledge Management. Ontologies and the Semantic Web. EKAW ’02. London, UK, UK: Springer-Verlag, pp. 166–181. isbn: 3-540-44268-5. url: http://dl.acm.org/citation.cfm?id=645362.650863. Ganitkevitch, Juri, Benjamin Van Durme, and Chris Callison-Burch (2013). “PPDB: The Paraphrase Database.” In: Proceedings of NAACL-HLT 2013. Atlanta, Georgia: ACL, pp. 758–764. url: http://cs.jhu.edu/~ccb/publications/ppdb.pdf. Gennari, Rosella, Sara Tonelli, and Pierpaolo Vittorini (2015). “Challenges in Quality of Temporal Data - Starting with Gold Standards.” In: J. Data and Information Quality 6.2-3, p. 9. doi: 10.1145/2736699. url: http://doi.acm.org/10.1145/2736699. Gerevini, Alfonso, Lenhart Schubert, and Stephanie Schaeffer (1995). “The temporal reasoning tools Timegraph I-II.” In: International Journal of Artificial Intelligence Tools 4.1-2, pp. 281–299. Gildea, Daniel and Daniel Jurafsky (Sept. 2002). “Automatic Labeling of Semantic Roles.” In: Comput. Linguist. 28.3, pp. 245–288. issn: 0891-2017.\n146 Bibliography\nGirju, Roxana, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret (2007). “SemEval-2007 Task 04: Classification of Semantic Relations between Nominals.” In: Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007). Prague, Czech Republic: Association for Computational Linguistics, pp. 13–18. url: http://www.aclweb.org/anthology/S/S07/S07-1003. Grivaz, Cécile (2010). “Human Judgements on Causation in French Texts.” In: Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10). Ed. by Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias. Valletta, Malta: European Language Resources Association (ELRA). isbn: 2-9517408-6-7. Gusev, Andrey, Nathanael Chambers, Pranav Khaitan, Divye Khilnani, Steven Bethard, and Dan Jurafsky (2011). “Using query patterns to learn the duration of events.” In: Proceedings of the Ninth International Conference on Computational Semantics. IWCS ’11. Oxford, United Kingdom: Association for Computational Linguistics, pp. 145–154. url: http://dl.acm.org/citation.cfm?id=2002669.2002685. Hage, Willem Robert van, Véronique Malaisé, Roxane Segers, Laura Hollink, and Guus Schreiber (2011). “Design and use of the Simple Event Model (SEM).” In: Journal of Web Semantics 9.2, pp. 128–136. url: http://dblp.uni-trier.de/db/journals/ws/ ws9.html#HageMSHS11. Hajič, Jan et al. (2009). “The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages.” In: Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task. Boulder, Colorado: Association for Computational Linguistics, pp. 1–18. url: http://www.aclweb.org/ anthology/W09-1201. Halevy, Alon, Peter Norvig, and Fernando Pereira (Mar. 2009). “The Unreasonable Effectiveness of Data.” In: IEEE Intelligent Systems 24.2, pp. 8–12. issn: 1541-1672. doi: 10.1109/MIS.2009.36. url: http://dx.doi.org/10.1109/MIS.2009.36. Harris, Zellig (1954). “Distributional structure.” In: Word 10.23, pp. 146–162. Hashimoto, Kazuma, Pontus Stenetorp, Makoto Miwa, and Yoshimasa Tsuruoka (2015).\n“Task-Oriented Learning of Word Embeddings for Semantic Relation Classification.” In: Proceedings of the Nineteenth Conference on Computational Natural Language Learning. Beijing, China: Association for Computational Linguistics, pp. 268–278. url: http: //www.aclweb.org/anthology/K15-1027.\nHopcroft, John E. and Jeffrey D. Ullman (1969). Formal Languages and Their Relation to Automata. Boston, MA, USA: Addison-Wesley Longman Publishing Co., Inc. Huang, Eric, Richard Socher, Christopher Manning, and Andrew Ng (2012). “Improving Word Representations via Global Context and Multiple Word Prototypes.” In: Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Jeju Island, Korea: Association for Computational Linguistics, pp. 873– 882. url: http://www.aclweb.org/anthology/P12-1092. Huang, Li-szu Agnes (2012). “The Effectiveness of a Corpus-based Instruction in Deepening EFL Learners’ Knowledge of Periphrastic Causatives.” In: TESOL Journal 6, pp. 83– 108. Ittoo, Ashwin and Gosse Bouma (2011). “Extracting Explicit and Implicit Causal Relations from Sparse, Domain-Specific Texts.” In: Natural Language Processing and Information Systems - 16th International Conference on Applications of Natural Language to Information Systems, NLDB 2011, Alicante, Spain, June 28-30, 2011. Proceedings, pp. 52–63. doi: 10.\nBibliography 147\n1007/978-3-642-22327-3_6. url: http://dx.doi.org/10.1007/978-3-642-223273_6. Jung, Hyuckchul and Amanda Stent (2013). “ATT1: Temporal Annotation Using Big Windows and Rich Syntactic and Semantic Features.” In: Proceedings of the Seventh International Workshop on Semantic Evaluation. SemEval ’13. Atlanta, Georgia, USA: Association for Computational Linguistics, pp. 20–24. url: http://www.aclweb.org/ anthology/S13-2004. Jurafsky, Daniel and James H. Martin (2000). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. 1st. Upper Saddle River, NJ, USA: Prentice Hall PTR. isbn: 0130950696. Khoo, Christopher S. G., Syin Chan, and Yun Niu (2000). “Extracting Causal Knowledge from a Medical Database Using Graphical Patterns.” In: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics. ACL ’00. Hong Kong: Association for Computational Linguistics, pp. 336–343. doi: 10.3115/1075218.1075261. url: http://dx.doi.org/10.3115/1075218.1075261. Knerr, S., L. Personnaz, and G. Dreyfus (1990). “Neurocomputing: Algorithms, Architectures and Applications.” In: ed. by Françoise Fogelman Soulié and Jeanny Hérault. Berlin, Heidelberg: Springer Berlin Heidelberg. Chap. Single-layer learning revisited: a stepwise procedure for building and training a neural network, pp. 41–50. isbn: 978- 3-642-76153-9. doi: 10.1007/978-3-642-76153-9_5. url: http://dx.doi.org/10. 1007/978-3-642-76153-9_5. Kolomiyets, Oleksandr and Marie-Francine Moens (2010). “KUL: Recognition and Normalization of Temporal Expressions.” In: Proceedings of the 5th International Workshop on Semantic Evaluation. Uppsala, Sweden: Association for Computational Linguistics, pp. 325–328. url: http://www.aclweb.org/anthology/S10-1072. Kolya, Anup Kumar, Amitava Kundu, Rajdeep Gupta, Asif Ekbal, and Sivaji Bandyopadhyay (2013). “JU_CSE: A CRF Based Approach to Annotation of Temporal Expression, Event and Temporal Relations.” In: Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013). Atlanta, Georgia, USA: Association for Computational Linguistics, pp. 64–72. url: http://www.aclweb.org/anthology/S13-2011. Koot, H van de and A Neeleman (2012). “The Theta System: Argument Structure at the Interface.” In: Oxford University Press: Oxford. Chap. The Linguistic Expression of Causation, pp. 20 –51. Kudo, Taku and Yuji Matsumoto (2003). “Fast Methods for Kernel-Based Text Analysis.” In: Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics. Sapporo, Japan: Association for Computational Linguistics, pp. 24–31. doi: 10.3115/ 1075096.1075100. url: http://www.aclweb.org/anthology/P03-1004. Lafferty, John D., Andrew McCallum, and Fernando C. N. Pereira (2001). “Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.” In: Proceedings of the Eighteenth International Conference on Machine Learning. ICML ’01. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., pp. 282–289. isbn: 1- 55860-778-1. Laokulrat, Natsuda, Makoto Miwa, Yoshimasa Tsuruoka, and Takashi Chikayama (2013). “UTTime: Temporal Relation Classification using Deep Syntactic Features.” In: Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013). Atlanta,\n148 Bibliography\nGeorgia, USA: Association for Computational Linguistics, pp. 88–92. url: http:// www.aclweb.org/anthology/S13-2015.\nLaokulrat, Natsuda, Makoto Miwa, and Yoshimasa Tsuruoka (2014). “Exploiting Timegraphs in Temporal Relation Classification.” In: Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing. Doha, Qatar: ACL, pp. 6–14. url: http://www.aclweb.org/anthology/W14-3702. Lee, Kenton, Yoav Artzi, Jesse Dodge, and Luke Zettlemoyer (2014). “Context-dependent Semantic Parsing for Time Expressions.” In: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Baltimore, Maryland: Association for Computational Linguistics, pp. 1437–1447. url: http://www. aclweb.org/anthology/P14-1135. Leech, G., R. Garside, and M. Bryant (1994). “CLAWS4: The tagging of the British National Corpus.” In: Proceedings of the 15th Intl. Conference on Computational Linguistics. ACL. Kyoto, Japan, pp. 622–628. Lewis, David (1973). “Causation.” English. In: The Journal of Philosophy 70.17, pp. 556–567. issn: 0022362X. url: http://www.jstor.org/stable/2025310. Lin, Dekang (1998). “An Information-Theoretic Definition of Similarity.” In: Proceedings of the 15th International Conference on Machine Learning. ICML ’98. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., pp. 296–304. isbn: 1-55860-556-8. url: http: //dl.acm.org/citation.cfm?id=645527.657297. Litkowski, Ken (2014). “Pattern Dictionary of English Prepositions.” In: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Baltimore, Maryland: Association for Computational Linguistics, pp. 1274– 1283. url: http://www.aclweb.org/anthology/P14-1120. Litkowski, Kenneth C. and Orin Hargraves (2006). “Coverage and inheritance in the preposition project.” In: 3rd ACL-SIGSEM Workshop on Prepositions. url: http://acl.ldc. upenn.edu/W/W06/W06-2106.pdf. Llorens, Hector, Estela Saquete, and Borja Navarro (2010). “TIPSem (English and Spanish): Evaluating CRFs and Semantic Roles in TempEval-2.” In: Proceedings of the 5th International Workshop on Semantic Evaluation. Uppsala, Sweden: Association for Computational Linguistics, pp. 284–291. url: http://www.aclweb.org/anthology/S101063. Llorens, Hector, Leon Derczynski, Robert Gaizauskas, and Estela Saquete (2012). “TIMEN: An Open Temporal Expression Normalisation Resource.” In: Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012), pp. 3044– 3051. Llorens, Hector, Nathanael Chambers, Naushad UzZaman, Nasrin Mostafazadeh, James Allen, and James Pustejovsky (2015). “SemEval-2015 Task 5: QA TempEval - Evaluating Temporal Information Understanding with Question Answering.” In: Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015). Denver, Colorado: Association for Computational Linguistics, pp. 792–800. url: http : / / www . aclweb.org/anthology/S15-2134. Magnini, B., E. Pianta, C. Girardi, M. Negri, L. Romano, M. Speranza, V. Bartalesi Lenzi, and R. Sprugnoli (2006). “I-CAB: the Italian Content Annotation Bank.” In: Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC-2006). ACL Anthology Identifier: L06-1044. Genoa, Italy: European Language Resources As-\nBibliography 149\nsociation (ELRA). url: http://www.lrec-conf.org/proceedings/lrec2006/pdf/91_ pdf.pdf.\nMalouf, Robert (2002). “A Comparison of Algorithms for Maximum Entropy Parameter Estimation.” In: Proceedings of the 6th Conference on Natural Language Learning - Volume 20. COLING-02. Stroudsburg, PA, USA: Association for Computational Linguistics, pp. 1–7. doi: 10.3115/1118853.1118871. url: http://dx.doi.org/10.3115/1118853. 1118871. Mani, Inderjeet, Marc Verhagen, Ben Wellner, Chong Min Lee, and James Pustejovsky (2006). “Machine Learning of Temporal Relations.” In: Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics. Sydney, Australia: Association for Computational Linguistics, pp. 753–760. doi: 10.3115/1220175.1220270. url: http://www.aclweb.org/ anthology/P06-1095. Mani, Inderjeet, Ben Wellner, Marc Verhagen, and James Pustejovsky (2007). Three Approaches to Learning TLINKs in TimeML. Tech. rep. Computer Science Department, Brandeis University. Manning, Christopher D., Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky (2014). “The Stanford CoreNLP Natural Language Processing Toolkit.” In: Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations. Baltimore, Maryland: Association for Computational Linguistics, pp. 55–60. url: http://www.aclweb.org/anthology/P/P14/P14-5010. Marcu, Daniel and Abdessamad Echihabi (2002). “An Unsupervised Approach to Recognizing Discourse Relations.” In: Proceedings of 40th Annual Meeting of the Association for Computational Linguistics. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics, pp. 368–375. doi: 10.3115/1073083.1073145. url: http: //www.aclweb.org/anthology/P02-1047. Marcus, Mitchell P., Mary Ann Marcinkiewicz, and Beatrice Santorini (June 1993). “Building a Large Annotated Corpus of English: The Penn Treebank.” In: Comput. Linguist. 19.2, pp. 313–330. issn: 0891-2017. url: http://dl.acm.org/citation.cfm?id=972470. 972475. Marneffe, Marie-Catherine de and Christopher D. Manning (2008). “The Stanford typed dependencies representation.” In: Coling 2008: Proceedings of the workshop on CrossFramework and Cross-Domain Parser Evaluation. Association for Computational Linguistics, pp. 1–8. Màrquez, Lluís, Xavier Carreras, Kenneth C. Litkowski, and Suzanne Stevenson (June 2008). “Semantic Role Labeling: An Introduction to the Special Issue.” In: Comput. Linguist. 34.2, pp. 145–159. issn: 0891-2017. doi: 10.1162/coli.2008.34.2.145. url: http://dx.doi.org/10.1162/coli.2008.34.2.145. McCallum, Andrew, Dayne Freitag, and Fernando C. N. Pereira (2000). “Maximum Entropy Markov Models for Information Extraction and Segmentation.” In: Proceedings of the Seventeenth International Conference on Machine Learning. ICML ’00. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., pp. 591–598. isbn: 1-55860-707-2. McDonald, Ryan and Joakim Nivre (2007). “Characterizing the Errors of Data-Driven Dependency Parsing Models.” In: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). Prague, Czech Republic: Association for Computational Linguistics, pp. 122–131. url: http://www.aclweb.org/anthology/D/D07/D07-1013.\n150 Bibliography\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean (2013). “Efficient Estimation of Word Representations in Vector Space.” In: CoRR abs/1301.3781. Miller, George A. and Walter G. Charles (1991). “Contextual correlates of semantic similarity.” In: Language and Cognitive Processes 6.1, pp. 1–28. doi: 10.1080/01690969108406936. url: http://dx.doi.org/10.1080/01690969108406936. Miller, Stephanie A. and Lenhart K. Schubert (1990). “Time Revisited.” In: Computational Intelligence 6, pp. 108–118. url: http://dblp.uni-trier.de/db/journals/ci/ci6. html#MillerS90. Minard, Anne-Lyse, Manuela Speranza, Eneko Agirre, Itziar Aldabe, Marieke van Erp, Bernardo Magnini, German Rigau, and Ruben Urizar (2015). “SemEval-2015 Task 4: TimeLine: Cross-Document Event Ordering.” In: Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015). Denver, Colorado: Association for Computational Linguistics, pp. 778–786. url: http://www.aclweb.org/anthology/ S15-2132. Mirza, Paramita (2014). “Extracting Temporal and Causal Relations between Events.” In: Proceedings of the ACL 2014 Student Research Workshop. Baltimore, Maryland, USA: Association for Computational Linguistics, pp. 10–17. url: http://www.aclweb.org/ anthology/P/P14/P14-3002. Mirza, Paramita (2015). “Recognizing and Normalizing Temporal Expressions in Indonesian Texts.” In: Proceedings of Pacific Association for Computational Linguistics (PACLING) 2015. Bali, Indonesia, pp. 801–805. Mirza, Paramita (2016). “Computational Linguistics: 14th International Conference of the Pacific Association for Computational Linguistics, PACLING 2015, Bali, Indonesia, May 19-21, 2015, Revised Selected Papers.” In: Singapore: Springer Singapore. Chap. Recognizing and Normalizing Temporal Expressions in Indonesian Texts, pp. 135– 147. url: http://dx.doi.org/10.1007/978-981-10-0515-2_10. Mirza, Paramita and Anne-Lyse Minard (2014). “FBK-HLT-time: a complete Italian Temporal Processing system for EVENTI-Evalita 2014.” In: Proceedings of the 4th International Workshop EVALITA-2014. Pisa, Italy, pp. 44–49. url: http://clic.humnet.unipi.it/ proceedings/Proceedings-EVALITA-2014.pdf. Mirza, Paramita and Anne-Lyse Minard (2015). “HLT-FBK: a Complete Temporal Processing System for QA TempEval.” In: Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015). Denver, Colorado: Association for Computational Linguistics, pp. 801–805. url: http://www.aclweb.org/anthology/S15-2135. Mirza, Paramita and Sara Tonelli (2014a). “An Analysis of Causality between Events and its Relation to Temporal Information.” In: Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin, Ireland: Dublin City University and Association for Computational Linguistics, pp. 2097–2106. url: http://www.aclweb.org/anthology/C14-1198. Mirza, Paramita and Sara Tonelli (2014b). “Classifying Temporal Relations with Simple Features.” In: Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Gothenburg, Sweden: Association for Computational Linguistics, pp. 308–317. url: http://www.aclweb.org/anthology/E14-1033. Mirza, Paramita and Sara Tonelli (2016). “CATENA: CAusal and Temporal relation Extraction from NAtural language texts.” Submitted to the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016).\nBibliography 151\nMirza, Paramita, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza (2014). “Annotating Causality in the TempEval-3 Corpus.” In: Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL). Gothenburg, Sweden: Association for Computational Linguistics, pp. 10–19. url: http://www.aclweb.org/ anthology/W14-0702. Mitchell, Jeff and Mirella Lapata (2010). “Composition in Distributional Models of Semantics.” In: Cognitive Science 34.8, pp. 1388–1429. Mitra, Debasis and Florent Launay (2006). “On-line Qualitative Temporal Reasoning with Explanation.” In: FLAIRS Conference, pp. 737–739. Nakov, Preslav and Hwee Tou Ng (2009). “Improved Statistical Machine Translation for Resource-Poor Languages Using Related Resource-Rich Languages.” In: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Singapore: Association for Computational Linguistics, pp. 1358–1367. url: http://www.aclweb. org/anthology/D/D09/D09-1141. Nguyen, Thien Huu and Ralph Grishman (2014). “Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction.” In: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Baltimore, Maryland: Association for Computational Linguistics, pp. 68–74. url: http://www.aclweb.org/anthology/P14-2012. Nivre, Joakim (2003). “An efficient algorithm for projective dependency parsing.” In: Proceedings of the 8th International Workshop on Parsing Technologies(IWPT), pp. 149–160. Nocedal, Jorge (1980). “Updating quasi-Newton matrices with limited storage.” In: Mathematics of computation 35.151, pp. 773–782. Padó, Sebastian and Mirella Lapata (June 2007). “Dependency-based construction of semantic space models.” In: Computational Linguistics 33.2, pp. 161–199. issn: 0891-2017. doi: 10.1162/coli.2007.33.2.161. url: http://dx.doi.org/10.1162/coli.2007.33. 2.161. Palmer, Martha, Daniel Gildea, and Paul Kingsbury (2005). “The Proposition Bank: An Annotated Corpus of Semantic Roles.” In: Computational Linguistics 31.1, pp. 71–106. url: http://www.aclweb.org/anthology/J05-1004. Paperno, Denis, Nghia The Pham, and Marco Baroni (2014). “A practical and linguisticallymotivated approach to compositional distributional semantics.” In: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Baltimore, Maryland: Association for Computational Linguistics, pp. 90–99. url: http://www.aclweb.org/anthology/P14-1009. Pennington, Jeffrey, Richard Socher, and Christopher D. Manning (2014). “GloVe: Global Vectors for Word Representation.” In: Empirical Methods in Natural Language Processing (EMNLP), pp. 1532–1543. url: http://www.aclweb.org/anthology/D14-1162. Pianta, Emanuele, Christian Girardi, and Roberto Zanoli (2008). “The TextPro Tool Suite.” In: Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08). http://www.lrec-conf.org/proceedings/lrec2008/. Marrakech, Morocco: European Language Resources Association (ELRA). isbn: 2-9517408-4-0. Pitler, Emily and Ani Nenkova (2009). “Using Syntax to Disambiguate Explicit Discourse Connectives in Text.” In: Proceedings of the ACL-IJCNLP 2009 Conference Short Papers. Suntec, Singapore: Association for Computational Linguistics, pp. 13–16. url: http: //www.aclweb.org/anthology/P/P09/P09-2004.\n152 Bibliography\nPitler, Emily, Annie Louis, and Ani Nenkova (2009). “Automatic sense prediction for implicit discourse relations in text.” In: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Suntec, Singapore: Association for Computational Linguistics, pp. 683–691. url: http://www.aclweb.org/anthology/P/P09/P09-1077. Prasad, Rashmi, Eleni Miltsakaki, Nikhil Dinesh, Alan Lee, Aravind Joshi, Livio Robaldo, and Bonnie L Webber (2007). The Penn Discourse Treebank 2.0 Annotation Manual. Tech. rep. Prasad, Rashmi, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber (2008). “The Penn Discourse TreeBank 2.0.” In: Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08). Ed. by Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, and Daniel Tapias. http://www.lrec-conf.org/proceedings/lrec2008/. Marrakech, Morocco: European Language Resources Association (ELRA). isbn: 2- 9517408-4-0. Pustejovsky, James, José Castaño, Robert Ingria, Roser Saurí, Robert Gaizauskas, Andrea Setzer, and Graham Katz (2003). “TimeML: Robust specification of event and temporal expressions in text.” In: Proceedings of the Fifth International Workshop on Computational Semantics (IWCS-5). url: http://www.timeml.org/site/publications/timeMLpubs/ IWCS-v4.pdf. Pustejovsky, James, Jessica Littman, Roser Saurí, and Marc Verhagen (2006). TimeBank 1.2 Documentation. Tech. rep. Brandeis University. url: http://www.timeml.org/site/ timebank/documentation-1.2.html. Pustejovsky, James, Kiyong Lee, Harry Bunt, and Laurent Romary (2010). “ISO-TimeML: An International Standard for Semantic Annotation.” In: Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10). Ed. by Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias. Valletta, Malta: European Language Resources Association (ELRA). isbn: 2-9517408-6-7. Rabiner, Lawrence R. (1990). “Readings in Speech Recognition.” In: ed. by Alex Waibel and Kai-Fu Lee. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc. Chap. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, pp. 267–296. isbn: 1-55860-124-4. Reichenbach, H (1947). Elements of symbolic logic. Berkeley, CA: University of California Press. Riaz, Mehwish and Roxana Girju (2013). “Toward a Better Understanding of Causality between Verbal Events: Extraction and Analysis of the Causal Power of Verb-Verb Associations.” In: Proceedings of the SIGDIAL 2013 Conference. Metz, France: Association for Computational Linguistics, pp. 21–30. url: http://www.aclweb.org/anthology/W134004. Rink, Bryan, Cosmin Adrian Bejan, and Sanda M. Harabagiu (2010). “Learning Textual Graph Patterns to Detect Causal Event Relations.” In: FLAIRS Conference. Sahlgren, Magnus (2006). “The Word-Space Model: using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces.” PhD thesis. Stockholm University. Sakaji, Hiroki, Satoshi Sekine, and Shigeru Masuyama (2008). “Extracting Causal Knowledge Using Clue Phrases and Syntactic Patterns.” In: Proceedings of the 7th International\nBibliography 153\nConference on Practical Aspects of Knowledge Management. PAKM ’08. Yokohama, Japan: Springer-Verlag, pp. 111–122. isbn: 978-3-540-89446-9. Santorini, Beatrice (1990). Part-Of-Speech Tagging Guidelines for the Penn Treebank Project (3rd revision, 2nd printing). Tech. rep. Philadelphia, PA, USA: Department of Linguistics, University of Pennsylvania. Saurí, Roser, Jessica Littman, Robert Gaizauskas, Andrea Setzer, and James Pustejovsky (2006). TimeML Annotation Guidelines, Version 1.2.1. Schäfer, Ulrich (June 2007). Integrating Deep and Shallow Natural Language Processing Components - Representations and Hybrid Architectures. Vol. 22. Saarbrücken Dissertations in Computational Linguistics and Language Te. hardcopies available here ; ISBN 978- 3-933218-21-6. Saarbrücken, Germany: DFKI GmbH and Computational Linguistics Department, Saarland University. Setzer, Andrea (2001). “Temporal Information in Newswire Articles: An Annotation Scheme and Corpus Study.” In: url: http://www.andrea-setzer.org.uk/PAPERS/thesis.pdf. Shaw, Ryan, Raphaël Troncy, and Lynda Hardman (2009). “LODE: Linking Open Descriptions of Events.” In: Proceedings of the 4th Asian Conference on The Semantic Web. ASWC ’09. Shanghai, China: Springer-Verlag, pp. 153–167. isbn: 978-3-642-10870-9. doi: 10.1007/978-3-642-10871-6_11. url: http://dx.doi.org/10.1007/978-3-64210871-6_11. Simamora, Agus (2013). Temporal Entity Tagging untuk Dokumen Bahasa Indonesia. url: http: //repository.ipb.ac.id/handle/123456789/64661. Strötgen, Jannik and Michael Gertz (2015). “A Baseline Temporal Tagger for all Languages.” In: Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal: Association for Computational Linguistics, pp. 541–547. url: http://aclweb.org/anthology/D15-1063. Strötgen, Jannik, Julian Zell, and Michael Gertz (2013). “HeidelTime: Tuning English and Developing Spanish Resources for TempEval-3.” In: Proceedings of the Seventh International Workshop on Semantic Evaluation. SemEval ’13. Atlanta, Georgia, USA: Association for Computational Linguistics, pp. 15–19. url: http://www.aclweb.org/ anthology/S13-2003. Strötgen, Jannik, Ayser Armiti, Tran Van Canh, Julian Zell, and Michael Gertz (2014). “Time for More Languages: Temporal Tagging of Arabic, Italian, Spanish, and Vietnamese.” In: ACM Transactions on Asian Language Information Processing (TALIP) 13.1, pp. 1–21. Surdeanu, Mihai, Richard Johansson, Adam Meyers, Lluís Màrquez, and Joakim Nivre (2008). “The CoNLL 2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies.” In: CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning. Manchester, England: Coling 2008 Organizing Committee, pp. 159–177. url: http://www.aclweb.org/anthology/W08-2121. Talmy, Leonard (1985). “Force dynamics in language and thought.” In: Chicago Linguistic Society 21, 293–337. Talmy, Leonard (1988). “Force dynamics in language and cognition.” In: Cognitive science 12.1, pp. 49–100. Tang, Duyu, Furu Wei, Bing Qin, Ting Liu, and Ming Zhou (2014). “Coooolll: A Deep Learning System for Twitter Sentiment Classification.” In: Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014). Dublin, Ireland: Association\n154 Bibliography\nfor Computational Linguistics and Dublin City University, pp. 208–212. url: http: //www.aclweb.org/anthology/S14-2033. Tatu, Marta and Munirathnam Srikanth (2008). “Experiments with Reasoning for Temporal Relations between Events.” In: Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008). Manchester, UK: Coling 2008 Organizing Committee, pp. 857–864. url: http://www.aclweb.org/anthology/C08-1108. Tenenbaum, Joshua B. and Thomas L. Griffiths (2001). “Structure Learning in Human Causal Induction.” In: IN. MIT Press, pp. 59–65. The PDTB Research Group (2008). The PDTB 2.0. Annotation Manual. Tech. rep. IRCS-08-01. Institute for Research in Cognitive Science, University of Pennsylvania. Tonelli, Sara, Rachele Sprugnoli, and Manuela Speranza (2014). NewsReader Guidelines for Annotation at Document Level, Extension of Deliverable D3.1. Tech. rep. NWR-2014-2. Fondazione Bruno Kessler. Turian, Joseph, Lev-Arie Ratinov, and Yoshua Bengio (2010). “Word Representations: A Simple and General Method for Semi-Supervised Learning.” In: Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Uppsala, Sweden: Association for Computational Linguistics, pp. 384–394. url: http://www.aclweb. org/anthology/P10-1040. Turney, Peter D. (2006). “Similarity of semantic relations.” In: Computational Linguistics 32, pp. 379–416. Turney, Peter D. and Patrick Pantel (Mar. 2010). “From Frequency to Meaning: Vector Space Models of Semantics.” In: J. Artif. Intell. Res. (JAIR) 37, pp. 141–188. doi: 10. 1613/jair.2934. eprint: 1003.1141. url: http://dx.doi.org/10.1613/jair.2934. UzZaman, Naushad (2012). “Interpreting the temporal aspects of language.” PhD thesis. University of Rochester. Dept. of Computer Science. UzZaman, Naushad and James Allen (2010). “TRIPS and TRIOS System for TempEval2: Extracting Temporal Information from Text.” In: Proceedings of the 5th International Workshop on Semantic Evaluation. Uppsala, Sweden: Association for Computational Linguistics, pp. 276–283. url: http://www.aclweb.org/anthology/S10-1062. UzZaman, Naushad and James Allen (2011). “Temporal Evaluation.” In: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Portland, Oregon, USA: Association for Computational Linguistics, pp. 351–356. url: http://www.aclweb.org/anthology/P11-2061. UzZaman, Naushad, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, and James Pustejovsky (2013). “SemEval-2013 Task 1: TempEval-3: Evaluating Time Expressions, Events, and Temporal Relations.” In: Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013). Atlanta, Georgia, USA: Association for Computational Linguistics, pp. 1–9. url: http://www.aclweb.org/anthology/S132001. Verhagen, Marc (2005a). “Temporal Closure in an Annotation Environment.” In: Language Resources and Evaluation 39.2-3, pp. 211–241. doi: 10.1007/s10579-005-7884-5. url: http://dx.doi.org/10.1007/s10579-005-7884-5. Verhagen, Marc (2005b). “Temporal Closure in an Annotation Environment.” In: Language Resources and Evaluation 39.2-3, pp. 211–241. Verhagen, Marc, Robert Gaizauskas, Frank Schilder, Mark Hepple, Graham Katz, and James Pustejovsky (2007). “SemEval-2007 Task 15: TempEval Temporal Relation Iden-\nBibliography 155\ntification.” In: Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007). Prague, Czech Republic: Association for Computational Linguistics, pp. 75–80. url: http://www.aclweb.org/anthology/S/S07/S07-1014. Verhagen, Marc, Roser Sauri, Tommaso Caselli, and James Pustejovsky (2010). “SemEval2010 Task 13: TempEval-2.” In: Proceedings of the 5th International Workshop on Semantic Evaluation. Uppsala, Sweden: Association for Computational Linguistics, pp. 57–62. url: http://www.aclweb.org/anthology/S10-1010. Viterbi, A. (Sept. 2006). “Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.” In: IEEE Trans. Inf. Theor. 13.2, pp. 260–269. issn: 0018-9448. doi: 10.1109/TIT.1967.1054010. Westphal, Matthias and Stefan Wölfl (2009). “Qualitative CSP, Finite CSP, and SAT: Comparing Methods for Qualitative Constraint-based Reasoning.” In: Proceedings of the 21st International Joint Conference on Artifical Intelligence. IJCAI’09. Pasadena, California, USA: Morgan Kaufmann Publishers Inc., pp. 628–633. url: http://ijcai.org/ papers09/Papers/IJCAI09-110.pdf. Wolff, Phillip (2007). “Representing causation.” In: Journal of experimental psychology: General 136.1, pp. 82–111. Wolff, Phillip and Grace Song (2003). “Models of causation and the semantics of causal verbs.” In: Cognitive Psychology 47.3, pp. 276–332. Wolff, Phillip, Bianca Klettke, Tatyana Ventura, and Grace Song (2005). “Categorization inside and outside the laboratory: Essays in honor of Douglas L. Medin. APA decade of behavior series.” In: ed. by Woo-kyoung Ahn, Robert L. Goldstone, Bradley C. Love, Arthur B. Markman, and Phillip Wolff. Washington, DC, US: American Psychological Association, xx, 316 pp. Chap. Expressing Causation in English and Other Languages, pp. 29–48. Yoshikawa, Katsumasa, Sebastian Riedel, Masayuki Asahara, and Yuji Matsumoto (2009). “Jointly Identifying Temporal Relations with Markov Logic.” In: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Suntec, Singapore: Association for Computational Linguistics, pp. 405–413. url: http://www.aclweb.org/anthology/P/P09/ P09-1046. Zhu, Xiangxin, Carl Vondrick, Deva Ramanan, and Charless C. Fowlkes (2012). “Do we need more training data or better models for object detection?” In: British Machine Vision Conference (BMVC). Zhu, Xiaojin, Andrew B. Goldberg, Ronald Brachman, and Thomas Dietterich (2009). Introduction to Semi-Supervised Learning. Morgan and Claypool Publishers. isbn: 1598295470, 9781598295474."
    } ],
    "references" : [ {
      "title" : "Exploiting Timegraphs in Temporal Relation Classification.",
      "author" : [ "Laokulrat", "Natsuda", "Makoto Miwa", "Yoshimasa Tsuruoka" ],
      "venue" : "Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing. Doha, Qatar: ACL,",
      "citeRegEx" : "Laokulrat et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Laokulrat et al\\.",
      "year" : 2014
    }, {
      "title" : "Context-dependent Semantic Parsing for Time Expressions.",
      "author" : [ "Lee", "Kenton", "Yoav Artzi", "Jesse Dodge", "Luke Zettlemoyer" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "Lee et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2014
    }, {
      "title" : "CLAWS4: The tagging of the British National Corpus.",
      "author" : [ "G. Leech", "R. Garside", "M. Bryant" ],
      "venue" : "Proceedings of the 15th Intl. Conference on Computational Linguistics. ACL. Kyoto,",
      "citeRegEx" : "Leech et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Leech et al\\.",
      "year" : 1994
    }, {
      "title" : "An Information-Theoretic Definition of Similarity.",
      "author" : [ "Lin", "Dekang" ],
      "venue" : "Proceedings of the 15th International Conference on Machine Learning. ICML ’98",
      "citeRegEx" : "Lin and Dekang,? \\Q1998\\E",
      "shortCiteRegEx" : "Lin and Dekang",
      "year" : 1998
    }, {
      "title" : "Pattern Dictionary of English Prepositions.",
      "author" : [ "Litkowski", "Ken" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "Litkowski and Ken,? \\Q2014\\E",
      "shortCiteRegEx" : "Litkowski and Ken",
      "year" : 2014
    }, {
      "title" : "Coverage and inheritance in the preposition project.",
      "author" : [ "Litkowski", "Kenneth C", "Orin Hargraves" ],
      "venue" : "ACL-SIGSEM Workshop on Prepositions. url: http://acl.ldc. upenn.edu/W/W06/W06-2106.pdf",
      "citeRegEx" : "Litkowski et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Litkowski et al\\.",
      "year" : 2006
    }, {
      "title" : "TIPSem (English and Spanish): Evaluating CRFs and Semantic Roles in TempEval-2.",
      "author" : [ "Llorens", "Hector", "Estela Saquete", "Borja Navarro" ],
      "venue" : "Proceedings of the 5th International Workshop on Semantic Evaluation. Uppsala, Sweden: Association for Computational Linguistics,",
      "citeRegEx" : "Llorens et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Llorens et al\\.",
      "year" : 2010
    }, {
      "title" : "TIMEN: An Open Temporal Expression Normalisation Resource.",
      "author" : [ "Llorens", "Hector", "Leon Derczynski", "Robert Gaizauskas", "Estela Saquete" ],
      "venue" : "Proceedings of the Eighth International Conference on Language Resources and Evaluation",
      "citeRegEx" : "Llorens et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Llorens et al\\.",
      "year" : 2012
    }, {
      "title" : "SemEval-2015 Task 5: QA TempEval - Evaluating Temporal Information Understanding with Question Answering.",
      "author" : [ "Llorens", "Hector", "Nathanael Chambers", "Naushad UzZaman", "Nasrin Mostafazadeh", "James Allen", "James Pustejovsky" ],
      "venue" : "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval",
      "citeRegEx" : "Llorens et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Llorens et al\\.",
      "year" : 2015
    }, {
      "title" : "I-CAB: the Italian Content Annotation Bank.",
      "author" : [ "B. Magnini", "E. Pianta", "C. Girardi", "M. Negri", "L. Romano", "M. Speranza", "V. Bartalesi Lenzi", "R. Sprugnoli" ],
      "venue" : "Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC-2006). ACL Anthology Identifier: L06-1044",
      "citeRegEx" : "Magnini et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Magnini et al\\.",
      "year" : 2006
    }, {
      "title" : "A Comparison of Algorithms for Maximum Entropy Parameter",
      "author" : [ "pdf.pdf. Malouf", "Robert" ],
      "venue" : null,
      "citeRegEx" : "Malouf and Robert,? \\Q2002\\E",
      "shortCiteRegEx" : "Malouf and Robert",
      "year" : 2002
    }, {
      "title" : "The Stanford typed",
      "author" : [ "Marneffe", "Marie-Catherine de", "Christopher D. Manning" ],
      "venue" : null,
      "citeRegEx" : "972475",
      "shortCiteRegEx" : "972475",
      "year" : 2008
    }, {
      "title" : "Characterizing the Errors of Data-Driven",
      "author" : [ "McDonald", "Ryan", "Joakim Nivre" ],
      "venue" : "CA, USA: Morgan Kaufmann Publishers Inc.,",
      "citeRegEx" : "McDonald et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2007
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space.",
      "author" : [ "Mikolov", "Tomas", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Contextual correlates of semantic similarity.",
      "author" : [ "Miller", "George A", "Walter G. Charles" ],
      "venue" : "Language and Cognitive Processes",
      "citeRegEx" : "Miller et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 1991
    }, {
      "title" : "SemEval-2015 Task 4: TimeLine: Cross-Document Event Ordering.",
      "author" : [ "Minard", "Anne-Lyse", "Manuela Speranza", "Eneko Agirre", "Itziar Aldabe", "Marieke van Erp", "Bernardo Magnini", "German Rigau", "Ruben Urizar" ],
      "venue" : "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval",
      "citeRegEx" : "Minard et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Minard et al\\.",
      "year" : 2015
    }, {
      "title" : "Extracting Temporal and Causal Relations between Events.",
      "author" : [ "Mirza", "Paramita" ],
      "venue" : "Proceedings of the ACL 2014 Student Research Workshop",
      "citeRegEx" : "Mirza and Paramita,? \\Q2014\\E",
      "shortCiteRegEx" : "Mirza and Paramita",
      "year" : 2014
    }, {
      "title" : "Recognizing and Normalizing Temporal Expressions in Indonesian Texts.",
      "author" : [ "Mirza", "Paramita" ],
      "venue" : "Proceedings of Pacific Association for Computational Linguistics (PACLING)",
      "citeRegEx" : "Mirza and Paramita,? \\Q2015\\E",
      "shortCiteRegEx" : "Mirza and Paramita",
      "year" : 2015
    }, {
      "title" : "Computational Linguistics: 14th International Conference of the Pacific Association for Computational Linguistics, PACLING 2015, Bali, Indonesia",
      "author" : [ "Mirza", "Paramita" ],
      "venue" : "Revised Selected Papers.” In: Singapore: Springer Singapore. Chap. Recognizing and Normalizing Temporal Expressions in Indonesian Texts,",
      "citeRegEx" : "Mirza and Paramita,? \\Q2016\\E",
      "shortCiteRegEx" : "Mirza and Paramita",
      "year" : 2016
    }, {
      "title" : "FBK-HLT-time: a complete Italian Temporal Processing system for EVENTI-Evalita 2014.",
      "author" : [ "Mirza", "Paramita", "Anne-Lyse Minard" ],
      "venue" : "Proceedings of the 4th International Workshop EVALITA-2014. Pisa, Italy,",
      "citeRegEx" : "Mirza et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mirza et al\\.",
      "year" : 2014
    }, {
      "title" : "HLT-FBK: a Complete Temporal Processing System for QA TempEval.",
      "author" : [ "Mirza", "Paramita", "Anne-Lyse Minard" ],
      "venue" : "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval",
      "citeRegEx" : "Mirza et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mirza et al\\.",
      "year" : 2015
    }, {
      "title" : "An Analysis of Causality between Events and its Relation to Temporal Information.",
      "author" : [ "Mirza", "Paramita", "Sara Tonelli" ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics:",
      "citeRegEx" : "Mirza et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mirza et al\\.",
      "year" : 2014
    }, {
      "title" : "Classifying Temporal Relations with Simple Features.",
      "author" : [ "Mirza", "Paramita", "Sara Tonelli" ],
      "venue" : "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Gothenburg, Sweden: Association for Computational Linguistics,",
      "citeRegEx" : "Mirza et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mirza et al\\.",
      "year" : 2014
    }, {
      "title" : "CATENA: CAusal and Temporal relation Extraction from NAtural language texts.",
      "author" : [ "Mirza", "Paramita", "Sara Tonelli" ],
      "venue" : "Submitted to the 54th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Mirza et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mirza et al\\.",
      "year" : 2016
    }, {
      "title" : "Composition in Distributional Models of Seman",
      "author" : [ "anthology/W14-0702. Mitchell", "Jeff", "Mirella Lapata" ],
      "venue" : null,
      "citeRegEx" : "Mitchell et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2010
    }, {
      "title" : "Employing Word Representations",
      "author" : [ "Thien Huu", "Ralph Grishman" ],
      "venue" : null,
      "citeRegEx" : "Nguyen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2014
    }, {
      "title" : "The TextPro Tool Suite.",
      "author" : [ "Pianta", "Emanuele", "Christian Girardi", "Roberto Zanoli" ],
      "venue" : null,
      "citeRegEx" : "Pianta et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Pianta et al\\.",
      "year" : 2008
    }, {
      "title" : "Automatic sense prediction for implicit discourse relations in text.",
      "author" : [ "Pitler", "Emily", "Annie Louis", "Ani Nenkova" ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Suntec,",
      "citeRegEx" : "Pitler et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Pitler et al\\.",
      "year" : 2009
    }, {
      "title" : "The Penn Discourse Treebank 2.0 Annotation Manual",
      "author" : [ "Prasad", "Rashmi", "Eleni Miltsakaki", "Nikhil Dinesh", "Alan Lee", "Aravind Joshi", "Livio Robaldo", "Bonnie L Webber" ],
      "venue" : null,
      "citeRegEx" : "Prasad et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Prasad et al\\.",
      "year" : 2007
    }, {
      "title" : "The Penn Discourse TreeBank 2.0.",
      "author" : [ "Prasad", "Rashmi", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind Joshi", "Bonnie Webber" ],
      "venue" : "Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08)",
      "citeRegEx" : "Prasad et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Prasad et al\\.",
      "year" : 2008
    }, {
      "title" : "TimeML: Robust specification of event and temporal expressions in text.",
      "author" : [ "Pustejovsky", "James", "José Castaño", "Robert Ingria", "Roser Saurí", "Robert Gaizauskas", "Andrea Setzer", "Graham Katz" ],
      "venue" : "Proceedings of the Fifth International Workshop on Computational Semantics (IWCS-5). url: http://www.timeml.org/site/publications/timeMLpubs/ IWCS-v4.pdf",
      "citeRegEx" : "Pustejovsky et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Pustejovsky et al\\.",
      "year" : 2003
    }, {
      "title" : "ISO-TimeML: An International Standard for Semantic Annotation.",
      "author" : [ "Pustejovsky", "James", "Kiyong Lee", "Harry Bunt", "Laurent Romary" ],
      "venue" : "Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10)",
      "citeRegEx" : "Pustejovsky et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pustejovsky et al\\.",
      "year" : 2010
    }, {
      "title" : "Readings in Speech Recognition.",
      "author" : [ "Rabiner", "Lawrence R" ],
      "venue" : "Publishers Inc. Chap. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,",
      "citeRegEx" : "Rabiner and R.,? \\Q1990\\E",
      "shortCiteRegEx" : "Rabiner and R.",
      "year" : 1990
    }, {
      "title" : "Elements of symbolic logic",
      "author" : [ "H Reichenbach" ],
      "venue" : null,
      "citeRegEx" : "Reichenbach,? \\Q1947\\E",
      "shortCiteRegEx" : "Reichenbach",
      "year" : 1947
    }, {
      "title" : "Toward a Better Understanding of Causality between Verbal Events: Extraction and Analysis of the Causal Power of Verb-Verb Associations.",
      "author" : [ "Riaz", "Mehwish", "Roxana Girju" ],
      "venue" : "Proceedings of the SIGDIAL 2013 Conference",
      "citeRegEx" : "Riaz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Riaz et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning Textual Graph Patterns to Detect Causal Event Relations.",
      "author" : [ "Rink", "Bryan", "Cosmin Adrian Bejan", "Sanda M. Harabagiu" ],
      "venue" : "FLAIRS Conference",
      "citeRegEx" : "Rink et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rink et al\\.",
      "year" : 2010
    }, {
      "title" : "The Word-Space Model: using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces.",
      "author" : [ "Sahlgren", "Magnus" ],
      "venue" : null,
      "citeRegEx" : "Sahlgren and Magnus,? \\Q2006\\E",
      "shortCiteRegEx" : "Sahlgren and Magnus",
      "year" : 2006
    }, {
      "title" : "Extracting Causal Knowledge Using Clue Phrases and Syntactic Patterns.",
      "author" : [ "Sakaji", "Hiroki", "Satoshi Sekine", "Shigeru Masuyama" ],
      "venue" : "Proceedings of the 7th International",
      "citeRegEx" : "Sakaji et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sakaji et al\\.",
      "year" : 2008
    }, {
      "title" : "Part-Of-Speech Tagging Guidelines for the Penn Treebank Project",
      "author" : [ "Santorini", "Beatrice" ],
      "venue" : null,
      "citeRegEx" : "Santorini and Beatrice,? \\Q1990\\E",
      "shortCiteRegEx" : "Santorini and Beatrice",
      "year" : 1990
    }, {
      "title" : "Integrating Deep and Shallow Natural Language Processing",
      "author" : [ "Schäfer", "Ulrich (June" ],
      "venue" : null,
      "citeRegEx" : "Schäfer and .June,? \\Q2007\\E",
      "shortCiteRegEx" : "Schäfer and .June",
      "year" : 2007
    }, {
      "title" : "Temporal Information in Newswire Articles: An Annotation Scheme",
      "author" : [ "Setzer", "Andrea" ],
      "venue" : null,
      "citeRegEx" : "Setzer and Andrea,? \\Q2001\\E",
      "shortCiteRegEx" : "Setzer and Andrea",
      "year" : 2001
    }, {
      "title" : "LODE: Linking Open",
      "author" : [ "Shaw", "Ryan", "Raphaël Troncy", "Lynda Hardman" ],
      "venue" : null,
      "citeRegEx" : "Shaw et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Shaw et al\\.",
      "year" : 2009
    }, {
      "title" : "Temporal Entity Tagging untuk Dokumen Bahasa Indonesia",
      "author" : [ "Simamora", "Agus" ],
      "venue" : null,
      "citeRegEx" : "Simamora and Agus,? \\Q2013\\E",
      "shortCiteRegEx" : "Simamora and Agus",
      "year" : 2013
    }, {
      "title" : "A Baseline Temporal Tagger for all Lan",
      "author" : [ "Strötgen", "Jannik", "Michael Gertz" ],
      "venue" : null,
      "citeRegEx" : "Strötgen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Strötgen et al\\.",
      "year" : 2015
    }, {
      "title" : "Force dynamics in language and thought.",
      "author" : [ "Talmy", "Leonard" ],
      "venue" : null,
      "citeRegEx" : "Talmy and Leonard,? \\Q1985\\E",
      "shortCiteRegEx" : "Talmy and Leonard",
      "year" : 1985
    }, {
      "title" : "Force dynamics in language and cognition.",
      "author" : [ "Talmy", "Leonard" ],
      "venue" : null,
      "citeRegEx" : "Talmy and Leonard,? \\Q1988\\E",
      "shortCiteRegEx" : "Talmy and Leonard",
      "year" : 1988
    }, {
      "title" : "Coooolll: A Deep",
      "author" : [ "Tang", "Duyu", "Furu Wei", "Bing Qin", "Ting Liu", "Ming Zhou" ],
      "venue" : null,
      "citeRegEx" : "Tang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2014
    }, {
      "title" : "Experiments with Reasoning for Temporal Relations between Events.",
      "author" : [ "Tatu", "Marta", "Munirathnam Srikanth" ],
      "venue" : "Proceedings of the 22nd International Conference on Computational Linguistics (Coling",
      "citeRegEx" : "Tatu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tatu et al\\.",
      "year" : 2008
    }, {
      "title" : "Structure Learning in Human Causal Induction.",
      "author" : [ "Tenenbaum", "Joshua B", "Thomas L. Griffiths" ],
      "venue" : null,
      "citeRegEx" : "Tenenbaum et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Tenenbaum et al\\.",
      "year" : 2001
    }, {
      "title" : "NewsReader Guidelines for Annotation at Document Level, Extension of Deliverable D3.1",
      "author" : [ "Tonelli", "Sara", "Rachele Sprugnoli", "Manuela Speranza" ],
      "venue" : null,
      "citeRegEx" : "Tonelli et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tonelli et al\\.",
      "year" : 2014
    }, {
      "title" : "Word Representations: A Simple and General Method for Semi-Supervised Learning.",
      "author" : [ "Turian", "Joseph", "Lev-Arie Ratinov", "Yoshua Bengio" ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Uppsala, Sweden: Association for Computational Linguistics,",
      "citeRegEx" : "Turian et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Turian et al\\.",
      "year" : 2010
    }, {
      "title" : "Similarity of semantic relations.",
      "author" : [ "Turney", "Peter D" ],
      "venue" : "Computational Linguistics",
      "citeRegEx" : "Turney and D.,? \\Q2006\\E",
      "shortCiteRegEx" : "Turney and D.",
      "year" : 2006
    }, {
      "title" : "From Frequency to Meaning: Vector Space Models of Semantics.",
      "author" : [ "Turney", "Peter D", "Patrick Pantel (Mar" ],
      "venue" : "J. Artif. Intell. Res. (JAIR)",
      "citeRegEx" : "Turney et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Turney et al\\.",
      "year" : 2010
    }, {
      "title" : "Interpreting the temporal aspects of language.",
      "author" : [ "UzZaman", "Naushad" ],
      "venue" : "PhD thesis. University of Rochester. Dept. of Computer Science",
      "citeRegEx" : "UzZaman and Naushad,? \\Q2012\\E",
      "shortCiteRegEx" : "UzZaman and Naushad",
      "year" : 2012
    }, {
      "title" : "TRIPS and TRIOS System for TempEval2: Extracting Temporal Information from Text.",
      "author" : [ "UzZaman", "Naushad", "James Allen" ],
      "venue" : "Proceedings of the 5th International Workshop on Semantic Evaluation. Uppsala, Sweden: Association for Computational Linguistics,",
      "citeRegEx" : "UzZaman et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "UzZaman et al\\.",
      "year" : 2010
    }, {
      "title" : "Temporal Evaluation.” In: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
      "author" : [ "UzZaman", "Naushad", "James Allen" ],
      "venue" : null,
      "citeRegEx" : "UzZaman et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "UzZaman et al\\.",
      "year" : 2011
    }, {
      "title" : "SemEval-2013 Task 1: TempEval-3: Evaluating Time Expressions, Events, and Temporal Relations.",
      "author" : [ "UzZaman", "Naushad", "Hector Llorens", "Leon Derczynski", "James Allen", "Marc Verhagen", "James Pustejovsky" ],
      "venue" : "Second Joint Conference on Lexical and Computational Semantics (*SEM),",
      "citeRegEx" : "UzZaman et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "UzZaman et al\\.",
      "year" : 2013
    }, {
      "title" : "Temporal Closure in an Annotation Environment.",
      "author" : [ "Verhagen", "Marc" ],
      "venue" : "Language Resources and Evaluation",
      "citeRegEx" : "Verhagen and Marc,? \\Q2005\\E",
      "shortCiteRegEx" : "Verhagen and Marc",
      "year" : 2005
    }, {
      "title" : "Temporal Closure in an Annotation Environment.",
      "author" : [ "Verhagen", "Marc" ],
      "venue" : "Language Resources and Evaluation",
      "citeRegEx" : "Verhagen and Marc,? \\Q2005\\E",
      "shortCiteRegEx" : "Verhagen and Marc",
      "year" : 2005
    }, {
      "title" : "SemEval-2007 Task 15: TempEval",
      "author" : [ "Verhagen", "Marc", "Robert Gaizauskas", "Frank Schilder", "Mark Hepple", "Graham Katz", "James Pustejovsky" ],
      "venue" : null,
      "citeRegEx" : "Verhagen et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Verhagen et al\\.",
      "year" : 2007
    }, {
      "title" : "SemEval2010 Task 13: TempEval-2.",
      "author" : [ "Verhagen", "Marc", "Roser Sauri", "Tommaso Caselli", "James Pustejovsky" ],
      "venue" : "Proceedings of the 5th International Workshop on Semantic Evaluation. Uppsala, Sweden: Association for Computational Linguistics,",
      "citeRegEx" : "Verhagen et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Verhagen et al\\.",
      "year" : 2010
    }, {
      "title" : "Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.",
      "author" : [ "Viterbi", "A. (Sept" ],
      "venue" : "IEEE Trans. Inf. Theor",
      "citeRegEx" : "Viterbi and .Sept.,? \\Q2006\\E",
      "shortCiteRegEx" : "Viterbi and .Sept.",
      "year" : 2006
    }, {
      "title" : "Qualitative CSP, Finite CSP, and SAT: Comparing Methods for Qualitative Constraint-based Reasoning.",
      "author" : [ "Westphal", "Matthias", "Stefan Wölfl" ],
      "venue" : "Proceedings of the 21st International Joint Conference on Artifical Intelligence. IJCAI’09",
      "citeRegEx" : "Westphal et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Westphal et al\\.",
      "year" : 2009
    }, {
      "title" : "Representing causation.",
      "author" : [ "Wolff", "Phillip" ],
      "venue" : "Journal of experimental psychology: General",
      "citeRegEx" : "Wolff and Phillip,? \\Q2007\\E",
      "shortCiteRegEx" : "Wolff and Phillip",
      "year" : 2007
    }, {
      "title" : "Models of causation and the semantics of causal verbs.",
      "author" : [ "Wolff", "Phillip", "Grace Song" ],
      "venue" : "Cognitive Psychology",
      "citeRegEx" : "Wolff et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Wolff et al\\.",
      "year" : 2003
    }, {
      "title" : "Categorization inside and outside the laboratory: Essays in honor of Douglas L. Medin. APA decade of behavior series.",
      "author" : [ "Wolff", "Phillip", "Bianca Klettke", "Tatyana Ventura", "Grace Song" ],
      "venue" : "American Psychological Association, xx,",
      "citeRegEx" : "Wolff et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Wolff et al\\.",
      "year" : 2005
    }, {
      "title" : "Jointly Identifying Temporal Relations with Markov Logic.",
      "author" : [ "Yoshikawa", "Katsumasa", "Sebastian Riedel", "Masayuki Asahara", "Yuji Matsumoto" ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Suntec,",
      "citeRegEx" : "Yoshikawa et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yoshikawa et al\\.",
      "year" : 2009
    }, {
      "title" : "Do we need more training data or better models for object detection?",
      "author" : [ "Zhu", "Xiangxin", "Carl Vondrick", "Deva Ramanan", "Charless C. Fowlkes" ],
      "venue" : "British Machine Vision Conference (BMVC)",
      "citeRegEx" : "Zhu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2012
    }, {
      "title" : "Introduction to Semi-Supervised Learning",
      "author" : [ "Zhu", "Xiaojin", "Andrew B. Goldberg", "Ronald Brachman", "Thomas Dietterich" ],
      "venue" : null,
      "citeRegEx" : "Zhu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "TimeML (Pustejovsky et al., 2003), being one of the prominent ones, is a language specification for events and temporal expressions, which was",
      "startOffset" : 7,
      "endOffset" : 33
    }, {
      "referenceID" : 56,
      "context" : "TimeML is the annotation framework used in a series of evaluation campaigns for temporal information processing called TempEval (UzZaman et al., 2013; Verhagen et al., 2007, 2010), in which the ultimate goal is the automatic identification of temporal expressions, events and temporal relations within a text.",
      "startOffset" : 128,
      "endOffset" : 179
    }, {
      "referenceID" : 2,
      "context" : ", 1993; Santorini, 1990), the British National Corpus (BNC) Tagset and the BNC Enriched Tagset2 (Leech et al., 1994).",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 26,
      "context" : "Meanwhile, the TextPro tool suite4 (Pianta et al., 2008), which is the one mainly used in our research, employs the BNC Basic Tagset.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 29,
      "context" : ", 2005), which added semantic role—or predicate-argument relations—annotations to the syntactic tree of the Penn Treebank corpus (Prasad et al., 2008).",
      "startOffset" : 129,
      "endOffset" : 150
    }, {
      "referenceID" : 67,
      "context" : "7 We took the explanations about Semi-supervised Learning from Zhu et al. (2009).",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "neural probabilistic language models (Mikolov et al., 2013).",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 13,
      "context" : "predictive models In predictive models, instead of first collecting context vectors and then re-weighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013; Turian et al., 2010).",
      "startOffset" : 265,
      "endOffset" : 401
    }, {
      "referenceID" : 50,
      "context" : "predictive models In predictive models, instead of first collecting context vectors and then re-weighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013; Turian et al., 2010).",
      "startOffset" : 265,
      "endOffset" : 401
    }, {
      "referenceID" : 13,
      "context" : "Word2Vec9 (Mikolov et al., 2013) is a particularly computationally-efficient predictive model for learning word embeddings from raw text.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 30,
      "context" : "There are several annotation frameworks for events and time expressions that can be viewed as event models, TimeML (Pustejovsky et al., 2003) and ACE (Consortium, 2005) being the prominent ones.",
      "startOffset" : 115,
      "endOffset" : 141
    }, {
      "referenceID" : 41,
      "context" : "(RDFS+OWL) such as LODE (Shaw et al., 2009), SEM (Hage et al.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 30,
      "context" : "Instead of treating a time expression as an event argument, TimeML introduces temporal link annotations to establish dependencies (temporal relations) between events and time expressions (Pustejovsky et al., 2003).",
      "startOffset" : 187,
      "endOffset" : 213
    }, {
      "referenceID" : 30,
      "context" : "TimeML introduces 4 major data structures: EVENT for events, TIMEX3 for time expressions, SIGNAL for temporal signals, and LINK for relations among EVENTs and TIMEX3s (Pustejovsky et al., 2003; Saurí et al., 2006).",
      "startOffset" : 167,
      "endOffset" : 213
    }, {
      "referenceID" : 31,
      "context" : "The resulting standard, ISO 24617-1:2012, SemAF-Time, specifies a formalized XML-based mark-up language facilitating the exchange of temporal information (Pustejovsky et al., 2010).",
      "startOffset" : 154,
      "endOffset" : 180
    }, {
      "referenceID" : 59,
      "context" : "• The corpus created for the first TempEval task (Verhagen et al., 2007) at SemEval2007 employs a simplified version of TimeML.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 60,
      "context" : "• As the TempEval-2 task (Verhagen et al., 2010) at SemEval-2010 attempted to address multilinguality, the corpus released within this task includes texts in Chinese, English, French, Italian, Korean and Spanish.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 56,
      "context" : "• The TempEval-3 corpus created for the TempEval-3 task (UzZaman et al., 2013) at SemEval-2013, however, is based on the latest TimeML annotation guideline version 1.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : "• The creation of evaluation corpus for QA-TempEval (Llorens et al., 2015) does not require manual annotation of all TimeML elements in the documents.",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "gz 8 The TempEval-3 silver corpus is obtained by running automatic annotation systems, TIPSem and TIPSemB (Llorens et al., 2010) and TRIOS (UzZaman and Allen, 2010), on 600K word corpus collected from Gigaword.",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "• The evaluation dataset released for the “TimeLine: Cross-Document Event Ordering” task (Minard et al., 2015) consists of 90 Wikinews articles within specific topics (e.",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 9,
      "context" : "The two corpora are (i) the CELCT corpus containing news articles taken from the Italian Content Annotation Bank (I-CAB) (Magnini et al., 2006), and (ii) the ILC corpus, which consists of 171 news articles collected from the Italian Syntactic-Semantic Treebank, the PAROLE corpus and the web.",
      "startOffset" : 121,
      "endOffset" : 143
    }, {
      "referenceID" : 59,
      "context" : "The VAGUE relation introduced at the first TempEval task (Verhagen et al., 2007) is adopted to cope with ambiguous temporal relations, or to indicate pairs for which no clear temporal relation exists.",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 30,
      "context" : "The ultimate goal of TempEval is the automatic identification of temporal expressions (timexes), events, and temporal relations within a text as specified in TimeML annotation (Pustejovsky et al., 2003).",
      "startOffset" : 176,
      "endOffset" : 202
    }, {
      "referenceID" : 59,
      "context" : "tempeval-1 The first TempEval (Verhagen et al., 2007) focuses only on the categorization of temporal relations into simplified TimeML TLINK types, and only for English.",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 60,
      "context" : "tempeval-2 TempEval-2 (Verhagen et al., 2010) extended the first TempEval, growing into a multilingual task, and adding three more tasks:",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 56,
      "context" : "tempeval-3 TempEval-3 (UzZaman et al., 2013) is different from its predecessor in several aspects:",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 15,
      "context" : "tempeval continuation At the last SemEval-2015, there are several tasks related to temporal processing, taking it further into different directions: cross-document event ordering (Minard et al., 2015), temporal-related question answering (Llorens et al.",
      "startOffset" : 179,
      "endOffset" : 200
    }, {
      "referenceID" : 8,
      "context" : ", 2015), temporal-related question answering (Llorens et al., 2015) and clinical domain (Bethard et al.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 8,
      "context" : "We are particularly interested in the QA TempEval task (Llorens et al., 2015), which requires the participants to perform end-to-end TimeML annotation from the plain text in the same way as in TempEval-3 (Task ABC), but evaluates the systems in terms of correctly answered questions instead of using common information extraction performance measures.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "For some tasks, TIPSem (Llorens et al., 2010), the best performing system in TempEval-2, also performed best in TempEval-3.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "66 Event Extraction TIPSem (Llorens et al., 2010) 82.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "17 Temporal Relation Extraction TIPSem (Llorens et al., 2010) 44.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 6,
      "context" : "35 Temporal Information Processing TIPSem (Llorens et al., 2010) 42.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "timex types, but used TIMEN (Llorens et al., 2012), which is rule-based, to normalize timex values.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "UWTime (Lee et al., 2014) uses a Combinatory Categorial Grammar (CCG) to construct compositional meaning representations, while also considering contextual cues (e.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 54,
      "context" : "TempEval-3 results reported by UzZaman et al. (2013) show that even though the performances of systems for extracting TimeML entities are quite good (>80% F1score), the overall performance of end-to-end temporal information extraction systems suffers due to the low performance on extracting temporal relations.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 30,
      "context" : "TimeML (Pustejovsky et al., 2003) is the annotation framework used in the TempEval series, evaluation exercises focused on temporal information processing, i.",
      "startOffset" : 7,
      "endOffset" : 33
    }, {
      "referenceID" : 29,
      "context" : "TimeML (Pustejovsky et al., 2003) is the annotation framework used in the TempEval series, evaluation exercises focused on temporal information processing, i.e. the extraction of temporal expressions (timexes), events and temporal relations in a text (see Section 3.2 and Section 3.4). According to TempEval-3 results reported by UzZaman et al. (2013), while systems for timex extraction and event extraction tasks yield quite high performances with over 80% F1-scores, the best performing system achieved very low performance on the temporal relation extraction task, bringing down the overall performance on the end-to-end temporal information processing task to only around 30% F1-score.",
      "startOffset" : 8,
      "endOffset" : 352
    }, {
      "referenceID" : 66,
      "context" : "Some other works tried to take advantage of global information to ensure that the pairwise classifications satisfy temporal logic transitivity constraints, using frameworks like Integer Linear Programming and Markov Logic Networks (Chambers and Jurafsky, 2008b; UzZaman and Allen, 2010; Yoshikawa et al., 2009).",
      "startOffset" : 231,
      "endOffset" : 310
    }, {
      "referenceID" : 0,
      "context" : "For the tasks related to temporal relation extraction (Task C and Task C relation type only), there were five participants in total, including ClearTK (Bethard, 2013a), UTTime (Laokulrat et al., 2013) and NavyTime (Chambers, 2013), which are reported in Section 3.5.3. These systems resorted to data-driven approaches for classifying the temporal relation types, using morphosyntactic information (e.g., PoS tags, syntactic parsing information) and lexical semantic information (e.g., WordNet synsets) as features. UTTime additionally used sentence-level semantic information (i.e., predicate-argument structure) as features. Our proposed approach for temporal relation type classification is inspired by recent works on hybrid classification models (Chambers et al., 2014; D’Souza and Ng, 2013). D’Souza and Ng (2013) introduce 437 hand-coded rules along with supervised classification models using lexical relation features (extracted from Merriam-Webster dictionary and WordNet), as well as semantic and discourse features.",
      "startOffset" : 177,
      "endOffset" : 819
    }, {
      "referenceID" : 0,
      "context" : "For the tasks related to temporal relation extraction (Task C and Task C relation type only), there were five participants in total, including ClearTK (Bethard, 2013a), UTTime (Laokulrat et al., 2013) and NavyTime (Chambers, 2013), which are reported in Section 3.5.3. These systems resorted to data-driven approaches for classifying the temporal relation types, using morphosyntactic information (e.g., PoS tags, syntactic parsing information) and lexical semantic information (e.g., WordNet synsets) as features. UTTime additionally used sentence-level semantic information (i.e., predicate-argument structure) as features. Our proposed approach for temporal relation type classification is inspired by recent works on hybrid classification models (Chambers et al., 2014; D’Souza and Ng, 2013). D’Souza and Ng (2013) introduce 437 hand-coded rules along with supervised classification models using lexical relation features (extracted from Merriam-Webster dictionary and WordNet), as well as semantic and discourse features. CAEVO, a CAscading EVent Ordering architecture by Chambers et al. (2014), combines rule-based and data-driven classifiers in a sieve-based architecture for temporal ordering.",
      "startOffset" : 177,
      "endOffset" : 1100
    }, {
      "referenceID" : 33,
      "context" : "• Reichenbach rules based on the analysis of the role played by various tenses of English verbs in conveying temporal discourse (Reichenbach, 1947).",
      "startOffset" : 128,
      "endOffset" : 147
    }, {
      "referenceID" : 26,
      "context" : "• TextPro tool suite5 (Pianta et al., 2008) to get the morphological analysis (PoS tags, shallow phrase chunk) of each token in the text.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 56,
      "context" : "TE3-Silver-data, instead, is a 600K word corpus annotated by the best performing systems at Tempeval-2, which we do not use because it was proven not so useful for temporal relation extraction task (UzZaman et al., 2013).",
      "startOffset" : 198,
      "endOffset" : 220
    }, {
      "referenceID" : 56,
      "context" : "4 the performance of TempRelPro to the other systems participating in temporal relation tasks of TempEval-3, Task C and Task C relation type only, according to the figures reported in (UzZaman et al., 2013).",
      "startOffset" : 184,
      "endOffset" : 206
    }, {
      "referenceID" : 54,
      "context" : "4 the performance of TempRelPro to the other systems participating in temporal relation tasks of TempEval-3, Task C and Task C relation type only, according to the figures reported in (UzZaman et al., 2013). We also compare TempRelPro performance with our preliminary results reported in Mirza and Tonelli (2014b) for Task C relation type only.",
      "startOffset" : 185,
      "endOffset" : 314
    }, {
      "referenceID" : 59,
      "context" : "Some relation types are not used, and the VAGUE relation introduced at the first TempEval task (Verhagen et al., 2007) is adopted to cope with ambiguous temporal relations, or to indicate pairs for which no clear temporal relation exists.",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 56,
      "context" : "dataset The training data set is the TimeML annotated data released by the task organizers, which includes TBAQ-cleaned and TE3-Platinum corpora reused from the TempEval-3 task (UzZaman et al., 2013).",
      "startOffset" : 177,
      "endOffset" : 199
    }, {
      "referenceID" : 8,
      "context" : "evaluation system Given the documents labelled by the participating systems, the evaluation process consists of three main steps (Llorens et al., 2015):",
      "startOffset" : 129,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "The QA TempEval organizers also provide an extra evaluation, augmenting the participating systems with a time expression reasoner (TREFL) as a post-processing step (Llorens et al., 2015).",
      "startOffset" : 164,
      "endOffset" : 186
    }, {
      "referenceID" : 6,
      "context" : "The QA TempEval organizers also provide an extra evaluation, augmenting the participating systems with a time expression reasoner (TREFL) as a post-processing step (Llorens et al., 2015). The TREFL component adds TLINKs between timexes based on their resolved values. Note that TempRelPro already includes the T-T links in the final TimeML documents produced, based on the output of the rule-based sieve for T-T pairs (Section 4.5.2.1). In Table 4.9 we report the performance of TempRelPro compared with participating systems in QA TempEval, augmented with TREFL, as reported in Llorens et al. (2015). A comparison with off-the-shelf systems not optimized for the task, i.",
      "startOffset" : 165,
      "endOffset" : 601
    }, {
      "referenceID" : 6,
      "context" : "TIPSem (Llorens et al., 2010), was also provided.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : ", 2014) and QA TempEval (Llorens et al., 2010).",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 29,
      "context" : ", 2010b) and of causal discourse relations in the Penn Discourse Treebank (Prasad et al., 2008).",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 28,
      "context" : ", 2010b) and of causal discourse relations in the Penn Discourse Treebank (Prasad et al., 2008). In Section 5.4, we propose annotation guidelines for explicit construction of causality inspired by TimeML, trying to take advantage of the clear definition of events, signals and relations proposed by Pustejovsky et al. (2003). This is the first step towards the annotation of a TimeML corpus with causality.",
      "startOffset" : 75,
      "endOffset" : 325
    }, {
      "referenceID" : 65,
      "context" : "In the psychology field, several models have been proposed to model causality, including the counterfactual model (Lewis, 1973), probabilistic contrast model (Cheng and Novick, 1991; Cheng and Novick, 1992) and the dynamics model (Wolff, 2007; Wolff and Song, 2003; Wolff et al., 2005), which is based on Talmy’s force dynamic account of causality (Talmy, 1985, 1988).",
      "startOffset" : 230,
      "endOffset" : 285
    }, {
      "referenceID" : 37,
      "context" : "A common approach is to look for specific cue phrases like because or since or to look for verbs that contain a cause as part of their meaning, such as break (cause to be broken) or kill (cause to die) (Girju et al., 2007; Khoo et al., 2000; Sakaji et al., 2008).",
      "startOffset" : 202,
      "endOffset" : 262
    }, {
      "referenceID" : 29,
      "context" : "” Another scheme annotates causal relations between discourse arguments, in the framework of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008).",
      "startOffset" : 128,
      "endOffset" : 149
    }, {
      "referenceID" : 19,
      "context" : "In Mirza et al. (2014), we have presented the annotation guidelines to capture causality between event pairs, inspired by TimeML.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 49,
      "context" : "As part of a wider annotation effort aimed to annotate texts at the semantic level (Tonelli et al., 2014), within the NewsReader project1, we propose guidelines for the annotation of causal information.",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 31,
      "context" : "In particular, we define causal relations between events based on the TimeML definition of events (Pustejovsky et al., 2010), as including all types of actions (punctual and durative) and states.",
      "startOffset" : 98,
      "endOffset" : 124
    }, {
      "referenceID" : 65,
      "context" : "As causal relations are often not overtly expressed in text (Wolff et al., 2005), we restrict the annotation of CLINKs to the presence of an explicit causal construction linking two events in the same sentence2, as detailed below:",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 19,
      "context" : "We combine the rule-base methods presented in Mirza et al. (2014) with the statistical-based methods presented in Mirza and Tonelli (2014a) in a similar fashion as for temporal relation extraction.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "We combine the rule-base methods presented in Mirza et al. (2014) with the statistical-based methods presented in Mirza and Tonelli (2014a) in a similar fashion as for temporal relation extraction.",
      "startOffset" : 46,
      "endOffset" : 140
    }, {
      "referenceID" : 19,
      "context" : "We combine the rule-base methods presented in Mirza et al. (2014) with the statistical-based methods presented in Mirza and Tonelli (2014a) in a similar fashion as for temporal relation extraction. Moreover, we tried to address one of the issues explained in Mirza and Tonelli (2014a) related to the dependency parser errors, by using another parser which has a better coverage for longrange dependencies.",
      "startOffset" : 46,
      "endOffset" : 285
    }, {
      "referenceID" : 35,
      "context" : "Rink et al. (2010) perform textual graph classification using the same corpus, and make use of manually annotated temporal relation types as a feature to build a classification model for causal relations between events.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 35,
      "context" : "Rink et al. (2010) perform textual graph classification using the same corpus, and make use of manually annotated temporal relation types as a feature to build a classification model for causal relations between events. This results in 57.9% F-score, 15% improvement in performance compared with the system without the additional feature of temporal relations. Do et al. (2011) developed an evaluation corpus by collecting 20 news articles from CNN, allowing the detection of causality between verb-verb, verb-noun, and noun-noun triggered event pairs.",
      "startOffset" : 0,
      "endOffset" : 378
    }, {
      "referenceID" : 35,
      "context" : "Rink et al. (2010) perform textual graph classification using the same corpus, and make use of manually annotated temporal relation types as a feature to build a classification model for causal relations between events. This results in 57.9% F-score, 15% improvement in performance compared with the system without the additional feature of temporal relations. Do et al. (2011) developed an evaluation corpus by collecting 20 news articles from CNN, allowing the detection of causality between verb-verb, verb-noun, and noun-noun triggered event pairs. Causality between event pairs is measured by taking into account Pointwise Mutual Information (PMI) between the cause and the effect. They also incorporate discourse information, specifically the connective types extracted from the Penn Discourse TreeBank (PDTB), and achieve a performance of 46.9% F-score. Ittoo and Bouma (2011) presented a minimally-supervised algorithm that extracts explicit and implicit causal relations based on syntactic-structure-based causal patterns.",
      "startOffset" : 0,
      "endOffset" : 884
    }, {
      "referenceID" : 35,
      "context" : "Rink et al. (2010) perform textual graph classification using the same corpus, and make use of manually annotated temporal relation types as a feature to build a classification model for causal relations between events. This results in 57.9% F-score, 15% improvement in performance compared with the system without the additional feature of temporal relations. Do et al. (2011) developed an evaluation corpus by collecting 20 news articles from CNN, allowing the detection of causality between verb-verb, verb-noun, and noun-noun triggered event pairs. Causality between event pairs is measured by taking into account Pointwise Mutual Information (PMI) between the cause and the effect. They also incorporate discourse information, specifically the connective types extracted from the Penn Discourse TreeBank (PDTB), and achieve a performance of 46.9% F-score. Ittoo and Bouma (2011) presented a minimally-supervised algorithm that extracts explicit and implicit causal relations based on syntactic-structure-based causal patterns. The most recent work of Riaz and Girju (2013) focuses on the identification of causal relations between verbal events.",
      "startOffset" : 0,
      "endOffset" : 1078
    }, {
      "referenceID" : 19,
      "context" : "In Mirza et al. (2014) we presented a simple rule-based system to extract (explicit) event causality from a text.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "In Mirza et al. (2014) we presented a simple rule-based system to extract (explicit) event causality from a text. The rule-based system relies on an algorithm that, given a term t belonging to affect, link, causative verbs or causal signals (as listed in the annotation guidelines presented in Section 5.4), looks for specific dependency constructions where t is connected to the two observed events. If such dependencies are found, a CLINK is automatically set between the two events. In Mirza and Tonelli (2014a) we presented a data-driven approach to extract causal relations between events, making use of the Causal-TimeBank corpus (Section 5.",
      "startOffset" : 3,
      "endOffset" : 515
    }, {
      "referenceID" : 19,
      "context" : "Following the success of a hybrid approach in a sieve-based architecture for the temporal relation extraction task, we decided to combine in a similar way the rule-based methods in Mirza et al. (2014) with the statistical methods in Mirza and Tonelli (2014a), for identifying explicit causal links (CLINKs) in a text.",
      "startOffset" : 181,
      "endOffset" : 201
    }, {
      "referenceID" : 19,
      "context" : "Following the success of a hybrid approach in a sieve-based architecture for the temporal relation extraction task, we decided to combine in a similar way the rule-based methods in Mirza et al. (2014) with the statistical methods in Mirza and Tonelli (2014a), for identifying explicit causal links (CLINKs) in a text.",
      "startOffset" : 181,
      "endOffset" : 259
    }, {
      "referenceID" : 19,
      "context" : "4320 Mirza et al. (2014) rule-based system 0.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 19,
      "context" : "4320 Mirza et al. (2014) rule-based system 0.3679 0.1226 0.1840 Mirza and Tonelli (2014a) data-driven system 0.",
      "startOffset" : 5,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : "3 the performance of CauseRelPro to the performances of Mirza et al. (2014) rule-based system (18.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 19,
      "context" : "3 the performance of CauseRelPro to the performances of Mirza et al. (2014) rule-based system (18.40% F1-score) and Mirza and Tonelli (2014a) data-driven system (33.",
      "startOffset" : 56,
      "endOffset" : 142
    }, {
      "referenceID" : 19,
      "context" : "CauseRelPro is a combination of the rule-based methods presented in Mirza et al. (2014) and the statisticalbased methods presented in Mirza and Tonelli (2014a), with some modifications regarding the tools and resources used, including the dependency parser (Stanford CoreNLP vs Mate tools), the lists of causal signals and verbs (augmented with paraphrases from PPDB, and clustered) and the algorithm for the classifier (YamCha SVM vs LIBLINEAR SVM).",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 19,
      "context" : "CauseRelPro is a combination of the rule-based methods presented in Mirza et al. (2014) and the statisticalbased methods presented in Mirza and Tonelli (2014a), with some modifications regarding the tools and resources used, including the dependency parser (Stanford CoreNLP vs Mate tools), the lists of causal signals and verbs (augmented with paraphrases from PPDB, and clustered) and the algorithm for the classifier (YamCha SVM vs LIBLINEAR SVM).",
      "startOffset" : 68,
      "endOffset" : 160
    }, {
      "referenceID" : 19,
      "context" : "CauseRelPro is a combination of the rule-based methods presented in Mirza et al. (2014) and the statisticalbased methods presented in Mirza and Tonelli (2014a), with some modifications regarding the tools and resources used, including the dependency parser (Stanford CoreNLP vs Mate tools), the lists of causal signals and verbs (augmented with paraphrases from PPDB, and clustered) and the algorithm for the classifier (YamCha SVM vs LIBLINEAR SVM). We have also evaluated CauseRelPro using the Causal-TimeBank corpus in stratified 10-fold cross-validation, resulting in 40.95% F1-score, much better than the previous two systems reported in Mirza et al. (2014) and Mirza and Tonelli (2014a), even though they are not directly comparable because 5-fold cross-validation was used instead.",
      "startOffset" : 68,
      "endOffset" : 663
    }, {
      "referenceID" : 19,
      "context" : "CauseRelPro is a combination of the rule-based methods presented in Mirza et al. (2014) and the statisticalbased methods presented in Mirza and Tonelli (2014a), with some modifications regarding the tools and resources used, including the dependency parser (Stanford CoreNLP vs Mate tools), the lists of causal signals and verbs (augmented with paraphrases from PPDB, and clustered) and the algorithm for the classifier (YamCha SVM vs LIBLINEAR SVM). We have also evaluated CauseRelPro using the Causal-TimeBank corpus in stratified 10-fold cross-validation, resulting in 40.95% F1-score, much better than the previous two systems reported in Mirza et al. (2014) and Mirza and Tonelli (2014a), even though they are not directly comparable because 5-fold cross-validation was used instead.",
      "startOffset" : 68,
      "endOffset" : 693
    }, {
      "referenceID" : 19,
      "context" : "CauseRelPro is a combination of the rule-based methods presented in Mirza et al. (2014) and the statisticalbased methods presented in Mirza and Tonelli (2014a), with some modifications regarding the tools and resources used, including the dependency parser (Stanford CoreNLP vs Mate tools), the lists of causal signals and verbs (augmented with paraphrases from PPDB, and clustered) and the algorithm for the classifier (YamCha SVM vs LIBLINEAR SVM). We have also evaluated CauseRelPro using the Causal-TimeBank corpus in stratified 10-fold cross-validation, resulting in 40.95% F1-score, much better than the previous two systems reported in Mirza et al. (2014) and Mirza and Tonelli (2014a), even though they are not directly comparable because 5-fold cross-validation was used instead. Furthermore, we manually evaluated the output of the causal verb rules, and found that among the 11 false positives, 7 causal links are actually correct. The wrongly extracted ones are due to the dependency parser errors. However, we found that compared with the previous dependency parser used, i.e. Stanford CoreNLP dependency parser, the dependency parser of Mate tools performs better in connecting the events involved in causal relations. As has been explained in Chapter 5, we intentionally added causality annotation on the TimeBank corpus, which is layered with the annotation of temporal entities and temporal relations, because we want to investigate the strict connection between temporal and causal relations. In fact, there is a temporal constraint in causality, i.e. the cause must occur BEFORE the effect. In the following chapter (Chapter 7) we will discuss the interaction between these two types of relations. Bethard and Martin (2008); Rink et al.",
      "startOffset" : 68,
      "endOffset" : 1743
    }, {
      "referenceID" : 19,
      "context" : "CauseRelPro is a combination of the rule-based methods presented in Mirza et al. (2014) and the statisticalbased methods presented in Mirza and Tonelli (2014a), with some modifications regarding the tools and resources used, including the dependency parser (Stanford CoreNLP vs Mate tools), the lists of causal signals and verbs (augmented with paraphrases from PPDB, and clustered) and the algorithm for the classifier (YamCha SVM vs LIBLINEAR SVM). We have also evaluated CauseRelPro using the Causal-TimeBank corpus in stratified 10-fold cross-validation, resulting in 40.95% F1-score, much better than the previous two systems reported in Mirza et al. (2014) and Mirza and Tonelli (2014a), even though they are not directly comparable because 5-fold cross-validation was used instead. Furthermore, we manually evaluated the output of the causal verb rules, and found that among the 11 false positives, 7 causal links are actually correct. The wrongly extracted ones are due to the dependency parser errors. However, we found that compared with the previous dependency parser used, i.e. Stanford CoreNLP dependency parser, the dependency parser of Mate tools performs better in connecting the events involved in causal relations. As has been explained in Chapter 5, we intentionally added causality annotation on the TimeBank corpus, which is layered with the annotation of temporal entities and temporal relations, because we want to investigate the strict connection between temporal and causal relations. In fact, there is a temporal constraint in causality, i.e. the cause must occur BEFORE the effect. In the following chapter (Chapter 7) we will discuss the interaction between these two types of relations. Bethard and Martin (2008); Rink et al. (2010) showed that including temporal relation information in detecting causal links results in improved classification performance.",
      "startOffset" : 68,
      "endOffset" : 1763
    }, {
      "referenceID" : 35,
      "context" : "Rink et al. (2010) performed textual graph classification using the same corpus, and make use of manually annotated temporal relation types as a feature to build a classification model for causal relations between events.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : ", 2014) and Word2Vec (Mikolov et al., 2013) are the two popular word embedding algorithms recently, each represents (i) and (ii), respectively.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 29,
      "context" : "Most works on implicit discourse relations focused on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), in which relations are annotated at the discourse level.",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 28,
      "context" : "Most works on implicit discourse relations focused on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), in which relations are annotated at the discourse level. There are five distinct groups of relations: implicit, explicit, alternative lexicalizations, entity relations and no relation; each could carry multiple relation types that are organized into a three-level hierarchy. The top level relations, for example, includes Temporal, Contingency, Comparison and Expansion. Braud and Denis (2015) presented a detailed comparative studies for assessing the benefit of unsupervised word representations, i.",
      "startOffset" : 90,
      "endOffset" : 506
    }, {
      "referenceID" : 28,
      "context" : "Most works on implicit discourse relations focused on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), in which relations are annotated at the discourse level. There are five distinct groups of relations: implicit, explicit, alternative lexicalizations, entity relations and no relation; each could carry multiple relation types that are organized into a three-level hierarchy. The top level relations, for example, includes Temporal, Contingency, Comparison and Expansion. Braud and Denis (2015) presented a detailed comparative studies for assessing the benefit of unsupervised word representations, i.e. one-hot word pair representations against low-dimensional ones based on Brown cluster and word embeddings, for identifying implicit discourse relations in PDTB. Baroni et al. (2014) provides a systematic comparison between count-based and predictive word embeddings, on a wide range of lexical semantic tasks, including semantic relatedness, synonym detection, concept categorization, selectional preferences and analogy.",
      "startOffset" : 90,
      "endOffset" : 798
    }, {
      "referenceID" : 46,
      "context" : "Furthermore, instead of using general-purpose word embeddings, several works presented methods for building task-specific word embeddings (Boros et al., 2014; Hashimoto et al., 2015; Nguyen and Grishman, 2014; Tang et al., 2014), which may also be beneficial for temporal ordering task.",
      "startOffset" : 138,
      "endOffset" : 228
    }, {
      "referenceID" : 56,
      "context" : "Several systems have been proposed to classify temporal relations between temporal entities according to TimeML specifications, as shown in the TempEval evaluation campaigns (UzZaman et al., 2013; Verhagen et al., 2007, 2010).",
      "startOffset" : 174,
      "endOffset" : 225
    }, {
      "referenceID" : 66,
      "context" : ", 2012) and Markov Logic Networks (Yoshikawa et al., 2009) to optimize the classification output.",
      "startOffset" : 34,
      "endOffset" : 58
    }, {
      "referenceID" : 28,
      "context" : "The empirical evaluation on the Penn Discourse TreeBank (PDTB) (Prasad et al., 2007) dataset yields 0.",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 63,
      "context" : ", 2012) and Markov Logic Networks (Yoshikawa et al., 2009) to optimize the classification output. To our knowledge, however, no studies have so far tackled the problem (i) to improve on the quality of temporal relation classification. The only work partially related to this topic is by Derczynski and Gaizauskas (2010), whose CAVaT tool is able to perform error checking and validation of TimeML documents.",
      "startOffset" : 35,
      "endOffset" : 320
    }, {
      "referenceID" : 0,
      "context" : "Laokulrat et al. (2014) partially complete documents by deducing relations before running a classifier: they extract data by using timegraphs, and apply a stacked learning method to temporal relation classification.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "Laokulrat et al. (2014) partially complete documents by deducing relations before running a classifier: they extract data by using timegraphs, and apply a stacked learning method to temporal relation classification. However, for performance reasons, they do not deduce all possible relations, e.g., they arbitrarily limit the number of deduced relations for each document to 10,000. More generally concerning the costs of (ii), no past work has taken them into account from a statistical experimental viewpoint. Semi-supervised setting for temporal relation extraction were made possible in the TempEval-3 shared task. The task organizers released the TempEval-3 silver corpus, a 600K corpus collected from Gigaword, which is automatically annotated by best performing systems in the preceding TempEval task. In TempEval-3, none of the participants submitted the systems trained on this silver data, most probably because the amount of gold data released within the task was enough to achieve good results in a fully supervised setting. Fisher and Simmons (2015) presents a semi-supervised spectral model for a sequential relation labelling task for discourse parsing, in which Temporal and Contingency/Causal relations are included in the possible relation types.",
      "startOffset" : 0,
      "endOffset" : 1063
    }, {
      "referenceID" : 60,
      "context" : "4), TempEval-2 (Verhagen et al., 2010), extended the first TempEval with a multilingual task.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "TIPSem and TIPSem-B (Llorens et al., 2010).",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "TIPSem and TIPSem-B (Llorens et al., 2010). For temporal expression extraction, HeidelTime (Strötgen et al., 2014) is perhaps the temporal expression tagging system covering the most languages. HeidelTime currently understands documents in 11 languages, including English, German, Dutch, Vietnamese, Arabic, Spanish, Italian, French, Chinese, Russian, and Croatian. Even though the most recent work by Strötgen and Gertz (2015) presented an automatic extension approach to cover around 200+ languages in the world, we believe that for low-resource (and lessexplored) languages such as Indonesian, a manual extension effort is still required.",
      "startOffset" : 21,
      "endOffset" : 428
    }, {
      "referenceID" : 43,
      "context" : "HeidelTime (Strötgen et al., 2014)) and event extraction (e.g. Caselli et al. (2011b)), there is no complete system for temporal information processing for Italian.",
      "startOffset" : 12,
      "endOffset" : 86
    }, {
      "referenceID" : 26,
      "context" : "• TextPro5 (Pianta et al., 2008), a suite of NLP tools for processing English and Italian texts.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : ", 2013) and TIMEN (Llorens et al., 2012).",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "For Italian, the performance for timex normalization is still considerably lower than the state-of-the-art system for English, UWTime (Lee et al., 2014), i.",
      "startOffset" : 134,
      "endOffset" : 152
    }, {
      "referenceID" : 35,
      "context" : "Confirming the finding of several previous works (Bethard and Martin, 2008; Mirza and Tonelli, 2014a; Rink et al., 2010), using temporal information as features boosted the performance of our causal relation extraction system.",
      "startOffset" : 49,
      "endOffset" : 120
    }, {
      "referenceID" : 46,
      "context" : "Furthermore, instead of using general-purpose word embeddings, several works presented methods for building task-specific word embeddings (Boros et al., 2014; Hashimoto et al., 2015; Nguyen and Grishman, 2014; Tang et al., 2014), which may also be beneficial for temporal ordering and causality extraction task.",
      "startOffset" : 138,
      "endOffset" : 228
    } ],
    "year" : 2016,
    "abstractText" : "Structured information resulting from temporal information processing is crucial for a variety of natural language processing tasks, for instance to generate timeline summarization of events from news documents, or to answer temporal/causal-related questions about some events. In this thesis we present a framework for an integrated temporal and causal relation extraction system. We first develop a robust extraction component for each type of relations, i.e. temporal order and causality. We then combine the two extraction components into an integrated relation extraction system, CATENA—CAusal and Temporal relation Extraction from NAtural language texts—, by utilizing the presumption about event precedence in causality, that causing events must happened BEFORE resulting events. Several resources and techniques to improve our relation extraction systems are also discussed, including word embeddings and training data expansion. Finally, we report our adaptation efforts of temporal information processing for languages other than English, namely Italian and Indonesian.",
    "creator" : "pdfLaTeX"
  }
}