{"conference": "ICLR 2017 conference submission", "title": "Batch Policy Gradient  Methods for  Improving Neural Conversation Models", "abstract": "We study reinforcement learning of chat-bots with recurrent neural network architectures when the rewards are noisy and expensive to obtain. For instance, a chat-bot used in automated customer service support can be scored by quality assurance agents, but this process can be expensive, time consuming and noisy.  Previous reinforcement learning work for natural language uses on-policy updates and/or is designed for on-line learning settings. We demonstrate empirically that such strategies are not appropriate for this setting and develop an off-policy batch policy gradient method (\\bpg). We demonstrate the efficacy of our method via a series of synthetic experiments and an Amazon Mechanical Turk experiment on a restaurant recommendations dataset.", "reviews": [{"is_meta_review": true, "comments": "The paper discuss a \"batch\" method for RL setup to improve chat-bots.\nThe authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. \n\nI find the writing clear, and the algorithm a natural extension of the online version.\n\nBelow are some constructive remarks:\n- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:\n- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.\n- section 2.2:\n   sentence before last: s' is not defined. \n   last sentence: missing \"... in the stochastic case.\" at the end.\n- Section 4.1 last paragraph: \"While Bot-1 is not significant ...\" => \"While Bot-1 is not significantly different from ML ...\"", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "This is an interesting and timely paper combining off-policy learning with seq2seq models to train a chatbot on a restaurant reservation task, using labels collected through Amazon Mechanical Turk while using the bot with a baseline maximum likelihood policy. \n The paper is clear, well-written and well-executed. Although the improvements are modest and the actual novelty of the paper is limited (combining known pieces in a rather straightforward way), this is still an interesting and informative read, and will probably be of interest to many people at ICLR.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Revisions", "OTHER_KEYS": "Kirthevasan Kandasamy", "comments": "Once again, we'd like to thank all reviewers for the feedback. We have updated the manuscript accordingly. The changes are done in magenta so that it would be easier to identify them.\n\n@Reviewer2:\ns' is defined in the first para of section 2.2. Also, the expectation in the statement at the end of section 2.2 is to account for the stochasticity in the rewards.", "IS_META_REVIEW": false, "DATE": "03 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "Review", "is_meta_review": false, "comments": "This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning \u2013 training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.\n\nWhile the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. \n\nMy main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.\n\nReferences:\n\nWen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. \"A Network-based End-to-End Trainable Task-oriented Dialogue System.\" arXiv preprint arXiv:1604.04562 (2016).\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "is_meta_review": false, "comments": "The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.\nThe approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on \"clarification regarding batch vs. online setting\").\nThe artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement. ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "clearly written, natural extension of previous work", "is_meta_review": false, "comments": "The paper discuss a \"batch\" method for RL setup to improve chat-bots.\nThe authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. \n\nI find the writing clear, and the algorithm a natural extension of the online version.\n\nBelow are some constructive remarks:\n- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:\n- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.\n- section 2.2:\n   sentence before last: s' is not defined. \n   last sentence: missing \"... in the stochastic case.\" at the end.\n- Section 4.1 last paragraph: \"While Bot-1 is not significant ...\" => \"While Bot-1 is not significantly different from ML ...\"\n\n\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Clarification regarding batch vs. online setting", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "08 Dec 2016", "is_meta_review": false}, {"TITLE": "synthetic experiment", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 8, "authors": "Kirthevasan Kandasamy, Yoram Bachrach, Ryota Tomioka, Daniel Tarlow, David Carter", "accepted": true, "id": ""}
