{
  "name" : "1510.04104.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Preliminary Study on the Learning Informativeness of Data Subsets",
    "authors" : [ "Simon Kaltenbacher", "Nicholas H. Kirk", "Dongheui Lee" ],
    "emails" : [ "simon.kaltenbacher@campus.lmu.de", "nicholas.kirk@tum.de", "dhlee@tum.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Estimating the internal state of a robotic system is complex: this is performed from multiple heterogeneous sensor inputs and knowledge sources. Discretization of such inputs is done to capture saliences, represented as symbolic information, which often presents structure and recurrence. As these sequences are used to reason over complex scenarios [1], a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention [1], [2]. Such problems need to be addressed to ensure timely and meaningful human-robot interaction.\nOur work is towards understanding the variability of learning informativeness when training on subsets of a given input dataset. This is in view of reducing the training size while retaining the majority of the symbolic learning potential. We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources [3].\nPosterior Evaluation Distribution of Subsets\nWe computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (∼ 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4]), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) [5]. In our semantic tests, on average 85% of the quality can be obtained by training on a random ∼ 4% subset of the original corpus (e.g. as in Fig. 1, 5 random million lines yield 64.14% instead of 75.14%).\nOur claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II). It is therefore possible to select the best random subset for a given size, if an information criterion is known. Such metric is currently under investigation. Within the robotics domain, in order to reduce computational complexity of the training phase, cardinality reduction of human-written instructions is particularly important for non-recursive online training algorithms, such as current symbol-based probabilistic reasoning systems [1], [3], [6].\n1S.K. is with Ludwig Maximilian University of Munich, Germany simon.kaltenbacher@campus.lmu.de\n2N.H.K. and D.L. are with the Technical University of Munich, Germany {nicholas.kirk,dhlee}@tum.de"
    }, {
      "heading" : "CHI-SQUARE AND ANDERSON-DARLING TESTS SHOWING THERE IS NO GAUSSIAN NULL HYPOTHESIS REJECTION FOR WORD2VEC AND PERPLEXITY ACCURACY VALUES OF RANDOM SUBSETS (10% SIGNIFICANCE LEVEL).",
      "text" : ""
    }, {
      "heading" : "RANDOM SUBSETS.",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Online prediction of activities with structure: Exploiting contextual associations and sequences",
      "author" : [ "N.H. Kirk", "K. Ramirez-Amaro", "E. Dean-Leon", "M. Saveriano", "G. Cheng" ],
      "venue" : "2015 IEEE-RAS International Conference on Humanoid Robots, IEEE, 2015.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Controlled natural languages for language generation in artificial cognition",
      "author" : [ "N.H. Kirk", "D. Nyga", "M. Beetz" ],
      "venue" : "2014 IEEE International Conference on Robotics and Automation (ICRA), pp. 6667–6672, IEEE, 2014.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Understanding and executing instructions for everyday manipulation tasks from the world wide web",
      "author" : [ "M. Tenorth", "D. Nyga", "M. Beetz" ],
      "venue" : "2010 IEEE International Conference on Robotics and Automation (ICRA), pp. 1486–1491, IEEE, 2010.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean" ],
      "venue" : "Advances in neural information processing systems, pp. 3111–3119, 2013.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Scaling to very very large corpora for natural language disambiguation",
      "author" : [ "M. Banko", "E. Brill" ],
      "venue" : "Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pp. 26–33, Association for Computational Linguistics, 2001.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Towards learning object affordance priors from technical texts",
      "author" : [ "N.H. Kirk" ],
      "venue" : "”Active Learning in Robotics” Workshop, 2014 IEEE-RAS International Conference on Humanoid Robots, IEEE, 2014.  ar  X  iv  :1  51 0.  04  10  4v  1 [  cs  .C  L  ]  2  8  Se  p  20  15",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "As these sequences are used to reason over complex scenarios [1], a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention [1], [2].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "As these sequences are used to reason over complex scenarios [1], a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention [1], [2].",
      "startOffset" : 292,
      "endOffset" : 295
    }, {
      "referenceID" : 1,
      "context" : "As these sequences are used to reason over complex scenarios [1], a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention [1], [2].",
      "startOffset" : 297,
      "endOffset" : 300
    }, {
      "referenceID" : 2,
      "context" : "We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources [3].",
      "startOffset" : 219,
      "endOffset" : 222
    }, {
      "referenceID" : 3,
      "context" : "We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4]), and of n-gram perplexity.",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 4,
      "context" : "semantic measures) [5].",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Within the robotics domain, in order to reduce computational complexity of the training phase, cardinality reduction of human-written instructions is particularly important for non-recursive online training algorithms, such as current symbol-based probabilistic reasoning systems [1], [3], [6].",
      "startOffset" : 280,
      "endOffset" : 283
    }, {
      "referenceID" : 2,
      "context" : "Within the robotics domain, in order to reduce computational complexity of the training phase, cardinality reduction of human-written instructions is particularly important for non-recursive online training algorithms, such as current symbol-based probabilistic reasoning systems [1], [3], [6].",
      "startOffset" : 285,
      "endOffset" : 288
    }, {
      "referenceID" : 5,
      "context" : "Within the robotics domain, in order to reduce computational complexity of the training phase, cardinality reduction of human-written instructions is particularly important for non-recursive online training algorithms, such as current symbol-based probabilistic reasoning systems [1], [3], [6].",
      "startOffset" : 290,
      "endOffset" : 293
    } ],
    "year" : 2015,
    "abstractText" : "Estimating the internal state of a robotic system is complex: this is performed from multiple heterogeneous sensor inputs and knowledge sources. Discretization of such inputs is done to capture saliences, represented as symbolic information, which often presents structure and recurrence. As these sequences are used to reason over complex scenarios [1], a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention [1], [2]. Such problems need to be addressed to ensure timely and meaningful human-robot interaction. Our work is towards understanding the variability of learning informativeness when training on subsets of a given input dataset. This is in view of reducing the training size while retaining the majority of the symbolic learning potential. We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources [3].",
    "creator" : "LaTeX with hyperref package"
  }
}