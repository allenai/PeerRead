{
  "name" : "1106.1803.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Improving the EÆ ien y of Indu tive Logi Programming\nThrough the Use of Query Pa ks\nHendrik Blo keel hendrik.blo keel s.kuleuven.a .be Katholieke Universiteit Leuven, Department of Computer S ien e Celestijnenlaan 200A, B-3001 Leuven, Belgium\nLu Dehaspe lu .dehaspe pharmadm. om PharmaDM, Amba htenlaan 54D, B-3001 Leuven, Belgium\nBart Demoen bart.demoen s.kuleuven.a .be Gerda Janssens gerda.janssens s.kuleuven.a .be Jan Ramon jan.ramon s.kuleuven.a .be Katholieke Universiteit Leuven, Department of Computer S ien e Celestijnenlaan 200A, B-3001 Leuven, Belgium\nHenk Vande asteele henk.vande asteele pharmadm. om PharmaDM, Amba htenlaan 54D, B-3001 Leuven, Belgium\nAbstra t\nIndu tive logi programming, or relational learning, is a powerful paradigm for ma hine learning or data mining. However, in order for ILP to be ome pra ti ally useful, the eÆ ien y of ILP systems must improve substantially. To this end, the notion of a query pa k is introdu ed: it stru tures sets of similar queries. Furthermore, a me hanism is des ribed for exe uting su h query pa ks. A omplexity analysis shows that onsiderable eÆ ien y improvements an be a hieved through the use of this query pa k exe ution me hanism. This laim is supported by empiri al results obtained by in orporating support for query pa k exe ution in two existing learning systems."
    }, {
      "heading" : "1. Introdu tion",
      "text" : "Many data mining algorithms employ to some extent a generate-and-test approa h: large amounts of partial or omplete hypotheses are generated and evaluated during the data mining pro ess. This evaluation usually involves testing the hypothesis on a large data set, a pro ess whi h is typi ally linear in the size of the data set. Examples of su h data mining algorithms are Apriori (Agrawal et al., 1996), de ision tree algorithms (Quinlan, 1993a; Breiman et al., 1984), algorithms indu ing de ision rules (Clark & Niblett, 1989), et .\nEven though the sear h through the hypothesis spa e is seldom exhaustive in pra ti al situations, and lever bran h-and-bound or greedy sear h strategies are employed, the number of hypotheses generated and evaluated by these approa hes may still be huge. This is espe ially true when a omplex hypothesis spa e is used, as is often the ase in indu tive logi programming (ILP), where the sheer size of the hypothesis spa e is an important\nontribution to the high omputational omplexity of most ILP approa hes. This omputational omplexity an be redu ed, however, by exploiting the fa t that there are many similarities between hypotheses.\n2002 AI A ess Foundation and Morgan Kaufmann Publishers. All rights reserved.\nMost ILP systems build a hypothesis one lause at a time. This sear h for a single lause is what we will be on erned with in the rest of this paper, and so the word \\hypothesis\" further on will usually refer to a single lause. The lause sear h spa e is typi ally stru tured as a latti e. Be ause lauses lose to one another in the latti e are similar, the omputations involved in evaluating them will be similar as well. In other words, many of the omputations that are performed when evaluating one lause (whi h boils down to exe uting a query\nonsisting of the body of the lause) will have to be performed again when evaluating the next lause. Storing ertain intermediate results during the omputation for later use ould be a solution (e.g., tabling as in the XSB Prolog engine, Chen & Warren, 1996), but may be infeasible in pra ti e be ause of its memory requirements. It be omes more feasible if the sear h is reorganised so that intermediate results are always used shortly after they have been omputed; this an be a hieved to some extent by rearranging the omputations. The best way of removing the redundan y, however, seems to be to re-implement the exe ution strategy of the queries in su h a way that as mu h omputation as possible is e e tively shared.\nIn this paper we dis uss a strategy for exe uting sets of queries, organised in so- alled query pa ks, that avoids the redundant omputations. The strategy is presented as an adaptation of the standard Prolog exe ution me hanism. The adapted exe ution me hanism has been implemented in ilProlog, a Prolog system dedi ated to indu tive logi programming. Several indu tive logi programming systems have been re-implemented to make use of this dedi ated engine, and using these new implementations we obtained experimental results showing in some ases a speed-up of more than an order of magnitude. Thus, our work signi antly ontributes to the appli ability of indu tive logi programming to real world data mining tasks. In addition, we believe it may ontribute to the state of the art in query optimisation in relational databases. Indeed, in the latter eld there has been a lot of work on the optimisation of individual queries or relatively small sets of queries, but mu h less on the optimisation of large groups of very similar queries, whi h understandably did not get mu h attention before the advent of data mining. Optimisation of groups of queries for relational databases seems an interesting resear h area now, and we believe te hniques similar to the ones proposed here might be relevant in that area.\nThe remainder of this paper is stru tured as follows. In Se tion 2 we pre isely des ribe the ILP problem setting in whi h this work is set. In Se tion 3 we de ne the notion of a query pa k and indi ate how it would be exe uted by a standard Prolog interpreter and what\nomputational redundan y this auses. We further des ribe an exe ution me hanism for query pa ks that makes it possible to avoid the redundant omputations that would arise if all queries in the pa k were run separately, and show how it an be implemented by making a few small but signi ant extensions to the WAM, the standard Prolog exe ution me hanism. In Se tion 4 we des ribe how the query pa k exe ution strategy an be in orporated in two existing indu tive logi programming algorithms (Tilde and Warmr). In Se tion 5 we present experimental results that illustrate the speed-up that these systems a hieve by using the query pa k exe ution me hanism. In Se tion 6 we dis uss related work and in Se tion 7 we present on lusions and some dire tions for future work."
    }, {
      "heading" : "2. Indu tive Logi Programming",
      "text" : "Indu tive logi programming (Muggleton & De Raedt, 1994) is situated in the interse tion of ma hine learning or data mining on the one hand, and logi programming on the other hand. It shares with the former elds the goal of nding patterns in data, patterns that an be used to build predi tive models or to gain insight in the data. With logi programming it shares the use of lausal rst order logi as a representation language for both data and hypotheses. In the remainder of this text we will use some basi notions from logi programming, su h as literals, onjun tive queries, and variable substitutions. We will use Prolog notation throughout the paper. For an introdu tion to Prolog and logi programming see Bratko (1990).\nIndu tive logi programming an be used for many di erent purposes, and the problem statements found in ILP papers onsequently vary. In this arti le we onsider the so- alled learning from interpretations setting (De Raedt & D zeroski, 1994; De Raedt, 1997). It has been argued elsewhere that this setting, while slightly less powerful than the standard ILP setting (it has problems with, e.g., learning re ursive predi ates), is suÆ ient for most pra ti al purposes and s ales up better (Blo keel et al., 1999).\nWe formulate the learning task in su h a way that it overs a number of di erent problem statements. More spe i ally, we onsider the problem of dete ting for a set of onjun tive queries for whi h instantiations of ertain variables ea h query su eeds. These variables are alled key variables, and a grounding substitution for them is alled a key instantiation. The intuition is that an example in the learning task is uniquely identi ed by a single key instantiation.\nThe link with ILP systems that learn lauses is then as follows. The sear h performed by an ILP system is dire ted by regularly evaluating andidate lauses. Let us denote su h a andidate lause by Head(X) Body(X;Y ) where X represents a ve tor of variables appearing in the head of the lause and Y represents additional variables that o ur in the body. We assume that the head is a single literal and that a list of examples is given, where ea h example is of the form Head(X) with a substitution that grounds X. Examples may be labelled (e.g., as positive or negative), but this is not essential in our setting. While an example an be represented as a fa t Head(X) when learning de nite Horn lauses, we\nan also onsider it just a tuple X . Both notations will be used in this paper.\nIntuitively, when positive and negative examples are given, one wants to nd a lause that overs as many positive examples as possible, while overing few or no negatives. Whether a single example Head(X) is overed by the lause or not an be determined by running the query ? Body(X;Y ) . In other words, evaluating a lause boils down to running a number of queries onsisting of the body of the lause. For simpli ity of notation, we will often denote a onjun tive query by just the onjun tion (without the ? symbol).\nIn some less typi al ILP settings, the ILP algorithm does not sear h for Horn lauses but rather for general lauses, e.g., Claudien (De Raedt & Dehaspe, 1997) or for frequent patterns that an be expressed as onjun tive queries, e.g., Warmr(Dehaspe & Toivonen, 1999). These settings an be handled by our approa h as well: all that is needed is a mapping from hypotheses to queries that allow to evaluate these hypotheses. Su h a mapping is de ned by De Raedt and Dehaspe (1997) for Claudien; for Warmr it is trivial.\nGiven a set of queries S and a set of examples E, the main task is to determine whi h queries Q 2 S over whi h examples e 2 E. We formalise this using the notion of a result set:\nDe nition 1 (Result set) The result set of a set of queries S in a dedu tive database D for key K and example set E, is\nRS(S;K;D;E) = f(K ; i)jQ\ni\n2 S and K 2 E and Q\ni\nsu eeds in Dg\nSimilar to the learning from interpretations setting de ned in (De Raedt, 1997), the\nproblem setting an now be stated as:\nGiven: a set of onjun tive queries S, a dedu tive database D, a tuple K of variables that o ur in ea h query in S, and an example set E\nFind: the result set RS(S;K;D;E); i.e., nd for ea h query Q in S those ground instantiations of K for whi h K 2 E and Q su eeds in D.\nExample 1 Assume an ILP system learning a de nition for grandfather/2 wants to evaluate the following hypotheses:\ngrandfather(X,Y) :- parent(X,Z), parent(Z,Y), male(X). grandfather(X,Y) :- parent(X,Z), parent(Z,Y), female(X).\nExamples are of the form grandfather(gf,g ) where gf and g are onstants; hen e ea h example is uniquely identi ed by a ground substitution of the tuple (X;Y ). So in the above problem setting the set of Prolog queries S equals f(?- parent(X,Z), parent(Z,Y), male(X)), (?- parent(X,Z), parent(Z,Y), female(X))g and the key K equals (X;Y ). Given a query Q\ni\n2 S, nding all tuples (x; y) for whi h ((x; y); i) 2 R (with R the result\nset as de ned above) is equivalent to nding whi h of the grandfather(x,y) fa ts in the example set are predi ted by the lause grandfather(X,Y) :- Q\ni\n.\nThe generality of our problem setting follows from the fa t that on e it is known whi h queries su eed for whi h examples, the statisti s and heuristi s that typi al ILP systems use an be readily obtained from this. A few examples:\ndis overy of frequent patterns (Dehaspe & Toivonen, 1999): for ea h query Q\ni\nthe\nnumber of key instantiations for whi h it su eeds just needs to be ounted, i.e., freq(Q\ni\n) = jfK j(K ; i) 2 Rgj with R the result set.\nindu tion of Horn lauses (Muggleton, 1995; Quinlan, 1993b): the a ura y of a\nlause H :- Q\ni\n(de ned as the number of examples for whi h body and head hold,\ndivided by the number of examples for whi h the body holds) an be omputed as jfK j(K ;i)2R^Dj=H gj\njfK j(K ;i)2Rgj\nwith R the result set.\nindu tion of rst order lassi ation or regression trees (Kramer, 1996; Blo keel & De Raedt, 1998; Blo keel et al., 1998): the lass entropy or varian e of the examples\novered (or not overed) by a query an be omputed from the probability distribution of the target variable; omputing this distribution involves simple ounts similar to the ones above.\nAfter transforming the grandfather/2 lauses into\ngrandfather((X,Y)),I) :- parent(X,Z), parent(Z,Y), male(X), I = 1. grandfather((X,Y)),I) :- parent(X,Z), parent(Z,Y), female(X), I = 2.\nthe result set an learly be omputed by olle ting for all grounding 's where K 2 E the answers to the query ?- grandfather(K ,I) . In Se tion 3 the queries will have a literal I = i at the end or another goal whi h by side-e e ts results in olle ting the result set.\nIn pra ti e, it is natural to ompute the result set using a double loop: one over examples and one over queries and one has the hoi e as to whi h is the outer loop. Both the \\examples in outer loop\" and the \\queries in outer loop\" have been used in data mining systems; in the ontext of de ision trees, see for instan e Quinlan (1993a) and Mehta et al. (1996). We shall see further that the redundan y removal approa h we propose uses the \\examples in outer loop\" strategy. In both approa hes however, given a query and a key instantiation, we are interested only in whether the query su eeds for that key instantiation. This implies that after a parti ular query has su eeded on an example, its exe ution an be stopped.\nIn other words: omputing the result set de ned above boils down to evaluating ea h query on ea h example, where we are only interested in the existen e of su ess for ea h su h evaluation. Computing more than one solution for one query on one example is unne essary."
    }, {
      "heading" : "3. Query Pa ks",
      "text" : "For simpli ity, we make abstra tion of the existen e of keys in the following examples. What is relevant here, is that for ea h query we are only interested in whether it su eeds or not, not in nding all answer substitutions.\nGiven the following set of queries\np(X), I = 1. p(X), q(X,a), I = 2. p(X), q(X,b), I = 3. p(X), q(X,Y), t(X), I = 4. p(X), q(X,Y), t(X), r(Y,1), I = 5.\nwe an hoose to evaluate them separately. Sin e we are only interested in one { the rst { su ess for ea h query, we would evaluate in Prolog the queries\non e((p(X), I = 1)). on e((p(X), q(X,a), I = 2)). on e((p(X), q(X,b), I = 3)). on e((p(X), q(X,Y), t(X), I = 4)). on e((p(X), q(X,Y), t(X), r(Y,1), I = 5)).\nThe wrapper on e/1 is a pruning primitive and prevents the unne essary sear h for more\nsolutions. Its de nition in Prolog is simply\non e(Goal) :- all(Goal), !.\nAn alternative way to evaluate the queries onsists in merging them into one (nested)\ndisjun tion as in:\np(X), (I=1 ; q(X,a), I=2 ; q(X,b), I=3 ; q(X,Y), t(X), (I=4 ; r(Y,1), I=5)).\nThe set of queries an now be evaluated as a whole: the su ess of one bran h in the\ndisjun tive query orresponds to the su ess of the orresponding individual query.\nCompared to the evaluation of the individual queries, the disjun tive query has both an\nadvantage and a disadvantage:\n+ all the queries have the same pre x p(X), whi h is evaluated on e in ea h individual\nquery, while in the disjun tive query, the goal p(X) is evaluated only on e; depending on the evaluation ost of p/1, this an lead to arbitrary performan e gains.\nthe usual Prolog pruning primitives are not powerful enough to prevent all the unne essary ba ktra king after a bran h in the disjun tive query has su eeded; this is explained further in Example 2.\nExample 2 In this example the literals I = i have been left out, be ause they do not\nontribute to the dis ussion:\np(X), q(X). p(X), r(X).\nEvaluating these queries separately means evaluating\non e((p(X), q(X))). on e((p(X), r(X))).\nor equivalently\np(X), q(X), !. p(X), r(X), !.\nThe orresponding disjun tive query is\np(X), (q(X) ; r(X)).\nWe an now try to pla e a pruning primitive in the disjun tive query: !/0 at the end of\nea h bran h results in\np(X), (q(X), ! ; r(X), !)\nThe s ope of the rst ut is learly too large: after the goal q(X) has su eeded, the ut will prevent entering the se ond bran h. It means that adding the ut in the disjun tive query leads to a wrong result.\nUsing on e/1 in the disjun tive query results in\np(X), (on e(q(X)) ; on e(r(X)))\nThis results in a orre t query. However, both bran hes are still exe uted for every\nbinding that the goal p(X) produ es, even if both bran hes have su eeded already.\nThe ombination of the advantage of the disjun tive query with the advantage of the individual query with pruning (on e or ut) results in the notion of the query pa k. Synta ti ally, a query pa k looks like a disjun tive query where the ; ontrol onstru t is repla ed by a new ontrol onstru t denoted by or. So the query pa k orresponding to the disjun tive query above is\np(X), (I=1 or q(X,a), I=2 or q(X,b), I=3 or q(X,Y), t(X), (I=4 or r(Y,1), I=5))\nThis query pa k an be represented as the tree in Figure 1. For a query pa k Q su h a tree has literals or onjun tions of literals in the nodes. Ea h path from the root to a leaf node represents a onjun tive query Q whi h is a member of Q, denoted Q 2 Q. The or\nThe intended pro edural behaviour of the or onstru t is that on e a bran h has su - eeded, it is e e tively pruned away from the pa k during the evaluation of the query pa k on the urrent example. This pruning must be re ursive, i.e., when all bran hes in a subtree of the query pa k have su eeded, the whole subtree must be pruned. Evaluation of the query pa k then terminates when all subtrees have been pruned or all of the remaining queries fail for the example.\nThe semanti s of the or onstru t and its eÆ ient implementation is the subje t of the rest of this se tion. It should however be lear already now that in the ase that all the answers of ea h query are needed, pruning annot be performed and the disjun tive query is already suÆ ient, i.e., query pa ks are useful when a single su ess per query suÆ es."
    }, {
      "heading" : "3.1 EÆ ient Exe ution of Query Pa ks",
      "text" : "In Se tion 3.1.2, a meta-interpreter is given that de nes the behaviour of query pa ks. In pra ti e this meta-interpreter is not useful, be ause in many ases the meta-interpreter itself\nauses more overhead than the use of query pa ks an ompensate for. Indeed, previously reported results (Demoen et al., 1999; Blo keel, 1998) indi ate that the overhead involved in a high-level Prolog implementation destroys the eÆ ien y gain obtained by redundan y redu tion. Moreover as dis ussed in Se tion 3.1.2, the meta-interpreter does not have the desired time- omplexity. This shows that the desired pro edural semanti s of or an be\nimplemented in Prolog itself, but not with the desired performan e be ause Prolog la ks the appropriate primitives.\nThe on lusion is that hanges are needed at the level of the Prolog engine itself. This requires an extension of the WAM (Warren Abstra t Ma hine) whi h is the underlying abstra t ma hine for most Prolog implementations. The extended WAM provides the or operator as dis ussed above: it permanently removes bran hes from the pa k that do not need to be investigated anymore. This extended WAM has be ome the basis of a new Prolog engine dedi ated to indu tive logi programming, alled ilProlog. This se tion ontinues with the introdu tion of some basi terminology for query pa ks and explains at a high level how query pa k exe ution works. Next our meta-interpreter for the query pa k exe ution is given and nally the hanges needed for the WAM are lari ed."
    }, {
      "heading" : "3.1.1 Prin iples of Query Pa ks (Exe ution)",
      "text" : "Before we dis uss query pa k exe ution in detail, note the following two points: (1) during the pa k exe ution, the pruning of a bran h must survive ba ktra king; (2) when exe uting a pa k we are not interested in any variable instantiations, just in whether a member of the pa k su eeds or not. In our previous des ription we were interested in the binding to the variable I. Sin e ea h bran h an bind I to only one value { the query number { we olle t these values in pra ti e by a side e e t denoted in Se tion 3.2 by report su ess.\nThe starting point for the query pa k exe ution me hanism is the usual Prolog exe ution of a query Q given a Prolog program P . By ba ktra king Prolog will generate all the solutions for Q by giving the possible instantiations su h that Q su eeds in P .\nA query pa k onsists of a onjun tion of literals and a set of alternatives, where ea h alternative is again a query pa k. Note that leaves are query pa ks with an empty set of alternatives. For ea h query pa k Q, onj(Q) denotes the onjun tion and hildren(Q) denotes the set of alternatives. A set of queries is then represented by a so- alled root query pa k. For every query pa k Q, there is a path of query pa ks starting from the root query pa k Q\nroot\nand ending at the query pa k itself, namely < Q\nroot\n, Q\n1\n, ..., Q\nn\n, Q >. The\nquery pa ks in this path are the prede essors of Q. Every query pa k has a set of dependent queries, dependent queries(Q). Let < Q\nroot\n, Q\ni\n1\n, ..., Q\ni\nn\n, Q > be the path to Q, then\ndependent queries(Q) = f onj(Q\nroot\n)^ onj(Q\ni\n1\n)^ : : :^ onj(Q\ni\nn\n)^ onj(Q)^ onj(Q\nj\n1\n)^\n: : : ^ onj(Q\nj\nm\n) ^ onj(Q\nl\n) j < Q;Q\nj\n1\n, ..., Q\nj\nm\n, Q\nl\n> is a path from Q to a leaf Q\nl\ng. Note\nthat dependent queries(Q\nroot\n) are a tually the members of the query pa k as des ribed\nearlier.\nExample 3 For the query pa k in Figure 1, Q\nroot\nis the root of the tree. onj(Q\nroot\n) is\np(X). The set hildren(Q\nroot\n) ontains the 4 query pa ks whi h orrespond to the trees\nrooted at the 4 sons of the root of the tree. Suppose that these query pa ks are named (from left to right) Q\n1\n, Q\n2\n, Q\n3\n, and Q\n4\n. Then onj(Q\n2\n) equals (q(X; a); I = 2), hildren(Q\n2\n)\nequals the empty set, onj(Q\n4\n) equals (q(X;Y ); t(X)), and dependent queries(Q\n4\n) equals\nf(p(X); q(X;Y ); t(X); I = 4), (p(X); q(X;Y ); t(X); r(Y; 1); I = 5)g.\nExe ution of a root query pa k Q\nroot\naims at nding out whi h queries of the set\ndependent queries(Q\nroot\n) su eed. If a query pa k is exe uted as if the ors were usual\ndisjun tions, ba ktra king o urs over queries that have already su eeded and too many\nsu esses are dete ted. To avoid this, it should be the ase that as soon as a query su eeds, the orresponding part of the query pa k should no longer be onsidered during ba ktra king. Our approa h realises this by reporting su ess of queries (and of query pa ks) to prede essors in the query pa k. A (non-root) query pa k Q an be safely removed if all the queries that depend on it (i.e., all the queries in dependent queries(Q)) su eeded on e. For a leaf Q (empty set of hildren), su ess of onj(Q) is suÆ ient to remove it. For a non-leaf Q, we wait until all the dependent queries report su ess or equivalently until all the query pa ks in hildren(Q) report su ess.\nAt the start of the evaluation of a root query pa k, the set of hildren for every query pa k in it ontains all the alternatives in the given query pa k. During the exe ution, query pa ks an be removed from hildren sets and thus the values of the hildren(Q) hange a ordingly. When due to ba ktra king a query pa k is exe uted again, it might be the ase that fewer alternatives have to be onsidered.\nThe exe ution of a query pa k Q is de ned by the algorithm exe ute qp(Q; ) (Figure\n2) whi h imposes additional ontrol on the usual Prolog exe ution.\nThe usual Prolog exe ution and ba ktra king behaviour is modelled by the while loop (line 1) whi h generates all possible solutions for the onjun tion in the query pa k. If no more solutions are found, fail is returned and ba ktra king will o ur at the level of the\nalling query pa k.\nThe additional ontrol manages the hildren(Q). For ea h solution , the ne essary hildren of Q will be exe uted. It is important to noti e that the initial set of hildren of a query pa k is hanged destru tively during the exe ution of this algorithm. Firstly, when a leaf is rea hed, su ess is returned (line 8) and the orresponding hild is removed from the query pa k (line 6). Se ondly, when a query pa k that initially had several hildren, nally ends up with an empty set of hildren (line 6), also this query pa k is removed (line 8). The fa t that hildren are destru tively removed, implies that when due to ba ktra king the same query pa k is exe uted again for a di erent , not all of the alternatives that were initially there, have to be exe uted any more. Moreover, by returning su ess the\nba ktra king over the urrent query pa k onjun tion onj(Q) is stopped: all bran hes have reported su ess."
    }, {
      "heading" : "3.1.2 A Meta-interpreter for Query Pa ks",
      "text" : "The rst implementation of the query pa k exe ution algorithm is the meta-interpreter meta exe ute qp(Q). The meta-interpreter uses the following labelling in its representation of a query pa k:\nQuery pa k number All the non-leaf query pa ks in the tree are numbered, depth\nrst, from left to right (qp(i)).\nQuery number Ea h leaf is numbered, from left to right. If the original queries were numbered sequentially, then the numbers at the leaves orrespond with these (q(i)).\nChild number For ea h non-leaf query pa k with N hildren, all hildren are numbered from 1 up to N sequentially ( h(i)).\nConsider the query pa k a, (b, ( or d or e) or f or g, (h or i or j)). Note that the atoms in the example ould in general be arbitrary onjun tions of non-ground terms. Its labelling is shown in Figure 3.\nA labelled query pa k Q is then represented as a Prolog term as follows (with Q\nf\nthe\nfather of Q):\nA leaf Q is represented by the term ( ; leaf(qpnbf; hnb; qnb)) with the onj(Q), qpnbf the query pa k number of Q\nf\n, hnb the hild number of Q w.r.t. Q\nf\n, and qnb\nthe query number of Q.\nA non-leaf Q is represented by the term ( ; or( s; qpnbf; qpnb; hnb; tot s) with the\nonj(Q), s the list hildren(Q), qpnbf the query pa k number of Q\nf\n, qpnb the query\npa k number of Q, hnb the hild number of Q w.r.t. Q\nf\n, and tot s the total number\nof hildren(Q)). The query pa k number of the father of the root query pa k is assumed to be zero.\nThe example of Figure 3 has the following representation (as a Prolog term):\n(a, or([(b,or([( ,leaf(2,1,1)),(d,leaf(2,2,2)),(e,leaf(2,3,3))℄,1,2,1,3)),\n(f,leaf(1,2,4)), (g,or([(h,leaf(3,1,5)),(i,leaf(3,2,6)),(j,leaf(3,3,7))℄,1,3,3,3))℄,\n0,1,1,3))\nDuring the exe ution of the meta-interpreter, solved/2 fa ts are asserted. Ea h fa t solved(qpnb, hnb) denotes that the hild with number hnb from query pa k with number qpnb has su eeded. Su h fa ts are asserted when rea hing a leaf and also when all hildren of a query pa k have su eeded. The meta-interpreter only exe utes hildren for whi h no solved/2 fa t has been asserted.\nNote that the time- omplexity of this meta-interpreter is not yet as desired. Exe ution of a query pa k will always be dependent on the number of original hildren, instead of the number of remaining (as yet unsu essful) hildren.\nrun QueryPa k(Q) :-\nprepro ess(Q, Qlabeled, 0, 1, 1, 1, , ),\n% The ode for prepro essing is given in Appendix A\nretra tall(solved( , )), meta exe ute qp(Qlabeled), solved(0, ), !.\nmeta exe ute qp((A,B)) :- !,\nall(A),\nmeta exe ute qp(B).\nmeta exe ute qp(or(Cs, QpNbF, QpNb, ChildNb, TotCs)) :-\n!, % 'or' orresponds to a non-leaf query pa k handle hildren(Cs, QpNb, 1), all solved(QpNb, 0, TotCs), assert(solved(QpNbF,ChildNb)).\nmeta exe ute qp(leaf(QpNbF, ChildNb , QueryNb)) :-\n!, % 'leaf' orresponds to the end of a query write(su eed(QueryNb)), nl, assert(solved(QpNbF,ChildNb)).\nhandle hildren([℄, , ). handle hildren([C| ℄, QpNb, ChildNb) :-\nnot(solved(QpNb,ChildNb)), on e(meta exe ute qp(C)), fail.\nhandle hildren([ |Cs℄, QpNb, ChildNb) :-\nChildNb1 is ChildNb + 1, handle hildren(Cs, QpNb, ChildNb1).\nall solved(QpNb, ChildNb, TotCs) :-\n(ChildNb = TotCs -> true ; ChildNb1 is ChildNb + 1,\nsolved(QpNb, ChildNb1), all solved(QpNb, ChildNb1, TotCs)\n)."
    }, {
      "heading" : "3.1.3 WAM Extensions",
      "text" : "To fully exploit the potential of a query pa k (shared omputation and avoidan e of unne-\nessary ba ktra king) hanges have to be made at the level of the Prolog engine itself. The explanation assumes a WAM-based Prolog engine (A t-Ka i, 1991) but a short explanation of the exe ution of disjun tion in Prolog is given rst, so that it be omes more easy to see what was newly introdu ed in the WAM.\nAssume that the body of a lause to be exe uted is a, (b, ; d ; e). Assume also that all predi ates have several lauses. At the moment that exe ution has rea hed the rst\nlause of , the hoi e point sta k looks like Figure 4(a): there are hoi e points for the a tivation of a, the disjun tion itself, b and . The hoi e points are linked together so that ba ktra king an easily pop the top most one. Ea h hoi e point ontains a pointer to the next alternative to be tried: only for the disjun tion hoi e point, this alternative pointer is shown. It points to the beginning of the se ond bran h of the disjun tion. After all alternatives for b and have been exhausted, this se ond bran h is entered and d be omes a tive: this is the situation shown in Figure 4(b). At that point, the alternative of the disjun tion hoi e point refers to the last alternative bran h of the disjun tion. Finally, on e e is entered, the disjun tion hoi e point is already popped.\nWhen the goal a produ es a new solution, all bran hes of the disjun tion must be tried again. It is exa tly this we want to avoid for query pa ks: a bran h that has su eeded on e, should never be re-entered. We therefore adapt the disjun tion hoi e point to be ome an or- hoi e point whi h is set up to point into a data stru ture that ontains referen es to ea h alternative in the or disjun tion. This data stru ture is named the pa k table. Figure 5(a) shows the state of the exe ution when it has rea hed : it is similar to Figure 4(a). The or- hoi e point now ontains the information that the rst bran h is being exe uted. As the exe ution pro eeds, there are two possibilities: either this rst bran h su eeds or it fails. We des ribe the failing situation for the rst bran h and explain what happens on su ess of\nthe se ond bran h. If the rst bran h has no solution, ba ktra king updates the alternative in the or- hoi e point, to point to the next bran h in the pa k table. The situation after the se ond bran h is entered is shown in 5(b) and is again similar to 4(b). Suppose now that the bran h with the goal d su eeds: the entry in the pa k table with or-alternatives is now adapted by erasing the se ond alternative bran h, ba ktra king o urs, and the next alternative bran h of the or- hoi e point is taken. This is shown in 5( ).\nWhen a produ es a new solution and the or-disjun tion is entered again, the pa k table does no longer ontain the se ond alternative bran h and it is never re-entered. The pa k table is a tually arranged in su h a way that entries are really removed instead of just erased so that they ause no overhead later.\nTwo more issues must be explained: rst, the pa k table with alternatives must be onstru ted at runtime every time the query pa k is entered for evaluation. This is done by emitting the ne essary instru tions in the beginning of the ode for the query pa k. As an example, we show the ode for the query pa k a, (b, or d or e) in Figure 6.\nFinally, in the example it is lear that at the moment that all alternatives of an ordisjun tion have su eeded, a an stop produ ing more solutions. So the omputation an be stopped. In general - with nested query pa ks - it means that one pa k table entry of the next higher or-node an be erased and this in a re ursive way. The re ursive removal of entries from the pa k tables, is done by the instru tion query pa k prune.\nWe have implemented this s hema in ilProlog. Se tion 5 presents some measurements\nin ilProlog."
    }, {
      "heading" : "3.2 Using Query Pa ks",
      "text" : "Figure 7 shows an algorithm that makes use of the pa k exe ution me hanism to ompute the result set R as de ned in our problem statement. The set S of queries is here typi ally\nthe set of all re nements of a given query, i.e., it does not orrespond to the whole hypothesis spa e. From a query pa k Q ontaining all queries in S, a derived pa k Q 0 is onstru ted by adding a report su ess/2 literal to ea h leaf of the pa k; the (pro edural) task of report su ess(K,i) is simply to add (K; i) to the result set R. Obviously a spe i ILP system not interested in the result set itself ould provide its own report su ess/2 predi ate and thus avoid the overhead of expli itly building the result set. 1\nNote that the algorithm in Figure 7 follows the strategy of running all queries for ea h single example before moving on to the next example: this ould be alled the \\examples in outer loop\" strategy, as opposed to the \\queries in outer loop\" strategy used by most ILP\n1. In our urrent implementation the result set is implemented as a bit-matrix indexed on queries and\nexamples. This implementation is pra ti ally feasible (on typi al omputers at the time of writing) even when the number of queries in the pa k multiplied by the number of examples is up to a billion, a bound whi h holds for most urrent ILP appli ations.\nsystems. The \\examples in outer loop\" strategy has important advantages when pro essing large data sets, mainly due to the ability to pro ess them eÆ iently without having all data in main memory at the same time (Mehta et al., 1996; Blo keel et al., 1999)."
    }, {
      "heading" : "3.3 Computational Complexity",
      "text" : "We estimate the speedup fa tor that an be a hieved using query pa k exe ution in two steps: rst we onsider one-level pa ks, then we extend the results towards deeper pa ks.\nLower and upper bounds on the speedup fa tor that an be a hieved by exe uting a one-level pa k instead of separate queries an be obtained as follows. For a pa k ontaining n queries q\ni\n= (a; b\ni\n), let T\ni\nbe the time needed to ompute the rst answer substitution of\nq\ni\nif there are any, or to obtain failure otherwise. Let t\ni\nbe the part of T\ni\nspent within a\nand t\n0 i the part of T i spent in b i . Then T s =\nP\ni\n(t\ni\n+ t\n0 i ) and T p = max(t i ) +\nP\ni\nt\n0 i with T s\nrepresenting the total time needed for exe uting all queries separately and T\np\nthe total time\nneeded for exe uting the pa k. Introdu ing =\nP\ni\nt\ni\n=\nP\ni\nt\n0 i , whi h roughly represents the\nratio of the omputational omplexity in the shared part over that in the non-shared part, we have\nT\ns\nT\np\n=\nP\ni\nt\ni\n+\nP\ni\nt\n0 i\nmax\ni\nt\ni\n+\nP\ni\nt\n0 i\n=\n+ 1\nmax\ni\nt\ni\nP\ni\nt\n0 i\n+ 1\n(1)\nNow de ning K as the ratio of the maximal t\ni\nover the average t\ni\n, i.e.\nK =\nmax\ni\nt\ni\nP\ni\nt\ni\n=n\nwe an rewrite Equation (1) as\nT\ns\nT\np\n=\n+ 1\nK n + 1\n(2)\nSin e\nP\ni\nt\ni\nn\nmax t\ni\nP\ni\nt\ni\nwe know 1 K n, whi h leads to the following bounds:\n1\nT\ns\nT\np\n+ 1\nn\n+ 1\n< min( + 1; n) (3)\nThus the speedup fa tor is bounded from above by the bran hing fa tor n and by the ratio of omputational omplexity in the shared part over the omputational omplexity of the non-shared part; and a maximal speedup an be attained when max t\ni\n'\nP\nt\ni\n=n (or,\nK ' 1), in other words when the t\ni\nfor all queries are approximately equal.\nFor multi-level pa ks, we an estimate the eÆ ien y gain as follows. Given a query q\ni\n,\nlet T\ni\nbe de ned as above (the total time for nding 1 answer to q\ni\nor obtaining failure).\nInstead of t\ni\nand t\n0 i , we now de ne t i;l as the time spent on level l of the pa k when solving q i ;\nounting the root as level 0 and denoting the depth of the pa k with d we have T\ni\n=\nP\nd l=0 t i;l .\nFurther de ne T\ni;l\nas the time spent on level l or deeper: T\ni;l\n=\nP\nd j=l t i;j with d the depth\nof the pa k. (Thus T\ni\n= T\ni;0\n.). We will assume a onstant bran hing fa tor b in the pa k.\nFinally, we de ne t\nl\n=\nP\ni\nt\ni;l\n=n with n = b\nd\n. For simpli ity, in the formulae we impli itly\nassume that i always ranges from 1 to n with n the number of queries, unless expli itly\nspe i ed otherwise. We then have\nT\np\n= max\ni\nt\ni;0\n+\nX\ni\nT\ni;1\n= max\ni\nt\ni;0\n+\nb\nX\nj=1\n(max\ni2G\nj\nt\ni;1\n+\nX\ni2G\nj\nT\ni;2\n) (4)\nwhere j = 1 : : : b is the index of a hild of the root and G\nj\nis the set of indexes of the\nqueries belonging to that hild. Now de ne K\n0\n= max\ni\nt\ni;0\n= t\n0\nand de ne K\n1\nas the smallest\nnumber su h that max\ni2G\nj\nt\ni;1\nK\n1\nt\nj;1\nwith t\nj;1\n=\nP\ni2G\nj\nt\ni;1\n=b. Note 1 K\n0\n;K\n1\nb. It\nthen follows that\nb\nX\nj=1\nmax i2G\nj\nt\ni;1\nK\n1\nb\nX\nj=1\nt\nj;1\n= K\n1\nb t\n1\n(5)\nwhi h allows us to rewrite Equation (4) into\nT\np\nK\n0\nt\n0\n+K\n1\nb t\n1\n+\nX\ni\nT\ni;2\n(6)\nwhere the equality holds if max\ni2G\nj\nt\ni;1\nis equal in all G\nj\n. The reasoning an be ontinued\nup till the lowest level of the pa k, yielding\nT\np\nK\n0\nt\n0\n+ bK\n1\nt\n1\n+ b\n2\nK\n2\nt\n2\n+ + b\nd 1\nK\nd 1\nt\nd 1\n+\nX\ni\nt\ni;d\n(7)\nand nally\nT\np\nK\n0\nt\n0\n+ bK\n1\nt\n1\n+ b\n2\nK\n2\nt\n2\n+ + b\nd 1\nK\nd 1\nt\nd 1\n+ b\nd\nt\nd\n(8)\nwith all K\nl\nbetween 1 and b. We will further simplify the omparison with T\ns\nby assuming\n8l : K\nl\n= 1; the K\nl\nan then be dropped and the inequality be omes an equality (be ause\nall maxima must be equal):\nT\np\n= t\n0\n+ b t\n1\n+ b\n2\nt\n2\n+ + b\nd 1\nt\nd 1\n+ b\nd\nt\nd\n(9)\nNote that for T\ns\nwe have\nT\ns\n= b\nd\nt\n0\n+ b\nd\nt\n1\n+ b\nd\nt\n2\n+ + b\nd\nt\nd 1\n+ b\nd\nt\nd\n(10)\nIt is lear, then, that the speedup will be governed by how the b\nd\nt\nk\nterms ompare to the\nb\nk\nt\nk\nterms. (In the worst ase, where K\nk\n= b, the latter be ome b\nk+1\nt\nk\n.) We therefore\nintrodu e R\nl;m\nas follows:\nR\nl;m\n=\nP\nm k=l b m t k\nP\nm k=l b k t k\n(11)\nTheR oeÆ ients are always between 1 (if t\nm\ndominates) and b\nm l\n(if t\nl\nstrongly dominates);\nfor all t\nl\nequal, R\nl;m\nis approximately m l.\nFurther, similar to in our previous analysis, de ne\nl\n=\nP\nl k=0 b k t k\nP\nd k=l+1 b k t k\n(12)\nSome algebra then gives\nT\ns\nT\np\n=\nb\nd l\nl\nR\n0;l\n+R\nl+1;d\nl\n+ 1\n(13)\nwhi h needs to hold for all l. We an interpret this as follows: for a ertain level l,\nl\nroughly\nre e ts the speedup gained by the fa t that the part up till level l needs to be exe uted only on e; the R fa tors re e t the speedup obtained within these parts be ause of the pa k me hanism.\nThis inequality holds for all l, hen e we will nd the best lower bound for the spee-\ndup fa tor by maximizing the right hand side. Note that\nl\nin reases and b\nd l\nde reases\nmonotoni ally with l. It is lear that if at some point\nl\nbe omes mu h larger than 1, a\nspeedup fa tor of roughly b\nd l\nis obtained. On the other hand, if\nl\nis smaller than 1, then\nthe behaviour of b\nd l\nl\nis ru ial. Now,\nb\nd l\nl\n=\nt\nl\n+\n1 b t l 1 + + 1\nb\nl\nt\n0\nt\nd\n+\n1 b t d 1 + + 1\nb\nd l 1\nt\nl+1\n:\nOur on lusion is similar to that for the one-level pa k. If for some l,\nl\n>> 1, i.e., if in\nthe upper part of the pa k (up till level l) omputations take pla e that are so expensive that they dominate all omputations below level l (even taking into a ount that the latter are performed b d l times more often), then a speedup of b d l an be expe ted. If\nl\n<< 1,\nwhi h will usually be the ase for all l ex ept those near d, the speedup an roughly be estimated as t\nl\n= t\nd\n. The maximum of all these fa tors will determine the a tual speedup."
    }, {
      "heading" : "4. Adapting ILP Algorithms to Use Query Pa ks",
      "text" : "In this se tion we dis uss how the above exe ution method an be in luded in ILP algorithms, and illustrate this in more detail for two existing ILP algorithms. Experimental results on erning a tual eÆ ien y improvements this yields are presented in the next se - tion."
    }, {
      "heading" : "4.1 Re nement of a Single Rule",
      "text" : "Many systems for indu tive logi programming use an algorithm that onsists of repeatedly re ning lauses. Any of these systems ould in prin iple be rewritten to make use of a query pa k evaluation me hanism and thus a hieve a signi ant eÆ ien y gain. We rst show this for a on rete algorithm for de ision tree indu tion, then dis uss the more general ase."
    }, {
      "heading" : "4.1.1 Indu tion of De ision Trees",
      "text" : "The rst algorithm we dis uss is Tilde (Blo keel & De Raedt, 1998), an algorithm that builds rst-order de ision trees. In a rst-order de ision tree, nodes ontain literals that together with the onjun tion of the literals in the nodes above this node (i.e., in a path from the root to this node) form the query that is to be run for an example to de ide whi h subtree it should be sorted into. When building the tree, the literal (or onjun tion of literals) to be put in one node is hosen as follows: given the query orresponding to a path from the root to this node, generate all re nements of this query (a re nement of a query\nis formed by adding one or more literals to the query); evaluate these re nements on the relevant subset of the data, 2 omputing, e.g., the information gain (Quinlan, 1993a) yielded by the re nement; hoose the best re nement; and put the literals that were added to the original lause to form this re nement in the node.\nAt this point it is lear that a lot of omputational redundan y exists if ea h re nement is evaluated separately. Indeed all re nements ontain exa tly the same literals ex ept those added during this single re nement step. Organising all re nements into one query pa k, we obtain a query pa k that essentially has only one level (the root immediately bran hes into leaves). When Tilde's lookahead fa ility is used (Blo keel & De Raedt, 1997), re nements form a latti e and the query pa k may ontain multiple (though usually few) levels.\nNote that the root of these pa ks may onsist of a onjun tion of many literals, giving the pa k a broom-like form. The more literals in the root of the pa k, the greater the bene t of query pa k exe ution is expe ted to be.\nExample 4 Assume the node urrently being re ned has the following query asso iated with it: ?- ir le(A,C),leftof(A,C,D),above(A,D,E), i.e., the node overs all examples A where there is a ir le to the left of some other obje t whi h is itself above yet another obje t.\nThe query pa k generated for this re nement ould for instan e be\nin(A,D,L) circle(A,C), leftof(A,C,D), above(A,D,E), in(A,C,M)\nabove(A,E,N) above(A,D,O) above(A,C,P) leftof(A,E,Q) leftof(A,D,R) leftof(A,C,S)\nin(A,E,K) large(A,J) small(A,I) circle(A,H) triangle(A,F)\nWhen evaluating this pa k, all ba ktra king through the root of the pa k (the \\sti k\" of the broom) will happen only on e, instead of on e for ea h re nement. In other words: when evaluating queries one by one, for ea h query the Prolog engine needs to sear h on e again for all obje ts C, D and E ful lling the onstraint ir le(A,C), leftof(A,C,D), above(A,D,E); when exe uting a pa k this sear h is done only on e."
    }, {
      "heading" : "4.1.2 Other Algorithms Based on Rule Refinement",
      "text" : "As mentioned, any ILP algorithm that onsists of repeatedly re ning lauses ould in prin-\niple be rewritten to make use of a query pa k evaluation me hanism and thus a hieve a\nsigni ant eÆ ien y gain. Consider, e.g., a rule indu tion system performing an A sear h through a re nement latti e, su h as Progol (Muggleton, 1995). Sin e A imposes a ertain order in whi h lauses will be onsidered for re nement, it is hard to reorganise the\nomputation at this level. However, when taking one node in the list of open nodes and produ ing all its re nements, the evaluation of the re nements involves exe uting all of them; this an be repla ed by a pa k exe ution, in whi h ase a positive eÆ ien y gain is guaranteed. In prin iple one ould also perform several levels of re nement at this stage,\n2. I.e., that subset of the original data set for whi h the parent query su eeded; or, in the de ision tree\nontext: the examples sorted into the node that is being re ned.\nadding all of the re nements to A 's queue; part of the eÆ ien y of A is then lost, but the pa k exe ution me hanism is exploited to a larger extent. Whi h of these two e e ts is dominant will depend on the appli ation: if most of the rst-level re nements would be further re ned anyway at some point during the sear h, learly there will be a gain in exe uting a two-level pa k; otherwise there may be a loss of eÆ ien y. For instan e, if exe uting a two-level pa k takes x times as mu h time as a one-level pa k, it will bring an eÆ ien y gain only if at least x of the rst level re nements would afterwards be re ned themselves."
    }, {
      "heading" : "4.2 Level-wise Frequent Pattern Dis overy",
      "text" : "An alternative family of data mining algorithms s ans the re nement latti e in a breadth-\nrst manner for queries whose frequen y ex eeds some user-de ned threshold. The bestknown instan e of these level-wise algorithms is the Apriori method for nding frequent item-sets (Agrawal et al., 1996). Warmr (Dehaspe & Toivonen, 1999) is an ILP variant of attribute-value based Apriori.\nQuery pa ks inWarmr orrespond to hash-trees of item-sets in Apriori: both are used to store a subgraph of the total re nement latti e down to level n. The paths from the root down to level n 1 in that subgraph orrespond to frequent patterns. The paths from root to the leaves at depth n orrespond to andidates whose frequen y has to be omputed. Like hash-trees in Apriori, query pa ks in Warmr exploit massive similarity between\nandidates to make their evaluation more eÆ ient. Essentially theWarmr algorithm starts with an empty query pa k and iterates between pa k evaluation and pa k extension (see Figure 8). The latter is a hieved by adding all potentially frequent re nements 3 of all leaves in the pa k, i.e., adding another level of the total re nement latti e."
    }, {
      "heading" : "5. Experiments",
      "text" : "The goal of this experimental evaluation is to empiri ally investigate the a tual speedups that an be obtained by re-implementing ILP systems so that they use the pa k exe ution me hanism. At this moment su h re-implementations exist for the Tilde and Warmr systems, hen e we have used these for our experiments. These re-implementations are available within the ACE data mining tool, available for a ademi use upon request. 4 We attempt to quantify (a) the speedup of pa ks w.r.t. to separate exe ution of queries (thus validating our omplexity analysis), and (b) the total speedup that this an yield for an ILP system.\nThe data sets that we have used for our experiments are the following:\nThe Mutagenesis data set : an ILP ben hmark data set, introdu ed to the ILP ommunity by Srinivasan et al. (1995), that onsists of stru tural des riptions of 230 mole ules that are to be lassi ed as mutageni or not. Next to the standard Mutagenesis data set, we also onsider versions of it where ea h example o urs n times;\n3. Re nements found to be spe ialisations of infrequent queries annot be frequent themselves, and are\npruned onsequently.\n4. See http://www. s.kuleuven.a .be/~dtai/ACE/.\ncircle(A,B)\nleftof(A,B,C)\ncircle(A,B)\nabove(A,B,C)\nleftof(A,B,C)\ntriangle(A,B)\ntriangle(A,B)\nabove(A,B,C) leftof(A,B,C) above(A,B,C)leftof(A,B,C)\ntriangle(A,B)circle(A,B)\ntriangle(A,C)circle(A,C) circle(A,C)triangle(A,C) circle(A,C)triangle(A,C)\nleftof(A,B,C)\nabove(A,B,C)\ncircle(A,B) triangle(A,B)\nleftof(A,B,C)\nEXPAND\nEXPAND\nEVALUATE\nthis allows us to easily generate data sets of larger size where the average example and query omplexity are onstant and equal to those of the original data set.\nBongard data sets : introdu ed in ILP by De Raedt and Van Laer (1995), the so- alled \\Bongard problems\" are a simpli ed version of problems used by Bongard (1970) for resear h on pattern re ognition. A number of drawings are shown ontaining ea h a number of elementary geometri al gures; the drawings have to be lassi ed a ording to relations that hold on the gures in them. We use a Bongard problem generator to reate data sets of varying size.\nThe experiments were run on SUN workstations: a Spar Ultra-60 at 360 MHz for Tilde, a Spar Ultra-10 at 333 Mhz for Warmr. Tilde and Warmr were run with their default settings, ex ept where mentioned di erently."
    }, {
      "heading" : "5.1 Tilde",
      "text" : "We onsider three di erent ways in whi h Tilde an be run in its ilProlog implementation:\n1. No pa ks: the normal implementation of Tilde as des ribed by Blo keel and De Raedt\n(1998), where queries are generated one by one and ea h is evaluated on all relevant examples. Sin e queries are represented as terms, ea h evaluation of a query involves a meta- all in Prolog.\n2. Disjoint exe ution of pa ks: a query pa k is exe uted in whi h all queries in the pa k\nare put beside one another; i.e., ommon parts are not shared by the queries. The\nomputational redundan y in exe uting su h a pa k is the same as that in exe uting all queries one after another; the main di eren e is that in this ase all queries are\nompiled.\n3. Pa ked exe ution of pa ks: a ompiled query pa k is exe uted where queries share as\nmu h as possible.\nThe most interesting information is obtained by omparing (a) the a tual query evaluation time in settings 2 and 3: this gives a view of the eÆ ien y gain obtained by the removal of redundant omputation itself (we will abbreviate this as exe in the tables); and (b) the total exe ution time in settings 1 and 3: this provides an indi ation of how mu h is gained by implementing pa ks in an ILP system, taking all other e e ts into a -\nount (re-implementation of the omputation of heuristi s via a bit matrix, use of ompiled queries instead of meta- alls, et .), or in other words: what the net e e t of the whole re-implementation is (indi ated as net in the tables).\nIn a rst experiment we used Bongard problems, varying (1) the size of the data sets; (2) the omplexity of the target hypothesis; and (3) Tilde's lookahead parameter. The\nomplexity of the target hypothesis an be small, medium, or none. In the latter ase the examples are random, whi h auses Tilde to grow ever larger trees in an attempt to nd a good hypothesis; the size of the nal tree then typi ally depends on the size of the data set. The lookahead parameter is used to ontrol the number of levels the pa k ontains; with lookahead n, pa ks of depth n+ 1 are generated.\nTable 1 gives an overview of results for the Bongard problems. The total indu tion time is reported, as well as (for pa k-based exe ution me hanisms) the time needed for pa k ompilation and pa k exe ution. Note that the total time in ludes not only pa k\nompilation and exe ution, but also all other omputations not dire tly related to pa ks (e.g., the omputation of heuristi s from the bitmatrix). The results an be interpreted as follows.\nFirst of all, the table shows that signi ant speedups an be obtained by using the pa k me hanism; net speedups of over a fa tor 5.5 are obtained, while the exe ution itself is up to 75 times faster ompared to disjoint exe ution.\nA further observation is that for more omplex target hypotheses greater speedups are obtained. This an be explained by the broom-like form of the pa ks in Tilde. Complex target hypotheses orrespond to deep trees, and re nement of a node at a lower level of su h a tree yields a pa k with a long lause before the bran hing, whi h in a ordan e with our previous analysis should yield a speedup loser to the bran hing fa tor b in the ase of lookahead 0 (and more generally, loser to b l+1 for lookahead l, although the latter is mu h harder to a hieve). Note that the maximum bran hing fa tor o urring in ea h pa k is in luded in the table in olumn bf .\nFinally, deeper pa ks also yield higher speedups, and this e e t is larger for more omplex theories. This is understandable onsidering the following. Let us all the lause that is being re ned . With lookahead l, onjun tions of l+ 1 literals are added to the lause. In some ases the rst of these l+1 literals may fail immediately, whi h auses this bran h of the pa k to have almost no exe ution time, while utting away b l queries. Remember that\nLA bf original disjoint pa ked speedup\ntotal omp exe total omp exe net exe\nSimple target hypothesis\n1007 examples\n0 16 0.74 0.62 0.14 0.13 0.49 0.05 0.07 1.51 1.86 1 24 2.44 1.64 0.35 0.45 1.09 0.14 0.11 2.24 4.09 2 18 7.49 4.07 0.8 1.57 2.15 0.27 0.16 3.48 9.81 3 21 29.9 16.52 3.65 7.26 7.18 1.26 0.28 4.17 25.9\n2473 examples\n0 16 1.82 1.43 0.17 0.34 1.13 0.07 0.16 1.61 2.13 1 24 5.72 3.34 0.34 1.17 2.24 0.11 0.3 2.55 3.9 2 18 17.2 8.45 0.78 3.95 4.4 0.27 0.39 3.92 10.1 3 21 69.8 33.0 3.57 17.5 13.7 1.13 0.69 5.11 25.4\n4981 examples\n0 19 3.69 2.72 0.29 0.67 2.16 0.12 0.32 1.71 2.09 1 24 11.4 6.22 0.35 2.41 4.17 0.13 0.63 2.74 3.83 2 18 34.7 16.0 0.74 8.14 8.24 0.25 0.88 4.21 9.25 3 21 142 62.4 3.61 36.5 24.9 1.09 1.45 5.69 25.1\nMedium omplexity target hypothesis\n1031 examples\n0 19 1.01 0.93 0.29 0.18 0.66 0.11 0.07 1.53 2.57 1 21 3.26 2.8 0.98 0.56 1.66 0.35 0.14 1.96 4 2 15 6.36 3.47 0.68 1.22 1.95 0.25 0.15 3.26 8.13 3 18 27.2 14.6 3.75 5.75 6.71 1.20 0.27 4.06 21.3\n2520 examples\n0 22 3.16 2.82 0.89 0.62 1.91 0.3 0.24 1.65 2.58 1 24 8.38 5.88 1.5 1.86 3.3 0.44 0.41 2.54 4.54 2 27 38.5 29.8 13.14 9.52 10.3 2.44 0.6 3.73 15.9 3 18 124 58.02 10.3 28.6 23.9 3.00 1.11 5.21 25.7\n5058 examples\n0 25 6.35 5.41 1.47 1.3 3.73 0.56 0.53 1.70 2.45 1 24 18.14 12.98 3.2 4.15 7.5 0.93 0.91 2.42 4.56 2 27 119 93.2 38.1 31.0 35.3 9.09 1.7 3.36 18.2 3 27 384 275 108 89.1 106 25.9 2.83 3.62 31.5\nNo target hypothesis\n1194 examples\n0 28 4.74 6.65 3.34 0.94 3.93 0.98 0.20 1.21 4.70 1 24 16.32 21.29 10.97 2.24 11.65 3.41 0.31 1.40 7.23 2 24 87.5 130 82.3 13.8 54.7 20.4 0.57 1.60 24.1 3 30 373 519 316 61.1 220 74.9 1.34 1.70 45.6\n2986 examples\n0 31 12.7 16.5 7.04 2.68 9.8 2.16 0.56 1.30 4.79 1 36 65.1 83.7 42.9 10.7 42.47 11.2 1.14 1.53 9.39 2 33 430 606 396 84 211.3 82.58 2.57 2.03 32.6 3 33 1934 2592 1610 375 946 332 6.58 2.04 57.0\n6013 examples\na ording to our analysis, the speedup an in the limit approximate b\nl\nif the omplexity of\nlause dominates over the omplexity of the rest of the pa k; su h \\early failing bran hes\"\nin the pa k ause the a tual situation to approximate loser this ideal ase.\nWe have also run experiments on the Mutagenesis data set (Table 2), both in a regression and a lassi ation setting. Here, query pa ks are mu h larger than for the Bongard data set (there is a higher bran hing fa tor); with a lookahead of 2 the largest pa ks had over 20000 queries. For these large pa ks a signi ant amount of time is spent ompiling the pa k, but even then lear net speedups are obtained. 5 A omparison of exe ution times turned out infeasible be ause in the disjoint exe ution setting the pa k stru tures onsumed too mu h memory."
    }, {
      "heading" : "5.2 Warmr",
      "text" : ""
    }, {
      "heading" : "5.2.1 Used Implementations",
      "text" : "For Warmr we onsider the following implementations:\n1. No pa ks: the normal implementation of Warmr, where queries are generated, and\nfor all examples the queries are evaluated one by one.\n2. With pa ks: An implementation where rst all queries for one level are generated and\nput into a pa k, and then this pa k is evaluated on ea h example."
    }, {
      "heading" : "5.2.2 Datasets",
      "text" : "Mutagenesis We used the Mutagenesis dataset of 230 mole ules, where ea h example is repeated 10 times to make more a urate timings possible and to have a better idea of the e e t on larger datasets. We used three di erent language biases. 'small' is a language\n5. In one ase, with a relatively small pa k, the system be ame slower. The timings indi ate that this is\nnot due to the ompilation time, but to other hanges in the implementation whi h for this relatively simple problem were not ompensated by the faster exe ution of the pa ks.\nMutagenesis\nbias that was hosen so as to generate a limited number of re nements (i.e., a relatively small bran hing fa tor in the sear h latti e); this allows us to generate query pa ks that are relatively deep but narrow. 'medium' and 'large' use broader but more shallow pa ks. Table 3 summarises the number of queries and the number of frequent queries found for ea h level in the di erent languages.\nBongard We use Bongard-6013 for experiments with Warmr as this system does not\nonstru t a theory and hen e the existen e of a simple theory is not expe ted to make mu h\ndi eren e."
    }, {
      "heading" : "5.2.3 Results",
      "text" : "In Tables 4, 5 and 6 the exe ution times ofWarmr on Mutagenesis are given, with maximal sear h depth varying from 3 for the large language to 9 levels for the small language. Here, 'total' is the total exe ution time and 'exe ' is the time needed to test the queries against the examples. In Table 7 the exe ution times of Warmr on Bongard are given."
    }, {
      "heading" : "5.2.4 Dis ussion",
      "text" : "The exe ution time of Warmr has a large omponent that is not used to evaluate queries. This is aused by the fa t that Warmr needs to do a lot of administrative work. In parti ular, theta-subsumption tests should be done on the queries to he k wether a query is equivalent to another andidate, or if a query is a spe ialisation of an infrequent one. In the propositional ase (the Apriori algorithm), these tests are very simple, but in the\nrst order ase they require exponential time in the size of the queries. Of ourse, when using larger datasets, the relative ontribution of these administrative osts will de rease proportionally. It an be observed that at deeper levels, these osts are less for the setting using pa ks. One of the auses is the fa t that the no-pa ks version also uses more memory than the pa ks setting (and hen e auses proportionally more memory management).\nHere again, the most important numbers are the speedup fa tors for the exe ution of queries. Speedup fa tors of query exe ution do not always in rease with in reasing depth of\nthe pa ks, in ontrast to Tilde where larger pa ks yielded higher speedups. At rst sight we found this surprising; however it be omes less so when the following observation is made. When re ning a pa k into a new pa k by adding a level,Warmr prunes away bran hes that lead only to infrequent queries. There are thus two e e ts when adding a level to a pa k: one is the widening of the pa k at the lowest level (at least on the rst few levels, a new pa k typi ally has more leaves than the previous one), the se ond is the narrowing of the pa k as a whole (be ause of pruning). Sin e the speedup obtained by using pa ks largely depends on the bran hing fa tor of the pa k, speedup fa tors an be expe ted to de rease when the narrowing e e t is stronger than the widening-at-the-bottom e e t. This an be seen, e.g, in the small-mutagenesis experiment, where at the deepest levels queries are be oming less frequent. For the mutagenesis experiment with the medium size language, query exe ution speedup fa tors are larger as the number of queries in reases mu h faster. For the mutagenesis experiment with the large language, it is the total speedup that is large, as the language generates so many queries that the most time- onsuming part be omes the administration and storage in memory. The pa ks version is mu h faster as it stores the queries in trees, requiring signi antly less memory."
    }, {
      "heading" : "5.3 Comparison with Other Engines",
      "text" : "Implementing a new spe ial-purpose Prolog engine, di erent from the already existing ones,\narries a risk: given the level of sophisti ation of popular Prolog engines, it is useful to he k whether the new engine performs omparably with these existing engines, at least for the tasks under onsideration here. The eÆ ien y gain obtained through query pa k exe ution should not be o set by a less eÆ ient implementation of the engine itself.\nOriginally the Tilde and Warmr systems were implemented in MasterProLog. In an attempt to allow them to run on other platforms, parts of these systems were reimplemented into a kind of \\generi \" Prolog from whi h implementations for spe i Prolog engines (SICStus, ilProlog) an easily be derived (the low level of standardisation of Prolog made this ne essary). Given this situation, there are two questions to be answered:\n(a) does the move from MasterProLog to other Prolog engines in uen e performan e in a negative way; and (b) does the performan e loss, if any, redu e the performan e improvements due to the use of pa ks?\nTilde and Warmr have been tuned for fast exe ution on MasterProLog and ilProlog but not for SICStus, whi h makes a omparison with the latter unfair; therefore we just report on the former 2 engines. Table 8 shows some results. These on rm that ilProlog is ompetitive with state-of-the-art Prolog engines."
    }, {
      "heading" : "5.4 Summary of Experimental Results",
      "text" : "Our experiments on rm that (a) query pa k exe ution in itself is mu h more eÆ ient than exe uting many highly similar queries separately; (b) existing ILP systems (we use Tilde and Warmr as examples) an use this me hanism to their advantage, a hieving signi ant speedups; and ) although a new Prolog engine is needed to a hieve this, the urrent state of development of this engine is su h that with respe t to exe ution speed it an ompete with state-of-the-art engines. Further, the experiments are onsistent with our omplexity analysis of the exe ution time of pa ks."
    }, {
      "heading" : "6. Related Work",
      "text" : "The re-implementation of Tilde is related to the work by Mehta et al. (1996) who were the rst to des ribe the \\examples in outer loop\" strategy for de ision tree indu tion. The query pa k exe ution me hanism, here des ribed from the Prolog exe ution point of view,\nan also be seen as a rst-order ounterpart of Apriori's me hanism for ounting item-sets\n(Agrawal et al., 1996).\nOther lines of work on eÆ ien y improvements for ILP involves sto hasti methods whi h trade a ertain amount of optimality for eÆ ien y by, e.g., evaluating lauses on a sample of the data set instead of the full data set (Srinivasan, 1999), exploring the lause sear h spa e in a random fashion (Srinivasan, 2000), or sto hasti ally testing whether a\nquery su eeds on an example (Sebag & Rouveirol, 1997). The rst of these is entirely orthogonal to query pa k exe ution and an easily be ombined with it.\nThe idea of optimising sets of queries instead of individual queries has existed for a while in the database ommunity. The typi al ontext onsidered in earlier resear h on multi-query optimisation (e.g., Sellis, 1988) was that of a database system that needs to handle disjun tions of onjun tive queries, or of a server that may re eive many queries from di erent lients in a brief time interval. If several of these queries are expe ted to ompute the same intermediary relations, it may be more eÆ ient to materialise these relations instead of having them re omputed for ea h query. Data mining provides in a sense a new\nontext for multi-query optimisation, in whi h the multi-query optimisation approa h is at the same time easier (the similarities among the queries are more systemati , so one need not look for them) and more promising (given the huge number of queries that may be generated at on e).\nTsur et al. (1998) des ribe an algorithm for eÆ ient exe ution of so- alled query o ks in this ontext. Like our query pa k exe ution me hanism, the query o k exe ution me hanism is inspired to some extent by Apriori and is set in a dedu tive database setting. The main di eren e between our query pa ks and the query o ks des ribed by Tsur et al. (1998) is that query pa ks are more hierar hi ally stru tured and the queries in a pa k are stru turally less similar than the queries in a o k. (A o k is represented by a single query with pla eholders for onstants, and is equal to the set of all queries that an be obtained by instantiating the pla eholders to onstants. Flo ks ould not be used for the appli ations we onsider here.)\nDekeyser and Paredaens (2001) des ribe work on multi-query optimisation in the ontext of relational databases. They also onsider tree-like stru tures in whi h multiple queries are\nombined; the main di eren e is that their trees are rooted in one single table from whi h the queries sele t tuples, whereas our queries orrespond to joins of multiple tables. Further, Dekeyser and Paredaens de ne a ost measure for trees as well as operators that map trees onto semanti ally equivalent (but less ostly) trees, whereas we have onsidered only the\nreation of pa ks and an eÆ ient top-down exe ution me hanism for them. Combining both\napproa hes seems an interesting topi for further resear h.\nFinally, other optimisation te hniques for ILP have been proposed that exploit results from program analysis (Santos Costa et al., 2000; Blo keel et al., 2000) or from propositional data mining te hnology (Blo keel et al., 1999). These are omplementary to our pa k exe ution optimisation. Espe ially the approa h of Blo keel et al. (1999) an easily be\nombined with our pa k me hanism. The te hniques dis ussed by Santos Costa et al. (2000) and Blo keel et al. (2000) involve optimisations for single query exe ution, some of whi h an to some extent be upgraded to the pa k setting. This is future work."
    }, {
      "heading" : "7. Con lusions",
      "text" : "There is a lot of redundan y in the omputations performed by most ILP systems. In this paper we have identi ed a sour e of redundan y and proposed a method for avoiding it: exe ution of query pa ks. We have dis ussed how query pa k exe ution an be in orporated in ILP systems. The query pa k exe ution me hanism has been implemented in a new Prolog system alled ilProlog and dedi ated to data mining tasks, and two ILP systems\nhave been re-implemented to make use of the me hanism. We have experimentally evaluated these re-implementations, and the results of these experiments on rm that large speedups may be obtained in this way. We onje ture that the query pa k exe ution me hanism an be in orporated in other ILP systems and that similar speedups an be expe ted.\nThe problem setting in whi h query pa k exe ution was introdu ed is very general, and allows the te hnique to be used for any kind of task where many queries are to be exe uted on the same data, as long as the queries an be organised in a hierar hy.\nFuture work in ludes further improvements to the ilProlog engine and the implementation of te hniques that will in rease the suitability of the engine to handle large data sets. In the best ase one might hope to ombine te hniques known from database optimisation and program analysis with our pa k exe ution me hanism to further improve the speed of ILP systems."
    }, {
      "heading" : "A knowledgements",
      "text" : "Hendrik Blo keel is a post-do toral fellow of the Fund for S ienti Resear h (FWO) of Flanders. Jan Ramon is funded by the Flemish Institute for the Promotion of S ienti Resear h in Industry (IWT). Henk Vande asteele was funded in part by the FWO proje t G.0246.99, \\Query languages for database mining\". The authors thank Lu De Raedt for his in uen e on this work, Ashwin Srinivasan for suggesting the term \\query pa ks\", the anonymous reviewers for their useful omments, and Kurt Driessens for proofreading this text. This work was motivated in part by the Esprit proje t 28623, Aladin."
    }, {
      "heading" : "Appendix A. Preparing the Query for the Meta-interpreter",
      "text" : "Note that the following prepro essor assumes that the pa k of the form a, (b, ( or d or e) or f or g, (h or i or j)) was already transformed to the form a , or([(b, or([ ,d,e℄)), f, (g, or([h,i,j℄))℄).\nprepro ess((A,B),(A,NewB),PrevNode,NodeNr0,LeafNr0,Bran hNr,NodeNr1,LeafNr1):- !,\nprepro ess(B,NewB,PrevNode,NodeNr0,LeafNr0,Bran hNr,NodeNr1,LeafNr1).\nprepro ess(or(Querys),or(NQuerys,PrevNode,NodeNr0,Bran hNr,Length),\nPrevNode,NodeNr0,LeafNr0,Bran hNr, NodeNr1,LeafNr1):- !,\nNodeNr2 is NodeNr0 + 1, prepro essbran hes(Querys,NQuerys,NodeNr0,NodeNr2,LeafNr0,\n1,NodeNr1,LeafNr1,Length).\nprepro ess(A,(A,leaf(PrevNode,Bran hNr,LeafNr0)),\nPrevNode,NodeNr0,LeafNr0, Bran hNr,NodeNr0,LeafNr1):-\nLeafNr1 is LeafNr0 + 1.\nprepro essbran hes([℄,[℄, ,NodeNr,LeafNr,Bran hNr, NodeNr,LeafNr,Bran hNr). prepro essbran hes([QueryjQuerys℄,[NewQueryjNewQuerys℄,PrevNode,\nNodeNr0,LeafNr0,Bran hNr, NodeNr1,LeafNr1,Length):-\nprepro ess(Query,NewQuery,\nPrevNode,NodeNr0,LeafNr0,Bran hNr, NodeNr2,LeafNr2),\nBran hNr1 is Bran hNr + 1, prepro essbran hes(Querys,NewQuerys, PrevNode,\nNodeNr2,LeafNr2,Bran hNr1, NodeNr1,LeafNr1,Length).\nReferen es\nAgrawal, R., Mannila, H., Srikant, R., Toivonen, H., & Verkamo, A. (1996). Fast dis overy\nof asso iation rules. In Fayyad, U., Piatetsky-Shapiro, G., Smyth, P., & Uthurusamy, R. (Eds.), Advan es in Knowledge Dis overy and Data Mining, pp. 307{328. The MIT Press.\nA t-Ka i, H. (1991). Warren's Abstra t Ma hine: A Tutorial Re onstru tion. The MIT\nPress, Cambridge, Massa husetts. http://www.isg.sfu. a/~hak/do uments/wam.html.\nBlo keel, H. (1998). Top-down indu tion of rst order logi al de ision trees. Ph.D. thesis,\nDepartment of Computer S ien e, Katholieke Universiteit Leuven. http://www. s.kuleuven.a .be/~ml/PS/blo keel98:phd.ps.gz.\nBlo keel, H., & De Raedt, L. (1997). Lookahead and dis retization in ILP. In Pro eedings\nof the Seventh International Workshop on Indu tive Logi Programming, Vol. 1297 of Le ture Notes in Arti ial Intelligen e, pp. 77{85. Springer-Verlag.\nBlo keel, H., & De Raedt, L. (1998). Top-down indu tion of rst order logi al de ision trees.\nArti ial Intelligen e, 101 (1-2), 285{297.\nBlo keel, H., De Raedt, L., Ja obs, N., & Demoen, B. (1999). S aling up indu tive logi pro-\ngramming by learning from interpretations. Data Mining and Knowledge Dis overy, 3 (1), 59{93.\nBlo keel, H., De Raedt, L., & Ramon, J. (1998). Top-down indu tion of lustering trees.\nIn Pro eedings of the 15th International Conferen e on Ma hine Learning, pp. 55{63. http://www. s.kuleuven.a .be/~ml/PS/ML98-56.ps.\nBlo keel, H., Demoen, B., Janssens, G., Vande asteele, H., & Van Laer, W. (2000). Two\nadvan ed transformations for improving the eÆ ien y of an ILP system. In 10th International Conferen e on Indu tive Logi Programming, Work-in-Progress Reports, pp. 43{59, London, UK.\nBongard, M. (1970). Pattern Re ognition. Spartan Books.\nBratko, I. (1990). Prolog Programming for Arti ial Intelligen e. Addison-Wesley, Woking-\nham, England. 2nd Edition.\nBreiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classi ation and Regression\nTrees. Wadsworth, Belmont.\nChen, W., & Warren, D. S. (1996). Tabled evaluation with delaying for general logi pro-\ngrams. Journal of the ACM, 43 (1), 20{74. http://www. s.sunysb.edu/~sbprolog.\nClark, P., & Niblett, T. (1989). The CN2 algorithm. Ma hine Learning, 3 (4), 261{284.\nDe Raedt, L. (1997). Logi al settings for on ept learning. Arti ial Intelligen e, 95, 187{\n201.\nDe Raedt, L., & Dehaspe, L. (1997). Clausal dis overy. Ma hine Learning, 26, 99{146.\nDe Raedt, L., & D zeroski, S. (1994). First order jk- lausal theories are PAC-learnable.\nArti ial Intelligen e, 70, 375{392.\nDe Raedt, L., & Van Laer, W. (1995). Indu tive onstraint logi . In Jantke, K. P., Shino-\nhara, T., & Zeugmann, T. (Eds.), Pro eedings of the Sixth International Workshop on Algorithmi Learning Theory, Vol. 997 of Le ture Notes in Arti ial Intelligen e, pp. 80{94. Springer-Verlag.\nDehaspe, L., & Toivonen, H. (1999). Dis overy of frequent datalog patterns. Data Mining\nand Knowledge Dis overy, 3 (1), 7{36.\nDekeyser, S., & Paredaens, J. (2001). Query pa k trees for multi query optimization. Te h.\nrep. 01-04, University of Antwerp. ftp://wins.uia.a .be/pub/dekeyser/qpt.ps.\nDemoen, B., Janssens, G., & Vande asteele, H. (1999). Exe uting query flo ks for ILP. In\nEtalle, S. (Ed.), Pro eedings of the Eleventh Benelux Workshop on Logi Programming, pp. 1{14, Maastri ht, The Netherlands. 14 pages.\nKramer, S. (1996). Stru tural regression trees. In Pro eedings of the Thirteenth National\nConferen e on Arti ial Intelligen e, pp. 812{819, Cambridge/Menlo Park. AAAI Press/MIT Press.\nMehta, M., Agrawal, R., & Rissanen, J. (1996). SLIQ: A fast s alable lassi er for data\nmining. In Pro eedings of the Fifth International Conferen e on Extending Database Te hnology.\nMuggleton, S. (1995). Inverse entailment and Progol. New Generation Computing, Spe ial\nissue on Indu tive Logi Programming, 13 (3-4), 245{286.\nMuggleton, S., & De Raedt, L. (1994). Indu tive logi programming : Theory and methods.\nJournal of Logi Programming, 19,20, 629{679.\nQuinlan, J. R. (1993a). C4.5: Programs for Ma hine Learning. Morgan Kaufmann series in\nma hine learning. Morgan Kaufmann.\nQuinlan, J. (1993b). FOIL: A midterm report. In Brazdil, P. (Ed.), Pro eedings of the 6th\nEuropean Conferen e on Ma hine Learning, Le ture Notes in Arti ial Intelligen e. Springer-Verlag.\nSantos Costa, V., Srinivasan, A., & Cama ho, R. (2000). A note on two simple transform-\nations for improving the eÆ ien y of an ILP system. In Pro eedings of the Tenth International Conferen e on Indu tive Logi Programming, Vol. 1866 of Le ture Notes in Arti ial Intelligen e, pp. 225{242. Springer-Verlag.\nSebag, M., & Rouveirol, C. (1997). Tra table Indu tion and Classi ation in First-Order\nLogi via Sto hasti Mat hing. In Pro eedings of the 15th International Joint Conferen e on Arti ial Intelligen e. Morgan Kaufmann.\nSellis, T. (1988). Multiple-query optimization. ACM Transa tions on Database Systems,\n13 (1), 23{52.\nSrinivasan, A. (1999). A study of two sampling methods for analysing large datasets with\nILP. Data Mining and Knowledge Dis overy, 3 (1), 95{123.\nSrinivasan, A. (2000). A study of two probabilisti methods for sear hing large spa es with\nILP. Te h. rep. PRG-TR-16-00, Oxford University Computing Laboratory.\nSrinivasan, A., Muggleton, S., & King, R. (1995). Comparing the use of ba kground know-\nledge by indu tive logi programming systems. In De Raedt, L. (Ed.), Pro eedings of the Fifth International Workshop on Indu tive Logi Programming.\nTsur, D., Ullman, J., Abiteboul, S., Clifton, C., Motwani, R., Nestorov, S., & Rosenthal, A.\n(1998). Query o ks: A generalization of asso iation-rule mining. In Pro eedings of the ACM SIGMOD International Conferen e on Management of Data (SIGMOD-98), Vol. 27,2 of ACM SIGMOD Re ord, pp. 1{12, New York. ACM Press."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "Indu tive logi programming, or relational learning, is a powerful paradigm for ma hine learning or data mining. However, in order for ILP to be ome pra ti ally useful, the eÆ ien y of ILP systems must improve substantially. To this end, the notion of a query pa k is introdu ed: it stru tures sets of similar queries. Furthermore, a me hanism is des ribed for exe uting su h query pa ks. A omplexity analysis shows that onsiderable eÆ ien y improvements an be a hieved through the use of this query pa k exe ution me hanism. This laim is supported by empiri al results obtained by in orporating support for query pa k exe ution in two existing learning systems.",
    "creator" : "dvips(k) 5.86 Copyright 1999 Radical Eye Software"
  }
}