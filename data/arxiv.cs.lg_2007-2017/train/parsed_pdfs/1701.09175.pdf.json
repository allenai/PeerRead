{
  "name" : "1701.09175.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Skip Connections as Effective Symmetry-Breaking",
    "authors" : [ "Emin Orhan" ],
    "emails" : [ "aeminorhan@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Contents"
    }, {
      "heading" : "1 Introduction 1",
      "text" : ""
    }, {
      "heading" : "2 Results 2",
      "text" : "2.1 Symmetries in fully-connected networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2.2 Landscapes of small networks with binary weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.3 Dynamics of learning in linear networks with skip connections . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.3.1 Three-layer networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3.2 Networks with more than three-layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.4 Experiments with fully-connected networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.4.1 Alternative ways of breaking the permutation symmetry of hidden units . . . . . . . . . . . . . . 9 2.4.2 Non-identity skip connections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.4.3 Hyper-residual networks and breaking the rescaling symmetry . . . . . . . . . . . . . . . . . . . . 11 2.5 Symmetry-breaking in recurrent neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"
    }, {
      "heading" : "3 Discussion 13",
      "text" : ""
    }, {
      "heading" : "1 Introduction",
      "text" : "Introduction of skip (or residual) connections has substantially improved the training of very deep neural networks [6,7,9]. Despite informal intuitions and sometimes worryingly non-rigorous metaphors (“keeping a “clean” information path” [7], “to ... improve the information flow between layers” [9]) put forward to motivate skip connections, a clear understanding of how these connections improve training has been lacking. Such understanding is invaluable both in its own right and for the possibilities it might offer for further improvements in training very deep neural networks. A number of recent papers addressed different aspects of this question [4, 11, 14]. In this paper, we attempt to shed further light on this question. We argue that skip connections help break symmetries inherent in the loss landscapes of deep neural networks. Symmetries lead to saddle structures in the landscape, causing problems for gradient-based optimization methods [1,17]. Hence, symmetry breaking is useful to the extent that it eliminates such saddle structures. We show that skip connections between adjacent layers break the permutation symmetry of nodes at a given layer, whereas the more recently introduced DenseNet architecture [9], where each layer projects skip connections to every\nar X\niv :1\n70 1.\n09 17\n5v 1\n[ cs\n.N E\n] 3\n1 Ja\nn 20\n17\nlayer above it, breaks the rescaling symmetry of connectivity matrices between different layers. Breaking of these symmetries leads to drastically more regular landscapes than those of networks with no skip connections."
    }, {
      "heading" : "2 Results",
      "text" : ""
    }, {
      "heading" : "2.1 Symmetries in fully-connected networks",
      "text" : "Loss landscapes of fully-connected multilayer networks are riddled with several exact and approximate symmetries. We will focus on two particular symmetries in this paper. The first one is the permutation symmetry of nodes within a given layer: nodes within the same layer can be permuted without changing the function computed by the network (Figure 1a). This is a rather general symmetry that does not require any restrictive assumptions on the network.\nThe second symmetry is the invariance of the computed function to a rescaling of the connectivity matrices between different layers (Figure 1b): the connectivity matrix Wk can be scaled by a constant g and Wl by the constant 1/g without changing the function computed by the network (if biases are included, all the biases between the layers k and l also have to be scaled by g). This symmetry holds exactly in networks using the ReLU nonlinearity as the activation function (as well as in linear networks). A similar rescaling symmetry has been discussed before [15], where it has been noted that the incoming weights of any given node can be scaled up by a constant and its outgoing weights scaled down by the same constant without changing the function computed by the network and a modified stochastic gradient descent (SGD) algorithm has been proposed to effectively break this rescaling symmetry.\nHow do skip connections break these symmetries? Skip connections between adjacent layers break the permutation symmetry of hidden units at a given layer by ordering them according to the ordering of the hidden units at the previous layer (Figure 2a). Note that the input units are already ordered unambiguously. Additional skip connections between each layer and all layers above it, as in the DenseNet architecture introduced in [9], break the rescaling symmetry of the weight matrices by adding distinct sets of skip vectors to each layer (Figure 2b).\nTo illustrate the second point, we consider the simple linear case: the composite function computed by a linear L-layer plain network can be expressed as WL−1WL−2 . . .W1x1, where x1 is the input (ignoring the biases for simplicity). This expression clearly displays the rescaling symmetry as WL−1 . . .Wl . . .Wk . . .W1 = WL−1 . . . g\n−1Wl . . . gWk . . .W1. In linear networks with skip connections between each layer and all layers above it (we call this architecture “hyperresidual” henceforth) and assuming all skip connectivity matrices to be the identity, the composite function computed by the network becomes (WL−1+(L−1)I)(WL−2+(L−2)I) . . . (W1+I)x1 and the rescaling symmetry is broken, since (WL−1+(L−1)I) . . . (Wl+lI) . . . (Wk+kI) . . . (W1+I) 6= (WL−1+(L−1)I) . . . (g−1Wl+lI) . . . (gWk+kI) . . . (W1+I).\nWe note that the rescaling symmetry is also broken in the “residual” architecture with skip connections between adjacent layers only. In this case, the function computed by the network can be expressed as (WL−1 + I)(WL−2 + I) . . . (W1 +I)x1 and it is easy to see that (WL−1 +I) . . . (Wl+I) . . . (Wk+I) . . . (W1 +I) 6= (WL−1 +I) . . . (g−1Wl+ I) . . . (gWk + I) . . . (W1 + I). Although both residual and hyper-residual architectures break the rescaling symmetry,\nwe find empirically that the hyper-residual architecture is more effective in this respect, presumably because it sets the scales of each connectivity matrix less ambiguously than the residual architecture by adding more distinct skip connectivity matrices at each layer. We demonstrate this effect empirically in Figure 2c, where the training accuracy ratios of rescaled to non-rescaled networks are shown for the three architectures in 20-layer nonlinear fully-connected networks trained on the CIFAR-100 dataset. A ratio of 1 indicates perfect rescaling symmetry and smaller ratios indicate less rescaling symmetry."
    }, {
      "heading" : "2.2 Landscapes of small networks with binary weights",
      "text" : "To illustrate how skip connections change the loss landscapes of multilayer networks, we first consider a toy model with seven layers and two nodes at each layer, except for the final layer which has a single node (Figure 3). To be able to characterize the landscape completely, all the weights in the network are restricted to be +1 or −1. Biases of the units are set to zero (assigning random biases to the units leads to qualitatively similar results; supplementary Figure S1). This gives rise to a model with 23 binary parameters and a landscape with 223 (∼ 8.4M) possible parameter configurations, which we evaluate exhaustively on a two-dimensional regression problem with a random target function.\nWe considered the four architectures shown in Figure 3: (a) the plain architecture is a fully connected feedforward network with no skip connections (Figure 3a), described by the equation:\nxl+1 = f(Wlxl) (1)\nfor l = 1, . . . , 5. f(·) is chosen to be the ReLU nonlinearity and x1 denotes the input layer. (b) The residual architecture introduces identity skip connections between adjacent layers (Figure 3b). To avoid any trivial effects due to differences in scaling of the activities in different architectures, we normalized the unit activities by the number of incoming connections they receive and equalized this between all four models (note that the number of incoming connections for each hidden unit is 2 in the plain architecture). Hence, the equations describing the residual architecture are given by:\nxl+1 = 2\n3\n[ f(Wlxl) + xl ] (2)\nfor l = 1, . . . , 5. (c) The hyper-residual architecture adds identity skip connections between each layer and all the layers above it (Figure 3c):\nxl+1 = 2\nl + 2\n[ f(Wlxl) + xl + xl−1 + . . .+ x1 ] (3)\nfor l = 1, . . . , 5. This architecture is inspired by the DenseNet architecture introduced in [9]. In both architectures, each layer projects skip connections to layers above it, but the specific way in which these projections are combined at a given layer differs between the two architectures.\n(d) In the path-residual architecture, each individual connection has its own identity skip connection (Figure 3d).\nThis is equivalent to using a matrix of ones, instead of an identity matrix, as the skip connectivity matrix:\nxl+1 = 2\n4\n[ f(Wlxl) + 1xl ] (4)\nwhere 1 denotes a matrix of ones. Because each hidden unit receives the same skip connections from both units at the previous layer, this architecture does not break the permutation symmetry of the hidden units and therefore we expect its landscape to be more rugged than the landscapes of the residual and hyper-residual architectures.\nWe define the neighbors of a given configuration to be the configurations that can be reached from it by one bit flips, i.e. by flipping a single parameter from +1 to −1 or vice versa.\nFor each configuration, we computed the number of better neighbors of that configuration, i.e. neighbors that have a lower loss value. Figure 4 (left) shows the distribution of the number of better neighbors for the four architectures. Note that zero corresponds to local minima and 23 to local maxima in this plot. The plain architecture has a large number of local minima (∼ 3.2M) and the distribution is dominated by configurations with one or zero better neighbors, hinting at a very rugged landscape. The residual architecture drastically reduces the number of local minima to ∼ 140K and the hyper-residual architecture reduces it further to a mere 54 local minima. As predicted from a symmetry-breaking consideration, the path-residual architecture has a more rugged landscape than the residual one, having ∼ 505K local minima.\nStatistics of random or adaptive walks are also commonly used to describe various features of combinatorial landscapes [16]. We performed adaptive walks in all landscapes by starting from each configuration and moving to the best neighbor at each step until a local minimum is reached. Such walks take shorter in rugged landscapes than in more regular landscapes. Similarly, basin sizes of local minima should be smaller in rugged landscapes. We found that the plain architecture has the shortest walks to a local minimum and the smallest basin sizes. The hyper-residual architecture, on the other hand, has the longest walks and largest basin sizes (Figure 4 middle and right), consistent with the distributions of the number of better neighbors for the different architectures."
    }, {
      "heading" : "2.3 Dynamics of learning in linear networks with skip connections",
      "text" : "We next investigate how skip connections affect the learning dynamics in linear networks. We recall that in an L-layer linear plain network, the input-output mapping is given by:\nxL = WL−1WL−2 . . .W1x1 (5)\nwhere x1 and xL are the input and output vectors, respectively. In linear residual networks with identity skip connections between adjacent layers, the input-output mapping becomes:\nxL = (WL−1 + I)(WL−2 + I) . . . (W1 + I)x1 (6)\nFinally, in hyper-residual linear networks, the input-output mapping is given by: xL = ( WL−1 + (L− 1)I )( WL−2 + (L− 2)I ) . . . ( W1 + I ) x1 (7)\nIn the derivations to follow, we do not have to assume that the connectivity matrices are square matrices. If they are rectangular matrices, the identity matrix I should be interpreted to be a rectangular identity matrix of the appropriate size. This corresponds to zero-padding the layers when they are not the same size, as is usually done in practice."
    }, {
      "heading" : "2.3.1 Three-layer networks",
      "text" : "Dynamics of learning in plain linear networks with no skip connections was analyzed in [18]. For a three-layer network (L = 3), the learning dynamics can be expressed by the following differential equations [18]:\nτ d\ndt aα = (sα − aα · bα)bα − ∑ γ 6=α (aα · bγ)bγ (8)\nτ d\ndt bα = (sα − aα · bα)aα − ∑ γ 6=α (aγ · bα)aγ (9)\nHere aα and bα are n-dimensional column vectors (where n is the number of hidden units) connecting the hidden layer to the α-th input and output modes, respectively, of the input-output correlation matrix and sα is the corresponding singular value (see [18] for further details). The first term on the right-hand side of Equations ?? facilitates cooperation between aα and bα corresponding to the same input-output mode, while the second term encourages competition between vectors corresponding to different modes. These dynamics can be interpreted as gradient descent on the following energy function:\nE = 1\n2τ ∑ α (sα − aα · bα)2 + 1 2τ ∑ α6=β (aα · bβ)2 (10)\nThis energy function has several symmetries. The one that concerns us in connection with skip connections between adjacent layers is the permutation symmetry of hidden units which reveals itself as the invariance of the energy function to a (simultaneous) permutation of the elements of the vectors aα and bα for all α. This causes saddle structures in the landscape that slow down learning. Specifically, for the permutation symmetry of hidden units, these saddle structures are the hyperplanes aαi = a α j ∀α, for each pair of hidden units i, j (similarly, the hyperplanes bαi = bαj ∀α) that make the model non-identifiable. Formally, these correspond to the singularities of the Fisher information matrix, or the Hessian [1]. Indeed, it is easy to check that when aαi = a α j ∀α for any pair of hidden units i, j, the Hessian becomes singular in the plain network, but not in the residual network due to the broken symmetry (Supplementary Note 2). The Hessian also has additional singularities at the hyper-planes aαi = 0 ∀α for any i and at bαi = 0 for any i and α, which can be expected to affect the dynamics if, for instance, the parameters are initialized to small random values. However, these additional singularities are not caused by the permutation symmetry of the hidden units. The effect of such saddle structures on the learning dynamics has previously been analyzed in shallow non-linear feedforward networks [1,17]: in particular, it has been shown that they significantly slow down learning. Once on any of these manifolds, the dynamics\nnever leaves it. In practice, the variables are initialized randomly and hence they eventually escape the vicinity of the slow manifolds, but the manifolds can exert their effect for a long time [1,17].\nIn the simplest scenario where there are only two input and output modes, the learning dynamics of Equations 8, 9 reduces to:\nd dt a1 = (s1 − a1 · b1)b1 − (a1 · b2)b2 (11) d dt a2 = (s2 − a2 · b2)b2 − (a2 · b1)b1 (12)\nd dt b1 = (s1 − a1 · b1)a1 − (a1 · b2)a2 (13) d dt b2 = (s2 − a2 · b2)a2 − (a2 · b1)a1 (14)\nHow does adding skip connections between adjacent layers change the learning dynamics? Considering again a three-layer network (L = 3) with only two input and output modes, a straightforward extension of Equations 11-14 shows that the learning dynamics changes as follows:\nd dt a1 =\n[ s1 − (a1 + v1) · (b1 + u1) ] (b1 + u1)− [ (a1 + v1) · (b2 + u2) ] (b2 + u2) (15)\nd dt a2 =\n[ s2 − (a2 + v2) · (b2 + u2) ] (b2 + u2)− [ (a2 + v2) · (b1 + u1) ] (b1 + u1) (16)\nd dt b1 =\n[ s1 − (a1 + v1) · (b1 + u1) ] (a1 + v1)− [ (a1 + v1) · (b2 + u2) ] (a2 + v2) (17)\nd dt b2 =\n[ s2 − (a2 + v2) · (b2 + u2) ] (a2 + v2)− [ (a2 + v2) · (b1 + u1) ] (a1 + v1) (18)\nwhere u1 and u2 are orthonormal vectors (similarly for v1 and v2). The derivation proceeds essentially identically to the corresponding derivation for plain networks in [18]. The only differences are: (i) we substitute the plain weight matrices Wl with their residual counterparts Wl + I and (ii) when changing the basis from the canonical basis for the weight matrices W1, W2 to the input and output modes of the input-output correlation matrix, U and V, we note that:\nW2 + I = UW2 + UU > = U(W2 + U >) (19) W1 + I = W1V > + VV> = (W1 + V)V > (20)\nwhere U and V are orthogonal matrices and the vectors aα, bα, uα and vα in Equations 15-18 correspond to the α-th columns of the matrices W1, W > 2 , U and V, respectively.\nFigure 5 shows, for two different initializations, the evolution of the variables aα and bα in plain and residual networks with input and output modes and two hidden units. When the variables are initialized to small random values, the dynamics in the plain network initially evolves slowly (Figure 5a, blue); whereas it is much faster in the residual network (Figure 5a, red). This effect is attributable to two factors. First, the added orthonormal vectors uα and vα increase the initial velocity of the variables in the residual network. Second, even when we equalize the initial norms of the vectors, aα and aα + vα (and those of the vectors bα and bα + uα) in the plain and the residual networks, respectively, we still observe an advantage for the residual network (Figure 5b), because the cooperative and competitive terms are orthogonal to each other in the residual network (or close to orthogonal, depending on the initialization of aα and bα; see right-hand side of Equations 15-18), whereas in the plain network they are not necessarily orthogonal and hence can cancel each other (Equations 11-14), thus slowing down convergence."
    }, {
      "heading" : "2.3.2 Networks with more than three-layers",
      "text" : "As shown in [18], in linear networks with more than a single hidden layer, assuming that there are orthogonal matrices Rl and Rl+1 for each layer l that diagonalize the initial weight matrix of the corresponding layer (i.e. R > l+1Wl(0)Rl = Dl is a diagonal matrix), dynamics of different singular modes decouple from each other and each mode α evolves according to gradient descent dynamics in an energy landscape described by [18]:\nEplain = 1\n2τ\n( sα − Nl−1∏ l=1 aαl )2 (21)\nwhere aαl can be interpreted as the strength of mode α at layer l and Nl is the total number of layers. In residual networks, assuming further that the orthogonal matrices Rl satisfy R > l+1Rl = I, the energy function changes to:\nEhyperres = 1\n2τ\n( sα − Nl−1∏ l=1 (aαl + 1) )2\n(22)\nand in hyper-residual networks, it is:\nEres = 1\n2τ\n( sα − Nl−1∏ l=1 (aαl + l) )2\n(23)\nComparing these energy functions, we see that the plain network possesses a rescaling symmetry between mode strengths at different layers manifested in the rescaling symmetry of the product term in the right-hand sides of Equations 21. The residual and the hyper-residual models eliminate the rescaling symmetry by adding constants to the mode strength variables. As discussed before, we empirically find the symmetry-breaking to be more effective in the hyper-residual model (Figure 2c). Moreover, the residual energy function (Equation 22) possesses an additional symmetry: the permutation symmetry between mode strength variables aαl . This symmetry causes a singularity in the Hessian whenever aαi = a α j for any pair of layers i, j (Supplementary Note 3). The hyper-residual model eliminates this additional permutation symmetry as well by adding distinct constants to mode strength variables at different layers (Equation 23). Figure 6a illustrates the effect of skip connections on the phase portrait of a three layer network. The two axes, a and b, represent the mode strength variables for l = 1 and l = 2, respectively: i.e. a ≡ aα1 and b ≡ aα2 . The plain network has a saddle point at (0, 0) (Figure 6a; left). The dynamics around this point is slow, hence starting from small random values causes initially very slow learning. The network funnels the dynamics through the unstable manifold a = b to the stable hyperbolic solution corresponding to ab = s. Identity skip connections between adjacent layers in the residual architecture move the saddle point to (−1,−1) (Figure 6a; middle). This speeds up the dynamics around the origin, but not as much as in the hyper-residual architecture where the saddle point is moved further away from the origin and the main diagonal to (−1,−2) (Figure 6a; right). We found these effects to be more pronounced in deeper networks. Figure 6b shows the dynamics of learning in 10-layer linear networks, demonstrating a clear advantage for the residual architecture over the plain architecture and for the hyper-residual architecture over the residual architecture."
    }, {
      "heading" : "2.4 Experiments with fully-connected networks",
      "text" : "To test the symmetry-breaking hypothesis in more realistic networks, we conducted several experiments with deep fully-connected feedforward networks. In this section, we present the results of these experiments. We recall that the equations describing the plain networks are given by:\nxl+1 = f(Wlxl + bl+1) (24)\nThe equations for the residual networks are given by:\nxl+1 = f(Wlxl + bl+1) + Qlxl (25)\nwhere Ql denotes the skip connectivity matrix, which can be different from the identity matrix, and the equations describing the hyper-residual networks are given by:\nxl+1 = f(Wlxl + bl+1) + Qlxl + 1\nl − 1\n[ Ql−1xl−1 + . . .+ Q1x1 ] (26)\nwhere every layer projects to all layers above itself. We divided the contribution from the non-adjacent layers by l− 1, because we found that this performed better than the non-normalized version. As usual, f(·) is chosen to be the ReLU nonlinearity. The networks all have 30 fully-connected hidden layers (20 layers in Figure 11) with n = 128 hidden units in each hidden layer."
    }, {
      "heading" : "2.4.1 Alternative ways of breaking the permutation symmetry of hidden units",
      "text" : "If the success of the residual network architecture can be attributed, at least partly, to symmetry breaking, then alternative ways of breaking the permutation symmetry of the hidden units at the same layer should also improve training. We tested this hypothesis by introducing a particularly simple way of breaking the permutation symmetry of the hidden units. For each layer in the network, we drew random biases from a Gaussian distribution, N (0, σ2), for each hidden unit in that layer. We used these random values as soft targets for the biases of the hidden units. In particular, we put an l2-norm penalty on deviations from those bias values. This imposes a particular order on the hidden units according to their target biases and hence breaks their permutation symmetry. Note that setting σ = 0 corresponds to the standard l2-norm regularization of the biases, which does not break the symmetry of the hidden units. Hence, we expect the performance to be worse in this case than in cases with properly broken symmetry. On the other hand, although larger values of σ correspond to greater symmetry-breaking, the network also has to perform well in the classification task and very large σ values might be inconsistent with the latter requirement. Therefore, we expect the performance to be optimal for intermediate values of σ. In the experiments reported below, we found the values of σ, the standard deviation of the target bias distribution, and λ, the strength of the bias regularization term, that achieved the best average training accuracy through grid search.\nPutting a prior over the biases can be considered as indirectly putting a prior over the activities of the units. More complicated joint priors over hidden unit responses that favor decorrelated [3] or clustered [13] responses have been proposed before. Although the primary motivation for these regularization schemes was to improve the generalizability or interpretability of the learned representations, they can potentially be understood from a symmetry-breaking perspective as well. For example, a prior that favors decorrelated responses can help facilitate the breaking of permutation symmetries between hidden units, even though it does not directly break those symmetries itself (unlike the bias regularizer we have used).\nWe trained 30-layer feedforward networks on CIFAR-10 and CIFAR-100 (with coarse labels) datasets. Because we are primarily interested in understanding how symmetry-breaking changes the shape of the loss landscape and consequently affects the optimization difficulty, we primarily monitor the training accuracy. Thus, unless otherwise noted, all the results reported below are training accuracies. Figure 7 shows the training accuracy of different models on CIFAR-10 and CIFAR-100 datasets.\nFor both datasets, the residual network performs the best and the plain network the worst. Symmetry-breaking through bias regularization (BiasSymmBreak, black) leads to a significant improvement over the plain network. Importantly, just putting an l2-norm penalty on the biases (BiasL2Reg (σ = 0)) does not improve performance over the plain network. These results are consistent with the symmetry-breaking hypothesis. In all these models, we initialized the biases to zero. We also tested a plain network where the biases were initialized to random values drawn from the same target bias distribution as in the BiasSymmBreak model, but the regularization was not enforced throughout training. We call this network “BiasInit” (green in Figure 7). The BiasInit network performed significantly worse than the BiasSymmBreak network, suggesting that enforcing the symmetry-breaking bias regularization through the entire training period is more beneficial than breaking the symmetry through initialization only.\nIt is important to note that the BiasSymmBreak network does not perform nearly as well as the residual network. We conjecture that this is due to additional benefits of the residual architecture over and above the symmetry-breaking it enables. One of those advantages is its ability to deal effectively with the vanishing gradients problem encountered in training deep networks [2, 8]. Indeed, we found that the gradient norms with respect to the layer activities do not\ndiminish in earlier layers of the residual network (Figure 8a), demonstrating that it effectively solves the vanishing gradients problem. On the other hand, both in the plain network and in the BiasSymmBreak network, the gradient norms decay quickly as one descends from the top of the network. Adding a single batch normalization layer in the middle of the BiasSymmBreak network alleviates the vanishing gradients problem and brings its performance close to that of the residual network (Figure 8a-b; BiasSymmBreak+BN)."
    }, {
      "heading" : "2.4.2 Non-identity skip connections",
      "text" : "If the symmetry-breaking hypothesis is correct, there should be nothing special about identity skip connections. Skip connections other than identity should lead to training improvements as well, as long as they break the permutation symmetries. The crucial condition for symmetry-breaking is that the skip connection vector for each unit should disambiguate that unit optimally from all other nodes in that layer. Mathematically, this corresponds to an orthogonality condition on the skip connectivity matrix. We therefore tested random dense orthogonal matrices as skip connectivity matrices. Random dense orthogonal matrices performed at least as well as the identity skip connections (Figure 9, black vs. red). In fact, these dense orthogonal matrices performed slightly better than identity skip connections in both CIFAR-10 and CIFAR-100 datasets. We suspect that the reason for this is that sometimes units go silent because of the ReLU nonlinearity. When that happens to two distinct units at layer l, the identity skips cannot disambiguate the corresponding units at the next layer; whereas with dense orthogonal skips, all units at layer l are made use of, so even if some of them go silent, the units at layer l + 1 can still be disambiguated with the remaining active units.\nNext, we gradually decreased the degree of “orthogonality” of the skip connectivity matrix to see how the orthogonality of the matrix affects the performance. Starting from a random dense orthogonal matrix, we first divided the matrix into two halves and copied the first half to the second half. Starting from n orthonormal vectors, this reduces the number of orthonormal vectors to n/2. We continued on like this until the columns of the matrix were repeats of a single unit vector. We predict that as the number of orthonormal vectors in the skip connectivity matrix is decreased, the performance should deteriorate, because the symmetry-breaking capacity of the skip connectivity matrix is reduced. Figure 9 shows the results for n = 128 hidden units. Darker colors correspond to “more orthogonal” matrices (e.g. 128 means all 128 vectors are orthonormal to each other etc.) The red line is the identity skip connectivity. Clearly more orthogonal skip connectivity matrices yield better performance, consistent with the symmetry breaking hypothesis.\nThe less orthogonal skip matrices also suffer from the vanishing gradients problem. So, their failure could be partly attributed to the vanishing gradients problem. To control for this effect, we also designed skip connectivity matrices with eigenvalues on the unit circle (hence with eigenvalue spectra equivalent to an orthogonal matrix), but with varying degrees of orthogonality (Supplementary Note 4). More specifically, the columns (or rows) of an orthogonal matrix are orthonormal to each other, hence the covariance matrix of these vectors is the identity matrix. We designed matrices where this covariance matrix was allowed to have non-zero off-diagonal values, reflecting the fact that the vectors are not orthogonal any more. By controlling the magnitude of the correlations between the vectors, we manipulated the degree of orthogonality of the vectors. We achieved this by setting the eigenvalue spectrum of the covariance matrix to be given by λi = exp(−τ(i−1)) where λi denotes the i-th eigenvalue of the covariance matrix and τ is the parameter that controls the degree of orthogonality: τ = 0 corresponds to the identity covariance matrix, hence to an orthonormal set of vectors, whereas larger values of τ correspond to gradually more correlated vectors. This orthogonality manipulation was done\nwhile fixing the eigenvalue spectrum of the skip connectivity matrix to be on the unit circle (see supplementary Note 4 for details). Hence, the effects of this manipulation cannot be attributed to any change in the eigenvalue spectrum, but only to the degree of orthogonality of the skip vectors.\nThe results of this experiment is shown in Figure 10. Clearly, more orthogonal skip connectivity matrices still perform better than less orthogonal ones, even when their eigenvalue spectrum is fixed, suggesting that the results of the earlier experiment (Figure 9) cannot be explained solely by the vanishing gradients problem."
    }, {
      "heading" : "2.4.3 Hyper-residual networks and breaking the rescaling symmetry",
      "text" : "The results for the hyper-residual networks are shown in Figure 11. In these networks, we chose the skip connectivity matrix between adjacent layers (Ql in Equation 26) to be the identity matrix. For the skip connectivity matrices between non-adjacent layers, we used dense random matrices that were twice folded starting from an orthogonal matrix (corresponding to matrices labeled “32” in Figure 9). The results show that these hyper-residual networks perform better than residual networks with identity skip connections only between adjacent layers (represented by the solid red lines in Figure 11).\nTo provide evidence that the breaking of the rescaling symmetry contributes to the performance improvement in the hyper-residual architecture, we introduced an alternative way of breaking the rescaling symmetry and tested its effectiveness. A particularly simple way of breaking the rescaling symmetry in a plain network is to add constant, untrained biases to each layer in addition to the trainable biases that they possess. This can be thought of as adding skip connections to each layer with a constant input of 1. Specifically, we added biases of the form al + b to each layer, where l denotes the layer index and a and b are hyper-parameters. All units in a given layer received the same constant bias, therefore this manipulation does not break the permutation symmetry of hidden units. Also, note that setting a = b = 0 corresponds to a plain network. We performed a grid search over the hyper-parameters a and b. We predicted that the networks where the rescaling symmetry is broken would generally perform better than the plain network. Moreover, adding distinct biases to each layer, as opposed to adding the same bias to each layer, should be more effective at breaking the rescaling symmetry (analogous to how the hyper-residual architecture is more effective at breaking the rescaling symmetry than the residual architecture: Figure 2c), and hence, is predicted to perform better. Both of these predictions were borne out: the best-performing network had a decreasing bias profile as a function of layer index l (black line in Figure 12a). The best-performing network with a constant bias profile is shown by the blue line in Figure 12a. The network with the decreasing bias profile displayed less rescaling-symmetry than the best-performing network with a constant bias profile, as measured by the rescaling experiment discussed in connection with Figure 2c above (compare black and blue bars in Figure 12b). Both of these networks had less rescaling-symmetry, and performed better, than the plain network (Figure 12b-c)."
    }, {
      "heading" : "2.5 Symmetry-breaking in recurrent neural networks",
      "text" : "It is well-known that a recurrent neural network unfolded in time is equivalent to a feedforward network with shared weights between successive layers. This suggests that recurrent neural networks should suffer from similar optimization difficulties as multilayer feedforward networks. In particular, the permutation symmetry of hidden units still holds\nin unfolded recurrent networks; however, the rescaling symmetry does not, since the connectivities between different “layers” cannot be rescaled independently due to weight sharing across time.\nHow can the permutation symmetry of hidden units be broken in recurrent neural networks? In vanilla recurrent networks, the analog of adding identity skip connections between adjacent layers would be to change the equation describing the recurrent dynamics to (ignoring the biases for simplicity of notation):\nrt = rt−1 + f(Wrrt−1 + Wxxt) (27)\nThis model, and variations thereof, were proposed in [12] with performance comparisons suggesting that they perform competitively with state-of-the-art feedforward models in standard image recognition benchmarks."
    }, {
      "heading" : "3 Discussion",
      "text" : "In this paper, we proposed a novel explanation for the benefits of skip connections in terms of symmetry-breaking. We argued that skip connections between adjacent layers in a multilayer network help break the permutation symmetry between the hidden units and skip connections between a layer and all layers above it break the rescaling symmetry of connectivity matrices between different layers.\nWe found that dense orthogonal skip connectivity matrices perform slightly better than the typically used identity skip connections. This result can be understood from a symmetry-breaking perspective as identity skip connections use a single hidden unit from the previous layer to disambiguate the hidden units at the next layer, whereas a dense orthogonal matrix uses all the hidden units at the previous layer, and is more robust to the hidden units becoming silent (hence becoming less disambiguating) due to the ReLU nonlinearity. On the other hand, sparse orthogonal matrices such as the identity matrix are computationally more efficient than dense orthogonal matrices.\nOur results suggest that symmetry-breaking contributes at least partly to the success of skip connections. However, we emphasize that symmetry-breaking is not the sole explanation for the benefits of skip connections. We presented evidence suggesting that skip connections are also quite effective at dealing with the problem of vanishing/exploding gradients and not every form of symmetry-breaking can be expected to be equally effective at dealing with such additional problems that beset the training of deep networks.\nAs an intriguing observation, we finally note, from a symmetry-breaking viewpoint, a particular benefit of using differentiated classes of neurons with distinct connectivity patterns, a strategy seemingly employed by the brain [5], over using undifferentiated neurons, as is the common practice in artificial neural networks. Specifically, if one divides n undifferentiated neurons into distinct classes, one can reduce their permutation symmetry. For instance, for K classes with d neurons in each, the number of possible permutations is K!(d!)K which is smaller than the number of\npermutations of n = Kd (1 < K < n) undifferentiated neurons, which is n!. Symmetry is a remarkably rich and powerful idea in the physical sciences [19]. The results reported in this paper suggest that it could be useful for neural network researchers to pay closer attention to the symmetries inherent in their models as well. As a general design principle, we recommend reducing the symmetries in a model as much as possible, but without reducing the model’s expressive capacity at the same time."
    }, {
      "heading" : "Acknowledgments",
      "text" : "I would like to thank Xaq Pitkow for helpful discussions and the HPC facilities at NYU for making the experiments reported in this paper possible to run."
    } ],
    "references" : [ {
      "title" : "Singularities affect dynamics of learning in neuromanifolds",
      "author" : [ "S Amari", "H Park", "T Ozeki" ],
      "venue" : "Neural Comput",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Y Bengio", "P Simard", "P Frasconi" ],
      "venue" : "IEEE Trans Neural Netw",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1994
    }, {
      "title" : "Reducing overfitting in deep networks by decorrelating representations. arXiv:1511.06068",
      "author" : [ "M Cogswell", "F Ahmed", "R Girshick", "L Zitnick", "D Batra" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K He", "X Zhang", "S Ren", "J Sun" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Identity mappings in deep residual networks. arXiv:1603.05027",
      "author" : [ "K He", "X Zhang", "S Ren", "J Sun" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Untersuchungen zu dynamischen neuronalen Netzen",
      "author" : [ "S Hochreiter" ],
      "venue" : "Diploma thesis, Institut f. Informatik,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1991
    }, {
      "title" : "Densely connected convolutional networks. arXiv:1608.06993",
      "author" : [ "G Huang", "Z Liu", "KQ Weinberger", "L van der Maaten" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S Ioffe", "C Szegedy" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Bridging the gaps between residual learning, recurrent neural networks and visual cortex",
      "author" : [ "Q Liao", "T Poggio" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Learning deep parsimonious representations",
      "author" : [ "R Liao", "AG Schwing", "RS Zemel", "R Urtasun" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "The loss surface of residual networks: ensembles and the role of batch normalization",
      "author" : [ "E Littwin", "L Wolf" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Path-SGD: Path-normalized optimization in deep neural networks. arXiv:1506.02617",
      "author" : [ "B Neyshabur", "R Salakhutdinov", "N Srebro" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "On-line learning in soft committee machines",
      "author" : [ "D Saad", "SA Solla" ],
      "venue" : "Phys Rev E",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1995
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv:1312.6120",
      "author" : [ "AM Saxe", "JM McClelland", "S Ganguli" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Introduction of skip (or residual) connections has substantially improved the training of very deep neural networks [6,7,9].",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "Introduction of skip (or residual) connections has substantially improved the training of very deep neural networks [6,7,9].",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : "Introduction of skip (or residual) connections has substantially improved the training of very deep neural networks [6,7,9].",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "Despite informal intuitions and sometimes worryingly non-rigorous metaphors (“keeping a “clean” information path” [7], “to .",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 6,
      "context" : "improve the information flow between layers” [9]) put forward to motivate skip connections, a clear understanding of how these connections improve training has been lacking.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "A number of recent papers addressed different aspects of this question [4, 11, 14].",
      "startOffset" : 71,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "Symmetries lead to saddle structures in the landscape, causing problems for gradient-based optimization methods [1,17].",
      "startOffset" : 112,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "Symmetries lead to saddle structures in the landscape, causing problems for gradient-based optimization methods [1,17].",
      "startOffset" : 112,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : "We show that skip connections between adjacent layers break the permutation symmetry of nodes at a given layer, whereas the more recently introduced DenseNet architecture [9], where each layer projects skip connections to every",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 11,
      "context" : "A similar rescaling symmetry has been discussed before [15], where it has been noted that the incoming weights of any given node can be scaled up by a constant and its outgoing weights scaled down by the same constant without changing the function computed by the network and a modified stochastic gradient descent (SGD) algorithm has been proposed to effectively break this rescaling symmetry.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "Additional skip connections between each layer and all layers above it, as in the DenseNet architecture introduced in [9], break the rescaling symmetry of the weight matrices by adding distinct sets of skip vectors to each layer (Figure 2b).",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "This architecture is inspired by the DenseNet architecture introduced in [9].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "1 Three-layer networks Dynamics of learning in plain linear networks with no skip connections was analyzed in [18].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 13,
      "context" : "For a three-layer network (L = 3), the learning dynamics can be expressed by the following differential equations [18]:",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "Here a and b are n-dimensional column vectors (where n is the number of hidden units) connecting the hidden layer to the α-th input and output modes, respectively, of the input-output correlation matrix and sα is the corresponding singular value (see [18] for further details).",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 0,
      "context" : "Formally, these correspond to the singularities of the Fisher information matrix, or the Hessian [1].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "The effect of such saddle structures on the learning dynamics has previously been analyzed in shallow non-linear feedforward networks [1,17]: in particular, it has been shown that they significantly slow down learning.",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 12,
      "context" : "The effect of such saddle structures on the learning dynamics has previously been analyzed in shallow non-linear feedforward networks [1,17]: in particular, it has been shown that they significantly slow down learning.",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "In practice, the variables are initialized randomly and hence they eventually escape the vicinity of the slow manifolds, but the manifolds can exert their effect for a long time [1,17].",
      "startOffset" : 178,
      "endOffset" : 184
    }, {
      "referenceID" : 12,
      "context" : "In practice, the variables are initialized randomly and hence they eventually escape the vicinity of the slow manifolds, but the manifolds can exert their effect for a long time [1,17].",
      "startOffset" : 178,
      "endOffset" : 184
    }, {
      "referenceID" : 13,
      "context" : "The derivation proceeds essentially identically to the corresponding derivation for plain networks in [18].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "2 Networks with more than three-layers As shown in [18], in linear networks with more than a single hidden layer, assuming that there are orthogonal matrices Rl and Rl+1 for each layer l that diagonalize the initial weight matrix of the corresponding layer (i.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "R > l+1Wl(0)Rl = Dl is a diagonal matrix), dynamics of different singular modes decouple from each other and each mode α evolves according to gradient descent dynamics in an energy landscape described by [18]:",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 2,
      "context" : "More complicated joint priors over hidden unit responses that favor decorrelated [3] or clustered [13] responses have been proposed before.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "More complicated joint priors over hidden unit responses that favor decorrelated [3] or clustered [13] responses have been proposed before.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "One of those advantages is its ability to deal effectively with the vanishing gradients problem encountered in training deep networks [2, 8].",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 5,
      "context" : "One of those advantages is its ability to deal effectively with the vanishing gradients problem encountered in training deep networks [2, 8].",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 8,
      "context" : "How can the permutation symmetry of hidden units be broken in recurrent neural networks? In vanilla recurrent networks, the analog of adding identity skip connections between adjacent layers would be to change the equation describing the recurrent dynamics to (ignoring the biases for simplicity of notation): rt = rt−1 + f(Wrrt−1 + Wxxt) (27) This model, and variations thereof, were proposed in [12] with performance comparisons suggesting that they perform competitively with state-of-the-art feedforward models in standard image recognition benchmarks.",
      "startOffset" : 397,
      "endOffset" : 401
    } ],
    "year" : 2017,
    "abstractText" : "Skip connections made the training of very deep neural networks possible and have become an indispendable component in a variety of neural architectures. A satisfactory explanation for their success remains elusive. Here, we present an explanation for the benefits of skip connections in training very deep neural networks. We argue that skip connections help break symmetries inherent in the loss landscapes of deep networks, leading to drastically simplified landscapes. In particular, skip connections between adjacent layers in a multilayer network break the permutation symmetry of nodes in a given layer, and the recently proposed DenseNet architecture, where each layer projects skip connections to every layer above it, also breaks the rescaling symmetry of connectivity matrices between different layers. This hypothesis is supported by evidence from a toy model with binary weights and from experiments with fully-connected networks suggesting (i) that skip connections do not necessarily improve training unless they help break symmetries and (ii) that alternative ways of breaking the symmetries also lead to significant performance improvements in training deep networks, hence there is nothing special about skip connections in this respect. We find, however, that skip connections confer additional benefits over and above symmetry-breaking, such as the ability to deal effectively with the vanishing gradients problem.",
    "creator" : "LaTeX with hyperref package"
  }
}