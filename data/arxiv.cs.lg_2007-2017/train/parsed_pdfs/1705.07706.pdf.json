{
  "name" : "1705.07706.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Out-of-the-box Full-network Embedding for Convolutional Neural Networks",
    "authors" : [ "Dario Garcia-Gasulla", "Armand Vilalta" ],
    "emails" : [ "dario.garcia@bsc.es", "armand.vilalta@bsc.es", "ferran.pares@bsc.es" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep learning models, and particularly convolutional neural networks (CNN), have become the standard approach for tackling image processing tasks. The key to the success of these methods lies in the rich representations deep models build, which are generated after an exhaustive and computationally expensive learning process [1]. To generate deep representations, deep learning models have strong training requirements in terms of dataset size, computational power and optimal hyper-parametrization. For any domain or application in which either of those factors is an issue, training a deep model from scratch becomes unfeasible.\n∗Both authors contributed equally to this work\nar X\niv :1\n70 5.\n07 70\n6v 1\nWithin deep learning, the field of transfer learning studies how to extract and reuse pre-trained deep representations. This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9]. The first two cases, where training a deep network remains the end purpose of the transfer learning process, are commonly known as transfer learning for fine-tuning, while the third case, where the end purpose of the transfer learning does not necessarily include training a deep net, is typically referred as transfer learning for feature extraction.\nOf the three limiting factors of training deep networks (i.e., dataset size, computational cost, and optimal hyper-parametrization), transfer learning for fine-tuning partly solves the first. Indeed, one can successfully train a CNN on a dataset composed by roughly a few thousand instances using a pre-trained model as starting point, and achieve state-of-the-art-results. Unfortunately, fine-tuning a model still requires a minimum dataset size, a significant amount of computational resources, and lots of time to optimize the multiple hyper-parameters involved in the process.\nTransfer learning for feature extraction on the other hand is based on processing a set of data instances through a pre-trained neural network, extracting the activation values so these can be used by another learning mechanism. This is applicable to datasets of any size, as each data instance is processed independently. It has a relatively small computational cost, since there is no deep net training. And finally, it requires no hyper-parameter optimization, since the pre-trained model can be used out-ofthe-box. Significantly, the applications of transfer learning for feature extraction are limited only by the capabilities of the methods that one can execute on top of the generated deep representations.\nAs previously mentioned, designing and training a deep model to maximize classification performance is a time consuming task. In this paper we explore the opposite approach, minimizing the design and tuning effort using a feature extraction process. Our goal is to build an out-of-the-box classification tool (which could be used by anyone regardless of technical background) capable of defining a full-network embedding (integrating the representations built by all layers of a source CNN model). When compared to single-layer embeddings, this approach generates richer and more powerful embeddings, while also being more robust to the use of inappropriate pre-trained models. We asses the performance of such solution when compared with thoroughly designed and tuned models."
    }, {
      "heading" : "2 Related Work",
      "text" : "Transfer learning studies how to extract and reuse deep representations learnt for a given task t0, to solve a different task t1. Fine tuning approaches require the t1 target dataset to be composed by at least a few thousands instances, to avoid overfitting during the fine-tuning process. To mitigate this limitation, it has been proposed to reuse carefully selected parts of the t0 dataset in the fine-tuning process alongside the t1 (i.e., selective joint fine-tuning) [5], and also to use large amounts of noisy web imagery alongside with clean curated data [10]. In fine-tuning, choosing which layers of weights from the t0 model should be transferred, and which should be transferred and kept unchanged on the t1 training phase has a large impact on performance. Extensive research on that regard has shown that the optimal policy depends mostly on the properties of both t0 and t1 [7, 11, 12]. This dependency, together with the hyper-parameters inherent to deep network training, defines a large set of problem specific adaptations to be done by fine-tuning solutions.\nGiven a pre-trained model for t0 one may use alternative machine learning methods for solving t1, instead of fine-tuning a deep model. For that purpose, one needs to generate a representation of t1 data instances as perceived by the model trained for t0. This feature extraction process is done through a forward pass of t1 data instances on the pre-trained CNN model, which defines a data embedding that can be fed to the chosen machine learning method (e.g., a Support Vector Machine, or SVM, for classification). In most cases, the embedding is defined by capturing and storing the activation values of a single layer close to the output [7, 8, 9, 13, 14, 15]. The rest of layers (e.g., most convolutional layers) are discarded because these \"are unlikely to contain a richer semantic representation than the later feature\" [13]. So far, this choice has been supported by performance comparisons of single-layer embeddings, where high-level layer embeddings have been shown to consistently outperform low-level layer embeddings [7, 8]. However, it is also known that all layers within a deep network, including low-level ones, can contribute to the characterization of the data in different ways [16]. This implies that the richest and most versatile representation that can be\ngenerated by a feature extraction process must include all layers from the network, i.e., it must define a full-network embedding. However, no full-network embedding has been proposed in the literature so far, due to the difficulty of successfully integrating the features found on such an heterogeneous set of layers as the one defined by a full deep network architecture.\nBeyond the layers to extract, there are many other parameters that can affect the feature extraction process. Some of those are evaluated in [7], which includes parameters related with the architecture and training of the initial CNN (e.g., network depth and width, distribution of training data, optimization parameters), and parameters related with transfer learning process (e.g., fine-tuning, spatial pooling and dimensionality reduction). Among the most well established transformations of deep embeddings are a L2 normalization [7, 8], and an unsupervised feature reduction like Principal Component Analysis (PCA) [7, 8, 9]. The quality of the resultant embedding is typically evaluated by the performance of an SVM, trained using the embedding representations, to solve a classification task [7, 8]. Recently, extracted features have also been combined with more sophisticated computer vision techniques, such as the constellation model [6] and Fisher vectors [17], with significant success."
    }, {
      "heading" : "3 Full-network Embedding",
      "text" : "Transfer learning for feature extraction is used to embed a dataset t1 in the representation language learnt for a task t0. To do so, one must forward pass each data instance of t1 through the model pre-trained on t0, capturing the internal activations of the net. This is the first step of our method, but, unlike previous contributions to feature extraction, we store the activation values generated on every convolutional and fully connected layer of the model to generate a full-network embedding.\nEach filter within a convolutional layer generates several activations for a given input, as a result of convolving the filter. This corresponds to the presence of the filter at various locations of the input. In a resultant feature extracted embedding this implies a significant increase in dimensionality, which is in most cases counterproductive. At the same time, the several values generated by a given filter provide only relative spatial information, which may not be particularly relevant in a transfer learning setting (i.e., one where the problem for which the filters were learnt is not the same as the problem where the filter is applied). To tackle this issue, a recurrent solution in the field is to perform a spatial average pooling on each convolutional filter, such that a single value per filter is obtained by averaging all its spatially-depending activations [7, 8]. After this pooling operation, each feature in the embedding corresponds to the degree with which a convolutional filter is found on average in the whole image, regardless of location. While losing spatial information, this solution maintains most of the embedding descriptive power, as all convolutional filters remain represented. A spatial average pooling on the filters of convolutional layers is the second step of our method. The values resulting from the spatial pooling are concatenated with the features from the fully connected layers into a single vector, to generate a complete embedding. In the case of the well-known VGG16 architecture [18] this embedding vector is composed by 12,416 features.\nThe features composing the embedding vector so far described are obtained from neurons of different type (e.g., convolutional and fully connected layers) and location (i.e., from any layer depth). These differences account for large variations in the corresponding feature activations (e.g., distribution, magnitude, etc.). Since our method considers an heterogeneous set of features, a feature standard-\nization is needed. Our proposed standardization computes the z-values of each feature, using the train set to compute the mean and standard deviation. This process transforms each feature value so that it indicates how separated the value is from the feature mean in terms of positive/negative standard deviations. In other words, the degree with which the feature value is atypically high (if positive) or atypically low (if negative) in the context of the dataset. A similar type of feature normalization is frequently used in deep network training (i.e., batch normalization) [19], but this is the first time this technique has been applied to a feature extraction solution. As discussed in §2, most feature extraction approaches apply an L2 norm by data instance, thus normalizing by image instead of by feature. As seen in §5, this approach provides competitive results, but is not appropriate when using features coming from many different layers. By using the z-values per feature, we use activations across the dataset as reference for normalization. This balances each feature individually, which allows us to successfully integrate all types of features in the embedding. Significantly, this feature standardization process generates a context depending embedding, as the representation of each instance depends on the rest of instances being computed with it. Indeed, consider how the features relevant for characterizing a bird in a context of cars are different that the ones relevant for characterizing the same bird in a context of other birds. Such a subjective representation makes the approach more versatile, as it is inherently customized for each specific problem. After the feature standardization, the final step of the proposed pipeline is a feature discretization, which is described in §3.1. An end-to-end overview of the proposed embedding generation is shown in Figure 1."
    }, {
      "heading" : "3.1 Feature Discretization",
      "text" : "The embedding vectors we generate are composed by a large number of features. Exploring a representation space of such high-dimensionality is problematic for most machine learning algorithms, as it can lead to overfitting and other issues related with the curse of dimensionality. A common solution is to use dimensionality reduction techniques like PCA [14, 7]. We propose an alternative approach, which keeps the same number of features (and thus keeps the size of the representation language defined by the embedding) but reduces their expressiveness. In detail, we discretize each standardized feature value to represent either an atypically low value (-1), a typical value (0), or an atypically high value (1). This discretization is done by mapping feature values to the {−1, 0, 1} domain by defining two thresholds ft− and ft+.\nTo find consistent thresholds, we consider the work of Garcia-Gasulla et al. [16], who use a supervised statistical approach to evaluate the importance of CNN features for characterization. Given a feature f and a class c, this work uses an empirical statistic to measure the difference between activation values of f for instances of c and activation values of f for the rest of classes in the dataset. This allows them to quantify the relevance of feature/class pairs for class characterization. In their work, authors further separate these pairs in three different sets: characteristic by abscence, uncharacteristic and characteristic by presence. We use these three sets to find our thresholds ft− and ft+, by mapping the feature/class relevances of Garcia-Gasulla et al. [16] to our corresponding feature/image activations. We do so on the datasets explored in [16]: mit67, flowers102 and cub200, by computing the average values of the features belonging to each of the three sets. Figure 2 shows the three\nresulting distributions of values for the mit67 dataset. Clearly, a strong correlation exists between the supervised statistic feature relevance defined in [16] and the standardized feature values generated by the full-network embedding, as features in the characteristic by absence set correspond to activations which are particularly low, while features in the characteristic by presence set correspond to activations which are particularly high. We obtain the ft− and ft+ values through the Kolmogrov-Smirnov statistic, which provides the maximum gap between two empirical distributions. Vertical dashed lines of Figure 2 indicate these optimal thresholds for the mit67 dataset, the rest are shown in Table 1. To obtain a parameter free methodology, and considering the stable behavior of the ft+ and ft− thresholds, we chose to set ft+ = 0.15 and ft− = −0.25 in all our experiments. Thus, after the step of feature standardization, we discretize the values above 0.15 to 1, the values below −0.25 to −1, and the rest to 0."
    }, {
      "heading" : "4 Datasets",
      "text" : "One of the goals of this paper is to identify a full-network feature extraction methodology which provides competitive results out-of-the-box. For that purpose, we evaluate the embedding proposed in §3.1 on a set of 9 datasets which define different image classification challenges. The list includes datasets of classic object categorization, fine-grained categorization, and scene and textures classification. The disparate type of discriminative features needed to solve each of these problems represents a challenge for any approach which tries to solve them without specific tuning of any kind.\nThe MIT Indoor Scene Recognition dataset [20] (mit67) consists of different indoor scenes to be classified in 67 categories. Its main challenge resides in the class dependence on global spatial properties and on the relative presence of objects. The Caltech-UCSD Birds-200-2011 dataset [21] (cub200) is a fine-grained dataset containing images of 200 different species of birds. The Oxford Flower dataset [22] (flowers102) is a fine-grained dataset consisting of 102 flower categories. The dataset contains only 20 samples per class for training. The Oxford-IIIT-Pet dataset [23] (cats-dogs) is a fine-grained dataset covering 37 different breeds of cats and dogs. The Stanford Dogs dataset [24] (sdogs) contains images from the 120 breeds of dogs found in ImageNet. The dataset is complicated by little inter-class variation, and large intra-class and background variation. The Caltech 101 dataset [25] (caltech101) is a classical dataset of 101 object categories containing clean images with low level of occlusion. The Food-101 dataset [26] (food101) is a large dataset of 101 food categories. Test labels are reliable but train images are noisy (e.g., occasionally mislabeled). The Describable Textures Dataset [27] (textures) is a database of textures categorized according to a list of 47 terms inspired from human perception. The Oulu Knots dataset [28] (wood) contains knot images from spruce wood, classified according to Nordic Standards. This dataset of industrial application is considered to be challenging even for human experts.\nDetails for these datasets are provided in Table 2. This includes the train/test splits used in our experiments. In most cases we follow the train/test splits as provided by the dataset authors in order to obtain comparable results. A specific case is caltech101 where, following the dataset authors instructions [25], we randomly choose 30 training examples per class and a maximum of 50 for test, and repeat this experiment 5 times. The other particular case is the food101 dataset. Due to its large\nsize, we use only the provided test set for both training and testing, using a stratified 5-fold cross validation. The same stratified 5-fold cross validation approach is used for the wood dataset, where no split is provided by the authors."
    }, {
      "heading" : "5 Classification Experiments",
      "text" : "In this section we analyze the performance gap between thoroughly tuned models (those which currently provide state-of-the-art results) and the approach described in §3. To evaluate the consistency of our method out-of-the-box, we decide not to use additional data when available on the dataset (e.g., image segmentation, regions of interest or other metadata), or to perform any other type of problem specific adaptation (e.g., tuning hyper-parameters).\nAs source model for the feature extraction process we use the classical VGG16 CNN architecture [18] pre-trained on the Places2 scene recognition dataset [29] for the mit67 experiments, and the same VGG16 architecture pre-trained on the ImageNet 2012 classification dataset [30] for the rest (these define our t0 tasks). As a result the proposed embedding is composed by 12,416. On top of that, we use a linear SVM with the default hyperparameter C = 1 for classification, with a one-vs-the-rest strategy. Standard data augmentation is used in the SVM training, using 5 crops per sample (4 corners + central) with horizontal mirroring (total of 10 crops per sample). At test time, all the 10 crops are classified, using a voting strategy to decide the label of each data sample.\nBeyond the comparison with the current state-of-the-art, we also compare our approach with the most frequently used feature extraction solution. As discussed in §2, a popular embedding is obtained by extracting the activations of one of the fully connected layers (fc6 or fc7 for the VGG16 model) and applying a L2 normalization per data instance [7, 8, 13]. We call this our baseline method, an overview of it is shown in Figure 3. The same pre-trained model used as source for the fullnetwork embedding is used for the baseline. For both baselines (fc6 and fc7), the final embedding is composed by 4,096 features. This is used to train the same type of SVM classifier trained with the full-network embedding."
    }, {
      "heading" : "5.1 Classification Results",
      "text" : "The results of our classification experiments are shown in Table 3. Performance is measured with average per-class classification accuracy. For each dataset we provide the accuracy provided by the baselines, by our method, and by the best method we found in the literature (i.e., the state-of-the-art or SotA). For a proper interpretation of the performance gap between the SotA methods and ours, we further indicate if the SotA uses external data (beyond the t1 dataset and the t0 model) and if it performs fine-tuning.\nOverall, our method outperforms the best baseline (fc6) by 2.2% accuracy on average. This indicates that the proposed full-network embedding successfully integrates the representations generated at the various layers. The datasets where the baseline performs similarly or slightly outperforms the full-network embedding (cub200, cats-dogs and sdogs) are those where the target task t1 overlaps with the source task t0 (e.g., ImageNet 2012). The largest difference happens for the sdogs, which is explicitly a subset of ImageNet 2012. In this sort of easy transfer learning problems, the fully connected layer used by the baseline methods has been partly optimized to solve the t1 problem during the original CNN training phase. This explains why the baselines based on the fully connected layers are particularly competitive on these datasets.\nState-of-the-art performance is in most cases a few accuracy points above the performance of the full-network embedding (7.8% accuracy on average). These results are encouraging, considering that our method uses no additional data, requires no tuning of parameters and it is computationally cheap (e.g., it does not require deep network training). The dataset where our full-network embedding is more clearly outperformed is the cub200. In this dataset Krause et al. [10] achieve a remarkable state-of-the-art performance by using lots of additional data (roughly 5 million additional images of birds) to train a deep network from scratch, and then fine-tune the model using the cub200 dataset. In this case, the large gap in performance is caused by the huge disparity in the amount of training data used. A similar context happens in the evaluation of food101, where Liu et al. [4] use the complete training set for fine-tuning, while we only use a subset of the test set (see §4 for details). If we consider the results for the other 6 datasets, the average performance gap between the state-of-the-art and the full-network embedding is 4.2% accuracy on average.\nAmong the methods which achieve the best performance on at least one dataset, there is one which is not based on fine tuning. The work of [17] obtains the best results for the textures dataset by using a combination of bag-of-visual-words, Fisher vectors and convolutional filters. Authors demonstrate how this approach is particularly competitive on texture based datasets. Our more generic methodology is capable of obtaining an accuracy 2.5% accuracy lower in this highly specific domain.\nThe wood dataset is designed to be particularly challenging, even for human experts; according to the dataset authors the global accuracy of an experienced human sorter is about 75-85% [32, 28]. There is currently no reported results in average per-class accuracy for this dataset, so the corresponding values in Table 3 are left blank. Consequently, the results we report represent the current state-of-the-art to the best of our knowledge (74.1%±6.9 in average per-class accuracy). The best results previously reported in the literature for wood correspond to Ren et al. [15], which are 94.3% in global accuracy. However, the difference between average per-class accuracy and global accuracy is particularly relevant in this dataset, given the variance in images per class (from 3 to 37). To evaluate the average per-class accuracy, we tried our best to replicate the method of Ren et al. [15], which resulted in 71.0%±8.2 average per-class accuracy when doing a stratified 5-fold cross validation. A performance similar to that of our baseline method."
    }, {
      "heading" : "5.2 Study of Variants",
      "text" : "In this section we consider removing and altering some of the components of the full-network embedding to understand their impact. First we remove feature discretization, and evaluate the embeddings obtained after the feature standardization step (FS). Secondly, we consider a partial feature discretization which only maps values between ft+ and ft− to zero, and evaluate an embedding which keeps the rest of the original values ({−v, 0, v}). The purpose of this second experiment is to study if the increase in performance provided by the feature discretization is caused by the noise reduction effect of mapping frequent values to 0, or if it is caused by the space simplification resultant of mapping all\nactivations to only three values. Only in these particular experiments, performance is measured in global accuracy.\nAs shown in Table 4, the full-network embedding outperforms all the other variants, with the only exception of flowers102 where FS is slightly more competitive (0.6% accuracy). The noise reduction variant (i.e., {−v, 0, v}) outperforms the FS variant in 6 out of 8 datasets. The main difference between both is that the former sparsifies the embeddings by transforming typical values to zeros, with few informative data being lost in the process. The complete feature discretization done by the full-network model (i.e., {−1, 0, 1}) further boosts performance, outperforming the {−v, 0, v} embedding on 8 of 9 datasets. This shows the potential benefit of reducing the complexity of the embedding space.\nThe feature discretization also has the desirable effect of reducing the training cost of the SVM applied on the resulting embedding. Using the FS embedding as control (the slowest of all variants), the {−v, 0, v} embedding trains the SVM between 3 and 30 times faster depending on the dataset, while the full-network embedding with its complete discretization trains between 20 and 100 times faster. Significantly, all three embeddings are composed by 12,416 features. For comparison, the baseline method, which uses shorter embeddings of 4,096 features, trains the SVM between 300 and 700 times faster than the FS. For both the baseline and the full-network embeddings, training the SVM takes a few minutes on a single CPU.\nA different variation we consider is to an inappropriate task t0 as source for generating the baseline and full-network embeddings. This tests the robustness of each embedding when using an illsuited pre-trained model. We use the model pre-trained on ImageNet 2012 for generating the mit67 embeddings, and the model pre-trained on Places2 for the rest of datasets. Table 5 shows that the full-network embedding is much more robust, with an average reduction in accuracy of 13.6%, against 21.2% of the baseline. Finally, we also considered using different network depths, a parameter also analyzed in [7]. We repeated the full-network experiments using the VGG19 architecture instead of the VGG16, and found performance differences to be minimal (maximum difference of 0.3%) and inconsistent."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper we describe a feature extraction process which leverages the information encoded in all the features of a deep CNN. The full-network embedding introduces the use of feature standardization and of a novel feature discretization methodology. The former provides context-dependant embeddings, which adapt the representations to the problem at hand. The later reduces noise and regularizes the embedding space while keeping the size of the original representation language\n(i.e., the pre-trained model used as source). Significantly, the feature discretization restricts the computational overhead resultant of processing much larger embeddings when training an SVM. Our experiments also show that the full-network is more robust that single-layer embeddings when an appropriate source model is not available.\nThe resultant full-network embedding is shown to outperform single-layer embeddings in several classification tasks, and to provide the best reported results on one of those tasks (wood). Within the state-of-the-art, the full-network embedding represents the best available solution when one of the following conditions apply: When the accessible data is scarce, or an appropriate pre-trained model is not available (e.g., specialized industrial applications), when computational resources are limited (e.g., no GPUs availability), or when development time or technical expertise is restricted or non cost-effective."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is partially supported by the Joint Study Agreement no. W156463 under the IBM/BSC Deep Learning Center agreement, by the Spanish Government through Programa Severo Ochoa (SEV-2015-0493), by the Spanish Ministry of Science and Technology through TIN2015-65316-P project and by the Generalitat de Catalunya (contracts 2014-SGR-1051), and by the Core Research for Evolutional Science and Technology (CREST) program of Japan Science and Technology Agency (JST)."
    } ],
    "references" : [ {
      "title" : "Augmenting strong supervision using web data for fine-grained categorization",
      "author" : [ "Zhe Xu", "Shaoli Huang", "Ya Zhang", "Dacheng Tao" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Bird species categorization using pose normalized deep convolutional nets",
      "author" : [ "Steve Branson", "Grant Van Horn", "Serge Belongie", "Pietro Perona" ],
      "venue" : "arXiv preprint arXiv:1406.2952,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Deepfood: Deep learning-based food image recognition for computer-aided dietary assessment",
      "author" : [ "Chang Liu", "Yu Cao", "Yan Luo", "Guanling Chen", "Vinod Vokkarane", "Yunsheng Ma" ],
      "venue" : "In International Conference on Smart Homes and Health Telematics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning",
      "author" : [ "Weifeng Ge", "Yizhou Yu" ],
      "venue" : "arXiv preprint arXiv:1702.08690,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2017
    }, {
      "title" : "Neural activation constellations: Unsupervised part model discovery with convolutional networks",
      "author" : [ "Marcel Simon", "Erik Rodner" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Factors of transferability for a generic convnet representation",
      "author" : [ "Hossein Azizpour", "Ali Sharif Razavian", "Josephine Sullivan", "Atsuto Maki", "Stefan Carlsson" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Cnn features off-the-shelf: an astounding baseline for recognition",
      "author" : [ "Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Multi-scale orderless pooling of deep convolutional activation features",
      "author" : [ "Yunchao Gong", "Liwei Wang", "Ruiqi Guo", "Svetlana Lazebnik" ],
      "venue" : "In European conference on computer vision,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "The unreasonable effectiveness of noisy data for finegrained recognition",
      "author" : [ "Jonathan Krause", "Benjamin Sapp", "Andrew Howard", "Howard Zhou", "Alexander Toshev", "Tom Duerig", "James Philbin", "Li Fei-Fei" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "How transferable are features in deep neural networks",
      "author" : [ "Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Learning transferable features with deep adaptation networks",
      "author" : [ "Mingsheng Long", "Yue Cao", "Jianmin Wang", "Michael I Jordan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Decaf: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell" ],
      "venue" : "In Icml,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Deep convolutional features for image based retrieval and scene categorization",
      "author" : [ "Arsalan Mousavian", "Jana Kosecka" ],
      "venue" : "arXiv preprint arXiv:1509.06033,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "A generic deep-learning-based approach for automated surface inspection",
      "author" : [ "Ruoxu Ren", "Terence Hung", "Kay Chen Tan" ],
      "venue" : "IEEE Transactions on Cybernetics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2017
    }, {
      "title" : "On the behavior of convolutional nets for feature extraction",
      "author" : [ "Dario Garcia-Gasulla", "Ferran Parés", "Armand Vilalta", "Jonatan Moreno", "Eduard Ayguadé", "Jesús Labarta", "Ulises Cortés", "Toyotaro Suzumura" ],
      "venue" : "arXiv preprint arXiv:1703.01127,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2017
    }, {
      "title" : "Deep filter banks for texture recognition and segmentation",
      "author" : [ "Mircea Cimpoi", "Subhransu Maji", "Andrea Vedaldi" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Recognizing indoor scenes",
      "author" : [ "Ariadna Quattoni", "Antonio Torralba" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "Automated flower classification over a large number of classes",
      "author" : [ "Maria-Elena Nilsback", "Andrew Zisserman" ],
      "venue" : "In Computer Vision, Graphics & Image Processing,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "Cats and dogs",
      "author" : [ "Omkar M Parkhi", "Andrea Vedaldi", "Andrew Zisserman", "CV Jawahar" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Novel dataset for fine-grained image categorization: Stanford dogs",
      "author" : [ "Aditya Khosla", "Nityananda Jayadevaprakash", "Bangpeng Yao", "Fei-Fei Li" ],
      "venue" : "In Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories",
      "author" : [ "Li Fei-Fei", "Rob Fergus", "Pietro Perona" ],
      "venue" : "Computer vision and Image understanding,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2007
    }, {
      "title" : "Food-101–mining discriminative components with random forests",
      "author" : [ "Lukas Bossard", "Matthieu Guillaumin", "Luc Van Gool" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Describing textures in the wild",
      "author" : [ "Mircea Cimpoi", "Subhransu Maji", "Iasonas Kokkinos", "Sammy Mohamed", "Andrea Vedaldi" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    }, {
      "title" : "Wood inspection with non-supervised clustering",
      "author" : [ "Olli Silvén", "Matti Niskanen", "Hannu Kauppinen" ],
      "venue" : "Machine Vision and Applications,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2003
    }, {
      "title" : "Places: An image database for deep scene understanding",
      "author" : [ "Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Antonio Torralba", "Aude Oliva" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Spatial pyramid pooling in deep convolutional networks for visual recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    }, {
      "title" : "Wood defect recognition with self-organizing feature selection. In Photonics for Industrial Applications, pages 385–395",
      "author" : [ "Jouko Lampinen", "Seppo Smolander" ],
      "venue" : "International Society for Optics and Photonics,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1994
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].",
      "startOffset" : 135,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].",
      "startOffset" : 135,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].",
      "startOffset" : 135,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].",
      "startOffset" : 219,
      "endOffset" : 225
    }, {
      "referenceID" : 4,
      "context" : "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].",
      "startOffset" : 219,
      "endOffset" : 225
    }, {
      "referenceID" : 5,
      "context" : "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].",
      "startOffset" : 308,
      "endOffset" : 317
    }, {
      "referenceID" : 6,
      "context" : "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].",
      "startOffset" : 308,
      "endOffset" : 317
    }, {
      "referenceID" : 7,
      "context" : "This approach has three main applications: improving the performance of a network by initializing its training from a non-random state [2, 3, 4], enabling the training of deep networks for tasks of limited dataset size [5, 6], and exploiting deep representations through alternative machine learning methods [7, 8, 9].",
      "startOffset" : 308,
      "endOffset" : 317
    }, {
      "referenceID" : 3,
      "context" : ", selective joint fine-tuning) [5], and also to use large amounts of noisy web imagery alongside with clean curated data [10].",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : ", selective joint fine-tuning) [5], and also to use large amounts of noisy web imagery alongside with clean curated data [10].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "Extensive research on that regard has shown that the optimal policy depends mostly on the properties of both t0 and t1 [7, 11, 12].",
      "startOffset" : 119,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "Extensive research on that regard has shown that the optimal policy depends mostly on the properties of both t0 and t1 [7, 11, 12].",
      "startOffset" : 119,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : "Extensive research on that regard has shown that the optimal policy depends mostly on the properties of both t0 and t1 [7, 11, 12].",
      "startOffset" : 119,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "In most cases, the embedding is defined by capturing and storing the activation values of a single layer close to the output [7, 8, 9, 13, 14, 15].",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "In most cases, the embedding is defined by capturing and storing the activation values of a single layer close to the output [7, 8, 9, 13, 14, 15].",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : "In most cases, the embedding is defined by capturing and storing the activation values of a single layer close to the output [7, 8, 9, 13, 14, 15].",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 11,
      "context" : "In most cases, the embedding is defined by capturing and storing the activation values of a single layer close to the output [7, 8, 9, 13, 14, 15].",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 12,
      "context" : "In most cases, the embedding is defined by capturing and storing the activation values of a single layer close to the output [7, 8, 9, 13, 14, 15].",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : "In most cases, the embedding is defined by capturing and storing the activation values of a single layer close to the output [7, 8, 9, 13, 14, 15].",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 11,
      "context" : ", most convolutional layers) are discarded because these \"are unlikely to contain a richer semantic representation than the later feature\" [13].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "So far, this choice has been supported by performance comparisons of single-layer embeddings, where high-level layer embeddings have been shown to consistently outperform low-level layer embeddings [7, 8].",
      "startOffset" : 198,
      "endOffset" : 204
    }, {
      "referenceID" : 6,
      "context" : "So far, this choice has been supported by performance comparisons of single-layer embeddings, where high-level layer embeddings have been shown to consistently outperform low-level layer embeddings [7, 8].",
      "startOffset" : 198,
      "endOffset" : 204
    }, {
      "referenceID" : 14,
      "context" : "However, it is also known that all layers within a deep network, including low-level ones, can contribute to the characterization of the data in different ways [16].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "Some of those are evaluated in [7], which includes parameters related with the architecture and training of the initial CNN (e.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "Among the most well established transformations of deep embeddings are a L2 normalization [7, 8], and an unsupervised feature reduction like Principal Component Analysis (PCA) [7, 8, 9].",
      "startOffset" : 90,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "Among the most well established transformations of deep embeddings are a L2 normalization [7, 8], and an unsupervised feature reduction like Principal Component Analysis (PCA) [7, 8, 9].",
      "startOffset" : 90,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "Among the most well established transformations of deep embeddings are a L2 normalization [7, 8], and an unsupervised feature reduction like Principal Component Analysis (PCA) [7, 8, 9].",
      "startOffset" : 176,
      "endOffset" : 185
    }, {
      "referenceID" : 6,
      "context" : "Among the most well established transformations of deep embeddings are a L2 normalization [7, 8], and an unsupervised feature reduction like Principal Component Analysis (PCA) [7, 8, 9].",
      "startOffset" : 176,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "Among the most well established transformations of deep embeddings are a L2 normalization [7, 8], and an unsupervised feature reduction like Principal Component Analysis (PCA) [7, 8, 9].",
      "startOffset" : 176,
      "endOffset" : 185
    }, {
      "referenceID" : 5,
      "context" : "The quality of the resultant embedding is typically evaluated by the performance of an SVM, trained using the embedding representations, to solve a classification task [7, 8].",
      "startOffset" : 168,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "The quality of the resultant embedding is typically evaluated by the performance of an SVM, trained using the embedding representations, to solve a classification task [7, 8].",
      "startOffset" : 168,
      "endOffset" : 174
    }, {
      "referenceID" : 4,
      "context" : "Recently, extracted features have also been combined with more sophisticated computer vision techniques, such as the constellation model [6] and Fisher vectors [17], with significant success.",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 15,
      "context" : "Recently, extracted features have also been combined with more sophisticated computer vision techniques, such as the constellation model [6] and Fisher vectors [17], with significant success.",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "To tackle this issue, a recurrent solution in the field is to perform a spatial average pooling on each convolutional filter, such that a single value per filter is obtained by averaging all its spatially-depending activations [7, 8].",
      "startOffset" : 227,
      "endOffset" : 233
    }, {
      "referenceID" : 6,
      "context" : "To tackle this issue, a recurrent solution in the field is to perform a spatial average pooling on each convolutional filter, such that a single value per filter is obtained by averaging all its spatially-depending activations [7, 8].",
      "startOffset" : 227,
      "endOffset" : 233
    }, {
      "referenceID" : 16,
      "context" : "In the case of the well-known VGG16 architecture [18] this embedding vector is composed by 12,416 features.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "Figure 2: For the mit67 dataset, distribution of average standardized feature values for those features belonging to the sets identified in [16].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 17,
      "context" : ", batch normalization) [19], but this is the first time this technique has been applied to a feature extraction solution.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 12,
      "context" : "A common solution is to use dimensionality reduction techniques like PCA [14, 7].",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "A common solution is to use dimensionality reduction techniques like PCA [14, 7].",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "[16], who use a supervised statistical approach to evaluate the importance of CNN features for characterization.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[16] to our corresponding feature/image activations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "We do so on the datasets explored in [16]: mit67, flowers102 and cub200, by computing the average values of the features belonging to each of the three sets.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : "Clearly, a strong correlation exists between the supervised statistic feature relevance defined in [16] and the standardized feature values generated by the full-network embedding, as features in the characteristic by absence set correspond to activations which are particularly low, while features in the characteristic by presence set correspond to activations which are particularly high.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 18,
      "context" : "The MIT Indoor Scene Recognition dataset [20] (mit67) consists of different indoor scenes to be classified in 67 categories.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 19,
      "context" : "The Oxford Flower dataset [22] (flowers102) is a fine-grained dataset consisting of 102 flower categories.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 20,
      "context" : "The Oxford-IIIT-Pet dataset [23] (cats-dogs) is a fine-grained dataset covering 37 different breeds of cats and dogs.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 21,
      "context" : "The Stanford Dogs dataset [24] (sdogs) contains images from the 120 breeds of dogs found in ImageNet.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 22,
      "context" : "The Caltech 101 dataset [25] (caltech101) is a classical dataset of 101 object categories containing clean images with low level of occlusion.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 23,
      "context" : "The Food-101 dataset [26] (food101) is a large dataset of 101 food categories.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 24,
      "context" : "The Describable Textures Dataset [27] (textures) is a database of textures categorized according to a list of 47 terms inspired from human perception.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 25,
      "context" : "The Oulu Knots dataset [28] (wood) contains knot images from spruce wood, classified according to Nordic Standards.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 22,
      "context" : "A specific case is caltech101 where, following the dataset authors instructions [25], we randomly choose 30 training examples per class and a maximum of 50 for test, and repeat this experiment 5 times.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "As source model for the feature extraction process we use the classical VGG16 CNN architecture [18] pre-trained on the Places2 scene recognition dataset [29] for the mit67 experiments, and the same VGG16 architecture pre-trained on the ImageNet 2012 classification dataset [30] for the rest (these define our t0 tasks).",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : "As source model for the feature extraction process we use the classical VGG16 CNN architecture [18] pre-trained on the Places2 scene recognition dataset [29] for the mit67 experiments, and the same VGG16 architecture pre-trained on the ImageNet 2012 classification dataset [30] for the rest (these define our t0 tasks).",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 27,
      "context" : "As source model for the feature extraction process we use the classical VGG16 CNN architecture [18] pre-trained on the Places2 scene recognition dataset [29] for the mit67 experiments, and the same VGG16 architecture pre-trained on the ImageNet 2012 classification dataset [30] for the rest (these define our t0 tasks).",
      "startOffset" : 273,
      "endOffset" : 277
    }, {
      "referenceID" : 5,
      "context" : "As discussed in §2, a popular embedding is obtained by extracting the activations of one of the fully connected layers (fc6 or fc7 for the VGG16 model) and applying a L2 normalization per data instance [7, 8, 13].",
      "startOffset" : 202,
      "endOffset" : 212
    }, {
      "referenceID" : 6,
      "context" : "As discussed in §2, a popular embedding is obtained by extracting the activations of one of the fully connected layers (fc6 or fc7 for the VGG16 model) and applying a L2 normalization per data instance [7, 8, 13].",
      "startOffset" : 202,
      "endOffset" : 212
    }, {
      "referenceID" : 11,
      "context" : "As discussed in §2, a popular embedding is obtained by extracting the activations of one of the fully connected layers (fc6 or fc7 for the VGG16 model) and applying a L2 normalization per data instance [7, 8, 13].",
      "startOffset" : 202,
      "endOffset" : 212
    }, {
      "referenceID" : 3,
      "context" : "5 [5] [10] [5] [6] [5] [31] [4] [17] -",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 8,
      "context" : "5 [5] [10] [5] [6] [5] [31] [4] [17] -",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 3,
      "context" : "5 [5] [10] [5] [6] [5] [31] [4] [17] -",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "5 [5] [10] [5] [6] [5] [31] [4] [17] -",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "5 [5] [10] [5] [6] [5] [31] [4] [17] -",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 28,
      "context" : "5 [5] [10] [5] [6] [5] [31] [4] [17] -",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "5 [5] [10] [5] [6] [5] [31] [4] [17] -",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "5 [5] [10] [5] [6] [5] [31] [4] [17] -",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : "[10] achieve a remarkable state-of-the-art performance by using lots of additional data (roughly 5 million additional images of birds) to train a deep network from scratch, and then fine-tune the model using the cub200 dataset.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "[4] use the complete training set for fine-tuning, while we only use a subset of the test set (see §4 for details).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "The work of [17] obtains the best results for the textures dataset by using a combination of bag-of-visual-words, Fisher vectors and convolutional filters.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 29,
      "context" : "The wood dataset is designed to be particularly challenging, even for human experts; according to the dataset authors the global accuracy of an experienced human sorter is about 75-85% [32, 28].",
      "startOffset" : 185,
      "endOffset" : 193
    }, {
      "referenceID" : 25,
      "context" : "The wood dataset is designed to be particularly challenging, even for human experts; according to the dataset authors the global accuracy of an experienced human sorter is about 75-85% [32, 28].",
      "startOffset" : 185,
      "endOffset" : 193
    }, {
      "referenceID" : 13,
      "context" : "[15], which are 94.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[15], which resulted in 71.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "Finally, we also considered using different network depths, a parameter also analyzed in [7].",
      "startOffset" : 89,
      "endOffset" : 92
    } ],
    "year" : 2017,
    "abstractText" : "Transfer learning for feature extraction can be used to exploit deep representations in contexts where there is very few training data, where there are limited computational resources, or when tuning the hyper-parameters needed for training is not an option. While previous contributions to feature extraction propose embeddings based on a single layer of the network, in this paper we propose a full-network embedding which successfully integrates convolutional and fully connected features, coming from all layers of a deep convolutional neural network. To do so, the embedding normalizes features in the context of the problem, and discretizes their values to reduce noise and regularize the embedding space. Significantly, this also reduces the computational cost of processing the resultant representations. The proposed method is shown to outperform single layer embeddings on several image classification tasks, while also being more robust to the choice of the pre-trained model used for obtaining the initial features. The performance gap in classification accuracy between thoroughly tuned solutions and the full-network embedding is also reduced, which makes of the proposed approach a competitive solution for a large set of applications.",
    "creator" : "LaTeX with hyperref package"
  }
}