{
  "name" : "1503.07795.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-Labeled Classification of Demographic Attributes of Patients: a case study of diabetics patients",
    "authors" : [ "Naveen Kumar", "Parachur Cotha", "Marina Sokolova" ],
    "emails" : [ "npara051@uottawa.ca", "sokolova@uottawa.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "model is based on different race and gender groups. The resulting model can be further integrated into Privacy-Preserving Data Mining, where they can be used to assess risk of identification of different patient groups.\nOur project considers relations between diabetes and demographics of patients as a multi-labelled problem. Most research in this area has been done as binary classification, where the target class is finding if a person has diabetes or not. But very few, and maybe no work has been done in multi-labeled analysis of the demographics of patients who are likely to be diagnosed with diabetes. To identify such groups, we applied ensembles of several multilabel learning algorithms. The best performing multi label ensembles include BR/Hoeffding Tree, CC/Hoeffding Tree, BCC/Hoeffding Tree, BR/JRIP, CC/JRIP, BCC/ JRIP respectively. In the empirical part of this study, we used on the UCI Diabetics dataset of over 100,000 records, collected from 130 US hospitals. The dataset consisted of attributes that included personal demographics, diagnoses code, lab results, etc. Experiments conducted on datasets of 1000, 10000, 20000 samples, show that BR/JRip model achieves a high overall accuracy of 0.533 (1000 samples), 0.702 (10000 samples), 0.569 (20000 samples), improving over the baseline model ZeroR with accuracy of 0.526, 0.586, .562 respectively. Loss functions such as Rank Loss, One Error, Hamming Loss, and Zero One Loss are also low for BCC/JRIP model for all samples of dataset, making it the best candidate for better performance given the label dependencies.\n1. Introduction: Machine Learning is a part of data mining which deals with automatically building models on large dataset from which interesting patterns can be learnt. Traditional single label classification is concerned with learning from a set of examples that are associated with single label λ from a set of disjoint labels L, |L|>1. In multilabel classification, the examples are associated with a set of labels Y ⊆ L (Trohidis, Tsoumakas, Kalliris, & Vlahavas, 2008). The problem of learning from multi label data has recently attracted significant attention, motivated from an increasing amount of new applications such as semantic annotation of images (Boutell, Luo, Shen, & Brown, 2004) (Zhang & Zhou, 2007) (Yang, Kim, & Ro, 2007) and video (Qi, et al., 2007) (Snoek, Worring, Van Germet, Geusebroek, & Smeulders, 2006), functional genomics (Clare & King, 2001) (Elisseeff & Weston, 2002) (Blockeel, Schietgat, Struyf, Dz?eroski, & Clare, 2006) (Cesa-Bianchi, Gentile, & Zaniboni, 2006) (Barutcuoglu, Schapire, & Troyanskaya, 2006), music categorizations into emotions (Li & Ogihara, Detecting emotion in music, 2003) (Li & Ogihara, Toward intelligent music information retrieval, 2006) (Wieczorkowska, Synak, & Ras, 2006) (Trohidis, Tsoumakas, Kalliris, & Vlahavas, 2008) and directed marking (Zhang, Burer, & Street, 2006).\nJafer et al., (Jafer, Matwin, & Sokolova, 2014) talk about various types of attributes related to\npersonal privacy. The four types discussed are (i) Explicit Identifier, (ii) Quasi Identifier, (iii) Sensitive Identifier, (iv) Non–sensitive identifier. Explicit identifiers are the set of attributes from which the individuals can be explicitly identified. Explicit identifier includes SIN, Name, Insurance ID, which are directly linked to the individual. Quasi Identifiers (QI) refer to a set of attributes that when “combined”, could be linked to external datasets and potentially breach the privacy. Race, gender, age are commonly present among QIs. Sensitive attributes includes salary, disease and so on, which corresponds to person-specific private information. Finally, remaining attributes that do not fall into any of the above categories are grouped as non-sensitive attributes.\nA large growth in the availability and need of dataset containing personal information had\nincreased the importance of protecting privacy of individuals. Datasets which originally contain personal health information can provide evident data to identify the person and their condition. To prevent and control identification of individuals, the dataset has to go through the process of de-identification, where the features relating to a person’s personal and social part is removed without compromising on accuracy of the dataset. With respect to analyzing and learning demographics from data, to the best of our knowledge, there is no empirical work that uses multi-label classification to estimate the risk of patient identification.\nOur experiments were conducted on newly released dataset collected from 130 US hospitals for\nyears 1999-2008 1 . The dataset is introduced in (Strack, et al., 2014). The initial dataset consisted of 101,766 records. The dataset presents more than 50 features in medical setup representing patient demographic features such as race, gender, age, weight etc., and hospital features such as medical specialty, lab test results, diagnosis code. In the field of the diabetes research, most of the data mining work deals with finding if a person has diabetes or not; lesser amount of work has been done in reverse engineering, i.e. to identify the identity of persons who are likely to be diagnosed with diabetes. Straight forward problem of identifying patients with diabetes can be seen as a supervised binary classification, with a data record having one target label with 2 category values (Cortez & Morais, 2007) (Garcia, Lee, Woodard, & Titus, 1996) (Arrue, Ollero, & Matinez de Dios, 2000) (Dzeroski, Kobler, Gjorgioski, & Panov, 2006), whereas the reverse engineered problem of simultaneous learning of the demographic attributes (race, age, gender) belongs to multilabel classification. In multilabel classification, a data record has k, k >1, target labels. Each label can have different number of categorical or numerical values. For example, categorizing the patient to a list of infected allergies, as an individual may be allergic to more than one different allergy at a given time.\nThe reminder of the report is organized as follows. Section 2 presents multilabel classification\nproblems. Section 3 presents performance evaluation metrics. Section 4 introduces the dataset. Section 5 presents the experimental setup and the empirical results. Conclusions and future work are drawn in Section 6.\n2. Multi-Labeled Classification\nMultilabel classification task has been studied in traditional database mining scenarios, where the problem is split into two steps where each label is learnt individually and then merged later. Most of the multilabel algorithms can be seen as an ensemble of binary label algorithms. Besides the concept of multilabel classification, multilabel learning introduces the concept of multilabel ranking (Brinker, Urnkranz, & Ullermeier, 2006). Multilabel ranking can be seen as a generalization of multiclass classification, where instead of predicting only a single label, it predicts the ranking of all labels.\n1 http://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008#\nFormally, let C be the set of instances and C (c1, c2, c3…cn, xi), where c1, c2, c3…cn represent the\nattributes and xi represents the target class label, for a given instance ci in C. In single label binary classification task xi = 2, takes only two values, ci will belong to only one value of xi. In single label multi class classification, xi > 2, takes more than two values, ci belongs to one of the values of xi. In multi label classification, there exists more than one target class xi, xj… xn, such can each target label is binary and takes only values, each instance will belong to more than one target class. Multilabel classification methods can be categorized into two different groups (Trohidis & Tsoumakas, 2007): i) problem transformation methods, and ii) algorithm adaption methods. Problem transformation methods transform the problem of multilabel classification task into one or more single label classification, regression or ranking tasks. These group of methods are independent of the classification algorithm. Algorithm adaption category contains methods that extend specific single label learning algorithm to handle multiple data directly.\n2.1 Problem transformation methods: Problem transformation methods transform the problem of multilabel classification into a single label classification problem, they transform the data in such a way that existing single label algorithms can be applied. Transformation is done on the dataset so that the single label algorithms are applied. In problem transformation methods, the instance features are ignored, because they are not really important. In label power set transformation method, each unique set of labels in a multilabel training dataset is considered as one class in the new transformed data thus making the resulting dataset contain only one unique class for every entry. If the classifier can output a probability distribution over all new formed class, then it is possible to produce a ranking of labels (Read, 2008). Binary Relevance (BR) is one of the most popular approaches as a transformation that actually creates k datasets, k is the total number of classes, each for one class label and trains the classifier on each of these datasets. Each instance in the dataset Dλj, 1 ≤ j ≤ k, is either positively or negatively labelled, if they belong to class λj and each of the datasets contain same number of instances as the original data. Binary classifier is trained for each of these datasets, once the datasets are transformed. BR assumes label independence for which it is implicitly criticized. Ranking by pairwise comparison transforms the multilabel dataset into binary label datasets, one for each pair of labels. Instance from the datasets, belonging to at least one of the corresponding labels but not both, are retained. Calibrated label ranking introduces the concept of introducing and additional label called calibration label, in the original dataset, to distinguish between the relevant and irrelevant labels. The calibration label can be seen as a neutral breaking point and all the labels that are less that the rank is treated as relevant set of labels, the ones that exceeds are considered as irrelevant. Each example that is annotated with a particular label, clearly is a positive example for that label and is treated as a negative example for the calibration label. Each example that is not annotated with a label is clearly a negative example for that label and is treated as a positive example for the calibration label.\n2.2 Algorithm adaption methods:\nAdaboost.MH and Adaboost.MR (Schapire, 2000) are two implementations based on tree\nboosting Adaboost algorithm, where Adaboost.MH tries to reduce hamming loss and the latter tries to find a hypothesis with optimal ranking. In Adaboost.MH, examples are presented as example label pairs and in each iteration increases the weights of misclassified example label pairs, but in contrast, AdaboostM.MR works on a pair of labels for any instance and in each iteration increases the weights of the example with mis-ordered label pairs. There exist many Lazy learning algorithms which are very similar, but the main differences occur in the way they aggregate the label sets for the given instances. BRkNN (Tsoumakas, T, Spyromitros, & Vlahavas, 2008) is a simple method which is logically equivalent to applying Binary Relevance followed by kNN. There are two main issued related to this method, computational complexity is multiplied by the number of labels and the other that none of the labels are included in at least half of the k nearest neighbors. BP-MLL is the common method that is\nbased on Neural Network and Multilayer Perceptron based algorithms, where the error function (back propagation) has been modified to handle multilabel data. In this case, to handle multilabel data, one output is maintained for each class label. Multiclass Multilayer perceptron (MMP) proposed by Crammer and Singer in (Crammer & Kearns, 2003) leads to correct label ranking by updating the weight of the perceptron.\nBR with SVM proposed by Godbole & Sarawagi (2004) proposed three ideas to improve the\nmargin of overfitting for smaller training set. The first idea deals with having an extended dataset with K (=|L|) additional features so that BR will consider potential label dependencies, additional features are actually the predictions of each binary classifier at the first round. Confmat, is the second idea based on confusion matrix which removes negative training samples of a complete label if it is very similar to the positive label. The third idea is BandSVM, where on the learned decision hyperplane very similar negative examples are removed that are within a threshold distance and if there is a presence of overlapping classes, better models are built.\n3 Performance Evaluation In multi-labeled classification classifiers output a set of labels for every example, their prediction can be fully correct or partially correct or fully incorrect. Hence, single label accuracy metrics applied in their original form only partially capture multilabel performance and have to be adapted for multilabel problems (Sokolova, 2011). We present the evaluation measures used in our study.\nExact Match Ratio is an extension of accuracy of single label for multilabel prediction where\npartially correct and complete incorrect labels are treated as incorrect, since prediction of instances in multilabel data is a set of relevant and irrelevant labels and that the prediction can be fully correct or partially correct or fully incorrect.\nI is the indicator function.\nPrecision is the proportion of predicted correct labels to the total number of actual labels,\naveraged across all the instances.\nRecall is the proportion of predicted correct labels to the total number of predicted labels,\naveraged over all instances.\nAccuracy is defined as the proportion of predicted correct labels to the total number of labels for\nthat instance, averaged across all the instances.\nF1 score is defined as the harmonic mean of precision and recall, averaged over all the instances.\nHamming loss is the average of correctly predicted example to class ratio. If the HL is 0, then it\nwould imply that there is no error, but it is nearly impossible, so smaller the value of HL, the better is the performance.\nOne-error is the count of how many times the top ranked predicted label is not in the set of true\nlabels of the instance.\nThe top ranked predicted label is the label the classifier is most confident on and getting it wrong would clearly be an indication of overall lower performance of the classifier.\nRanking loss evaluates the average proportion of label pairs that are incorrectly ordered for an\ninstance.\nSimilar to One-error, the smaller the ranking loss, the better the performance of the learning algorithm.\nCoverage is the metric that evaluates how far on average a learning algorithm need to go down in\norder to cover all the true labels of an instance. Smaller the value of coverage, the better the performance.\n4 Data Set\nIn 2010, diabetes was the seventh leading cause of death mentioned in a total of 234,051 death\ncertificates 2 . About 208,000 Americans under the age 20 are estimated to have diagnosed diabetes, which constitutes about 0.25% of the American population. In 2012, there were 1.7 million newly identified cases of diabetes. By population groups, 12.8%of Hispanics are diagnosed with diabetes. It is estimated that 7.6% and 13.2% is occupied by non-Hispanic black and non-Hispanic whites respectively. 9.0% of Asian Americans are affected by diabetes, while 15.9% of American Indians are diagnosed with diabetes. The dataset used in these experiments is extracted by Strack et. al. from national data warehouse that collects comprehensive clinical records across hospitals throughout the United States. The initial database\n2 http://www.cdc.gov/diabetes/pubs/statsreport14/national-diabetes-report-web.pdf\nconsisted of data systematically collected from participating institutions and includes encounter data such as emergency, inpatient, and outpatient date. Patient demographics such as age, sex, and race were also present. Few medical parameters such as diagnosis code, in-hospital procedures documented by ICD-9CM codes, laboratory results, in hospital mortality and hospital characteristics were also included. Dataset gathering was the combined work of Strack et. al. (2014) representing 10 years (1999-2008) of clinical care at 130 hospitals and integrated delivery networks throughout the United States.\nInitial dataset consisted of 101767 records of male and female gender combined, each belonging to one of the following race: Caucasian, African American, Asian, Hispanic and other. The collection was created from large clinical database of diabetes patients based in the US hospitals. In the actual dataset, value of HbA1c was used as a marker of attention to identify if an individual is having a diagnosis of diabetes or not. 55 out of total 117 features were retained in the original dataset describing encounters, demographics, diagnoses, diabetic medications, number of visits and payment details. All the attribute information is briefly discussed in Table 1, Appendix A. In many researches of diabetic data mining and machine learning, the research entails on learning presence or absence of diabetes and HBA1c has great significance as diabetes marker and such learning process is a binary classification.\nOur work involves learning demographic information of diabetes patients such as race and gender. Every male or female patient belonged to one of the following race groups: Caucasian, African American, Asian, Hispanic and Other. The age attribute which is grouped [(0-10), (10-20), etc.,] was not taken into current experiments as we considered that the process will be more complicated and beyond the scope of this project. On the pre-processing step, we removed several features that could not be treated directly since they had a high percentage of missing. These features were weight (97%), payer code (40%), and medical specialty (47%). Weight attribute was considered to be too sparse and it was not included in further analysis. Payer code was removed since it had a high percentage of missing values and it was not considered relevant to the outcome. Medical specialty attribute was maintained, adding the value “missing” in order to account for missing values. Personal Health Indicators (PHI) such as Encounter ID and Patient number were removed from the dataset as a step in de-identification process. Upon preprocessing and eliminating the features that were not related, we ended with 98054 instances and 45 attributes.\n5 Empirical Results Our experiments were conducted in three stages to find best classifier for learning demographics: the first stage consisted of applying several multi label classifiers on a sample dataset of first 1000 samples; the best performing algorithms were re-applied on second (10000 samples) and third (20000 samples) stages. We have chosen MEKA/ Mulan framework for running experiments. MEKA is based on WEKA framework. MEKA uses multiple attributes, one for each target label, where all variables are binary, indicating label relevance (1) or irrelevance (0), rather than a multi class – binary attribute. MULAN contains an evaluation framework that calculates a rich variety of performance measures; it is embedded into MEKA.\nThe first 1000 samples of the processed data were allocated for the initial set of experiments. Models were built on multilabel classifiers such as Binary Relevance (BR), Classifier Chains (CC), and Bayesian Classifier Chains (BCC), Binary Relevance quick (BRq), Conditional Dependency Network (CDN), and Majority Labeset. Based on accuracy and data type compatibility, we considered Binary Relevance, Classifier Chains and Bayesian Classifier Chains for further experiments. Using these three multi label classifiers as the base classifier (Step I) for transforming multilabel data into single label data, and for step 2, we decided on a few single label classifiers. (Recall that in MEKA each multilabel model is an ensemble of single label and multilabel classifier.) We ran Random Tree, Decision Tree, Decision Table, Multilayer Perceptron, Hoeffding Tree, k Nearest Neighbor, Naïve Bayes, JRIP. Testing time of Multilayer perceptron was longer than all other algorithms. The ones that were considered for further\nexperiments are Random Tree, Decision Table, KNN, Hoeffding Tree, NaiveBayes, JRIP, and ZeroR. All the models were tested using test/ train split and Cross validation method and their results are tabulated in Table 1 below and individual values of accuracies and other measures are tabulated in Table 1 of Appendix A.\nBR/Hoeffding Tree and BCC/JRIP were chosen as the best algorithms based on their overall and individual accuracy measures. The results of these two models, from 10 fold cross validation method, is tabulated below in table 2.\nOnce the results of the experiments were available for 1000 samples, further experiments were\nperformed on two different randomly chosen bigger datasets of 10000 and 20000 samples respectively. Overall and individual accuracy for 10000 and 20000 samples are presented in Tables 3and 4.\nOur empirical results showed that Hoeffding Trees (Geoff, Laurie, & Pedro, 2001) and JRip\n(Cohen, 1995) outperformed other algorithms. Hoeffding Tree is an incremental, anytime decision tree induction algorithm. HT exploit the fact that a small sample can often be enough to choose an optimal setting attribute. HT is a common decision tree variant that supports the idea by Hoeffding bound, which quantifies the number of observations. JRip implements a propositional rule learner. It is based on association rule with reduced error pruning (REP). In REP rules algorithms, the training data is split into a growing set and a pruning set. First, an initial rule set is formed the growing set, using some heuristic method. This growing set rule set are repeatedly pruned by applying one of a set of pruning operators. At each stage of simplification, the pruning operator chosen is the one that yields the greatest reduction of error on the pruning set. Table 5 below compares various performance measures of all multilabel models and datasets. Other than accuracy, other performance measures available from MEKA that were considered are Exact Match Ratio, Hamming Score, Harmonic Score, F1 Micro Average, Rank Loss, One Error, Hamming Loss and Zero One Loss. The latter four measures Rank Loss, One Error, Hamming Loss and Zero One Loss represent loss functions, i.e. lesser loss function indicates better algorithms. The relations are vice versa for Hamming Score, Harmonic Score, Exact Match Ratio, F1 Micro Average where the performance is better if the performance values are higher. Exact Match Ratio exemplifies the most difficult to achieve overall accuracy: it counts only examples with fully correctly predicted labels. Note that the best Exact Match Ratio is achieved by BCC / JRIP. BCC is based on Bayesian probability of Classifier Chains; it considers label dependencies, which can an advantage in studies of patient data. Loss functions such as Rank Loss, One Error, Hamming Loss, and Zero One Loss are also low for BCC/JRIP model for 1000 samples, making it the best candidate for better performance given the label dependencies.\nOn 10000 and 20000 samples, BR/Hoeffding Tree performed better than the majority class classification. All the CC and BCC models of Hoeffding Tree and JRIP considerably outperformed the baseline accuracy. This shows the overall good performance prediction accuracy of the models built using Hoeffding Tree and JRIP algorithms. Though the individual performance of all models on three datasets (1000, 10000, 20000 samples) is better than the baseline (ZeroR), one of the contrasting observation is that individual accuracy, overall accuracy, and values of most of the other evaluation parameters are boosted from small dataset of 1000 samples to 100000 samples, whereas inconsistent between 10000 to 20000 samples. This observation can be due to the inconsistency in the dataset, were large subset of samples belonged to one or two target labels. Since ZeroR only predicts the mode for nominal class labels, the underlying multilabel base classifier has no impact on the overall performance of the ZeroR multiclass ensemble, which is the reason why accuracy of all the ZeroR models for a given dataset it same. Overall accuracy, individual accuracy and values of all the performance measures are tabulated in Tables 3 and 4 of Appendix A respectively.\nAlthough the performance of BCC / Hoeffding Tree model did not outperform BCC/ JRIP model,\non a much larger dataset of 20000 samples, BCC / Hoeffding Tree model performed better overall in terms of Hamming Score, Harmonic Score, F1 Micro Average and loss functions such as Hamming loss and Zero One Loss values were lesser than all other models.\n6 Conclusion and Future work In this empirical study, we approached learning of patient model based on different race and gender groups as multi-label problem. We used the UCI Diabetics dataset to obtain empirical evidence. The data set consisted of over 100,000 records and more than 50 features in medical setup based on 130 US hospitals. The dataset included personal demographics, diagnoses code, lab results, etc. and hospital features such as medical specialty, lab test results, diagnosis code etc. Our target classification labels were diabetes patients’ demographics such as race and gender. Using MEKA/MULAN, we applied multi-label learning algorithms BR/Hoeffding Tree, CC/Hoeffding Tree, BCC/Hoeffding Tree, BR/JRIP. CC / JRIP, BCC/ JRIP respectively. The results of the models built were evaluated and compared based on several multi-label performance measures. Experiments conducted on 1000, 10000, 20000 samples of the Diabetics data sets have shown that the BR/JRIP algorithm achieves Exact Match Ratio of 0.533 (1000 samples), 0.702 (10000 samples), 0.569 (20000 samples), improving over the baseline model ZeroR with accuracy of 0.526, 0.586, .562 respectively.\nOur results can be further used in Privacy-Preserving Data Mining. The important factor to be considered when involved in removing personal information is to make sure that removed features have no or very less effect on the classification accuracy. Data anonymization is an important process before releasing the dataset to make sure that the individuals representing the dataset are anonymous and has been considered seriously in the past. LeFevre, DeWitt, & Ramakrishnan (2006) proposed a suite of greedy algorithms in order to address the K anonymization problem for a number of analysis tasks such as classification and regression analysis for single/multiple categorical and numerical target attribute(s) respectively. Byun, Bertino, & Li (2005) proposed a comprehensive approach for privacy preserving access control based on the notion of purpose. Xiong and Rangachari (2008) presented an application-oriented approach for data anonymization which considers the relative attribute importance for the target applications\nAs an extension to the work done we foresee experiments utilizing the entire dataset. The results\nof experiments on the entire dataset would give a better insight into how the patients are dispersed. It would also be an interesting task to predict the age demographic in addition to race and gender, as it would represent complete demographic of the patient. Since this is the first work of its kind, it would be great idea to build models using algorithms that is capable of handling label dependencies. Apart from those mentioned above, it would be ideal if the rate of classification of different target label groups can be computed. In addition, our application can be further integrated into Privacy-Preserving Data Mining, where they can be used to assess risk of identification of different patient groups.\nBibliography Arrue, B., Ollero, A., & Matinez de Dios, J. (2000). An Intelligent System for False Alarm Reduction in\nInfrared Forest Fire Detection. IEEE Intelligent Systems 15(3), 64-73.\nBarutcuoglu, Z., Schapire, R., & Troyanskaya, O. (2006). Hierarchical multi-label prediction of gene\nfunction. Bioinformatics, 830-836.\nBlockeel, H., Schietgat, L., Struyf, J., Dz?eroski, S., & Clare, A. (2006). Decision trees for hierarchical\nmultilabel classification: A case study in functional genomics. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 18-29.\nBoutell, M., Luo, J., Shen, X., & Brown, C. (2004). Learning Multilabel scene classification. Pattern\nRecognition. 1757-1771.\nBrinker, K., Urnkranz, J., & Ullermeier, E. (2006). A unified model for multilabel classification and\nranking. In: Proc. 17th European Conference on Artificial Intelligence.\nByun, J., Bertino, E., & Li, N. (2005). Purpose based access control of complex data for privacy\nprotection. The 10 th ACM Symposium on Access Control Models and Technologies, Stockholm, Sweden, 102-110.\nCesa-Bianchi, N., Gentile, C., & Zaniboni, L. (2006). Hierarchical classification: combining bayes with\nSupport Vector Machines. ICML ’06: Proceedings of the 23rd international conference on Machine learning, 177-184.\nChang, C., & Lin, C. (n.d.). LIBSVM: A Library for support vector machines. Retrieved from\nwww.csie.ntu.edu.tw/ cjlin/libsvm\nClare, A., & King, R. (2001). Knowledge discovery in multi-label phenotype data. In: Proceedings of the\n5th European Conference on Principles of Data Mining and Knowledge Discovery, 42-53.\nCohen, W. (1995). Fast Effective Rule Induction. Machine Learning: Proceedings of the Twelfth\nInternational Conference (ML 195), 115-125.\nCortez, P., & Morais, A. (2007). A Data mining approach to predict forest fires using metrological data.\n13th EPIA - Portugese Conference on Artificial Intelligence, Portugal, 521-523.\nCrammer, K., & Kearns, Y. (2003). A family of additive online algorithms for category ranking. Journal\nof Machine Learning Research,3, 1025-1058.\nDzeroski, S., Kobler, V., Gjorgioski, V., & Panov, P. (2006). Using Decision Trees to predict forest stand\nheight and canopy cover from LANSAT and LIDAR data. 20th International conference on Informatics of Environmental Protection.\nElisseeff, A., & Weston, J. (2002). A kernel method for multi-labelled classification. : Advances in\nNeural Information Processing Systems.\nGarcia, V., Lee, B., Woodard, P., & Titus, S. (1996). Applying Neural Network technology to human\ncaused wilfire occurrence prediction. AI Applications 10(3), 9-18.\nGeoff, H., Laurie, S., & Pedro, D. (2001). Mining time-changing data streams. In: ACM SIGKDD Intl.\nConf. on Knowledge Discovery and Data Mining, 97-106.\nGodbole, S., & Sarawagi, S. (2004). Discriminative Methods for Multi-labeled Classification.\nProceedings of the 8th Pacific-Asia Conference on Knowledge Discovery and Data Mining. (PAKDD), 22-30.\nJafer, Y., Matwin, S., & Sokolova, M. (2014). Task Oriented Privacy Preserving Data Publishing Using\nFeature Selection. Proceedings of 27th Canadian Conference on Artificial Intelligence, Montreal, 143-154.\nLeFevre, K., DeWitt, D. J., & Ramakrishnan, R. (2006). Workload-aware anonymization. Proceedings of\nthe 12th ACM SIGKDD International Conference on Knowledge Disco very and Data Mining, Philadelphia, USA,, 277-286.\nLi, T., & Ogihara, M. (2003). Detecting emotion in music. Proceedings of the International Symposium\non Music Information Retrieval, Washington D.C., USA, 239-240.\nLi, T., & Ogihara, M. (2006). Toward intelligent music information retrieval. IEEE Transactions on\nMultimedia, 564-574.\nQi, G., Hua, X., Rui, Y., Tang, J., Mei, T., & Zhang, H. (2007). Correlative multilabel video annotation.\nMultimedia'07 Proceedings of the 15th international conference on Multimedia, New York, 17- 26.\nRead, J. (2008). A Pruned Problem Transformation Method for Multilabel classification. In: Proc 2008\nNew Zealand Computer Science Research Student Conference (NZCSRS ), 143-150.\nSchapire. (2000). Boostexter: A boosting-based system for text categorization. . Machine Learning,\n39(2/3), 135-168.\nSnoek, C., Worring, M., Van Germet, J., Geusebroek, J., & Smeulders, A. (2006). The challenge problem\nfor automated detection of 101 semantic concepts in multimedia. Multimedia 06, Proceedings of the 14th anuual ACM international Conference on Multimedia, 421-430.\nSokolova, M. (2011). Evaluation Measures for Detection of Personal Health Information. Proc. 2nd\nWorkshop on Biomedical Natural Language Processing, Bulgaria.\nSorrower, M. (2010). A Literature Survey on Algorithms for Multi-label Learning. Corvallis, OR, Oregon\nState University. Corvallis, OR, Oregon State University.\nStrack, B., DeShazo, J., Gennings, C., Olmo, J., Ventura, S., Cios, K., & Clore, J. (2014). Impact of\nHbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records. Research International, vol. 2014, Article ID 781670, 11 Pages.\nTrohidis, K., & Tsoumakas, G. (2007). Multilabel classification: An overview. In: International Journal\nof Data Warehousing and Mining, 1-13.\nTrohidis, K., Tsoumakas, G., Kalliris, G., & Vlahavas, I. (2008). Multilabel classification of music into\nemotions. Proc. 9th International Conference on Music Information Retrieval, Philadelphia, PA, USA.\nTsoumakas, G., T, Spyromitros, & Vlahavas, I. (2008). An Emperical Study of Lazy Multilabel\nClassification Algorithms. 5th Hellenic Conference on Artificial Intelligence (SETN ), 401-406.\nWieczorkowska, A., Synak, P., & Ras, Z. (2006). Multi-label classification of emotions in music.\nProceedings of the 2006 International Conference on Intelligent Information Processing and web mining, 307-315.\nXiong, L., & Rangachari, K. (2008). Towards Application-Oriented Data Anonymization. First SIAM\nInternational Workshop on Practical Privacy-Preserving Data Mining, Atlanta, US, 1-10.\nYang, S., Kim, S., & Ro, Y. (2007). Semantic home photo categorization. Circuits and Systems for home\ntechnology, IEEE Transactions., 324-335.\nZhang, M., & Zhou, Z. (2007). A lazy learning approach to multi label learning. Pattern Recognition,\n2038-2048.\nZhang, Y., Burer, S., & Street, W. (2006). Ensemble pruning via semi-definite programming. Journal of\nMachine Learning Research, 1315-1338.\nAppendix A:"
    }, {
      "heading" : "CC/",
      "text" : ""
    }, {
      "heading" : "BR/",
      "text" : ""
    }, {
      "heading" : "BCC/",
      "text" : ""
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Automated learning of patients’ demographics can be seen as multilabel problem where a patient model is based on different race and gender groups. The resulting model can be further integrated into Privacy-Preserving Data Mining, where they can be used to assess risk of identification of different patient groups. Our project considers relations between diabetes and demographics of patients as a multi-labelled problem. Most research in this area has been done as binary classification, where the target class is finding if a person has diabetes or not. But very few, and maybe no work has been done in multi-labeled analysis of the demographics of patients who are likely to be diagnosed with diabetes. To identify such groups, we applied ensembles of several multilabel learning algorithms. The best performing multi label ensembles include BR/Hoeffding Tree, CC/Hoeffding Tree, BCC/Hoeffding Tree, BR/JRIP, CC/JRIP, BCC/ JRIP respectively. In the empirical part of this study, we used on the UCI Diabetics dataset of over 100,000 records, collected from 130 US hospitals. The dataset consisted of attributes that included personal demographics, diagnoses code, lab results, etc. Experiments conducted on datasets of 1000, 10000, 20000 samples, show that BR/JRip model achieves a high overall accuracy of 0.533 (1000 samples), 0.702 (10000 samples), 0.569 (20000 samples), improving over the baseline model ZeroR with accuracy of 0.526, 0.586, .562 respectively. Loss functions such as Rank Loss, One Error, Hamming Loss, and Zero One Loss are also low for BCC/JRIP model for all samples of dataset, making it the best candidate for better performance given the label dependencies.",
    "creator" : "Microsoft® Office Word 2007"
  }
}