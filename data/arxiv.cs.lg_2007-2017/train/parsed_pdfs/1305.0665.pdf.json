{
  "name" : "1305.0665.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "yanwu@tongji.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 5.\n06 65\nv2 [\ncs .L\nG ]\n1 3\nO ct\n2 01\n3\nKeywords: astronomical instrumentation, methods and techniques—methods: analytical—methods: data analysis— methods: statistical"
    }, {
      "heading" : "1 Introduction",
      "text" : "With the rapid development of both the astronomical instruments and various machine learning algorithms, we can apply the spectral characteristics of stars to classify the stars. A great quantity of astronomical observatories have been built to get the spectra, such as the Large Sky Area Multi-Object Fibre Spectroscopic Telescope (LAMOST) in China. A variety of machine learning methods, e.g., principal component analysis (PCA), locally linear embedding (LLE), artificial neural network (ANN) and decision tree etc., have been applied to classify these spectra in an automatic and efficient way. In this study, we apply a novel machine learning method, restricted Boltzmann machine, to classify the CVs and non-CVs.\nCVs are composed of the close binaries that contain a white dwarf accreting material from its companion (Warner 2003). Generally, they are small with an orbital period of 1 to 10 hours. The white dwarf is often called ”primary” star, while the normal star is called the ”companion” or the ”secondary” star. The companion star, which is ”normal” like our Sun, usually loses its material onto the white dwarf via accretion.\nThe three main types of CVs are novae, dwarf novae and magnetic CVs. Magnetic CVs (mCVs) are binary star systems with low mass and also with a Roche lobe-filling red dwarf which ”gives” material to a magnetic white dwarf. Polars (AM Herculis systems) and Intermediate Polars (IPs) are two major subclasses of mCVs (Wu 2000). More than a dozen of objects have been classified as AM Her systems. Most of the objects were found to be X-ray sources1 before classified as AM Her’s resorting to optical observations.\nBesides, Muno et al. presented a catalog of 9017\n1http://ttt.astro.su.se/∼stefan/amher0.html\nX-ray sources identified in Chandra observations of a 2× 0.8◦ field around the Galactic center (Muno et al. 2009). And they found that the detectable stellar population of external galaxies in X-rays was dominated by accreting black holes and neutron stars, while most of their X-ray sources may be CVs."
    }, {
      "heading" : "1.1 Previous work in spectral classification in astronomy",
      "text" : "In 1998, Singh et al. applied principal component analysis (PCA) and artificial neural network (ANN) to stellar spectral classification (Singh, Gulati, & Gupta 1998) on O to M type stars, where O type stars are the hottest and the letter sequence (O to M) indicates successively cooler stars up to the coolest M type stars. They adopted PCA for dimension reduction firstly, in which they reduced the dimension to 20, with the cumulative percentages larger than 99.9 %. Then they used multi-layer back propagation (BP) neural network for classification.\nIn 2006, Sarty and Wu applied two well known multivariate analysis methods, i.e., PCA and discriminant function analysis, to analyze the spectroscopic emission data collected by Williams (1983). By using the PCA method, they found that the source of variation had correlation to the binary orbital period. With the discriminant function analysis, they found that the source of variation was connected with the equivalent width of the Hβ line (Sarty & Wu 2006).\nIn 2010, Rosalie et al. applied PCA to analyze the stellar spectra obtained from SDSS (Sloan Digital Sky Survey) DR6 (McGurk, Kimball, & Ivezić 2010). They found that the first 4 principal components (PCs) could remain enough information of the original data without overpassing the measurement noise. Their work made classifying novel spectra, find-\n1\ning out unusual spectra and training a variety of spectral classification methods etc. not as hard as before.\nIn 2012, Bazarghan applied self-organizing map (SOM, a kind of unsupervised artificial Neural Network (ANN) algorithm) to stellar spectra obtained from the Jacoby, Hunter and Christian (JHC) library, and the author obtained the accuracy of about 92.4% (Bazarghan 2012). In the same year, Navarro et al. used the ANN method to classify the stellar spectra with low signalto-noise ratio (S/N) on the samples of field stars which were along the line of sight toward NGC 6781 and NGC 7027 etc. (Navarro, Corradi, & Mampaso 2012). They not only trained, but also tested the ANNs with various S/N levels. They found that the ANNs were insensitive to noise and the ANN’s error rate was smaller when there were two hidden layers in the architecture of the ANN in which there were more than 20 hidden units in each hidden layer.\nIn the above, some applications of PCA for dimension reduction and ANN for spectral classification were reviewed in astronomy. Furthermore, SVM and decision trees have also been used for spectral classification in astronomy.\nIn 2004, Zhang and Zhao applied single-layer perceptron (SLP) and support vector machines (SVMs) etc. for the binary classification problem, i.e., the classification of AGNs (active galactic nucleus) and S & G (stars and normal galaxies) (Zhang & Zhao 2004), in which they first selected features using the histogram method. They found that SVM’s performance was as good as or even better than that of the neural network method when there were more features chosen for classification. In 2006, Ball et al. applied decision trees to SDSS DR3 (Ball et al. 2006). They investigated the classification of 143 million photometric objects and they trained the classifier with 477,068 objects. There were three classes, i.e., galaxy, star and neither of the former two classes, in their experiment.\nFrom the perspective of feature extraction methods, some researches in spectral classification based on linear dimension reduction technique, e.g., PCA, have been reviewed. Except from linear dimension reduction method, nonlinear dimension reduction technique has also been applied in spectral classification for feature extraction.\nIn 2011, Daniel et al. applied locally linear embedding (LLE, a well known nonlinear dimension reduction technique) to classify the stellar spectra coming from the SDSS DR7 (Daniel et al. 2011). There were 85,564 objects in their experiment. They found that most of the stellar spectra was approximately a 1d sequence lying in a 3d space. Based on the LLE method, they proposed a novel hierarchical classification method being free of the feature extraction process."
    }, {
      "heading" : "1.2 Previous application of RBM",
      "text" : "In this subsection, we present some representative applications of the RBM algorithm so far.\nIn 2007, Salakhutdinov et al. (Salakhutdinov, Mnih, & Hinton 2007) applied RBM for collaborative filtering, which is closely related to recommendation sys-\ntem in machine learning community. In 2008, Gunawardana and Meek (Gunawardana & Meek 2008) applied RBM for cold start recommendations. In 2009, Taylor and Hinton (Taylor & Hinton 2009) applied RBM for modeling motion style. In 2010, Dahl et al. (Dahl et al. 2010) applied RBM to phone recognition on the TIMIT dataset. In 2011, Schluter and Osendorfer (Schluter & Osendorfer 2011) applied RBM to estimate music similarity. In 2012, Tang et al. (Tang, Salakhutdinov, & Hinton 2012) applied RBM for recognition and de-noising on some public face databases."
    }, {
      "heading" : "1.3 Our work",
      "text" : "In this study, we applied the binary RBM algorithm to classify spectra of CVs and non-CVs obtained from the SDSS.\nGenerally, before applying a classifier for classification, the researchers always preprocess the original data, for example, normalization to get better features and thus to get better performance. Thus, firstly, we normalize the spectra with unit norm2. Then, to apply binary RBM for spectral classification, we binarize the normalized spectra by some rule which we will discuss in the experiment. Finally, we use the binary RBM for classification of the data, one half of all the given data for training and the other half for testing. The experiment result shows that the classification accuracy is 100%, which is state-of-the-art. And RBM outperforms the prevalent classifier, SVM, with accuracy of 99.14% (Bu et al. 2013).\nThe rest of this paper is organized as follows. In section 2, we review the prerequisites for training restricted Boltzmann machine. In section 3, we introduce the binary RBM and the training algorithm for RBM. In section 4, we present the experiment result. Finally, in section 5, we conclude our work in this study and also present the future work."
    }, {
      "heading" : "2 Prerequisites",
      "text" : ""
    }, {
      "heading" : "2.1 Markov Chain",
      "text" : "A Markov chain is a sequence composed of a number of random variables. Each element in the sequence can transit from one state to another one randomly. Indeed, a Markov chain belongs to a stochastic process (Andrieu et al. 2003). In general, the number of possible states for each element or random variable in a Markov chain is finite. And a Markov chain is a random process without memory. It is the current state rather than the states preceding the current state that can influence the next state of a Markov chain. This is the well known Markov Property (Xiong, Jiang, & Wang 2012).\nMathematically, a Markov chain is a sequence, X1, X2, X3, . . ., with the following property:\nP (Xn+1 = xn+1|X1 = x1, X2 = x2, . . . , Xn = xn)\n= P (Xn+1 = xn+1|Xn = xn),\n2We say the norm of a vector x = (x1, . . . , xn) is unit, if ∑\ni x 2 i = 1.\nwhere the Xi (i = 1, 2, . . .) is a random variable and it usually can take on finite values for a specific problem in the real world. And all the values as a whole can form a denumerable set S, which is commonly called the state space of the Markov chain (Yang et al. 2009).\nGenerally, all the probabilities of the transition from one state to another one can be represented as a whole by a transition matrix. And the transition matrix has the following three properties:\n• square: both the row number of the matrix and the column number of the matrix equal the total number of the states that the random variable in the Markov chain can take on;\n• the value of a specific element is between 0 and 1: it represents the transition probability from one state to another one;\n• all the elements in each row sum to 1: The sum of the transition probabilities from any specific one state to all the states equals 1.\nIf the initial vector, a row vector, is X 0, and the transition matrix is T, then after n steps of inference, we can get the final vector X 0 · T\nn. Then we introduce the equilibrium of a Markov chain. If there exists an integer Ñ , which renders all the elements in the resulting matrix TÑ nonzero, or rather, greater than 0, then we say that the transition matrix is a regular transition matrix (Greenwell, Ritchey, & Lial 2003). If the transition matrix T is a regular transition matrix, and there exists one and only one row vector V satisfying the condition that v · Tn approximately equals V , for any probability vector v and large enough integer n, then we call the vector V as the equilibrium vector of the Markov chain."
    }, {
      "heading" : "2.2 MCMC",
      "text" : "Markov chain Monte Carlo (MCMC) is a sampling algorithm from a specific probability distribution. For the detailed information of MCMC, the readers are referred to Andrieu et al. (2003). The sampling process proceeds in the form of a Markov chain and the goal of MCMC is to get a desired distribution, or rather, the equilibrium distribution via running many inference steps. The larger the number of iterations is, the better the performance of the MCMC is. And MCMC can be applied for unsupervised learning with some hidden variables or maximum likelihood estimation (MLE) learning of some unknown parameters (Andrieu et al. 2003)."
    }, {
      "heading" : "2.3 Gibbs Sampling",
      "text" : "Gibbs sampling method can be used to obtain a sequence of approximate samples from a specific probability distribution, in which sampling directly is usually not easy to implement. For the detailed information of Gibbs sampling, the readers are referred to Gelfand (2000). The sequence obtained via the Gibbs sampling method can be applied to approximate the joint distribution and the marginal distribution with\nrespect to (w.r.t.) one of all the variables etc. In general, Gibbs sampling method is a method for probabilistic inference.\nGibbs sampling method can generate a Markov chain of random samples under the condition that each of the sample is correlated with the nearby sample, or rather, the probability of choosing the next sample equals to 1 in Gibbs sampling (Andrieu et al. 2003)."
    }, {
      "heading" : "3 RBM",
      "text" : "Considering that RBM is a generalized version of Boltzmann Machine (BM), we first review BM in this section. For the detailed information of BM, the readers are referred to Ackley, Hinton, & Sejnowski (1985).\nBM can be regarded as a bipartite graphical generative model composed of two layers in which there are a number of units with both inter-layer and innerlayer connections. One layer is a visible layer v with m binary visible units vi, i.e., vi = 0 or vi = 1 (i = 1, 2, . . . ,m). For each unit in the visible layer, the corresponding value is observable. The other layer is a hidden (latent) layer h with n binary hidden units hj . As in the visible layer, hj = 0 or hj = 1 (j = 1, 2, . . . , n). For each unit or neuron in the hidden layer, the corresponding value is hidden, latent or unobservable, and it needs to be inferred.\nThe units coming from the two layers of a BM are connected with weighted edges completely, with the weights wij (vi ↔ hj) (i = 1, 2, . . . ,m, j = 1, 2, . . . , n). For the two layers, the units within each specific layer are also connected with each other, and also with weights.\nFor a BM, the energy function can be defined as follows:\nE(v ,h) = −\nm∑\ni,j=1\nviaijvj −\nn∑\ni,j=1\nhidijhj\n− m∑\ni=1\nn∑\nj=1\nviwijhj − m∑\ni=1\nvici − n∑\nj=1\nhjbj ,\n(1)\nwhere aij is the weight of the edge connecting visible units vi and vj , dij the weight of the edge connecting hidden units hi and hj , wij the weight of the edge connecting visible unit vi and hidden unit hj . For a RBM, the bj is the bias for the hidden unit hj in the following activation function (Sigmoid function f(x) = sigmoid(x) = 1/(1 + e−x))\np(hj = 1|v ) = 1\n1 + e−bj− ∑ m i=1 wijvi .\nAnd in a RBM, the ci is the bias for the visible unit vi in the following formula:\np(vi = 1|h) = 1\n1 + e−ci− ∑ n j=1 wijhj .\nThen for each pair of a visible vector and a hidden vector (v ,h), the probability of this pair can be defined as follows:\np(v ,h) = e−E(v,h)\nPF ,\nwhere the denominator PF in the fraction (a partition function) is:\nPF = ∑\nṽ,h̃\np(ṽ , h̃). (2)\nBesides, a RBM is a graphical model with the units for both layers not connected within a specific layer, i.e., there are only connections between the two layers for the RBM (Hinton & Salakhutdinov 2006). Mathematically, for a RBM, aij = 0 for i, j = 1, 2, . . . ,m and dij = 0 for i, j = 1, 2, . . . , n. Thus, the states of all the hidden units hj ’s are independent given a specific visible vector v and so are the visible units vi’s given a specific hidden vector h . Then we can obtain the following formula:\np(h |v) = ∏\nj\np(hj |v) and p(v |h) = ∏\ni\np(vi|h)."
    }, {
      "heading" : "3.1 Contrastive Divergence",
      "text" : "Contrastive Divergence (CD) is proposed by Hinton and it can be used to train RBM (Hinton, Osindero, & Teh 2006). Initially, we are given vi (i = 1, 2, . . . ,m), then we can obtain hj (j = 1, 2, . . . , n) by the sigmoid function given in the above. And the value of hj is determined by comparing a random value r ranging from 0 to 1 with the probability p(hj = 1|v ). Then we can reconstruct v by p(vi = 1|h).\nWe can repeat the above process backward and forward until the reconstruction error is small enough or it has reached the maximum number of iterations which is set beforehand. To update the weights and biases in a RBM, it is necessary to compute the following partial derivative:\n∂ log p(v ,h)\n∂wij = Edata[vihj ]− Erecon[vihj ], (3)\n∂ log p(v ,h)\n∂ci = vi − Erecon[vi], (4)\n∂ log p(v , h)\n∂bj = Edata[hj ]− Erecon[hj ], (5)\nwhere E[⋆] represents the expectation of ⋆, and the subscript ’data’ means that the probability is originaldata-driven while the subscript ’recon’ means that the probability is reconstructed-data-driven.\nThen the weight can be updated according to the following rule:\n∆wij = η(Edata[vihj ]− Erecon[vihj ]),\nwhere η is a learning rate, which influences the speed of convergence. And the biases can be updated similarly.\nIn equations (3)-(5), Edata[⋆]’s are easy to compute. To compute or inference the latter term Erecon[⋆], we can resort to MCMC."
    }, {
      "heading" : "3.2 Free energy and Soft-max",
      "text" : "To apply RBM for classification, we can resort to the following technique. We can train a RBM for each specific class. And for classification, we need the free\nenergy and the soft-max function for help. For a specific visible input vector v , its free energy equals to the energy that a single configuration must own and it equals the sum of the probabilities of all the configurations containing v . In this study, the free energy (Hinton 2012) for a specific visible input vector v can be computed as follows:\nF (v) = −[ ∑\ni\nvici + ∑\nj\nlog(1 + exj )], (6)\nwhere xj = bj + ∑\ni viwij .\nFor a given specific test vector v , after training the RBMc on a specific class c, the log probability that RBMc assigns to v can be computed according to the following formula:\nlog p(v |c) = −Fc(v)− logPFc,\nhere the PFc is the partition function of RBMc. For a specific classification problem, if the total number of classes is small, there will be no difficulty for us to get the unknown log partition function. In this case, given a specific training set, we can just train a ”soft-max” model to predict the label for a visible input vector v resorting to the free energies of all the class-dependent RBMc’s:\nlog p(label = c|v ) = e−Fc(v)−log P̃F c∑ d e−Fd(v)−log P̃Fd . (7)\nIn the above formula Equation (7), all the partition functions P̃F ′\ns can be learned by maximum likelihood (ML) training of the ”soft-max” function, where the maximum likelihood method is a kind of parameter estimation method generally with the help of the log probability. Here, the ”soft-max” function for a specific unit is generally defined in the following form:\npj = exj∑k i=1 e xi ,\nand the parameter k means that there are totally k different states that the unit can take on.\nFor clarity, we show the complete RBM algorithm in the following. The RBM algorithm as a whole based on the CD method can be summarized as follows:\n• Input: a visible input vector v ; the size of the hidden layer nh; the learning rate η and the maximum epoch Me;\n• Output: a weight matrix W, a biases vector for the hidden layer b and a biases vector for the visible layer c;\n• Training: Initialization: Set the visible state with v1 = x , and set W, b and c with small (random) values, For t = 1, . . . ,Me, For j = 1, . . . , nh, Compute the following value p(h1j = 1|v 1) = sigmoid(bj + ∑ i v1iWij);\nSample h1j from the conditional distribution P (h1j |v 1) with Gibbs sampling method; End\nFor i = 1, 2, . . . , nv , //Here, the nv is the size of the visible input vector v Compute the following value p(v2j = 1|h1) = sigmoid(ci + ∑ j Wijh1j); Sample v2i from the conditional distribution P (v2i|h1) with Gibbs sampling method; End For j = 1, . . . , nh, Compute the following value p(h2j = 1|v2) = sigmoid(bj + ∑ i v2iWij); End Update the parameters: W = W+ η[P (h1 = 1|v1)v1 − P (h2 = 1|v2)v2]; c = c + η(v1 − v2); c = c + η[P (h1 = 1|v1)− P (h2 = 1|v2)]; End\nFor classification, after training the RBM using the above algorithm, we need to compute the free energy function by Equation (6) and then we can assign a label for the sample v with Equation (7)."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Data description",
      "text" : "There have been a large amount of surveys in astronomy. SDSS is one of those surveys and it is one of the most not only ambitious but also influential ones (The official website of SDSS is http://www.sdss.org/). The SDSS has begun collecting data since 2000. From 2000 to 2008, the SDSS collected deep and multi-color images containing no less than a quarter of the sky and it also created 3d maps for over 930,000 galaxies and also for over 120,000 quasars. Data Release 7 (DR7) is the seventh major data release and it provides spectra and redshifts etc. for downloading.\nAll the data used in our experiment is coming from the SDSS. All the samples in the entire data set are divided into two classes, one class composed of nonCVs while the other class composed of CVs. There are totally 6818 non-CVs and 208 CVs in our data set. Each sample is composed of 3522 variables, or rather, spectral components. Among the total 6818 non-CVs, there are 1,559 belonging to Galaxies, 3,981 belonging to Stars and the remaining 1,278 belonging to QSOs (Quasi-stellar objects)3.\nIn the following, we show the CVs in detail in our experiment. It is common that there will be transparent Balmer absorption lines in their spectra when the CVs outburst. A representative spectrum of the CV from the SDSS is shown in Figure 1. Much work has been done on the CVs for ages. Without hightech, the earlier researches are focused on the optical characteristics of the spectrum. Then with the help of the high-tech astronomical instruments, the multiwavelength studies of the spectrum become to be true and the astronomers can obtain much more information about the CVs than before (Bu et al. 2013). From 2001 to 2006, Szkody et al. had been using the SDSS\n3For detail, the readers are referred to the official website of SDSS DR7: http://www.sdss.org/dr7/\nto search for CVs. The CVs in our data set are from their studies (Szkody et al. 2002, 2003, 2004, 2005, 2006, 2007), and we are deeply grateful to their researches. For clarity, we show the number of the CVs they found using the SDSS in Table 1. And the spectrum of a CV in our data set is shown in Figure 2.\nTable 1: The number of the CVs that Szkody et al. searched using the SDSS.\nPaper # of CVs Szkody et al. 2002 22 Szkody et al. 2003 42 Szkody et al. 2004 36 Szkody et al. 2005 44 Szkody et al. 2006 41 Szkody et al. 2007 28\nIn our experiment, we chose randomly half of the whole data for training and the remaining half for testing for both non-CVs and CVs. In detail, for nonCVs, half of the total 6818 samples (i.e. 3414) were randomly chosen to train the RBM classifier and the remaining half to test the RBM classifier. Similarly, for CVs, half of all the 208 samples (i.e. 104) were randomly chosen to train the RBM classifier and the remaining half to test the RBM classifier. To explain it clearly, we showed the data used for training and testing the RBM classifier in the following table (Table 2)."
    }, {
      "heading" : "3522 is the dimension of the original data.",
      "text" : ""
    }, {
      "heading" : "4.2 Parameter chosen",
      "text" : "In this subsection, we present the parameters in our experiment. We chose all the parameters referring to Hinton (2012). The learning rate in the process of updating was set to be 0.1. The momentum for smoothness and to prevent over-fitting was chosen to be 0.5. The maximum number of epochs was chosen to be 50. The weight decay factor, penalty, was chosen to be 2×10−4. The initial weights were randomly generated from the standard normal distribution, while the biases vectors b and c were initialized with 0 . For clarity, we present them in the following table (Table 3)."
    }, {
      "heading" : "4.3 Experiment result",
      "text" : "We first normalized the data to make it have unit l2 norm, i.e., for a specific vector x = [x1, x2, . . . , xn], the l2 norm of the vector satisfies ∑ i x 2 i = 1. Then we could get two matrixes, one was A = 6818 × 3522 and the other was B = 208 × 3522. Then we found out the maximum element and the minimum element for CVs and non-CVs respectively. Finally, to apply binary RBM for classification, we found a parameter to assign the value of the variable in our experiment with 0 or 1, or rather, binarization.\nMathematically, if\nS(i, j) −minS(i, j) < α(maxS(i, j) −minS(i, j)),\nthen we set S(i, j) with 0, otherwise we set S(i, j) with 1. Here we used S(i, j) (after binarization) to denote the element of the matrix A and B in the ith row and the jth column. The parameter α satisfied 0 < α < 1. To investigate the influence of the parameter α on the final performance of the binary RBM algorithm, we first chose it to be 1/2 heuristically. Then we chose it to be 1/3. The experiment result shows that the classification accuracy is 100%, which is state-of-theart and it outperforms the prevalent classifier SVM (Bu et al. 2013).\nFor clarity, we show the result in Table 4, in which we also show the performance the binary RBM algorithm based on other values for the variable α. From Table 4, we can see that the classification accuracy is 97.2% when α = 1/2. However, almost all of the CVs for testing is labeled as non-CVs.\nTable 4 shows the classification accuracy computed by the following formula:\nAcc =\n∑ [ŷ == y ]\nCard(y) , (8)\nwhere y is a vector denoting the label of all the test samples. In our experiment, there are 3413 (3409 nonCVs + 104 CVs) test samples. And ”Card(y)” represents the number of elements in vector y . In Equation (8), the denominator ŷ is the label of all the test samples predicted by Equation (7), in which c = +1 or c = −1. In this paper, c = +1 means that the sample belongs to non-CVs, while c = −1 means that the sample belongs to CVs4. And ∑ [ŷ == y ] means the total number of equal elements in vector y and vector ŷ ."
    }, {
      "heading" : "5 Conclusion and future work",
      "text" : "Restricted Boltzmann machine is a bipartite generative graphical model which can extract features representing the original data well. By introducing free energy and soft-max function, RBM can be used for classification. In this paper we apply restricted Boltzmann machine (RBM) for spectral classification of non-CVs and CVs. And the experiment result shows that the\n4You can use any two different integers to represent the labels of the samples belonging to non-CVs and CVs, and this does not impact the result of the experiment.\nclassification accuracy is 100 %, which is the state-ofthe-art and outperforms the rather prevalent classifier, SVM.\nSince RBM is the building block of deep belief nets (DBNs) and deep Boltzmann machine (DBM), then we can infer that deep Boltzmann machine (Salakhutdinov & Hinton 2009) and deep belief net can also perform well on spectral classification, which is our future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors are very grateful to the anonymous reviewer for a thorough reading, many valuable comments and rather helpful suggestions. The authors thank the editor Bryan Gaensler a lot for the helpful suggestions on the organization of the manuscript. The authors also thank Jiang Bin for providing the CV data."
    } ],
    "references" : [ {
      "title" : "Cataclysmic variable stars (Cam",
      "author" : [ "B. Warner" ],
      "venue" : null,
      "citeRegEx" : "1025",
      "shortCiteRegEx" : "1025",
      "year" : 2003
    } ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "In this study, a novel machine learning algorithm, restricted Boltzmann machine (RBM), is introduced. The algorithm is applied for the spectral classification in astronomy. RBM is a bipartite generative graphical model with two separate layers (one visible layer and one hidden layer), which can extract higher level features to represent the original data. Despite generative, RBM can be used for classification when modified with a free energy and a soft-max function. Before spectral classification, the original data is binarized according to some rule. Then we resort to the binary RBM to classify cataclysmic variables (CVs) and non-CVs (one half of all the given data for training and the other half for testing). The experiment result shows state-of-the-art accuracy of 100%, which indicates the efficiency of the binary RBM algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}