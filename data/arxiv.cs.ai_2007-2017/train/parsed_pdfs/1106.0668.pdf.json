{
  "name" : "1106.0668.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Tapio Elomaa", "elomaa s.helsinki.fi", "Matti Kääriäinen" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "An Analysis of Redu ed Error Pruning",
      "text" : "Tapio Elomaa elomaa s.helsinki.fi Matti Kääriäinen matti.kaariainen s.helsinki.fi Department of Computer S ien e P. O. Box 26 (Teollisuuskatu 23) FIN-00014 University of Helsinki, Finland\nAbstra t\nTop-down indu tion of de ision trees has been observed to su er from the inadequate fun tioning of the pruning phase. In parti ular, it is known that the size of the resulting tree grows linearly with the sample size, even though the a ura y of the tree does not improve. Redu ed Error Pruning is an algorithm that has been used as a representative te hnique in attempts to explain the problems of de ision tree learning.\nIn this paper we present analyses of Redu ed Error Pruning in three di erent settings. First we study the basi algorithmi properties of the method, properties that hold independent of the input de ision tree and pruning examples. Then we examine a situation that intuitively should lead to the subtree under onsideration to be repla ed by a leaf node, one in whi h the lass label and attribute values of the pruning examples are independent of ea h other. This analysis is ondu ted under two di erent assumptions. The general analysis shows that the pruning probability of a node tting pure noise is bounded by a fun tion that de reases exponentially as the size of the tree grows. In a spe i analysis we assume that the examples are distributed uniformly to the tree. This assumption lets us approximate the number of subtrees that are pruned be ause they do not re eive any pruning examples.\nThis paper lari es the di erent variants of the Redu ed Error Pruning algorithm, brings new insight to its algorithmi properties, analyses the algorithm with less imposed assumptions than before, and in ludes the previously overlooked empty subtrees to the analysis."
    }, {
      "heading" : "1. Introdu tion",
      "text" : "De ision tree learning is usually a two-phase pro ess (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1993). First a tree re e ting the given sample as faithfully as possible is\nonstru ted. If no noise prevails, the a ura y of the tree is perfe t on the training examples that were used to build the tree. In pra ti e, however, the data tends to be noisy, whi h may introdu e ontradi ting examples to the training set. Hen e, 100% a ura y annot ne essarily be obtained even on the training set. In any ase, the resulting de ision tree is over tted to the sample; in addition to the general trends of the data, it en odes the pe uliarities and parti ularities of the training data, whi h makes it a poor predi tor of the\nlass label of future instan es. In the se ond phase of indu tion, the de ision tree is pruned in order to redu e its dependen y on the training data. Pruning aims at removing from the tree those parts that are likely to only be due to the han e properties of the training set.\nThe problems of the two-phased top-down indu tion of de ision trees are well-known and have been extensively reported (Catlett, 1991; Oates & Jensen, 1997, 1998). The size\n2001 AI A ess Foundation and Morgan Kaufmann Publishers. All rights reserved.\nof the tree grows linearly with the size of the training set, even though after a while no a ura y is gained through the in reased tree omplexity. Obviously, pruning is intended to\nght this e e t. Another defe t is observed when the data ontains no relevant attributes; i.e., when the lass labels of the examples are independent of their attribute values. Clearly, a single-node tree predi ting the majority label of the examples should result in this ase, sin e no help an be obtained by querying the attribute values. In pra ti e, though, often large de ision trees are built from su h data.\nMany alternative pruning s hemes exist (Mingers, 1989a; Esposito, Malerba, & Semeraro, 1997; Frank, 2000). They di er, e.g., on whether a single pruned tree or a series of pruned trees is produ ed, whether a separate set of pruning examples is used, whi h aspe ts ( lassi ation error and tree omplexity) are taken into a ount in pruning de isions, how these aspe ts are determined, and whether a single s an through the tree su es or whether iterative pro essing is required. The basi pruning operation that is applied to the tree is the repla ement of an internal node together with the subtree rooted at it with a leaf. Also more elaborated tree restru turing operations are used by some pruning te hniques (Quinlan, 1987, 1993). In this paper, the only pruning operation that is onsidered is the repla ement of a subtree by the majority leaf, i.e., a leaf labeled by the majority lass of the examples rea hing it. Hen e, a pruning of a tree is a subtree of the original tree with just zero, one, or more internal nodes hanged into leaves.\nRedu ed Error Pruning (subsequently rep for short) was introdu ed by Quinlan (1987) in the ontext of de ision tree learning. It has subsequently been adapted to rule set learning as well (Pagallo & Haussler, 1990; Cohen, 1993). rep is one of the simplest pruning strategies. In pra ti al de ision tree pruning rep is seldom used, be ause it has the disadvantage of requiring a separate set of examples for pruning. Moreover, it is onsidered too aggressive a pruning strategy that overprunes the de ision tree, deleting relevant parts from it (Quinlan, 1987; Esposito et al., 1997). The need for a pruning set is often onsidered harmful be ause of the s ar eness of the data. However, in the data mining ontext the examples are often abundant and setting a part of them aside for pruning purposes presents no problem.\nDespite its short omings rep is a baseline method to whi h the performan e of other pruning algorithms is ompared (Mingers, 1989a; Esposito, Malerba, & Semeraro, 1993; Esposito et al., 1997). It presents a good starting point for understanding the strengths and weaknesses of the two-phased de ision tree learning and o ers insight to de ision tree pruning. rep has the advantage of produ ing the smallest pruning among those that are the most a urate with respe t to the pruning set. Re ently, Oates and Jensen (1999) analyzed rep in an attempt to explain why and when de ision tree pruning fails to ontrol the growth of the tree, even though the data do not warrant the in reased size. We approa h the same subje t, but try to avoid restri ting the analysis with unne essary assumptions. We also\nonsider an explanation for the unwarranted growth of the size of the de ision tree.\nIn this paper we analyze rep in three di erent settings. First, we explore the basi algorithmi properties of rep, whi h apply regardless of the distribution of examples presented to the learning algorithm. Se ond, we study, in a probabilisti setting, the situation in whi h the attribute values are independent of the lassi ation of an example. Even though this pure noise tting situation is not expe ted to arise when the whole pruning set is onsidered, it is en ountered at lower levels of the tree, when all relevant attributes have already been exhausted. We further assume that all subtrees re eive at least one pruning example, so that\nnone of them an be dire tly pruned due to not re eiving any examples. The lass value is also assigned at random to the pruning examples. In our third analysis it is assumed that ea h pruning example has an equal han e to end up in any one of the subtrees of the tree being pruned. This rather theoreti al setting lets us take into a ount those subtrees that do not re eive any examples. They have been left without attention in earlier analyses.\nThe rest of this paper is organized as follows. The next se tion dis usses the di erent versions of the rep algorithm and xes the one that is analyzed subsequently. In Se tion 3 we review earlier analyses of rep. Basi algorithmi properties of rep are examined in Se tion 4. Then, in Se tion 5, we arry out a probabilisti analysis of rep, without making any assumptions about the distribution of examples. We derive a bound for the pruning probability of a tree whi h depends exponentially on the relation of the number of pruning examples and the size of the tree. Se tion 6 presents an analysis, whi h assumes that the pruning examples distribute uniformly to the subtrees of the tree. This assumption lets us sharpen the pre eding analysis on ertain aspe ts. However, the bounds of Se tion 5 hold with ertainty, while those of Se tion 6 are approximate results. Further related resear h is brie y reviewed in Se tion 7 and, nally, in Se tion 8 we present the on luding remarks of this study."
    }, {
      "heading" : "2. Redu ed Error Pruning Algorithm",
      "text" : "rep was never introdu ed algorithmi ally by Quinlan (1987), whi h is a sour e of mu h\nonfusion. Even though rep is onsidered and appears to be a very simple, almost trivial, algorithm for pruning, there are many di erent algorithms that go under the same name. No onsensus exists whether rep is a bottom-up algorithm or an iterative method. Neither is it obvious whether the training set or pruning set is used to de ide the labels of the leaves that result from pruning."
    }, {
      "heading" : "2.1 High-Level Control",
      "text" : "Quinlan's (1987, p. 225 226) original des ription of rep does not learly spe ify the pruning algorithm and leaves room for interpretation. It in ludes, e.g., the following hara terizations.\nFor every non-leaf subtree S of T we examine the hange in mis lassi ations over the test set that would o ur if S were repla ed by the best possible leaf. If the new tree would give an equal or fewer number of errors and S ontains no subtree with the same property, S is repla ed by the leaf. The pro ess ontinues until any further repla ements would in rease the number of errors over the test set.\n[...℄ the nal tree is the most a urate subtree of the original tree with respe t to the test set and the smallest tree with that a ura y.\nQuinlan (1987, p. 227) also later ontinues to give the following des ription.\nThis method [pessimisti pruning℄ has two advantages. It is mu h faster than either of the pre eding methods [ ost- omplexity and redu ed error pruning℄ sin e ea h subtree is examined at most on e.\nOn one hand this des ription requires the nodes to be pro essed in a bottom-up manner, sin e subtrees must be he ked for the same property before pruning a node but, on the other hand, the last quotation would indi ate rep to be an iterative method. We take rep to have the following single-s an bottom-up ontrol strategy like in most other studies (Oates & Jensen, 1997, 1998, 1999; Esposito et al., 1993, 1997; Kearns & Mansour, 1998).\nNodes are pruned in a single bottom-up sweep through the de ision tree, pruning ea h node is onsidered as it is en ountered. The nodes are pro essed in postorder.\nBy this order of node pro essing, any tree that is a andidate for pruning itself annot\nontain a subtree that ould still be pruned without in reasing the tree's error.\nDue to the ambiguity of rep's de nition, a di erent version of rep also lives on (Mingers, 1989a; Mit hell, 1997). It is probably due to Mingers' (1989) interpretation of Quinlan's ambiguous de nition.\nNodes are pruned iteratively, always hoosing the node whose removal most in reases the de ision tree a ura y over the pruning set. The pro ess ontinues until further pruning is harmful.\nHowever, this algorithm appears to be in orre t. Esposito et al. (1993, 1997) have shown that a tree produ ed by this algorithm does not meet the obje tive of being the most a urate subtree with respe t to the pruning set. Moreover, this algorithm overlooks the expli it requirement of he king whether a subtree would lead to redu tion of the lassi ation error.\nOther iterative algorithms ould be indu ed from Quinlan's original des ription. However, if the expli it requirement of he king whether a subtree ould be pruned before pruning a supertree is obeyed, then these versions of rep will all redu e to the more e ient bottom-up algorithm."
    }, {
      "heading" : "2.2 Leaf Labeling",
      "text" : "Another sour e of onfusion in Quinlan's (1987) des ription of rep is that it is not learly spe i ed how to hoose the labels for the leaves that are introdu ed to the tree through pruning. Oates and Jensen (1999) interpreted that the intended algorithm would label the new leaves a ording to the majority lass of the training examples, but themselves analyzed a version of the algorithm where the new leaves obtain as their labels the majority of the pruning examples. Oates and Jensen motivated their hoi e by the empiri al observation that in pra ti e there is very little di eren e between hoosing the leaf labels in either way. However, hoosing the labels of pruned leaves a ording to the majority of pruning examples will set su h leaves into a di erent status than the original leaves, whi h have as their label the majority lass of training examples.\nExample Figure 1 shows a de ision tree that will be pruned into a single leaf if the training examples are used to label pruned leaves. A negative leaf repla es the root of the tree and makes two mistakes on the pruning examples, while the original tree makes three mistakes. With this tree we an illustrate an important di eren e in using training and\npruning examples to label pruned leaves. Using training examples and pro eeding bottomup, observe that neither subtree is pruned, sin e the left one repla ed with a negative leaf would make two mistakes instead of the original one mistake. Similarly, the right subtree repla ed with a positive leaf would result in an in reased number of lassi ation errors. Nevertheless, the root node even though its subtrees have not been pruned an still be pruned.\nWhen pruning examples are used to label pruned leaves, a node with two non-trivial subtrees annot be pruned unless both its subtrees are ollapsed into leaves. The next se tion will prove this. In the tree of Figure 1 both subtrees would be ollapsed into zeroerror leaves. However, in this ase the root node will not be pruned.\nA further possibility for labeling the leaf nodes would be to take both training and pruning examples into a ount in de iding the label of a pruned leaf. Depending on the relation of the numbers of training and pruning examples this strategy resembles one or the other of the above-des ribed approa hes. Usually the training examples are more numerous than the pruning examples, and will thus dominate. In pra ti e it is impossible to dis ern this labeling strategy from that of using the majority of training examples."
    }, {
      "heading" : "2.3 Empty Subtrees",
      "text" : "Sin e rep uses di erent sets of examples to onstru t and to prune a de ision tree, it is possible that some parts of the tree do not re eive any examples in the pruning phase. Su h parts of the de ision tree, naturally, an be repla ed with a single leaf without hanging the number of lassi ation errors that the tree makes on the pruning examples. In other words, subtrees that do not obtain any pruning examples are always pruned. Quinlan (1987) already noted that the parts of the original tree that orrespond to rarer spe ial ases, whi h are not represented in the pruning set, may be ex ised."
    }, {
      "heading" : "De isionTree REP( De isionTree T, ExampleArray S )",
      "text" : "{ for ( i = 0 to S.length-1 ) lassify( T, S[i℄ );\nIntuitively, it is not lear whi h is the best-founded strategy for handling empty subtrees, those that do not re eive any examples. On one hand they obtain support from the training set, whi h usually is more numerous than the pruning set but, on the other hand, the fa t that no pruning example orresponds to these parts of the tree would justify drawing the\non lusion that these parts of the de ision tree were built by han e properties of the training data. In rep, onsistently with preferring smaller prunings also otherwise, the latter view is adopted.\nThe problem of empty subtrees is onne ted to the problem of small disjun ts in ma hine learning algorithms (Holte, A ker, & Porter, 1989). A small disjun t overs only a small number of the training examples. Colle tively the small disjun ts are responsible for a small number of lassi ation de isions, but they a umulate most of the error of the whole\non ept. Nevertheless, small disjun ts annot be eliminated altogether, without adversely\na e ting other disjun ts in the on ept."
    }, {
      "heading" : "2.4 The Analyzed Pruning Algorithm",
      "text" : "Let us brie y reiterate the details of the rep algorithm that is analyzed subsequently. As already stated, the ontrol strategy of the algorithm is the single-sweep bottom-up pro essing. First, a top-down traversal drives the pruning examples through the tree to the appropriate leaves. The ounters of the nodes en route are updated. Se ond, during a bottom-up traversal the pruning operations indi ated by the lassi ation errors are exe uted. The errors\nan be determined on the basis of the node ounter values. In the bottom-up traversal ea h node is visited only on e. The pruned leaves are labeled by the majority of the pruning set (see Table 1)."
    }, {
      "heading" : "3. Previous Work",
      "text" : "Pruning of de ision trees has re ently re eived a lot of analyti al attention; existing pruning methods have been analyzed (Esposito et al., 1993, 1997; Oates & Jensen, 1997, 1998, 1999) and new analyti ally-founded pruning te hniques have been developed (Helmbold & S hapire, 1997; Pereira & Singer, 1999; Mansour, 1997; Kearns & Mansour, 1998). Also many empiri al omparisons of pruning have appeared (Mingers, 1989a; Malerba, Esposito, & Semeraro, 1996; Frank, 2000). In this se tion we review earlier work that on erns the rep algorithm. Further related resear h is onsidered in Se tion 7.\nEsposito et al. (1993) viewed the rep algorithm, among other pruning methods, as a sear h pro ess in the state spa e. In addition to noting that the iterative version of rep\nannot produ e the optimal result required by Quinlan (1987), they also observed that even though rep is a linear-time algorithm in the size of the tree, with respe t to the height of the tree rep requires exponential time in the worst ase. In their subsequent omparative analysis Esposito et al. (1997) sket hed a proof for Quinlan's (1987) laim that the pruning produ ed by rep is the smallest among the most a urate prunings of the given de ision tree.\nThe bias of rep was brie y examined by Oates and Jensen (1997, 1998). They observed\nthat the error, r\nL\n, of the best majority leaf that ould repla e a subtree T only depends on\n(the lass distribution of) the examples that rea h the root N of T . In other words, the tree stru ture above T and N de ides the error r\nL\n. Let r\nT\ndenote the error of the subtree T at\nthe moment when the pruning sweep rea hes N ; i.e., when some pruning may already have taken pla e in T . All pruning operations performed in T have led either r\nT\nto de rease from\nthe initial situation or to stay un hanged. In any ase, pruning that has taken pla e in T potentially de reases r\nT\n, but does not a e t r\nL\n. Hen e, the probability that r\nT\n< r\nL\ni.e.,\nthat T will not be pruned in reases through pruning in T . This error propagation bias is inherent to rep. Oates and Jensen (1997, 1998) onje ture that the larger the original tree and the smaller the pruning set, the larger this e e t, be ause a large tree provides more pruning opportunities and the high varian e of a small pruning set o ers more random\nhan es for r\nL\nr\nT\n. Subsequently we study some of these e e ts exa tly.\nIn a follow-up study Oates and Jensen (1999) used rep as a vehi le for explaining the problems that have been observed in the pruning phase of top-down indu tion of de ision trees. They analyzed rep in a situation in whi h the de ision node under onsideration ts noise i.e., when the lass of the examples is independent of the value of the attribute tested in the node at hand and built a statisti al model of rep in this situation. It indi ates,\nonsistently with their earlier onsiderations, that even though the probability of pruning a node that ts noise prior to pruning beneath it is lose to 1, pruning that o urs beneath the node redu es its pruning probability lose to 0. In parti ular, this model shows that if even one des endant of node N at depth d is not pruned, then N will not be pruned (assuming there are no leaves until depth d+1). The onsequen e of this result is that in reasing depth d leads to an exponential de rease of the node's pruning probability.\nThe rst part of Oates and Jensen's (1999) analysis is easy to omprehend, but its signifi an e is un ertain, be ause this situation does not rise in any bottom-up pruning strategy. The statisti al model is based on the assumption that the number, n, of pruning instan es that pass through the node under onsideration is large, in whi h ase independen e assumptions prevailing the errors ommitted by the node an be approximated by the normal distribution. The expe ted error of the original tree is the mean of the distribution, while, if pruned to a leaf, the tree would mis lassify a proportion of the n examples that orresponds to that of the minority lass. Oates and Jensen show that the latter number is always less than the mean of the standard distribution of errors. Hen e, the probability of pruning is over 0.5 and approa hes 1 as n grows.\nIn the se ond part of the analysis, in onsidering the pruning probability of a node N after pruning has taken pla e beneath it, Oates and Jensen assume that the proportion of positive examples in any des endant of N at depth d is the same as in N . In this setting, assuming further that N has a positive majority, all its des endants at level d also have a positive majority. It dire tly follows that if all des endants at level d are pruned, they are all repla ed by a positive leaf. Hen e, the fun tion represented by this pruning is identi ally positive. The majority leaf that would repla e N also represents the same fun tion and is smaller than the above pruning. Therefore, rep will hoose the single leaf pruning. On the other hand, if one or more of the des endants of N at depth d are not pruned, then the pruning of the tree rooted at N , in whi h these subtrees are maintained and all other nodes at level d are pruned into positive leaves, is more a urate than the majority leaf. In this\nase the tree will not be pruned.\nOates and Jensen (1999) also assume that starting from any node at level d the probability of routing an example to a positive leaf is the same. In the following analyses we try to rid all unne essary assumptions; the same results an be obtained without any knowledge of the example distribution.\n4. Basi Properties of rep\nBefore going to the detailed probabilisti analysis of the rep algorithm, we examine some of its basi algorithmi properties. Throughout this paper we review the binary ase for simpli ity. The results, however, also apply with many-valued attributes and several lasses.\nNow that the pro essing ontrol of the rep algorithm has been settled, we an a tually prove Quinlan's (1987) laim of the optimality of the pruning produ ed by rep. Observe that the following result holds true independent of the leaf labeling strategy.\nTheorem 1 Applying rep with a set of pruning examples, S, to a de ision tree T produ es T 0 a pruning of T su h that it is the smallest of those prunings of T that have minimal error with respe t to the example set S.\nProof We prove the laim by indu tion over the size of the tree. Observe that a de ision tree T is a full binary tree, whi h has 2L(T ) 1 nodes, where L(T ) is the number of leaves in the tree.\nBase ase. If L(T ) = 1, then the original tree T onsists of a single leaf node. T is the only possible pruning of itself. Thus, it is, trivially, also the smallest among the most a urate prunings of T .\nIndu tive hypothesis. The laim holds when L(T ) < k.\nIndu tive step. L(T ) = k. Let N be the root of the tree and T\n0\nand T\n1\nthe left and the\nright subtree, respe tively. Subtrees T\n0\nand T\n1\nmust have stri tly less than k leaves. When\nthe pruning de ision for N is taken, then by the bottom-up re ursive ontrol strategy of rep T\n0\nand T\n1\nhave already been pro essed by the algorithm. By the indu tive hypothesis,\nthe subtrees after pruning, T\n0 0 and T 0 1 , are the smallest possible among the most a urate\nprunings of these trees.\n(i): A ura y. The pruning de ision for the node N onsists of hoosing whether to ollapse\nN and the tree rooted at it into a majority leaf, or whether to maintain the whole tree. If both alternatives make the same number of errors, then N is ollapsed and the original a ura y with respe t to the pruning set is retained. Otherwise, by the rep algorithm, the pruning de ision is based on whi h of the resulting trees would make less errors with respe t to the pruning set S. Hen e, whi hever hoi e is made, the resulting tree T 0 will make the smaller number of errors with respe t to S.\nLet us now assume that a pruning T\n00\nof T makes even less errors with respe t to S\nthan T\n0\n. Then T\n00\nmust onsist of the root N and two subtrees T\n00 0 and T 00 1 , be ause the\nmajority leaf annot be more a urate than T\n0\n. Sin e T\n00\nis a more a urate pruning of\nT than T\n0\n, it must be that either T\n00 0 is a more a urate pruning of T 0 than T 0 0 or T 00 1 is\na more a urate pruning of T\n1\nthan T\n0 1 . By the indu tive hypothesis both possibilities\nare false. Therefore, T\n0\nis the most a urate pruning of T .\n(ii): Size. To see that the hosen alternative is also as small as possible, rst assume that\nT\n0\nonsists of a single leaf. Su h a tree is the smallest pruning of T , and in this ase the\nlaim follows. Otherwise, T\n0\nonsists of the root node N and the two pruned subtrees\nT\n0 0 and T 0 1 . Sin e this tree was not ollapsed, the tree must be more a urate than the\ntree onsisting of a single majority leaf. Now assume that there exists a pruning T of T that is as a urate as T 0 , but smaller. Be ause the majority leaf is less a urate than T 0 , T must onsist of the root node N and two subtrees T\n0\nand T\n1\n. Then, either\nT\n0\nis a smaller pruning of T\n0\nthan T\n0 0 , but as a urate, or T 1 is a smaller pruning of\nT\n1\nthan T\n0 1 , but as a urate. Both ases ontradi t the indu tive hypothesis. Hen e,\nT\n0\nis the smallest among the most a urate prunings of T .\nThus, in any ase, the laim follows for T . 2\nWe onsider next the situation in an internal node of the tree, when the bottom-up pruning sweep rea hes the node. From now on we are ommitted to leaf labeling by the majority of the pruning examples.\nTheorem 2 An internal node, whi h prior to pruning had no leaves as its hildren, will not be pruned by rep if it has a non-trivial subtree when the bottom-up pruning sweep rea hes it.\nProof For an internal node N we have two possible ases in whi h it has non-trivial subtrees; either both its subtrees are non-trivial (non-leaf) or one of them is trivial. Let us review these ases.\nLet r\nT\ndenote the error of (sub)tree T with respe t to the part of the pruning set that\nrea hes the root of T . By r\nL\nwe denote the mis lassi ation rate of the majority leaf L that\nwould repla e T , if T was hosen to be pruned.\nCase I: Let the two subtrees of T , T\n0\nand T\n1\n, be non-trivial. Hen e, both of them have been\nretained when the pruning sweep has passed them. Thus, r\nT\n0\n< r\nL\n0\nand r\nT\n1\n< r\nL\n1\n,\nwhere L\n0\nand L\n1\nare the majority leaves that would repla e T\n0\nand T\n1\n, respe tively,\nif pruned. Be ause r\nT\n= r\nT\n0\n+ r\nT\n1\n, it must be that r\nT\n< r\nL\n0\n+ r\nL\n1\n.\nIf T\n0\nand T\n1\nhave the same the majority lass, then it is also the majority lass of T .\nThen r\nL\n= r\nL\n0\n+ r\nL\n1\n, where L is the majority leaf orresponding to T . Otherwise,\nr\nL\nr\nL\n0\n+ r\nL\n1\n. In any ase, r\nL\nr\nL\n0\n+ r\nL\n1\n. Combining this with the fa t that\nr\nT\n< r\nL\n0\n+ r\nL\n1\nmeans that r\nT\n< r\nL\n. Hen e, T is not pruned.\nCase II: Let T have one trivial subtree, whi h was produ ed by pruning, and one non-\ntrivial subtree. We assume, without loss of generality, that T\n0\nis non-trivial and L\n1\nis\na majority leaf whi h has repla ed T\n1\nin the pruning pro ess. Then, r\nT\n0\n< r\nL\n0\n. Hen e,\nwe have that r\nT\n= r\nT\n0\n+ r\nL\n1\n< r\nL\n0\n+ r\nL\n1\n.\nIn the same way as in the Case I, we an dedu e that r\nL\nr\nL\n0\n+ r\nL\n1\n. Therefore,\nr\nT\n< r\nL\nand T will be retained in the pruned tree.\nT annot be pruned in either ase, and the pruning pro ess an be stopped on the bran h ontaining T unless an original leaf appears along the path from the root to T . 2\nIf node N has an original leaf, then it may be pruned even if the other subtree of N is non-trivial. Also when N has two trivial subtrees, it may be pruned. Whether pruning takes pla e depends on the lass distribution of examples rea hing N and its subtrees.\nIn the analysis of Oates and Jensen (1999) it was shown that the prerequisite for pruning a node N from the tree is that all its des endants at depth d have been pruned. d is the depth just above the rst (original) leaf in the subtree rooted at N . If we apply the above result to this situation, we an orroborate their nding that N will not be pruned if one or more of its des endants at depth d are retained. Applying Theorem 2 re ursively gives the result.\nCorollary 3 A tree T rooted at node N will be retained by rep if one or more of the des endants of N at depth d are not pruned.\nTo avoid the analysis being restri ted by the leaf globally losest to the root, we need to be able to onsider the set of leaves losest to the root on all bran hes of the tree. Let us de ne that the fringe of a de ision tree ontains any node that prior to pruning had a leaf\nas its hild. Furthermore, any node that is in a subtree rooted at a node belonging to the fringe of the tree is also in the fringe. Those nodes not belonging to the fringe make up the interior of the tree. Safe nodes themselves belong to the fringe of the tree, but have their parent in the interior of the tree (see Figure 2). Be ause the fringe of a de ision tree is losed downwards, the safe nodes of a tree orrespond to the leaves of some pruning of it. Observe also that along the path from the root to a safe node there are no leaves. Therefore, if the pruning pro ess ever rea hes a safe node, Theorem 2 applies on the orresponding bran h from there on.\nIf the de ision tree under onsideration will be pruned into a single majority leaf, safe nodes also need to be turned into leaves at some point, not ne essarily simultaneously. If the pruning sweep ontinues to the safe nodes, from then on the question whether a node is pruned is settled solely on the basis of whether all nodes on the path to the root have the same majority lass. The pruning of the whole tree an be hara terized as below.\nLet T be the tree to be pruned and S the set of pruning examples, jSj = n. We assume, without loss of generality, that at least half of the pruning examples are positive. Let p be the proportion of positive examples in S; p 0:5. If T was to be repla ed by a majority leaf, that leaf would have a positive lass label. Under these assumptions we an prove the following.\nTheorem 4 A tree T will be pruned into a single leaf if and only if\nall subtrees rooted at the safe nodes of T are pruned and\nat least as many positive as negative pruning examples rea h ea h safe node in T .\nProof To begin we show that the two onditions are ne essary for the pruning of T . First, we show that if the former ondition is not ful lled, then T annot be pruned into a single leaf. Se ond, we prove that neither will T be pruned if the former ondition holds, but the latter not. Third, we show the su ien y of the onditions; i.e., prove that if they both hold, then T will be pruned into a single leaf.\n(i): Let us rst assume that in T there is a safe node N su h that it will not be pruned. By\nthe de nition of a safe node, the parent P of N originally had no leaves as its hildren. Therefore, by Theorem 2, P will not be pruned. It is easy to see, indu tively, that neither will the root of T be pruned.\n(ii): Let us then assume that all subtrees rooted at safe nodes get pruned and that there are\none or more safe nodes in T into whi h more negative than positive pruning examples fall. Observe that all safe nodes annot be su h. Let us now onsider the pruning of T in whi h the leaves are situated in pla e of the safe nodes; the leaves re eive the same examples as the original safe nodes. Be ause safe nodes are internal nodes, in rep the orresponding pruned leaves are labeled by the majority of the pruning examples. In parti ular, the safe nodes that re eive more negative than positive examples are repla ed by negative leaves. All other leaves are labeled positive. This pruning of the original tree is more a urate than the majority leaf. Hen e, by Theorem 1, rep will not prune T into a single-leaf tree.\n(iii): Let us now assume that all subtrees rooted at the safe nodes of T are pruned and that\nat least as many positive as negative pruning examples rea h ea h safe node. Then all interior nodes must also have a majority of positive pruning examples. Otherwise, there is an interior node N in T that has more negative than positive examples. Thus, at least one of the hildren of N has a majority of negative examples. Carrying the indu tion all the way to the safe nodes shows that no su h node N an exist in T . Hen e, all interior prunings of T represent the same fun tion (identi ally positive) and all of them have the same error with respe t to S. The majority leaf is the unique, smallest of these prunings and will, by Theorem 1, be hosen.\n2\n5. A Probabilisti Analysis of rep\nLet us now turn our attention to the question of what the prerequisites for pruning a de ision tree T into a single majority leaf are. Sin e, by Theorem 1, rep produ es a pruning of T whi h is the most a urate with respe t to the pruning set and su h that it is as small as possible, to show that T does not redu e to a single leaf it su es to nd its pruning that has a better predi tion a ura y on the pruning examples than the majority leaf has.\nIn the following the lass of an example is assumed to be independent of its attribute values. Obviously, if in a de ision tree there is a node where this assumption holds for the examples arriving to it, we would like the pruning algorithm to turn it into a majority leaf. We do not make any assumptions about the de ision tree. However, similar to the analysis of Oates and Jensen (1999), for the obtained bounds to be tight, the shortest path from the root of the tree to a leaf should not be too short."
    }, {
      "heading" : "5.1 Probability Theoreti al Preliminaries",
      "text" : "Let us re all some basi probabilisti on epts and results that are used subsequently. We denote the probability of an event E by PrfEg and its expe tation by EE. A dis rete (integer-valued) random variable X is said to be binomially distributed with parameters n and p, denoted by X B(n; p), if\nPrfX = k g =\nn\nk\n!\np\nk\n(1 p)\nn k\n; k = 0; 1; : : : ; n:\nIfX B(n; p), then its expe ted value or mean is EX = = np, varian e varX = np(1 p), and standard deviation = p np(1 p).\nAn indi ator variable is is a dis rete random variable that takes on only the values 0 and 1. An indi ator variable I is used to denote the o urren e or non-o urren e of an event. If A\n1\n; : : : ; A\nn\nare independent events with PrfA\ni\ng = p and I\nA\n1\n; : : : ; I\nA\nn\nare the respe tive\nindi ator variables, then X =\nP\nn i=1 I A\ni\nis binomially distributed with parameters n and p.\nI\nA\ni\nis alled a Bernoulli random variable with parameter p. The density fun tion f\nX\n: IN ! [0; 1℄ for a dis rete random variable X is de ned as\nf\nX\n(x) = PrfX = x g. The umulative distribution fun tion F\nX\n: IN ! [0; 1℄ for X is\nde ned as F\nX\n(y) = PrfX y g =\nP\nx y\nf\nX\n(x).\nLet X B(n; p) be a random variable with mean = np and standard deviation\n=\np\nnp(1 p). The normalized random variable orresponding to X is\ne\nX =\nX\n:\nBy the entral limit theorem we an approximate the umulative distribution fun tion F\ne X\nof\ne\nX by the normal or Gaussian distribution\nF\ne X\n(y) = Pr\nn\ne\nX y\no\n(y):\nis the umulative distribution fun tion of the bell urve density fun tion e\nx\n2\n=2\n=\np\n2 .\nRespe tively, we an apply the normal approximation to the orresponding random vari-\nable X\nF\nX\n(y) = PrfX y g = F\ne X\ny\ny\n:"
    }, {
      "heading" : "5.2 Bounding the Pruning Probability of a Tree",
      "text" : "Now, the pruning set is onsidered to be a sample from a distribution in whi h the lass attribute is independent of the other attributes. We assume that the lass attribute is distributed a ording to Bernoulli(p) distribution; i.e., the lass is positive with probability p and negative with probability 1 p. We assume that p > 0:5.\nIn the following we will analyze the situation in whi h the subtrees rooted at safe nodes have already been pruned into leaves. We bound the pruning probability of the tree starting from this initial on guration. Sin e the bottom-up pruning may already have ome to a halt before that situation, the following results a tually give too high a probability for pruning. Hen e, the following upper bounds are not as tight as possible.\nWe onsider pruning a de ision tree by rep as a trial whose result is de ided by the set of pruning examples. By Theorem 4 we an approximate the probability that a tree will be pruned into a majority leaf by approximating the probability that all safe nodes get a positive majority or a negative majority. The latter alternative is not very probable under the assumption p > :5. It is safe to assume that it never happens.\nWe an onsider sampling the pruning examples in two phases. First the attribute values are assigned. This de ides the leaf into whi h the example falls. In the se ond phase we independently assign the lass label for the example.\nLet the safe nodes of tree T be Z(T ) = fz\n1\n; : : : ; z\nk\ng and let the number of examples in\nthe pruning set S be jSj = n. The number of pruning examples falling to a safe node z\ni\nis\ndenoted by n\ni\n;\nP\nk i=1 n i = n. For the time being we assume that n i > 0 for all i. The number\nof positive examples falling to safe node z\ni\nis the sum of independent Bernoulli variables\nand, thus, it is binomially distributed with parameters n\ni\nand p. Respe tively, the number\nof negative pruning examples in safe node z\ni\nis X\ni\nB(n\ni\n; 1 p). The probability that there\nis a majority of negative examples in safe node z\ni\nis PrfX\ni\n> n\ni\n=2 g. We an bound this\nprobability from below by using the following inequality (Slud, 1977).\nLemma 5 (Slud's inequality) Let X B(m; q) be a random variable with q 1=2. Then for m(1 q) h mq,\nPrfX h g 1\nh mq\np\nmq(1 q)\n!\n:\nSin e p > :5 and the random variable orresponding to the number of negative examples\nin safe node z\ni\nisX\ni\nB(n\ni\n; 1 p), the rst ondition of Slud's inequality holds. Furthermore,\nto see that ondition m(1 q) h mq holds in safe node z\ni\nsubstitute h = n\ni\n=2, m = n\ni\n,\nand q = 1 p to obtain n\ni\np n\ni\n=2 n\ni\n(1 p). Thus,\nPr X\ni\n>\nn\ni 2\n1\nn\ni\n=2 n\ni\n(1 p)\np\nn\ni\np(1 p)\n!\n= 1\n(p 1=2)n\ni\np\nn\ni\np(1 p)\n!\n: (1)\nAs n\ni\n, the number of pruning instan es rea hing safe node z\ni\n, grows, then the standard\nnormal distribution term in the above bound also grows. Hen e, the bound on the probability that the majority of the pruning examples rea hing z\ni\nis negative is the smaller the more\npruning examples rea h it. The probability of a negative majority also redu es through the growing probability of positive lass for an example, p. These both are also re e ted in the pruning probabilities of the whole tree.\nWe an now roughly approximate the probability that T will be pruned into a single majority leaf as follows. By Theorem 4, T will be pruned into a leaf if and only if ea h safe node in T re eives a majority of positive examples. Be ause T has k safe nodes and there are n pruning examples, then a ording to the pigeon-hole prin iple at least half of the safe nodes re eive at most r = 2n=k examples. Ea h safe node z\ni\nwith n\ni\nr examples has, by\nInequality 1, a negative majority at least with probability\n1\n(p 1=2)r\np\nrp(1 p)\n!\n:\nObserve that Inequality 1 also holds when n\ni\n< r, be ause the umulative distribution\nfun tion is an in reasing fun tion. The argument n\ni\n(p 1=2)=\np\nn\ni\np(1 p) an be rewritten\nas\np\nn\ni p\n, where\np\nis a positive onstant depending on the value of p. Sin e (\np\nn\ni p\n) grows\nas n\ni\ngrows, 1 (\np\nn\ni p\n) grows with de reasing n\ni\n. Hen e, the lower bound of Inequality\n1 also applies for values 0 < n\ni\n< r.\nThus, the probability that the half of the safe nodes that re eive at most r examples\nhave a positive majority is at most\n(p 1=2)r\np\nrp(1 p)\n!!\nk=2\n: (2)\nThis is an upper bound for the probability that the whole tree T will be pruned into a single leaf. The only distribution assumption that was made to rea h the result is that p > :5. In order to obtain tighter bounds, one has to make assumptions about the shape of the tree T and the distribution of examples.\nThe bound of Equation 2 depends on the size of the de ision tree (re e ted by k), the number (n) and the lass distribution (p) of the pruning examples. Keeping other parameters\nonstant and letting k grow redu es the pruning probability exponentially. If the number of pruning examples grows in the same proportion so that r = 2n=k stays onstant, the pruning probability still falls exponentially. Class distribution of the pruning examples also a e ts the pruning probability whi h is the smaller, the loser p is to value .5."
    }, {
      "heading" : "5.3 Impli ations of the Analysis",
      "text" : "It has been empiri ally observed that the size of the de ision tree grows linearly with the training set size, even when the trees are pruned (Catlett, 1991; Oates & Jensen, 1997, 1998). The above analysis gives us a possibility to explain this behavior. However, let us\nrst prove that when there is no orrelation between the attribute values and the lass label of an example, the size of the tree that perfe tly ts the training data depends linearly on the size of the sample.\nOur setting is as simple as an be. We only have one real-valued attribute x and the lass attribute y, whose value is independent of that of x. As before, y has two possible values, 0 and 1. The tree is built using binary splits of a numeri al value range; i.e., propositions of type x < r are assigned to the internal leaves of the tree. In this analysis dupli ate instan es o ur with probability 0.\nTheorem 6 Let the training examples (x; y) be drawn from a distribution, where x is uniformly distributed in the range [0; 1) and y obtains value 1, independent of x, with probability p, and value 0 with probability 1 p. Then the expe ted size of the de ision tree that ts the data is linear in the size of the sample.\nProof Let S = h(x\n1\n; y\n1\n); : : : ; (x\nt\n; y\nt\n)i be a sample of the above des ribed distribution. We\nmay assume that x\ni\n6= x\nj\n, when i 6= j, be ause the probability of the omplement event is 0.\nLet us, further, assume that the examples of S have been indexed so that x\n1\n< x\n2\n< : : : < x\nt\n.\nLet A\ni\nbe the indi ator variable for the event that instan es i and i+ 1 have di erent lass\nlabels; i.e., y\ni\n6= y\ni+1\n, 1 i t 1. Then EA\ni\n= PrfA\ni\n= 1 g = p(1 p)+(1 p)p = 2p(1 p),\nbe ause when the event y\ni\n= 1 has probability p, at the same time the event y\ni+1\n= 0 has\nprobability 1 p, and vi e versa. Now the number of lass alternations is A =\nP\nt 1 i=1 A i and\nits expe tation is\nEA =\nt 1 X\ni=1\nEA\ni\n=\nt 1 X\ni=1\n2p(1 p) = 2p(1 p)\nt 1 X\ni=1\n1 = 2(t 1)p(1 p): (3)\nLet T be a de ision tree that has been grown on the sample S. The growing has been ontinued until the training error is 0. Ea h leaf in T orresponds to a half open interval\n[a; b) in [0; 1). If y\ni\n6= y\ny+1\n, then x\ni\nand x\ni+1\nmust fall into di erent leaves of T , be ause\notherwise one or the other example is falsely lassi ed by T . Thus, the upper boundary b of the interval orresponding to the leaf into whi h x\ni\nfalls in must have a value less than x\ni+1\n.\nRepetitively applying this observation when s anning through the examples from left to right, we see that T must at least have one leaf for x\n1\nand one leaf for ea h lass alternation;\ni.e., A + 1 leaves in total. By using Equation 3 we see that the expe ted number of leaves in T is\nEA+ 1 = 2(t 1)p(1 p) + 1:\nIn parti ular, this is linear in the size of the sample S; jSj = t. 2\nThe above theorem only on erns zero training error trees built in the rst phase of de ision tree indu tion. The empiri al observations of Catlett (1991) and Oates and Jensen (1997, 1998), however, on ern de ision trees that have been pruned in the se ond phase of indu tion. We ome ba k to the topi of pruned trees shortly.\nConsider how rep is used in pra ti e. There is some amount of ( lassi ed) data available from the appli ation domain. Let there be a total of t examples available. Some part of the data is used for tree growing and the remaining portion 1 of it is reserved as the separate pruning set; 0 < < 1. Quite a ommon pra ti e is to use two thirds of the data for growing and one third for pruning or nine tenths for growing and one tenth for pruning when (ten-fold) ross-validation is used. In the de ision tree onstru tion phase the tree is\ntted to the t examples as perfe tly as possible. If we hypothesize that the previous result holds for noisy real-world data sets, whi h by empiri al eviden e would appear to be the\nase, and that the number of safe nodes also grows linearly with the number of leaves, then the tree grown will ontain t safe nodes, where > 0. Sin e the pruning set size also is a linear fra tion of the training set size, the ratio r = 2n=k stays onstant in this setting. Hen e, by Equation 2, the growing data set size for es the pruning probability to zero, even quite fast, be ause the redu tion in the probability is exponential."
    }, {
      "heading" : "5.4 Limitations of the Analysis",
      "text" : "Empty subtrees, whi h do not re eive any pruning examples, were left without attention above; we assumed that n\ni\n> 0 for ea h i. Empty subtrees, however, de isively a e t the\nanalysis; they are automati ally pruned away. Unfortunately, one annot derive a non-trivial upper bound for the number of empty subtrees. In the worst ase all pruning examples are routed to the same safe node, whi h leaves k 1 empty safe nodes to the tree. Subsequently we review the ase where the examples are distributed uniformly to the safe nodes. Then better approximations an be obtained.\nEven though we assume that ea h pruning example is positive with a higher probability than .5, there are no guarantees that the majority of all examples is positive. However, the probability that the majority of all examples hanges is very small, even negligible, by Cherno 's inequality (Cherno , 1952; Hagerup & Rüb, 1990) when the number of pruning examples, n, is high and p is not extremely lose to one half.\nSlud's inequality bounds the probability PrfX h g, but above we used it to bound the probability PrfX > h g. Some ontinuity orre tion ould be used to ompensate this. In pra ti e, the inexa tness does not make any di eren e.\nEven though it would appear that the number of safe nodes in reases in the same proportion as that of leaves when the size of the training set grows, we have not proved this result. Theorem 6 essentially uses leaf nodes, and does not lend itself to modi ation, where safe nodes ould be substituted in pla e of leaves.\nThe relation between the number of safe nodes and leaves in a de ision tree depends on the shape of the tree. Hen e, the splitting riterion that was used in tree growing de isively a e ts this relation. Some splitting riteria aim at keeping the produ ed split as balan ed as possible, while others aim at separating small lass oherent subsets from the data (Quinlan, 1986; Mingers, 1989b). For example, the ommon entropy-based riteria have a bias that favors balan ed splits (Breiman, 1996). Using a balan ed splitting riterion would seem to imply that the number of safe nodes in a tree depends linearly on the number of leaves in the tree. In that ase the above reasoning would explain the empiri ally observed linear growth of pruned de ision trees."
    }, {
      "heading" : "6. Pruning Probability Under Uniform Distribution",
      "text" : "We now assume that all n pruning examples have an equal probability to end up in ea h of the k safe nodes; i.e., a pruning example falls to the safe node z\ni\nwith probability 1=k.\nContrary to the normal uniform distribution assumption analysis, for our analysis this is not the best ase. Here the best distribution of examples into safe nodes would have one pruning example in ea h of the safe nodes ex ept one, into whi h all remaining pruning instan es would gather. Nevertheless, the uniformity lets us sharpen the general approximation by using standard te hniques.\nThe expe ted number of examples falling into any safe node is n=k. Let us al ulate the expe ted number of those safe nodes that re eive at most n=k examples, where is an arbitrary positive onstant. Let Q\ni\nbe the indi ator for the event safe node z\ni\nre eives at\nmost n=k examples. Then Q =\nP\nk i=1 Q i is the number of those safe nodes that re eive less\nthan n=k examples. By the linearity of expe tation EQ =\nP\nk i=1 EQ i = kEQ 1 , in whi h\nthe last equality follows from the fa t that the Q\ni\n-s are identi ally distributed.\nLet Y\n1\nbe the number of examples rea hing safe node z\n1\n. Be ause ea h of the n exam-\nples rea hes z\n1\nwith probability 1=k independent of the other examples, Y\n1\nis binomially\ndistributed with parameters n and 1=k. Clearly EQ\n1\n= PrfY\n1\nn=k g. We an approxi-\nmate the last probability by the normal approximation, from whi h we obtain\nPr Y\n1\nn\nk\nn=k n=k\np\nn 1=k (1 1=k)\n!\n=\n( 1)n=k\np\nn=k(1 1=k)\n!\n:\nHen e, by the above observation,\nEQ = kEQ\n1\nk\n( 1)n=k\np\nn=k(1 1=k)\n!\n: (4)\nWe now use Approximation 4 to determine the probability that the whole de ision tree T will be pruned into a single leaf. Let P be a random variable that represent the number of those safe nodes in T that re eive at most n=k examples and at least one example. If we denote by R the number of empty safe nodes, we have P = Q R. Hen e, EP = E(Q R) = EQ ER.\nThe following result (Kamath, Motwani, Palem, & Spirakis, 1994; Motwani & Raghavan,\n1995) lets us approximate the number of empty safe nodes when n k.\nTheorem 7 Let Z be the number of empty bins when m balls are thrown randomly into h bins. Then\n= EZ = h 1\n1\nh\nm\nhe\nm=h\nand for > 0,\nPrf jZ j g 2 exp\n2\n(h 1=2)\nh\n2\n2\n!\n:\nBy this result the expe ted number of empty safe nodes is approximately ke\nn=k\n; this\nnumber is small when k is relatively small ompared to n.\nSubstituting the above obtained approximation for EQ (Equation 4) and using the pre-\nvious result, we get\nEP = EQ ER k\n( 1)n=k\np\nn=k(1 1=k)\n!\ne\nn=k\n!\n:\nApplying Slud's inequality we an, as before, bound from above the probability that the majority lass does not hange in a safe node that re eives n=k pruning examples. Sin e there are P su h safe nodes and the lass distribution of examples within them is independent, the event majority lass does not hange in any safe node that re eives at least one and at most n=k examples has the upper bound\n(p :5)r\np\nrp(1 p)\n!!\nP\n; (5)\nwhere r = n=k. Repla ing P with its expe ted value in this equation we have an approximation for the pruning probability. This approximation is valid if P does not deviate a lot from its expe ted value. We onsider the deviation of P from its expe ted value below.\nThe above upper bound for the pruning probability is similar to the upper bound that was obtained without any assumptions about the distribution of the examples. However, the earlier onstant 2 has been repla ed by a new, ontrollable parameter , and empty subtrees are now expli itly taken into a ount. If is hosen suitably, this upper bound is more stri t than the one obtained in the general ase."
    }, {
      "heading" : "6.1 An Illustration of the Upper Bound",
      "text" : "Figure 3 plots the upper bound of the pruning probability of a tree with 100 safe nodes when 500 pruning examples are used. The value of the parameter varies from 0 to 2 and p varies from 0.5 to 1. We an observe that the surfa e orresponding to the upper bound stays very\nlose to 0 when the lass distribution is not too skewed and when the parameter does not have a very small value. When the probability of an example having a positive lass label hits value 0.75 or the value of approa hes 0, the upper bound limbs very steeply. At least on the part of the parameter this is due to the inexa tness of the approximation on the extreme values.\nWhen the probability p that an example has a positive lass approa hes 1, the error ommitted by a single positive leaf falls to 0. Hen e, the a ura y of a non-trivial pruning has to be better, the loser p is to 1 for it to beat the majority leaf. Intuitively, the probability that su h a pruning exists i.e., that the root node is not pruned should drop to zero as p in reases. The bound re e ts this intuition.\nWhen the value of parameter falls lose to 0, the safe nodes that are taken into a ount in the upper bound only re eive very few pruning examples. The number of su h nodes is\nsmall. On the other hand, when is in reased, the number of nodes under onsideration grows together with the upper limit on the number of examples rea hing ea h single one of them. Thus, both small and large values of yield loose bounds. In the stri test bounds the value of is somewhere in the middle, in our example around values 1.0 1.5. In the bound of Equation 5 the argument of the umulative distribution fun tion tends towards zero when the value of is very small, but at the same time the exponent de reases. The value of approa hes 1/2, when its argument goes to zero. On the other hand, when has a large value, approa hes value 1 and the exponent P also in reases."
    }, {
      "heading" : "6.2 On the Exa tness of the Approximation",
      "text" : "Above we used the expe ted value of P in the analysis; EP = EQ ER. We now probe into the deviation of P from its expe ted value. The deviation of R is dire tly available from Theorem 7:\nPrf jR ERj g 2 exp\n2\n(k 1=2)\nk\n2\nE\n2\nR\n!\n:\nFor Q we do not have a similar result yet. In this se tion we provide one.\nLet us rst re apitulate the de nition of the Lips hitz ondition.\nDe nition Let f : D\n1\nD\nm\n! IR be a real-valued fun tion with m arguments from\npossibly distin t domains. The fun tion f is said to satisfy the Lip hitz ondition if for any x\n1\n2 D\n1\n; : : : ; x\nm\n2 D\nm\n, any i 2 f1; : : : ;mg, and any y\ni\n2 D\ni\n,\njf(x\n1\n; : : : ; x\ni 1\n; x\ni\n; x\ni+1\n; : : : ; x\nm\n) f(x\n1\n; : : : ; x\ni 1\n; y\ni\n; x\ni+1\n; : : : ; x\nm\n)j 1:\nHen e, a fun tion satis es the Lips hitz ondition if an arbitrary hange in the value of\nany one argument does not hange the value of the fun tion more than 1.\nThe following result (M Diarmid, 1989) holds for fun tions satisfying the Lips hitz ondition. More general results of the same kind an be obtained using martingales (see e.g., (Motwani & Raghavan, 1995)).\nTheorem 8 (M Diarmid) Let X\n1\n; : : : ;X\nm\nbe independent random variables taking values\nin a set V . Let f : V\nm\n! IR be su h that, for i = 1; : : : ;m:\nsup\nx\n1\n;:::;x\nm\n;y\ni\n2V\njf(x\n1\n; : : : ; x\ni 1\n; x\ni\n; x\ni+1\n; : : : ; x\nm\n) f(x\n1\n; : : : ; x\ni 1\n; y\ni\n; x\ni+1\n; : : : ; x\nm\n)j\ni\n:\nThen for > 0,\nPrf jf(X\n1\n; : : : ;X\nm\n) Ef(X\n1\n; : : : ;X\nm\n)j g 2 exp\n2\n2\nP\nm i=1 2\ni\n!\n:\nLet W\ni\n, i = 1; : : : ; n, be a random variable su h that W\ni\n= j if the i-th example is\ndire ted to the safe node z\nj\n. By the uniform distribution assumption W\ni\n-s are independent.\nThey have their values within the set f1; : : : ; kg. Let us de ne the fun tion f so that f(w\n1\n; : : : ; w\nn\n) is the number of those safe nodes that re eive at most r = n=k examples,\nwhen the i-th example is dire ted to the safe node z\nw\ni\n. That is,\nf(w\n1\n; : : : ; w\nn\n) = jf i 2 f 1; : : : ; k g j jS\ni\nj r gj;\nwhere S\ni\nis the set of those examples that are dire ted to safe node z\ni\n;\nS\ni\n= fh 2 f 1; : : : ; n g j w\nh\n= i g:\nHen e, Q = f(W\n1\n; : : : ;W\nn\n). Moving any one example from one safe node to another ( hang-\ning the value of any one argument w\ni\n), an hange one more safe node z\ni\nto ful ll the on-\ndition jS\ni\nj r, one less safe node to ful ll it, or both at the same time. Thus, the value\nof f hanges by at most 1. Hen e, the fun tion ful lls the Lips hitz ondition. Therefore, we an apply M Diarmid's inequality to it by substituting\ni\n= 1 and observing that then\nP\nn i=1 2\ni\n= n:\nPrf jf(W\n1\n; : : : ;W\nn\n) Ef(W\n1\n; : : : ;W\nn\n)j g 2e\n2\n2\n=n\n;\nor equally\nPrf jQ EQj g 2e\n2\n2\n=n\n:\nUnfortunately, this on entration bound is not very tight. Nevertheless, ombining the on entration bounds for Q and R we have for P the following deviation from its expe ted\nvalue.\nSin e jP EP j = jQ R E(Q R)j = jQ EQ+ER Rj jQ EQj+ jR ERj,\njQ R E(Q R)j implies that jQ EQj =2 or jR ERj =2. Thus,\nPrf jP EP j g = Prf jQ R E(Q R)j g\nPr jQ EQj\n2\n+Pr jR ERj\n2\n2 exp\n2\n2n\n!\n+ 2 exp\n2\n(k 1=2)\n4(k\n2\nE\n2\nR)\n!\n:"
    }, {
      "heading" : "7. Related Work",
      "text" : "Traditional pruning algorithms like ost- omplexity pruning (Breiman et al., 1984), pessimisti pruning (Quinlan, 1987), minimum error pruning (Niblett & Bratko, 1986; Cestnik & Bratko, 1991), riti al value pruning (Mingers, 1989a), and error-based pruning (Quinlan, 1993) have already been overed extensively in earlier work (Mingers, 1989a; Esposito et al., 1997; Frank, 2000). Thus we will not tou h on these methods any further. Instead, we review some of the more re ent work on pruning.\nrep produ es an optimal pruning of the given de ision tree with respe t to the pruning set. Other approa hes for produ ing optimal prunings have also been presented (Breiman et al., 1984; Bohane & Bratko, 1994; Oliver & Hand, 1995; Almuallim, 1996). However, often optimality is measured over the training set. Then it is only possible to maintain the initial a ura y, assuming that no noise is present. Neither is it usually possible to redu e the size of the de ision tree without sa ri ing the lassi ation a ura y. For example, in the work of Bohane and Bratko (1994) it was studied how to e iently nd the optimal pruning in the sense that the output de ision tree is the smallest pruning whi h satis es a given a ura y requirement. A somewhat improved algorithm for the same problem was presented subsequently by Almuallim (1996).\nThe high level ontrol of Kearns and Mansour's (1998) pruning algorithm is the same bottom-up sweep as in rep. However, the pruning riterion in their method is a kind of a\nost- omplexity ondition (Breiman et al., 1984) that takes both the observed lassi ation error and (sub)tree omplexity into a ount. Moreover, their pruning s heme does not require the pruning set to be separate from the training set. Both Mansour's (1997) and Kearns and Mansour's (1998) algorithms are pessimisti : they try to bound the true error of a (sub)tree by its training error. Sin e the training error is by nature optimisti , the pruning riterion has to ompensate it by being pessimisti about the error approximation.\nConsider yet another variant of rep, one whi h is otherwise similar to the one analyzed above, with the ex eption that the original leaves are not put to a spe ial status, but an be relabeled by the majority of the pruning examples just like internal nodes. This version of rep produ es the optimal pruning with respe t to whi h the performan e of Kearns and Mansour's (1998) algorithm is measured. Their pessimisti pruning produ es a de ision tree that is smaller than that produ ed by rep.\nKearns and Mansour (1998) are able to prove that their algorithm has a strong performan e guarantee. The generalization error of the produ ed pruning is bounded by that of the best pruning of the given tree plus a omplexity penalty. The pruning de isions are lo al in the same sense as those of rep and only the basi pruning operation of repla ing a subtree with a leaf is used in this pruning algorithm."
    }, {
      "heading" : "8. Con lusion",
      "text" : "In this paper the rep algorithm has been analyzed in three di erent settings. First, we studied the algorithmi properties of rep alone, without assuming anything about the input de ision tree nor pruning set. In this setting it is possible to prove that rep ful lls its intended task and produ es an optimal pruning of the given tree. The algorithm pro eeds to prune the nodes of a bran h as long as both subtrees of an internal node are pruned and stops immediately if even one subtree is kept. Moreover, it prunes an interior node only if all its des endants at level d have been pruned. Furthermore, rep either halts before the safe nodes are rea hed or prunes the whole tree only in ase all safe nodes have the same majority lass.\nIn the se ond setting the tree under onsideration was assumed to t noise; i.e., it was assumed that the lass label of the pruning examples is independent of their attribute values. In this setting the pruning probability of the tree ould be bound by an equation that depends exponentially on the size of the tree and linearly on the number and lass distribution of the pruning examples. Thus, our analysis orroborates the main nding of Oates and Jensen (1999) that rep fails to ontrol the growth of a de ision tree in the extreme\nase that the tree ts pure noise. Moreover, our analysis opened a possibility to initially explain why the learned de ision tree grows linearly with an in reasing data set. Our bound on the pruning probability of a tree is based on bounding the probability that all safe nodes have the same majority lass. Surprisingly, essentially the same property, whose probability we try to bound lose to 0, is assumed to hold with probability 1 in the analysis of Oates and Jensen (1999).\nIn rep it may happen that no pruning examples are dire ted to a given subtree. Su h subtrees have not been taken into a ount in earlier analyses. In our nal analysis we\nin luded empty subtrees in the equation for a tree's pruning probability. Taking empty subtrees into a ount gives a more realisti bound for the pruning probability of a tree.\nUnfortunately, one annot draw very de nite general on lusions on the two-phased topdown indu tion of de ision trees on the basis of analyses on the rep algorithm, be ause its bias is quite unique among pruning algorithms. The fa t that rep does not penalize the size of a tree, but only rests on the lassi ation error on the pruning examples makes the method sensitive to small hanges in the lass distribution of the pruning set. Other de ision tree pruning algorithms also have their individual hara teristi s. Therefore, uni ed analysis of de ision tree pruning may be impossible.\nThe version of rep, in whi h one is allowed to relabel original leaves, as well, is used as the performan e obje tive in Kearns and Mansour's (1998) pruning algorithm. Thus, the performan e of pruning algorithms that use both error and size penalty is related to those that use only error estimation. In the version of rep used by Kearns and Mansour our analysis based on safe nodes applies with leaves in pla e of safe nodes. Hen e for this algorithm the derived bounds are stri ter.\nWe leave the detailed analysis of other important pruning algorithms as future work. Only through su h investigation is it possible to dis lose the di eren es and similarities of pruning algorithms. Empiri al examination has not managed to reveal lear performan e di eren es between the methods. Also, the relationship of the number of safe nodes and leaves of a tree ought to be examined analyti ally and empiri ally. In parti ular, one should study whether the number of safe nodes does in rease linearly with a growing training set, as onje tured in this paper. Deeper understanding of existing pruning algorithms may help to over ome the problems asso iated with the pruning phase of de ision tree learning.\nReferen es\nAlmuallim, H. (1996). An e ient algorithm for optimal pruning of de ision trees. Arti ial\nIntelligen e, 83, 347 362.\nBohane , M., & Bratko, I. (1994). Trading a ura y for simpli ity in de ision trees. Ma hine\nLearning, 15 (3), 223 250.\nBreiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classi ation and\nRegression Trees. Wadsworth, Pa i Grove, CA.\nBreiman, L. (1996). Some properties of splitting riteria. Ma hine Learning, 24 (1), 41 47.\nCatlett, J. (1991). Overpruning large de ision trees. In Pro eedings of the Twelfth Interna-\ntional Joint Conferen e on Arti ial Intelligen e, pp. 764 769, San Mateo, CA. Morgan Kaufmann.\nCestnik, B., & Bratko, I. (1991). On estimating probabilities in tree pruning. In Kodrato ,\nY. (Ed.), Ma hine Learning EWSL-91: Pro eedings of the Fifth European Working Session, Vol. 482 of Le ture Notes in Arti ial Intelligen e, pp. 138 150, Berlin, Heidelberg, New York. Springer-Verlag.\nCherno , H. (1952). A measure of asymptoti e ien y for tests of a hypothesis based on\nthe sum of observations. Annals of Mathemati al Statisti s, 23 (4), 493 507.\nCohen, W. W. (1993). E ient pruning methods for separate-and- onquer rule learning\nsystems. In Pro eedings of the Thirteenth International Joint Conferen e on Arti ial Intelligen e, pp. 988 994, San Mateo, CA. Morgan Kaufmann.\nEsposito, F., Malerba, D., & Semeraro, G. (1993). De ision tree pruning as a sear h in\nthe state spa e. In Brazdil, P. B. (Ed.), Ma hine Learning: ECML-93, Pro eedings of the Sixth European Conferen e, Vol. 667 of Le ture Notes in Arti ial Intelligen e, pp. 165 184, Berlin, Heidelberg, New York. Springer-Verlag.\nEsposito, F., Malerba, D., & Semeraro, G. (1997). A omparative analysis of methods for\npruning de ision trees. IEEE Transa tions on Pattern Analysis and Ma hine Intelligen e, 19 (5), 476 491.\nFrank, E. (2000). Pruning De ision Trees and Lists. Ph.D. thesis, University of Waikato,\nDepartment of Computer S ien e, Hamilton, New Zealand.\nHagerup, T., & Rüb, C. (1990). A guided tour of Cherno bounds. Information Pro essing\nLetters, 33 (6), 305 308.\nHelmbold, D. P., & S hapire, R. E. (1997). Predi ting nearly as well as the best pruning of\na de ision tree. Ma hine Learning, 27 (1), 51 68.\nHolte, R. C., A ker, L., & Porter, B. (1989). Con ept learning and the problem of small\ndisjun ts. In Pro eedings of the Eleventh International Joint Conferen e on Arti ial Intelligen e, pp. 813 818, San Mateo, CA. Morgan Kaufmann.\nKamath, A., Motwani, R., Palem, K., & Spirakis, P. (1994). Tail bounds for o upan y\nand the satis ability threshold onje ture. In Pro eedings of the Thirty-Fifth Annual IEEE Symposium on Foundations of Computer S ien e, pp. 592 603, Los Alamitos, CA. IEEE Press.\nKearns, M., & Mansour, Y. (1998). A fast, bottom-up de ision tree pruning algorithm with\nnear-optimal generalization. In Shavlik, J. (Ed.), Pro eedings of the Fifteenth International Conferen e on Ma hine Learning, pp. 269 277, San Fran is o, CA. Morgan Kaufmann.\nMalerba, D., Esposito, F., & Semeraro, G. (1996). A further omparison of simpli ation\nmethods for de ision-tree indu tion. In Fisher, D., & Lenz, H.-J. (Eds.), Learning from Data: AI and Statisti s V, pp. 365 374, Berlin, Heidelberg, New York. Springer-Verlag.\nMansour, Y. (1997). Pessimisti de ision tree pruning based on tree size. In Fisher, D. H.\n(Ed.), Pro eedings of the Fourteenth International Conferen e on Ma hine Learning, pp. 195 201, San Fran is o, CA. Morgan Kaufmann.\nM Diarmid, C. J. H. (1989). On the method of bounded di eren es. In Siemons, J. (Ed.),\nSurveys in Combinatori s: Invited Papers of the 12th British Combinatorial Conferen e, pp. 148 188, Cambridge, U.K. Cambridge University Press.\nMingers, J. (1989a). An empiri al omparison of pruning methods for de ision tree indu tion.\nMa hine Learning, 4 (2), 227 243.\nMingers, J. (1989b). An empiri al omparison of sele tion measures for de ision-tree indu -\ntion. Ma hine Learning, 3 (4), 319 342.\nMit hell, T. M. (1997). Ma hine Learning. M Graw-Hill, New York.\nMotwani, R., & Raghavan, P. (1995). Randomized Algorithms. Cambridge University Press,\nNew York.\nNiblett, T., & Bratko, I. (1986). Learning de ision rules in noisy domains. In Bramer, M. A.\n(Ed.), Resear h and Development in Expert Systems III, pp. 25 34, Cambridge, UK. Cambridge University Press.\nOates, T., & Jensen, D. (1997). The e e ts of training set size on de ision tree omplexity.\nIn Fisher, D. H. (Ed.), Pro eedings of the Fourteenth International Conferen e on Ma hine Learning, pp. 254 261, San Fran is o, CA. Morgan Kaufmann.\nOates, T., & Jensen, D. (1998). Large datasets lead to overly omplex models: An expla-\nnation and a solution. In Agrawal, R., Stolorz, P., & Piatetsky-Shapiro, G. (Eds.), Pro eedings of the Fourth International Conferen e on Knowledge Dis overy and Data Mining, pp. 294 298, Menlo Park, CA. AAAI Press.\nOates, T., & Jensen, D. (1999). Toward a theoreti al understanding of why and when\nde ision tree pruning algorithms fail. In Pro eedings of the Sixteenth National Conferen e on Arti ial Intelligen e, pp. 372 378, Menlo Park, CA/Cambridge, MA. AAAI Press/MIT Press.\nOliver, J. J., & Hand, D. J. (1995). On pruning and averaging de ision trees. In Prieditis, A.,\n& Russell, S. (Eds.), Pro eedings of the Twelfth International Conferen e on Ma hine Learning, pp. 430 437, San Fran is o, CA. Morgan Kaufmann.\nPagallo, G., & Haussler, D. (1990). Boolean feature dis overy in empiri al learning. Ma hine\nLearning, 5 (1), 71 99.\nPereira, F., & Singer, Y. (1999). An e ient extension to mixture te hniques for predi tion\nand de ision trees. Ma hine Learning, 36 (3), 183 199.\nQuinlan, J. R. (1986). Indu tion of de ision trees. Ma hine Learning, 1, 81 106.\nQuinlan, J. R. (1987). Simplifying de ision trees. International Journal of Man-Ma hine\nStudies, 27 (3), 221 248.\nQuinlan, J. R. (1993). C4.5: Programs for Ma hine Learning. Morgan Kaufmann, San\nMateo, CA.\nSlud, E. V. (1977). Distribution inequalities for the binomial law. The Annals of Probability,\n5 (3), 404 412."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "Top-down indu tion of de ision trees has been observed to su er from the inadequate fun tioning of the pruning phase. In parti ular, it is known that the size of the resulting tree grows linearly with the sample size, even though the a ura y of the tree does not improve. Redu ed Error Pruning is an algorithm that has been used as a representative te hnique in attempts to explain the problems of de ision tree learning. In this paper we present analyses of Redu ed Error Pruning in three di erent settings. First we study the basi algorithmi properties of the method, properties that hold independent of the input de ision tree and pruning examples. Then we examine a situation that intuitively should lead to the subtree under onsideration to be repla ed by a leaf node, one in whi h the lass label and attribute values of the pruning examples are independent of ea h other. This analysis is ondu ted under two di erent assumptions. The general analysis shows that the pruning probability of a node tting pure noise is bounded by a fun tion that de reases exponentially as the size of the tree grows. In a spe i analysis we assume that the examples are distributed uniformly to the tree. This assumption lets us approximate the number of subtrees that are pruned be ause they do not re eive any pruning examples. This paper lari es the di erent variants of the Redu ed Error Pruning algorithm, brings new insight to its algorithmi properties, analyses the algorithm with less imposed assumptions than before, and in ludes the previously overlooked empty subtrees to the analysis.",
    "creator" : "dvips(k) 5.86 Copyright 1999 Radical Eye Software"
  }
}