{
  "name" : "1612.08810.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "David Silver", "Hado van Hasselt", "Matteo Hessel", "Tom Schaul", "Arthur Guez", "Tim Harley", "Gabriel Dulac-Arnold", "David Reichert", "Neil Rabinowitz", "Andre Barreto", "Thomas Degris" ],
    "emails" : [ "davidsilver@google.com", "hado@google.com", "mtthss@google.com", "schaul@google.com", "aguez@google.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "The central idea of model-based reinforcement learning is to decompose the RL problem into two subproblems: learning a model of the environment, and then planning with this model. The model is typically represented by a Markov reward process (MRP) or decision process (MDP). The planning component uses this model to evaluate and select among possible strategies. This is typically achieved by rolling forward the model to construct a value function that estimates cumulative reward. In prior work, the model is trained essentially independently of its use within the planner. As a result, the model is not well-matched with the overall objective of the agent. Prior deep reinforcement learning methods have successfully constructed models that can unroll near pixel-perfect reconstructions (Oh et al., 2015; Chiappa et al., 2016); but are yet to surpass state-of-the-art modelfree methods in challenging RL domains with raw inputs (e.g., Mnih et al., 2015; 2016; Lillicrap et al., 2016). In this paper we introduce a new architecture, which we call the predictron, that integrates learning and planning into one end-to-end training procedure. At every step, a model is applied to an internal state, to produce a next state, reward, discount, and value estimate. This model is completely abstract and its only goal is to facilitate accurate value prediction. For example, to plan effectively in a game, an agent must be able to predict the score. If our model makes accurate predictions, then an optimal plan with respect to our model will also be an optimal plan for the underlying game – even if that model uses a different state space (e.g., an abstract representation of enemy positions, ignoring their shapes and colours), action space (e.g., a high-level action to move away from an enemy), rewards (e.g., a single abstract step could have a higher value than any real reward), or even timestep (e.g., a single abstract step could “jump” the agent to the end of a corridor). All we require is that trajectories through the abstract model produce scores that are consistent with trajectories through the real environment. This is achieved by training the predictron end-to-end, so as to make its value estimates as accurate as possible. An ideal model could generalise to many different prediction tasks, rather than overfitting to a single task; and could learn from a rich variety of feedback signals, not just a single extrinsic reward. We therefore train the predictron to predict a host of different value functions for a variety of pseudoreward functions and discount factors. These pseudo-rewards can encode any event or aspect of the environment that the agent may care about, e.g., staying alive or reaching the next room. We focus upon the prediction task: estimating value functions in MRP environments with uncontrolled dynamics. In this case, the predictron can be implemented as a deep neural network with an\n*Primary contributors\nar X\niv :1\n61 2.\n08 81\n0v 1\n[ cs\n.L G\n] 2\n8 D\nec 2\n01 6\nMRP as a recurrent core. The predictron unrolls this core multiple steps and accumulates rewards into an overall estimate of value. We applied the predictron to procedurally generated random mazes, and a simulated pool domain, directly from pixel inputs. In both cases, the predictron significantly outperformed model-free algorithms with conventional deep network architectures; and was much more robust to architectural choices such as depth."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "We consider environments defined by an MRP with states s ∈ S , e.g., a joint configuration of a robot, or a history of raw input sensors. The MRP is defined by a function, s′, r, γ = p(s, α), where s′ is the next state, r is the reward, and γ is the discount factor, which can for instance represent the non-termination probability for this transition. The process may be stochastic, given IID noise α. The return of an MRP is the cumulative discounted reward over a single trajectory, gt = rt+1 + γt+1rt+2 +γt+1γt+2rt+3 + ... , where γt can vary per time-step. We consider a generalisation of the MRP setting that includes vector-valued rewards r, diagonal-matrix discounts γ , and vector-valued returns g; definitions are otherwise identical to the above. We use this bold font notation to closely match the more familiar scalar MRP case; the majority of the paper can be comfortably understood by reading all rewards as scalars, and all discount factors as scalar and constant, i.e., γt = γ. The value function of an MRP p is the expected return from state s, vp(s) = Ep [gt | st = s]. In the vector case, these are known as general value functions (Sutton et al., 2011). We will say that a (general) value function v(·) is consistent with environment p if and only if v = vp which satisfies the following Bellman equation (Bellman, 1957),\nvp(s) = Ep [r + γvp(s′)] . (1)\nIn model-based reinforcement learning (Sutton and Barto, 1998), an approximation m ≈ p to the environment is learned. In the uncontrolled setting this model is normally an MRP s′, r, γ = m(s, β) that maps from state s to subsequent state s′ and additionally outputs rewards r and discounts γ ; the model may be stochastic given an IID source of noise β. A (general) value function vm(·) is consistent with model m (or valid, (Sutton, 1995)), if and only if it satisfies a Bellman equation vm(s) = Em [r + γvm(s′)] with respect to model m. Conventionally, model-based RL methods focus on finding a value function v that is consistent with a separately learned model m."
    }, {
      "heading" : "3 PREDICTRON ARCHITECTURE",
      "text" : "The predictron is composed of three main components. First, a state representation s = f(s) that encodes raw input s (this could be a history of observations, in the partially observed setting, for example when f is a recurrent network) into an abstract (internal, hidden) state s. Second, a model s′, r, γ = m(s, β) that maps from abstract state s to subsequent abstract state s′, rewards r, and discounts γ . Third, a value function v that outputs an estimate v = v(s) of future cumulative discounted rewards from internal state s onwards. The predictron is applied by unrolling its model m multiple “planning” steps to produce rewards, discounts and values. The components of the predictron can be composed together to form many different predictions of the real returns. We use superscripts •k to indicate internal steps of the model (which have no necessary connection to time steps •t of the environment). The k-step predictron return gk (henceforth abbreviated as preturn) is the discounted cumulative reward obtained by taking k model steps, plus an estimated value of future cumulative reward, vk, from the kth step:\ngk = r1 + γ1(r2 + γ2(. . . (rk−1 + γk−1(rk + γkvk)) . . .)). (2)\nThe 0-step preturn is simply the first value g0 = v0. The 1-step preturn is g1 = r1 + γ1v1, and so on (see Fig. 1a). The predictron outputs many preturns gk in a single forward pass, each of which should accurately estimate the target, i.e. the true value of state s. The predictron can be viewed as a function approximator that produces an ensemble of values g0, ...,gK .\nWe now augment the predictron architecture with λ-parameters that aggregate over the k-step preturns using diagonal weight matrices w0, ...,wK , and output a λ-preturn gλ,\ngλ = K∑ k=0 wkgk where wk =  (I− λk) ∏k−1 j=0 λ j if k < K ∏K−1 j=0 λ j otherwise. (3)\nThis λ-preturn is analogous to the λ-return in the forward-view TD(λ) algorithm (Sutton, 1988; Sutton and Barto, 1998). It may also be computed by a backward recursion through intermediate steps gk,λ,\ngk,λ = (I− λk)vk + λk ( rk+1 + γk+1gk+1,λ ) , (4)\nwhere gK,λ = vK , and then using gλ = g0,λ. Computation in the λ-predictron operates in a sweep, iterating first through the predictron network from k = 0 . . .K and then back through the λ-network from k = K . . . 0 in a single “forward” pass of the network (see Figure 1b). Each λk weight acts as a gate on the computation of the λ-preturn: a value of λk = 0 will truncate the λ-preturn at layer k, while a value of λk = I will utilise deeper layers based on additional steps of the model m; the final weight is always λK = 0. The individual λk weights may depend on the corresponding abstract state sk and can differ per prediction. This enables the predictron to compute to an adaptive depth (Graves, 2016) depending on the internal state and learning dynamics of the network."
    }, {
      "heading" : "4 PREDICTRON LEARNING UPDATES",
      "text" : "We now consider how to jointly optimise the parameters θ of all components f,m, v of the predictron. First, we will discuss how to learn from Monte Carlo returns from the real environment. Then, we discuss internal consistency updates that can be applied even in the absence of real data."
    }, {
      "heading" : "4.1 SUPERVISED (MONTE-CARLO) LEARNING WITH THE PREDICTRON",
      "text" : "We can update all the k-step preturns g0, . . . ,gK towards a target outcome g, such as the Monte Carlo return from the outcomes of episodes in the environment, by minimising a mean-squared error\nloss,\nL = 1\n2 K∑ k=0 ∥∥Ep [g]− Em [gk]∥∥2 . (5) This loss depends on the parameters of the value function, model, and state representation parameters, which we together denote θ, and we can use the gradient of L to update these, e.g., by stochastic gradient descent on the sample loss l = 12 ∑K k=0\n∥∥g − gk∥∥2, using the gradient ∂l ∂θ = K∑ k=0 ( g − gk ) ∂gk ∂θ . (6)\nFor stochastic models, two independent samples are required for gk and ∂g k\n∂θ to get unbiased samples for the gradient of loss (5). The λk weights of the λ-preturn are produced by a λ-network with separate parameters η. These weights are adjusted by modifying the parameters of the λ-network so as to minimise a Monte-Carlo loss,\nL = 1\n2 ∥∥Ep [g]− Em [gλ]∥∥2 , ∂l ∂η = ( g − gλ ) ∂gλ ∂η . (7)\nIn summary, the joint parameters θ of the state representation f , the modelm, and the value function v are updated to make each of the k-step preturns gk more similar to the target g, and the parameters η of the λ-network are updated to make the aggregate λ-preturn gλ more similar to the target g."
    }, {
      "heading" : "4.2 CONSISTENCY (SEMI-SUPERVISED) LEARNING WITH THE PREDICTRON",
      "text" : "Ideally, the predictron (f,m, v) learns preturns that are all equal in expectation to the true value function of the environment, Em [ gk|s ] = Ep [gt|st = s] = vp(s), in which case the preturns must\nbe equal in expectation, Em [ g0|s ] = Em [ g1|s ] = ... = Em [ gK |s ] . This may be interpreted as satisfying a Bellman equation, unrolled K times, on the model m. In addition, each k-step preturn must be equal in expectation to the λ-preturn, Em [ gk|s ] = Em [ gλ|s ] , for any λ parameters. All these consistency relations between preturns give rise to additional constraints upon the predictron. Specifically, we may adjust the parameters of the predictron to lead to consistent preturns, even in the absence of labelled targets. Concretely, we can adjust each preturn gk towards the λ-preturn gλ; in other words, we can update each individual value estimate towards the best aggregated estimate by minimizing\nL = 1\n2 K∑ k=0 ∥∥Em [gλ]− Em [gk]∥∥2 , ∂l ∂θ = K∑ k=0 ( gλ − gk ) ∂gk ∂θ . (8)\nHere gλ is considered fixed; the parameters θ are only updated to make gk more similar to gλ, not vice versa. This consistency update does not require any labels g or samples from the environment. As a result, it can be applied to (potentially hypothetical) states that have no associated ‘real’ (e.g. Monte-Carlo) outcome: we update the value estimates to be self-consistent with each other. Note the similarity with the semi-supervised setting, where we may have unlabelled inputs."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We conducted experiments on two domains. The first domain consists of randomly generated 20×20 mazes in which each location either is empty or contains a wall. Two locations in a maze are considered connected if they are both empty and we can reach one from the other by moving horizontally or vertically through adjacent empty cells. The goal is to predict, for each of the locations on the diagonal from top-left to bottom-right of the maze, whether the bottom-right corner is connected to that location, given the entire maze as an input image. Some of these predictions will be straightforward, for instance for locations on the diagonal that contain a wall themselves and for locations close to the bottom right. Many other predictive questions seem to require a simple algorithm, such as some form of a flood fill or search; our hypothesis is that an internal model can learn to emulate such algorithms, where naive approximation may struggle. A few example mazes are shown in Figure 2.\nOur second domain is a simulation of the game of pool, using four balls and four pockets. The simulator is implemented in the physics engine Mujoco (Todorov et al., 2012). We generate sequences of RGB frames starting from a random arrangement of balls on the table. The goal is to simultaneously learn to predict future events for each of the four balls, given 5 RGB frames as input. These events include: collision with any other ball, collision with any boundary of the table, entering a quadrant (×4, for each quadrant), being located in a quadrant (×4, for each quadrant), and entering a pocket (×4, for each pocket). Each of these 14×4 events provides a binary pseudo-reward that we combine with 5 different discount factors {0, 0.5, 0.9, 0.98, 1} and predict their cumulative discounted sum over various time spans. This yields a total of 280 general value functions. An example trajectory is shown in Figure 2. In both domains, inputs are presented as minibatches of i.i.d. samples with their regression targets. Additional domain details are provided in Appendix D."
    }, {
      "heading" : "5.1 EXPLORING THE PREDICTRON ARCHITECTURE",
      "text" : "Our first set of experiments examines three binary dimensions that differentiate the predictron from standard deep networks. We compare eight predictron variants corresponding to the corners of the cube on the left in Figure 3. The first dimension corresponds to whether or not the predictron architecture utilises the structure of an MRP model. In the MRP case, labelled r, γ, internal rewards and discounts are both learned. In the non-r, γ case, which corresponds to a vanilla hidden-to-hidden neural network module, internal rewards and discounts are ignored by fixing their values to rk = 0 and γk = 1. The second dimension is whether or not a λ-network is used to aggregate over preturns. When a λ-network is used, a λ-preturn is computed as described in Section 3. Otherwise, intermediate preturns are ignored by fixing their values to λk = 1 for k < K. In this case, the overall output of the predictron is simply the maximum-depth preturn gK . The third dimension, labelled usage weighting, defines the loss that is used to update the parameters. This loss is combined over the preturns gk at each depth k. These were previously assumed to be uniformly weighted, (Equation 5). But instead they can be weighted according to the amount that preturn is actually used in the λ-predictron’s overall output, i.e., its weight wk. For architectures without a λ-network, wk = 0 for k < K, and wK = 1. Then usage weighting means that we only backpropagate the final loss. All variants utilise a convolutional core with 2 intermediate hidden layers (see Appendix A); parameters were updated by supervised learning (see Appendix B for more details). Root mean squared prediction errors for each architecture, aggregated over all predictions, are shown in Figure 3. The top row corresponds to the random mazes and the bottom row to the pool domain. The main conclusion is that learning an MRP model improved performance greatly. The inclusion of λ weights helped as well, especially on pool. Usage weighting further improved performance."
    }, {
      "heading" : "5.2 COMPARING THE PREDICTRON TO OTHER DEEP NETWORKS",
      "text" : "Our second set of experiments compares the predictron to feedforward and recurrent deep learning architectures, with and without skip connections. We compare the corners of a new cube, as depicted on the left in Figure 4, based on three different binary dimensions. The first dimension of this second cube is whether we use a predictron, or a (non-λ, non-r, γ) deep network that does not have an internal model and does not output or learn from intermediate predictions. We use the most effective predictron from the previous section, i.e., the (r, γ, λ)-predictron with usage weighting.\nr, w e ig h t sh arin g skip connections (r, , )-predictron\nConvNet\nrecurrent ConvNet\nResNet\nrecurrent ResNet\nusage w eighting\n0 1M 2M 3M 4M 5M\n0.0001\n0.001\n0.01\nR M\nS E o\nn r\na n d o m\nm a ze s (l o g s ca le )\nShared core\ndeep net deep net with skips (r, γ, λ)-predictron (r, γ, λ)-predictron with skips\n0 1M 2M 3M 4M 5M\nUnshared cores\n0 500K 1M\nUpdates\n0.2\n0.3\n0.4\nR M\nS E o\nn p\no o l\n0 500K 1M\nUpdates\nFigure 4: Comparing predictron to baselines. Aggregated prediction errors on random mazes (top) and pool (bottom) over all predictions for the eight architectures corresponding to the cube on the left. Each line is the median of RMSE over five seeds; shaded regions encompass all seeds. The full (r, γ, λ)-predictron (red), consistently outperformed conventional deep network architectures (black), with and without skips and with and without weight sharing.\nThe second dimension is whether weights are shared between all cores (as in a recurrent network), or whether each core uses separate weights (as in a feedforward network). We note that the nonλ, non-r, γ variants of the predictron then correspond to standard (convolutional) feedforward and (unrolled) recurrent neural networks respectively. The third dimension is whether we include skip connections. This is equivalent to defining the model step to output a change to the current state, ∆s, and then defining sk+1 = h(sk + ∆sk), where h is the non-linear function—in our case a ReLU, h(x) = max(0, x). The deep network with skip connections is a variant of ResNet (He et al., 2015). Root mean squared prediction errors for each architecture are shown in Figure 4. All (r, γ, λ)predictrons (red lines) outperformed the corresponding feedforward or recurrent neural network baselines (black lines) both in the random mazes and in pool. We also investigated the effect of changing the depth of the networks (see Appendix C). The predictron outperformed the corresponding feedforward or recurrent baselines for all depths, with and without skip connections."
    }, {
      "heading" : "5.3 SEMI-SUPERVISED LEARNING BY CONSISTENCY",
      "text" : "We now consider how to use the predictron for semi-supervised learning, training the model on a combination of labelled and unlabelled random mazes. Semi-supervised learning is important because a common bottleneck in applying machine learning in the real world is the difficulty of collecting labelled data, whereas often large quantities of unlabelled data exist. We trained a full (r, γ, λ)-predictron by alternating standard supervised updates with consistency updates, obtained by stochastically minimizing the consistency loss (8), on the unlabelled samples. For each supervised update we apply either 0, 1, or 9 consistency updates. Figure 5 shows that the performance improved monotonically with the number of consistency updates, measured as a function of the number of labelled samples consumed."
    }, {
      "heading" : "5.4 ANALYSIS OF ADAPTIVE DEPTH",
      "text" : "In principle, the predictron can adapt its depth to ‘think more’ about some predictions than others, perhaps depending on the complexity of the underlying target. We investigate this by looking at qualitatively different prediction types in pool: ball collisions, rail collisions, pocketing balls, and entering or staying in quadrants. For each prediction type we consider several different time-spans (determined by the real-world discount factors associated with each pseudo-reward). Figure 6 shows distributions of depth for each type of prediction. The ‘depth’ of a predictron is here defined as the effective number of model steps. If the predictron relies fully on the very first value (i.e., λ0 = 0), this counts as 0 steps. If, instead, it learns to place equal weight on all rewards and on the final value, this counts as 16 steps. Concretely, the depth d can be defined recursively as d = d0 where dk = λk(1 + γkdk+1) and dK = 0. Note that even for the same input state, each prediction has a separate depth. The depth distributions exhibit three properties. First, different types of predictions used different depths. Second, depth was correlated with the real-world discount for the first four prediction types. Third, the distributions are not strongly peaked, which implies that the depth can differ per input\neven for a single real-world discount and prediction type. In a control experiment (not shown) we used a scalar λ shared among all predictions, which reduced performance in all scenarios, indicating that the heterogeneous depth is a valuable form of flexibility."
    }, {
      "heading" : "5.5 VISUALIZING THE PREDICTIONS IN THE POOL DOMAIN",
      "text" : "We test the quality of the predictions in the pool domain to evaluate whether they are well-suited to making decisions. For each sampled pool position, we consider a set I of different initial conditions (different angles and velocity of the white ball), and ask which is more likely to lead to pocketing coloured balls. For each initial condition s ∈ I , we apply the (r, γ, λ)-predictron (shared cores, 16 model steps, no skip connections) to obtain predictions gλ. We sum the predictions that correspond to pocketing any ball except the white ball, and to real-world discounts γ = 0.98 and γ = 1. We select the condition s∗ that maximises this sum. We then roll forward the pool simulator from s∗ and log the number of pocketing events. Figure 2 shows a sampled rollout, using the predictron to pick s∗. When providing the choice of 128 angles and two velocities for initial conditions (|I| = 256), this procedure resulted in pocketing 27 coloured balls in 50 episodes. Using the same procedure with an equally deep convolutional network only resulted in 10 pocketing events. These results suggest that the lower loss of the learned (r, γ, λ)-predictron translated into meaningful improvements when informing decisions. A video of the rollouts selected by the predictron is available here: https://youtu.be/BeaLdaN2C3Q."
    }, {
      "heading" : "6 RELATED WORK",
      "text" : "Lee et al. (2015) introduced a neural network architecture where classifications branch off intermediate hidden layers. An important difference with respect to the λ-predictron, is that the weights are hand-tuned as hyper-parameters, whereas in the predictron the λ weights are learnt and, more importantly, conditional on the input. Another difference is that the loss on the auxiliary classifications is used to speed up learning, but the classifications themselves are not combined into an aggregate prediction; the output of the model itself is the deepest prediction. Value iteration networks (Tamar et al., 2016) use convolutional and max-pooling layers to represent a step of value iteration. This is somewhat similar to a r-predictron, without γ and λ, with a singlelayer convolutional core that is specialised to two-dimensional domains. Schmidhuber (2015) dicusses learning abstract models, but maintains separate losses for the model and a controller, and suggests training the model unsupervised to compactly encode the entire history of observations, through predictive coding. The predictron’s abstract model is instead trained endto-end to obtain accurate values."
    }, {
      "heading" : "7 DISCUSSION",
      "text" : "The predictron is a single differentiable architecture that rolls forward an internal model to estimate values. This internal model may be given both the structure and the semantics of traditional reinforcement learning models. But unlike most approaches to model-based reinforcement learning, the model is fully abstract: it need not correspond to the real environment in any human understandable fashion, so long as its rolled-forward “plans” accurately predict outcomes in the true environment. The predictron may be viewed as a novel network architecture that incorporates several separable ideas. First, the predictron outputs a value by accumulating rewards over a series of internal planning steps. Second, each forward pass of the predictron outputs values at multiple planning depths. Third, these values may be combined together, also within a single forward pass, to output an overall ensemble value. Finally, the different values output by the predictron may be encouraged to be self-consistent with each other, to provide an additional signal during learning. Our experiments demonstrate that these differences result in more accurate predictions of value, in reinforcement learning environments, than more conventional network architectures. We have focused on value prediction tasks in uncontrolled environments. However, these ideas may transfer to the control setting, for example by using the predictron as a Q-network (Mnih et al., 2015). Even more intriguing is the possibility of learning an internal MDP with abstract internal actions, rather than the MRP model considered in this paper. We aim to explore these ideas in future work."
    }, {
      "heading" : "A ARCHITECTURE",
      "text" : "The state representation f is a two-layer convolutional neural network (LeCun et al., 1998). There is a core c, again based on convolutions, that combines both MRP model and λ-network into a single repeatable module, such that sk+1, rk+1, γk+1,λk = c(sk). This core is deterministic, and is duplicated K times in the predictron with shared weights. (The predictron with unshared weights hasK distinct cores.) Finally, the value network v is a fully connected neural network that computes vk = v(sk). Concretely, the core (Figure 7) consists first of a convolutional layer that maps into an intermediate (hidden) layer. From this layer, another two convolutions compute the next abstract state of the predictron. Additionally, this same hidden layer is flattened and fed into three separate networks, with two fully connected layers each. The outputs of these three networks represent the internal rewards, discounts, and lambdas. A similar small network also hangs off the internal states, in addition to the core, and computes the values. All convolutions use 3×3 filters and a stride of one, and use padding to retain the size of the feature maps. All feature maps have 32 channels. The hidden layers within the MLPs have 32 hidden units. In Figure 7 the convolutional layers are schematically drawn with three channels, flattening is represented by curly brakets, while the arrows represent the small multi-layer perceptrons which compute values, rewards, discounts and lambdas. We allow up to 16 model steps in our experiments, resulting in 52-layer deep networks—two convolutional layers for the state representations, 3× 16 = 48 convolutional layers for the core steps, and two fully-connected layers for the values on top of the final state. Between each two layers we apply batch normalization (Ioffe and Szegedy, 2015) followed by a ReLU non-linearity (Glorot et al., 2011). The value and reward networks end with a linear layer, whereas the discount and lambda networks additionally add a sigmoid non-linearity to ensure that these quantities are in [0, 1]."
    }, {
      "heading" : "B TRAINING",
      "text" : "All experiments used the supervised (Monte-Carlo) update described in Section 4.1 except for the semi-supervised experiment which used the consistency update described in Section 4.2. We update all parameters by applying the Adam optimiser (Kingma and Adam, 2015) to stochastic gradients of the corresponding loss functions. Each return is normalised by dividing it by its standard deviation (as measured, prior to the experiment, on a set of 20,000 episodes). In all experiments, the learning rate was 0.001, and the other parameters of the Adam optimiser were β1 = 0.9, β2 = 0.999, and = 10−8. We used mini-batches of 100 samples."
    }, {
      "heading" : "C COMPARING ARCHITECTURES OF DIFFERENT DEPTHS",
      "text" : "We investigated the effect of changing the depth of the networks, with and without skip connections. Figure 8 in shows that skip connections (dashed lines) make the conventional architectures\n(black/grey lines) more robust to the depth (i.e., the black/grey dashed lines almost overlap, especially on pool), and that the predictron outperforms the corresponding feedforward or recurrent baselines for all depths, with and without skips."
    }, {
      "heading" : "D ADDITIONAL DOMAIN DETAILS",
      "text" : "D.1 RANDOM MAZES\nTo generate mazes we first determine, with a stochastic line search, a number of walls so that the topleft corner is connected to the bottom-right corner (both always forced to be empty) in approximately 50% of the mazes. We then shuffle the walls uniformly randomly. For 20 by 20 mazes this means 70% of locations are empty and 30% contain walls. More than a googol different such 20-by-20 mazes exist (as ( 398 120 ) > 10100).\nD.2 POOL\nTo generate sequences in the Pool domain, the initial locations of 4 balls of different colours are sampled at random. The white ball is the only one moving initially. Its velocity has a norm sampled uniformly between 7 and 14. The initial angle is sampled uniformly in the range (0, 2π). From the initial condition, the Mujoco simulation is run forward until all balls have stopped moving; sequences that last more than 151 frames are rejected, and a new one is generated as replacement. Each frame is rendered by Mujoco as a 280x280 RGB image, and subsequently downsampled through bilinear interpolation to a 28x28 RGB input (see Figure 9 for an example). Since the 280 signals described in Section 6.1 as targets for the Pool experiments have very different levels of sparsity, resulting in values with very different scales, we have normalised the pseudo returns. The normalization procedure consisted in dividing all targets by their standard deviation, as empirically measured across an initial set of 20,000 sequences."
    } ],
    "references" : [ {
      "title" : "Dynamic programming",
      "author" : [ "R. Bellman" ],
      "venue" : null,
      "citeRegEx" : "Bellman.,? \\Q1957\\E",
      "shortCiteRegEx" : "Bellman.",
      "year" : 1957
    }, {
      "title" : "Recurrent environment simulators",
      "author" : [ "S. Chiappa", "S. Racaniere", "D. Wierstra", "S. Mohamed" ],
      "venue" : null,
      "citeRegEx" : "Chiappa et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chiappa et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "In Aistats,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Adaptive computation time for recurrent neural networks",
      "author" : [ "A. Graves" ],
      "venue" : "CoRR, abs/1603.08983,",
      "citeRegEx" : "Graves.,? \\Q2016\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J.B. Adam" ],
      "venue" : "In International Conference on Learning Representation,",
      "citeRegEx" : "Kingma and Adam.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Adam.",
      "year" : 2015
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "T. Lillicrap", "J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Action-conditional video prediction using deep networks in atari games",
      "author" : [ "J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Oh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2015
    }, {
      "title" : "On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "arXiv preprint arXiv:1511.09249,",
      "citeRegEx" : "Schmidhuber.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 2015
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Sutton.,? \\Q1988\\E",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1988
    }, {
      "title" : "TD models: Modeling the world at a mixture of time scales",
      "author" : [ "R.S. Sutton" ],
      "venue" : "In Proceedings of the Twelfth International Conference on Machine Learning,",
      "citeRegEx" : "Sutton.,? \\Q1995\\E",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1995
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "The MIT press, Cambridge MA,",
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
      "author" : [ "R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup" ],
      "venue" : "In The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume",
      "citeRegEx" : "Sutton et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2011
    }, {
      "title" : "Value iteration networks",
      "author" : [ "A. Tamar", "S. Levine", "P. Abbeel" ],
      "venue" : "arXiv preprint arXiv:1602.02867,",
      "citeRegEx" : "Tamar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tamar et al\\.",
      "year" : 2016
    }, {
      "title" : "Mujoco: A physics engine for model-based control",
      "author" : [ "E. Todorov", "T. Erez", "Y. Tassa" ],
      "venue" : "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
      "citeRegEx" : "Todorov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Todorov et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Prior deep reinforcement learning methods have successfully constructed models that can unroll near pixel-perfect reconstructions (Oh et al., 2015; Chiappa et al., 2016); but are yet to surpass state-of-the-art modelfree methods in challenging RL domains with raw inputs (e.",
      "startOffset" : 130,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "Prior deep reinforcement learning methods have successfully constructed models that can unroll near pixel-perfect reconstructions (Oh et al., 2015; Chiappa et al., 2016); but are yet to surpass state-of-the-art modelfree methods in challenging RL domains with raw inputs (e.",
      "startOffset" : 130,
      "endOffset" : 169
    }, {
      "referenceID" : 8,
      "context" : ", 2016); but are yet to surpass state-of-the-art modelfree methods in challenging RL domains with raw inputs (e.g., Mnih et al., 2015; 2016; Lillicrap et al., 2016).",
      "startOffset" : 109,
      "endOffset" : 164
    }, {
      "referenceID" : 16,
      "context" : "In the vector case, these are known as general value functions (Sutton et al., 2011).",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "We will say that a (general) value function v(·) is consistent with environment p if and only if v = vp which satisfies the following Bellman equation (Bellman, 1957),",
      "startOffset" : 151,
      "endOffset" : 166
    }, {
      "referenceID" : 15,
      "context" : "In model-based reinforcement learning (Sutton and Barto, 1998), an approximation m ≈ p to the environment is learned.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "A (general) value function vm(·) is consistent with model m (or valid, (Sutton, 1995)), if and only if it satisfies a Bellman equation vm(s) = Em [r + γvm(s)] with respect to model m.",
      "startOffset" : 71,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : "This λ-preturn is analogous to the λ-return in the forward-view TD(λ) algorithm (Sutton, 1988; Sutton and Barto, 1998).",
      "startOffset" : 80,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "This λ-preturn is analogous to the λ-return in the forward-view TD(λ) algorithm (Sutton, 1988; Sutton and Barto, 1998).",
      "startOffset" : 80,
      "endOffset" : 118
    }, {
      "referenceID" : 3,
      "context" : "This enables the predictron to compute to an adaptive depth (Graves, 2016) depending on the internal state and learning dynamics of the network.",
      "startOffset" : 60,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "The simulator is implemented in the physics engine Mujoco (Todorov et al., 2012).",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "The deep network with skip connections is a variant of ResNet (He et al., 2015).",
      "startOffset" : 62,
      "endOffset" : 79
    }, {
      "referenceID" : 17,
      "context" : "Value iteration networks (Tamar et al., 2016) use convolutional and max-pooling layers to represent a step of value iteration.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "Schmidhuber (2015) dicusses learning abstract models, but maintains separate losses for the model and a controller, and suggests training the model unsupervised to compactly encode the entire history of observations, through predictive coding.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 9,
      "context" : "However, these ideas may transfer to the control setting, for example by using the predictron as a Q-network (Mnih et al., 2015).",
      "startOffset" : 109,
      "endOffset" : 128
    } ],
    "year" : 2016,
    "abstractText" : "One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple “imagined” planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.",
    "creator" : "LaTeX with hyperref package"
  }
}