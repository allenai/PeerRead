{
  "name" : "1705.02175.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distributed Online Learning of Event Definitions",
    "authors" : [ "Nikos Katzouris", "Alexander Artikis", "Georgios Paliouras" ],
    "emails" : [ "nkatz@iit.demokritos.gr", "a.artikis@iit.demokritos.gr", "paliourg@iit.demokritos.gr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Event recognition systems [5] process sequences of simple events, such as sensor data, and recognize complex events, i.e. events that satisfy some pattern. Logic-based systems for event recognition typically use a knowledge base of first-order rules to represent complex event patterns and a reasoning engine to detect such patterns in the incoming data. The Event Calculus (EC) [16] has been used as the basis for event recognition systems [1], offering direct connections to machine learning, via Inductive Logic Programming (ILP) [4].\nEvent recognition applications deal with noisy data streams. Methods that learn from such streams typically build a decision model by a single pass over the input [9]. OLED (Online Learning of Event Definitions) [15] is an ILP system that learns event definitions in the form of EC theories in a single pass over a relational data stream. In this work we present an extension of OLED, that allows for learning a theory in an online and parallel fashion, from disjoint, possibly geographically distributed data streams (it therefore assumes no shared memory). Our approach is based on a simple parallelization scheme of the core OLED functionality and it is approapriate for distributed learning. OLED learns clauses in top-down manner, by gradually specializing an over-general clause. Its single-pass strategy is based on the Hoeffding bound [12], a tool that allows to build decision models by estimating their quality on a small subset of the input. OLED uses the Hoeffding bound to estimate the quality of candidate specializations of a clause on subsets of the input stream. In the proposed prallelization strategy, clauses are evaluated independently on disjoint data streams and their scores are combined whenever a specialization decision must be made. We present an evaluation of our approach on an a benchmark activity recognition dataset and show that ar X iv :1 70 5.\n02 17\n5v 1\n[ cs\n.A I]\n5 M\nay 2\n01 7\nwe can significantly reduce training times, exchanging minimal information between processing nodes. This work, therefore, paves the way for relational learning in highvelocity data streams.\nThe rest of this paper is structured as follows: In Section 2 we present some background on the EC and provide a running example. In Section 3 we present OLEDand in Section 4 we present its distributed version. In Section 5 we present our experimental results, while in Section 6 we discuss related work. Finally, in Section 7 we discuss some directions for future work and conclude."
    }, {
      "heading" : "2 Background and Running Example",
      "text" : "The Event Calculus (EC) [16] is a temporal logic for reasoning about events and their effects. Its ontology consists of time points (integer numbers); fluents, i.e. properties that have different values in time; and events, i.e. occurrences in time that may alter fluents’ values. The axioms of the EC incorporate the common sense law of inertia, according to which fluents persist over time, unless they are affected by an event. We use a simplified version of the EC that has been shown to suffice for event recognition [1]. The basic predicates and its domain-independent axioms are presented in Table 1. Axiom (1) states that a fluent F holds at time T if it has been initiated at the previous time point, while Axiom (2) states that F continues to hold unless it is terminated. Definitions for initiatedAt/2 and terminatedAt/2 predicates are given in an applicationspecific manner by a set of domain-specific axioms.\nWe illustrate our approach using the task of activity recognition, as defined in the CAVIAR project3. The CAVIAR dataset consists of videos where actors perform some activities. Manual annotation (performed by the CAVIAR team) provides ground truth for two activity types. The first type corresponds to simple events and consists of knowledge about the activities of a person at a certain video frame/time point, such as walking, or standing still. The second activity type corresponds to complex events and consists of activities that involve more than one person, e.g. two people meeting each other, or moving together. The goal is to recognize complex events as combinations of simple events and additional domain knowledge, such as a person’s direction and position.\nTable 2(a) presents some example CAVIAR data, consisting of a narrative of simple events in terms of happensAt/2, expressing people’s short-term activities, and context properties in terms of holdsAt/2, denoting people’ coordinates and direction. Table 2(a)\n3 http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/\n(a) (b) Narrative for time 1: Narrative for time 2: Two Domain-specific axioms:\nhappensAt(walk(id1 ), 1) happensAt(walk(id1 ), 2) initiatedAt(moving(X,Y ), T )← happensAt(walk(id2 ), 1) happensAt(walk(id2 ), 2) happensAt(walk(X ),T), holdsAt(coords(id1 , 201 , 454), 1) holdsAt(coords(id1 , 201 , 454), 2) happensAt(walk(Y ),T), holdsAt(coords(id2 , 230 , 440), 1) holdsAt(coords(id2 , 227 , 440), 2) distLessThan(X ,Y , 25 ,T), holdsAt(direction(id1 , 270), 1) holdsAt(direction(id1 , 275), 2) dirLessThan(X ,Y , 45 ,T). holdsAt(direction(id2 , 270), 1) holdsAt(direction(id2 , 278), 2) Annotation for time 1: Annotation for time 2: terminatedAt(move(X,Y ), T )← not holdsAt(move(id1, id2), 1) holdsAt(move(id1, id2), 2) happensAt(inactive(X ),T),\ndistMoreThan(X ,Y , 30 ,T).\nTable 2. (a) Example data from activity recognition. E.g., at time point 1 person id1 is walking, her (x, y) coordinates are (201, 454) and her direction is 270◦. The annotation for the same time point states that persons id1 and id2 are not moving together, in contrast to the annotation for time point 2. (b) An example of two domain-specific axioms in the EC. E.g. the first clause dictates that moving of two persons X and Y is initiated at time T if both X and Y are walking at time T , their euclidean distance is less than 25 and their difference in direction is less than 45◦.\nalso shows the annotation of complex events (long-term activities) for each time-point in the narrative. Negated complex events’ annotation is obtained via the closed world assumption (although both positive and negated annotation atoms are presented in Table 2, to avoid confusion). Table 2(b) presents two domain-specific axioms in the EC.\nOur goal is to learn definitions of complex events in terms of initiation and termination conditions, as in Table 2(b). In the Learning from Interpretations [3] ILP setting that we use in this work, each training example is an interpretation, i.e. a set of true ground atoms, as in Table 2(a). Given a set of training interpretations I and some background theory B, which in our case consists of the domain-independent axioms of the EC, the goal is to find a theory H , such that B ∪H covers as many I ∈ I as possible, where B ∪H covers I when I is a model of B ∪H . Although different semantics are possible, in this work a “model” is a stable model [10].\n3 The OLED System\nOLED [15] learns a theory by joining together independently-constructed clauses, each of which is learnt in an online fashion. OLED relies on the Hoeffding bound [12] to approximate the quality of a clause on the entire input using only a subset of the input. Given a random variable X with range in [0, 1] and an observed mean X of its values after n independent observations, the Hoeffding Bound states that, with probability 1 − δ, the true mean X̂ of the variable lies in an interval (X − ,X + ), where =√\nln(1/δ) 2n . In other words, the true average can be approximated by the observed one with probability 1 − δ, given an error margin that becomes smaller as the number of observations n increases.\nOLED learns a clause in a top-down fashion, by specializing it using literals from a bottom clause [4]. The Hoeffding bound is utilized in the specialization process a follows: Given a clause evaluation function G and some clause r, OLED evaluates r and all of its candidate specializations on training examples that stream-in. Assume that\nafter n training examples from the input stream, r1 is r’s specialization with the highest observed meanG-scoreG and r2 is the second-best one, i.e.∆G = G(r1)−G(r2) > 0. Then by the Hoeffding bound we have that for the true mean of the scores’ difference\n∆Ĝ it holds that ∆Ĝ > ∆G− , with probability 1− δ, where = √\nln(1/δ) 2n . Hence,\nif ∆G > then ∆Ĝ > 0, implying that r1 is indeed the best specialization, with probability 1 − δ. In order to decide which specialization to select, it thus suffices to accumulate examples from the input stream until ∆G > . These examples need not be stored or reprocessed. Each example is processed once to extract the necessary statistics for calculating G-scores and it is subsequently discarded, thus giving rise to an online (single-pass) clause construction strategy. To ensure that no clause r is replaced by a specialization of lower quality, r itself is also considered as a potential candidate along with its specializations, ensuring that specializing r is a better decision, with probability 1− δ, than not specializing it at all.\nThe default specialization process follows a FOIL-like, hill-climbing strategy, where a single literal is added to a clause at each specialization step. However, OLED supports different specialization strategies as well, e.g. by allowing to simultaneously try all specializations up to a given clause length, or by supporting user-defined, TILDE-like look-ahead specifications [2].\nTo calculate G-scores, each clause r is equipped with a true positive (TP ), a false positive (FP ) and a false negative (FN ) counter, whose values are updated accordingly as r gets evaluated on training examples that stream-in. True negative counts are not taken into account, since the annotation for complex events is acquired via the closed world assumption. Also, r has an example counter that counts the number of examples on which r has been evaluated so far and is used in the calculation of in the Hoeffding bound-based search heuristic. Although different scoring functions may be plugged into OLED, in this work we use precision, to score initiation clauses, and recall, to score termination clauses, as in [15]. Moreover, OLED supports a clause pruning mechanism, that allows to remove low-quality clauses (e.g. clauses that have been generated from noisy examples) and a tie-breaking mechanism, that allows to randomly select between equally good specializations. We refer to [15] for more details on these features.\nIn the general case, a theory learnt by OLED is a collection of clauses constructed with the online mechanism described above. Starting with an empty theory H = ∅, an initial clause is generated from the first positive example that streams-in, by constructing a bottom clause ⊥ from that example and adding the empty-bodied clause r = head(⊥) ← to theory H . From that point on, r is gradually specialized by the addition of literals from ⊥ to its body. New clauses are added to H whenever existing clauses in H become too specific to account for new incoming examples.\nWhen learning domain-specific axioms in the Event Calculus, the aforementioned generic theory construction strategy must be modified to account for the fact that initiation and termination clauses behave differently w.r.t. encountered TP ,FP and FN complex event instances. A description of this behavior is illustrated in Figure 1(A). To handle this behavior, initiation and termination clauses are learnt separately, by two parallel processes, each of which runs the core OLED Algorithm. The input stream is forwarded to each of these processes simultaneously. Figure 1(B) presents the different actions that each learner takes whenever it encounters TP ,FP and FN instances.\n4 A Distributed Version of OLED\nWe now proceed to the description of a distributed version of OLED. Evaluating a clause and its candidate specializations on incoming examples, may be performed in parallel, by distributing the clause evaluation workload across multiple processing nodes that operate on independent data partitions. When needed, e.g. when the Hoeffding test succeeds at some processing node, evaluation results from other nodes may be combined in order to make a more informed decision. We next describe this strategy in more detail.\nWe assume that learning is performed by a set N of independent, possibly distributed processing nodes. Each node Ni ∈ N handles a separate stream Si of training examples. The nodes inN communicate by exchanging messages and they learn a theory H simultaneously, each node using its own training stream. The distributed version of OLED differs from the sequential algorithm in the following respects:\nNew clause addition: When a node Ni generates a new clause r, it broadcasts r to all other nodes in N , via a AddNewClause(r) message (see Table 3 for the main types of message of distributed OLED). Each node that receives such a message adds clause r\nto its own theory and starts scoring r, and its candidate specializations, on its own data. As in the single-core version of OLED, a new clause r consists of an empty-bodied clause head(⊥r )←, where ⊥r is a bottom clause generated at Ni.\nClause specialization: When a node Ni is about to specialize a clause r, i.e. when OLED’s Hoeffding test for clause r succeeds, locally at Ni, node Ni sends a SpecializeRequest(rid) message to all other nodes, where rid is a unique identifier of clause r, common to all copies of r shared among processing nodes. Upon receiving such a message, each node uses rid to retrieve its own evaluation statistics for clause r and its candidate specializations, which are sent over to the requesting node Ni. These statistics consist of TP ,FP ,FN and E counts for clause r and its candidate specializations, where by E we denote the number of examples on which a clause has been evaluated so far. The received counts for clause r and its specializations are combined with node Ni’s local counts as follows (we describe the process for clause r only, but it is similar for each one of its specializations). Denoting by TP jr ,FP j r ,FN j r and E j r the respective counts for clause r, received from node Nj ∈ N , j 6= i, the current node Ni updates r’s counts accordingly, by increasing r’s local counts with those received from other nodes. For instance, the new TP count for clause r in node Ni becomes TP ir = TP i r +\n∑ Nj∈N TP jr . FP i r ,FN i r and E i r counts are updated in a similar fashion.\nEach processing node Ni ∈ N maintains a record, for each clause r in its theory and each one of r’s specializations, that contains the exact counts previously received for them, from each node Nj ∈ N , j 6= i. When node Ni receives a set of new TP jr ,FP j r ,FN j r and E j r counts for clause r from node Nj , j 6= i, the respective previous counts are subtracted from the new ones, to avoid over-scoring r with counts that have already been taken into account in previous updates. The same holds for r’s specializations.\nOnce individual clause evaluation statistics are combined as described above, node Ni repeats the Hoeffding test for clause r to assess if the test still succeeds after the accumulated counts from all other nodes, for clause r and its specializations, have been taken into account. If it does, clause r is replaced in H , the current theory at node Ni, by its best-scoring specialization r′ that results from the Hoeffding test. Then node Ni sends out a Replace(rid , r ′) message to all other nodes, instructing them to also replace r in their own theories with r′. If, on the other hand, the Hoeffding test fails at node Ni after the updated counts are taken into account, clause r is not specialized and all nodes continue evaluating their theories on new incoming examples from their training streams.\nClause pruning: For a clause r to be pruned away, two conditions must hold: First, clause r must be unchanged (not specialized) for a sufficiently long period, which, in the single-core version of OLED, is set to the average number of examples, observed so far in the learning process, for which the Hoeffding test succeeds, i.e. the average value of n = O( 1 2 ln 1 δ ) that has resulted in clause specializations so far. Second, from that point on where clause r remains unchanged, a sufficiently large number of examples must be seen, in order to use a Hoeffding test to infer that, with probability 1 − δ, the quality of clause r is below the pruning threshold.\nIn the distributed version of OLED, each node uses the above heuristics to decide locally whether a clause r should be pruned. Once it has seen enough data from its\nown stream to make that decision for clause r, it sends a PruneRequest(rid) message to all other nodes. Each node that receives such a message sends back to the requesting node the necessary information (period for which clause r remains unchanged and TP ,FP ,FN and Er counts for clause r), which node Ni uses to re-assess whether clause r should be pruned, based on the global view of clause r, obtained by combining r’s separate evaluations from all processing nodes. If node Ni eventually decides to pruned clause r, it sends a Remove(rid) to all other nodes, which instructs them to also remove clause r from their theories.\nAlgorithm 1 illustrates learning in distributed OLED by displaying the functionality of each processing node.\nTo ensure that all nodes have the same theory at each point during the learning process, processing nodes often block their execution. For instance, whenever a node Nj sends out a SpecializeRequest(rid), it blocks untils it receives the necessary statistics for clause rid from all other nodes. Similarly, whenever a node Ni, i 6= j receives such a message, it replies by sending over to Nj the necessary statistics for clause rid and then blocks its execution, waiting for a “verdict” from Nj . A “verdict” may be a Replace(rid, r′) message, which instructs node Ni to replace clause rid by its specialization r′, or, it may be a Proceed message (this type of message is omitted from Table 3), signifying the fact that the Hoeffding test for clause rid at nodeNj failed, after accumulated counts from all other nodes were taken into account, and therefore node Ni may continue processing new examples without altering its current theory. A similar blocking behavior occurs during message passing for clause pruning, to ensure that all all nodes remove (or preserve) a clause in a synchronized fashion.\nWhile exchanging messages during learning, care must be taken to avoid deadlocks and race conditions. An example of such situations is the case where the Hoeffding test\nAlgorithm 1 OLEDNode(H,G, I,N ′) Input: H: A potentially empty hypothesis; G: A clause evaluation function; I: A stream of training interpretations; N ′: Set of peer processing nodes. 1: H := ∅ 2: while true do 3: Let I be the next training interpretation 4: G-score each clause r ∈ H on I 5: if a new clause should be generated then 6: Generate a new clause r and add to H . 7: Send r to each node in N ′. 8: for each clause r ∈ H do 9: if the Hoeffding test for r succeeds then 10: Request the counts for r, and all of r’s specializations, from all nodes in N ′. 11: Add the received counts to the current ones and repeat the Hoeffding test. 12: if the Hoeffding test for r still succeeds then 13: Replace r in H with its best-scoring specialization. 14: Notify all nodes in N ′ to also replace r with its best-scoring specialization. 15: if r should be pruned then 16: Request the counts for r from all nodes in N ′. 17: Add the received counts for r to the current ones. 18: if r still should be pruned then 19: Remove r from the current theory. 20: Notify all nodes in N ′ to also remove r from their theories. 21: return H\nfor specializing a single clause r succeeds simultaneously at two different nodes Ni and Nj . In such a case, nodes Ni and Nj send a SpecializeRequest(rid) message to each other (in addition to all other nodes), requesting each other’s counts for clause r. Subsequently, both nodes enter a “waiting state” as mentioned above, waiting to receive the requested counts in order to proceed with repeating the Hoeffding test. This results in a deadlock, since each node is waiting for the other node’s reply. A similar situation may occur with pruning. To avoid such problems, all communication between nodes regarding clause specialization and pruning is mediated by a separate node, whose sole role is to assign priorities in such cases, so that such deadlocks are avoided. When two or more nodes request to specialize or prune a clause (almost) simultaneously, the mediator node prioritizes one of them randomly, enqueueing the other nodes for proceeding at a later time. The enqueued nodes reply to the requests of the prioritized node. Once the prioritized node finishes, the next node from the queue is prioritized. In such cases, a node from the queue abandons its effort to specialize or prune a clause r, if r gets specialized or pruned by the previously prioritized node.\nTo sum-up, all nodes in N share a copy of the same theory H at each point in learning. H is learnt in an online fashion, simultaneously from all nodes in N , each node handling its own training stream. As in the single-core version, initiation and termination clauses are learnt independently, by two separate groups of processes, each of which learns one type of clause and implements the distributed version of OLED described in this section."
    }, {
      "heading" : "5 Experimental Evaluation",
      "text" : "We present an experimental evaluation of our approach on CAVIAR (described in Section 2), a benchmark dataset for activity recognition. CAVIAR contains 282,067 training interpretations with a mean size of 25 atoms each.OLED is implemented in the Scala programming language. It uses Clingo4 as its main reasoning component and Scala’s akka Actors library5 to model the behavior of a processing node (Algorithm 1) and implement message passing. The code and data are available online6. All experiments were conducted on a Linux machine with a 3.6GHz processor (4 cores and 8 threads) and 16GB of RAM.\nThe purpose of our experiments was to compare the distributed version of OLED with its monolithic counterpart. We performed learning with 1, 2, 4 and 8 processing cores for acquiring the definitions of two target complex events, related to two persons meeting each other or moving together. CAVIAR contains 6,272 interpretations in which moving occurs and 3,722 in which meeting occurs. For the experiments with the distributed version of OLED positive and negative interpretations for the target complex events were evenly distributed across different processing cores. The results were obtained by tenfold cross-validation and are presented in Table 4(A), in the form of averages for training time, F1-score and theory size (total number of literals), as well\n4 http://potassco.sourceforge.net/ 5 http://akka.io/ 6 https://github.com/nkatzz/OLED\nas average exchanged message number and size. F1-scores were obtained by microaveraging results from each fold.\nWith respect to the predictive accuracy, multiple-core learning resulted in theories of slightly higher F1-score for the meeting complex event, as compared to single-core learning. In the monolithic setting, OLED postpones the generation of new clauses, up to the point where existing clauses become too specific to cover new examples. During this time, positive examples which may result in good clauses (recall that OLED learns by “encoding” examples into bottom clauses), are “skipped”, i.e. they are not used for learning new clauses, since they are covered by existing ones. In contrast, the data distribution in the multi-core setting results in cases where interesting examples that would have been missed in the monolithic setting, are actually used for learning. This resulted in OLED learning slightly “richer” theories for the meeting complex event in the multi-core setting.A similar effect was not observed for the moving complex event, which has a simpler definition than meeting.\nRegarding training times, OLED achieves a signicant speed-up, by moving from monolithic learning to learning with two cores. This is achieved by exchanging very little information between the processing cores (see the last column of Table 4(A)). When additional cores are added, the speed-up is sub-linear, mainly due to the increased number of exchanged messages that result in blocking processing cores.\nTo test distributed OLED further, we used a larger dataset obtained by sequentially appending to each other 10 copies of CAVIAR, “pushing forward” the time-stamps of the data in each copy. The experimental setting for the ×10 CAVIAR experiment was identical to the one described above for the ×1 CAVIAR experiment. The results are presented in Table 4(B).\nTraining times in the ×10 CAVIAR experiment follow the same speed-up pattern observed in the ×1 experiment. In this set of experiments, the speed-up factor reaches 3.8, leading to highly reduced training times.\nDue to the repetition of the training data in the×10 CAVIAR experiment, each fold of the tenfold cross-validation process converged to identical theories, regardless of the number of cores used. F1-scores are therefore identical for all number of cores, and are also improved as compared to the ×1 CAVIAR experiment. In the latter experiment, good clauses were often constructed “too-late”, from examples that were encountered shortly before the data were exhausted (in which case OLED terminates). Such clauses may be discarded, since OLED uses a “warm-up” period parameter that controls a minimum number of examples on which a clause must be evaluated in order to be included in an output hypothesis. In contrast, in the ×10 CAVIAR experiment such problems were avoided, thanks to the increase in training data size."
    }, {
      "heading" : "6 Related Work",
      "text" : "An overview of existing approaches to learning theories in the Event Calculus with ILP may be found at [14,13] and a discussion on how OLED compares to such approaches may be found at [15,13]. In this section we mainly discuss distributed/parallel ILP algorithms, for which a substantial amount of work exists in the literature. A thorough review may be found in [8,22]. Parallel ILP algorithms exploit parallelism across three\nmain axes [8]: Searching through the hypothesis space in parallel (search parallelism); splitting the training data and learning from data subsets (data parallelism); and evaluating candidate clauses in parallel (evaluation/coverage parallelism).\nIn [20] the authors present a a data-parallel version of a standard set-cover loop: Each processing node learns a fragment of the concept definition from a partition of the data, and then these fragments are exchanged between all nodes. Good-enough clauses are kept by all nodes. A cover removal step is subsequently implemented by each core and the set-cover loop continues. Overall, the approach in [20] learns much faster that a sequential algorithm, achieving super-linear speed-ups. A similar approach is proposed in [7], where the training examples are split across multiple nodes and searched in parallel, while the best rules from each node are “pipe-lined” to all other nodes.\nIn [22] the authors use a MapReduce-based framework to parallelize the operation of a classical set-cover ILP algorithm towards both evaluation-parallelism and searchparallelism. In the former case, coverage tests of candidate clauses are performed in parallel, on disjoint partitions of the data. In the latter case, bottom clauses (which are generalized to acquire a hypothesis clause) are generated and searched in a concurrent fashion from more than one “seed” examples. The reducer then selects the best hypothesis clause that results from this process. A similar approach for parallel exploration of independent hypotheses has been proposed in [18], while similar approaches towards parallel coverage tests have been proposed in [11,6]. In [17], the approach of [22] was extended to a framework that is capable of self-regulating the workload of distributing learning costs across multiple nodes.\nA main difference of the work presented here from the aforementioned approaches to parallel ILP is that they all rely on iterative ILP algorithms (basically, a set-cover loop), and they all require several passes over the data to compute a hypothesis. In contrast, OLED is an online, single-pass algorithm. In relation to the latter, some work on streaming ILP exists. However, existing approaches are either oriented towards unsupervised tasks like frequent pattern discovery [19], or they rely on propositionalization techniques and off-the-self, online propositional learners [21]."
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "We presented a distributed version of a recently proposed algorithm for online learning of complex event definitions in the form of domain-specific axioms in the Event Calculus. We also presented an experimental evaluation of our approach on a benchmark dataset for activity recognition, which demonstrates that we can significantly reduce training times. As future work, we aim to evaluate our approach on larger datasets, in terms of in-situ, geographically distributed learning, as in the case of maritime monitoring. Also, we plan to remove the requirement that all processing nodes block their execution, while waiting for replies during message passing.\nAcknowledgments. This work was funded by the H2020 project DATACRON."
    } ],
    "references" : [ {
      "title" : "An event calculus for event recognition",
      "author" : [ "Alexander Artikis", "Marek Sergot", "Georgios Paliouras" ],
      "venue" : "Knowledge and Data Engineering, IEEE Transactions on,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Top-down induction of first-order logical decision trees",
      "author" : [ "Hendrik Blockeel", "Luc De Raedt" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1998
    }, {
      "title" : "Scaling up inductive logic programming by learning from interpretations",
      "author" : [ "Hendrik Blockeel", "Luc De Raedt", "Nico Jacobs", "Bart Demoen" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1999
    }, {
      "title" : "Logical and relational learning",
      "author" : [ "Luc De Raedt" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Event processing in action",
      "author" : [ "Opher Etzion", "Peter Niblett" ],
      "venue" : "Manning Publications Co.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Customisable multi-processor acceleration of inductive logic programming",
      "author" : [ "Andreas K Fidjeland", "Wayne Luk", "Stephen H Muggleton" ],
      "venue" : "Latest Advances in Inductive Logic Programming,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "A pipelined data-parallel algorithm for ILP",
      "author" : [ "Nuno A. Fonseca", "Fernando M.A. Silva", "Vı́tor Santos Costa", "Rui Camacho" ],
      "venue" : "IEEE International Conference on Cluster Computing (CLUSTER",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "Parallel ilp for distributed-memory architectures",
      "author" : [ "Nuno A Fonseca", "Ashwin Srinivasan", "Fernando Silva", "Rui Camacho" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Knowledge discovery from data streams",
      "author" : [ "Joao Gama" ],
      "venue" : "CRC Press,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Answer set solving in practice",
      "author" : [ "Martin Gebser", "Roland Kaminski", "Benjamin Kaufmann", "Torsten Schaub" ],
      "venue" : "Synthesis Lectures on Artificial Intelligence and Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Accelerating the drug design process through parallel inductive logic programming data mining",
      "author" : [ "James H. Graham", "C. David Page Jr.", "Ahmed H. Kamal" ],
      "venue" : "IEEE Computer Society Bioinformatics Conference,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "Wassily Hoeffding" ],
      "venue" : "Journal of the American statistical association,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1963
    }, {
      "title" : "Scalable relational learning for event recognition",
      "author" : [ "Nikos Katzouris" ],
      "venue" : "PhD Thesis, University of Athens,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2017
    }, {
      "title" : "Incremental learning of event definitions with inductive logic programming",
      "author" : [ "Nikos Katzouris", "Alexander Artikis", "Georgios Paliouras" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Online learning of event",
      "author" : [ "Nikos Katzouris", "Alexander Artikis", "Georgios Paliouras" ],
      "venue" : "definitions. TPLP,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "A logic-based calculus of events",
      "author" : [ "Robert Kowalski", "Marek Sergot" ],
      "venue" : "New Generation Computing,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1986
    }, {
      "title" : "Yet another parallel hypothesis search for inverse entailment",
      "author" : [ "Hiroyuki Nishiyama", "Hayato Ohwada" ],
      "venue" : "In ILP,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Parallel execution for speeding up inductive logic programming systems",
      "author" : [ "Hayato Ohwada", "Fumio Mizoguchi" ],
      "venue" : "In International Conference on Discovery Science,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1999
    }, {
      "title" : "Multi-relational pattern mining over data streams",
      "author" : [ "Andreia Silva", "Cláudia Antunes" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Parallel and sequential algorithms for data mining using inductive logic",
      "author" : [ "David B Skillicorn", "Yu Wang" ],
      "venue" : "Knowledge and Information Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2001
    }, {
      "title" : "Relational models with streaming ilp",
      "author" : [ "Ashwin Srinivasan", "Michael Bain" ],
      "venue" : "In ILP,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Data and task parallelism in ilp using mapreduce",
      "author" : [ "Ashwin Srinivasan", "Tanveer A Faruquie", "Sachindra Joshi" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Event recognition systems [5] process sequences of simple events, such as sensor data, and recognize complex events, i.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : "The Event Calculus (EC) [16] has been used as the basis for event recognition systems [1], offering direct connections to machine learning, via Inductive Logic Programming (ILP) [4].",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "The Event Calculus (EC) [16] has been used as the basis for event recognition systems [1], offering direct connections to machine learning, via Inductive Logic Programming (ILP) [4].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "The Event Calculus (EC) [16] has been used as the basis for event recognition systems [1], offering direct connections to machine learning, via Inductive Logic Programming (ILP) [4].",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 8,
      "context" : "Methods that learn from such streams typically build a decision model by a single pass over the input [9].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "OLED (Online Learning of Event Definitions) [15] is an ILP system that learns event definitions in the form of EC theories in a single pass over a relational data stream.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 11,
      "context" : "Its single-pass strategy is based on the Hoeffding bound [12], a tool that allows to build decision models by estimating their quality on a small subset of the input.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 15,
      "context" : "The Event Calculus (EC) [16] is a temporal logic for reasoning about events and their effects.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "We use a simplified version of the EC that has been shown to suffice for event recognition [1].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "In the Learning from Interpretations [3] ILP setting that we use in this work, each training example is an interpretation, i.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "Although different semantics are possible, in this work a “model” is a stable model [10].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "OLED [15] learns a theory by joining together independently-constructed clauses, each of which is learnt in an online fashion.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 11,
      "context" : "OLED relies on the Hoeffding bound [12] to approximate the quality of a clause on the entire input using only a subset of the input.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "Given a random variable X with range in [0, 1] and an observed mean X of its values after n independent observations, the Hoeffding Bound states that, with probability 1 − δ, the true mean X̂ of the variable lies in an interval (X − ,X + ), where = √",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 3,
      "context" : "OLED learns a clause in a top-down fashion, by specializing it using literals from a bottom clause [4].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "by allowing to simultaneously try all specializations up to a given clause length, or by supporting user-defined, TILDE-like look-ahead specifications [2].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 14,
      "context" : "Although different scoring functions may be plugged into OLED, in this work we use precision, to score initiation clauses, and recall, to score termination clauses, as in [15].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 14,
      "context" : "We refer to [15] for more details on these features.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 13,
      "context" : "An overview of existing approaches to learning theories in the Event Calculus with ILP may be found at [14,13] and a discussion on how OLED compares to such approaches may be found at [15,13].",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "An overview of existing approaches to learning theories in the Event Calculus with ILP may be found at [14,13] and a discussion on how OLED compares to such approaches may be found at [15,13].",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 14,
      "context" : "An overview of existing approaches to learning theories in the Event Calculus with ILP may be found at [14,13] and a discussion on how OLED compares to such approaches may be found at [15,13].",
      "startOffset" : 184,
      "endOffset" : 191
    }, {
      "referenceID" : 12,
      "context" : "An overview of existing approaches to learning theories in the Event Calculus with ILP may be found at [14,13] and a discussion on how OLED compares to such approaches may be found at [15,13].",
      "startOffset" : 184,
      "endOffset" : 191
    }, {
      "referenceID" : 7,
      "context" : "A thorough review may be found in [8,22].",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 21,
      "context" : "A thorough review may be found in [8,22].",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : "main axes [8]: Searching through the hypothesis space in parallel (search parallelism); splitting the training data and learning from data subsets (data parallelism); and evaluating candidate clauses in parallel (evaluation/coverage parallelism).",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 19,
      "context" : "In [20] the authors present a a data-parallel version of a standard set-cover loop: Each processing node learns a fragment of the concept definition from a partition of the data, and then these fragments are exchanged between all nodes.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "Overall, the approach in [20] learns much faster that a sequential algorithm, achieving super-linear speed-ups.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "A similar approach is proposed in [7], where the training examples are split across multiple nodes and searched in parallel, while the best rules from each node are “pipe-lined” to all other nodes.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "In [22] the authors use a MapReduce-based framework to parallelize the operation of a classical set-cover ILP algorithm towards both evaluation-parallelism and searchparallelism.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "A similar approach for parallel exploration of independent hypotheses has been proposed in [18], while similar approaches towards parallel coverage tests have been proposed in [11,6].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "A similar approach for parallel exploration of independent hypotheses has been proposed in [18], while similar approaches towards parallel coverage tests have been proposed in [11,6].",
      "startOffset" : 176,
      "endOffset" : 182
    }, {
      "referenceID" : 5,
      "context" : "A similar approach for parallel exploration of independent hypotheses has been proposed in [18], while similar approaches towards parallel coverage tests have been proposed in [11,6].",
      "startOffset" : 176,
      "endOffset" : 182
    }, {
      "referenceID" : 16,
      "context" : "In [17], the approach of [22] was extended to a framework that is capable of self-regulating the workload of distributing learning costs across multiple nodes.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 21,
      "context" : "In [17], the approach of [22] was extended to a framework that is capable of self-regulating the workload of distributing learning costs across multiple nodes.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 18,
      "context" : "However, existing approaches are either oriented towards unsupervised tasks like frequent pattern discovery [19], or they rely on propositionalization techniques and off-the-self, online propositional learners [21].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 20,
      "context" : "However, existing approaches are either oriented towards unsupervised tasks like frequent pattern discovery [19], or they rely on propositionalization techniques and off-the-self, online propositional learners [21].",
      "startOffset" : 210,
      "endOffset" : 214
    } ],
    "year" : 2017,
    "abstractText" : "Logic-based event recognition systems infer occurrences of events in time using a set of event definitions in the form of first-order rules. The Event Calculus is a temporal logic that has been used as a basis in event recognition applications, providing among others, direct connections to machine learning, via Inductive Logic Programming (ILP). OLED is a recently proposed ILP system that learns event definitions in the form of Event Calculus theories, in a single pass over a data stream. In this work we present a version of OLED that allows for distributed, online learning. We evaluate our approach on a benchmark activity recognition dataset and show that we can significantly reduce training times, exchanging minimal information between processing nodes.",
    "creator" : "LaTeX with hyperref package"
  }
}