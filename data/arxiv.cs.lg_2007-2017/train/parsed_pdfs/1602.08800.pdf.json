{
  "name" : "1602.08800.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ITERATIVE AGGREGATION METHOD FOR SOLVING PRINCIPAL COMPONENT ANALYSIS PROBLEMS",
    "authors" : [ "Vitaly Bulgakov" ],
    "emails" : [ "V@YAHOO.COM" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: principal component analysis, clustering method, power iteration method, aggregation method, eigenvalue problem"
    }, {
      "heading" : "1. Introduction",
      "text" : "This work was envisioned as application of the multilevel aggregation method [1] developed by the author back in 90s to PCA problems. Multilevel aggregation method was an extension of well-known multigrid methods[2] from boundary value problems to general structural analysis problems which brought it to the class of algebraic multigrid methods. The idea of the aggregation method was to use some naturally constructed course model of the original finite element approximation of a structure which provides a fast convergence for iterative methods for solving large algebraic systems of equations. One of applications of this method was an iterative solution of large eigenvalue problems arising in structural natural vibration and buckling analyses [3]. In these problems a sought set of lowest vibration modes can be thought of as principal components of structure behavior. An obvious similarity with PCA was a turning point to start looking for a proper way to create an aggregation model for data matrix approximation and use it for efficient solution of PCA problems.\nIn this study PCA[4] is applied to and the method is tested on text analysis problems. A tested data set consists of documents each of which produces an N-dimensional vector stored as a column of a data matrix which values are term frequencies. Our raw data comes in the form of text files from data sets such as medical abstracts and news groups. The purpose of PCA is to iteratively compute a set of highest eigenvalues and corresponding eigenvectors of the covariance matrix. Covariance matrix is never formed explicitly. The main operation is multiplication of large sparse data matrix or its transpose by a vector. The course aggregation model of the original covariance matrix is used in the iterative solution of the eigenvalue problem. Original covariance matrix and its approximation of small size assumes similarity of leading eigenvalues and eigenvectors. This fact allows fast convergence of subspace iterations at minimal additional computational cost.\nFor numerical experiments we use R language which is rich of linear algebra, statistical and graphical packages."
    }, {
      "heading" : "2. PCA problem formulation",
      "text" : "PCA in multivariate statistics is widely used as an effective way to perform unsupervised dimension reduction. The essence of this method lies in using Singular Value Decomposition (SVD)\nar X\niv :1\n60 2.\n08 80\n0v 1\n[ cs\n.N A\n] 2\n9 Fe\nb 20\n16\nwhich provides the best low rank approximation to original data according to Eckart-Young theorem [5]. Let n data points inm dimensional space be contained in the data matrix which is assumed already centered around the origin for computational stability\n(x1, x2, ..., xn) = X (1)\nThen covariance matrix is\nA = XXT (2)\nLet (λk, φk) be an eigenpair of A, where eigenvectors φk define principal directions."
    }, {
      "heading" : "3. Aggregation model",
      "text" : "In order to create an aggregation model we divide the entire set of data vectors xi into n0 clusters using some similarity criteria where n0 << n. We will explain later how we do clustering. We assume that all vectors within the cluster are similar and a single representative of a cluster is an average of all vectors xi where i ∈ clusterk or for cluster k we have\nx0k = 1/dimk ∗ ∑\ni∈clusterk\nxi (3)\nTransformation of matrix X to X0 is done using matrix R which we call aggregator\nX0 = XR (4)\nwhere R[i, k] = if i ∈ clusterk then 1/dimk else 0. X0 is of size (m,n0). Approximation A0 of covariance matrix A is\nA0 = X0X T 0 = XRR TXT (5)\nFormally matrix A0 is of the same size as A but has a much lower rank. We do not need to use form (5) for computations. For matrix vector multiplication we rather use sparse matrix X0 which according to (3) is constructed by simple averaging of vectors inside a cluster and\nA0v = X0X T 0 v (6)\nTherefore A0v requires O(mn0) operations which is much lower than O(mn) operations required for Av. We also expect and this is confirmed by numerical experiments that convergence of iterative methods for solving partial eigenvalue problem for A0 is faster than that for A.\nThere are quite a few clustering techniques known as computationally efficient. Besides since we need clustering as an auxiliary procedure we do not need highly accurate clustering results. In this study we use K-means clustering algorithm [6] which became very popular in data mining, unsupervised classification, etc. and which converges quickly to a local optimum. Our experience says that the aggregated problem with a small number of clusters provides a good resemblance of the original and approximated covariance matrices in terms of first (highest) eigenvalues which is important for the iterative method described below. In Figure 1 this resemblance is demonstrated where we show distribution of first 10 eigenvalues of both matrices where the data matrix X was obtained by processing 2014 documents of ”Cardiovascular Diseases Abstracts” corpus. Matrix X0 was obtained by K-means method with 10 clusters."
    }, {
      "heading" : "4. Iterative method",
      "text" : "We use power iteration method [7] for for solving auxiliary aggregated eigenvalue problem and a modified power method for solving the original eigenvalue problem. This method is also known as subspace iteration when used to simultaneously iterate a set of eigenvectors. One iteration of the power algorithm consists of the following steps:\nfor i = 1 to l : ũi k+1 =\n1\n‖Auki ‖ ∗ Auki\nuk+11 , ..., u k+1 l = orthonorm(ũ1 k+1, ..., ũl k+1)\nwith approximation of eigenvalues\nλki = (Auki , u k i )\n(uki , u k i )\n(7)\nwhich starts with a set of l initial approximations of first eigenvectors (u01, u 0 2, ..., u 0 l ) = U 0. The key property of the power method is that if approximation u0i is spanned by matrix A eigenvectors subspace, then after k multiplications of matrix A by this vector the linear combination of eigenvectors will be weighted by λi to the power k which gives boost to terms corresponding to highest eigenvalues:\nAku = ∑ k ciλ k i φi (8)\nIn the method proposed for the first l principal directions of PCA we will need first k orthonormal eigenvectors of A0 q1, q2, ..., qk where k >= l. These vectors can be obtained by algorithms (7). We will also need matrix Pi\nPi = qiq T i (9)\nSince qTi qj = δi,j and PiPi = Pi, it is a projector to the subspace of i-th eigenvector of A0. We will modify method (7) using this projector in the following manner:\nfor i = 1 to l : ũk+1i = 1\n‖Buki ‖ ∗Buki where\nBuki = Au k i + αi ∗ PiAPiuki and αi => min‖Aũk+1i − λki ũk+1i ‖\nuk+11 , ..., u k+1 l = orthonorm(ũ1 k+1, ..., ũl k+1)\nwith approximation of eigenvalues\nλki = (Auki , u k i )\n(uki , u k i )\n(10)\nThis approach can be thought of as ”help” to the power iteration method to converge on the subspace of eigenvectors of the aggregated problem. The intuition for that is similarity of first eigenvectors and eigenvalues of the original and aggregated problem if clustering is done properly. Let u = ∑ ciφi where φi are eigenvectors of the original covariance matrixA and Pk be a projector on subspace of φk. Then\nAu+ αPkAPku = ∑ i 6=k ciλiφi + ckλk(1 + α)φk (11)\nIf α is chosen big then the second term of this expression dominates over the first term thus providing convergence for φk in one iteration step. α can be derived from the condition stated in (10):\nΦ(ũk+1i ) = ‖Aũk+1i − λki ũk+1i ‖\nα => minΦ => dΦ\ndα = 0\n(12)\nThis equation leads to the quadratic equation for α. Omitting indexes and skipping details we arrive at the following expression for α\nα = −(A 2u,AFu)− 2λ(A2u, Fu) + λ2(Au, Fu)\n(Au,AFu)− 2λ(Au, Fu) + λ2(Fu, Fu) (13)\nwhere F = PAP . We note that as you can see from (12) α is chosen from the previous step to simplify computations. This can also be justified by the fact that eigenvalues converge faster than eigenvectors. Detailed algorithm discussion is out of scope of this paper. We just mention here that all operations with matrix A are reduced to the matrix vector multiplications of the sparse data matrix X or its transpose XT ."
    }, {
      "heading" : "5. Numerical experiments",
      "text" : "For numerical experiments we used two data sets. The fist one is ”Cardiovascular Diseases Abstracts” which is a set where each abstract is an individual document. The data matrix X size is 16058 by 2014 where the first value is the total number of terms and the second one is the number of documents. We searched for 10 first eigenvalues of the covariance matrix A = XXT and used 10 clusters for constructing auxiliary aggregation problem A0 = X0XT0 . So the size of this problem is more than 201 times lower than that for the original problem.\nThe problem is solved using algorithm (10). Figure 2 shows changes of parameter α for the first three eigenvectors. As expected the biggest contribution of projectors (9) is observed in first\niterations to suppress errors caused by initial eigenvector guesses. After some number of iteration contribution of projectors is getting smaller while eigenvectors are getting more accurate.\nWe measure convergence of eigenvalues through Error1 = ‖Λk+1 − Λk‖F/‖Λk‖F and convergence of eigenvectors by the residual matrix through Error2 = ‖AUk − UkΛk‖F where ‖‖F is a matrix Frobenius norm, Uk consists of orthonormal vectors uk1, ..., u k l which are approximations of the eigenvectors and Λk is a diagonal matrix of approximations of eigenvalues. Errors graph is demonstrated in Figure 3.\nA good convergence rate of the iterative process is demonstrated. After 40 iterations we got Error1 = 0.00038 and Error2 = 0.0017.\nThe second corpus was ”talk politics” set from the news groups. Size of this problem is 13511 (terms) by 1171 (documents). We searched for 10 first eigenvalues of the covariance matrix and used 10 clusters again. The quality of the clustering aggregated model can be viewed by comparing eigenvalues of the original and aggregated covariance matrices. Figure 4 demonstrates a good resemblance of eigenvalues distribution. Convergence graph is demonstrated in Figure 5. After 40 iterations we got Error1 = 0.00044 and Error2 = 0.00049."
    } ],
    "references" : [ {
      "title" : "Multi-Grid Methods and Applications",
      "author" : [ "W. Hackbush" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1985
    }, {
      "title" : "Principal Component Analysis, Series: Springer Series in Statistics, 2nd ed",
      "author" : [ "I.T. Jolliffe" ],
      "venue" : "XXIX,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2002
    }, {
      "title" : "The approximation of one matrix by another of lower rank",
      "author" : [ "C. Eckart", "G. Young" ],
      "venue" : "Psychometrika, Volume",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1936
    }, {
      "title" : "Some Methods for classification and Analysis of Multivariate Observations",
      "author" : [ "J.B. MacQueen" ],
      "venue" : "Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability. University of California Press",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1967
    }, {
      "title" : "Simultaneous iteration method for symmetric matrices",
      "author" : [ "H. Rutishauser" ],
      "venue" : "Numer. Math.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1970
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Multilevel aggregation method was an extension of well-known multigrid methods[2] from boundary value problems to general structural analysis problems which brought it to the class of algebraic multigrid methods.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "In this study PCA[4] is applied to and the method is tested on text analysis problems.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "which provides the best low rank approximation to original data according to Eckart-Young theorem [5].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "In this study we use K-means clustering algorithm [6] which became very popular in data mining, unsupervised classification, etc.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "Iterative method We use power iteration method [7] for for solving auxiliary aggregated eigenvalue problem and a modified power method for solving the original eigenvalue problem.",
      "startOffset" : 47,
      "endOffset" : 50
    } ],
    "year" : 2016,
    "abstractText" : "Motivated by the previously developed multilevel aggregation method for solving structural analysis problems a novel two-level aggregation approach for efficient iterative solution of Principal Component Analysis (PCA) problems is proposed. The course aggregation model of the original covariance matrix is used in the iterative solution of the eigenvalue problem by a power iterations method. The method is tested on several data sets consisting of large number of text documents.",
    "creator" : "LaTeX with hyperref package"
  }
}