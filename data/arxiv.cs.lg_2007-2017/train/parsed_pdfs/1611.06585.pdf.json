{
  "name" : "1611.06585.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Variational Boosting: Iteratively Refining Posterior Approximations",
    "authors" : [ "Andrew C. Miller", "Nicholas Foti", "Ryan P. Adams" ],
    "emails" : [ "acm@seas.harvard.edu", "nfoti@uw.edu", "rpa@seas.harvard.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Variational inference (VI) [2, 16, 30] is a family of methods designed to approximate an intractable target distribution (typically known only up to a constant) with a tractable surrogate distribution. VI procedures typically minimize the Kullback-Leibler (KL) divergence of the approximation to the target by maximizing an appropriately defined tractable objective. Often, the class of approximating distributions is fixed, and typically excludes the neighborhood surrounding the target distribution, which prevents the variational approximation from becoming arbitrarily close to the true posterior. Often this mismatch between the variational family and the true posterior manifests as underestimating the posterior variances of the model parameters [30].\nMarkov chain Monte Carlo (MCMC), an alternative class of inference methods, instead approximates target distributions with samples drawn from a Markov chain constructed to leave the target distribution invariant. MCMC methods allow a user to trade computation\nar X\niv :1\n61 1.\n06 58\n5v 1\n[ st\nat .M\nL ]\ntime for increased accuracy — drawing more samples will make the approximation closer to the true target distribution. However, MCMC algorithms typically must be run iteratively and it can be difficult to assess convergence to the true posterior. Furthermore, correctly specifying MCMC moves can be more algorithmically restrictive than optimizing an objective (e.g., data subsampling in stochastic gradient methods).\nIn order to alleviate the mismatch between tractable variational approximations and complicated posterior distributions, we propose a variational inference method that iteratively allows the approximating distribution to become more complex and to eventually represent the true distribution arbitrarily well. This choice allows the practitioner to trade time performing inference against accuracy of posterior estimates, where in the limit the exact posterior can be recovered as in MCMC. Our algorithm grows the complexity of tfhe approximating class in two ways: 1) incorporating richer covariance structure in the component distributions, and 2) by sequentially adding new components to the approximating distribution. Our method builds on black-box variational inference methods using the re-parameterization trick [18, 25, 28], applicable to a broad class of target distributions.\nThe following section discusses variational inference methods, drawing comparisons to alternative approximate inference algorithms. Section 3 and subsections therein describe variational boosting. We show how to adapt the re-parameterization trick for mixture approximations in Section 3.1. Section 4 describes various numerical experiments on real and synthetic data."
    }, {
      "heading" : "2 Variational Inference",
      "text" : "Given a target distribution with density1 π(x) for a random variable x ∈ X ⊂ Rd, variational inference approximates π(x) with a tractable approximate distribution,2 q(x;λ), from which we can draw samples and form sample-based estimates of functions of x. Variational methods minimize the KL-divergence, KL(q||π), between q(·;λ) and the true π as a function of variational parameters λ [1]. Direct optimization of KL(q||π) is often intractable; however, we can derive a tractable objective based on properties of the KL-divergence. This objective is often referred to as the evidence lower bound (ELBO), written\nL(λ) = Eqλ [ln π̃(x)− ln q(x;λ)] (1) = Eqλ [lnπ(x)− ln q(x;λ)] + ln C (2) = ln C −KL(qλ||π) (3)\n≤ ln C = ln ∫ π̃(x)dx (4)\nwhich, due to the positivity of KL(q||π), is a lower bound on the normalization constant3 of π̃(x),\n1We assume π(x) is known up to a constant, π̃(x) = Cπ(x) for some constant C, omitting ∼ to simplify notation.\n2We treat the density function as a synecdoche for the entire law, and use q(x;λ) and qλ(x) interchangeably at the risk of slight notational abuse.\n3Often referred to as the marginal likelihood, p(data), in Bayesian inference.\nVariational methods typically define (or derive) a family of distributions Q = {q(·;λ) : λ ∈ Λ} parameterized by λ, and maximize the ELBO with respect to λ ∈ Λ. Most commonly the class Q is fixed, and there exists some (possibly non-unique) λ∗ ∈ Λ for which KL(q||π) is minimized. When the family Q does not include π, there will be a non-zero KL gap between q(·;λ∗) and π, and that discrepancy will realize itself in the form of biased estimates of functions of x ∼ π.\nVariational inference is often seen as an alternative to other approximate inference algorithms, most notably Markov chain Monte Carlo (MCMC). MCMC methods construct a Markov Chain such that the target distribution remains invariant (i.e., the target is admitted along the margins). Expectations with respect to the target can be calculated as an average with respect to these correlated samples. MCMC typically enjoys nice asymptotic properties; as the number of samples grows, MCMC samplers represent the true target distribution with higher and higher fidelity. However, rules for constructing correct Markov steps are quite restrictive. With a few exceptions [21, 31], most MCMC algorithms require evaluating a log-likelihood that touches all data at each step in the chain (sometimes many times per step). This is problematic in analyses with a large amount of data — MCMC methods are often considered unusable because of this computational bottleneck.\nData sub-sampling, on the other hand, can often be used in conjunction with variational inference methods. Unbiased estimates of the log-likelihood based on data sub-sampling can often be used for optimization methods. Because variational methods recast inference as optimization, data sub-sampling can often be a way to make an already efficient approximation even more efficient.\nIn the next section, we propose an algorithm that iteratively grows the approximating class Q and reframes the VI procedure as a series of optimization problems, resulting in an inference method that can both represent complex distributions and scale to large data sets."
    }, {
      "heading" : "3 Method: Variational Boosting",
      "text" : "We define our approximate distribution to be a mixture of C simpler component distributions\nq(C)(x;λ) = C∑ c=1 ρcqc(x;λc) s.t. ρc ≥ 0 and ∑ c ρc = 1 (5)\nwhere we have defined component distributions qc4, mixture component parameters λ = (λ1, . . . , λC), and mixing proportion parameters ρ = (ρ1, . . . , ρC). The component distributions can be any distribution over x from which we can draw samples using a continuous mapping that depends on λc (e.g., multivariate normals [14], or a composition of invertible maps [27]).\nWhen posterior expectations and variances are of interest, mixture distributions provide 4We denote full mixtures with parenthetical superscripts, q(C), and components with naked subscripts, qc.\ntractable summaries (so long as the component distributions are tractable)5. Expectations are easily expressed in terms of component expectations\nEq(C) [f(x)] = ∫ q(C)(x)f(x)dx (6)\n= ∑ c ρcEqc [f(x)] . (7)\nIn the case of multivariate normal components, the mean and covariance of a mixture are easy to compute in closed form\nEq(C) [x] = ∑ c ρcµ(λc) = µ (C) (8)\nCq(C) [x] = ∑ c ρcΣ(λc)− ρc ( µ(λc)− µ(C) )( µ(λc)− µ(C) )ᵀ (9)\nas are marginal distributions along any set of dimensions\nq(C)(xd) = ∑ c ρcN (xd|µd(λc),Σdd(λc)) (10)\nwhere µ(λc) and Σ(λc) isolate the mean and covariance from variational component parameter λc.\nOur method begins with a single mixture component, C = 1. We use existing BBVI methods to fit the first component parameter, λ1, and ρ1 is fixed to 1 by definition. At the next iteration, we fix λ1, and then introduce a new component into the mixture, q2(x;λ2), a new ELBO objective as a function of new component parameters, λ2, and a new mixture weight, ρ2. We then optimize this objective with respect to λ2 and ρ2 until convergence. At each subsequent iteration, k, we introduce new component parameters and a mixing weight, (λk, ρk), which are then optimized according to the new ELBO objective. We refer to this procedure as variational boosting, inspired by methods for learning strong classifiers by weighting an ensemble of weak classifiers.\nIn order for our method to be applicable to a general class of target distributions, we use black-box variational inference methods and the re-parameterization trick [18, 25, 28] to fit each component and mixture weights. The re-parameterization trick is a method for obtaining unbiased estimates of the gradient of the ELBO. These gradient estimates can then be used to optimize the ELBO objective using a stochastic gradient optimization method. However, using mixtures as the variational approximation complicates the use of the re-parameterization trick.\n5Mixtures are also simple to sample from so that more complicated functionals can also easily be estimated."
    }, {
      "heading" : "3.1 The re-parameterization trick and mixture distributions",
      "text" : "The re-parameterization trick is a method for computing low-variance estimates of the gradient of an objective for which we only have an unbiased estimator\nL(λ) = Eq [lnπ(x)− ln q(x;λ)]\n≈ 1 L L∑ `=1 [ lnπ(x(`))− ln q(x(`);λ) ] where samples x(`) are drawn from q(x;λ). To obtain a Monte Carlo gradient of L(λ) using the re-parameterization trick, we first separate the randomness needed to generate x(`) from the parameters λ, by defining a deterministic map fq(x0;λ) = x(`) such that x0 ∼ q0 implies6 x(`) ∼ q(x;λ). Then, we can differentiate through fq with respect to λ to obtain a gradient estimator.\nThe re-parameterization trick when q is a mixture, however, is less straightforward. The sampling procedure for a mixture model typically contains a discrete component (i.e., sampling component identities), which is a process that cannot be differentiated through. We circumvent this complication by re-writing the variational objective as a weighted combination of expectations with respect to individual mixture components. Because of the form of the mixture, we can write the ELBO as\nL(λ, ρ) = Eq [lnπ(x)− ln q(x;λ)]\n= ∫ ( C∑ c=1 ρcqc(x;λc) ) [lnπ(x)− ln q(x;λ)] dx\n= C∑ c=1 ρc ∫ qc(x;λc) [lnπ(x)− ln q(x;λ)] dx\n= C∑ c=1 ρcEqc [lnπ(x)− ln q(x;λ)]\nwhich is a function of expectations with respect to mixture components. If these distributions are continuous, and there exists some function fc(x0;λ) such that x = fc(x0;λ) and x ∼ qc(·;λ) when x0 ∼ q0, then we can apply the re-parameterization trick to each component to obtain gradients of the ELBO\n∇λcL(λ, ρ) = ∇λc C∑ c=1 ρcEx∼q(x;λ) [lnπ(x)− ln q(x;λ)]\n= C∑ c=1 ρcEx0∼q0 [ ∇λc lnπ(fc(x0;λc))−∇λc ln q(fc(x0;λc)) ] .\nVariational Boosting uses the above fact with the re-parameterization trick in a componentby-component manner, allowing us to improve the variational approximation as we incorporate and fit new components.\n6Here, q0 is some base distribution that is, importantly, not a function of λ."
    }, {
      "heading" : "3.2 Adding Components",
      "text" : "In this section we present details of the proposed algorithm. We first describe the process of fitting a single component and then the process for adding an additional component to an existing mixture distribution.\nFitting the first component The procedure starts by fitting an approximation to π(x) with a distribution that consists of a single component. We do this by maximizing the first ELBO objective\nL(1)(λ1) = Eq [lnπ(x)− ln q1(x;λ1)] (11) λ∗1 = arg max\nλ1\nL(1)(λ1) . (12)\nDepending on the forms of π and q1, optimizing the ELBO can be accomplished by various methods. One general method for fitting a continuous valued component is to compute stochastic, unbiased gradients of L(λ1), and use stochastic gradient optimization. After convergence (or close to it) we fix λ1 to be λ∗1.\nFitting component C + 1 After iteration C, our current approximation to π(x) is a mixture distribution with C components\nq(C)(x;λ) = C∑ c=1 ρcqc(x;λc) (13)\nwhere λ = ({ρc, λc}c) is a list of component parameters and mixing weights, and qc(x;λc) is the component distribution parameterized by λc. Adding a new component introduces a new component parameter, λC+1, and a new mixing weight, ρC+1. In this section, the mixing parameter ρC+1 ∈ [0, 1] mixes between the new component, qC+1(·;λC+1) and the existing approximation, q(C). The new approximate distribution is\nq(C+1)(x; ρC+1, λC+1) = (1− ρC+1)q(C)(x) + ρC+1qC+1(x;λC+1) . (14)\nThe new optimization objective, as a function of ρC+1 and λC+1 is L(C+1)(ρC+1, λC+1) = Ex∼q(C+1) [ lnπ(x)− ln q(C+1)(x;λC+1, ρC+1) ] (15)\n= (1− ρC+1)Eq(C) [ lnπ(x)− ln q(C+1)(x;λC+1, ρC+1) ] (16)\n+ ρC+1EqC+1 [ lnπ(x)− ln q(C+1)(x;λC+1, ρC+1) ] . (17)\nAbove we have separated out two expectations — one with respect to the existing approximation (which is fixed), and the other with respect to the new component distribution. Because we have fixed the existing approximation, we only need to optimize the new component parameters, λC+1, ρC+1, allowing us to use the re-parameterization trick to obtain gradients of L(C+1). As we have fixed the existing component distribution and we only need to optimize the new component λC+1, we can use the re-parameterization trick and Monte Carlo gradients to optimize L(C+1) with respect to ρC+1 and λC+1.\nFigure 1 illustrates the algorithm on a simple one-dimensional example — showing the initialization of a new component and the resulting mixture after optimizing the second objective, L(2)(ρ2, λ2). Figure 2 depicts the result of the variational boosting procedure on a two-dimensional, multi-modal target distribution. In both cases, the component distributions are Gaussians with diagonal covariance."
    }, {
      "heading" : "3.3 Structured Multivariate Normal Components",
      "text" : "Though our method can use any component distribution that can be sampled using a continuous mapping, a sensible choice of component distribution is a multivariate normal\nq(x;λ) = N (x;µ(λ),Σ(λ)) (18) = |2πΣ(λ)|−1/2 exp ( − 12 (x− µ(λ)) ᵀΣ(λ)−1(x− µ(λ)) ) (19)\nwhere the variational parameter λ is transformed into a mean vector µ(λ) and covariance matrix Σ(λ).\nSpecifying the structure of the covariance matrix is a choice that largely depends on the dimensionality of x (x ∈ RD) and correlation structure of the target distribution. A common first-choice of covariance parameterization is a diagonal matrix\nΣ(λ) = diag(σ21 , . . . , σ 2 D) (20)\nwhich implies that x is independent across dimensions. When the approximation only consists of one component, this structure is commonly referred to as the mean field family. While computationally efficient, mean field approximations cannot model posterior correlations, which often leads to underestimation of marginal variances. Additionally, when diagonal covariances are used as the component distributions in Eq. (5) the resulting mixture may require a exorbitant number of components to represent the strong correlations. Further, the independence restriction can introduce local optima in the variational objective [30].\nOn the other end of the spectrum, we can parameterize the entire covariance matrix by parameterizing the lower triangle of a Cholesky decomposition, L, such that LLᵀ = Σ. This allows Σ to be any positive semi-definite matrix, enabling q to have the full flexibility of a D-dimensional multivariate normal distribution. However, this introduces D(D + 1)/2 parameters, which can become computationally cumbersome when D is large. Furthermore, it may not be the case that all pairs of variables exhibit posterior correlation, particularly in multi-level models where different parameter types may be more or less independent in the posterior.\nAlternatively, we can incorporate some capacity to capture correlations between dimensions of x without introducing many more parameters. The next subsection discusses\na covariance specification that provides this tradeoff, while remaining computationally tractable within the BBVI framework.\nLow-rank plus diagonal covariance Black-box variational inference methods with the re-parameterization trick rely on sampling from the variational distribution, and efficiently computing (or approximating) the entropy of the variational distribution. For multivariate normal distributions, the entropy is a function of the determinant of the covariance matrix, Σ, while computing the log likelihood requires inverting the covariance matrix, Σ−1. When the dimensionality of the target, D, is large, computing determinants and inverses will be O(D3) and therefore may be prohibitively expensive to compute at every iteration.\nHowever, it may be unnecessary to represent all D(D − 1)/2 possible correlations in the target distribution, particularly if certain dimensions are close to independent. One way to increase the capacity of q(x;λ) is to model the covariance as a low-rank plus diagonal (LR+D) matrix\nΣ = CCᵀ + diag(exp(v)) (21)\nwhere C ∈ RD×r and v ∈ RD are the rank-r and log diagonal components. Note that both C and v are represented by the component parameter λ.\nThe choice of r presents a tradeoff — with a larger rank, the variational approximation can be more flexible; with a lower rank, the computations necessary for fitting the variational approximation can be more efficient. As a concrete example, in the Experiments section we present a D = 40 dimensional posterior resulting from a non-conjugate hierarchical model, and we show that a “rank r = 2 plus diagonal\" covariance does an excellent job capturing all D(D − 1)/2 = 780 pairwise correlations and D marginal variances. Incorporating more components using the variational boosting framework further improves the approximation of the distribution.\nTo use the re-parameterization trick with this low rank covariance, we can simulate from q in two steps\nz(lo) ∼ N (0, Ir) (22) z(hi) ∼ N (0, ID) (23)\nx = Cz(lo) + µ+ I(v/2)z(hi) (24)\nwhere z(lo) generates the randomness due to the low-rank structure, and z(hi) generates the randomness due to the diagonal structure. We use the operator I(a) = diag(exp(a)) for notational brevity. This generative process can be differentiated through, yielding Monte Carlo estimates of the gradient with respect to C and v suitable for stochastic optimization.\nIn order to use LR+D covariance structure within variational boosting, we will need to efficiently compute the determinant and inverse of Σ. The matrix determinant lemma [11]\nallows us to represent the determinant of Σ as the product of two determinants\n|CCᵀ + I(v))| = |I(v))||Ir + CᵀI(−v)C| (25)\n= exp (∑ d vd ) |Ir + CᵀI(−v)C| (26)\nwhere the left term is simply the product of the diagonal component, and the right term is the determinant of a r × r dimensional matrix, computable in O(r3) time.\nSimilarly, the Woodbury matrix identity [9] allows us to represent the inverse of Σ as\n(CCᵀ + I(v))−1 = I(−v)− I(−v)C(Ir + CᵀI(−v)C)−1CTI(−v) (27)\nwhich involves the inversion of a smaller, r× r matrix, which can be done in O(r3) time. Importantly, the above operations are efficiently differentiable and amenable for use in the BBVI framework.\nFitting the rank To specify the ELBO objective, we need to choose a rank r for the component covariance. Because fitting a single component is relatively cheap, we start by a single component with rank r = 0, continue to fit r = 1, 2, . . . , and rely on a heuristic stopping criterion. For a single Gaussian, one such criterion is the average change in marginal variances — if the marginal variation along each dimension remains the same from rank r to r + 1, then the new covariance component is not incorporating explanatory power, particularly if marginal variances are of interest. As the KL(q||π) objective tends to underestimate variances when restricted to a particular model class, we observe that the marginal variances grow as new covariance rank components are added. When fitting rank r+ 1, we can monitor the average absolute change in marginal variance (or standard deviation) as more covariance structure is incorporated. Figure 9 in Section 4 depicts this measurement for a D = 37-dimensional posterior."
    }, {
      "heading" : "3.4 Initializing Components",
      "text" : "Introducing a new component requires initialization of component parameters. When our component distributions are mixtures of Gaussians, we found that the optimization procedure is sensitive to initialization. This section describes an importance-weighting scheme for initialization that produces (empirically) good initial values of component and mixing parameters.\nConceptually, a good initial component is located in a region of the target π(x) that is underrepresented by the existing approximation q(C). A good initial weight is close to the proportion of mass in the unexplained region. Following this principle, we construct this component by first drawing importance-weighted samples from our existing approximation\nx(`) ∼ q(C) , w(`) = π(x (`))\nq(C)(x(`)) for ` = 1, . . . , L. (28)\nThe samples with the largest weights w(`) tell us where regions of the target are unexplained by our approximation. In fact, as L grows, and if q(C) is “close” enough\nto π, we can interpret {x(`), w(`)} as a weighted sample from π. Based on this interpretation, we can fit a mixture distribution (or some components of a mixture distribution) to this weighted sample using maximum likelihood, and recover a type of target approximation. For mixture distributions, an efficient inference procedure is Expectation-Maximization (EM) [3].\nThis approach, however, presents a few complications. First, we must adapt EM to fit a weighted sample. Second, importance weights can suffer from extremely high variance — one or two w(`) values may be extremely large compared to all other weights. This destabilizes our new component parameters and mixing weight, particularly the variance of the component. Intuitively, if a single weight w(`) is extremely large, this would correspond to many samples being located in a single location, and maximum likelihood with EM would want to shrink the variance of the new component to zero right on that location. To combat this behavior, we use a simple method to break up the big weights using a resampling and re-weighting step before applying weighted EM. Empirically, this improves our new component initializations and subsequent ELBO convergence.\nWeighted EM Expectation-maximization is typically used to perform maximum likelihood in latent variable models. Mixture distributions are easily represented with latent variables — a sample’s latent variable corresponds to the mixture component that produced it. EM starts with some initialization of model parameters (e.g.,component means, variances and mixing weights). The algorithm then iterates between two steps: 1) the E-step, which computes the distribution over the latent variables given the current setting of parameters, and 2) the M-step, which maximizes the expected complete data log-likelihood with respect to the distributions computed in the E-step.\nWe suppress details of the general treatment of EM, and focus on EM for mixture models as presented in [1]. For mixture distributions, the E-step computes “responsibilities”, or the probability that a datapoint came from one of the components. The M-step then computes a weighted maximum likelihood, where the log-likelihood of a datapoint for a particular component is weighted by the associated “responsibility”. This weighted maximum likelihood is an easy entry-point for an additional set of weights — weights associated with each datapoint from the importance-weighting.\nMore concretely, for a sample of data, x(`), C mixture components, and current mixture component parameters and weights λ = {ρc, λc}Cc=1, the E-step computes the following quantities\nγ(`)c = p(z (`) = c|x(`), λ) (29)\n∝ p(x(`)|z(`),λc = c)p(z(`) = c) (30)\nwhere γ(`)c is the “responsibility” of cluster c for datapoint `. The M-step then computes component parameters by a weighted maximum likelihood\nλ∗c = arg max λ L∑ `=1 γ(`)c · ln p(x(`)|z(`) = c, λc) . (31)\nAlgorithm 1 Importance-weighted initialization of new components. This algorithm takes in the target distribution, π(x), the current approximate distribution q(C)(x), and a number of samples L. This returns an initial value of new component parameters, λC+1 and a new mixing weight ρC+1. 1: procedure InitComp(π, q(C), L) 2: x(`) ∼ q(C) for ` = 1, . . . , L . sample from existing approx 3: w(`) ← π(x\n(`)) q(C)(x(`))\n. set importance weights 4: O ← outlier-weights({w(`)}) 5: q(IW ) ← make-mixture(O, {w(`), x(`)}, q(C)) . break up big weights 6: x (`) r ∼ q(IW ) for ` = 1, . . . , L . sample from new mixture 7: w (`) r ← π(x (`) r )\nq(IW )(x(`)) . re-sampled importance weights\n8: λC+1, ρC+1 ← weighted-em({x(`)r , w(`)r }) . fit new component 9: return λC+1, ρC+1\nTo incorporate importance weights w(`), we only need to slightly change the M-step.\nλ∗c = arg max λ L∑ `=1 w(`) · γ(`)c · ln p(x(`)|z(`) = c, λc) (32)\nBecause we are adding a new component, we would like our weighted EM routine to leave the remaining components unchanged. For instance, we want λ1, . . . , λC−1 to be fixed, while λC is free to explain the weighted sample. This can be accomplished in a straightforward manner by simply clamping the first C − 1 parameters during the M-step.\nResampling importance weights If our current approximation q(C) is sufficiently different in certain regions of the posterior, then some weights w(`) will end up being large compared to other weights. For instance, the objective KL(q||p) tends to under-cover regions of the posterior, allowing π(x) to be much larger than q(c)(x), meaning the weight associated with x will be large. This will create instability in the weighted EM approximation — likelihood maximization will want to put a zero-variance component on the single highest-weighted sample, which does not accurately reflect the local curvature of π(x). To combat this, we construct a slightly more complicated proposal distribution. Conceptually, we first create this naïve importance-weighted sample, and then find samples with outlier weights, and break those samples up. We do this by constructing a new proposal distribution that mixes the existing proposal, q(C), and component means located at the outlier samples. We define this proposal to be\nq(IW )(x) = p0q (C)(x) + ∑ `∈O w(`)N (x|x(`),Σ(`)) (33)\nwhere ` ∈ O denote the set of outlier samples from our original sample, and p0 = 1− ∑ `∈O w (`) is the mass not placed on outlier samples. The variance of each outlier component, Σ(`) is set to some heuristic value — we typically use the diagonal of the covariance of q(C) as a good-enough guess.\nWe then create a new importance-weighted sample, using q(IW ) and π(x) just as we did before. By placing new components (with some non-zero variance) on the outlier samples, which are known to be in a region of high target probability and low approximate probability, we assume that there is more local probability around that region that needs to be explored. This allows us to inflate the local variance of the samples in this region — the region that weighted EM will place a component. Algorithm 1 unites the components from above sections into our final initialization procedure."
    }, {
      "heading" : "3.5 Related Work",
      "text" : "Using a mixture model as an approximating distribution in variational inference is a well-studied idea. Mixtures of mean field approximations [14] introduced mean fieldlike updates for a mixture approximation using a bound on the entropy term and model-specific parameter updates. Nonparametric variational inference [8] is a black-box variational inference algorithm that approximates a target distribution with a mixture of equally-weighted isotropic normals. The authors use a lower-bound on the entropy term in the ELBO to make the optimization procedure tractable. Similarly, [28] present a method for fitting mixture distributions as an approximation. However, their method is restricted to mixture component distributions within the exponential family, and a joint optimization procedure. Finally, we note that [10] independently and in parallel proposed a closely-related idea for iterative “boosted” construction of variational mixture approximations.\nUsing a low-rank Gaussian as a variational approximation was explored in [29], using a PCA-like algorithm. We fit the low-rank components of a Gaussian using black-box methods and joint optimization.\nWe also note that mixture distributions are a type of hierarchical variational model [26], where the component identity can be thought of as latent variables in our variational distribution. While in [26] the authors optimize a lower bound on the ELBO to fit general hierarchical variational models, our approach integrates out the discrete latent variables because it is tractable to do so."
    }, {
      "heading" : "4 Experiments and Analysis",
      "text" : "To supplement the illustrative synthetic examples, in this section we apply variational boosting to approximate various intractable posterior distributions resulting from real statistical analyses."
    }, {
      "heading" : "4.1 Hierarchical Binomial Regression",
      "text" : "We test out our posterior approximation on a hierarchical binomial regression model.7 We borrow an example from [4], and estimate the binomial rates of success (batting\n7Model and data from the mc-stan case studies, http://mc-stan.org/documentation/case-studies/ pool-binary-trials.html\naverages) of baseball players using a hierarchical model. The model describes a latent\n“skill” parameter for baseball players — the probability of obtaining a hit in a given at bat. The model of the data is\nφ ∼ Unif hyper prior κ ∼ Pareto(1, 1.5) hyper prior θj ∼ Beta(φ · κ, (1− φ) · κ) player j prior yj ∼ Binomial(Kj , θj) player j hits\nwhere yj is the number of successes (hits) player j has attempted inKj attempts (at bats). Each player has a latent success rate θj , which is governed by two global variables κ and φ. There are 18 players in this example, creating a posterior distribution with D = 20 parameters. This model is not conjugate, and requires approximate Bayesian inference.\nWe use adam [17] for each stochastic optimization problem with default parameters. For stochastic gradients, we use 400 samples for the new component, and 400 samples for the previous component. In all experiments, we use autograd [23, 22] to obtain automatic gradients with respect to new component parameters.\nTo highlight the fidelity of our method, we compare Variational Boosting to mean field VI and the No-U-Turn Sampler (NUTS) [13]. The empirical distribution resulting from 20k NUTS samples is considered the “ground truth” posterior in this example. Figure 3 compares a selection of univariate and bivariate posterior marginals. We see that Variational Boosting is able to closely match the NUTS posteriors, improving upon the MFVI approximation.\nFigure 4 compares the variational boosting covariance estimates to the “ground truth” estimates of MCMC at various stages of the algorithm."
    }, {
      "heading" : "4.2 Multi-level Poisson GLM",
      "text" : "We apply variational boosting to approximate the posterior for a common hierarchical model, a hierarchical Poisson GLM. This model was formulated to measure the relative rates of stop-and-frisk events for different ethnicities and in different precincts [6], and has been used as illustrative example of multi-level modeling [7, Chapter 15, Section 1].\nThe model incorporates a precinct and ethnicity effect to describe the relative rate of stop-and-frisk events.\nµ ∼ N (0, 102) mean offset lnσ2α, lnσ\n2 β ∼ N (0, 102) group variances αe ∼ N (0, σ2α) ethnicity effect βp ∼ N (0, σ2β) precinct effect\nlnλep = µ+ αe + βp + lnNep log rate Yep ∼ P(λep) stop-and-frisk events\nwhere Yep are the number of stop-and-frisk events within ethnicity group e and precinct p over some fixed period of time; Nep is the total number of arrests of ethnicity group e in precinct p over the same period of time; αe and βp are the ethnicity and precinct effects.\nAs before, we simulate 20k NUTS samples, and compare various variational approximations. Because of the high posterior correlations present in this example, variational boosting with diagonal covariance components is inefficient in its representation of this structure. As such, this example more heavily relies on the low-rank approximation to shape the posterior.\nFigure 5 show how increasing the rank of a single multivariate normal component can result in better variance approximations. Figure 6 shows a handful of bivariate marginal posterior approximations as a function of covariance rank. Figure 7 shows the same bivariate marginals as more rank-3 components are added to the approximation. Lastly, Figure 8 compares the marginal standard deviations and covariances to MCMC-based measurements. These results indicate that while the incorporation of covariance structure increases the accuracy of marginal variance approximations, the non-Gaussianity afforded by the incorporation of mixture components allows for a better posterior approximation that translates into even more accurate moment estimates."
    }, {
      "heading" : "5 Discussion and Conclusion",
      "text" : "We have proposed a practical variational inference method that incorporates new components into the approximation and is applicable to a large number of Bayesian models of interest. We demonstrated the ability of the method to learn rich representations of complex posteriors over a moderate number of parameters.\nWe see a few avenues of future work. First, while it is known that mixtures of Gaus-\nsians can approximate smooth distributions to arbitrary precision (with enough components) [5], it remains an open question if our approach of fixing and iteratively adding components using this sequence of ELBO objectives will converge. Existing work has shown that this is the case for the alternative direction of KL-divergence, KL(π||q) [19, 24], but it remains to be shown for KL(q||π). Furthermore, the rate of\nconvergence would, ideally, be characterized.\nThe variational boosting framework allows for more flexible component distributions. For instance, compositions of invertible maps have been used to enrich variational families [27], as well as auxiliary variable variational models [20].\nAlthough our mixture component fitting algorithm is greedy, our determination of an appropriate covariance rank is not. We imagine that we can use the result of the r = 0 diagonal covariance to inform the procedure for r = 1, and so on. We leave this sort of nested boosting for low-rank determination to future work.\nWhen optimizing parameters of a variational family, it has been shown that the natural gradient can be more robust and lead to better optima [12, 15]. While the Fisher information, required for computing the natural gradient, for a single Gaussian component can be computed in closed form, it is less straightforward for a mixture component. We hope to incorporate natural gradient updates in future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to acknowledge Arjumand Masood, Mike Hughes, and Finale Doshi-Velez for helpful conversations. ACM is supported by the Applied Mathematics Program within the Office of Science Advanced Scientific Computing Research of the U.S. Department of Energy under contract No. DE-AC02-05CH11231. NF is supported by a Washington Research Foundation Innovation Postdoctoral Fellowship in Neuroengineering\nand Data Science. RPA is supported by NSF IIS-1421780 and the Alfred P. Sloan Foundation."
    } ],
    "references" : [ {
      "title" : "d  iff  in  m  ar  gi  na  l  s  ds Figure 9: Mean percent change in marginal variances for the Poisson GLM. After rank 5, the average percent change is less than 5% — this estimate is slightly noisy due to the stochastic optimization procedure",
      "author" : [ "C Bishop" ],
      "venue" : "Pattern recognition and machine learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Variational inference: A review for statisticians",
      "author" : [ "David M Blei", "Alp Kucukelbir", "Jon D McAuliffe" ],
      "venue" : "arXiv preprint arXiv:1601.00670,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society",
      "author" : [ "Arthur P Dempster", "Nan M Laird", "Donald B Rubin" ],
      "venue" : "Series B (methodological),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1977
    }, {
      "title" : "Data analysis using stein’s estimator and its generalizations",
      "author" : [ "Bradley Efron", "Carl Morris" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1975
    }, {
      "title" : "Non-parametric estimation of a multivariate probability density",
      "author" : [ "V.A. Epanechnikov" ],
      "venue" : "Theory Probab. Appl., 14(1):153–158",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "An analysis of the nypd’s stop-and-frisk policy in the context of claims of racial bias",
      "author" : [ "Andrew Gelman", "Jeffrey Fagan", "Alex Kiss" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Data analysis using regression and multilevel/hierarchical models",
      "author" : [ "Andrew Gelman", "Jennifer Hill" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "Nonparametric variational inference",
      "author" : [ "Samuel Gershman", "Matt Hoffman", "David M Blei" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Matrix Computations",
      "author" : [ "G.H. Golub", "C.F. Van Loan" ],
      "venue" : "JHU Press",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Boosting variational inference",
      "author" : [ "Fangjian Guo", "Xiangyu Wang", "Kai Fan", "Tamara Broderick", "David B. Dunson" ],
      "venue" : "[stat.ML],",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Matrix Algebra from a Statistician’s Perspective",
      "author" : [ "D.A. Harville" ],
      "venue" : "Springer-Verlag",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Stochastic variational inference",
      "author" : [ "Matthew D Hoffman", "David M Blei", "Chong Wang", "John William Paisley" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo",
      "author" : [ "Matthew D Hoffman", "Andrew Gelman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Improving the mean field approximation via the use of mixture distributions",
      "author" : [ "Tommi S Jaakkola", "Michael I Jordan" ],
      "venue" : "In Learning in graphical models,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1998
    }, {
      "title" : "Composing graphical models with neural networks for structured representations and fast inference",
      "author" : [ "Matthew J. Johnson", "David K. Duvenaud", "Alex B. Wiltschko", "Sandeep R. Datta", "Ryan P. Adams" ],
      "venue" : "Arxiv preprint arXiv:1603.06277,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "Michael I Jordan", "Zoubin Ghahramani", "Tommi S Jaakkola", "Lawrence K Saul" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1999
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Mixture density estimation",
      "author" : [ "Q.J. Li", "A.R. Barron" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Auxiliary deep generative models",
      "author" : [ "Lars Maaløe", "Casper Kaae Sønderby", "Søren Kaae Sønderby", "Ole Winther" ],
      "venue" : "arXiv preprint arXiv:1602.05473,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Firefly monte carlo: Exact mcmc with subsets of data",
      "author" : [ "Dougal Maclaurin", "Ryan P Adams" ],
      "venue" : "arXiv preprint arXiv:1403.5693,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Autograd: Reverse-mode differentiation of native python",
      "author" : [ "Dougal Maclaurin", "David Duvenaud", "Ryan P. Adams" ],
      "venue" : "ICML workshop on Automatic Machine Learning,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Autograd: Reverse-mode differentiation of native Python",
      "author" : [ "Dougal Maclaurin", "David Duvenaud", "Matthew Johnson", "Ryan P. Adams" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Risk bounds for mixture density estimation",
      "author" : [ "Rakhlin A", "D. Panchenko", "S. Mukherjee" ],
      "venue" : "ESAIM: Probability and Statistics,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2006
    }, {
      "title" : "Black box variational inference",
      "author" : [ "Rajesh Ranganath", "Sean Gerrish", "David M Blei" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Hierarchical variational models",
      "author" : [ "Rajesh Ranganath", "Dustin Tran", "David M Blei" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "Variational inference with normalizing flows",
      "author" : [ "Danilo Rezende", "Shakir Mohamed" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Fixed-form variational posterior approximation through stochastic linear regression",
      "author" : [ "Tim Salimans", "David A Knowles" ],
      "venue" : "Bayesian Analysis,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "Gaussian covariance and scalable variational inference",
      "author" : [ "M.W. Seeger" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "Martin J Wainwright", "Michael I Jordan" ],
      "venue" : "Foundations and Trends R  © in Machine Learning,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    }, {
      "title" : "Bayesian learning via stochastic gradient langevin dynamics",
      "author" : [ "Max Welling", "Yee W Teh" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Variational inference (VI) [2, 16, 30] is a family of methods designed to approximate an intractable target distribution (typically known only up to a constant) with a tractable surrogate distribution.",
      "startOffset" : 27,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "Variational inference (VI) [2, 16, 30] is a family of methods designed to approximate an intractable target distribution (typically known only up to a constant) with a tractable surrogate distribution.",
      "startOffset" : 27,
      "endOffset" : 38
    }, {
      "referenceID" : 29,
      "context" : "Variational inference (VI) [2, 16, 30] is a family of methods designed to approximate an intractable target distribution (typically known only up to a constant) with a tractable surrogate distribution.",
      "startOffset" : 27,
      "endOffset" : 38
    }, {
      "referenceID" : 29,
      "context" : "Often this mismatch between the variational family and the true posterior manifests as underestimating the posterior variances of the model parameters [30].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 17,
      "context" : "Our method builds on black-box variational inference methods using the re-parameterization trick [18, 25, 28], applicable to a broad class of target distributions.",
      "startOffset" : 97,
      "endOffset" : 109
    }, {
      "referenceID" : 24,
      "context" : "Our method builds on black-box variational inference methods using the re-parameterization trick [18, 25, 28], applicable to a broad class of target distributions.",
      "startOffset" : 97,
      "endOffset" : 109
    }, {
      "referenceID" : 27,
      "context" : "Our method builds on black-box variational inference methods using the re-parameterization trick [18, 25, 28], applicable to a broad class of target distributions.",
      "startOffset" : 97,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "Variational methods minimize the KL-divergence, KL(q||π), between q(·;λ) and the true π as a function of variational parameters λ [1].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 20,
      "context" : "With a few exceptions [21, 31], most MCMC algorithms require evaluating a log-likelihood that touches all data at each step in the chain (sometimes many times per step).",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 30,
      "context" : "With a few exceptions [21, 31], most MCMC algorithms require evaluating a log-likelihood that touches all data at each step in the chain (sometimes many times per step).",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : ", multivariate normals [14], or a composition of invertible maps [27]).",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 26,
      "context" : ", multivariate normals [14], or a composition of invertible maps [27]).",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "In order for our method to be applicable to a general class of target distributions, we use black-box variational inference methods and the re-parameterization trick [18, 25, 28] to fit each component and mixture weights.",
      "startOffset" : 166,
      "endOffset" : 178
    }, {
      "referenceID" : 24,
      "context" : "In order for our method to be applicable to a general class of target distributions, we use black-box variational inference methods and the re-parameterization trick [18, 25, 28] to fit each component and mixture weights.",
      "startOffset" : 166,
      "endOffset" : 178
    }, {
      "referenceID" : 27,
      "context" : "In order for our method to be applicable to a general class of target distributions, we use black-box variational inference methods and the re-parameterization trick [18, 25, 28] to fit each component and mixture weights.",
      "startOffset" : 166,
      "endOffset" : 178
    }, {
      "referenceID" : 0,
      "context" : "In this section, the mixing parameter ρC+1 ∈ [0, 1] mixes between the new component, qC+1(·;λC+1) and the existing approximation, q.",
      "startOffset" : 45,
      "endOffset" : 51
    }, {
      "referenceID" : 29,
      "context" : "Further, the independence restriction can introduce local optima in the variational objective [30].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "The matrix determinant lemma [11]",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "Similarly, the Woodbury matrix identity [9] allows us to represent the inverse of Σ as (CC + I(v))−1 = I(−v)− I(−v)C(Ir + CTI(−v)C)−1CTI(−v) (27) which involves the inversion of a smaller, r× r matrix, which can be done in O(r) time.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "For mixture distributions, an efficient inference procedure is Expectation-Maximization (EM) [3].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "We suppress details of the general treatment of EM, and focus on EM for mixture models as presented in [1].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "Mixtures of mean field approximations [14] introduced mean fieldlike updates for a mixture approximation using a bound on the entropy term and model-specific parameter updates.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "Nonparametric variational inference [8] is a black-box variational inference algorithm that approximates a target distribution with a mixture of equally-weighted isotropic normals.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 27,
      "context" : "Similarly, [28] present a method for fitting mixture distributions as an approximation.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 9,
      "context" : "Finally, we note that [10] independently and in parallel proposed a closely-related idea for iterative “boosted” construction of variational mixture approximations.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 28,
      "context" : "Using a low-rank Gaussian as a variational approximation was explored in [29], using a PCA-like algorithm.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 25,
      "context" : "We also note that mixture distributions are a type of hierarchical variational model [26], where the component identity can be thought of as latent variables in our variational distribution.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 25,
      "context" : "While in [26] the authors optimize a lower bound on the ELBO to fit general hierarchical variational models, our approach integrates out the discrete latent variables because it is tractable to do so.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 3,
      "context" : "7 We borrow an example from [4], and estimate the binomial rates of success (batting 7Model and data from the mc-stan case studies, http://mc-stan.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "We use adam [17] for each stochastic optimization problem with default parameters.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 22,
      "context" : "In all experiments, we use autograd [23, 22] to obtain automatic gradients with respect to new component parameters.",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 21,
      "context" : "In all experiments, we use autograd [23, 22] to obtain automatic gradients with respect to new component parameters.",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "To highlight the fidelity of our method, we compare Variational Boosting to mean field VI and the No-U-Turn Sampler (NUTS) [13].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "This model was formulated to measure the relative rates of stop-and-frisk events for different ethnicities and in different precincts [6], and has been used as illustrative example of multi-level modeling [7, Chapter 15, Section 1].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 4,
      "context" : "sians can approximate smooth distributions to arbitrary precision (with enough components) [5], it remains an open question if our approach of fixing and iteratively adding components using this sequence of ELBO objectives will converge.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "Existing work has shown that this is the case for the alternative direction of KL-divergence, KL(π||q) [19, 24], but it remains to be shown for KL(q||π).",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 23,
      "context" : "Existing work has shown that this is the case for the alternative direction of KL-divergence, KL(π||q) [19, 24], but it remains to be shown for KL(q||π).",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 26,
      "context" : "For instance, compositions of invertible maps have been used to enrich variational families [27], as well as auxiliary variable variational models [20].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 19,
      "context" : "For instance, compositions of invertible maps have been used to enrich variational families [27], as well as auxiliary variable variational models [20].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 11,
      "context" : "When optimizing parameters of a variational family, it has been shown that the natural gradient can be more robust and lead to better optima [12, 15].",
      "startOffset" : 141,
      "endOffset" : 149
    }, {
      "referenceID" : 14,
      "context" : "When optimizing parameters of a variational family, it has been shown that the natural gradient can be more robust and lead to better optima [12, 15].",
      "startOffset" : 141,
      "endOffset" : 149
    } ],
    "year" : 2016,
    "abstractText" : "We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class. Our method, termed variational boosting, iteratively refines an existing variational approximation by solving a sequence of optimization problems, allowing the practitioner to trade computation time for accuracy. We show how to expand the variational approximating class by incorporating additional covariance structure and by introducing new components to form a mixture. We apply variational boosting to synthetic and real statistical models, and show that resulting posterior inferences compare favorably to existing posterior approximation algorithms in both accuracy and efficiency.",
    "creator" : "LaTeX with hyperref package"
  }
}