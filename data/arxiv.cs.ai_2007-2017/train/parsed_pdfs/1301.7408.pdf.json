{
  "name" : "1301.7408.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Context-specific approximation in probabilistic inference",
    "authors" : [ "David Poole" ],
    "emails" : [ "poole@cs.ubc.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "There is evidence that the numbers in probabilis tic inference don't really matter. This paper con siders the idea that we can make a probabilis tic model simpler by making fewer distinctions. Unfortunately, the level of a Bayesian network seems too coarse; it is unlikely that a parent will make little difference for all values of the other parents. In this paper we consider an approxima tion scheme where distinctions can be ignored in some contexts, but not in other contexts. We elab orate on a notion of a parent context that allows a structured context-specific decomposition of a probability distribution and the associated proba bilistic inference scheme called probabilistic par tial evaluation (Poole 1997). This paper shows a way to simplify a probabilistic model by ignor ing distinctions which have similar probabilities, a method to exploit the simpler model, a bound on the resulting errors, and some preliminary em pirical results on simple networks.\n1 Introduction\nBayesian networks (Pearl 1988) are a representation of in dependence amongst random variables. They are of interest because the independence is useful in many domains, they allows for compact representations of problems of proba bilistic inference, and there are algorithms to exploit the compact representations.\nRecently there has some evidence (Pradhan, Henrion, Provan, Del Favero & Huang 1996) that small distinctions in probability don't matter very much to the final probabil ity. Experts can't tell whether some value should be, for example, 0.6 or 0.7, but it doesn't seem to matter anyway. This would seem to indicate that, if we don't make such distinctions between close probabilities, it may be possible to simplify the probabilistic model, thus leading to faster inference.\nApproximation techniques have been used that give bounds\non probabilities. These have included stochastic simula tion methods that give estimates of probabilities by gener ating samples of instantiations of the network (Dagum & Luby 1997), search-based approximation techniques that search through a space of possible values to estimate prob abilities (Henrion 1991, D'Ambrosio 1992, Poole 1996), and methods that exploit special features of the conditional probabilities (Jordan, Ghahramani, Jaakkola & Saul 1997). Another class of methods have been suggested to approxi mate by simplifying a network, including to remove parents of a node (remove arcs) (Sarkar 1993), to remove nodes that are distant from the node of interest (Draper & Hanks 1994), or to ignore dependencies when the resultant factor will ex ceed some width bound (Dechter 1997). None of these methods take contextual structure into account.\nThis paper is based on simplifying the network based on making fewer distinctions. The network is simplified a pri ori as well as during inference, and posterior bounds on the resulting probability are obtained. This is done by remov ing distinctions in the probabilities. Unlike the search based methods that bound the probabilities by ignoring extreme probabilities (close to 0 or 1), it is the intermediate prob abilities that we want to collapse, rather than the extreme probabilities. As pointed out by Pradhan et al. (1996), al though probabilities such as 0.6 and 0.7 may be similar enough to be treated as the same, 0.0001 and 0, although close as numbers, are qualitatively different probabilities.\nUnfortunately the Bayesian network doesn't seem to be the most appropriate level to facilitate such simplifications. We wouldn't expect that the conditional probability of the child would not be affected very much for all values of its other parents. It seems more plausible that in some contexts the value of the parent doesn't make much difference.\nThe general idea is to simplify the network, by ignoring distinctions that don't make much difference in the con ditional probability, but what may be ignored may change from context to context. This builds on a method to exploit the contextual structure during inference (Poole 1997). In this paper we show how to simplify the network and how to give a bound on the error. Note that we are only able to give a posterior error at this stage; once we have make the simplifications to the network, we can derive bounds\n448 Poole\non the probability of the original network; it is still an open problem to predict the errors when simplifying the network.\nTo enable us to get computational levemge from the sim plified network, we need an inference method that can ex ploit the structure. We build on a notion of parent con texts (Poole 1997) where what acts as the parents of a vari able may depend on the values. This is similar to the rule based representations (Poole 1993) and related to the tree based representations (Boutilier, Friedman, Goldszmidt & Koller 1996) of conditional probability tables, but differs from the tree-based structure in a number of respects. First, the simplifications of collapsing distinctions preserves the rule-structure, but not the tree structure. Second, by treat ing rules as separately manipulable items, we can give more compact intermediate representations in the inference algo rithms than similar algorithms that use trees (Poole 1997).\nIn the next section we introduce Bayesian networks, a no tion of contextual parent that reflects structure in probability tables, an algorithm for Bayesian networks that exploits the network structure. and show how the algorithm can be ex tended to exploit the \"rule-based\" representation. Finally we show how to simplify the representation by ignoring distinctions between close probabilities, and give a bound on the resultant probabilities. Empirical results on networks that were not designed with context-specific independence in mind are presented.\n2 Background\n2.1 Bayesian Networks\nA Bayesian network (Pearl 1988) is an acyclic directed graph (DAG), with nodes labelled by random variables. We use the terms node and random variable interchangeably. Associated with a random variable x is its domain, val (x), which is the set of values the variable can take on. Similarly for sets of variables.\nA Bayesian network specifies a way to decompose a joint probability distribution. First, we totally order the variables of interest, XI, ... , Xn. Then we can factorise the joint prob ability:\nn P(XI, ... ,Xn) = fl P(xiiXi-1 ... xi)\ni=l n\n= fl P(xil1rx) i=l\nThe first equality is the chain rule for conjunctions, and the second uses 1rx;• the parents of x;, which are a minimal set of those predecessors of x; such that the other predeces sors of x; are independent of x; given 1fx;. Associated with the Bayesian network is a set of probabilities of the form P(xi1rx ), the conditional probability of each variable given its parents (this includes the prior probabilities of those vari ables with no parents). A Bayesian network represents a particular independence assumption: each node is indepen-\ndent of its non-descendants given its parents.\n2.2 Contextual Independence\nDefinition 2.1 Given a set of variables C, a context on C is an assignment of one value to each variable in C. Usually C is left implicit, and we simply talk about a context. Two contexts are incompatible if there exists a variable that is assigned different values in the contexts; otherwise they are compatible. A complete context is a context on all of the variables in a domain.\nBoutilier et al. (1996) present a notion of contextually in dependent that we simplify. We use this definition for a representation that looks like a Bayesian networks, but with finer-grain independence that can be exploited.\nDefinition 2.2 (Boutilier et al. 1996) Suppose X, Y and C are disjoint sets of variables, we say that X and Y are contextually independent given context c E val(C) if P(XIY=YI· C=c) = P(XIY=yz, C=c) for all YI. Yz E val(Y) such thatP(yi, c)> OandP(yz, c)> 0.\nWe use the notion of contextual independence to build a fac torisation of a joint probability that is related to the factori sation of a Bayesian network. We start with a total ordering the variables, as in the definition of a Bayesian network.\nDefinition 2.3 (Poole 1997) Suppose we have a total order ing of variables. Given variable x;, we say that c E val(C) where C s; {Xi-1, ... , xi}) is a parent context for x; if x; is contextually independent of {Xi-1· ... , xi}- C given c.\nIn a Bayesian network, each row of a conditional probability table for a variable forms a parent context for the variable. However, there are often not the smallest such set; there is often a much smaller set of parent contexts. A minimal parent context for variable x; is a parent context such that no subset is also a parent context.\nFor each variable x; and for each assignment Xi-I=Vi-I •... ,XI=VI of values to its preceding vari ables, there is a compatible minimal1 parent context 1r;;-J ... v1• The probability of an assignment of a value to each variable is then given by:\nn = fl P(x;=VniXi-I=Vi-1, ... , XI=VI)\ni=l n\n= flPC ·- ·I Vj-J ... VJ ) x, v, 1rx; i=l\n(1)\n(2)\nThis looks like the definition of Bayesian network, but which variables act as the parents depends on the values. The numbers required are the probability of each variable for each of its minimal parent contexts. There can be many fewer minimal parent contexts that the number of assign-\n1 If there is more than one, one is selected arbitrarily. This could happen, if for example, P(aib) = P(alc) :j:. P(aib, c).\nContext-specific approximation in probabilistic inference 449\nments to parents in a Bayesian network.\nFor this paper, we assume that the parent contexts for each variable are disjoint. That is, they each assign a different value to some variable. Any set of parent contexts can be converted into this form. This form is also the form that is the result of converting a tree into parent contexts. This assumption can be relaxed, but it makes the description of the algorithm more complicated.\nThe idea of the inference is instead of manipulating con ditional probability distributions, we maintain lower-level conditional probability assertions that we write as rules.\n2.3 Rule-based representations\nWe write the probabilities in contexts as rules, the general form of which is:\nYl =VI 1\\ • . . 1\\ Yj=Vj +--\nYj+I=Vj+ll\\ .. · I\\ Yk=Vk : p\nwhere each y; is a different variable, and v; E val (y;). Often we treat the left and right hand sides as sets of assignments of values to variables."
    }, {
      "heading" : "To represent a Bayesian network with context-specific inde pendence, j = 1 (i.e., there is only one variable in the head",
      "text" : "of the rule), z1 =v1 1\\ · · · 1\\zk=Wk is a parent context and\np = P(yl =WIIZI =Vi 1\\ ... 1\\ Zk=Wk)\nDefinition 2.4 Suppose R is a rule\nYl =VI 1\\ ... 1\\ Yj=Vj +--\nYj+I=Vj+l 1\\ . .. 1\\ Yk=Vk : p\nand z is a context on Z such that {YI • . . • , y,t} s; Z s; {XI, . .. , Xn}. We say that R is applicable in context z if z assigns v; to y; for each i such that 0 < i s k.\nDefinition 2.5 A rule base is a set of rules such that exactly one rule is applicable for each variable in each complete context.\nLemma 2.6 Given a rule base, the probability of any con text on {XI, ... , Xn} is the product of the probabilities of the rules that are applicable on that context.\nFor each x;, there is exactly one rule with x; in the head that is applicable on the context. The lemma now follows from equation (2) .\nDefinition 2.7 Two rules are compatible if there exists a context on which they are both applicable. Equivalently, they are compatible if they assign the srune value to each variable they have in common.\nIntuitively, the rule\na1 1\\ . .. 1\\ aj +-- b1 1\\ . . . 1\\ bk : p\nrepresents the contribution of the propositions a 1 1\\ . . . 1\\ aj\nin the context b1 1\\ . . . 1\\ bk. This often, but not always2, represents the conditional probability assertion\nP(a11\\ • • . 1\\ ajlb1 1\\ . . . 1\\ bk) = p.\n2.4 Probabilistic inference\nThe aim of probabilistic inference is to determine the poste rior probability of variables given some observations. In this section we outline a simple algorithm for Bayesian net inference called variable elimination, VE, (Zhang & Poole 1996) or bucket elimination for belief assessment, BEBA, (Dechter 1996), and is closely related to SPI (Shachter, D'Ambrosio & Del Favero 1990). This is a query oriented algorithm that exploits network structure for effi cient inference.\nTo determine the probability of variable h given evidence e, the conjunction of assignments to some variables e1, . . . , es. namely e1=0J/\\ . . . I\\ e8=o8, we use:\nP(hle!=Oil\\ . . • 1\\ es=Os)\nP(h 1\\ e1 =011\\ . • • 1\\ e8=0s) =\nP(e!=Oil\\ • • • I\\ es=Os)\nHere P(ei =01 1\\ • . . 1\\ e8=o3) is a normalising factor. The problem of probabilistic inference is thus reduced to the problem of computing a marginal probability (the proba bility of a conjunction). Let {YI, . . . , Yk} = {xi, . .. , Xn} - {h} - {e1. . .. , es}. and suppose that the y;'s are ordered according to some elimination ordering. To compute the marginal distribution, we sum out the y;'s in order. Thus:\nP(h 1\\ e1 =01 1\\ • • . 1\\ e8=0s)\n= L · · · LP(XJ, • · · ,Xn){e1=o1A ... Ae8=o,) Yk Y1\nn\n= L · · · L fl P(x;lnx){eJ=OJA ... Ae8=o8) Yk Y1 i=l\nwhere the subscripted probabilities mean that the associ-\n2In some cases, intermediate to the algorithm of Section 2.5, the value of some b; may also depend on some a;. This doesn't cause the invariant to be violated or any problems with the algorithm, but does affect the interpretation of the intermediate rules as statements of conditional probability. In terms of the VE or BEBA algorithm (Section 2.4), this can be seen in the network:\nwhich can be represented as the factors: P(aJbe)P(bJc)P(cJe)P(eJd)P(d) when e is eliminated, we construct the factor f(acbd) and the distribution is represented as P(bJc)P(d)f(acbd) In some sense f(acbd) can be considered as the contribution of ac in the context of bd, but does not represent the conditional proba bility P(acJbd). However, this factor would represent P(acJbd) if b was a parent of d rather than of c.\n450 Poole\nated variables are assigned the corresponding values in the function.\nThus the problem reduces to that of summing out variables from a product of functions. To sum out a variable y; from a product, we first distribute the factors that don't involve y; out of the sum. Suppose /J, . . . .fk are some functions of the variables that are multiplied together (initially these are the conditional probabilities), then\nL!J · · ·fk = /J · · ·fm Lfm+l • · ·fk y, Yi\nwhere /J . . . fm are those functions that don't involve vari able y;, and fm+I .. fk are those that do involve y;. We explicitly construct a representation for the new function Ly/m+I . . . fk, and continue summing out the remaining variables. After all the y;'s have been summed out, there sult is a function on h that is proportional to h's posterior distribution.\nUnfortunately space precludes a more detailed description; see Zhang & Poole (1996) and Dechter (1996) for more details.\n2.5 Probabilistic Partial Evaluation\nIn this section we show how the rule structure can be ex ploited in evaluation. This is essentially the same as Poole (1997) but one bug has been fixed and it is described at a different level of detail. The general idea is based on VE or BEBA, but we operate at the finer-grained level of rules, not on the level of factors or buckets. What is analogous to a factor or a bucket consists of sets of rules. In particular, given a variable to eliminate, we distribute out all rules that don't involve this variable. We create a new rule set that is the result of summing out the variable; we only need to consider those rules that involve the variable.\nGiven a set of rules representing a probability distribution, a query variable, a set of observations, and an elimination ordering on the remaining variables, we set the observed variables to their observed values, eliminate the remain ing variables in order, and normalise (see Figure 1). We maintain a set of rules with the following invariant when eliminating the variables:\nThe probability of a context on the non-eliminated non-observed variables conjoined with the obser vations can be obtained by multiplying the prob abilities associated with rules that are applicable on that context. Moreover, for each context on the non-eliminated non-observed variables, and for each such variable, there is exactly one appli cable rule with that variable in the head.\nThe following section describe the details of the algorithm. See Poole (1997) for detailed examples.\nNote that when we are eliminating e, we just look at the rules that contain e. All other rules are preserved.\nProcedure compute belief Input: rules, observations, query variable, elimination ordering Output: posterior distribution on query variable\n1. Set the observed variables (Section 2.5.1). 2. For each variable e in the elimination ordering:\n2a. Combine compatible rules containing e (Section 2.5.2) 2b. Variable partial evaluation to eliminate e (Section 2.5.3)\n2.5.1 Evidence\nWe can set the values of all evidence variables before sum ming out the remaining non-query variables (as in VE). Sup pose e1=01 1\\ • . • 1\\ es=Os is observed.\n• Remove any rule that contains e;=oj, where o; oft oi in the head or the body.\n• Remove any term e;=o; in the body of a rule.\n• Replace any e;=o; in the head of a rule by true.\nThe first two rules preserve the feature that the contexts of the rules are exclusive and covering. These rules also set up the loop invariant (as only the rules compatible with the observations will be chosen).\nThe rules with true in the head are treated as any other rules, but we never eliminate true. When constructing rules with true in the head, we use the equivalence: true 1\\ a = a. true is compatible with every context.\nNote that incorporating observations always simplifies the rule-base. This is why we advocate doing it first.\n2.5.2 Combining compatible rules\nThe first step when eliminating e is to combine the rules for the variables that become dependent on eliminating e. For each value Vj E val(e), and for each maximal set of consistent rules that contain e = Vj in the body,\nar +- br 1\\ e = VJ :PI\nak +- bk 1\\ e = VJ : Pk where a; and b; are sets of assignments of values to variables, we construct the intermediate rule with head U;a; and body (U;b;) - (U;a;) and with probability D;Pi· Note that the rules constructed are all incompatible and cover all of the cases the original rules covered.\nContext-specific approximation in probabilistic inference 451\nWe can then remove all of the original rules with e in the body.\nIntuitively, the program invariant is maintained because, for every complete context, the new rule is used instead of the k original rules. (The complete proof relies on showing that every complete context has the same probability).\n2.5.3 Variable partial evaluation\nTo eliminate e, we must sum over all of the values of e. Suppose the domain of e is val(e) = {vr, • . . , vm}. For each set of rules resulting from combining compatible rules:\nar � bt 1\\ e = VI : P I\nam � bm 1\\ e = Vm : Pm cr 1\\ e = vr � di : qi\nCm 1\\ e = Vm � dm : qm such that (Uiai) U (Uibi) U (UiCi) U (Uidi) is compatible, we construct the rule with head (Uiai) u (UiCi) and with body (Uibi) U (Uidi)- (Uiai) U (UiCi), and with probability LiPiqi. We remove all of the rules containing e, and e is eliminated. Intuitively, the program invariant is maintained because each complete context c on the remaining variables (not including e) has probability Li c 1\\ e=vi.\n2.5.4 Determining the posterior probability\nOnce the evidence has been incorporated into the rule-base, the program invariant implies that the posterior probability of any context of the non-eliminated, non-observed vari ables is proportional to the product of the probabilities of the rules that are applicable on the context.\nOnce all non-query, non-evidence variables have been elim inated, we end up with rules of the form\ntrue � h = Vi : p h= Vi� :p\nWe can determine the probability of h = Vi 1\\ e, where e is the evidence, by multiplying the rules containing h = Vi together. The posterior probability can be obtained by dividing by LiP(h = Vi 1\\ e).\n3 Approximation\nThe approximation method relies on the algorithm for ex ploiting the structure. Intuitively we make the rule-base simpler by ignoring distinctions in close probabilities.\nLet's call the given conditional probabilities of the variables the parameters of the network. In a probability distribution\nthey have restrictions such as\nVc L P(x=vlc) = 1. veval(x)\nConsider what happens when we increase any of the param eters (and thus violate the restrictions):\nLemma 3.1 The \"probability\" of a conjunction is mono tonic in the parameters.\nWhen we increase the parameters, the \"probabilities\" of conjuncts increases. The term ''probability\" is in scare quotes, as when the parameters are increased, the number can't be interpreted as probabilities, as they no longer sum to one. This lemma can be easily proved as the probabil ity of a conjunction is the sum of products of non-negative numbers.\nWe can bound P(c) for any conjunction c of values to vari ables by p-(c) andP+(c), such that\nP-(c)-;:;. P(c)-;:;. p+(c) p- can be constructed by decreasing the parameters and p+ can be constructed by increasing the parameters.\nGiven such functions, we can bound the posterior probabil ity of h given evidence e using\nP(hje) = P(h 1\\ e)\nP(h 1\\ e)+ P(h 1\\ e) where P(ii 1\\ e) is the sum of the probabilities for the other values for h conjoined with the observations e. We can use the bounds on P to give us:\np-(h 1\\ e) P-(h 1\\ e)+ P+(h 1\\ e)\np+(h 1\\ e) -;:;. P(hie) -;:;. ------=--\np+(h 1\\ e)+ p-(h 1\\ e) The general idea is to simplify the rules by dropping condi tions. That is, we make fewer distinctions in the conditional probabilities. Each rule now has two associated values, the parameter for p- and the parameter for p+. Definition 3.2 An approximating rule is of the form:\nYl =VI 1\\ · · · 1\\ Yj=Vj � Yj+t=Vj+I 1\\ · · · 1\\Yk=Vk :pi,Pu\nwhere all of the Yi are distinct variables, each Vi E val(yi), and 0 -;:;. PI -;:;. Pu· The definitions of applicable and com patible are the same as for the standard rule bases.\nDefinition 3.3 An approximating rule base is a set of ap proximating rules such that exactly one rule is applicable for each variable in each complete context.\nDefinition 3.4 An approximating rule-base ARB approxi mates rule-base RB if for every rule\nh �b : p\n452 Poole\nin RB , where the ai and the Cj are assignments of values to variables, there are rules ·\nh1 +-- b1 : [j, UI\nSuch that h = Uihi, and for all i, bi is compatible with hub, and ni [j ::5 p ::5 ni Uj.\nThe idea of this definition is that them rules in the approx imating will be used instead of the rules in the exact rule base.\nThe reason that we allow multiple rules to approximate a single rule is that it is often useful to approximate, say, a 1\\ b +-- c, where a and b are dependent with the two rules a +-- c and b +-- c. This is the basis of the mini-bucket approximation scheme of Dechter (1997).\nA single rule in the ARB typically approximates many rules in theRB .\n3.1 Approximating a rule base\nThis paper considers two ways to simplify the rule base.\n3.1.1 Dropping conditions\nThe first is to just drop conditions from rules (as in Quinlan (1993)). The lower bound of the resulting rule is the mini mum of the rules with the same head and with bodies that are compatible with the newly created rule. Similarly the upper bound is the maximum of the upper bounds on the consistent rules with the same head. Rules with the same head and with bodies that are supersets can be removed. Compatible rules may need to be made disjoint.\nExample 3.5 Consider the rules3 for a:\na +-- b 1\\ c : 0.6, 0.6 (3) a +-- b 1\\ c 1\\ d : 0.8, 0.8 (4) a +-- b 1\\ c 1\\ d : 0.4, 0.4 (5) a +-- b 1\\ e : 0.06, 0.06 (6) a +-- b 1\\ e 1\\ c : 0.96, 0.96 (7) a +-- b 1\\ e 1\\ c: 0.16, 0.16 (8)\nWe can remove the c condition from rule (3) resulting in the rule:\na +-- b : 0.4; 0.8 (9) In this case rules (4) and (5) can be removed as they are covered by rule (9). Note that this has simplified the rule case considerably, but not reduced the number of parents of a. c is still relevant, but only in the context of b 1\\ e. Example 3.6 Not all simplifications are useful. If we re-\n3Here we assume the variables are Boolean, and writex = true as x and x =false as :X.\nmove b from rule (3), we get the rule\na+-- c: 0.06, 0.96 (10)\nRule (7) can be deleted, and we need to add the condition c to rule (6). This shows that removing conditions can be quite subtle and not all cases of removing conditions lead to something useful.\n3.1.2 Resolving Rules\nA second method of simplifying the rule base is as a form of resolution. From rules of the form:\na1 +-- b1 1\\ e = VI : l1, UI\nwe can \"resolve\" one, and derive:\nUiai +-- Ui bi: min([j, ... , lm), max(u1 , . . . , Um) The intuition is that for any context for which the resulting rule is applicable, one of the fonnerrules must be applicable. We must be careful to carry out enough resolutions and remove enough rules so that for each variable in each context a single rule is applicable.\nIn our implementation and in the experiments reported in Section 4 we restrict the resolution to the case where the ai are all identical and the bi are all identical. In this case, when two rules are resolved they can be removed.\nExample 3.7 Given the rules for Example 3.5, we can re solve rules (3) and ( 4 ), and resolve rules (3) and (5) resulting in:\na +-- b 1\\ d : 0.6, 0.8 a +-- b 1\\ d : 0.4, 0.6\n(11)\n(12)\nThese two rules can replace rules (3), (4), and (5). Notice how this results in a different knowledge base than that ob tained by removing conditions. These two rules could be combined again to produce rule (9).\nThe second method of resolving complementary literals in the bodies is more subtle than removing conditions. If the rules cover the cases, and are exclusive then repeated reso lution results in the same rule as obtained by removing the conditions. However, as Example 3. 7 shows, there are some approximations that can be obtained via resolution that are not just removing literals.\nFinally note that these simplification operations preserve the rule structure of a conditional probability table, but do not necessarily preserve tree structure (as in Boutilier et al. (1996)). The reduced rule base need not be equivalent to a simpler decision tree than the original.\nContext-specific approximation in probabilistic inference 453\nR( th) ts the number of rules resultmg from carrymg out the resolution step constrained so that the resulting bounds are less than or equal to th. In particular, the third column given an exact representation of the conditional probability table.\nFigure 3: Effect on rule size of structure and approximation\n3.2 Inference and Approximation\nThe simplifications can be carried out prior to inference as well as during inference. When doing inference for the approximate rule base, we just maintain two numbers for each rule. This is equivalent to running the algorithm once on the upper bounds and once on the lower bounds.\n4 Experimental Results\nWe have make preliminary tests of the algorithm on an 18- node car diagnosis Bayesian network shown in Figure 2. The network was not designed for structured tables or ap proximation.\nThe reduction of the initial representation is shown in Fig ure 3. This shows just the variables with more than one parent. The second column shows the size of the table in the traditional Bayesian network. The third column shows how the use of resolution can extract rules without any ap proximation. To obtain these numbers, we carried out the resolving rule (section 3.1.2) approximation, only resolv-\ning rules when the heads and bodies (other than the value being removed) are identical. These numbers show the in herent structure in the conditional probability distributions. We carried out a myopic choice of which rules to resolve. This could have resulted in not as much structure as possible being found, but we couldn't find any cases where another choice would have created fewer choices.\nThe fourth column shows the number of rules when we carry out the same resolving rule approximation, but allowed any resolution (in a myopic manner) that resulted in a rule where the range of probabilities was less than or equal to 0 .1. The 0.1 was an arbitrarily chosen threshold, but the results are not very sensitive to the exact number chosen.· Note that we did not distinguish close extreme probabilities (e.g., 0 and 0.09) and close non-extreme probabilities (e.g., 0.6 and 0.69), even though this could have made a difference.\nFirst consider determining P(st). Suppose we eliminate the variables in order: af , as, fs, hl, al, cs, ba, sp, sm, tm, sq, pv, ds, cc, ss, bv, mf . The largest factor created using VE or BEBA (corresponding to the width (Dechter 1996)) is is 72 when summing out fs. This is also the number of rules created when using the rules without any contextual sim plification. The maximum number of rules created for the simplified rule base (where structure is exploited, but no ap proximation) is 32 when summing outpv. For the approxi mate rule base, the maximum number of rules created at any stage is 14, when summingfs. The probability computed in the exact case is 0.280. Collapsing rules whose probability differs by at most 0.1 gives the range 0.210 : 0.327.\nIn computing P(pvist =false), (pv is \"voltage at plug\") with elimination ordering af , as, fs, hl, al, cs, ba, sp, sm, tm, sq, ds, cc, ss, bv, mf . For the exact case, the maximum number of rules created was 22 when summing out ds. For the approximate case, the maximum number of rules created was 18 also when summing out ds. VE has a table size of 36 at the same stage. The posterior probability of pv = strong is 0.192. The error range given the collapsed rule set is 0.148 : 0.268. Conditioning on the fact that the car starts gives P(pvist = true) = 0.802. The error bounds with the collapsed rule set is 0. 750 : 0.846.\nSimilar results arise from the Alarm network (Beinlich, Suermondt, Chavez & Cooper 1989)4. The variable re quiring the largest table, catechol, has a table size of 108. There are 34 rules when converted to rules with no thresh old. There are 16 rules when the rule base is simplified so that rules whose probabilities differ by 0.1 are collapsed.\nThese results are still preliminary. We need more experience with how much structure we gain by approximation, how much we lose structure during inference and how large the posterior errors are.\n4This is based on the version available from the Norsys web site. This network is simpler than the car diagnosis network.\n454 Poole\n5 Conclusion\nThis paper has presented a method for approximating poste rior probabilities in Bayesian networks with structured prob ability tables given as rules. This algorithm lets us maintain the contextual structure, avoiding the necessity to do a case analysis on the parents of a node at the most detailed level.\nIt does the approximation by maintaining upper and lower bounds. Note that these are very different to the upper and lower bounds of say Dempster-Shafer belief functions (Provan 1990). Here the bounds represent approximations rather than ignorance. We are doing Bayesian inference, but approximately.\nThe method in this paper of collapsing rules is related to the method of Dearden & Boutilier (1997) to prune decision trees in structured MDPs, but is more general in applying to arbitrary Bayesian networks.\nThe main open problem is in finding good heuristics for elimination ordering, and knowing when it is good to !lP proximate.\nReferences\nBeinlich, I., Suermondt, H., Chavez, R. & Cooper, G. (1989). The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks, Proc. Conference on Artificial Intel ligence in Medical Care, London.\nBoutilier, C., Friedman, N., Goldszmidt, M. & Koller, D. (1996). Context-specific independence in Bayesian networks, in E. Horvitz & F. Jensen (eds), Proc. Twelfth Conf on Uncertainty in Artificial Intelligence (UAI-96), Portland, OR, pp. 115-123.\nDagum, P. & Luby, M. (1997). An optimal approxima tion algorithm for Bayesian inference, Artificial Intel ligence 93(1-2): 1-27.\nD'Ambrosio, B. (1992). Real-time value-driven diagnosis, Proc. Third International Workshop on the Principles of Diagnosis, Rosario, Washington, pp. 86-95.\nDearden, R. & Boutilier, C. (1997). Abstraction and ap proximate decision theoretic planning, Artificial In telligence 89(1): 219-283.\nDechter, R. (1996). Bucket elimination: A unifying framework for probabilistic inference, in E. Horvitz & F. Jensen (eds), Proc. Twelfth Conf on Uncer tainty in Artificial Intelligence (UA/-96), Portland, OR, pp. 211-219.\nDechter, R. (1997). Mini-buckets: A general scheme for generating approximations in automated reasoning, Proc. 15th International Joint Conf. on Artificial Intel ligence (IJCAI-97), Nagoya, Japan, pp. 1297-1302.\nDraper, D. & Hanks, S. (1994). Localized partial evaluation of belief networks, in R. L. de Mantaras & D. Poole (eds), Proc. Tenth Conf. on Uncertainty in Artificial Intelligence (UAI-94), Morgan Kaufmann Publishers, Seattle, WA., pp. 170-177.\nHenrion, M. (1991). Search-based methods to bound diag nostic probabilities in very large belief networks, Proc. Seventh Conf on Uncertainty in Artificial Intelligence (UA/-91), Los Angeles, CA, pp. 142-150.\nJordan, M. I., Ghahramani, Z., Jaakkola, T. S. & Saul, L. K. (1997). An introduction to variational methods for graphical models, Technical report, MIT Computa tional Cognitive Science. URL: http://www.ai.mit.edu/projects/jordan.html\nPearl, J. (1988). Probabilistic Reasoning in Intelligent Sys tems: Networks of Plausible Inference, Morgan Kauf mann, San Mateo, CA.\nPoole, D. (1993). Probabilistic Horn abduction and Bayesian networks, Artificial Intelligence 64(1): 81- 129.\nPoole, D. (1996). Probabilistic conflicts in a search algorithm for estimating posterior probabilities in Bayesian networks, Artificial Intelligence 88: 69-100.\nPoole, D. (1997). Probabilistic partial evaluation: Ex ploiting rule structure in probabilistic inference, Proc. 15th International Joint Conf on Artificial In telligence (/JCAI-97), Nagoya, Japan, pp. 1284-1291. URL: http://www. cs. ubc. calspider/poolelabstractslpro pa.html\nPradhan, M., Henrion, M., Provan, G., Del Favero, B. & Huang, K. (1996). The senstivitity of belief networks to imprecise probabilities: an empirical investigation, Artificial Intelligence 85: 363-397.\nProvan, G. (1990). A logic-based analysis of Dempster Shafer theory, International Journal of Approximate Reasoning 4:451-498.\nQuinlan, J. R. (1993). C4.5 Programs for Machine Learn ing, Morgan Kaufmann, San Mateo, CA.\nSarkar, S. (1993). Using tree-decomposible structures to ap proximate belief networks, Proc. Ninth Conf on Un certainty in Artificial Intelligence (UA/-93 ), Washing ton DC, pp. 376-382.\nShachter, R. D., D'Ambrosio, B. D. & Del Favero, B. D. (1990). Symbolic probabilistic inference in belief net works, Proc. 8th National Conference on Artificial In telligence, MIT Press, Boston, pp. 126-131.\nZhang, N. & Poole, D. (1996). Exploiting causal inde pendence in Bayesian network inference, Journal of Artificial Intelligence Research 5: 301-328."
    } ],
    "references" : [ {
      "title" : "An optimal approxima­",
      "author" : [ "P. Dagum", "M. Luby" ],
      "venue" : null,
      "citeRegEx" : "Dagum and Luby,? \\Q1997\\E",
      "shortCiteRegEx" : "Dagum and Luby",
      "year" : 1997
    }, {
      "title" : "Real-time value-driven diagnosis",
      "author" : [ "B. D'Ambrosio" ],
      "venue" : null,
      "citeRegEx" : "D.Ambrosio,? \\Q1992\\E",
      "shortCiteRegEx" : "D.Ambrosio",
      "year" : 1992
    }, {
      "title" : "Abstraction and ap­",
      "author" : [ "R. Dearden", "C. Boutilier" ],
      "venue" : null,
      "citeRegEx" : "Dearden and Boutilier,? \\Q1997\\E",
      "shortCiteRegEx" : "Dearden and Boutilier",
      "year" : 1997
    }, {
      "title" : "Bucket elimination: A unifying",
      "author" : [ "R. Dechter" ],
      "venue" : null,
      "citeRegEx" : "Dechter,? \\Q1996\\E",
      "shortCiteRegEx" : "Dechter",
      "year" : 1996
    }, {
      "title" : "Mini-buckets: A general scheme for",
      "author" : [ "R. Dechter" ],
      "venue" : null,
      "citeRegEx" : "Dechter,? \\Q1997\\E",
      "shortCiteRegEx" : "Dechter",
      "year" : 1997
    }, {
      "title" : "Localized partial evaluation of belief networks",
      "author" : [ "D. Draper", "S. Hanks" ],
      "venue" : "Proc. Tenth Conf. on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Draper and Hanks,? \\Q1994\\E",
      "shortCiteRegEx" : "Draper and Hanks",
      "year" : 1994
    }, {
      "title" : "Search-based methods to bound diag­ nostic probabilities in very large belief networks",
      "author" : [ "M. Henrion" ],
      "venue" : "Proc. Seventh Conf on Uncertainty in Artificial Intelligence (UA/-91),",
      "citeRegEx" : "Henrion,? \\Q1991\\E",
      "shortCiteRegEx" : "Henrion",
      "year" : 1991
    }, {
      "title" : "An introduction to variational methods for graphical models, Technical report, MIT Computa­",
      "author" : [ "M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul" ],
      "venue" : null,
      "citeRegEx" : "Jordan et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Jordan et al\\.",
      "year" : 1997
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Sys­ tems: Networks of Plausible Inference, Morgan Kauf­",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "Probabilistic Horn abduction and Bayesian networks",
      "author" : [ "D. Poole" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "Poole,? \\Q1993\\E",
      "shortCiteRegEx" : "Poole",
      "year" : 1993
    }, {
      "title" : "Probabilistic conflicts in a search algorithm for estimating posterior probabilities in Bayesian networks",
      "author" : [ "D. Poole" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "Poole,? \\Q1996\\E",
      "shortCiteRegEx" : "Poole",
      "year" : 1996
    }, {
      "title" : "Probabilistic partial evaluation: Ex­ ploiting rule structure in probabilistic inference",
      "author" : [ "D. Poole" ],
      "venue" : "Proc. 15th International Joint Conf on Artificial In­ telligence (/JCAI-97),",
      "citeRegEx" : "Poole,? \\Q1997\\E",
      "shortCiteRegEx" : "Poole",
      "year" : 1997
    }, {
      "title" : "The senstivitity of belief networks to imprecise probabilities: an empirical investigation",
      "author" : [ "M. Pradhan", "M. Henrion", "G. Provan", "B. Del Favero", "K. Huang" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "Pradhan et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 1996
    }, {
      "title" : "A logic-based analysis of Dempster­ Shafer theory, International Journal of Approximate Reasoning 4:451-498",
      "author" : [ "G. Provan" ],
      "venue" : null,
      "citeRegEx" : "Provan,? \\Q1990\\E",
      "shortCiteRegEx" : "Provan",
      "year" : 1990
    }, {
      "title" : "Using tree-decomposible structures to ap­ proximate belief networks",
      "author" : [ "S. Sarkar" ],
      "venue" : "Proc. Ninth Conf on Un­ certainty in Artificial Intelligence (UA/-93",
      "citeRegEx" : "Sarkar,? \\Q1993\\E",
      "shortCiteRegEx" : "Sarkar",
      "year" : 1993
    }, {
      "title" : "Symbolic probabilistic inference in belief net­",
      "author" : [ "R.D. Shachter", "B.D. D'Ambrosio", "B.D. Del Favero" ],
      "venue" : "works, Proc. 8th National Conference on Artificial In­ telligence,",
      "citeRegEx" : "Shachter et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Shachter et al\\.",
      "year" : 1990
    }, {
      "title" : "Exploiting causal inde­ pendence in Bayesian network inference",
      "author" : [ "N. Zhang", "D. Poole" ],
      "venue" : "Journal of Artificial Intelligence Research",
      "citeRegEx" : "Zhang and Poole,? \\Q1996\\E",
      "shortCiteRegEx" : "Zhang and Poole",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "We elab­ orate on a notion of a parent context that allows a structured context-specific decomposition of a probability distribution and the associated proba­ bilistic inference scheme called probabilistic par­ tial evaluation (Poole 1997).",
      "startOffset" : 227,
      "endOffset" : 239
    }, {
      "referenceID" : 8,
      "context" : "Bayesian networks (Pearl 1988) are a representation of in­ dependence amongst random variables.",
      "startOffset" : 18,
      "endOffset" : 30
    }, {
      "referenceID" : 14,
      "context" : "Another class of methods have been suggested to approxi­ mate by simplifying a network, including to remove parents of a node (remove arcs) (Sarkar 1993), to remove nodes that are distant from the node of interest (Draper & Hanks 1994), or to ignore dependencies when the resultant factor will ex­ ceed some width bound (Dechter 1997).",
      "startOffset" : 140,
      "endOffset" : 153
    }, {
      "referenceID" : 4,
      "context" : "Another class of methods have been suggested to approxi­ mate by simplifying a network, including to remove parents of a node (remove arcs) (Sarkar 1993), to remove nodes that are distant from the node of interest (Draper & Hanks 1994), or to ignore dependencies when the resultant factor will ex­ ceed some width bound (Dechter 1997).",
      "startOffset" : 320,
      "endOffset" : 334
    }, {
      "referenceID" : 12,
      "context" : "As pointed out by Pradhan et al. (1996), al­ though probabilities such as 0.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 11,
      "context" : "This builds on a method to exploit the contextual structure during inference (Poole 1997).",
      "startOffset" : 77,
      "endOffset" : 89
    }, {
      "referenceID" : 11,
      "context" : "We build on a notion of parent con­ texts (Poole 1997) where what acts as the parents of a vari­ able may depend on the values.",
      "startOffset" : 42,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : "This is similar to the rule­ based representations (Poole 1993) and related to the tree­ based representations (Boutilier, Friedman, Goldszmidt & Koller 1996) of conditional probability tables, but differs from the tree-based structure in a number of respects.",
      "startOffset" : 51,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : "Second, by treat­ ing rules as separately manipulable items, we can give more compact intermediate representations in the inference algo­ rithms than similar algorithms that use trees (Poole 1997).",
      "startOffset" : 184,
      "endOffset" : 196
    }, {
      "referenceID" : 8,
      "context" : "A Bayesian network (Pearl 1988) is an acyclic directed graph (DAG), with nodes labelled by random variables.",
      "startOffset" : 19,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "3 (Poole 1997) Suppose we have a total order­ ing of variables.",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 3,
      "context" : "In this section we outline a simple algorithm for Bayesian net inference called variable elimination, VE, (Zhang & Poole 1996) or bucket elimination for belief assessment, BEBA, (Dechter 1996), and is closely related to SPI (Shachter, D'Ambrosio & Del Favero 1990).",
      "startOffset" : 178,
      "endOffset" : 192
    }, {
      "referenceID" : 7,
      "context" : "Unfortunately space precludes a more detailed description; see Zhang & Poole (1996) and Dechter (1996) for more details.",
      "startOffset" : 71,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "Unfortunately space precludes a more detailed description; see Zhang & Poole (1996) and Dechter (1996) for more details.",
      "startOffset" : 88,
      "endOffset" : 103
    }, {
      "referenceID" : 9,
      "context" : "This is essentially the same as Poole (1997) but one bug has been fixed and it is described at a different level of detail.",
      "startOffset" : 32,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "See Poole (1997) for detailed examples.",
      "startOffset" : 4,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "This is the basis of the mini-bucket approximation scheme of Dechter (1997).",
      "startOffset" : 61,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "The largest factor created using VE or BEBA (corresponding to the width (Dechter 1996)) is is 72 when summing out fs.",
      "startOffset" : 72,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : "Note that these are very different to the upper and lower bounds of say Dempster-Shafer belief functions (Provan 1990).",
      "startOffset" : 105,
      "endOffset" : 118
    } ],
    "year" : 2011,
    "abstractText" : "There is evidence that the numbers in probabilis­ tic inference don't really matter. This paper con­ siders the idea that we can make a probabilis­ tic model simpler by making fewer distinctions. Unfortunately, the level of a Bayesian network seems too coarse; it is unlikely that a parent will make little difference for all values of the other parents. In this paper we consider an approxima­ tion scheme where distinctions can be ignored in some contexts, but not in other contexts. We elab­ orate on a notion of a parent context that allows a structured context-specific decomposition of a probability distribution and the associated proba­ bilistic inference scheme called probabilistic par­ tial evaluation (Poole 1997). This paper shows a way to simplify a probabilistic model by ignor­ ing distinctions which have similar probabilities, a method to exploit the simpler model, a bound on the resulting errors, and some preliminary em­ pirical results on simple networks.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}