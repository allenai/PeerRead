{
  "name" : "1512.00165.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Using Local Membership Queries",
    "authors" : [ "Galit Bary", "Shai Shalev-Shwartz", "Amit Daniely", "Alon Gonen", "Nir Rosenfled", "Yoav Wald", "Yossi Arjevani", "Nomi Vinokurov" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Learning Using Local Membership Queries\nGalit Bary\nSubmitted in partial fulfillment of the requirements of the degree of Master of Science\nUnder the supervision of\nProf. Shai Shalev-Shwartz\nAmit Daniely\nSeptember 2015\nRachel and Selim Benin School of Computer Science and Engineering\nThe Hebrew University of Jerusalem Israel\nar X\niv :1\n51 2.\n00 16\n5v 1\n[ cs\n.L G\n] 1\nD ec\n2 01\n5\nAbstract\nClassic machine learning algorithms learn from labelled examples. For example, to design a machine translation system, a typical training set will consist of English sentences and their translation to French. There is a stronger model, in which the algorithm can also query for labels of new examples it creates. E.g, in the translation task, the algorithm can create a new English sentence, and request its translation from the user during training. This combination of examples and queries, that resembles human learning patterns, has been widely studied. Yet, despite many theoretical results, query algorithms are almost never used. One of the main causes for this is a report (Baum and Lang, 1992) on very disappointing empirical performance of a query algorithm. These poor results were mainly attributed to the fact that the algorithm queried for labels of examples that are artificial, and impossible to interpret by humans.\nIn this work we study a new model of local membership queries (Awasthi et al., 2012), which tries to resolve the problem of artificial queries. In this model, the algorithm is only allowed to query the labels of examples which are close to examples from the training set. E.g., in translation, the algorithm can change individual words in a sentence it has already seen, and then ask for the translation. In this model, the examples queried by the algorithm will be close to natural examples and hence, hopefully, will not appear as artificial or random. In this work we focus on 1-local membership queries (i.e., queries of distance 1 from an example in the training sample). We show that 1-local membership queries are already stronger than the standard learning model. We also present an experiment on a well known NLP task of sentiment analysis. In this experiment, the users were asked to provide, in a way that resembles 1-local queries, more information than merely indicating the label. We present results that illustrate that this extra information is beneficial in practice."
    }, {
      "heading" : "Acknowledgments",
      "text" : "I would like to thank my advisor Prof. Shai Shalev-Shwartz for having me on his outstanding team and for his support and inspiration. I would also like to thank Amit Daniely, for his guidance and mentorship. His extensive knowledge and patience were invaluable. It has been a privilege to work with him.\nTo Alon Gonen, Nir Rosenfled, Yoav Wald, Yossi Arjevani, Nomi Vinokurov and Avishai Wagner for their remarkable friendship and counsel. To the NLP lab and especially Effi Levi for all his assistance in the empirical work and to my officemates Zahi Ajami and Dikla Cohn for their wonderful companionship.\nI would like to thank my parents for all the love and support throughout the years, and to the Weisberg family for their help, especially Susan for her editorial comments. Last but not least, I would like to thank my husband Dov for his enduring support that is expressed on so many levels – encouraging me during difficult times, editing my drafts and providing home cooked meals.\nContents"
    }, {
      "heading" : "1 Introduction 6",
      "text" : ""
    }, {
      "heading" : "2 Previous Work 8",
      "text" : ""
    }, {
      "heading" : "2.1 PAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8",
      "text" : "2.2 Membership Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3 Baum and Lang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.4 Local Membership Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.5 Other Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"
    }, {
      "heading" : "3 Setting 12",
      "text" : "3.1 The PAC Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2 (Local) Membership Queries Model . . . . . . . . . . . . . . . . . . . . . . . 12"
    }, {
      "heading" : "4 Learning DNFs with Evident Examples Using 1-local MQ 13",
      "text" : "4.1 Definitions and Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4.2 Upper Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 4.3 A Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20"
    }, {
      "heading" : "5 Experiments 23",
      "text" : "5.1 Is the additional data useful? . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5.2 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n5.2.1 Sentiment Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5.2.2 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5.2.3 Pre-processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.2.4 Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.2.5 Scoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.2.6 The algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n5.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.3.1 Precision and Recall . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.3.2 Over-fitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5.4 Comparing to other Feature Selection Methods . . . . . . . . . . . . . . . . 29\n6 Conclusion and Future Work 31"
    }, {
      "heading" : "1 Introduction",
      "text" : "How do humans learn? Say we look at the process of a child learning how to recognize a cat. We can focus on two types of input. The first type of input is when a child’s parent points at a cat and states “Look, a cat!”. The second type of input is an answer to the child’s frequent question “What is that?”, which the child may pose when seeing a cat, but also when seeing a dog, a mouse, a rabbit, or any other small animal.\nThese two types of input were the basis for the learning model originally suggested in the celebrated paper “A theory of the learnable” (Valiant, 1984). In Valiant’s learning model, the learning algorithm has access to two sources of information - EXAMPLES and ORACLE. The learning algorithm can call EXAMPLES to receive an example with its label (sampled from the “nature”). Additionally, the learning algorithm can use ORACLE, which provides the label of any example presented to it. With these two input types, we can look at two models of learning: learning using only calls for EXAMPLES, and learning using calls for both EXAMPLES and ORACLE. The first is the standard Probably Approximately Correct (PAC) model. The second is the so called PAC+MQ (Membership Queries) model. There has been a lot of theoretical work searching for the limits of the additional strength of membership queries. The use of membership queries in addition to examples was proven to be stronger than the standard PAC model in many cases (Angluin, 1987; Blum and Rudich, 1992; Bshouty, 1995; Jackson, 1994)(see section 2).\nDespite that the MQ model seems much stronger, both intuitively and formally, it is rarely used in practice. This is commonly believed to result from the fact that in many cases it is not easy to implement MQ algorithms, that can create new and artificial examples to be labeled as part of the training phase. This problem of labeling artificial examples was highlighted by the experiment of Baum and Lang (1992). Baum and Lang implemented a membership query algorithm proposed by Baum (1991) for learning halfspaces . Their algorithm had very poor results, which was attributed to the fact that the algorithm created artificial and unnatural examples, which resulted in a noisy labeling. We elaborate on this experiment and criticize its conclusions in section 2.\nA suggested solution to the problem of unnatural examples was proposed by Awasthi et al. (2012). They suggested a mid-way model of learning with queries, but only restricted ones. The queries that their model allows the algorithm to ask are only local queries, i.e., queries that are close in some sense to examples from the sample set. Hopefully, examples which are similar to natural examples will also appear to be natural, or at least close to natural, and in any case will be far from appearing random or artificial. In their work, Awasti et al. started to investigate the power and the limitations of this model of local queries. They proved positive results on learning sparse polynomials with Oplogpnqqlocal queries under what they defined as locally smooth distributions1, which in some sense generalize the uniform and product distributions. They also proposed an algorithm that learns DNF formulas under the uniform distribution in quasi-polynomial time using only Oplogpnqq-local queries.\nThe exciting ideas of Awasthi et al. (2012) leave many directions for future work. One issue is that their analysis holds for a restricted family of distributions. While these results\n1locally α-smooth distributions can be defined as the class of distributions for which the logarithm of the density function is logpαq-Lipschitz with respect to the Hamming distance.\nprovide evidence of the excessive power of local queries, the distributional assumptions are rather strong.\nOur work follows Awasthi et al., and is focused on 1-local queries, which are the closest to the original PAC model. We formulate an arguably natural distributional assumption, and present an algorithm that uses 1-local membership queries to learn DNF formulas under this assumption. We also provide a matching lower bound: Namely, we prove that learning DNFs under our assumption is hard without the use of queries, assuming that learning decision trees is hard. This is the first example of a natural problem in which 1-local queries are stronger than the vanilla PAC model (it complements the work of Awasthi et al. who showed a similar result for a highly artificial problem).\nFinally, we provide some empirical evidence that using local queries can be helpful in practice, and importantly, that the implementation of the queries is easy, straightforward, and can be acquired by crowdsourcing without the use of an expert. We present a method for using local queries to perform a user-induced feature selection process, and present results of this protocol on the task of sentiment analysis of tweets. Our results show that by acquiring a more expressive data set, using (a variant of) 1-local queries, we can achieve better results with fewer examples. Based on the fact that a smaller data set is sufficient, we gain twice: we need less manpower for the labeling process and less computing power for the training process. We note that similar experiments also present encouraging results along this line (Raghavan and Allan, 2007; Raghavan et al., 2005; Settles, 2011; Druck et al., 2009). This supplies more evidence that such query-based methods can be useful in practice."
    }, {
      "heading" : "2 Previous Work",
      "text" : ""
    }, {
      "heading" : "2.1 PAC",
      "text" : "Valiant’s Probably Approximately Correct (PAC) model of learning (Valiant, 1984) formulates the problem of learning a concept from examples. Examples are chosen according to a fixed but unknown and arbitrary distribution on the instance space. The learner’s task is to find a prediction rule. The requirement is that with high probability, the prediction rule will be correct on all but a small fraction of the instances.\nA few positive results are known in this model - i.e., concept classes that have been proven to be PAC-learnable. Maybe the most significant example is the class of halfspaces. More examples include relatively weak classes such as DNFs and CNFs with constantly many terms (Valiant, 1984), and rank k decision trees (Ehrenfeucht and Haussler, 1989) for a constant k.\nDespite these positive results, most PAC learning problems are probably intractable. In fact, beyond the results mentioned above, almost no positive results are known. Furthermore, several negative results are known. For example, learning automatons, logarithmic depth circuits, and intersections of polynomially many halfspaces are all intractable, assuming the security of various cryptographic schemes (Kearns and Valiant, 1994; Klivans et al., 2006). In (Daniely et al., 2014; Daniely and Shalev-Shwatz, 2014; Daniely et al., 2013), it is shown that learning DNF formulas, and learning intersections of ωplogpnqq halfspaces are intractable under the assumption that refuting random k-SAT is hard."
    }, {
      "heading" : "2.2 Membership Queries",
      "text" : "The PAC model is a “passive” model in which the learner receives a random data set of examples and their labels and then outputs a classifier. A stronger version would be an active model in which the learner gathers information about the world by asking questions and receiving responses. Several types of active models have been proposed: the Membership Query Synthesis, Stream-Based Selective Sampling, and Pool-Based Sampling (Settles, 2010). Our work is in the area of the “Membership Queries” (MQ) model which was presented in (Valiant, 1984). In this model the learner is allowed to query for the label of any particular example that it chooses (even examples that are not in the given sample).\nThis model has been shown to be stronger in several scenarios. Some examples of concept classes that have been proven to be PAC-learnable only if membership queries are available include: The class of Deterministic Finite Automatons (Angluin, 1987), the class of k-term DNF for k “ logpnqlogplogpnqq (Blum and Rudich, 1992), the class of decision trees and k-almost monotone-DNF formulas (Bshouty, 1995), the class of intersections of k-halfspaces (Baum, 1991) and the class of DNF formulas under the uniform distribution (Jackson, 1994). The last of these results was built upon Freund’s boosting algorithm (Freund, 1995) and the Fourier-based technique for learning using membership queries due to (Kushilevitz and Mansour, 1993).\nIt should be noted that there are cases in which the additional strength of MQ does not help. E.g., in the case of learning DNF and CNF formulas (Angluin and Kharitonov, 1995), and in the case of distribution free agnostic learning (although in the distribution-specific agnostic setting membership queries do increase the power of the learner) (Feldman, 2009)."
    }, {
      "heading" : "2.3 Baum and Lang",
      "text" : "As discussed above, there has been widespread and significant theoretical work in the PAC + MQ model. On the other hand, almost no practical work on implementing these ideas has been done. A well-known exception is the work of Baum and Lang (1992). They applied a variation of the MQ algorithm for learning a linear classifier proposed in Baum (1991). This algorithm uses the idea that given two examples, one positive and one negative, and a query oracle, it is possible to find an approximately accurate separating halfspace by using a binary search on the line between the positive and negative examples. Their experiment attempts to evaluate this idea in practice. The task that they chose is the task of binary digit classification. The algorithm would receive two examples, one positive and one negative (say, an image of the digit 4 and an image of the digit 7) and would return the weights of the halfspace. The generalization error of the halfspace would then be tested on other examples from the data. The query technique they used in the experiment is different than in the original algorithm: “A direct implementation of this algorithm would repeatedly flash images on the screen during the binary search and would require the test subject to type in the correct label for each image. Because this process seemed likely to be error prone, we instead provided an interface that permitted the test subject to scan through the input space using the mouse and then click on an image that seemed to lie right at the edge of recognizability” (from Baum and Lang (1992)).\nFor an example of what the users saw on the screen see figure 1.\nThey compared the performance of their algorithm to five other variants, three classic PAC (sample based) algorithms: Backpropogation, Perceptron and simplex, and two baselines: the first returns the perpendicular bisection of the line segments connecting the two examples, and the second returns a randomly oriented hyperplane through the midpoint of the line. The query learning algorithm uses the additional information obtained from the users as described above, while the three PAC algorithms use additional examples drawn from the data set. All three PAC algorithms outperformed the query-based algorithm. More surprisingly, even the baseline of choosing the perpendicular bisection line had significantly better results than the halfspace created by the query algorithm. The only method that\nwas worse than the query based method was the random bisector method. They suggest that the reason for the poor results is that the question the users had to answer, to find the boundary pattern, lay outside the range of the human competence.\nThis work led many to the conclusion that membership queries are not useful in practice (Settles (2010); Balcan et al. (2006); Dasgupta (2004) and more). We argue that there are several problems with this conclusion. First and foremost, the task that the users were asked to perform (scanning through images and finding the boundary between digits) is not an intuitive task, and it is very easy to think of other variants for queries which would be more suitable. It is therefore not surprising that the labeling turned out to be noisy considering the nature of the question at hand. Second, their algorithm did not use the PAC abilities; it used queries but did not use the additional option to sample extra points for the data."
    }, {
      "heading" : "2.4 Local Membership Queries",
      "text" : "Several suggestions have been made of ways to solve the problem of the algorithm’s generation of unnatural examples. The most common one was to drop the whole framework of membership queries and focus on the other types of active learning: stream-based and pool-based. The idea is to filter existing examples taken from a large unlabeled data set drawn from the distribution rather than creating artificial examples. Another suggestion is to give the human annotator the option of answering “I don’t know”, or to be tolerant of some incorrect answers. The theoretical framework is the model of an incomplete membership oracle in which the answers to a random subset of the queries may be missing. This notion was first presented in Angluin and Slonim (1994), and then followed by the notion of limited MQ and malicious MQ. (Angluin et al. (1997); Blum et al. (1995); Sloan and Turán (1994); Bisht et al. (2008)).\nThe third method is to restrict the examples that the learning algorithm can query to examples that are similar to examples drawn from the distribution. This is formalized in the work of Awasthi et al. (2012). They present the concept of learning using only local membership queries. This framework deals with the problem raised by (Baum and Lang, 1992). By questioning about examples which are close to examples from the distribution we escape the problem of generating random or non-classifiable examples.\nThe work of Awasthi et al. focused on the n-dimensional boolean hyper-cube X “ t´1, 1un and on Oplogpnqq-local queries, i.e., the learning algorithm is given the option to query the label of any point for which there exists a point in the training sample with hamming distance lower than Oplogpnqq. The model they suggested is a mid-way model between the PAC model (0-local queries) and the PAC + MQ model (n-local queries). Their main result is that t-sparse polynomials are learnable under locally smooth distributions using O plogpnq ` logptqq-local queries. Another interesting result that they presented is that the class of DNF formulas is learnable under the uniform distribution in quasi-polynomial time (nOplog lognq) using Oplogpnqq-local queries. They also presented some results regarding the strength of local MQ. They proved that under standard cryptographic assumptions, using pr ` 1q-local queries is more powerful than using r-local queries (for every 1 ď r ď n ´ 1). They also showed that local queries do not always help. They showed that if a concept class is agnostically learnable under the uniform distribution using k-local queries\n(for constant k) then it is also agnostically learnable (under the uniform distribution) in the PAC model."
    }, {
      "heading" : "2.5 Other Related Work",
      "text" : "In section 5, we give some experimental evidence that the use of extra information from the user is helpful. There have been other works along the same line. Druck et al. (2009) propose a pool-based active learning approach in which the user provides labels for input features, rather than instances. The users are asked to provide a “label” for input features, where a labeled input feature denotes that a particular feature is highly indicative of a particular label. Following that, Settles (2011) presented an active learning annotation interface, in which the users label instances and features simultaneously. At any point in time, an instance and a list of features for each label is presented on the screen. The user can choose to either label the instance, choose a feature from the list as being indicative, or add a new feature of his or her choice. Another similar work is of Raghavan and Allan (2007) and Raghavan et al. (2005). They studied the problem of tandem learning where they combine uncertainty sampling for instances along with co-occurrence-based interactive feature selection. All the above experiments were conducted on the text domain and the features were always unigrams. The experiments presented encouraging results of using the human annotators, either by reaching better results, or by showing that the excessive use of annotators can reduce the size of the data set, and sometimes both."
    }, {
      "heading" : "3 Setting",
      "text" : ""
    }, {
      "heading" : "3.1 The PAC Model",
      "text" : "Our framework is an extension of the PAC (Probably Approximately Correct) model of learning. Before introducing it, we will briefly review PAC learning. We will only consider binary classification where the instance space is X “ Xn “ t´1, 1un and the label space is Y “ t0, 1u. A learning problem is defined by a hypothesis class H Ă t0, 1uX . We assume that the learner receives a training set\nS “ tpx1, h‹px1qq, px2, h‹px2qq, . . . , pxm, h‹pxmqqu P pX ˆ Yqm\nwhere the xi’s are sampled i.i.d. from some unknown distribution D on X and h‹ : X Ñ Y is some unknown hypothesis. We will focus on the so-called realizable case where h‹ is assumed to be in H. The learner returns (a description of) a hypothesis ĥ : X Ñ Y. The goal is to approximate h‹, namely to find ĥ : X Ñ Y with loss as small as possible, where the loss is defined as LD,h‹pĥq “ Px„D ´ ĥpxq ‰ h‹pxq ¯ . We will require our algorithms to return a hypothesis with loss ă in time that is polynomial in n and 1 . Concretely,\nDefinition 1 (Learning algorithm) We say that a learning algorithm A PAC learns H if\n• There exists a function mA pn, q ď poly ` n, 1 ˘ , such that for every distribution D over X , every h‹ P H and every ą 0, if A is given a training sequence\nS “ tpx1, h‹px1qq, px2, h‹px2qq, . . . , pxm, h‹pxmqqu\nwhere the xi’s are sampled i.i.d. from D and m ě mApn, q, then with probability of at least 34 (over the choice of S) 2, the output ĥ of A satisfies LD,h‹pĥq ă .\n• Given a training set of size m\n– A runs in time polypm,nq. – The hypothesis returned by A can be evaluated in time polypm,nq.\nDefinition 2 (PAC learnability) We say that a hypothesis class H is PAC learnable if there exists a PAC learning algorithm for this class."
    }, {
      "heading" : "3.2 (Local) Membership Queries Model",
      "text" : "Learning with membership queries is an extension of the PAC model in which the learning algorithm is allowed to query the labels of specific examples in the domain set. A membership query is a call to an ORACLE which receives as input some x P X and returns h‹pxq. This is called a “membership query” because the ORACLE returns 1 if x is in the set of examples positively labeled by h‹.\n2The success probability can be amplified to 1´ δ by repetition.\nDefinition 3 (Membership-Query Learning Algorithm) We say that a learning algorithm A learns H with membership queries if\n• There exists a function mA pn, q ď poly ` n, 1 ˘ , such that for every distribution D over X , every h‹ P H and every ą 0, if A is given access to membership queries, and a training sequence\nS “ tpx1, h‹px1qq, px2, h‹px2qq, . . . , pxm, h‹pxmqqu\nwhere the xi’s are sampled i.i.d. from D and m ě mApn, q, then with probability of at least 34 (over the choice of S), the output ĥ of A satisfies LD,h‹pĥq ă .\n• Given a training set of size m\n– A asks at most polypm,nq membership queries. – A runs in time polypm,nq. – The hypothesis returned by A can be evaluated in time polypm,nq.\nOur work will deal with a specific type of membership queries, ones that are in some way close to examples that are already in the sample. Concretely, we say that a membership query x P X is q-local if there exists a training example x1 whose Hamming distance3 from x is at most q.\nDefinition 4 (Local-Query Learning Algorithm) We say that a learning algorithm A learns H with q-local membership queries if A learns H with membership queries that are all q-local.\nDefinition 5 We say that a hypothesis class H is q-LQ learnable if there exists a q-Localquery learning algorithm for this class.\nLearning Under a Specific Family of Distributions\nIn the classic PAC model discussed above, the learning algorithm needs to be probablyapproximately correct for any distribution D on X and any hypothesis h‹ P H. In this work we will have guarantees with respect to more restricted families. We will say that A learns H w.r.t a family D of pairs pD, hq of distributions on X and hypotheses in H if the following holds: The algorithm A satisfies the requirements of a learning algorithm whenever the pair D and h in the definition of a learning algorithm belongs to D . Similar considerations apply also to the notion of learning with (local) membership queries."
    }, {
      "heading" : "4 Learning DNFs with Evident Examples Using 1-local MQ",
      "text" : ""
    }, {
      "heading" : "4.1 Definitions and Notations",
      "text" : "Definition 6 (Disjunction Normal Form Formula) A DNF term is a conjunction of literals. A DNF formula is a disjunction of DNF terms.\n3We only consider the instance space t´1, 1un, so the hamming distance is natural. However, the definition can be extended to other metrics.\nEach DNF formula over n variables naturally induces a function h : t´1, 1un Ñ t0, 1u (when we standardly identify t0, 1u with “True” and “False”). We denote by hF the function induced by the DNF formula F .\nRemark 1 We will look at succinctly described hypotheses (e.g., a DNF with a small number of terms) and on small, but non-negligible probabilities. For simplicity, we will take the convention that small is at most n2 and non negligible is at least 1\nn3 . All of our results\ncan be easily generalized to the case where “small” and “non-negligible” are defined as ď nc1 and ě 1nc2 for any constants c1, c2 ą 0.\nDefinition 7 Denote by HDNF the hypothesis class of all functions that can be realized by a DNF with a small number of terms. That is\nHDNF “ thF : F is a DNF formula with at most n2 termsu\nIntuitively, when evaluating a DNF formula on a given example, we check a few conditions (corresponding to the formula’s terms), and deem the example positive if one of the conditions holds. We will consider the case that for each of these conditions, there is some chance to see a “prototype example”. Namely, an example that satisfies only this condition in a strong (or evident) way.\nDefinition 8 Let F “ T1 _ T2 _ . . . _ Td be a DNF formula. An example x P t´1, 1un satisfies a term Ti (with respect to the formula F ) evidently if :\n• It satisfies Ti. (In particular, hF pxq “ 1)\n• It does not satisfy any other term Tk (for k ‰ i) from F.\n• No coordinate change will turn Ti False and another term Tk True. Concretely, if for j P rns we denote x‘j “ px1, . . . , xj´1,´xj , xj`1, . . . , xnq, then for every coordinate j P rns, if x‘j satisfies F (i.e. if hF px‘jq “ 1) then x‘j satisfies Ti and only Ti.\nThe first distributional assumption that we consider is that each positive example satisfies one term evidently.\nDefinition 9 A pair pD, h‹q of a distribution D over t´1, 1un and h‹ : t´1, 1un Ñ t0, 1u is realized by a small DNF with evident examples if there exists a DNF formula F “ T1_T2_ . . ._Td over t´1, 1un with d ď n2 such that h‹ “ hF and additionally, every positive example x P t´1, 1un with Dpxq ą 0 satisfies one of F ’s terms evidently.\nOne of the assumptions in our definition is that the target function can be realized by a DNF formula for which every example satisfies at most one term. For a function that is realized by a decision tree this always holds. So, in a sense, our assumption holds for functions that can be realized by a “stable” decision tree.\nThe above definition makes a strong assumption, namely that every positive example is an evidence for one term. The next definition relaxes that assumption and only assumes that for every term there is a non-negligible probability to see an evident example.\nDefinition 10 A pair pD, h‹q of a distribution D over t´1, 1un and h‹ : t´1, 1un Ñ t0, 1u is weakly realized by a small DNF with evident examples if there exists a DNF formula F “ T1 _ T2 _ . . . _ Td over t´1, 1un with d ď n2 such that h‹ “ hF and for every term Ti there is a non-negligible\n4 probability to see an example that satisfies this term evidently.\nFor example, our assumption holds for every distribution D, provided that h‹ can be realized by a DNF formulas in which any pair of different terms contains two opposite literals."
    }, {
      "heading" : "4.2 Upper Bounds",
      "text" : "We will now present two learning algorithms that use 1-LQ, and prove that each of these algorithms learn the class HDNF with respect to the families of distributions defined above. Both algorithms use the following claim that follows directly from definition 8\nClaim 1 Let F “ T1 _ T2 _ . . . _ Td be a DNF formula over t´1, 1un. Then for every x P t´1, 1un that satisfies a term Ti evidently (with respect to F ), for every j P rns it holds that:\nhF px‘jq “ 1 ðñ the term Ti does not contain the variable xj\nAlgorithm 1 Create a DNF formula Input: S P pt´1, 1un ˆ t0, 1uqm Output: A DNF formula H\nstart with an empty DNF formula H for all px, yq P S do\nif y “ 1 then define T “ x1 ^ x1 ^ x2 ^ x2 ^ . . .^ xn ^ xn for 1 ď j ď n do\nquery x‘j (to get h‹px‘jq) if h‹px‘jq “ 1 then\nremove xj and xj from T if h‹px‘jq “ 0 then\nif xj “ 1 then remove xj from T if xj “ 0 then remove xj from T\nH “ H _ T return H\nTheorem 1 The hypothesis class HDNF is 1-LQ learnable with respect to distributions that are realized by a DNF with evident examples.\n4Recall that non-negligible is at least 1 n3\nProof We will prove that algorithm 1 learns HDNF with 1-local membership queries. First, it is easy to see that this algorithm is efficient: For a training set of size m the algorithm asks for at most n ¨m 1-local membership queries, and runs in time Opnmq. Likewise, the hypothesis that the algorithm returns is a DNF formula with at most m terms and every term is of size at most n, therefore it can be evaluated in time polynomial in mn.\nNow, let D be a distribution on t´1, 1un and h‹ : t´1, 1un Ñ t0, 1u be a hypothesis such that the pair pD, h‹q is realized by a small DNF with evident examples. Let F “ T1 _ T2 _ . . . _ Td be that small DNF formula, (in particular h‹ “ hF and d ď n2). For ą 0 we take a sample S “ tpxi, h‹pxiqumi“1 where txiumi“1 are sampled i.i.d from D and m “ 2n2 log 2n2 ě 2d log 2d .\nLet H be the DNF formula returned by the algorithm after running on S, and let ĥ be the function induced by H. We will prove that with probability of at least 3/4 (over the choice of the examples) LD,h‹pĥq ă 4 .\nFrom the assumption on the distribution we get that every instance x that satisfies the formula (in our case every x such that px, 1q P S), satisfies exactly one term T . For every one of these positive instances from S, we will show that we add that exact term to H. For every such x we start with a full term (containing all the possible literals) and then for every j P rns, at iteration j:\n• if h‹pxq “ h‹px‘jq “ 1 we know from claim 1 that the variable xj cannot appear in T - so we remove it and its negation from the current term.\n• if h‹pxq “ 1 and h‹px‘jq “ 0 we know that either xj or xj appears in T and we remove the one that cannot appear in T according to the value of xj .\nAfter n iterations we get exactly T - the term that x satisfies evidently. Therefore - H will contain every term from F for which there was an instance x in S that satisfies it - other then that H will contain no other terms. In other words,\nP x„D\nrh‹pxq “ 0^ ĥpxq “ 1s “ 0\nand we get that\nLD,h‹pĥq “ P x„D rh‹pxq ‰ ĥpxqs “ P x„D rh‹pxq “ 1^ ĥpxq “ 0s\nDenote by pi the probability to sample x (from D) that will satisfy Ti, and let Ai be the event that S did not contain any x which satisfies Ti. Then\nP x„D rh‹pxq “ 1^ ĥpxq “ 0s “ P x„D rDi P rds such that x satisfies Ti ^ ĥpxq “ 0s\nď d ÿ\ni“1 P x„D rx satisfies Ti ^ ĥpxq “ 0s “\nd ÿ i“1 pi ¨ 1Ai\nNotice that since pi is the probability to sample x we get that P S„Dm rAis “ p1´ piqm\nNow if we look at the expectation we get\nE S„Dm rLD,h‹pĥqs ď E S„Dm\nr d ÿ\ni“1 pi ¨ 1Ais\n“ d ÿ\ni“1 pi E S„Dm r1Ais\n“ d ÿ\ni“1 pi P S„Dm rAis\n“ d ÿ\ni“1 pip1´ piqm\n“ ÿ\ni|piă 2d\npip1´ piqm ` ÿ\ni|piě 2d\npip1´ piqm\nď ÿ\ni|piă 2d\n2d `\nÿ\ni|piě 2d\np1´ piqm\nď d ¨ 2d `\nÿ\ni|piě 2d\ne´mpi\nď 2 ` d ¨ e´m 2d\nSince m ě 2d log 2d we get ErLD,h‹pĥqs ă and using Markov’s inequality we obtain\nP S„Dm\nrLD,h‹pĥqs ě 4 s ď ErLD,h‹pĥqs 4 ă 1 4\nAlgorithm 2 Create a DNF formula with checking and deleting false terms Input: S1, S2 Ď pt´1, 1un ˆ t´1, 1uqm Output: a DNF formula H\nstart with an empty DNF formula H for all px, yq P S1 do\nif y “ 1 then define T “ x1 ^ x1 ^ x2 ^ x2 ^ . . .^ xn ^ xn for 1 ď j ď n do\nquery x‘j (to get h‹px‘jq) if h‹px‘jq “ 1 then\nremove xj and xj from T if h‹px‘jq “ 0 then\nif xj “ 1 then remove xj from T if xj “ 0 then remove xj from T\nH “ H _ T for all T in H do\nfor all px, yq P S2 do if T pxq “ 1 but y “ 0 then\nremove T from H return H\nTheorem 2 The hypothesis class HDNF is 1-LQ learnable with respect to distributions that are weakly realized by a DNF with evident examples.\nProof We will prove that algorithm 2 learns HDNF with 1-local membership queries. In this case we will have two sample sets - S1 of size m1 which will be used as before - to build the terms of H, and S2 of size m2 - a separate set to check the terms that were built. Again, it is easy to see that this algorithm is efficient. For training sets S1 of size m1 and S2 of size m2 the algorithm asks for at most n ¨m1 1-local membership queries. The running time of the first loop is Opnm1q and in that loop we add at most m1 terms to H so the running time of the second loop is Opm1m2q. All in all the running time is polynomial in pm1,m2, nq. Also, the hypothesis that the algorithm returns is a DNF formula with at most m1 terms and every term is of size at most n, therefore it can be evaluated at time polynomial in m1n.\nNow, let D be a distribution on t´1, 1un and h‹ : t´1, 1un Ñ t0, 1u be a hypothesis such that the pair pD, h‹q is realized by a small DNF with evident examples. Let F “ T1 _ T2 _ . . ._ Td be that small DNF formula, (in particular h‹ “ hF and d ď n2). Denote by H “ T̂1 _ T̂2 _ . . . _ T̂k the DNF formula algorithm 2 returns. Following the same argument from the last proof, a term Ti will be added to H in the first loop if S1 contains an example that satisfies Ti evidently. We will define m1 so that with high probability for every term Ti there will be px, 1q P S1 such that x satisfies Ti evidently. Denote by si the probability to sample x (from D) that satisfies Ti evidently, and let\ns “ mintsiudi“1. Since for every term the probability to see an evident example is nonnegligible, s ě n´3. For every i, the probability of not seeing an example in S1 that satisfies Ti evidently is\np1´ siqm ď p1´ sqm ď e´sm ď e´ m n3\nIf we set m1 to be n 3 logp8n2q ě n3 logp8dq we get that the probability of not seeing an example that satisfies Ti evidently (when sampling S1 from Dm1) is less than 18d and from the union bound we get that the probability that the sample will contain an evident example for every term is at least 78 . Therefore with probability of at least 7 8 we will add every Ti to H in the first loop. In the second loop, when we remove terms from H, we only remove terms which contradicts one of the examples in S2. Since all of the examples in the sample set are labeled by F , we will never remove a term that is a part of F Therefore with probability of at least 78 H will contain all of F ’s terms. Formally,\nP S1„Dm1 r P x„D rh‹pxq “ 1^ ĥpxq “ 0s “ 0s ě 7 8\nNote that we are not done, as the algorithm might create a wrong term (when using a ”non-evident” example). For this reason we add the second loop. We use the sample S2 to test every term T̂i that was added to H in the first loop. If we see an example x such that T̂ipxq “ 1 but h‹pxq “ 0 we remove T̂i and continue to the next term. Now denote by pi the probability to sample x (from D) that will satisfy T̂i, and by Ai the event that T̂i is a wrong term (not from F) but the ”checking” step did not discover that. Then\nP x„D rĥpxq “ 1^ h‹pxq “ 0s “ P x„D rDi P rks such that x satisfies T̂i ^ h‹pxq “ 0s\nď k ÿ\ni“1 P\nx„D rx satisfies T̂i ^ h‹pxq “ 0s\n“ k ÿ\ni“1 pi ¨ 1Ai\nNote that since Ai is the event that there wasn’t any example in S2 which satisfied T̂i (otherwise the checking step would discover that T̂i is wrong) this is the same situation as in the proof of theorem 1, so\nP S2„Dm2\nrAis “ p1´ piqm2\nBy the same analysis of the former proof, we get that if the size of S2 is ě 2k log 2k then\nP S2„Dm2 r P x„D rh‹pxq “ 0^ ĥpxq “ 1s ě 4 s ď 1 4\nFinally we notice that k ď m1, because for each example in S1 the algorithm adds at most one term to H. So we can set m1 as above and m2 “ 2m1 log 2m1 and if we run algorithm\n2 on S1 and S2 we get that with probability of at least 1´ p14 ` 1 8q “ 3 4 ´ 1 8 over sampling"
    }, {
      "heading" : "S1 and S2",
      "text" : "LD,h‹pĥq “ P x„D\nrh‹pxq ‰ ĥpxqs\n“ P x„D rh‹pxq “ 1^ ĥpxq “ 0s ` P x„D rh‹pxq “ 0^ ĥpxq “ 1s ď 0` 4 “ 4"
    }, {
      "heading" : "4.3 A Lower Bound",
      "text" : "In this section we provide evidence that the use of queries in our upper bounds is crucial. We will show that the problem of learning poly-sized decision trees can be reduced to the problem of learning DNFs w.r.t. distributions that are realized by a small DNF with evident examples. As learning decision trees is widely believed to be intractable (in fact, even learning the much smaller class of logpnq-juntas is conjectured to be hard), this reduction serves as an indication that the problems we considered are hard without membership queries.\nDefinition 11 A decision tree over t´1, 1un is a binary tree with labels chosen from x1, . . . , xn on the internal nodes, and labels from t0, 1u on the leaves. Each internal node’s left branch is viewed as the ´1 branch; the right branch is the 1 branch. Each decision tree over n variables induces a function h : t´1, 1un Ñ t0, 1u in the following way: For a decision tree T , a vector a P t´1, 1un defines a path in the tree from the root to a specific leaf by choosing ai’s branch at each node xi and the value that the function hT returns on a is defined to be the label of the leaf at the end of this path.\nDefinition 12 Denote by HDT the hypothesis class of all functions that can be realized by a decision tree with a small number of leaves. That is\nHDT “ thT : T is a DT with at most n2 leavesu\nTheorem 3 PAC learning the hypothesis class HDNF w.r.t distributions that are realized by a small DNF with evident examples is as hard as PAC learning HDT.\nThe proof will follow from the following claim:\nClaim 2 There exists a mapping (a reduction) ϕ : t´1, 1un Ñ t´1, 1u2n, that can be evaluated in polypnq time so that for every decision tree T over t´1, 1un there exists a DNF formula F over t´1, 1u2n such that the following holds:\n1. The number of terms in F is upper bounded by the number of leaves in T\n2. hT “ hF ˝ ϕ\n3. @x such that hT pxq “ 1 , ϕpxq satisfies some term in F evidently.\nProof We will denote t´1, 1un by Xn and t´1, 1u2n by X2n. Define ϕ as follows:\n@x “ px1, x2, . . . , xnq P Xn ϕpx1, x2, . . . , xnq “ px1, x1, x2, x2, . . . , xn, xnq\nNow, for every tree T , we will build the desired DNF formula F as follows: First we build F 1 - a DNF formula over t´1, 1un . Every leaf labeled ’1’ in T will define the following termtake the path from the root to that leaf and form the logical AND of the literals describing the path. F 1 will be a disjunction of these terms. Now, for every term T in F 1 we will define a term φpT q over X2n in the following way: Let PT “ ti P rns : xi appear in Tu and NT “ ti P rns : xi appear in Tu. So\nT “ ľ\njPPT\nxj ľ\njPNT\nxj\nDefine\nφpT q “ ľ\njPPT\nx2j´1 ľ\njPPT\nx2j ľ\njPNT\nx2j´1 ľ\njPNT\nx2j\nFinally, define F to be the DNF formula over X2n by\nF “ ł\nTPF 1 φpT q\nWe will now prove that ϕ and F satisfy the required conditions. First, ϕ can be evaluated in linear time in n. Second, it is easy to see that hT “ hF ˝ϕ, and as every term in F matches one of T ’s leaves, the number of terms in F cannot exceed the number of leaves in T . It is left to show that the third requirement holds. Let there be an x such that hT pxq “ 1, then x is matched to one and only one path from T ’s root to a leaf labeled ’1’. From the construction of F , x satisfies one and only one term in F 1 because every term is matched to exactly one path from T ’s root to a leaf labeled 1. Regarding the last requirement - that no coordinate change will make one term from F False and another one True - we made sure this will not happen by “doubling” each variable. By this construction, in order to change a term from False to True at least two coordinate must change their value.\nProof [of theorem 3] Suppose we have an efficient algorithm A that PAC learns HDNF with respect to distributions that are realized by DNF with evident examples. Using the reduction from claim 2 we will build an efficient algorithm B that will PAC learn HDT. For every training set with examples from Xn:\nS “ tpx1, h‹px1qq, px2, h‹px2qq, . . . , pxm, h‹pxmqqu P pXn ˆ t0, 1uqm\nwe define a matching training set with examples from X2n, using ϕ from the above claim:\nS̃ :“ tpϕpx1q, h‹px1qq, pϕpx2qq, h‹px2qq, . . . , pϕpxmq, h‹pxmqqu P pX2n ˆ t0, 1uqm\nThe algorithm B will work as follows: Given a training set S, B will construct S̃ “ ϕpSq and then run A with input S̃. Let ĥ be the output of A when running on S̃, B will return ĥ ˝ ϕ. Since ϕ can be evaluated in polypnq time and A is efficient, we get that B is also efficient.\nWe will prove that algorithm B is a learning algorithm for the class HDT. Since A is a learning algorithm for the class HDNF with respect to distributions that are realized by a small DNF with evident examples, there exists a function mA pn, q ď poly ` n, 1 ˘\n, such that for every pD, h‹q that is realized by a small DNF with evident examples and every ą 0, if A is given a training sequence\nS “ tpx1, h‹px1qq, px2, h‹px2qq, . . . , pxm, h‹pxmqqu\nwhere the xi’s are sampled i.i.d. from D and m ě mApn, q, then with probability of at least 34 (over the choice of S), the output ĥ of A satisfies LD,h‹pĥq ď .\nLet D be a distribution on Xn and let hT be a hypothesis that can be realized by a small DT. Define a distribution D̃ on X2n by,\n˜pDqpzq “ #\nDpxq if Dx P Xn such that z “ ϕpxq 0 otherwise\nSince ϕ is one-to-one, D̃ is well defined and is a valid distribution on X2n. Now, as hT is realized by a small DT, then from the conditions that ϕ satisfies we get that there exists a DNF formula F such that hT “ hF ˝ ϕ and the pair pD̃, hF q is realized by a small DNF with evident examples. Now for every ą 0 we take a sample S “ tpx1, hT px1qq, px2, hT px2qq, . . . , pxm, hT pxmqqu with m “ mAp2n, q and obtain that with probability of at least 34 it holds that\nLD,hT pBpSqq “ LD,hT pĥ ˝ ϕq “ P\nx„D rhT pxq ‰ ĥ ˝ ϕpxqs\n“ P x„D rhF ˝ ϕpxq ‰ ĥ ˝ ϕpxqs “ P z„D̃ rhF pzq ‰ ĥpzqs “ LD̃,hF pĥq “ LD̃,hF pApS̃qq ă\nSo B is indeed a learning algorithm for the class HDT"
    }, {
      "heading" : "5 Experiments",
      "text" : "Membership queries are a mean by which we can use human knowledge for improving performance in learning tasks. Human beings have a very rich knowledge and understanding of many problems that the ML community works on. They can provide much more information than merely the category of the object or an answer to a “yes” or “no” question. This knowledge is often basic, and can be acquired without the use of an expert (e.g., using crowd-sourcing). In this section we will present empirical results of an algorithm which takes advantage of this extensive knowledge in order to perform smart feature selection.\nIn standard supervised classification tasks the user is only asked to give the label of each example. What we did in this task, is to ask for additional information. Specifically, we faced a situation where we had a large number of features, and that these features had an interpretation that is easily understood. For every example in the sample set, we asked the user for its label and in addition, we asked which features indicate that this instance is labeled as such. After we finished iterating over the entire sample, we used the information on the relevant features to narrow down the feature space. Concretely, we trained linear classifiers only on the features that were chosen to be indicative by the users.\nArguably, this algorithm gathers additional information in a manner that is similar to using 1-local membership queries. 1-local query tests whether changing the value of a single feature changes the label. This can be seen as asking whether this feature is relevant to the prediction or not. In the algorithm presented here, we ask for the relevant features in a broader way. Namely, we explicitly ask which words are relevant to the corresponding label."
    }, {
      "heading" : "5.1 Is the additional data useful?",
      "text" : "When humans make decisions, it is often by very complex thought processes and we do not know whether we can access specific considerations that were used in the decision making process. The first goal of this experiment is to show that at least for some tasks, important parts of this thought process are easily accessible. I.e., that the annotators’ knowledge can be retrieved by asking simple questions. The second goal is to show that using this extra knowledge can help significantly decrease the number of tagged examples that are required.\nWe will formulate the above goals using the notion of error decomposition. Let ĥ be the classifier returned by the algorithm. We decompose LDpĥq as a sum of the approximation error (the error of the best linear classifier) and the estimation error (the difference between LDpĥq and the approximation error):\nLDphSq “ app ` est where app “ min hPH LDphq and est “ LDphSq ´min hPH LDphq\nThe approximation error app measures how good is the class of linear classifiers that we restrict ourselves to. In other words, since the class is linear, how informative are the features we use. The estimation error measures to which extent the algorithm overfits the data.\nWe can now formulate the above goals into claims on the approximation and estimation error. By applying the user induced feature selection mentioned above we can only increase the approximation error, as we reduce the hypothesis class to a smaller one. We will want\nto show that the feature space chosen by the users is still expressive enough, so that the increase in the approximation error will be minor. In addition, we will show that the feature selection is effective in the sense that the estimation error decreases significantly."
    }, {
      "heading" : "5.2 Experimental setup",
      "text" : ""
    }, {
      "heading" : "5.2.1 Sentiment Analysis",
      "text" : "Sentiment analysis (SA) is the Natural Language Processing task of identifying the attitude of a given text (usually whether it is positive, neutral or negative). This task has been studied in the NLP community for many years at different scale levels. It started off from being a document level classification task (Pang and Lee, 2004), and then the focus shifted to handling the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004). The newest focus is sentiment analysis of Microblog data like Twitter. Working with these informal text genres, on which users post their opnions, emotions, and recations about practically everything, presents new challenges for natural language processing beyond those encountered when working with more traditional text genres such as news-wire or product reviews. Indeed, classical approaches to Sentiment Analysis (Pang and Lee, 2008) are not directly applicable to tweets. While most of them focus on relatively large texts, e.g. movie or product reviews, tweets are very short and fine-grained. Nevertheless, the great prominence of Social Media during the last few years encouraged a focus on the sentiment detection over a microblogging domain. There has been a lot of recent work on sentiment analysis of twitter data. Some examples are (Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Barbosa and Feng, 2010).\nWe chose this task to demonstrate our method since each example (tweet) is constructed from a limited number of features (words), making each of these features very important for classification. Therefore, it seems that information supplied by users, can be useful in focusing our attention on the important features. Secondly, if in fact the two claims above hold, it will enable us to use a smaller data set, which is very important for this kind of tasks, since SA (and many more NLP tasks) require a large labeled data set which is often costly.\n5.2.2 Dataset\nWe worked with the data set from SemEval (Nakov et al., 2013), a shared task for Sentiment Analysis of Tweets . This dataset is constructed of 13,140 (8,439 train+development and 4,701 test, see Table 1) tweets which were collected over a one-year period spanning from January 2012 to January 2013. The tweets were labeled using the crowd sourcing tool Amazon Mechanical Turk and the labels were filtered to get rid of spammers.\nFor each sentence (tweet), the users were asked to indicate the overall sentiment of the sentence - positive, negative or neutral 5 and also to mark all the subjective (positive or negative) words/phrases in the sentence6. The learning task that we worked on is classifying the sentiment of the entire sentence. Although we only want to predict the sentiment of the tweet, we use these two labellings to get one “richer” labelled data-set. I.e., each instance in our training set holds additional information to its sentiment - which words/phrases in the sentence indicate a positive or negative sentiment."
    }, {
      "heading" : "5.2.3 Pre-processing",
      "text" : "Beside simple text, tweets may contain URL addresses, references to other Twitter users (appear as @ăusernameą) or content tags (also called hashtags) assigned by the tweeter (# ătagą). During preprocessing, we performed the following standard manipulations:\n• Words were switched to lower case and punctuation marks were removed (apart from a fixed set of smileys)\n• Every hyperlink was replaced by the meta-word URL\n• Every word starting with @, i.e. a username in twitter syntax, was replaced by the meta-word USR.\n• The hashtag sign 1#1 was removed from every tag to get a simple word. For example #perfect was changed to perfect."
    }, {
      "heading" : "5.2.4 Language Model",
      "text" : "We used the simple bag-of-words language models of n-grams (in our case unigrams, bigrams and trigrams). I.e., each tweet is represented as a sparse vector in t0, 1ud, where d is the size of the dictionary and the i’th coordinate equals 1 if and only if the i’th word in the dictionary appears in the tweet. We performed a standard cut-off of rare n-grams 7."
    }, {
      "heading" : "5.2.5 Scoring",
      "text" : "The results were evaluated on averaged F1 scores. This scoring function is used in the SemEval shared task, and overall a very common scoring function for NLP tasks. The F1 score is the harmonic mean of Precision and Recall. Every label has it’s F1 score. For the positive label, the Precision is the number of tweets that were correctly labeled as positive divided by the total number of tweets that were labeled as positive:\nPPOS “ TP\nTP ` FP 5The original labeling had 4 classes-[objective, positive, negative, or neutral] but since the turkers tended to mix up between the objective and neutral, the two classes were combined in the final task. 6This labelling procedure was originally intended to be used for two separate tasks. The first is, when given a tweet containing a marked instance of a word or a phrase, to identify the sentiment of that instance (i.e., whether the word is negative or positive). The second is identifying the sentiment of the whole tweet (without using the marked words).\n7without performing this cutoff, the results for the non-query variant are much worse\nThe Recall of the positive label is the number of tweets that were correctly labeled as positive divided by the total number of positive tweets in the data:\nRPOS “ TP\nTP ` FN\nThe positive label F1-score is computed as follows:\nFPOS “ 2 PPOS ¨RPOS PPOS `RPOS\nThe negative label F1-score FNEG is computed similarly. The final score that the results are evaluated on is the average of the above two:\nF1 “ 1 2 pFPOS ` FNEGq"
    }, {
      "heading" : "5.2.6 The algorithm",
      "text" : "We compare two variants for the feature space: using the entire feature space (after cutting off the rare n-grams), and using the ”query acquired” feature space which contains only features that were selected by the users as positive or negative for some example. Information about the data and the number of features is given in table 2.\nWe used a simple Naive Bayes classifier, with a small smoothing parameter. We also checked other classification algorithms- random forests, logistic regression, and multiclass SVM, (with } ¨ }1-regularization and } ¨ }2-regularization), but the results of the Naive Bayes predictor were the highest for both feature spaces."
    }, {
      "heading" : "5.3 Results",
      "text" : "The results that we will present are the results of the unigram model. The test scores of the other language models (unigram+bigrams and unigram+bigram+trigram) are almost identical for both feature spaces, and the training scores gets higher with the model complexity, as expected. Since our training set only contains approximately 8000 instances, we chose to present the results of the simplest model, so that the number of features would be comparable to the number of instances.\nThe results of both variants are presented in figure 2. As can be seen by the test scores, our algorithm outperforms the other variant which does not uses the additional information. The difference in test performance is approximately constant across different training sizes. Getting back to our claims - regarding the approximation error, by looking at the final training scores (using the larger training set possible), it can seen that both variants are\nalmost identical in all of the measurements. This fact indicates that we did not increase the approximation error. Regarding the improvement of estimation error, this can be seen clearly by looking at the gap between the test scores and the train scores. The gap in the query acquired model is smaller than the gap in the other model."
    }, {
      "heading" : "5.3.1 Precision and Recall",
      "text" : "Additional interesting properties can be seen in the precision and recall graphs (figure 3). For example, by looking at the results for positive samples (a & b) we can see that the improvement in the results from using the query model is almost only due to the improvement in the precision scores. If we only use 10% of the data, the query model reaches 0.77 test precision, while the non-query model only reaches 0.71 test precision score\neven when using the whole data set. Another interesting property that can be seen is that when a small training set is used, the difference in the test scores between the query and non-query methods is about twice as large as the difference when the largest possible training set is used."
    }, {
      "heading" : "5.3.2 Over-fitting",
      "text" : "When using the naive bayes algorithm, we estimate Ppf |cq for every feature f and every label c. This term measures how much the appearance of f contributes to the fact that c is the correct label 8 . Using those terms, we can sort the features by an order which conveys their informativeness. Since our features are words (or bigrams or trgrams), we can get some interesting insights by looking at the most informative features that each variant uses. If we only look at the top of the list (the top 20), the chosen features by both variants are almost identical. But, if we look a bit further we see how the algorithm which uses the entire feature space, chooses some significant features which clearly over-fit the training data. Some example are : ”nick”, ”lloyd”, and ”justin” in the unigram model, ”saturday\n8by the naive assumption that all of the features are independent given the label, this information is actually the only information we use in order to build the classifier\nkitchen”, ”ghost rider”, ”ray lewis” in the bigram model and ”rugby world cup” in the trigram model.\nThis over-fitting will obviously decrease as we increase the training size (and practically by checking the most informative features at different training sizes, the smaller the sample is, the more easy it is to find over-fitting features like the above). But as already stated, generally in Natural Language Processing it is much harder to acquire a large labeled data set. Therefore a method that avoids or significantly decrease this kind of over-fitting will be of high value."
    }, {
      "heading" : "5.4 Comparing to other Feature Selection Methods",
      "text" : "A question that can be raised is whether the improvement in the results is just an effect of the feature selection itself, or that the fact that the features were selected by a query process is the important part. In order to answer this, we compared our algorithm to using other automatic feature selection techniques. We checked two feature selection methods - filter and backward elimination. For each training set, the number of features that the method was instructed to select was the same as the number of features chosen by the users on that set. The results are presented in figure 4. The training scores of the automatic feature selection techniques are much lower than the training score of using the entire feature space (and much more similar to those of our method already for small training sets). This fact is reasonable, as we use a much smaller hypothesis class. If we look at the test scores it can be seen that using other feature selection techniques does improve the test score a little when compared to no feature selection at all, but still lies well under the score of our query acquired features method.\nAnother feature selection method that we compared our results to was using a SVM classifier with } ¨ }1-regularization, which is known to induce sparsity. Here again, using our query acquired feature set outperforms in all of the measurements."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "We have presented both theoretical and empirical evidence that local-membership queries are useful and beneficial. In the theoretical setup we have shown that even 1-local queries are stronger than the vanilla PAC model in an arguably natural problem. In the empirical setup we have demonstrated that by getting additional information from the users, significantly better results can be achieved. Moreover, the data in the experiment was created using crowdsourcing, and by asking very simple questions. This shows that getting extra knowledge can be an easy task.\nToday, the use of the MQ model in practice is almost non-existent. Even the more popular models of active learning, pool-based or stream-based, are fairly rare. E.g., in a recent survey of annotation projects for natural language processing tasks, only 20% of the respondents stated they had ever decided to use active learning (Tomanek and Olsson, 2009). It seems that there is plenty of room for incorporating more profound human knowledge to the field of machine learning, especially since today this knowledge can be collected quite easily.\nMore concrete directions for future work include: developing, implementing and analyzing more algorithms that use (local) membership queries and investigating the strength and limitations of the general Op1q-local queries model. Some examples of open questions: Is the use of 2-local queries stronger than the use of 1-local queries on a natural environment? What are the limitations of a model that uses Op1q-local queries with comparison to the model of (Awasthi et al., 2012) that uses logpnq-local queries?"
    } ],
    "references" : [ {
      "title" : "Learning regular sets from queries and counterexamples",
      "author" : [ "D. Angluin" ],
      "venue" : "Information and computation,",
      "citeRegEx" : "Angluin,? \\Q1987\\E",
      "shortCiteRegEx" : "Angluin",
      "year" : 1987
    }, {
      "title" : "When won t membership queries help",
      "author" : [ "D. Angluin", "M. Kharitonov" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Angluin and Kharitonov,? \\Q1995\\E",
      "shortCiteRegEx" : "Angluin and Kharitonov",
      "year" : 1995
    }, {
      "title" : "Malicious omissions and errors in answers to membership queries",
      "author" : [ "D. Angluin", "M. Kriķis", "R.H. Sloan", "G. Turán" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Angluin et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Angluin et al\\.",
      "year" : 1997
    }, {
      "title" : "Randomly fallible teachers: Learning monotone dnf with an incomplete membership oracle",
      "author" : [ "D. Angluin", "D.K. Slonim" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Angluin and Slonim,? \\Q1994\\E",
      "shortCiteRegEx" : "Angluin and Slonim",
      "year" : 1994
    }, {
      "title" : "Learning using local membership queries. arXiv preprint arXiv:1211.0996",
      "author" : [ "P. Awasthi", "V. Feldman", "V. Kanade" ],
      "venue" : null,
      "citeRegEx" : "Awasthi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Awasthi et al\\.",
      "year" : 2012
    }, {
      "title" : "Agnostic active learning",
      "author" : [ "Balcan", "M.-F", "A. Beygelzimer", "J. Langford" ],
      "venue" : "In Proceedings of the 23rd international conference on Machine learning,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2006
    }, {
      "title" : "Robust sentiment detection on twitter from biased and noisy data",
      "author" : [ "L. Barbosa", "J. Feng" ],
      "venue" : "In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,",
      "citeRegEx" : "Barbosa and Feng,? \\Q2010\\E",
      "shortCiteRegEx" : "Barbosa and Feng",
      "year" : 2010
    }, {
      "title" : "Neural net algorithms that learn in polynomial time from examples and queries",
      "author" : [ "E.B. Baum" ],
      "venue" : "Neural Networks, IEEE Transactions",
      "citeRegEx" : "Baum,? \\Q1991\\E",
      "shortCiteRegEx" : "Baum",
      "year" : 1991
    }, {
      "title" : "Query learning can work poorly when a human oracle is used",
      "author" : [ "E.B. Baum", "K. Lang" ],
      "venue" : "In International Joint Conference on Neural Networks,",
      "citeRegEx" : "Baum and Lang,? \\Q1992\\E",
      "shortCiteRegEx" : "Baum and Lang",
      "year" : 1992
    }, {
      "title" : "Learning with errors in answers to membership queries",
      "author" : [ "L. Bisht", "N.H. Bshouty", "L. Khoury" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Bisht et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bisht et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning with unreliable boundary queries",
      "author" : [ "A. Blum", "P. Chalasani", "S.A. Goldman", "D.K. Slonim" ],
      "venue" : "In Proceedings of the eighth annual conference on Computational learning theory,",
      "citeRegEx" : "Blum et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Blum et al\\.",
      "year" : 1995
    }, {
      "title" : "Fast learning of k-term dnf formulas with queries",
      "author" : [ "A. Blum", "S. Rudich" ],
      "venue" : "In Proceedings of the twenty-fourth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Blum and Rudich,? \\Q1992\\E",
      "shortCiteRegEx" : "Blum and Rudich",
      "year" : 1992
    }, {
      "title" : "Exact learning boolean functions via the monotone theory",
      "author" : [ "N.H. Bshouty" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "Bshouty,? \\Q1995\\E",
      "shortCiteRegEx" : "Bshouty",
      "year" : 1995
    }, {
      "title" : "More data speeds up training time in learning halfspaces over sparse vectors",
      "author" : [ "A. Daniely", "N. Linial", "S. Shalev-Shwartz" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Daniely et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Daniely et al\\.",
      "year" : 2013
    }, {
      "title" : "From average case complexity to improper learning complexity",
      "author" : [ "A. Daniely", "N. Linial", "S. Shalev-Shwartz" ],
      "venue" : "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Daniely et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Daniely et al\\.",
      "year" : 2014
    }, {
      "title" : "Complexity theoretic limitations on learning dnf’s",
      "author" : [ "A. Daniely", "S. Shalev-Shwatz" ],
      "venue" : "arXiv preprint arXiv:1404.3378",
      "citeRegEx" : "Daniely and Shalev.Shwatz,? \\Q2014\\E",
      "shortCiteRegEx" : "Daniely and Shalev.Shwatz",
      "year" : 2014
    }, {
      "title" : "Analysis of a greedy active learning strategy",
      "author" : [ "S. Dasgupta" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Dasgupta,? \\Q2004\\E",
      "shortCiteRegEx" : "Dasgupta",
      "year" : 2004
    }, {
      "title" : "Enhanced sentiment learning using twitter hashtags and smileys",
      "author" : [ "D. Davidov", "O. Tsur", "A. Rappoport" ],
      "venue" : "In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,",
      "citeRegEx" : "Davidov et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Davidov et al\\.",
      "year" : 2010
    }, {
      "title" : "Active learning by labeling features",
      "author" : [ "G. Druck", "B. Settles", "A. McCallum" ],
      "venue" : "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume",
      "citeRegEx" : "Druck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Druck et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning decision trees from random examples",
      "author" : [ "A. Ehrenfeucht", "D. Haussler" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "Ehrenfeucht and Haussler,? \\Q1989\\E",
      "shortCiteRegEx" : "Ehrenfeucht and Haussler",
      "year" : 1989
    }, {
      "title" : "On the power of membership queries in agnostic learning",
      "author" : [ "V. Feldman" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Feldman,? \\Q2009\\E",
      "shortCiteRegEx" : "Feldman",
      "year" : 2009
    }, {
      "title" : "Boosting a weak learning algorithm by majority",
      "author" : [ "Y. Freund" ],
      "venue" : "Information and computation,",
      "citeRegEx" : "Freund,? \\Q1995\\E",
      "shortCiteRegEx" : "Freund",
      "year" : 1995
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "M. Hu", "B. Liu" ],
      "venue" : "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Hu and Liu,? \\Q2004\\E",
      "shortCiteRegEx" : "Hu and Liu",
      "year" : 2004
    }, {
      "title" : "An efficient membership-query algorithm for learning dnf with respect to the uniform distribution",
      "author" : [ "J. Jackson" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "Jackson,? \\Q1994\\E",
      "shortCiteRegEx" : "Jackson",
      "year" : 1994
    }, {
      "title" : "Cryptographic limitations on learning boolean formulae and finite automata",
      "author" : [ "M. Kearns", "L. Valiant" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Kearns and Valiant,? \\Q1994\\E",
      "shortCiteRegEx" : "Kearns and Valiant",
      "year" : 1994
    }, {
      "title" : "Determining the sentiment of opinions",
      "author" : [ "Kim", "S.-M", "E. Hovy" ],
      "venue" : "In Proceedings of the 20th international conference on Computational Linguistics,",
      "citeRegEx" : "Kim et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2004
    }, {
      "title" : "Cryptographic hardness for learning intersections of halfspaces",
      "author" : [ "A.R. Klivans", "A Sherstov" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "Klivans and Sherstov,? \\Q2006\\E",
      "shortCiteRegEx" : "Klivans and Sherstov",
      "year" : 2006
    }, {
      "title" : "Twitter sentiment analysis: The good the bad and the omg! Icwsm, 11:538–541",
      "author" : [ "E. Kouloumpis", "T. Wilson", "J. Moore" ],
      "venue" : null,
      "citeRegEx" : "Kouloumpis et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kouloumpis et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning decision trees using the fourier spectrum",
      "author" : [ "E. Kushilevitz", "Y. Mansour" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Kushilevitz and Mansour,? \\Q1993\\E",
      "shortCiteRegEx" : "Kushilevitz and Mansour",
      "year" : 1993
    }, {
      "title" : "Semeval-2013 task 2: Sentiment analysis in twitter",
      "author" : [ "P. Nakov", "Z. Kozareva", "A. Ritter", "S. Rosenthal", "V. Stoyanov", "T. Wilson" ],
      "venue" : null,
      "citeRegEx" : "Nakov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nakov et al\\.",
      "year" : 2013
    }, {
      "title" : "Twitter as a corpus for sentiment analysis and opinion mining",
      "author" : [ "A. Pak", "P. Paroubek" ],
      "venue" : "In LREC,",
      "citeRegEx" : "Pak and Paroubek,? \\Q2010\\E",
      "shortCiteRegEx" : "Pak and Paroubek",
      "year" : 2010
    }, {
      "title" : "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "author" : [ "B. Pang", "L. Lee" ],
      "venue" : "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,",
      "citeRegEx" : "Pang and Lee,? \\Q2004\\E",
      "shortCiteRegEx" : "Pang and Lee",
      "year" : 2004
    }, {
      "title" : "Opinion mining and sentiment analysis",
      "author" : [ "B. Pang", "L. Lee" ],
      "venue" : "Foundations and trends in information retrieval,",
      "citeRegEx" : "Pang and Lee,? \\Q2008\\E",
      "shortCiteRegEx" : "Pang and Lee",
      "year" : 2008
    }, {
      "title" : "An interactive algorithm for asking and incorporating feature feedback into support vector machines",
      "author" : [ "H. Raghavan", "J. Allan" ],
      "venue" : "In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,",
      "citeRegEx" : "Raghavan and Allan,? \\Q2007\\E",
      "shortCiteRegEx" : "Raghavan and Allan",
      "year" : 2007
    }, {
      "title" : "Interactive feature selection",
      "author" : [ "H. Raghavan", "O. Madani", "R. Jones" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Raghavan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Raghavan et al\\.",
      "year" : 2005
    }, {
      "title" : "Active learning literature survey",
      "author" : [ "B. Settles" ],
      "venue" : "University of Wisconsin,",
      "citeRegEx" : "Settles,? \\Q2010\\E",
      "shortCiteRegEx" : "Settles",
      "year" : 2010
    }, {
      "title" : "Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances",
      "author" : [ "B. Settles" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Settles,? \\Q2011\\E",
      "shortCiteRegEx" : "Settles",
      "year" : 2011
    }, {
      "title" : "Learning with queries but incomplete information",
      "author" : [ "R.H. Sloan", "G. Turán" ],
      "venue" : "In Proceedings of the seventh annual conference on Computational learning theory,",
      "citeRegEx" : "Sloan and Turán,? \\Q1994\\E",
      "shortCiteRegEx" : "Sloan and Turán",
      "year" : 1994
    }, {
      "title" : "A web survey on the use of active learning to support annotation of text data",
      "author" : [ "K. Tomanek", "F. Olsson" ],
      "venue" : "In Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing,",
      "citeRegEx" : "Tomanek and Olsson,? \\Q2009\\E",
      "shortCiteRegEx" : "Tomanek and Olsson",
      "year" : 2009
    }, {
      "title" : "A theory of the learnable",
      "author" : [ "L.G. Valiant" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Valiant,? \\Q1984\\E",
      "shortCiteRegEx" : "Valiant",
      "year" : 1984
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "One of the main causes for this is a report (Baum and Lang, 1992) on very disappointing empirical performance of a query algorithm.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "In this work we study a new model of local membership queries (Awasthi et al., 2012), which tries to resolve the problem of artificial queries.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 39,
      "context" : "These two types of input were the basis for the learning model originally suggested in the celebrated paper “A theory of the learnable” (Valiant, 1984).",
      "startOffset" : 136,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "The use of membership queries in addition to examples was proven to be stronger than the standard PAC model in many cases (Angluin, 1987; Blum and Rudich, 1992; Bshouty, 1995; Jackson, 1994)(see section 2).",
      "startOffset" : 122,
      "endOffset" : 190
    }, {
      "referenceID" : 11,
      "context" : "The use of membership queries in addition to examples was proven to be stronger than the standard PAC model in many cases (Angluin, 1987; Blum and Rudich, 1992; Bshouty, 1995; Jackson, 1994)(see section 2).",
      "startOffset" : 122,
      "endOffset" : 190
    }, {
      "referenceID" : 12,
      "context" : "The use of membership queries in addition to examples was proven to be stronger than the standard PAC model in many cases (Angluin, 1987; Blum and Rudich, 1992; Bshouty, 1995; Jackson, 1994)(see section 2).",
      "startOffset" : 122,
      "endOffset" : 190
    }, {
      "referenceID" : 23,
      "context" : "The use of membership queries in addition to examples was proven to be stronger than the standard PAC model in many cases (Angluin, 1987; Blum and Rudich, 1992; Bshouty, 1995; Jackson, 1994)(see section 2).",
      "startOffset" : 122,
      "endOffset" : 190
    }, {
      "referenceID" : 0,
      "context" : "The use of membership queries in addition to examples was proven to be stronger than the standard PAC model in many cases (Angluin, 1987; Blum and Rudich, 1992; Bshouty, 1995; Jackson, 1994)(see section 2). Despite that the MQ model seems much stronger, both intuitively and formally, it is rarely used in practice. This is commonly believed to result from the fact that in many cases it is not easy to implement MQ algorithms, that can create new and artificial examples to be labeled as part of the training phase. This problem of labeling artificial examples was highlighted by the experiment of Baum and Lang (1992). Baum and Lang implemented a membership query algorithm proposed by Baum (1991) for learning halfspaces .",
      "startOffset" : 123,
      "endOffset" : 620
    }, {
      "referenceID" : 0,
      "context" : "The use of membership queries in addition to examples was proven to be stronger than the standard PAC model in many cases (Angluin, 1987; Blum and Rudich, 1992; Bshouty, 1995; Jackson, 1994)(see section 2). Despite that the MQ model seems much stronger, both intuitively and formally, it is rarely used in practice. This is commonly believed to result from the fact that in many cases it is not easy to implement MQ algorithms, that can create new and artificial examples to be labeled as part of the training phase. This problem of labeling artificial examples was highlighted by the experiment of Baum and Lang (1992). Baum and Lang implemented a membership query algorithm proposed by Baum (1991) for learning halfspaces .",
      "startOffset" : 123,
      "endOffset" : 700
    }, {
      "referenceID" : 0,
      "context" : "The use of membership queries in addition to examples was proven to be stronger than the standard PAC model in many cases (Angluin, 1987; Blum and Rudich, 1992; Bshouty, 1995; Jackson, 1994)(see section 2). Despite that the MQ model seems much stronger, both intuitively and formally, it is rarely used in practice. This is commonly believed to result from the fact that in many cases it is not easy to implement MQ algorithms, that can create new and artificial examples to be labeled as part of the training phase. This problem of labeling artificial examples was highlighted by the experiment of Baum and Lang (1992). Baum and Lang implemented a membership query algorithm proposed by Baum (1991) for learning halfspaces . Their algorithm had very poor results, which was attributed to the fact that the algorithm created artificial and unnatural examples, which resulted in a noisy labeling. We elaborate on this experiment and criticize its conclusions in section 2. A suggested solution to the problem of unnatural examples was proposed by Awasthi et al. (2012). They suggested a mid-way model of learning with queries, but only restricted ones.",
      "startOffset" : 123,
      "endOffset" : 1068
    }, {
      "referenceID" : 0,
      "context" : "The use of membership queries in addition to examples was proven to be stronger than the standard PAC model in many cases (Angluin, 1987; Blum and Rudich, 1992; Bshouty, 1995; Jackson, 1994)(see section 2). Despite that the MQ model seems much stronger, both intuitively and formally, it is rarely used in practice. This is commonly believed to result from the fact that in many cases it is not easy to implement MQ algorithms, that can create new and artificial examples to be labeled as part of the training phase. This problem of labeling artificial examples was highlighted by the experiment of Baum and Lang (1992). Baum and Lang implemented a membership query algorithm proposed by Baum (1991) for learning halfspaces . Their algorithm had very poor results, which was attributed to the fact that the algorithm created artificial and unnatural examples, which resulted in a noisy labeling. We elaborate on this experiment and criticize its conclusions in section 2. A suggested solution to the problem of unnatural examples was proposed by Awasthi et al. (2012). They suggested a mid-way model of learning with queries, but only restricted ones. The queries that their model allows the algorithm to ask are only local queries, i.e., queries that are close in some sense to examples from the sample set. Hopefully, examples which are similar to natural examples will also appear to be natural, or at least close to natural, and in any case will be far from appearing random or artificial. In their work, Awasti et al. started to investigate the power and the limitations of this model of local queries. They proved positive results on learning sparse polynomials with Oplogpnqqlocal queries under what they defined as locally smooth distributions1, which in some sense generalize the uniform and product distributions. They also proposed an algorithm that learns DNF formulas under the uniform distribution in quasi-polynomial time using only Oplogpnqq-local queries. The exciting ideas of Awasthi et al. (2012) leave many directions for future work.",
      "startOffset" : 123,
      "endOffset" : 2017
    }, {
      "referenceID" : 33,
      "context" : "We note that similar experiments also present encouraging results along this line (Raghavan and Allan, 2007; Raghavan et al., 2005; Settles, 2011; Druck et al., 2009).",
      "startOffset" : 82,
      "endOffset" : 166
    }, {
      "referenceID" : 34,
      "context" : "We note that similar experiments also present encouraging results along this line (Raghavan and Allan, 2007; Raghavan et al., 2005; Settles, 2011; Druck et al., 2009).",
      "startOffset" : 82,
      "endOffset" : 166
    }, {
      "referenceID" : 36,
      "context" : "We note that similar experiments also present encouraging results along this line (Raghavan and Allan, 2007; Raghavan et al., 2005; Settles, 2011; Druck et al., 2009).",
      "startOffset" : 82,
      "endOffset" : 166
    }, {
      "referenceID" : 18,
      "context" : "We note that similar experiments also present encouraging results along this line (Raghavan and Allan, 2007; Raghavan et al., 2005; Settles, 2011; Druck et al., 2009).",
      "startOffset" : 82,
      "endOffset" : 166
    }, {
      "referenceID" : 39,
      "context" : "1 PAC Valiant’s Probably Approximately Correct (PAC) model of learning (Valiant, 1984) formulates the problem of learning a concept from examples.",
      "startOffset" : 71,
      "endOffset" : 86
    }, {
      "referenceID" : 39,
      "context" : "More examples include relatively weak classes such as DNFs and CNFs with constantly many terms (Valiant, 1984), and rank k decision trees (Ehrenfeucht and Haussler, 1989) for a constant k.",
      "startOffset" : 95,
      "endOffset" : 110
    }, {
      "referenceID" : 19,
      "context" : "More examples include relatively weak classes such as DNFs and CNFs with constantly many terms (Valiant, 1984), and rank k decision trees (Ehrenfeucht and Haussler, 1989) for a constant k.",
      "startOffset" : 138,
      "endOffset" : 170
    }, {
      "referenceID" : 24,
      "context" : "For example, learning automatons, logarithmic depth circuits, and intersections of polynomially many halfspaces are all intractable, assuming the security of various cryptographic schemes (Kearns and Valiant, 1994; Klivans et al., 2006).",
      "startOffset" : 188,
      "endOffset" : 236
    }, {
      "referenceID" : 14,
      "context" : "In (Daniely et al., 2014; Daniely and Shalev-Shwatz, 2014; Daniely et al., 2013), it is shown that learning DNF formulas, and learning intersections of ωplogpnqq halfspaces are intractable under the assumption that refuting random k-SAT is hard.",
      "startOffset" : 3,
      "endOffset" : 80
    }, {
      "referenceID" : 15,
      "context" : "In (Daniely et al., 2014; Daniely and Shalev-Shwatz, 2014; Daniely et al., 2013), it is shown that learning DNF formulas, and learning intersections of ωplogpnqq halfspaces are intractable under the assumption that refuting random k-SAT is hard.",
      "startOffset" : 3,
      "endOffset" : 80
    }, {
      "referenceID" : 13,
      "context" : "In (Daniely et al., 2014; Daniely and Shalev-Shwatz, 2014; Daniely et al., 2013), it is shown that learning DNF formulas, and learning intersections of ωplogpnqq halfspaces are intractable under the assumption that refuting random k-SAT is hard.",
      "startOffset" : 3,
      "endOffset" : 80
    }, {
      "referenceID" : 35,
      "context" : "Several types of active models have been proposed: the Membership Query Synthesis, Stream-Based Selective Sampling, and Pool-Based Sampling (Settles, 2010).",
      "startOffset" : 140,
      "endOffset" : 155
    }, {
      "referenceID" : 39,
      "context" : "Our work is in the area of the “Membership Queries” (MQ) model which was presented in (Valiant, 1984).",
      "startOffset" : 86,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "Some examples of concept classes that have been proven to be PAC-learnable only if membership queries are available include: The class of Deterministic Finite Automatons (Angluin, 1987), the class of k-term DNF for k “ logpnq logplogpnqq (Blum and Rudich, 1992), the class of decision trees and k-almost monotone-DNF formulas (Bshouty, 1995), the class of intersections of k-halfspaces (Baum, 1991) and the class of DNF formulas under the uniform distribution (Jackson, 1994).",
      "startOffset" : 170,
      "endOffset" : 185
    }, {
      "referenceID" : 11,
      "context" : "Some examples of concept classes that have been proven to be PAC-learnable only if membership queries are available include: The class of Deterministic Finite Automatons (Angluin, 1987), the class of k-term DNF for k “ logpnq logplogpnqq (Blum and Rudich, 1992), the class of decision trees and k-almost monotone-DNF formulas (Bshouty, 1995), the class of intersections of k-halfspaces (Baum, 1991) and the class of DNF formulas under the uniform distribution (Jackson, 1994).",
      "startOffset" : 238,
      "endOffset" : 261
    }, {
      "referenceID" : 12,
      "context" : "Some examples of concept classes that have been proven to be PAC-learnable only if membership queries are available include: The class of Deterministic Finite Automatons (Angluin, 1987), the class of k-term DNF for k “ logpnq logplogpnqq (Blum and Rudich, 1992), the class of decision trees and k-almost monotone-DNF formulas (Bshouty, 1995), the class of intersections of k-halfspaces (Baum, 1991) and the class of DNF formulas under the uniform distribution (Jackson, 1994).",
      "startOffset" : 326,
      "endOffset" : 341
    }, {
      "referenceID" : 7,
      "context" : "Some examples of concept classes that have been proven to be PAC-learnable only if membership queries are available include: The class of Deterministic Finite Automatons (Angluin, 1987), the class of k-term DNF for k “ logpnq logplogpnqq (Blum and Rudich, 1992), the class of decision trees and k-almost monotone-DNF formulas (Bshouty, 1995), the class of intersections of k-halfspaces (Baum, 1991) and the class of DNF formulas under the uniform distribution (Jackson, 1994).",
      "startOffset" : 386,
      "endOffset" : 398
    }, {
      "referenceID" : 23,
      "context" : "Some examples of concept classes that have been proven to be PAC-learnable only if membership queries are available include: The class of Deterministic Finite Automatons (Angluin, 1987), the class of k-term DNF for k “ logpnq logplogpnqq (Blum and Rudich, 1992), the class of decision trees and k-almost monotone-DNF formulas (Bshouty, 1995), the class of intersections of k-halfspaces (Baum, 1991) and the class of DNF formulas under the uniform distribution (Jackson, 1994).",
      "startOffset" : 460,
      "endOffset" : 475
    }, {
      "referenceID" : 21,
      "context" : "The last of these results was built upon Freund’s boosting algorithm (Freund, 1995) and the Fourier-based technique for learning using membership queries due to (Kushilevitz and Mansour, 1993).",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 28,
      "context" : "The last of these results was built upon Freund’s boosting algorithm (Freund, 1995) and the Fourier-based technique for learning using membership queries due to (Kushilevitz and Mansour, 1993).",
      "startOffset" : 161,
      "endOffset" : 192
    }, {
      "referenceID" : 1,
      "context" : ", in the case of learning DNF and CNF formulas (Angluin and Kharitonov, 1995), and in the case of distribution free agnostic learning (although in the distribution-specific agnostic setting membership queries do increase the power of the learner) (Feldman, 2009).",
      "startOffset" : 47,
      "endOffset" : 77
    }, {
      "referenceID" : 20,
      "context" : ", in the case of learning DNF and CNF formulas (Angluin and Kharitonov, 1995), and in the case of distribution free agnostic learning (although in the distribution-specific agnostic setting membership queries do increase the power of the learner) (Feldman, 2009).",
      "startOffset" : 247,
      "endOffset" : 262
    }, {
      "referenceID" : 7,
      "context" : "A well-known exception is the work of Baum and Lang (1992). They applied a variation of the MQ algorithm for learning a linear classifier proposed in Baum (1991).",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "A well-known exception is the work of Baum and Lang (1992). They applied a variation of the MQ algorithm for learning a linear classifier proposed in Baum (1991). This algorithm uses the idea that given two examples, one positive and one negative, and a query oracle, it is possible to find an approximately accurate separating halfspace by using a binary search on the line between the positive and negative examples.",
      "startOffset" : 38,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : "A well-known exception is the work of Baum and Lang (1992). They applied a variation of the MQ algorithm for learning a linear classifier proposed in Baum (1991). This algorithm uses the idea that given two examples, one positive and one negative, and a query oracle, it is possible to find an approximately accurate separating halfspace by using a binary search on the line between the positive and negative examples. Their experiment attempts to evaluate this idea in practice. The task that they chose is the task of binary digit classification. The algorithm would receive two examples, one positive and one negative (say, an image of the digit 4 and an image of the digit 7) and would return the weights of the halfspace. The generalization error of the halfspace would then be tested on other examples from the data. The query technique they used in the experiment is different than in the original algorithm: “A direct implementation of this algorithm would repeatedly flash images on the screen during the binary search and would require the test subject to type in the correct label for each image. Because this process seemed likely to be error prone, we instead provided an interface that permitted the test subject to scan through the input space using the mouse and then click on an image that seemed to lie right at the edge of recognizability” (from Baum and Lang (1992)).",
      "startOffset" : 38,
      "endOffset" : 1386
    }, {
      "referenceID" : 8,
      "context" : "Figure 1: An example taken from (Baum and Lang, 1992): the images the user saw on the screen for the digits 5 and 7",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 33,
      "context" : "This work led many to the conclusion that membership queries are not useful in practice (Settles (2010); Balcan et al.",
      "startOffset" : 89,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "This work led many to the conclusion that membership queries are not useful in practice (Settles (2010); Balcan et al. (2006); Dasgupta (2004) and more).",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "This work led many to the conclusion that membership queries are not useful in practice (Settles (2010); Balcan et al. (2006); Dasgupta (2004) and more).",
      "startOffset" : 105,
      "endOffset" : 143
    }, {
      "referenceID" : 8,
      "context" : "This framework deals with the problem raised by (Baum and Lang, 1992).",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "This notion was first presented in Angluin and Slonim (1994), and then followed by the notion of limited MQ and malicious MQ.",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "This notion was first presented in Angluin and Slonim (1994), and then followed by the notion of limited MQ and malicious MQ. (Angluin et al. (1997); Blum et al.",
      "startOffset" : 35,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "This notion was first presented in Angluin and Slonim (1994), and then followed by the notion of limited MQ and malicious MQ. (Angluin et al. (1997); Blum et al. (1995); Sloan and Turán (1994); Bisht et al.",
      "startOffset" : 35,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : "This notion was first presented in Angluin and Slonim (1994), and then followed by the notion of limited MQ and malicious MQ. (Angluin et al. (1997); Blum et al. (1995); Sloan and Turán (1994); Bisht et al.",
      "startOffset" : 35,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : "This notion was first presented in Angluin and Slonim (1994), and then followed by the notion of limited MQ and malicious MQ. (Angluin et al. (1997); Blum et al. (1995); Sloan and Turán (1994); Bisht et al. (2008)).",
      "startOffset" : 35,
      "endOffset" : 214
    }, {
      "referenceID" : 0,
      "context" : "This notion was first presented in Angluin and Slonim (1994), and then followed by the notion of limited MQ and malicious MQ. (Angluin et al. (1997); Blum et al. (1995); Sloan and Turán (1994); Bisht et al. (2008)). The third method is to restrict the examples that the learning algorithm can query to examples that are similar to examples drawn from the distribution. This is formalized in the work of Awasthi et al. (2012). They present the concept of learning using only local membership queries.",
      "startOffset" : 35,
      "endOffset" : 425
    }, {
      "referenceID" : 18,
      "context" : "Druck et al. (2009) propose a pool-based active learning approach in which the user provides labels for input features, rather than instances.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 18,
      "context" : "Druck et al. (2009) propose a pool-based active learning approach in which the user provides labels for input features, rather than instances. The users are asked to provide a “label” for input features, where a labeled input feature denotes that a particular feature is highly indicative of a particular label. Following that, Settles (2011) presented an active learning annotation interface, in which the users label instances and features simultaneously.",
      "startOffset" : 0,
      "endOffset" : 343
    }, {
      "referenceID" : 18,
      "context" : "Druck et al. (2009) propose a pool-based active learning approach in which the user provides labels for input features, rather than instances. The users are asked to provide a “label” for input features, where a labeled input feature denotes that a particular feature is highly indicative of a particular label. Following that, Settles (2011) presented an active learning annotation interface, in which the users label instances and features simultaneously. At any point in time, an instance and a list of features for each label is presented on the screen. The user can choose to either label the instance, choose a feature from the list as being indicative, or add a new feature of his or her choice. Another similar work is of Raghavan and Allan (2007) and Raghavan et al.",
      "startOffset" : 0,
      "endOffset" : 756
    }, {
      "referenceID" : 18,
      "context" : "Druck et al. (2009) propose a pool-based active learning approach in which the user provides labels for input features, rather than instances. The users are asked to provide a “label” for input features, where a labeled input feature denotes that a particular feature is highly indicative of a particular label. Following that, Settles (2011) presented an active learning annotation interface, in which the users label instances and features simultaneously. At any point in time, an instance and a list of features for each label is presented on the screen. The user can choose to either label the instance, choose a feature from the list as being indicative, or add a new feature of his or her choice. Another similar work is of Raghavan and Allan (2007) and Raghavan et al. (2005). They studied the problem of tandem learning where they combine uncertainty sampling for instances along with co-occurrence-based interactive feature selection.",
      "startOffset" : 0,
      "endOffset" : 783
    }, {
      "referenceID" : 31,
      "context" : "It started off from being a document level classification task (Pang and Lee, 2004), and then the focus shifted to handling the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 22,
      "context" : "It started off from being a document level classification task (Pang and Lee, 2004), and then the focus shifted to handling the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004).",
      "startOffset" : 143,
      "endOffset" : 181
    }, {
      "referenceID" : 32,
      "context" : "Indeed, classical approaches to Sentiment Analysis (Pang and Lee, 2008) are not directly applicable to tweets.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 30,
      "context" : "Some examples are (Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Barbosa and Feng, 2010).",
      "startOffset" : 18,
      "endOffset" : 113
    }, {
      "referenceID" : 27,
      "context" : "Some examples are (Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Barbosa and Feng, 2010).",
      "startOffset" : 18,
      "endOffset" : 113
    }, {
      "referenceID" : 17,
      "context" : "Some examples are (Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Barbosa and Feng, 2010).",
      "startOffset" : 18,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "Some examples are (Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Barbosa and Feng, 2010).",
      "startOffset" : 18,
      "endOffset" : 113
    }, {
      "referenceID" : 29,
      "context" : "We worked with the data set from SemEval (Nakov et al., 2013), a shared task for Sentiment Analysis of Tweets .",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 38,
      "context" : ", in a recent survey of annotation projects for natural language processing tasks, only 20% of the respondents stated they had ever decided to use active learning (Tomanek and Olsson, 2009).",
      "startOffset" : 163,
      "endOffset" : 189
    }, {
      "referenceID" : 4,
      "context" : "Some examples of open questions: Is the use of 2-local queries stronger than the use of 1-local queries on a natural environment? What are the limitations of a model that uses Op1q-local queries with comparison to the model of (Awasthi et al., 2012) that uses logpnq-local queries?",
      "startOffset" : 227,
      "endOffset" : 249
    } ],
    "year" : 2015,
    "abstractText" : "Classic machine learning algorithms learn from labelled examples. For example, to design a machine translation system, a typical training set will consist of English sentences and their translation to French. There is a stronger model, in which the algorithm can also query for labels of new examples it creates. E.g, in the translation task, the algorithm can create a new English sentence, and request its translation from the user during training. This combination of examples and queries, that resembles human learning patterns, has been widely studied. Yet, despite many theoretical results, query algorithms are almost never used. One of the main causes for this is a report (Baum and Lang, 1992) on very disappointing empirical performance of a query algorithm. These poor results were mainly attributed to the fact that the algorithm queried for labels of examples that are artificial, and impossible to interpret by humans. In this work we study a new model of local membership queries (Awasthi et al., 2012), which tries to resolve the problem of artificial queries. In this model, the algorithm is only allowed to query the labels of examples which are close to examples from the training set. E.g., in translation, the algorithm can change individual words in a sentence it has already seen, and then ask for the translation. In this model, the examples queried by the algorithm will be close to natural examples and hence, hopefully, will not appear as artificial or random. In this work we focus on 1-local membership queries (i.e., queries of distance 1 from an example in the training sample). We show that 1-local membership queries are already stronger than the standard learning model. We also present an experiment on a well known NLP task of sentiment analysis. In this experiment, the users were asked to provide, in a way that resembles 1-local queries, more information than merely indicating the label. We present results that illustrate that this extra information is beneficial in practice.",
    "creator" : "LaTeX with hyperref package"
  }
}