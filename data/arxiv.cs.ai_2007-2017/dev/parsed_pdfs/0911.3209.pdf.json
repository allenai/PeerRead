{
  "name" : "0911.3209.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Chao-Yang Pang", "Yun-Fei Wang" ],
    "emails" : [ "cypang@live.com;", "cypang@sicnu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "To find all extreme points of multimodal functions is called extremum problem, which is a well\nknown difficult issue in optimization fields. Applying ant colony optimization (ACO) to solve this\nproblem is rarely reported. The method of applying ACO to solve extremum problem is explored\nin this paper. Experiment shows that the solution error of the method presented in this paper is less than 10−8.\nKeywords: Extremum Problem; Ant Colony Optimization (ACO)\n∗Electronic address: cypang@live.com; Electronic address: cypang@sicnu.edu.cn\nI. INTRODUCTION"
    }, {
      "heading" : "A. Extremum Problem",
      "text" : "Multimodal function refers to the function which has more than one extreme point. To find all extreme points is called extremum problem, which is a well known difficult issue in optimization fields. Many practical engineering problems can be converted as this problem, such as the detection of multiple objects in military field. Therefore, solving the extremum problem is a useful study topic. To solve the extremum problem, many methods of optimization are applied, such as genetic algorithm (GA) [1], simulated annealing (SA) [2], particle swarm optimization algorithm (PSO) [3, 4], immune algorithm ( IA) [5], and so on. However, currently there is rare report that applying Ant Colony Optimization (ACO) to solve the extremum problem. The motivation of this paper is to apply ACO to search all extreme points of function.\nB. Introduction of Ant Colony Optimization (ACO)\nAnt Colony Optimization (ACO) was first proposed by Dorigo (1991) [6, 7, 8]. The inspiring source of ACO is the foraging behavior of real ants. When ants search for food, they initially explore the area surrounding their nest in a random manner. As soon as an ant finds a food source, it remembers the route passed by and carries some food back to the nest. During the return trip, the ant deposits pheromone on the ground. The deposited pheromone, guides other ants to the food source. And the feature has been shown, indirect communication among ants via pheromone trails enables them to find the shortest routes between their nest and food sources. ACO imitates this feature and it becomes an effective algorithm for the optimization problems [9]. It has been successfully applied to many combinatorial optimization problems [10, 11, 12], such as Traveling Salesman Problem (TSP) [13, 14], Quadratic Assignment Problem(QAP) [15], Job-shop Scheduling Problem(JSP) [16], Vehicle Routing Problem(VRP) [17, 18], Data Mining(DM) [19] and so on.\nThe application of ACO pushes the study of ACO theory, and its two main study topics are the analysis of convergence and runtime. M. Birattari proves the invariance of ACO and introduced three new ACO algorithms [20]. Convergence is one of focus study of ACO. Walter J. Gutjahr studied the convergence of ACO firstly in 2000 [21]. T. St .. uezle and\nM. Dorigo proved the existence of the ACO convergence under two conditions, one is to only update the pheromone of the shortest route generated at each iteration step, the other is that the pheromone on all routes has lower bound [22]. C.-Y. Pang and et.al. found a potential new view point to study ACO convergence under general condition, that is entropy convergence in 2009 [23]. In ref.[23], the following conclusion is get: ACO may not converges to the optimal solution in practice, but its entropy is convergent. The other study focus of ACO is time complexity. ACO has runtime O(tmN2), where t, m and N refers to the number of iteration steps, ants, cities and M = [ N 1.5 ] in general. To reduce runtime, cutting down parameter t and N is the main way possibly. In 2008, Walter J. Gutjahr presented some theoretical results about ACO runtime [24, 25]. Since runtime is proportional to the square of N , parameter N is the key factor of runtime. Through cutting down N to reduce runtime is a choice. And Pang and et al. do the following attempt [26]. Firstly, cluster all cities into some classes (group) and let ACO act on these small classes respectively to get some local TSP routes. And then joint these local route to form the whole TSP route. If class is compact, the length of local route got at every iteration step will change continually possibly, where compactness refers to all data cluster in a small region tightly. And this property result in the conclusion: the convergence criterion |Lt−Lt+1| Lt → 0 is the marker of ACO convergence, where Lt is the length of local route at t − th iteration step. Thus, minimum iteration number t can be estimated by the marker approximately.\nThe study of ACO theory speeds up its application again. ACO not only can be applied to solve discrete optimization problems, but also to continuous ones. The first method for continuous-space optimization problems, called Continuous ACO (CACO), was proposed by Bilchev and Parmee (1995)[29], and later it was used by some others [30, 31, 32, 33]. In general, the application of ACO to continuous optimization problems need to transform a continuous search space to a discrete one. Other methods include that, in 2002, Continuous Interacting Ant Colony (CIAC) was proposed by Dreo and Siarry [34], and in 2003, an adaptive ant colony system algorithm for continuous-space optimization problems was proposed by Li Yan-jun [35], and so on."
    }, {
      "heading" : "C. Framework of ACO",
      "text" : "Traveling Salesman Problem (TSP) is a famous combinatorial problem, it can be stated\nvery simply:\nA salesman visit N cities cyclically provided that he visits each city just once. In what\norder should he visit them to minimis the distance traveled?\nThe typical application of ACO is to solve TSP, and its basic idea is stated as below: When an ant passes through an edge, it releases pheromone on this edge. The shorter the edge is, the bigger the amount of pheromone is. And the pheromone induces other ants to passes through this edge. At last, all ants select a unique route, and this route is shortest possibly.\nThe framework of ACO is introduced as below: Step1 (Initialization): Posit M ants at different M cities randomly; Pre-assign maximum iteration number tmax; Let t = 0, where t denotes the t − th iteration step; Initialize amount of pheromone of every edge.\nStep2 While(t < tmax) {\nStep2.1: Every ant selects its next city according to the transition probability. The transition probability from the i− th city to the j − th city for the k − th ant is defined as Eq.1.\np (k) ij (t) =\n  \n \nταij(t).η β ij\nP\ns∈allowedk\nταis(t).η β is\n, if j ∈ allowedk\n0, otherwise\n(1)\n, where allowedk denotes the set of cities that can be accessed by the k − th ant; τij(t) is the pheromone value of the edge (i, j); ηij is local heuristic function defined as\nηij = 1\ndij\n, where dij is the distance between the i − th city and the j − th city; the parameters α and β determine the relative influence of the trail strength and the heuristic information respectively.\nStep2.2: After all ants finish their travels, all pheromone values are updated according\nto Eq.2.\nτij(t+ 1) = (1− ρ) · τij(t) + ∆τij(t) (2)\n∆τij(t) =\nm ∑\nk=1\n∆τ (k) ij (t)\n∆τ (k) ij (t) =\n \n\nQ\nL(k)(t) , if the k − th ant pass edge (i, j)\n0, otherwise\n, where L(k)(t) is the length of the route passed by the k− th ant during the t− th iteration; ρ is the persistence percentage of the trail (thus, 1− ρ corresponds to the evaporation); Q denotes constant quantity of pheromone.\nStep2.3: Increase iteration step: t← t+ 1\n} Step3: End procedure and select the shortest route as output from the routes traveled\nby the ants."
    }, {
      "heading" : "II. APPLY ACO TO SEARCH ALL EXTREME POINTS OF FUNCTION",
      "text" : ""
    }, {
      "heading" : "A. Basic Idea",
      "text" : "Assume that the function is f(x), x is real number and it belongs to a closed interval [a, b]. The task of this paper is to extract all extreme points that the corresponding value is minimal locally. The basic idea of this paper is stated roughly as below:\nDivide interval [a, b] into many tiny intervals with equal size. Suppose these small interval are {I1, I2, · · · , In} and the center of interval Ii is denoted by xi. Suppose the neighbor interval of interval Ii is Ii+1 (or Ii−1). And an ant is put at the center of each small interval. If f(xi) > f(xi+1), the ant at interval Ii will move to interval Ii+1 possibly, just liking there is an virtual edge between Ii and Ii+1. And assume the virtual edge is e(Ii, Ii+1). The weight (virtual distance) of edge e(Ii, Ii+1) is proportional to f(xi) − f(xi+1). That is, the bigger f(xi) − f(xi+1) is, the more possibly the ant moves to Ii+1 from Ii. When the ant moves to Ii+1, it releases pheromone at Ii+1, and the pheromone is proportional to value f(xi)− f(xi+1). The pheromone depositing on Ii+1 will attract other ants move to it.\nAfter some iteration steps, some intervals contain many ants while other ones contain no ant. The intervals containing ants include extreme points possibly, while other ones include no extreme point possibly. And then keep the intervals which contain ants, and divide them into much more small intervals, repeat the same procedure again until the size of intervals is sufficient small. At last all ants will stay around extreme points. The centers of these sufficient small intervals are the approximations of extreme points.\nFrom above discussion, it can be seen that the realization of the basic idea consists of four parts: partition of interval [a, b] and initialization, rule of ant moving, rule of pheromone updating, and keeping the intervals containing ants. The contents of the four parts are stated as below."
    }, {
      "heading" : "B. Partition of Interval and Initialization",
      "text" : "Suppose interval [a, b] is partitioned into n small intervals with equal size, which are\ndenoted by {I1, I2, · · · , In}, where n is a pre-assigned number. Then each interval has size\nδ = b− a\nn\nIi = [a+ (i− 1)δ, a+ iδ]\nThe i− th interval Ii has center xi.\nxi = a + (i− 1\n2 )δ\nSuppose t denotes the t − th iteration step of ACO and it is initialized as zero (i.e., t = 0). Put n ants at the centers of the n intervals, and each interval has only one ant. Suppose these ants are denoted by a1, a2, · · ·an respectively, and ant ai is associated with the i − th interval Ii. Each ant will release an initial pheromone at its associated interval Ii (i.e., τi(0) = const, const is a constant number). In addition, set the increment of the pheromone of each interval to zero (i.e., ∆τi(0) = 0)\nFigure 1 shows a diagram of initialization."
    }, {
      "heading" : "C. Rule of Ant Moving",
      "text" : "Let Neighbor(Ii) be a set of neighboring intervals of Ii. Take one-dimensional function\nfor example\nNeighbor(Ii) =\n    \n   \n{Ii+1} , i = 1 {Ii−1, Ii+1} , i = 2 · · ·n− 1 {Ii−1} , i = n\nAs it is discussed in section A, the ant staying at interval Ii will move to neighbor interval denoted by Ij, just like there is a virtual edge e(Ii, Ij). The weight of the virtual edge is |f(xi)− f(xj)|. Then the heuristic factor is\nηij = |f(xi)− f(xj)|\nSuppose interval Ii contains ant ak. If f(xi) > f(xj), ant ak is allowed to move to neighbor interval Ij. Otherwise, it is forbidden to move. Suppose all intervals allowed to be accessed by ant ak is marked as allowedk. The transition probability of ant ak is defined as\np (k) ij (t) =\n  \n \nταj (t).η β ij\nP\nh∈allowedk\nτα h (t).ηβ ih\n, if f(xi) > f(xj)\n0, otherwise\n(3)\nIn Eq.3, α is the relative influence of the trail strength; β is the heuristic information;\nτj(t) is the pheromone of interval Ij."
    }, {
      "heading" : "D. The Rule of Pheromone Updating",
      "text" : "Suppose ak ant is staying at interval Ii and it will move to neighbor interval Ij. After it\nmoves to Ij, releases pheromone at Ij . The pheromone amount is denoted by ∆τ (k) j (t)\n∆τ (k) j (t) = C1(f(xi)− f(xj))\n, where C1 is a positive constant. The bigger f(xi) − f(xj) is, the higher the amount of released pheromone is, the more\npossibly that other ants will be attracted to interval Ij.\nNot only ant ak arrives Ij and releases pheromone, but also other ants which move to interval Ij and release pheromone too. Suppose there are q ants will move to interval Ij during the t− th iteration step, which are denoted by aj1 , aj2, · · · , ajq . The sum of released pheromone by aj1 , aj2, · · · , ajq is ∆τj(t). Then\n∆τj(t) =\nq ∑\np=1\n∆τ (jp) j (t)\nWhen all ants aj1, aj2, · · · , ajq move to Ij , the amount of pheromone τj is changed as\nEq.4.\nτj(t+ 1) = (1− ρ) · τj(t) + ∆τj(t) (4)\n, where ρ is the evaporation percentage of the trail (thus, 1−ρ corresponds to persistence)."
    }, {
      "heading" : "E. Keeping Only the Intervals Containing Ants to Cut Down Search Range",
      "text" : "The intervals that have smaller function values depositing much more pheromone, and it will attract ants more powerfully. After several iterations, the distribution of ants has the feature that all ants stay at the intervals that have smaller function values and other intervals contain no ants. That is, extreme points are included in the intervals containing ants. And then keep the intervals having ant and delete other intervals to update search\nrange. Thus, the updated search range became smaller. Dividing the updated search range into smaller intervals will result in much smaller search range at next iteration step. When intervals becoming sufficient small, all ants will stay around extreme points, the centers of the intervals are their approximations.\nFigure 2 shows the distribution feature of ants. In addition, as it is well known, ACO runs slow, which is the bottleneck of application. And it evades this bottleneck that keeping only the intervals having ants to cut down search range."
    }, {
      "heading" : "F. Method of Searching All ExtremePoints",
      "text" : "Step1 (Initialization): Divide domain [a, b] into many small intervals and put an ant in each interval; Do other initialization. The detail is shown at section B. Suppose δ is the length of interval and ε is a stop threshold.\nStep2 While (δ > ε) {\nStep2.1: All ants move to new intervals according the rule shown at section C Step2.2: Update pheromone according the rule shown at section D Step2.3: Update search range according to section E and divide it into smaller intervals\n(Suppose the number of these intervals is n1) . Calculate the size of interval and set it to δ.\n} Step3 Extract all intervals that contain ants, the centers of the intervals are the approx-\nimations of extreme points.\nIf argument x is multi-dimensional vector, divide the range of every component of vector into smaller intervals, the combination of these intervals forms many small lattices. And then put an ant in each lattice, apply the above method, all extreme points can be extracted."
    }, {
      "heading" : "G. An Example",
      "text" : "To understand above method easily, an simple example is stated as below: Assume that the domain of 1-dimensional function is divided into 3 intervals I1, I2, I3, which associated center is x1, x2 and x3 respectively. Initially ant a1, a2, and a3 is put at x1, x2 and x3 respectively.\nCheck the first ant: If f(x1) > f(x2), ant a1 moves to interval I2. Otherwise, do nothing. Check the 2nd ant: If their is unique interval (e.g. I3 ) such that f(x2) > f(x3), ant a2 moves to interval I3. If f(x2) is smaller than both f(x1) and f(x3), do nothing. If f(x2) is bigger than both f(x1) and f(x3), it is uncertain that ant a2 moves to I1 or I3. And ant a2 will select its visiting interval randomly according to its transition probability defined at Eq.3.\nCheck the 3rd ant using same way. After all ants are checked, update their associated interval (position) and interval\npheromone. Repeat above processing until all ants can not move.\nThen keep the intervals which contains ants, and delete other blank intervals. And divided the intervals containing ants into smaller interval, repeat above process until the size of interval is sufficient small. And then all interval centers are the approximations of extreme points."
    }, {
      "heading" : "III. EXPERIMENT",
      "text" : "In this section, several functions will be tested. The parameters are listed as below: const = 10, α = 1, β = 1, C1 = 1, ρ = 0.3, ε = 0.0001 Two performances are considered, which are error (ratio of inaccuracy) r and runtime.\nError r is defined as\nr =\n( ∣\n∣ ∣ ∣\nf(x′0)− f(x0)\nf(x0)\n∣ ∣ ∣ ∣ ) × 100%\n, where (x0, f(x0)) denotes the true extreme point on theory and (x ′ 0, f(x ′ 0)) is its ap-\nproximation calculated by the method presented in this paper.\nIn addition, the hardware condition is: notebook PC DELL D520, CPU 1.66 GHZ.\nA. Instance 1 (see table 1 and Fig.3):\nf1(x) = sin 6(5.1πx+ 0.5), x ∈ [0, 1] In the experiment, additional parameter is n = 20 and n1 = 10. Table 1 and Fig.3 show\nall extreme points (local maximal points) of f1(x). Only 0.7106 seconds is cost.\nOther functions are tested, their errors are less than 10−8 except the boundary. And\nruntime is less than 1 second (see appendix I).\nB. Instance 2 (see Fig.4):\nf2(x) = 5e −0.5x sin(30x) + e0.2x sin(20x) + 6, x ∈ [0, 8] Instance 2 is a typical test function, which include many extreme points and any small change of argument x will result in big change. In addition, the theoretical calculation of extreme points of instance 2 is difficult.\nThe additional parameters are n = 480 and n1 = 10. Fig.4 shows all the calculated\nextreme points, and the real numbers are listed at appendix (see appendix II).\nC. Instance 3 (see Fig.5):\nf3(x1, x2) = x 2 1 + x 2 2 − cos(18x1)− cos(18x2), x1, x2 ∈ [−1, 1] The interval [−1, 1]⊗ [−1, 1] is divided into n = 40× 40 intervals initially. And at next iteration steps, search domain is divided into n1 = 20×20 small intervals. 36 extreme points are got and shown at Fig.5, and the digital solutions are listed at appendix (see appendix III)\nInstance 3 is a 2-dimensional function and 3203.2968 seconds is cost. And it is slower than 1-dimensional function instance 1 and instance 2. To improve running speed is the next work.\nMany functions are tested by the authors, and some experiments results are listed at appendix. These tests demonstrate that solution error is less than 10−8 except the special case that extreme point is at the border of the domain. In addition, these testes also demonstrate that the method is very fast for 1-dimensional function.\nNotice: From the Table 1, we can see that the border point has big error because the value calculated is the center of the interval, not boundary. To evade this drawback, the function value at boundary can be calculated directly."
    }, {
      "heading" : "IV. CONCLUSION",
      "text" : "To find all extreme points of multimodal functions is called extremum problem, which is a well known difficult issue in optimization fields. It is reported rarely that applying Ant Colony Optimization(ACO) to solve the problem. And the motivation of this paper is to explore ACO application method to solve it. In this paper, the following method is presented:\nDivide the domain of function into many intervals and put an ant in each interval. And then design rule such that every ant moves to the interval containing extreme point near by. At last all ants stay around extreme points.\nThe method presented in this paper has following three advantages:\n1. Solution accuracy is high. Experiment shows that solution error is less than 10−8. 2. Solution calculated is stable (robust). Ant only indicates the interval containing extreme point, not the accurate position of extreme point. It is easy for ant to find a interval although finding a special point in interval is difficult.\n3. The method is fast for 1-dimensional function. ACO is slow. But some feature is\nfound to speed ACO (see section 2.5)"
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors appreciate the discussion from the members of Gene Computation Group, J. Gang, X. Li, C.-B. Wang, W. Hu, S.-P. Wang, Q. Yang, J.-L. Zhou, P. Shuai, L.-J. Ye. The authors appreciate the help from Prof. J. Zhang, Z.-Lin Pu, X.-P. Wang, J. Zhou, and Q. Li.\n[1] Min-qiang Li, Ji-song Kou, “Coordinate multi-population genetic algorithms for multimodal\nfunction optimization,” Acta Automatica Sinica, 2002,28(04):497-504.\n[2] Qing-yun Tao, Hui-yun Quan, “The simulated annealing algorithm for multi-modal function\nproblem,” Computer Engineering and Applications, 2006,(14):63-64,92.\n[3] Li Li, Hong-qi Li, Shao-long Xie, “ Effective optimization algorithm for multimodal functions,”\nApplication Research of Computers, 2008,25(10):4792, 5792,6792.\n[4] Jiang Wu, Han-ying Hu, Ying Wu, “Application-oriented fast optimizer for multi-peak\nsearchin,” Application Research of Computers 2008,25(12):3617-3620.\n[5] Rui-ying Zhou, Jun-hua Gu, Na-na Li, Qing Tan, “New algorithm for multimodal function op-\ntimization based on immune algorithm and Hopfield neural network,” Computer Applications,\n2007,27(7):1751-1753,1756.\n[6] M. Dorigo, V. Maniezzo, and A. Colorni, “Positive feedback as a search strategy”. Technical\nReport 91-016, Dipartimento di Elettronica, Politecnico di Milano, Milan, Italy, 1991.\n[7] A. Colorni, M. Dorigo, and V. Maniezzo, “Distributed Optimization by Ant Colonies,” In F.\nJ.Varela and P. Bourgine, editors, Towards a Practice of Autonomous Systems: Proceedings\nof the First European Conference on Artificial Life, pages 134-142. MIT Press, Cambridge,\nMA,1992.\n[8] M. Dorigo, “Optimization Learning and Natural Algorithms,” PhD thesis, Dipartimento di\nElettronica, Politecnico di Milano, Milan, Italy, 1992.\n[9] M. Dorigo, V. Maniezzo, and A. Colorni, “THE Ant System: Optimization by a colony of\ncooperating agents,” IEEE Transactions on Systems, Man, and, Cybernetics Part B: Cyber-\nnetics. 26(1): 29-41.1996.\n[10] M.O. Ball, T.L. Magnanti, C.L. Monma, and G.L. Nemhauser, HANDBOOKS IN OPER-\nATIONS RESEARCH AND MANAGEMENT SCIENCE, 7: NETWORK MODELS, North\nHolland, 1995.\n[11] M.O. Ball, T.L. Magnanti, C.L. Monma, and G.L. Nemhauser,HANDBOOKS IN OPERA-\nTIONS RESEARCH AND MANAGEMENT SCIENCE, 8: NETWORK ROUTING, North\nHolland, 1995.\n[12] K. Doerner, Walter J. Gutjahr, R.F. Hartl, C. Strauss, C. Stummer, “Pareto ant colony\noptimization with IP preprocessing in multiobjective project portfolio selection,” European\nJournal of Operational Research 171, pp. 830-841 ,2006.\n[13] M. Dorigo and L. M. Gambardella, “Ant Colony System: A cooperative learning approach to\nthe traveling salesman problem,” IEEE Transactions on Evolutionary Computation, 1(1):53-"
    }, {
      "heading" : "66, 1997.",
      "text" : "[14] M. Manfrin, M. Birattari, T. Stűtzle, and M. Dorigo, “Parallel ant colony optimization for the\ntraveling salesman problem,” In M. Dorigo, L. M. Gambardella, M. Birattari, A. Martinoli, R.\nPoli, and T. Stűtzle (Eds.) Ant Colony Optimization and Swarm Intelligence, 5th International\nWorkshop, ANTS 2006, LNCS 4150 pp. 224-234. Springer, Berlin, Germany. Conference held\nin Brussels, Belgium. September 4-7, 2006.\n[15] L. M. Gambardella, D. Taillard, and M. Dorigo, “Ant colonies for the quadratic assignment\nproblem,” Journal of the Operational Research Society, 50(2):167-176, 1999.\n[16] L. M. Gambardella and M. Dorigo, “Ant-Q: A reinforcement learning approach to the traveling\nsalesman problem,” In A. Prieditis and S. Russell, editors, Machine Learning: Proceedings of\nthe Twelfth International Conference on Machine Learning, pages 252-260. Morgan Kaufmann\nPublishers, San Francisco, CA, 1995.\n[17] B. Bullnheimer, R. F. Hartl, and C. Strauss, “ Applying the ant system to the vehicle\nrouting problem,” IN I. H. Osman, S. Vo, S. Martello and C. Roucairol, editors, Meta-\nHeuristics:Advances and Trends in Local Search Paradigms for Optimization, pages 109-120.\nKluwerAcademics, 1998.\n[18] P. Forsyth and A. Wren, “An ant systemfor bus driver scheduling,” Technical Report 97.25,\nUniversity of Leeds , School of Computer Studies , July 1997. Presented at the 7th Interna-\ntional Workshop on Computer - Aided Scheduling of Public Transport , Boston , July 1997.\n[19] Rafael S. Parpinelli, Heitor S. Lopes, “Data mining with an ant colony optimization algo-\nrithm,” IEEE Transactions on Evolutionary Computation, vol. 6, no. 4, pp. 321-332, 2002.\n[20] M. Birattari, P. Pellegrini, and M. Dorigo, “On the invariance of ant colony optimization,”\nIEEE Transactions on Evolutionary Computation, vol. 11, no. 6, pp. 732-742, 2007.\n[21] W. J. Gutijahr, “A graph-based ant system and its convergence,” Future Generation Computer\nSystems 16 , 873-888, 2000.\n[22] T. St .. uezle and M. Dorigo. A Short Convergence Proof for a Class of ACO Algorithms. IEEE\nTransactions on Evolutionary Computation, 6(4):358-365, 2002.\n[23] Chao-Yang Pang, Chong-Bao Wang and Ben-Qiong Hu, “Experiment study of entropy\nconvergence of ant colony optimization,” arXiv:0905.1751v4 [cs.NE] 25 Oct 2009. [on line]\nhttp://arxiv.org/abs/0905.1751\n[24] Walter J. Gutjahr and Giovanni Sebastiani, “Runtime analysis of ant colony optimization\nwith best-so-far reinforcement,” Methodology and Computing in Applied Probability 10, pp."
    }, {
      "heading" : "409-433 , 2008.",
      "text" : "[25] Walter J. Gutjahr, “First steps to the runtime complexity analysis of ant colony optimization,”\nComputers and Operations Research 35 (no. 9), pp. 2711-2727 ,2008.\n[26] Chao-Yang Pang, Wei Hu, Xia Li, and Ben-Qiong Hu, “Applying local clustering method\nto improve the running speed of Ant Colony Optimization,” arXiv:0907.1012v2 [cs.NE] 7 Jul\n2009. [on line] http://arxiv.org/abs/0907.1012\n[27] Chao-Yang Pang, Wei Hu, Xia Li, and Ben-Qiong Hu, “Applying local clus-\ntering method to improve the running speed of Ant Colony Optimization,” [on\nline]http://arxiv.org/pdf/0907.1012\n[28] Chao-Yang Pang, Chong-Bao Wang and Ben-Qiong Hu, “Experiment study of entropy con-\nvergence of ant colony optimization,”[on line] http:// arxiv.org/pdf/0905.1751\n[29] G. A. Bilchev , I. C. Parmee, “The ant colony metaphor for searching continuous spaces,”\nLecture Notes in Computer Science, 993:25˜39, 1995.\n[30] M. R. Jalali , A. Afshar and M. A. Mariňo, “Multi-Colony Ant Algorithm for Continuous\nMulti-Reservoir Operation Optimization Problem,” Water Resour Manage 21:1429–1447,2007.\n[31] Wodrich M, Bilche G, “Cooperative distributed search: the ant’s way,” Control Cybern\n(3):413–446,1997.\n[32] Mathur M, Karale SB, Priye S, Jyaraman VK, Kulkarni BD, “Ant colony approach to con-\ntinuous function optimization,” Ind Eng Chem Res 39:3814–3822, 2000.\n[33] M. R. Jalali, A. Afshar, “Semi-continuous ACO algorithms (technical report),” Hydroin-\nformatics Center,Civil Engineering Department, Iran University of Science and Technology,\nTehran, Iran,2005b.\n[34] J. Dreo, P.-Siarry,“A new ant colony algorithm using the hierarchical concept aimed at opti-\nmization of multiminima continuous functions,” In: M. Dorigo, GD. Caro , M. Sampels (eds)\nProceedings of the 3rd international workshop on ant algorithms (ANTS 2002), vol 2463 of\nLNCS. Springer, Berlin HeidelbergNew York, pp 216–221,2002.\n[35] Y -J Li, T -J Wu, “An adaptive ant colony algorithm for continuous-space optimization\nproblems,” Journal of Zhejiang University SCIENCE,2003,4(1):40˜46."
    }, {
      "heading" : "V. APPENDIX",
      "text" : ""
    }, {
      "heading" : "A. Appendix I",
      "text" : ""
    }, {
      "heading" : "1. Instance 4:",
      "text" : "f4(x) = (x+ 1)(x+ 2)(x+ 3)(x+ 4)(x+ 5) + 5 x ∈ [−5, 0]\nThe additional parameters are n = 30 and n1 = 10."
    }, {
      "heading" : "2. Instance 5:",
      "text" : "f5(x) = (x+ 2) cos(9x) + sin(7x) x ∈ [0, 4] The additional parameters are n = 95 and n1 = 10\nB. Appendix II.\nf2(x) = 5e −0.5x sin(30x) + e0.2x sin(20x) + 6, x ∈ [0, 8] The additional parameters are n = 480 and n1 = 10."
    }, {
      "heading" : "C. Appendix III.",
      "text" : "f3(x1, x2) = x 2 1 + x 2 2 − cos(18x1)− cos(18x2), x1, x2 ∈ [−1, 1] The interval [−1, 1]⊗ [−1, 1] is divided into n = 40× 40 intervals initially. And at next\niteration steps, search domain is divided into n1 = 20× 20 small intervals."
    } ],
    "references" : [ {
      "title" : "Coordinate multi-population genetic algorithms for multimodal function optimization",
      "author" : [ "Min-qiang Li", "Ji-song Kou" ],
      "venue" : "Acta Automatica Sinica, 2002,28(04):497-504.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The simulated annealing algorithm for multi-modal function problem",
      "author" : [ "Qing-yun Tao", "Hui-yun Quan" ],
      "venue" : "Computer Engineering and Applications, 2006,(14):63-64,92.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Effective optimization algorithm for multimodal functions",
      "author" : [ "Li Li", "Hong-qi Li", "Shao-long Xie" ],
      "venue" : "Application Research of Computers, 2008,25(10):4792, 5792,6792.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Application-oriented fast optimizer for multi-peak searchin",
      "author" : [ "Jiang Wu", "Han-ying Hu", "Ying Wu" ],
      "venue" : "Application Research of Computers 2008,25(12):3617-3620. 15",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "New algorithm for multimodal function optimization based on immune algorithm and Hopfield neural network",
      "author" : [ "Rui-ying Zhou", "Jun-hua Gu", "Na-na Li", "Qing Tan" ],
      "venue" : "Computer Applications, 2007,27(7):1751-1753,1756.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Positive feedback as a search strategy",
      "author" : [ "M. Dorigo", "V. Maniezzo", "A. Colorni" ],
      "venue" : "Technical Report 91-016,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1991
    }, {
      "title" : "Distributed Optimization by Ant Colonies",
      "author" : [ "A. Colorni", "M. Dorigo", "V. Maniezzo" ],
      "venue" : "F. J.Varela and P. Bourgine, editors, Towards a Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life, pages 134-142. MIT Press, Cambridge, MA,1992.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Optimization Learning and Natural Algorithms",
      "author" : [ "M. Dorigo" ],
      "venue" : "PhD thesis, Dipartimento di Elettronica, Politecnico di Milano, Milan, Italy, 1992.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "THE Ant System: Optimization by a colony of cooperating agents",
      "author" : [ "M. Dorigo", "V. Maniezzo", "A. Colorni" ],
      "venue" : "IEEE Transactions on Systems, Man, and, Cybernetics Part B: Cybernetics. 26(1): 29-41.1996.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "HANDBOOKS IN OPER- ATIONS RESEARCH AND MANAGEMENT SCIENCE, 7: NETWORK MODELS",
      "author" : [ "M.O. Ball", "T.L. Magnanti", "C.L. Monma", "G.L. Nemhauser" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1995
    }, {
      "title" : "Nemhauser,HANDBOOKS IN OPERA- TIONS RESEARCH AND MANAGEMENT SCIENCE, 8: NETWORK ROUTING",
      "author" : [ "M.O. Ball", "T.L. Magnanti", "C.L. Monma", "G.L" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1995
    }, {
      "title" : "Pareto ant colony optimization with IP preprocessing in multiobjective project portfolio selection",
      "author" : [ "K. Doerner", "Walter J. Gutjahr", "R.F. Hartl", "C. Strauss", "C. Stummer" ],
      "venue" : "European Journal of Operational Research 171, pp. 830-841 ,2006.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Ant Colony System: A cooperative learning approach to the traveling salesman problem",
      "author" : [ "M. Dorigo", "L.M. Gambardella" ],
      "venue" : "IEEE Transactions on Evolutionary Computation, 1(1):53- 66, 1997.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Parallel ant colony optimization for the traveling salesman problem",
      "author" : [ "M. Manfrin", "M. Birattari", "T. Stűtzle", "M. Dorigo" ],
      "venue" : "M. Dorigo, L. M. Gambardella, M. Birattari, A. Martinoli, R. Poli, and T. Stűtzle (Eds.) Ant Colony Optimization and Swarm Intelligence, 5th International Workshop, ANTS 2006, LNCS 4150 pp. 224-234. Springer, Berlin, Germany. Conference held in Brussels, Belgium. September 4-7, 2006. 16",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Ant colonies for the quadratic assignment problem",
      "author" : [ "L.M. Gambardella", "D. Taillard", "M. Dorigo" ],
      "venue" : "Journal of the Operational Research Society, 50(2):167-176, 1999.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Ant-Q: A reinforcement learning approach to the traveling salesman problem",
      "author" : [ "L.M. Gambardella", "M. Dorigo" ],
      "venue" : "A. Prieditis and S. Russell, editors, Machine Learning: Proceedings of the Twelfth International Conference on Machine Learning, pages 252-260. Morgan Kaufmann Publishers, San Francisco, CA, 1995.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Applying the ant system to the vehicle routing problem",
      "author" : [ "B. Bullnheimer", "R.F. Hartl", "C. Strauss" ],
      "venue" : "IN I. H. Osman, S. Vo, S. Martello and C. Roucairol, editors, Meta- Heuristics:Advances and Trends in Local Search Paradigms for Optimization, pages 109-120. KluwerAcademics, 1998.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "An ant systemfor bus driver scheduling",
      "author" : [ "P. Forsyth", "A. Wren" ],
      "venue" : "Technical Report 97.25, University of Leeds , School of Computer Studies , July 1997. Presented at the 7th International Workshop on Computer - Aided Scheduling of Public Transport , Boston , July 1997.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Data mining with an ant colony optimization algorithm",
      "author" : [ "Rafael S. Parpinelli", "Heitor S. Lopes" ],
      "venue" : "IEEE Transactions on Evolutionary Computation, vol. 6, no. 4, pp. 321-332, 2002.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "On the invariance of ant colony optimization",
      "author" : [ "M. Birattari", "P. Pellegrini", "M. Dorigo" ],
      "venue" : "IEEE Transactions on Evolutionary Computation, vol. 11, no. 6, pp. 732-742, 2007.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A graph-based ant system and its convergence",
      "author" : [ "W.J. Gutijahr" ],
      "venue" : "Future Generation Computer Systems 16 , 873-888, 2000.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A Short Convergence Proof for a Class of ACO Algorithms",
      "author" : [ "T. St  .. uezle", "M. Dorigo" ],
      "venue" : "IEEE Transactions on Evolutionary Computation,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2002
    }, {
      "title" : "Experiment study of entropy convergence of ant colony optimization",
      "author" : [ "Chao-Yang Pang", "Chong-Bao Wang", "Ben-Qiong Hu" ],
      "venue" : "arXiv:0905.1751v4 [cs.NE] 25 Oct 2009. [on line] http://arxiv.org/abs/0905.1751",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Runtime analysis of ant colony optimization with best-so-far reinforcement",
      "author" : [ "Walter J. Gutjahr", "Giovanni Sebastiani" ],
      "venue" : "Methodology and Computing in Applied Probability 10, pp. 409-433 , 2008.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "First steps to the runtime complexity analysis of ant colony optimization",
      "author" : [ "Walter J. Gutjahr" ],
      "venue" : "Computers and Operations Research 35 (no. 9), pp. 2711-2727 ,2008.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Applying local clustering method to improve the running speed of Ant Colony Optimization",
      "author" : [ "Chao-Yang Pang", "Wei Hu", "Xia Li", "Ben-Qiong Hu" ],
      "venue" : "arXiv:0907.1012v2 [cs.NE] 7 Jul 17  2009. [on line] http://arxiv.org/abs/0907.1012",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Applying local clustering method to improve the running speed of Ant Colony Optimization",
      "author" : [ "Chao-Yang Pang", "Wei Hu", "Xia Li", "Ben-Qiong Hu" ],
      "venue" : "[on line]http://arxiv.org/pdf/0907.1012",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1012
    }, {
      "title" : "The ant colony metaphor for searching continuous spaces",
      "author" : [ "G.A. Bilchev", "I.C. Parmee" ],
      "venue" : "Lecture Notes in Computer Science, 993:25 ̃39, 1995.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Multi-Colony Ant Algorithm for Continuous Multi-Reservoir Operation Optimization Problem",
      "author" : [ "M.R. Jalali", "A. Afshar", "M.A. Mariňo" ],
      "venue" : "Water Resour Manage 21:1429–1447,2007.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Cooperative distributed search: the ant’s way",
      "author" : [ "M Wodrich", "G Bilche" ],
      "venue" : "Control Cybern (3):413–446,1997.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Ant colony approach to continuous function optimization",
      "author" : [ "M Mathur", "SB Karale", "S Priye", "VK Jyaraman", "BD Kulkarni" ],
      "venue" : "Ind Eng Chem Res 39:3814–3822, 2000.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Semi-continuous ACO algorithms (technical report)",
      "author" : [ "M.R. Jalali", "A. Afshar" ],
      "venue" : "Hydroinformatics Center,Civil Engineering Department, Iran University of Science and Technology, Tehran, Iran,2005b.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "P.-Siarry,“A new ant colony algorithm using the hierarchical concept aimed at optimization of multiminima continuous functions,",
      "author" : [ "J. Dreo" ],
      "venue" : "Proceedings of the 3rd international workshop on ant algorithms (ANTS 2002),",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "To solve the extremum problem, many methods of optimization are applied, such as genetic algorithm (GA) [1], simulated annealing (SA) [2], particle swarm optimization algorithm (PSO) [3, 4], immune algorithm ( IA) [5], and so on.",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "To solve the extremum problem, many methods of optimization are applied, such as genetic algorithm (GA) [1], simulated annealing (SA) [2], particle swarm optimization algorithm (PSO) [3, 4], immune algorithm ( IA) [5], and so on.",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "To solve the extremum problem, many methods of optimization are applied, such as genetic algorithm (GA) [1], simulated annealing (SA) [2], particle swarm optimization algorithm (PSO) [3, 4], immune algorithm ( IA) [5], and so on.",
      "startOffset" : 183,
      "endOffset" : 189
    }, {
      "referenceID" : 3,
      "context" : "To solve the extremum problem, many methods of optimization are applied, such as genetic algorithm (GA) [1], simulated annealing (SA) [2], particle swarm optimization algorithm (PSO) [3, 4], immune algorithm ( IA) [5], and so on.",
      "startOffset" : 183,
      "endOffset" : 189
    }, {
      "referenceID" : 4,
      "context" : "To solve the extremum problem, many methods of optimization are applied, such as genetic algorithm (GA) [1], simulated annealing (SA) [2], particle swarm optimization algorithm (PSO) [3, 4], immune algorithm ( IA) [5], and so on.",
      "startOffset" : 214,
      "endOffset" : 217
    }, {
      "referenceID" : 5,
      "context" : "Ant Colony Optimization (ACO) was first proposed by Dorigo (1991) [6, 7, 8].",
      "startOffset" : 66,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "Ant Colony Optimization (ACO) was first proposed by Dorigo (1991) [6, 7, 8].",
      "startOffset" : 66,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : "Ant Colony Optimization (ACO) was first proposed by Dorigo (1991) [6, 7, 8].",
      "startOffset" : 66,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : "ACO imitates this feature and it becomes an effective algorithm for the optimization problems [9].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "It has been successfully applied to many combinatorial optimization problems [10, 11, 12], such as Traveling Salesman Problem (TSP) [13, 14], Quadratic Assignment Problem(QAP) [15], Job-shop Scheduling Problem(JSP) [16], Vehicle Routing Problem(VRP) [17, 18], Data Mining(DM) [19] and so on.",
      "startOffset" : 77,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "It has been successfully applied to many combinatorial optimization problems [10, 11, 12], such as Traveling Salesman Problem (TSP) [13, 14], Quadratic Assignment Problem(QAP) [15], Job-shop Scheduling Problem(JSP) [16], Vehicle Routing Problem(VRP) [17, 18], Data Mining(DM) [19] and so on.",
      "startOffset" : 77,
      "endOffset" : 89
    }, {
      "referenceID" : 11,
      "context" : "It has been successfully applied to many combinatorial optimization problems [10, 11, 12], such as Traveling Salesman Problem (TSP) [13, 14], Quadratic Assignment Problem(QAP) [15], Job-shop Scheduling Problem(JSP) [16], Vehicle Routing Problem(VRP) [17, 18], Data Mining(DM) [19] and so on.",
      "startOffset" : 77,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "It has been successfully applied to many combinatorial optimization problems [10, 11, 12], such as Traveling Salesman Problem (TSP) [13, 14], Quadratic Assignment Problem(QAP) [15], Job-shop Scheduling Problem(JSP) [16], Vehicle Routing Problem(VRP) [17, 18], Data Mining(DM) [19] and so on.",
      "startOffset" : 132,
      "endOffset" : 140
    }, {
      "referenceID" : 13,
      "context" : "It has been successfully applied to many combinatorial optimization problems [10, 11, 12], such as Traveling Salesman Problem (TSP) [13, 14], Quadratic Assignment Problem(QAP) [15], Job-shop Scheduling Problem(JSP) [16], Vehicle Routing Problem(VRP) [17, 18], Data Mining(DM) [19] and so on.",
      "startOffset" : 132,
      "endOffset" : 140
    }, {
      "referenceID" : 14,
      "context" : "It has been successfully applied to many combinatorial optimization problems [10, 11, 12], such as Traveling Salesman Problem (TSP) [13, 14], Quadratic Assignment Problem(QAP) [15], Job-shop Scheduling Problem(JSP) [16], Vehicle Routing Problem(VRP) [17, 18], Data Mining(DM) [19] and so on.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 15,
      "context" : "It has been successfully applied to many combinatorial optimization problems [10, 11, 12], such as Traveling Salesman Problem (TSP) [13, 14], Quadratic Assignment Problem(QAP) [15], Job-shop Scheduling Problem(JSP) [16], Vehicle Routing Problem(VRP) [17, 18], Data Mining(DM) [19] and so on.",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 16,
      "context" : "It has been successfully applied to many combinatorial optimization problems [10, 11, 12], such as Traveling Salesman Problem (TSP) [13, 14], Quadratic Assignment Problem(QAP) [15], Job-shop Scheduling Problem(JSP) [16], Vehicle Routing Problem(VRP) [17, 18], Data Mining(DM) [19] and so on.",
      "startOffset" : 250,
      "endOffset" : 258
    }, {
      "referenceID" : 17,
      "context" : "It has been successfully applied to many combinatorial optimization problems [10, 11, 12], such as Traveling Salesman Problem (TSP) [13, 14], Quadratic Assignment Problem(QAP) [15], Job-shop Scheduling Problem(JSP) [16], Vehicle Routing Problem(VRP) [17, 18], Data Mining(DM) [19] and so on.",
      "startOffset" : 250,
      "endOffset" : 258
    }, {
      "referenceID" : 18,
      "context" : "It has been successfully applied to many combinatorial optimization problems [10, 11, 12], such as Traveling Salesman Problem (TSP) [13, 14], Quadratic Assignment Problem(QAP) [15], Job-shop Scheduling Problem(JSP) [16], Vehicle Routing Problem(VRP) [17, 18], Data Mining(DM) [19] and so on.",
      "startOffset" : 276,
      "endOffset" : 280
    }, {
      "referenceID" : 19,
      "context" : "Birattari proves the invariance of ACO and introduced three new ACO algorithms [20].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "Gutjahr studied the convergence of ACO firstly in 2000 [21].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "Dorigo proved the existence of the ACO convergence under two conditions, one is to only update the pheromone of the shortest route generated at each iteration step, the other is that the pheromone on all routes has lower bound [22].",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 22,
      "context" : "found a potential new view point to study ACO convergence under general condition, that is entropy convergence in 2009 [23].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 22,
      "context" : "[23], the following conclusion is get: ACO may not converges to the optimal solution in practice, but its entropy is convergent.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "Gutjahr presented some theoretical results about ACO runtime [24, 25].",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 24,
      "context" : "Gutjahr presented some theoretical results about ACO runtime [24, 25].",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : "do the following attempt [26].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "The first method for continuous-space optimization problems, called Continuous ACO (CACO), was proposed by Bilchev and Parmee (1995)[29], and later it was used by some others [30, 31, 32, 33].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 28,
      "context" : "The first method for continuous-space optimization problems, called Continuous ACO (CACO), was proposed by Bilchev and Parmee (1995)[29], and later it was used by some others [30, 31, 32, 33].",
      "startOffset" : 175,
      "endOffset" : 191
    }, {
      "referenceID" : 29,
      "context" : "The first method for continuous-space optimization problems, called Continuous ACO (CACO), was proposed by Bilchev and Parmee (1995)[29], and later it was used by some others [30, 31, 32, 33].",
      "startOffset" : 175,
      "endOffset" : 191
    }, {
      "referenceID" : 30,
      "context" : "The first method for continuous-space optimization problems, called Continuous ACO (CACO), was proposed by Bilchev and Parmee (1995)[29], and later it was used by some others [30, 31, 32, 33].",
      "startOffset" : 175,
      "endOffset" : 191
    }, {
      "referenceID" : 31,
      "context" : "The first method for continuous-space optimization problems, called Continuous ACO (CACO), was proposed by Bilchev and Parmee (1995)[29], and later it was used by some others [30, 31, 32, 33].",
      "startOffset" : 175,
      "endOffset" : 191
    }, {
      "referenceID" : 32,
      "context" : "Other methods include that, in 2002, Continuous Interacting Ant Colony (CIAC) was proposed by Dreo and Siarry [34], and in 2003, an adaptive ant colony system algorithm for continuous-space optimization problems was proposed by Li Yan-jun [35], and so on.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "5), x ∈ [0, 1] In the experiment, additional parameter is n = 20 and n1 = 10.",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 7,
      "context" : "5x sin(30x) + e sin(20x) + 6, x ∈ [0, 8] Instance 2 is a typical test function, which include many extreme points and any small change of argument x will result in big change.",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "[1] Min-qiang Li, Ji-song Kou, “Coordinate multi-population genetic algorithms for multimodal function optimization,” Acta Automatica Sinica, 2002,28(04):497-504.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] Qing-yun Tao, Hui-yun Quan, “The simulated annealing algorithm for multi-modal function problem,” Computer Engineering and Applications, 2006,(14):63-64,92.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Li Li, Hong-qi Li, Shao-long Xie, “ Effective optimization algorithm for multimodal functions,” Application Research of Computers, 2008,25(10):4792, 5792,6792.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Jiang Wu, Han-ying Hu, Ying Wu, “Application-oriented fast optimizer for multi-peak searchin,” Application Research of Computers 2008,25(12):3617-3620.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Rui-ying Zhou, Jun-hua Gu, Na-na Li, Qing Tan, “New algorithm for multimodal function optimization based on immune algorithm and Hopfield neural network,” Computer Applications, 2007,27(7):1751-1753,1756.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] K.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] L.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] L.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] B.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] P.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] Rafael S.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] W.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] T.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23] Chao-Yang Pang, Chong-Bao Wang and Ben-Qiong Hu, “Experiment study of entropy convergence of ant colony optimization,” arXiv:0905.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "1751 [24] Walter J.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 24,
      "context" : "[25] Walter J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] Chao-Yang Pang, Wei Hu, Xia Li, and Ben-Qiong Hu, “Applying local clustering method to improve the running speed of Ant Colony Optimization,” arXiv:0907.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "1012 [27] Chao-Yang Pang, Wei Hu, Xia Li, and Ben-Qiong Hu, “Applying local clustering method to improve the running speed of Ant Colony Optimization,” [on line]http://arxiv.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 27,
      "context" : "1751 [29] G.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 28,
      "context" : "[30] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[31] Wodrich M, Bilche G, “Cooperative distributed search: the ant’s way,” Control Cybern (3):413–446,1997.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[32] Mathur M, Karale SB, Priye S, Jyaraman VK, Kulkarni BD, “Ant colony approach to continuous function optimization,” Ind Eng Chem Res 39:3814–3822, 2000.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[33] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[34] J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "f5(x) = (x+ 2) cos(9x) + sin(7x) x ∈ [0, 4] The additional parameters are n = 95 and n1 = 10 Table 3.",
      "startOffset" : 37,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "5x sin(30x) + e sin(20x) + 6, x ∈ [0, 8] The additional parameters are n = 480 and n1 = 10.",
      "startOffset" : 34,
      "endOffset" : 40
    } ],
    "year" : 2009,
    "abstractText" : "Ben-Qiong Hu College of Information Management, Chengdu University of Technology, 610059, China Abstract To find all extreme points of multimodal functions is called extremum problem, which is a well known difficult issue in optimization fields. Applying ant colony optimization (ACO) to solve this problem is rarely reported. The method of applying ACO to solve extremum problem is explored in this paper. Experiment shows that the solution error of the method presented in this paper is less than 10−8.",
    "creator" : "LaTeX with hyperref package"
  }
}