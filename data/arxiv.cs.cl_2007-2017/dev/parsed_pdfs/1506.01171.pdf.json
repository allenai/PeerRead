{
  "name" : "1506.01171.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Hybrid Model for Enhancing Lexical Statistical Machine Translation (SMT)",
    "authors" : [ "Ahmed G. M. ElSayed", "Ahmed S. Salama", "Ahmed Ragab Nabhan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "currently due to political and social events in the world. A proposed Statistical Machine Translation (SMT) based model that can be used to translate a sentence from the source Language (English) to the target language (Arabic) automatically through efficiently incorporating different statistical and Natural Language Processing (NLP) models such as language model, alignment model, phrase based model, reordering model, and translation model. These models are combined to enhance the performance of statistical machine translation (SMT). Many implementation tools have been used in this work such as Moses, Gizaa++, IRSTLM, KenLM, and BLEU. Based on the implementation, evaluation of this model, and comparing the generated translation with other implemented machine translation systems like Google Translate, it was proved that this proposed model has enhanced the results of the statistical machine translation, and forms a reliable and efficient model in this field of research. Keywords: Machine Learning, Statistical Machine Translation, Lexical Machine Translation, Linguistics."
    }, {
      "heading" : "1. Introduction",
      "text" : "Statistical Machine Translation (SMT) deals with automatically mapping sentences in one human language into another human language. Therefore, it translates from the source language to the target language. The goal of SMT is to analyze automatically existing human sentence translations, to build general translation rules. The problem of machine translation has not been solved yet. Much research and development still needed to earn the reliability of humans by preforming a fluency translation as humans do. Therefore, a method represented to tune the translation parameters and improve the translation quality."
    }, {
      "heading" : "2. Literature Review",
      "text" : "In general, many models have been used in machine translation for many years. SMT was the preferred approach for research and development of machine translation systems within this period. Rabeh Zbib conducted a research in 2010 for how to use linguistic knowledge in statistical machine translation in MIT, USA [1]. In 2007, Sarikaya and Deng published about joint morphological-lexical language modeling for machine translation in the conference of the North American Chapter of the Association for Computational Linguistics about [2]. Rabih Zbib, Spyros Matsoukas, Richard Schwartz, and John Makhoul.in 2010 conducted a research for using decision trees for lexical smoothing in statistical machine translation [3]. Ahmed Ragab Nabhan and Ahmed Rafea in 2005 have done a research on tuning statistical machine translation parameters in central laboratory for agricultural expert system (CLAES) [4]. Philipp Koehn in 2004 presented pharaoh: a beam search decoder for phrase-based statistical machine translation models [5]. Papineni, K.; Roukos, S.; Ward, T.; Zhu, W. J. in 2002 introduced the \"BLEU: which is a method for automatic evaluation of machine translation\" in ACL 40th annual meeting of the association for computational linguistics [6]. Nizar Habash and Owen Rambow, in 2005 conducted a research about Arabic tokenization using part-of-speech tagging and morphological disambiguation in one fell swoop [7]. Ibrahim Badr, Rabih Zbib, and James Glass in 2008 done a research on segmentation for English-toArabic statistical machine translation [8]. Franz Och and Hermann Ney have done a research in 2000 about the improved statistical alignment models in ACL [9]. Ibrahim Badr, Rabih Zbib, and James Glass in 2009 have conducted a research for syntactic phrase reordering for English-to-Arabic statistical machine translation in MIT,\n2015 International Journal of Computer Science Issues\nUSA [10]. Kenneth Heafiel, Philipp Koehn, and Alon Lavie did a research in 2013 about grouping language model boundary words to speed K-Best extraction from hypergraphs in proceedings of the conference of the north American chapter of the association for computational linguistics (NAACL) [11]. In 2004 Noah A. Smith has done a research on Log-Linear Models in Johns Hopkins University in USA [12]."
    }, {
      "heading" : "3. The Proposed Model Architecture",
      "text" : "The proposed model for enhancing lexical statistical machine translation architecture is shown in Fig. 1 Model components are discussed briefly in the following subsection.\n3.1 System Administrator\nSystem Admin is the first component in this model, which will use a user interface and the DBMS tool to enter vocabularies, patterns, and the already translated sentences into the database to use it as a reference for translations.\n3.2 User\nThe user is the person who will use the model to enter the sentence required for translation, and after he enters it, he will get the result and the translation for this sentence which has the best probability score.\n3.3 User Interface (UI)\nThis is the user interface, which used from system admin or the user of this model. System admin will have interface, which allows him to control the model and administrate it while the user will have interface, which allows him to enter the required sentence for translation then get the translation for this sentence. Both of the system admin and the user will use the user interface to interact with the DB across the Database Management System.\n3.4 The Database (DB)\nThis database will be used to store all data, which will be used in this model, plus the new translated sentences with their sequence. This component will contain the source English sentence before Arabic translation plus the Parallel corpus or corpora, which is the already translated sentences and patterns from human translations. It will also contain a dictionary or vocabularies and patterns for many and many words and sentences to get all possible translations for each word. The more human-translated patterns in the database, which the model can analyze, and use, the better the translation quality will be.\n3.5 Database Management System (DBMS)\nDBMS is the software, which allows system admin and users of this model to interact with this database. System admin will be allowed to enter new patterns or new sentences, and enter new translated words. User will be allowed to enter the sentence required for translation.\n3.6 Data Preparator\nThis component focuses on converting the input parallel corpus into format, which is suitable for using in this model. Data preparator of this model performs four main functions as follows:  Normalization by converting all words of source and\ntarget language to upper or lower cased in all sentences.\n Tokenization for all data in the both language corpus data by inserting putting spaces between words and\npunctuation.\n True-casing by generating probabilities for all words in the parallel corpus and building a model for this true-\ncasing to be used for generating the file of the truecasing for each language.\n Cleaning data by delete long sentences, which are longer than specific number or remove empty sentence\nor misaligned sentences, which can affect the translation quality.\n3.7 Trainer\nThis is the core of this model. In this component, many models combined to perform a hybrid model for data training to get better translation result. The trainer applies training on the following components:\n3.7.1 Language Model (LM)\nLanguage model built to make sure that translation has been performed correctly with fluency, so it has been built in the target language. Language model built based on a combination of ngram models. The “language model” or “lm” is a statistical description of one language that includes the frequencies of token-based n-grams occurrences in a corpus. Language model is trained from a large monolingual corpus and saved as a file. The language model file is a required component of every translation model. This model depends on a combination from the unigram language model to a 5gram language model and the following is an example of how the Bigram and Trigram language model was built [13].\n2015 International Journal of Computer Science Issues\nConsider V is a finite set of all words in the language. Assume that x1, x2, …., xn is the sequence of words in the language while xn >1 and xn is a special symbol STOP, and (STOP is not a member in the set V). Define V+ as a set of all sentences with the vocabulary V, this means that V+ is infinite set because sentences can be in any length. Consider a sequence of random variables X1;X2, ….., Xn. Each random variable can take any value in a finite set V, and n is the number of words + 1 (STOP symbol). A Markov bigram, and trigram language models are presented as follows:\n A Bigram Markov language model:\nP(X1 = x1;X2 = x2, …, Xn = xn)\n= P(X1 = x1)\n= P(X1 = x1) (1)\nwhere P (X1 = x1) is the probability of the first random variable X1 to be equal to the first word x1 in the target language, and so on. The first-order Markov assumption:\nfor any , for any x1….xi,\n A Trigram Markov language model: The Trigram Language Model consists of:\n A finite set\n A parameter q for each trigram such that , and .\nWhere the value for q is the probability\nof seeing the word after the bigram .\nTherefore q defines a distribution over\npossible words conditioned on the bigram\ncontext .\nFor any sentence x1 … xn where for i=1…(n-1), and xn=STOP, the probability of the sentence under the trigram language model is as follows:\nP(X1 = x1; X2 = x2; X3 = x3, …, Xn = xn)\n= P(X1 = x1)\n(2)\n(For convenience we assume x0 = xi-1 = *, where * is a special “start” symbol) From Error! Reference source not found.), Eq. (1) is as follows:\n(3)\nWhere we define x0 = xi-1 = * For example: ةركلا دمحأ بعلي STOP\nWe would have: بعليةركلا دمحأ STOP)\nA natural estimate (the “maximum likelihood estimate”):\n(4)\n3.7.2 Alignment Model\nAlignment models used in statistical machine translation to determine relation between translation words in a sentence in one language compared to the words in a sentence with the same meaning in a different language. The alignment model form an important part of the translation process, as it’s used to produce word-aligned text, which is used to create machine translation systems. IBM five models are famous models for alignment. They were proposed about 25 years ago from now but they still form the state of the art models for alignment. They consist of five models for word-to-word alignment called “IBM 1-5 models”. With alignment model, a generalization of the procedures of extracting the phrase of phrase-based systems with its corresponding sequence can be done. Alignment models will be used to align words to enhance translations. IBM model 4 has been used because it has increased sophistication of model 3, so it sums over high probability alignments, and it search for the most probable alignment [13]. Aligned data are elements of a parallel corpus in two languages. Each element in one language matches the corresponding element in the other language. This model\n2015 International Journal of Computer Science Issues\nconsists of the training model for inspecting the alignments for data and create alignment table which used later to align words and help in perform reordering model.\n3.7.3 Translation and Phrases Extractor\nThis component used to get the direct translation for words then create table with maximum likelihood for words translations and extract phrases in one file with their scores. It extracts the phrase table, which is a statistical description of a parallel corpus of source-target language sentence pairs.\n3.7.4 Reordering Model\nAfter phrases extraction with their scores, the reordering model uses the alignment model to generate reordering table. Reordering table contains statistical frequencies that describe changes in word order between source and target language.\n3.7.5 Translation Model\nThis is the final components generated as output after training all the previous components, which will be tuned later or can be used to generate the translation directly. The translation model contains translation table, alignment model, phrase table, reordering table, and language model, which will be used for translation.\n3.8 Tuner\nThe main purpose of tuning the trained SMT model is to enhance statistical machine translation results and improve its quality. Since SMT, training uses a linear model, the tuning aims to find the optimal weights for this linear model and minimize error rating, where optimal weights are those, which maximize translation performance. Therefore, that tuning is a process of finding the optimized settings for the translation model. The tuning process translates thousands of source language phrases in the tuning set with a translation model, compares the model's output to a set of reference human translations, then it adjusts the settings with the intention to improve the translation quality. This process continues through iterations. With each iteration, the tuning process repeats the steps until it reaches an optimized translation quality and minimized error rate.\n3.9 Translator\nThis is the final component of this model, which applies the translation model with all its components (language model, translation table, alignment model, phrase table, and reordering table) on sentence provided by user to\ntranslate it to the appropriate translation. The translator job is to implement translation model to the required sentence for translation and translate the sentence then send the translation back to user through the UI."
    }, {
      "heading" : "4. The proposed algorithms and activity diagrams for the proposed model",
      "text" : "In the coming two subsections, activity diagrams and algorithms are proposed.\n4.1 Proposed Model Activity Diagrams\nIn this section, two activity diagrams for this proposed model are shown in Error! Reference source not found. and Error! Reference source not found.. Error! Reference source not found. shows the activity diagram for the training and learning activities of the main model components. Error! Reference source not found. shows the steps starting from when user provides the source English sentence until the user receives the translated sentence in the target language (Arabic language).\n2015 International Journal of Computer Science Issues\n4.2 Model Algorithms\nThe proposed model algorithms for the model learning, training and the model translation are presented in this section.\n4.2.1 Model training and learning algorithm\n1. System administrator will support the dictionary and feed it with vocabularies and translated sentences or\nparallel corpora, which used as a reference for the translation model. He will enter this data through the user interface (UI) and Database Management System (DBMS) tool. 2. Prepare the parallel corpora for the training process by normalizing words, tokenization, truecasing, and\ncleaning.\n3. Create the language model for the source language to help in translation process. 4. Train the word alignment model with parallel corpus data. 5. Save the alignment model files for references and to help create the reordering model. 6. Generate translation table with maximum likelihood estimation. 7. Save the translation table. 8. Build phrases table for translated phrases with their\nscores.\n9. Save the phrases table. 10. Build reordering model. 11. Save the reordering table. 12. Build the translation model. 13. Save the translation model. 14. Tune the translation model and minimize the error rate\nfor translation to improve translation quality and add model upgrade.\n15. Binarise the phrase table and reordering table.\nBy performing all the previous steps, the translation model would be trained for parallel corpus.\n4.2.2 The user source statement translation algorithm\n1. User enters the English verb phrase statement, which will be translated through the UI and DBMS tool. 2. Store the English verb phrase to the DB. 3. Align the sentence using alignment model. 4. Translate the sentence using the translation table. 5. Reorder the translated sentence. 6. Save the translated sentence into the DB. 7. Display the translated sentence to the user.\nAfter performing the previous steps, the required sentence for translation will be translated into the target language\n“the Arabic language” and displayed to the user of this model."
    }, {
      "heading" : "5. PROPOSED MODEL IMPLEMENTATION",
      "text" : "This model presents integration between many components for enhancing lexical statistical machine translation including language model, alignment model, phrase based translation model, reordering model, translation modeling, and tuning the final translation model. All these components combined together in order to improve the quality of the output of statistical machine translation and build a more reliable model. Set of implementation tools were used to build and train the main components of the model. Data sets from different data sources were used during the training and learning phases of the model. The Implemented model was applied on the translation of test data set. The generated translation quality was measured by the BLEU score tool.\n5.1 The Proposed Model Implementation Tools\nThis model used a various set of tools to generate the final model. These tools are as follows:\n IRSTLM Toolkit for language modeling.  KenLM also for language modeling.  In language modeling, both IRSTLM Toolkit and\nKenLM tool were used together combined with the main tool for language modeling and training. Both of them do the same purpose but the only difference is IRSTLM is better in query and editing the model, but KenLM is faster in model creation because it is multithreaded and doing the language model binarisation.\n GIZA++ word aligning tool to align the parallel corpus and train the alignment model.\n This toolkit is an implementation of the IBM models that started statistical machine translation research and\nthe state of the art techniques.\n MOSES as a complete statistical machine translation system. In the rest of this model for training and\ntuning, Moses have been used which allows training translation models for language pairs. Once creating the translation model, an efficient search algorithm finds quickly the highest probability translation among the exponential number of choices.\n Bi-Lingual Evaluation Understudy (BLEU) score tool, which is the most famous scoring, and testing tool for\nstatistical machine translation quality was used for model evaluation. The BLEU score indicates how closely the token sequences in one set of data for example in machine translation output, correlate with\n2015 International Journal of Computer Science Issues\nor match the token sequences in another set of data, such as a reference human translation.\n5.2 Model training and learning\nIn the training and learning of the proposed model, different data sets were used. After the collection of these data sets, data preparation activities should be applied in these data sets before the model training, tuning, and evaluating. These preparation activities were presented in section 3.63.6. As input for this experiment to test this model, a small amount of data used from a collection of translated documents from the United Nation for model training. Model training data presented as a set of 74067 sentences (3138881 token) before data cleaning and 67575 sentences after data cleaning [14]. For tuning a collection of translated sentences from Tatoeba were used consists of 13000 sentence (65300 token) [15]. Finally for model evaluation a parallel corpus of Ubuntu localization files consist of 6000 sentences (24400 token) were used to get the BLEU result and assess the model [15].\n5.3 Model Evaluation Result\nAfter using the BLEU score tool in evaluating the implemented model, it was found that the score for this model before tuning was 8.99. This considered a good result compared to the number of data used for training in this model. In addition, the data used for the model evaluation is not from the same domain, which decreases the BLEU result. After tuning this model, the BLEU score shows a significant improvement as it achieved a result of 19.52. As known that the more data used in training the model, the better result we get. In case of using data from the same domain for testing, less errors would be achieved from the model and better translation BLEU result accomplished. When using small number of corpus, then the expected BLEU score is very low. As a conclusion for this model, it shows that after tuning the model, a better result achieved. The outcome for this model has jumped from 8.99 to 19.52, which is a significant improvement and a good result came from small sized corpus used in training. In addition, it’s considered a good result since the test data set were collected from different domains rather than the same domain for training. As the model training is continued using more new data sets in the same domains, the better the translation quality it generates.\n5.4 Model Translation Results Using Case Study\nIn the following sub sections, a set of English sentences were used as a case study to present the generated model Arabic translation before tuning and after tuning.\n5.4.1 English Sentences used for evaluation\n Britain has largely excluded itself from any leadership role in Europe.\n Many members of congress represent districts.\n I will transfer the money.  Italy is far from Brazil.\n5.4.2 Model Translation before Tuning\n The Arabic translation for the sentence “Britain has largely excluded itself from any\nleadership role in Europe” before tuning as shown in Fig. 3 is نم ةدعبتسم دق ايناطيربل اهتاذ دح\" \"ابروأ يف ىدايقلا رودلا يأ\n The Arabic translation for the sentence “Many members of congress represent\ndistricts” before tuning as shown in Fig. 4 is \"ةنملأاو ةرقتسملا لثمت رشع ءاضعلأا نم ديدعلا\"\n The Arabic translation for the sentence “I will transfer the money” before tuning as\nshown in Fig. 5 is لولأا\" لقنتس ىلإ \"لسغب\n2015 International Journal of Computer Science Issues\n5.4.3 Model Translation after Tuning\n The Arabic translation for the sentence “Britain has largely excluded itself from any\nleadership role in Europe” after tuning as shown in Fig. 7 is ىلإ اهسفن ايناطيرب تدعبتسا دقل\" \"ابروأ يف يدايق رود يأب علاطضلاا نم ريبك دح\n The Arabic translation for the sentence “Many members of congress represent\ndistricts” after tuning as shown in Fig. 8 is \"قطانملا نولثمي سرجنوكلا ءاضعأ نم ديدعلا نإ\"\n The Arabic translation for the sentence “I will transfer the money” after tuning as\nshown in Fig. 9 is لوّحأس\" \"لاملا\nAs shown from the previous translations, the translation before and after tuning are not the same. A huge improvement in translation noticed in translation after tuning than the translation before tuning. Tuning can be done for this model several times to improve the model quality, and many parallel corpuses can be trained to give better translation results. When comparing the model results with current SMT models such as Google it shows that there is a difference in the result between both systems, as the translation for the previous mentioned phrases in google translate are:\n The Arabic translation for the sentence “Britain has largely excluded itself from any\nleadership role in Europe” using Google\n2015 International Journal of Computer Science Issues\nTranslate is دقو\" ايناطيرب تدعبتسا اهسفن ىلإ دح ريبك نم يأ رود يدايق يف \"ابوروأ\n The Arabic translation for the sentence “Many members of congress represent\ndistricts” using Google Translate is ديدعلا\" نم ءاضعأ سرجنوكلا نولثمي رئاود \"ةيباختنا\n The Arabic translation for the sentence “I will transfer the money” using Google\nTranslate is انأ\" فوس لقن \"لاوملاا\n The Arabic translation for the sentence “Italy is far from Brazil” using Google Translate is\nايلاطياو\" يه ةديعب لك دعبلا نع \"ليزاربلا\nAfter making comparison, it was found that Google generated translation for these sentences is less accurate from the proposed model generated translation. From these translation results, a conclusion can be noticed that the proposed model has enhanced some translation results rather than the current SMT models."
    }, {
      "heading" : "6. Conclusions",
      "text" : "The proposed model in this paper incorporates efficiently many models at different levels namely: the language model, the alignment model, phrase based model, reordering model, translation model, and finally the tuning model. It was concluded that the achieved benefits of using these models are summarized as follows:\n The alignment models are used in statistical machine translation to determine translational correspondences\nbetween words in a sentence in one language with the words in a sentence with the same meaning in a different language.\n Language models increase the efficiency of the word alignment by using words depending on their context\nin the sentence.\n Phrase based model increases the capability of the proposed model by dealing with words and their\ncorrespondences or phrases in both languages at the same time. It adds value to the effectiveness of this model by translating a group of contiguous words in one language to a contiguous sequence of words in the other language.\n Reordering model uses all the mentioned previous models to generate reordering table by determining the\norientation of two phrases based on word alignments at training time. This adds extra points to the reliability of this model and increases its dependability.\nIn addition, a set of other conclusions can be listed as follows:\n When combining different models in building and training statistical machine translation model, this will\nhelp in achieving good results even when using a small number of data.\n With model tuning, the translation results have been improved and the error rate has been minimized.\n The proposed model is adaptive, since new words, vocabularies, phrases, and sentences can be added and\nused in model training and tuning. This leads to the enhancement the Arabic translation quality.\n Context dependent language model achieves better and accurate translation. However, it is more costly.\n The order in which the used models are combined and trained has an effect in achieving better translation\nresults.\n Generally, the implemented model proved by evaluation that it is a reliable, efficient, and effective\nin enhancing lexical statistical machine translation systems."
    }, {
      "heading" : "7. FUTURE WORK",
      "text" : "The problem of machine translation has not solved yet. Much research and development still needed to earn the reliability of humans by preforming a fluency translation as humans do. The demand of faster and cheaper translation between languages will only increase with the need to share information between nations. Based on this work, expected future work and recommendations are as follows:\n Clustering is one of the most important techniques, which needs to be done on the Arabic language to\nsolve the tokenization problem. Clustering as a general approach for dealing with issues of language sparsity and morphological analysis is promising.\n Use large amount of parallel corpus data to train the proposed model, to achieve better results.\n Use large amount of data for tuning from the same domain of the training data to get better tuning result.\n Every time this model is tuned, the better results it can achieve, so it would be better to tune it as much as\npossible.\n Use testing data from the same domain to get better actual results.\n Integration with other machine learning techniques such as artificial neural networks in model learning\nand training to achieve better results."
    } ],
    "references" : [ {
      "title" : "Zbib, “Using linguistic knowledge in statistical machine translation",
      "author" : [ "M. R" ],
      "venue" : "Doctoral dissertation, Massachusetts Institute of Technology,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Joint morphological-lexical language modeling for machine translation”, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics; Companion  IJCSI International Journal of Computer Science Issues, Volume 12, Issue 2, March 2015",
      "author" : [ "R. Sarikaya", "Y. Deng" ],
      "venue" : "www.IJCSI.org",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Decision trees for lexical smoothing in statistical machine translation",
      "author" : [ "R. Zbib", "S. Matsoukas", "R. Schwartz", "J. Makhoul" ],
      "venue" : "Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Tuning Statistical Machine Translation Parameters",
      "author" : [ "A.R. Nabhan", "A.A. Rafea" ],
      "venue" : "Proceedings of IEEE International Conference on Information Reuse and Integration,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Pharaoh: a beam search decoder for phrase-based statistical machine translation models”, Proceedings of Machine translation: From real users to research, 6th Conference of the Association for Machine Translation in the Americas",
      "author" : [ "P. Koehn" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "BLEU: a method for automatic evaluation of machine translation",
      "author" : [ "K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu" ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2002
    }, {
      "title" : "Arabic tokenization, part-ofspeech tagging and morphological disambiguation in one fell swoop",
      "author" : [ "N. Habash", "O. Rambow" ],
      "venue" : "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "Segmentation for English-to- Arabic statistical machine translation",
      "author" : [ "I. Badr", "R. Zbib", "J. Glass" ],
      "venue" : "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, Ohio,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Improved statistical alignment models",
      "author" : [ "F.J. Och", "H. Ney" ],
      "venue" : "Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2000
    }, {
      "title" : "Syntactic phrase reordering for English-to-Arabic statistical machine translation",
      "author" : [ "I. Badr", "R. Zbib", "J. Glass" ],
      "venue" : "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Grouping Language Model Boundary Words to Speed K-Best Extraction from Hypergraphs",
      "author" : [ "K. Heafield", "P. Koehn", "A. Lavie" ],
      "venue" : "Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics, Atlanta,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Log-linear models. Department of Computer Science, Center for Language and Speech Processing",
      "author" : [ "N.A. Smith" ],
      "venue" : "Johns Hopkins University,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2004
    }, {
      "title" : "Statistical machine translation",
      "author" : [ "P. Koehn" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "United Nations general assembly resolutions: A six-language parallel corpus",
      "author" : [ "A. Rafalovitch", "R. Dale" ],
      "venue" : "Proceedings of the MT Summit,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Parallel Data, Tools and Interfaces in OPUS",
      "author" : [ "J. Tiedemann" ],
      "venue" : "In LREC,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Rabeh Zbib conducted a research in 2010 for how to use linguistic knowledge in statistical machine translation in MIT, USA [1].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "In 2007, Sarikaya and Deng published about joint morphological-lexical language modeling for machine translation in the conference of the North American Chapter of the Association for Computational Linguistics about [2].",
      "startOffset" : 216,
      "endOffset" : 219
    }, {
      "referenceID" : 2,
      "context" : "in 2010 conducted a research for using decision trees for lexical smoothing in statistical machine translation [3].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : "Ahmed Ragab Nabhan and Ahmed Rafea in 2005 have done a research on tuning statistical machine translation parameters in central laboratory for agricultural expert system (CLAES) [4].",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 4,
      "context" : "Philipp Koehn in 2004 presented pharaoh: a beam search decoder for phrase-based statistical machine translation models [5].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "in 2002 introduced the \"BLEU: which is a method for automatic evaluation of machine translation\" in ACL 40th annual meeting of the association for computational linguistics [6].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 6,
      "context" : "Nizar Habash and Owen Rambow, in 2005 conducted a research about Arabic tokenization using part-of-speech tagging and morphological disambiguation in one fell swoop [7].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 7,
      "context" : "Ibrahim Badr, Rabih Zbib, and James Glass in 2008 done a research on segmentation for English-toArabic statistical machine translation [8].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "Franz Och and Hermann Ney have done a research in 2000 about the improved statistical alignment models in ACL [9].",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : "USA [10].",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 10,
      "context" : "Kenneth Heafiel, Philipp Koehn, and Alon Lavie did a research in 2013 about grouping language model boundary words to speed K-Best extraction from hypergraphs in proceedings of the conference of the north American chapter of the association for computational linguistics (NAACL) [11].",
      "startOffset" : 279,
      "endOffset" : 283
    }, {
      "referenceID" : 11,
      "context" : "Smith has done a research on Log-Linear Models in Johns Hopkins University in USA [12].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "This model depends on a combination from the unigram language model to a 5gram language model and the following is an example of how the Bigram and Trigram language model was built [13].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 12,
      "context" : "IBM model 4 has been used because it has increased sophistication of model 3, so it sums over high probability alignments, and it search for the most probable alignment [13].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 13,
      "context" : "Model training data presented as a set of 74067 sentences (3138881 token) before data cleaning and 67575 sentences after data cleaning [14].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 14,
      "context" : "For tuning a collection of translated sentences from Tatoeba were used consists of 13000 sentence (65300 token) [15].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "Finally for model evaluation a parallel corpus of Ubuntu localization files consist of 6000 sentences (24400 token) were used to get the BLEU result and assess the model [15].",
      "startOffset" : 170,
      "endOffset" : 174
    } ],
    "year" : 2015,
    "abstractText" : "The interest in statistical machine translation systems increases currently due to political and social events in the world. A proposed Statistical Machine Translation (SMT) based model that can be used to translate a sentence from the source Language (English) to the target language (Arabic) automatically through efficiently incorporating different statistical and Natural Language Processing (NLP) models such as language model, alignment model, phrase based model, reordering model, and translation model. These models are combined to enhance the performance of statistical machine translation (SMT). Many implementation tools have been used in this work such as Moses, Gizaa++, IRSTLM, KenLM, and BLEU. Based on the implementation, evaluation of this model, and comparing the generated translation with other implemented machine translation systems like Google Translate, it was proved that this proposed model has enhanced the results of the statistical machine translation, and forms a reliable and efficient model in this field of research.",
    "creator" : "Adobe Acrobat Pro 11.0.7"
  }
}