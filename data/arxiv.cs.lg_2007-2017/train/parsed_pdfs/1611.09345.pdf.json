{
  "name" : "1611.09345.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unifying Multi-Domain Multi-Task Learning: Tensor and Neural Network Perspectives",
    "authors" : [ "Yongxin Yang", "Timothy M. Hospedales" ],
    "emails" : [ "yongxin.yang@qmul.ac.uk,", "t.hospedales@qmul.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The multi-domain setting arises when there is data about a task in several different but related domains. For example in visual recognition of an object when viewed with different camera types. Multi-domain learning (MDL) models [Dredze et al., 2010, Daumé III, 2007, Yang and Hospedales, 2015] aim to learn a cross-domain parameter sharing strategy that reflects the domains’ similarities and differences. Such selective parameter sharing aims to be robust to the differences in statistics across domains, while exploiting data from multiple domains to improve performance compared to learning each domain separately.\nIn this chapter we derive a general framework that encompasses MDL and MTL from both neural network and tensor-factorisation perspectives. Many classic and recent MDL/MTL algorithms can be understood by the assumptions about the cross domain/task sharing structure encoded in their designs. E.g., the assumption that each task/domain’s model is a linear combination of a global and a task-specific parameter vector [Evgeniou and Pontil, 2004, Daumé III, 2007]. Our framework includes these as special cases corresponding to specific settings of a semantic descriptor vector parametrising tasks/domains [Yang and Hospedales, 2015]. This vector can be used to recover existing models from our framework, but more generally it allows one to relax the often implicit assumption that domains are atomic/categorical entities, and exploit available metadata about tasks/domains to guide sharing structure for better MDL/MTL [Yang and Hospedales, 2015, Yang and Hospedales, 2016b]. For example, in surveillance video analysis, exploiting the knowledge of the time of day and day of week corresponding to each domain for better MDL. Finally, the idea of a semantic task/domain descriptor, allows our framework to go beyond the conventional MDL/MTL setting, and address both zero-shot learning [Yang and Hospedales, 2015] and zero-shot domain adaptation [Yang and Hospedales, 2015, Yang and Hospedales, 2016b] – where a model can be deployed for a new task/domain without any training data, solely by specifying the task/domain’s semantic descriptor metadata.\nMulti-Domain versus Multi-Task Learning The difference between domains and tasks can be subtle, and some multi-domain learning problems can be addressed by methods proposed for multi-task learning and vice-versa. However, to better understand this work, it is useful to distinguish them clearly. Domains refer to multiple datasets addressing the same task, but with differing statistical bias. For example camera type for object recognition; time of day or year for surveillance video analysis; or more subtle human biases in data collection [Torralba and Efros, 2011]. Tasks, on the other hand would refer to different object categories to recognise. In other words, a task change affects the output label-space of a supervised learning problem, while a domain change does not.\nA classic benchmark with multiple domains is the Office dataset [Saenko et al., 2010]. It contains images of the same set of categories (e.g., mug, laptop, keyboard) from three data sources (Amazon website, webcam, and DSLR). In this context, multi-task learning could improve performance by sharing information about how to recognise keyboard and laptop; while multi-domain learning could improve performance by sharing knowledge about how to recognise those classes in Amazon product versus webcam images. Some problems can be interpreted as either setting. E.g., in the School dataset [Argyriou et al., 2008] the goal is to predict students’ exam scores based on their characteristics. This dataset is widely used to evaluate MTL algorithms, where students from different schools are grouped into different tasks. However, one can argue that school groupings are better interpreted as domains than tasks.\nAs a rule of thumb, multi-domain learning problems occur when a model from domain A could be directly applied to domain B albeit with reduced performance; while multi-task learning problems occur where a model for task A can not meaningfully be applied to task B because their label-spaces are fundamentally different. In some problems, the multi-domain and multi-task setting occur simultaneously. E.g., in the Office dataset there are both multiple camera types, and multiple objects to recognise. Few existing methods can deal with this setting. MTL methods break a multi-class problem into multiple 1-v-all tasks and share information across tasks [Argyriou et al., 2008, Kumar and Daumé III, 2012], while MDL methods typically deal with a single-output problem in multiple domains [Daumé III, 2007]. Our higher order generalisation addresses simultaneous multi-domain multi-task learning as required by problems such as the one posed by Office.\nar X\niv :1\n61 1.\n09 34\n5v 1\n[ cs\n.L G\n] 2\n8 N\nov 2\n01 6\nRelation to Domain Adaptation and Domain Generalisation Dataset bias/domain-shift means that models trained on one domain often have weak performance when deployed in another domain. The community has proposed two different approaches to alleviate this: (i) Domain adaptation (DA): calibrating a pre-trained model to a target domain using a limited amount of labelled data – supervised DA [Saenko et al., 2010], or unlabelled data only – unsupervised DA [Gong et al., 2012], or both –semi-supervised DA [Li et al., 2014]. (ii) Domain generalisation (DG): to train a model that can is insensitive to domain bias, e.g., learning domain invariant features [Muandet et al., 2013].\nThe objective of multi-domain learning is different from the mentioned domain adaptation and domain generalisation. MDL can be seen as a bi-directional generalisation of DA with each domain benefiting the others, so that all domains have maximal performance; rather than solely transferring knowledge from source→ target domain as in DA. In its conventional form MDL does not overlap with DG, because it aims to improve the performance on the set of given domains, rather than address a held out domain. However, our zero-shot domain adaptation extension of MDL, relates to DG insofar as aiming to address a held-out domain. The difference is that we require a semantic descriptor for the held out domain, while DG does not. However where such a descriptor exists, ZSDA is expected to outperform DG."
    }, {
      "heading" : "2 Methodology – Single Output Models",
      "text" : "We start from the linear model assumption made by most popular MTL/MDL methods [Argyriou et al., 2008, Ji and Ye, 2009] – i.e., that a domain/task corresponds to a univariate linear regression or binary classification problem. The case where a domain/task needs more than that (e.g., multi-class problem) will be discussed in Section 3. We also assume the domains/tasks are homogeneous, i.e., different domains/tasks have model representations and instance feature vectors of the same size."
    }, {
      "heading" : "2.1 Formulation: Vector-Matrix Inner Product",
      "text" : "Assume we have M domains (tasks), and the ith domain has Ni instances. We denote the D-dimensional feature vector of jth instance in ith domain (task) and its associated B-dimensional semantic descriptor12 by the pair {{x(i)j ,z (i)}Nij=1} M i=1 and the corresponding label {{y(i)j } Ni j=1} M i=1. All the weight vectors w\n(i) for each domain (task) i can be stacked into a single weight matrix W̃ . Without loss of generality, we to minimise the empirical risk for all domains (tasks),\nargmin W̃\n1 M\nM\n∑ i=1 ( 1 Ni Ni ∑ j=1 ` ( ŷ(i)j ,y (i) j )) . (1)\nHere `(ŷ,y) is a loss function depending on the predicted ŷ and true label y. For the ith domain/task, the linear model w(i) makes predictions as:\nŷ(i)j = x (i) j ·w (i) = x(i)j T w(i). (2)\nNow we introduce a key idea in our work: rather than being learned directly, each task/domain’s model w(i) is generated by a linear function f (·) of its descriptor, termed the weight generating function,\nw(i) = f (z(i)) =Wz(i). (3)\nThat is, the linear model for the ith domain/task is produced by its B-dimensional semantic descriptor hitting a D×B matrix W . If all semantic descriptors are stacked (as columns) into matrix Z, we have\nW̃ = f (Z) =WZ (4)\nRather than learning the per-domain models W̃ independently as in Eq. 1, we now learn the weight generating function parametrised by matrix W instead. By substituting Eq. 3 into Eq. 2, we can re-write the prediction for the ith task/domain into a bilinear form,\nŷ(i)j = x (i) j\nT Wz(i). (5)\nThis general formulation encompasses many MTL/MDL methods. As we will see, the two key axes of design space on which existing MTL/MDL algorithms differ are the encoding of descriptors z and decomposition or regularisation of matrix W . We will discuss these in the following two sections."
    }, {
      "heading" : "2.2 Semantic Descriptor Design",
      "text" : "One-hot encoding z In the simplest scenario z is a one-hot encoding vector that indexes domains (Fig. 1). The model generation function f (z(i)) then just selects one column from the matrix W . For example, z(1) = [1,0,0]T , z(2) = [0,1,0]T , z(3) = [0,0,1]T if there are M = 3 domains/tasks. In this case, the length of the descriptor and the number of unique domains (tasks) are equal B = M, and the stack of all zi vectors (denoted Z = [z(1),z(2), . . . ]) is an M×M identity matrix. Learning the ‘weight generating function’ is then equivalent to independently learning per-domain weights.\nOne-hot encoding z with a constant A drawback of one-hot encoding is that the zi are orthogonal to each other, which suggests that all domains/tasks are independent – there is no cross domain/task information sharing. To encode an expected sharing structure of an underlying commonality across all domains/tasks, an alternative approach to constructing z is to append a constant term after the one-hot encoding. For the case of M = 3, we might have z(1) = [1,0,0,1]T , z(2) = [0,1,0,1]T , z(3) = [0,0,1,1]T . Fig. 1 shows the resulting B×M\n1Note that, in any multi-domain or multi-task learning problem, all instances are at least implicitly associated with a semantic descriptor indicating their domain (task).\n2All vectors (feature x, weights w, and domain descriptor z) are by default column vectors.\nZ =  Domain-1 Domain-2 Domain-3\nIndex-1 1 0 0 Index-2 0 1 0 Index-3 0 0 1\n Z = \nDomain-1 Domain-2 Domain-3 Index-1 1 0 0 Index-2 0 1 0 Index-3 0 0 1 Shared 1 1 1 \nFigure 1: Domain descriptor for categorical/atomic domains. One-hot encoding (left), and one-hot with constant encoding (right).\nZ =  Domain-1 Domain-2 Domain-3 Domain-4\nA-1-B-1 1 0 0 0 A-1-B-2 0 1 0 0 A-2-B-1 0 0 1 0 A-2-B-2 0 0 0 1\n Z = \nDomain-1 Domain-2 Domain-3 Domain-4 A-1 1 1 0 0 A-2 0 0 1 1 B-1 1 0 1 0 B-2 0 1 0 1 \nFigure 2: Example domain descriptor for domains with multiple factors. One-hot encoding (left). Distributed encoding (right).\nmatrix Z (in this case, B = M+1). The prediction of task (domain) i is given as ŷ(i) = x(i) T Wz(i) = x(i) T (w(i)+w(4)), i.e., the sum of a task/domain specific and a globally shared prediction. Learning the weight generator corresponds to training both the local and shared predictors. This approach is implicitly used by some classic MDL/MTL algorithms [Evgeniou and Pontil, 2004, Daumé III, 2007].\nDistributed encoding z In most studies of MDL/MTL, domain or task is assumed to be an atomic category which can be effectively encoded by the above indexing approaches. However more structured domain/task-metadata is often available, such that a domain (task) is indexed by multiple factors (e.g., time: day/night, and date: weekday/weekend, for batches of video surveillance data). Suppose two categorical variables (A,B) describe a domain, and each of them has two states (1,2), then at most four distinct domains exist. Fig. 2(left) illustrates the semantic descriptors for 1-hot encoding. However this encoding does not exploit the sharing cues encoded in the metadata (e.g., in the surveillance example that day-weekday should be more similar to day-weekend and night-weekday than to night-weekend). Thus we propose to use a distributed encoding of the task/domain descriptor (Fig 2(right)). Now prediction weights are given by a linear combination of W ’s columns given by the descriptor, and learning the weight generating function means learning weights for each factor (e.g., day, night, weekday, weekend). We will demonstrate that the ability to exploit structured domain/task descriptors where available, improves information sharing compared to existing MTL/MDL methods in later experiments."
    }, {
      "heading" : "2.3 Unification of Existing Algorithms",
      "text" : "The key intuitions of various MDL/MTL methods are encoded in constraints on W in Eq. 3 and/or encodings of descriptors z. Besides enforcing local and shared components [Evgeniou and Pontil, 2004, Daumé III, 2007] (which we interpret as a one-hot+constant descriptor) many studies achieve information sharing via enforcing low-rank structure on W̃ (the D×T stack of weight vectors for each task). Popular choices for modelling W̃ are: (i) add regularisation that encourages W̃ to be a low-rank matrix (ii) explicit low rank factorisation W̃ = P̃Q̃, and optionally placing some constraint(s) on P̃ and/or Q̃. We assume our weight generating function, the D×B matrix W , is replaced by the dot-product of two factor matrices P and Q (D×K and K×B respectively). Thus Eq. 3 becomes\nw(i) = f (z(i)) = PQz(i) (6)\nThat is, we generate specific weights for prediction by combining the task/domain descriptor z(i) with matrices P and Q; and learning corresponds to fitting P and Q.\nA variety of existing algorithms3 are then special cases of our general framework. We illustrate this via MDL/MTL with M = 3 domains/tasks. Observe that RMTL [Evgeniou and Pontil, 2004], FEDA [Daumé III, 2007], MTFL [Argyriou et al., 2008], TNMTL [Ji and Ye, 2009], and GO-MTL [Kumar and Daumé III, 2012] correspond to asserting specific settings of Z, P and Q as shown in Tab 1."
    }, {
      "heading" : "2.4 A Two-sided Network View",
      "text" : "We now provide an alternative neural network view of the class of methods introduced above. By substituting Eq. 6 into Eq. 2, we see the prediction is given by\nŷ(i)j = x (i) j\nT PQz(i) (7)\nA mathematically equivalent neural network model is illustrated in Fig. 3. The introduction of this NN leads to a better understanding of our framework and the set of methods it encompasses. The left-hand side can be understood as a global representation learning network – xT P, and the right-hand side can be seen as a network that generates a model (the weight vector zT QT ) for the task/domain encoded by z. This interpretation allows existing MTL/MDL models to be implemented as neural networks – and conveniently optimised by standard neural network libraries – by setting inputs z appropriately, and activating appropriate weight regularisation (Table 1). However, going beyond existing methods, we exploit the semantic descriptor as an input, which allows information sharing to be guided by domain metadata [Yang and Hospedales, 2015] where available. Moreover, as a NN, both sides can go arbitrarily deeper [Yang and Hospedales, 2016a]. E.g., the representation learning network can be a full-sized CNN that extracts features from raw images and everything can be trained end-to-end with back propagation.\n3RMTL: Regularized Multi–Task Learning, FEDA: Frustratingly Easy Domain Adaptation, MTFL: Multi–Task Feature Learning, TNMTL: Trace-Norm Multi– Task Learning, and GO-MTL: Grouping and Overlap for Multi–Task Learning"
    }, {
      "heading" : "2.5 Application to Zero-Shot Learning",
      "text" : ""
    }, {
      "heading" : "2.5.1 Zero-Shot Recognition",
      "text" : "Zero-Shot Learning (ZSL) aims to eliminate the need for training data for a particular task. It has been widely studied in areas such as character [Larochelle et al., 2008] and object recognition [Lampert et al., 2009, Socher et al., 2013]. Typically for ZSL, the label space of training and test data are disjoint, so no data has been seen for test-time categories. Instead, test-time classifiers are constructed given some mid-level information. Most existing ZSL methods follow data flows that can be illustrated as either: X → Z→Y [Palatucci et al., 2009] or Z ++X // Y [Larochelle et al., 2008, Frome et al., 2013] where where Z is some “semantic descriptor”, e.g., attributes [Lampert et al., 2009] or semantic word vectors [Socher et al., 2013]. In our framework ZSL can be achieved via the latter pipeline, implemented by the network in Fig. 3 [Yang and Hospedales, 2015]. By presenting each novel semantic vector z(t) (assuming testing category are indexed by t) in turn along with novel category instance x∗. Zero-shot recognition for x∗ then is given by: t̂ = argmaxt x T ∗ PQz\n(t). In this chapter we focus our experiments on zero-shot domain adaptation (parametrised domains), the interested reader can see [Yang and Hospedales, 2015] for experiments applying our framework to zero-shot recognition (parametrised tasks)."
    }, {
      "heading" : "2.5.2 Zero-Shot Domain Adaptation",
      "text" : "Going beyond conventional ZSL, we generalise the notion of zero-shot learning of tasks to zero-shot learning of domains. In this context, zero-shot means no training data has been seen for the target domain prior to testing. The challenge is to construct a good model for a novel test domain based solely on its semantic descriptor [Yang and Hospedales, 2015, Yang and Hospedales, 2016b]. We denote this problem setting as Zero-Shot Domain Adaptation4 (ZSDA)\nZSDA becomes possible with a distributed rather than one-hot encoded domain descriptor, as in practice only a subset of domains is necessary to effectively learn Q. Thus a model w(∗) suitable for an unseen domain can be constructed without any training data – by applying its semantic descriptor z(∗) to the model generation matrix Q: w(∗) = Qz(∗). The generated domain-specific model – w(∗) – is then then used to make predictions for the re-represented input: xT∗ Pw (∗).\n4Note that despite the title, [Blitzer et al., 2009] actually considers unsupervised domain adaptation without target domain labels, but with target domain data."
    }, {
      "heading" : "3 Methodology – Multiple Output Models",
      "text" : "Thus far, the final output of each model is a scalar (single output). However for some practical applications, multiple outputs are desirable or required. For example, assume that we have M = 2 handwriting digit datasets (domains): MNIST and USPS. For any MNIST or USPS image, a D-dimensional feature vector is extracted. The task is to classify the image from 0 to 9 and thus we have D×C (C = 10) model parameters for each dataset. Therefore, the full model for all digits and datasets should contain D×C×M parameters. We denote this setting of multiple domains, each of which has multiple tasks, as multi-domain-multi-task learning. In some recent literature [Romera-Paredes et al., 2013] a similar setting is named multi-linear multi-task learning."
    }, {
      "heading" : "3.1 Formulation: Vector-Tensor Inner Product",
      "text" : "The key idea in the previous section was to generate a model vector via a descriptor vector hitting a matrix (Eq. 3). To adapt this idea for this new setting, we propose\nW (i) = f (z(i)) = W ×3 z(i) (8) where ×n indicates the n-mode product of a tensor and vector (this is also referred to as tensor contraction in some studies as it is a generalisation of inner product for vectors and/or matrices). The generated model is now a weight matrix W (i) rather than a vector w(i). The weight generating function is now parametrised by a third-order tensor W of size D×C×B, and it synthesises the model matrix for the ith domain by hitting the tensor with its B-dimensional semantic descriptor z(i). This is a natural extension: if the required model is a vector (single output), the weight generating function is a matrix (second-order tensor) hits the semantic descriptor z (Eq. 3); when the required model is a matrix (multiple outputs), the weight generating function is then z hits a third-order tensor.\nGiven one-hot encoding descriptors z(1) = [1,0]T and z(2) = [0,1]T indicating MNIST and USPS respectively. Eq. 8 would just slice an appropriate matrix from the tensor W . However alternative and more powerful distributed encodings of z(i) are also applicable. The model prediction from Eq. 5 is then generalised as\nŷ(i)j = W ×1 x (i) j ×3 z (i) (9)\nwhere ŷ(i)j is now a C-dimensional vector instead of a scalar as in Eq. 5. Nevertheless, this method does not provide information sharing in the case of conventional categorical (1-hot encoded) domains. For this we turn to tensor factorisation next."
    }, {
      "heading" : "3.2 Tensor (De)composition",
      "text" : "Recall that the key intuition of many classic (matrix-based) multi-task learning methods is to exploit the information sharing induced by the row-rank factorisation W̃ = P̃Q̃. I.e., composing the weight matrix for all tasks W̃ from factor matrices P̃ and Q̃. For MDL with multiple outputs, we aim to extend this idea to the factorisation of the weight tensor W . In contrast to the case with matrices, there are multiple approaches to factorising tensors, including CP [Hitchcock, 1927], Tucker [Tucker, 1966], and Tensor-Train [Oseledets, 2011] Decompositions."
    }, {
      "heading" : "3.2.1 CP decomposition",
      "text" : "For a third-order tensor W of size D×C×B, the rank-K CP decomposition is:\nWd,c,b = K\n∑ k=1\nU (D)k,d U (C) k,c U (B) k,b (10)\nW = K\n∑ k=1\nU (D)k,· U (C) k,· U (B) k,· (11)\nwhere is outer product. The factor matrices U (D), U (C), and U (B) are of respective size K×D, K×C, and K×B. Given a data point x and its corresponding descriptor z5, Eq. 9 will produce a C-dimensional vector y (e.g., C = 10 the scores of 10 digits for the MNIST/USPS example). By substituting Eq. 11 into Eq. 9 and some reorganising, we obtain\ny =U (C) T ((U (D)x)◦ (U (B)z)) (12)\nwhere ◦ is the element-wise product. It also can be written as,\ny =U (C) T diag(U (B)z)U (D)x (13)\nfrom which we obtain a specific form of the weight generating function in Eq. 8, which is motived by CP decomposition:\nW (i) = f (z(i)) = W ×3 z(i) =U (D) T diag(U (B)z)U (C) (14)\nIt is worth mentioning that this formulation has been used in the context of gated neural networks [Sigaud et al., 2015], such as Gated Autoencoders [Droniou and Sigaud, 2013]. However, [Droniou and Sigaud, 2013] uses the technique to model the relationship between two inputs (images), while we exploit it for knowledge sharing in multi-task/multi-domain learning.\n5We omit the upper- and lower- scripts for clarity."
    }, {
      "heading" : "3.2.2 Tucker decomposition",
      "text" : "Given the same sized tensor W , Tucker decomposition outputs a core tensor S of size KD×KC ×KB, and 3 matrices U (D) of size KD×D, U (C) of size KC×C, and U (B) of size KB×B, such that,\nWd,c,b = KD\n∑ kD=1\nKC\n∑ kC=1\nKB\n∑ kB=1 SkD,kC ,kBU (D) kD,d U (C)kC ,cU (B) kB,b\n(15)\nW = S×1U (D)×2U (C)×3U (B) (16)\nSubstituting Eq. 16 into Eq. 9, we get the prediction for instance x in domain/task z\ny = ((U (D)x)⊗ (U (B)z))S T(2)U (C) (17)\nwhere ⊗ is Kronecker product. S(2) is the mode-2 unfolding of S which is a KC×KDKB matrix, and its transpose S T(2) is a matrix of size KDKB×KC.\nThis formulation was used by studies of Gated Restricted Boltzmann Machines (GRBM) [Memisevic and Hinton, 2007] for similar image-transformation purposes as [Droniou and Sigaud, 2013]. The weight generating function (Eq. 8) for Tucker decomposition is\nW (i) = f (z(i)) = W ×3 z(i) = S×1U (D)×2U (C)×3(U (B)z(i)). (18)"
    }, {
      "heading" : "3.2.3 TT decomposition",
      "text" : "Given the same sized tensor W , Tensor-Train (TT) decomposition produces two matrices U (D) of size D×KD and U (B) of size KB×B and a third-order tensor S of size KD×C×KB, so that\nWd,c,b = KD\n∑ kD=1\nKB\n∑ kB=1 U (D)d,kDSkD,c,kBU (B) kB,b , (19)\nW = S×1U (D) T ×3U (B). (20)\nSubstituting Eq. 20 into Eq.9, we obtain the MDL/MTL prediction\ny = (U (D) T\nx)⊗ (U (B)z))S T(2) (21)\nwhere S(2) is the mode-2 unfolding of S which is a C×KDKB matrix, and its transpose S T(2) is a matrix of size KDKB×C. The weight generating function (Eq. 8) for Tensor Train decomposition is\nW (i) = f (z(i)) = W ×3 z(i) = S×1U (D) T ×3(U (B)z(i)). (22)"
    }, {
      "heading" : "3.3 Gated Neural Network Architectures",
      "text" : "We previously showed the connection between matrix factorisation for single-output models, and a two-sided neural network in Section 2.4. We will next draw the link between tensor factorisation and gated neural network [Sigaud et al., 2015] architectures. First we recap the factors used by different tensor (de)composition methods in Table 2.\nTo make the connection to neural networks, we need to introduce two new layers:\nHadamard Product Layer Takes as input two equal-length vectors u and v and outputs [u1v1,u2v2, · · · ,uKvK ]. It is a deterministic layer that does Hadamard (element-wise) product, where the input size is K +K and output size is K.\nKronecker Product Layer Takes as input two arbitrary-length vectors u and v and outputs [u1v1,u2v1, · · · ,uKu v1,u1v2, · · · ,uKu vKv ]. It is a deterministic layer that takes input of size Ku +Kv and returns the size KuKv Kronecker product.\nFig. 4 illustrates the approaches to multi-domain learning in terms of NNs. Single domain learning learning of M domains requires M single-layer NNs, each with a D×C weight matrix (Fig. 4(a)). Considering this set of weight matrices as the corresponding D×C×M tensor, we can use the introduced layers to define gated networks (Figs. 4(b)-(d)) that model low-rank versions of this tensor with the corresponding tensor-factorisation assumptions in Eq. 12, 17, and 21 and summarised in Tab 2. Rather than maintaining a separate NN for each domain as in Fig. 4(a), the networks in Fig. 4(b)-(d) maintain a single NN for all domains. The domain of each instance is signalled to the network via its corresponding descriptor, which the right hand side of the network uses to synthesise the recognition weights accordingly.\nWe note that we can further unify all three designs, as well as the single-output model proposed in Section 2.4, by casting them as special cases of the Tucker Network as shown in Table 3. Thus we can understand all these factorisation-based approaches by their connection to the idea of breaking down the stacked model parameters (matrix W or tensor W ) into a smaller number of parameters composing a domain-specific (U (B)), task-specific (U (C)) and shared components (U (D)). It is important to note however that, despite\nour model’s factorised representation assumption in common with tensor decomposition, the way to train our model is not by training a set of models and decomposing them – in fact matrix/tensor decomposition is not used at all. Rather a single Tucker network of Fig. 4(d) is trained end-to-end with backpropagation to minimise the multi-domain/task loss. The network architecture enforces that backpropagation trains the individual factors (Tab 2) such that their corresponding tensor composition solves the multi-domain-multitask problem. In summary, our framework can be seen as ‘discriminatively trained’ tensor factorisation, or as a gated neural network, where the NN’s weights are dynamically parametrised by a second input, the descriptor z."
    }, {
      "heading" : "4 Experiments",
      "text" : "We explore three sets of experiments on object recognition (Section 4.1), surveillance image analysis (Section 4.2) and person recognition/softbiometrics (Section 4.3). The first recognition experiment follows the conventional setting of domains/tasks as atomic entities, and the latter experiments explore the potential benefits of informative domain descriptors, including zero-shot domain adaptation.\nImplementation We implement our framework with TensorFlow [Abadi et al., 2015], taking the neural network interpretation of each method, thus allowing easy optimisation with SGD-based backpropagation. We use hinge loss for the binary classification problems and (categorical) cross-entropy loss for the multi-class classification problems."
    }, {
      "heading" : "4.1 Multi-domain Multi-task Object Recognition",
      "text" : "In this section we assume conventional atomic domains (so domain descriptors are simply indicators rather than distributed codes), but explore a multi-domain multi-task (MDMTL) setting. Thus there is a multi-class problem within each domain, and our method\n(Section 3) exploits information sharing across both domains and tasks. To deal with multi-class recognition within each domain, it generalises existing vector-valued MTL/MDL methods, and implements a matrix-valued weight generating function parametrised by a low-rank tensor (Fig. 4).\nDatasets We first evaluate the multi-domain multi-task setting using the well-known office dataset [Saenko et al., 2010]. Office includes three domains (data sources): Amazon: images downloaded from Amazon, DSLR high-quality images captured by digital camera, webcam low-quality images captured by webcam. For every domain, there are multiple classes of objects to recognise, e.g., keyboard, mug, headphones. In addition to the original Office dataset, add a 4th domain: Caltech-256 [Griffin et al., 2007], as suggested by [Gong et al., 2012]. Thus we evaluate recognising 10 classes in common the four domains. See Fig. 5 for an illustration. The feature is the 800-dimension SURF feature [Bay et al., 2006]. As suggested by [Gong et al., 2012], we pre-process the data by normalising the sum of each instance’s feature vector to one then applying a z-score function.\nSettings We compare the three proposed method variants: CP, Tucker, and TT-Networks with two baselines. SDL: training each domain independently and Aggregation: ignoring domains and training an aggregate model for all data. For these two baselines, we use a vanilla feed-forward neural network without hidden layers thus there are no hyper-parameters to tune. For our methods, the tensor rank(s), i.e., K for CP-Network, (KD, KC, KB) for Tucker-Network, and (KD, KB) for TT-network are chosen by 10-fold cross validation. The grids of KD, KC, and KB are respectively [16,64,256], [2,4,8], and [2,4]. The multi-class recognition error rate at 9 increasing training-testing-ratios (10%,20% . . .90%) is computed, and for each training-testing-ratio, we repeat the experiment 10 times with random splits.\nResults and Analysis The result is shown in Fig. 6. We can see that the proposed methods perform well compared to SDL. When the training data is extremely small, Aggregation is a reasonably good choice as the benefit of more data outweighs the drawback of mixing domains. However, the existence of domain bias eventually prevents a single model from working well for all domains. Our proposed methods can produce different models for the various domains so they generally outperform the baselines. Tucker-and TT-Network are better than CP-Network because of their greater flexibility on choosing more than one tensor rank. However as a drawback, this also introduces more hyper-parameters to tune."
    }, {
      "heading" : "4.2 Surveillance Image Classification",
      "text" : "In surveillance video analysis, there are many covariates such as season, workday/holiday and day/night. Each of these affects the distribution of image features, and thus introduces domain shift. Collecting the potentially years of training data required to train a single general model is both expensive and suboptimal (due to ignoring domain shift, and treating all data as a single domain). Thus in this section we explore the potential for multi-domain learning with distributed domain descriptors (Section 2) to improve performance by modelling the factorial structure in the domain shift. Furthermore, we demonstrate the potential of ZSDA to adapt a system to apply in a new set of conditions for which no training data exists.\nData We consider the surveillance image classification task proposed by [Hoffman et al., 2014]. This is a binary classification of each frame in a 12-day surveillance stream as being empty or containing cars. Originally, [Hoffman et al., 2014] investigated continuous domains (which can be seen as a 1-dimensional domain descriptor containing time-stamp). To explore a richer domain descriptor, we use a slightly different definition of domains, considering instead weekday/weekend and day/night as domain factors, generating 2× 2 = 4 distinct domains, each encoded by a 2-of-4 binary domain descriptor. Figure 7(top) illustrates the more obvious domain factor: day/night. This domain-shift induces a larger image change than the task-relevant presence or absence of a car.\nSettings We use the 512 dimensional GIST feature for each frame provided by [Hoffman et al., 2014]. We perform two experiments: Multi-domain learning, and zero-shot domain adaptation. For MDL, we split all domains’ data into half training and half testing, and repeat for 10 random splits. We use our single-output network (Fig. 3, Tab 3 bottom row) with a distributed domain descriptor for two categories with two states (i.e., same descriptor as Fig. 2, right). The baselines are: (i) SDL: train an independent model for each domain (ii) Aggregation: to train a single model covering all domains (ii) Multi-Domain I: a multi-domain model with low-rank factorisation of W̃ and one-hot encoding of domain descriptor (in this case, Z is an identity matrix thus W̃ = WZ = W – this roughly corresponds to our reimplementation of [Kumar and Daumé III, 2012]), and (iv) Multi-Domain II: a factorised multi-domain model with one-hot + constant term encoding (this is in fact the combination the sharing structure and factorisation proposed in [Evgeniou and Pontil, 2004] and [Kumar and Daumé III, 2012] respectively).\nFor ZSDA, we do leave-one-domain-out cross-validation: holding out one of the four domains for testing, and using the observed three domains’ data for training. Although the train/test splits are not random, we still repeat the procedure 10 times to reduce randomness of the SGD optimisation. Our model is constructed on the fly for the held-out domain based on its semantic descriptor. As\na baseline, we train an aggregated model from all observed domains’ data, and apply it directly to the held-out domain (denoted as Direct). We set our rank hyper parameter via the heuristic K = Dlog(D) . We evaluate the the mean and standard deviation of error rate. Results and Analysis The results shown in Table 4.2 demonstrate that our proposed method outperforms alternatives in both MDL and ZSDA settings. For MDL we see that training a per-domain model and ignoring domains altogether perform similarly (SDL vs Aggregation). By introducing more sharing structure, e.g., Multi-Domain I is built with low-rank assumption, and Multi-Domain II further assumes that there is a globally shared factor, the multi-domain models clearly improve performance. Finally our full method performs notably better than the others because it can benefits from both low-rank modelling and also exploiting the structured information in the distributed encoding of domain semantic descriptor.\nIn ZSDA, our proposed method also clearly outperforms the baseline of directly training on all the source domains. What information is our model able to exploit to achieve this? One cue is that various directions including right turn are common on weekends and weekdays are primarily going straight (illustrated in Figure 7(below) by way of an activity map). This can, e.g., be learned from the weekend-day domain, and transferred to the held-out weekend-night domain because the domain factors inform us that those two domains have the weekend factor in common."
    }, {
      "heading" : "4.3 Gait-based Soft-Biometrics and Recognition",
      "text" : "Gait-based person and soft biometric recognition are desirable capabilities due to not requiring subject cooperation [Zheng et al., 2011]. However they are challenging especially where there are multiple covariates such as viewing angle and accessory status (e.g., object carrying). Again training a model for every covariate combination is infeasible, and conventional domain adaptation is not scalable as the number of resulting domains grows exponentially with independent domain factors. In contrast, zero-shot domain adaptation could facilitate deploying a camera with a calibration step to specify covariates such as view-angle, but no data collection or re-training. Data We investigate applying our framework to this setting using the CASIA gait analysis dataset [Zheng et al., 2011] with data from 124 individuals under 11 viewing angles. Each person has three situations: normal (‘nm’), wearing overcoat (‘cl’) and carrying a bag (‘bg’). This naturally forms 3× 11 = 33 domains. We extract Gait Energy Image (GEI) features, followed by PCA reduction to 300 dimensions. Settings We consider two gait analysis problems: (i) Soft-biometrics: Female/Male classification and (ii) Person verification/matching. For matching each image pair xi and x j, generates a pairwise feature vector by xi j = |xi− x j|. The objective is to learn a binary verifi-\ncation classifier on xi j to predict if two images are the same person or not. All experiment settings (baseline methods, training/testing splits, experiments repeats, and the choice of hyper-parameter) are the same as in Section 4.2, except that for the verification problem we build a balanced (training and testing) set of positive/negative pairs by down-sampling negative pairs.\nResults and Analysis Figure 8 illustrates the nature of the domain factors here, where the cross-domain variability is again large compared to the cross-class variability. Our framework uniquely models the fact that each domain factor (e.g., view angle and accessory status) can occur independently. The results shown in Tables 5 and 6 demonstrate the same conclusions – that explicitly modelling MDL structure improves performance (Multi-domain I and II improve on SDL and Aggregation), with our most general method performing best overall."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this chapter, we discussed multi-domain learning, a bi-directional generalisation of domain adaptation and zero-shot domain adaptation, an alternative to domain-generalisation approaches. We introduced a semantic domain/task descriptor to unify various existing multi-task/multi-domain algorithms within a single matrix factorisation framework. To go beyond the single output problems considered by prior methods, we generalised this framework to tensor factorisation, which allows knowledge sharing for methods parametrised by matrices rather than vectors. This allows multi-domain learning for multi-output problems or simultaneous multi-task-multi-domain learning. All these approaches turn out to have equivalent interpretations as neural networks, which allow easy implementation and optimisation with existing toolboxes. Promising lines of future enquiry include extending this framework for end-to-end learning in convolutional neural networks [Yang and Hospedales, 2016a], tensor rank-based regularisation [Yang and Hospedales, 2016c], and applying these ideas to solve practical computer vision problems."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Multi-domain learning aims to benefit from simultaneously learning across several different but related domains. In this chapter, we propose a single framework that unifies multi-domain learning (MDL) and the related but better studied area of multi-task learning (MTL). By exploiting the concept of a semantic descriptor we show how our framework encompasses various classic and recent MDL/MTL algorithms as special cases with different semantic descriptor encodings. As a second contribution, we present a higher order generalisation of this framework, capable of simultaneous multi-task-multi-domain learning. This generalisation has two mathematically equivalent views in multi-linear algebra and gated neural networks respectively. Moreover, by exploiting the semantic descriptor, it provides neural networks the capability of zero-shot learning (ZSL), where a classifier is generated for an unseen class without any training data; as well as zero-shot domain adaptation (ZSDA), where a model is generated for an unseen domain without any training data. In practice, this framework provides a powerful yet easy to implement method that can be flexibly applied to MTL, MDL, ZSL and ZSDA.",
    "creator" : "LaTeX with hyperref package"
  }
}