{
  "name" : "1506.04908.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1. INTRODUCTION",
      "text" : "Using information from a supervised problem like regression or classification, supervised clustering seeks to discover hidden clusters of features, tasks or samples that both help inference, and provide additional structural insights on the data.\nIn the classical multi-task setting, clustering predictors of related tasks often helps prediction. In computer vision for instance, classifiers associated with different species of cats should be quite similar, but well separated from classifiers associated with cars, and incorporating this structural information at the training stage should improve performance. This problem has been well studied in the multi-task learning literature [1, 2, 3].\nSimilarly, when there exists some groups of highly correlated features, reducing dimensionality by assigning the same weights to some groups of features can be beneficial both in terms of prediction and interpretation [4]. This often occurs in text classification, where it is natural to group together words having the same meaning for a given task [5, 6].\nFinally, in some settings, it can be valuable to cluster sample points, with each cluster having its own distinct prediction function [7]. This last problem may have applications in the context of privacy learning, where the group information of an individual may not be revealed because of confidentiality issues.\nHere, we present a unified and flexible framework for supervised clustering over the “data cube” of either tasks, features or sample points (a representation introduced by [8]). We directly formulate supervised clustering as an optimization problem on the clustered predictors, where clustering is either a hard constraint or a soft regularization penalty, and losses are adapted to the application at hand (classification or regression, clustering tasks, features or sample points). We propose several optimization schemes to solve these problems efficiently. While the original optimization problem is non-convex, we show that the core nonconvexity is concentrated in a subproblem similar to k-means, which we solve using classical approximation techniques [9]. In the particular case of feature clustering for regression, the k-means steps are performed in dimension one, and can therefore be solved exactly by dynamic programming [10, 11]. Our formulation is then an explicit convex relaxation which can be solved efficiently using the conditional gradient algorithm [12, 13]. We describe experiments on both synthetic and real datasets involving large corpora of text where our method compares favorably with standard benchmarks.\nDate: June 17, 2015. Key words and phrases. Clustering, Multitask, Dimensionality Reduction, Multi Outpout. The two first authors contributed equally.\nar X\niv :1\n50 6.\n04 90\n8v 1\n[ cs\n.L G\n] 1\n6 Ju\nn 20\n15"
    }, {
      "heading" : "2. SUPERVISED CLUSTERING OF FEATURES, TASKS OR SAMPLES",
      "text" : "We write supervised clustering as a clustering problem penalized by a supervised learning loss. We let n be the number of training examples in the dataset, d the number of features (i.e. the ambient dimension) and K the number of tasks. For regression or binary classification K = 1, while for multi-classification K corresponds to the number of classes (using one-versus-all majority vote, training one binary classifier per class vs all others). For simplicity, we consider only square or logistic losses for linear predictors.\nOur main variable is a matrix of predictors with one column per task, writtenW = [w1, ...,wK ] ∈ Rd×K . We let Q be the desired number of clusters and m the number of items to cluster. When clustering tasks, m = K and we are grouping the predictors associated with each task, i.e. the columns of W . When clustering features, m = d and we are clustering the predictors associated with each feature, i.e. the rows of W . Finally, when clustering sample points, m = n and we introduce individual predictor vectors W (i) for each sample point i, which we cluster together to obtain Q distinct predictors associated with a partition of the points into Q groups. Hence, the term “predictors” either refers to columns of W , rows of W , or individual predictor vectorsW (i), depending on which dimension of the data cube (tasks, features or sample points) the clustering is performed. A summary of these settings is given in Table 1. In the following we use the generic notations U and V to designate predictors.\nA clustering can be seen as a partition C1 ∪ C2 ∪ . . . ∪ CQ = [1,m] of predictors, to which corresponds an assignment matrix Z ∈ {0, 1}m×Q such that Zij = 1 if item i is in cluster Cj . As in k-means, we define a matrix of centroids C = [c1, . . . , cQ], with each centroid cj equal to the mean of predictors in cluster j. This information is summarized in the matrix V = CZT , which we call matrix of individual centroids (MIC), whose columns vi are such that vi = cj if predictor i is in cluster Cj .\nGiven a supervised learning loss L(·) and a regularizing penalty Ω(·) on predictors, we formulate the supervised clustering problem as an optimization problem over the set of MIC matrix V . Enforcing the hard constraint that predictors are exactly confounded with centroids, we get the following hard supervised clustering problem (HSC)\nminimize L(V ) + Ω(V ) subject to V = CZT , Z ∈ {0, 1}m×Q, Z1 = 1. (HSC)\nIf instead we want to allow predictors wi to deviate from their assigned centroid vi, we can relax the hard clustering constraint using a regularizer ΩSC(U, V ) described below. This yields the soft supervised clustering problem\nminimize L(U) + ΩSC(U, V ) + Ω(V ) subject to V = CZT , Z ∈ {0, 1}m×Q, Z1 = 1. (SSC)\nGiven Q clusters Cq, we let sq = |Cq| be the size of the qth cluster, with s = ZT1 the corresponding vector. We write Π = I− 1m11T the centering matrix. As in [2], the clustering penalty ΩSC(W,V ) can be decomposed into three separable terms as follows (see Figure 1 for an illustration). • A measure of how large the barycenter of the centers c̄ = 1m ∑Q q=1 sqcq is\nΩmean(V ) = λm m\n2 ||c̄||22 = λm 2\nTr(V (I−Π)V T ). • A measure of the variance between clusters\nΩbetween(V ) = λb 2\nQ∑\nq=1\nsq||cq − c̄||22 = λb 2 Tr(VΠV T ).\n• A measure of the variance within clusters\nΩwithin(U, V ) = λw 2\nQ∑\nq=1\n∑\ni∈Jq\n||ui − cq||22 = λw 2 ||U − V ||2F .\nWe then simply add a penalty with parameter µ on the Frobenius norm of V , i.e. the norm of the centroids weighted by the number of items in each cluster sq\nΩ(V ) = µ\n2\nQ∑\nq=1\nsq‖cq‖22 = µ\n2 Tr(V TV ).\nWe now detail the losses associated with each dimension of the data cube: tasks, features or sample points. Input samples are given by the matrix X = [x1, . . . ,xn]T ∈ Rn×d, labels by (y1, . . . , yn), and x(j) refers to the jth coordinate of x.\n2.1. Clustering features. Given a regression or classification task, we want to reduce dimensionality by grouping together features which have a similar influence on the output. We present the linear regression case [4], which can be extended to logistic regression and classification. Imposing that all predictor coefficients within a cluster with (scalar) centroid cq are identical means the prediction function can be written f : x→∑Qq=1 cq ∑ j∈Cq x (j), which leads to the following loss\nL(V ) = 1\nn\nn∑\ni=1\nl  yi, Q∑\nq=1\ncq ∑\nj∈Cq\nx (j) i\n  = 1\nn\nn∑\ni=1\nl  yi, d∑\nj=1\nQ∑\nq=1\nZjqcqx (j) i\n  ,\nin the variable V = ∑Q\nq=1 Zjqcq = CZ T ∈ Rd of predictor coefficients, quantized over Q values. In this\ncase, if we relax the hard clustering by imposing a soft clustering penalty, we lose the benefit of dimensionality reduction.\n2.2. Clustering tasks. Given a set of K supervised tasks like regression or binary classification, multitask learning aims at jointly solving these tasks, hoping that each task can benefit from the information given by other tasks [1, 2, 3]. For simplicity, we illustrate the case of multi-classification, which can be extended to the general multitask setting. When performing classification with one-versus-all majority vote, we train one binary classifier for each class vs all others. We define the total empirical loss as\nL(U) = 1\nn\nK∑\nk=1\nn∑\ni=1\nl(yi,u T k xi).\nin the matrix variable U ∈ Rd×K of classifier vectors (one per task). 2.3. Clustering sample points. Imagine for instance that we are interested in measuring the effect of two treatments, e.g. a medicine and a placebo, on uniformly distributed patients. Clearly, the regression function varies a lot between the groups of patients that have a different treatment. Now suppose that we do not know to which patients different treatments were given. Since patients are uniformly distributed, the groups cannot be predicted without supplementary information. We will use the effect of treatments to simultaneously\nretrieve the groups of patients and the associated regression functions. Note that this setting is different from a mixture of experts model in the sense that the latent cluster assignment variable Z can only be estimated once y is known and cannot be deduced from the input features X (cf. figure 2). 162 163 164 165 166 167 168 169 170 171 172 173\n174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\npatients different treatments were given. Since patients are uniformly distributed, the groups cannot be predicted without supplementary information. We will use the effect of treatments to simultaneously retrieve the groups of patients and the associated regression functions. Note that this setting is different from a mixture of experts model in the sense that the latent cluster assignm nt variable Z can only be estimated once y is known and cannot be deduced from the input features X . Z X Y Z X Y\nFigure 2: Learning multiple diverse predictors (left), mixture of experts model (right).\nGiven a regression or multi-classification task, we want to find Q groups of sample points to maximize the within-group prediction performance using a group specific predictor. This amount to producing Q diverse answers per sample point, considering only the best one. We thus learn Q predictors, each predictor having low error rate on some cluster of points. For simplicity, we illustrate the case of regression, which can be extended to multi-classification. We minimize the loss incurred by the best linear predictor fq : x! cTq x for each point, i.e. L(V ) = L(CZT ) = 1 n nX i=1 min q2{1,...,Q} l(yi, c T q xi) = 1 n nX i=1 l yi, QX q=1 Ziqc T q xi ! , in the matrix variable V 2 Rd⇥n of predictor vectors (one per sample point), where C 2 Rd⇥Q and Z 2 {0, 1}n⇥Q are the centroid and assignment matrices defined above. Loss Dim. U, V Predictors Goal Features 1n Pn i=1 l ⇣PQ q=1 Pd j=1 Zjqcqxij ⌘ 1⇥ d rows of W regression Tasks 1n PK k=1 Pn i=1 l(w T k xi) d⇥K columns of W classification Samples 1n Pn i=1 l ⇣PQ q=1 Ziqc T q xi ⌘ d⇥ n W (i) regression Table 1: Summary of the presented supervised clustering settings. 3 Algorithms\nWe now present optimization strategies to solve these supervised clustering problems. We begin by\nsimple greedy procedures, then propose a non-convex projected gradient descent scheme and finally a more refined convex relaxation solved using conditional gradient and approximations to k-means. 3.1 Simple strategies A straightforward strategy is to first minimize on predictors, as in a classical supervised learning problem, and then cluster predictors together using k-means. The procedure can be repeated in the case of a soft clustering penalty. In the same spirit, when clustering sample points, one can alternate minimization on the predictors of each group and assignment of each point to the group where its loss is smallest. These methods are fast but very dependent on the initialization. Alternating minimization can optionally be used to refine the solution of the more robust algorithms proposed below. 3.2 Projected gradient descent A natural strategy is to do a projected gradient descent on the non-convex problems (HSC) or (SSC). The projection of a matrix V is made by finding\nargmin Z,C\nkV CZT k2F = argmin QX\nq=1\nX i2Cq kvi cqk22,\nFIGURE 2. Learning ultiple diverse predictors (left), mixture of experts model (right).\nGiven a r gression or multi-classification task, w wa t to findQ groups of sample points t maximize the within-group prediction performance using a group specific predictor. This amount to producing Q diverse answers per sample point, considering only the best one. We thus learn Q predictors, each predictor having low error rate on some cluster of points. For simplicity, we illustrate the case of regression, which can be extended to multi-classification. We minimize the loss incurred by the best linear predictor fq : x → cTq x for each point, i.e.\nL(V ) = L(CZT ) = 1\nn\nn∑\ni=1\nmin q∈{1,...,Q}\nl(yi, c T q xi) =\n1\nn\nn∑\ni=1\nl  yi, Q∑\nq=1\nZiqc T q xi\n  ,\nin the matrix variable V ∈ Rd×n of predictor vectors (one per sample point), where C ∈ Rd×Q and Z ∈ {0, 1}n×Q are the centroid and assignment matrices defined above.\nLoss Dim. U, V Predictors Goal Features 1n ∑n i=1 l (∑Q q=1 ∑d j=1 Zjqcqxij ) 1× d rows of W regression Tasks 1n ∑K k=1 ∑n i=1 l(w T k xi) d×K columns of W classification Samples 1n ∑n i=1 l (∑Q q=1 Ziqc T q xi ) d× n W (i) regression\nTABLE 1. Summary of the presented supervised clustering settings."
    }, {
      "heading" : "3. ALGORITHMS",
      "text" : "We now prese t optimization stra egies to solve these supervised clustering problems. We begin by simple greedy procedures, then propose a non-convex projected gradient descent scheme and finally a more refined convex relaxation solved using conditional gradient and approximations to k-means.\n3.1. Simple strategies. A straightforward strategy is to first minimize on predictors, as in a classical supervised learning problem, and then cluster predictors together using k-means. The procedure can be repeated in the case of a soft clustering penalty. In the same spirit, when clustering sample points, one can alternate minimization on the predictors of each group and assignment of each point to the group where its loss is smallest. These methods are fast but very dependent on the initialization. Alternating minimization can optionally be used to refine the solution of the more robust algorithms proposed below.\n4\n3.2. Projected gradient descent. A natural strategy is to do a projected gradient descent on the non-convex problems (HSC) or (SSC). The projection of a matrix V is made by finding\nargmin Z,C\n‖V − CZT ‖2F = argmin Q∑\nq=1\n∑\ni∈Cq\n‖vi − cq‖22,\nwhere the minimum is taken over centroids ci and partitions (C1, . . . , CQ). This can be solved with the kmeans++ algorithm which performs alternate minimization on the assignments and the centroids. Although it is a non-convex problem, k-means++ gives general approximation bounds on its solution [9].\nWriting k-means++(V,Q) the approximate solution of the projection. Writing φ the objective function and using a backtracking line search for the stepsize αt, the full procedure is summarized in Algorithms 1 and 2. Details of gradient computations for each setting are given in the appendix.\nAlgorithm 1 Proj. Gradient Descent (SSC) Input: X, y,Q, , λb, λw, λm, µ\nInitialize W0 = V0 = 0 while |φ(Wt, Vt)− φ(Wt−1, Vt−1)| ≥ do Wt+1 = Wt − αt(∇L(Wt) +∇ΩS,C(Vt,Wt)) Vt+ 1\n2 = Vt − αt∇ΩS,C(Vt,Wt)\n[Zt+1, Ct+1] = k-means++(Vt+ 1 2 , Q) Vt+1 = Ct+1Z T t+1\nend while Z∗ and C∗ are given through Kmeans++\nOutput: W ∗, V ∗, Z∗, C∗\nAlgorithm 2 Proj. Gradient Descent (HSC) Input: X, y,Q, , µ\nInitialize V0 = 0 while |φ(Vt)− φ(Vt−1)| ≥ do Vt+ 1\n2 = Vt − αt(∇L(V ) +∇Ω(V ))\n[Zt+1, Ct+1] = k-means++(Vt+ 1 2 , Q) Vt+1 = Ct+1Z T t+1\nend while Z∗ and C∗ are given through Kmeans++\nOutput: V ∗, Z∗, C∗\n3.3. Convex relaxation using approximate conditional gradient . Another algorithmic approach to the supervised clustering problem is to minimize with respect to the assignment matrix Z using the Frank-Wolfe algorithm (a.k.a. conditional gradient, [12, 13]). Considering the squared loss l(f(x), y) = 12(y − f(x))2, we use the analytic form of the minimization in (W,V ) or V to rewrite the problem. The clustering is then captured in terms of the equivalence matrix M = Z(ZTZ)−1ZT , which satisfies Mij = 1/|Cq| if item i and j are in the same cluster q and Mij = 0 otherwise.\nWe describe here the simple case of supervised clustering of features in a regression task, introduced in §2.1. Detailed computations and explicit procedures for all settings are given in the appendix. When clustering features, we optimize over the coefficients associated with each feature and the k-means step can be performed exactly using dynamic programming [10, 11].\nThe loss L can be written here\nL(V ) = L(CZT ) = 1\n2n\nn∑\ni=1\n yi − d∑\nj=1\nQ∑\nq=1\nZjqcqx (j) i\n  2\n,\nwhere C = [c1, . . . , cQ] ∈ R1×Q, hence the regularized loss becomes\nφ(C,Z) = 1\n2n\nn∑\ni=1\n( yi − CZT xi )2 + µ\n2 ‖CZT ‖22\n= 1\n2n Tr(yT y) +\n1 2n Tr(CZTXTXZCT )− 1 n Tr(CZTXT y) + µ 2 Tr(CZTZCT ).\nMinimizing in C and using the Sherman-Woodbury-Morrison formula we get\nG(M) := min C L(CZT ) +\nµ 2 ‖CZT ‖22\n= 1\n2n\n( y yT ( I−XZ(ZTXTXZ + µnZTZ)−1ZTXT y ))\n= 1\n2n Tr\n( y yT ( I + 1\nnµ XMXT\n)−1) .\nDefiningM as the set of equivalence matrices of the formM = Z(ZTZ)−1ZT , forZ an assignment matrix, each iteration of the conditional gradient method requires solving an affine minimization subproblem over hull(M). The setM being discrete, we have argminM∈hull(M)Tr(M∇G(M)) = argminM∈MTr(M∇G(M)). Writing P = −∇G(M), we get\nP = 1\n2n2µ XT\n( I + 1\nnµ XMXT\n)−1 y yT ( I + 1\nnµ XMXT\n)−1 X,\nwhich is always semidefinite positive. Writing P 1 2 the matrix square root of P we have\nargmin M∈M Tr(M∇G(M)) = argmin M∈M\n−Tr(MP 12P 12 T )\n= argmin M∈M\nTr((I−M)P 12P 12 T ))\n= argmin M∈M\n‖P 12 −MP 12 ‖2F\n= argmin Z min C ‖P 12 − ZCT ‖2F ,\nwhere we recognize the k-means problem defined above. In fact, in this particular case, the k-means subproblem is one-dimensional and can be solved exactly using dynamic programming [10, 11]. We use the classical stepsize for conditional gradient αk = 2k+2 and Frank-Wolfe rounding. The procedure is summarized in Algorithm 3.\nAlgorithm 3 Conditional gradient on the equivalence matrix Input: X, y,Q,\nInitialize M0 ∈M while |G(Mk)−G(Mk−1)| ≥ do\nCompute the matrix square root P 1 2 of −∇G(Mk) Get oracle ∆k = k-means(P 1 2 , Q)\nMk+1 = Mk + αk(∆k −Mk) end while Use Frank Wolfe rounding to get a solution M∗ ∈M Z∗ is given by k-means C∗ is given by the analytic solution of the minimization for Z∗ fixed\nOutput: C∗, Z∗,M∗\n3.4. Complexity. The core complexity of Algorithms 1 and 2 is concentrated in the inner k-means subproblem, which standard alternating minimization approximates inO(tQS), where t is the number of alternating steps, Q is the number of clusters, and S is the product of the dimensions of V (see Table 1). Using a proper conditioning of the gradient, the number of iterations before convergence is typically below 100, which make Algorithms 1 and 2 both fast and scalable. For Algorithm 3, we also need to compute a matrix square\nroot of the gradient at each iteration, which can slow down computations for large datasets. The choice of the number of clusters can be done given an a priori on the problem (e.g. if we know the hierarchical structure of classes in a classification problem), or cross-validation, idem for the other regularization parameters.\n4. NUMERICAL EXPERIMENTS\n4.1. Synthetic dataset. Supervised clustering of sample points. We generate n data points (xi, yi) for i = 1, . . . , n with xi ∈ Rd and yi ∈ R, divided in two clusters corresponding to regression tasks with weights w1 and w2. Regression labels for points xi in cluster q are given by yi = wTq xi +η, where η ∼ N (0, σ2). We test the robustness of the algorithms to noise dimensions, i.e. we complete xi with dn dimensions of noise ηd ∼ N (0, σd). The results are reported in Table 2.\nHere, the intrinsic dimension is 10. “Oracle” refers to the least square fit given the true assignments. It can be seen as the best error rate that can be achieved. PGK refers to projected gradient, OM refers to conditional gradient on M , AM refers to alternate minimization. PGK and OM were followed by AM refinement. 200 points were used for training, 200 for testing. The regularization parameter was set to λ = 10−2 for all experiments. Noise on labels and added dimensions σy = σd = 2× 10−1. Results were averaged over 100 experiments, figures after the sign ± correspond to one standard deviation.\nIt appears that that PGK and OM give very similar results, significantly improving on the naive alternating minimization scheme. In view of standard deviations, OM seems more robust. Supervised clustering of features. We generate n data points (xi, yi) for i = 1, . . . , n with xi ∈ Rd and yi ∈ R. Regression weights have only 10 different values wq for q = 1, . . . , 10, uniformly distributed around 0. Regression labels are given by yi = wT xi +η, where η ∼ N (0, σ2). We test the robustness of the algorithms to the number of learning examples, i.e. the size of the training set n. We measure the l2 norm of the difference between the true vector of weights and the estimated ones.\nWe compare in Table 3 the proposed algorithms to OSCAR [4], Ridge, Lasso and Ridge followed by k-means on the weights (using associated centroids as predictors). “Oracle” refers to the mean square fit given the true assignments of features. It can be seen as the best error rate that can be achieved. PGK refers to projected gradient and was intitialized with the solution of Ridge followed by k-means, OM refers to conditional gradient on M and was followed by PGK. Noise on labels is set to σ = 10−1. Algorithm parameters were all cross-validated using a logarithmic grid. Results were averaged over 30 experiments and figures after the sign± correspond to one standard deviations. Results were multiplied by 100 to shorten the table.\nPGK and OM appear to give significantly better results than other methods and even reach the performance of the Oracle for n > d, while for n ≤ d results are in the same range. Note that contrary to OSCAR, Lasso and Ridge, reduction of dimensionality is guaranteed (the number of clusters is fixed).\n4.2. Real data.\n4.2.1. 20NewsGroup classification. We perform classification on 2800 documents extracted from the publicly available 20NewsGroup dataset, which contains 20 different newsgroups, each one corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware and comp.sys.mac.hardware), while others are highly unrelated (e.g. misc.forsale and soc.religion.christian). Our goal is to retrieve clusters of related newsgroups, while performing competitive classification. We used a dictionary of 5000 words chosen by tf-idf and took the empirical distribution over words as covariates for each document.\nIn Table 4, we compare our approach to other classical regularizations such as the Frobenius norm and the trace norm, as implemented in [3], using either a ridge or a logistic loss. As the projected gradient descent scheme is not convex we initialize it by the solution given by the logistic loss non regularized. All algorithms were 5-fold cross-validated on 80% of the data then tested on the remaining 20%. The number of clusters was set to 5, as suggested by the names of newsgroups. Figures after the sign ± correspond to one standard deviation when varying the training and test sets. As reported in Table 4, it appears that the clustering regularization helps prediction on topics."
    }, {
      "heading" : "Frobenius Logistic Trace Ridge OM Ridge PGK Logistic",
      "text" : "4.2.2. Predicting ratings associated with reviews using groups of words. We perform “sentiment” analysis of newspaper movie reviews. We use the publicly available dataset introduced in [14] which contains movie reviews paired with star ratings. We treat it as a regression problem, taking log-responses for y in (0, 1) and the empirical distribution over words as covariates. With a 5000 term vocabulary chosen by tf-idf, the corpus contains 5006 documents and comprises 1.6M words. We evaluate our algorithms for regression with clustered features against standard regression approaches: Lasso, Ridge, and Ridge followed by kmeans on predictors. All algorithms were 5-fold cross-validated on 80% of the dataset and then tested on the remaining 20%. The number of clusters was arbitrarily set to 10, though we did not notice significant changes when varying it. Results are reported in Table 5, figures after the sign± correspond to one standard deviation when varying the training and test sets. While all methods including ours give 10% mean absolute errors on the test set (up to 0.5% accuracy), our approach has the additional benefit of providing clusters of words which have a similar influence. Moreover we noticed that the clusters with highest absolute weights are also the ones with smallest number of words, which confirms the intuition that only a few words are very discriminative. We illustrate this in Table 6, listing words of the first and last clusters."
    }, {
      "heading" : "PGK OM+PGK Ridge+Kmeans Lasso Ridge",
      "text" : ""
    }, {
      "heading" : "5. DISCUSSION",
      "text" : "We have developed a unified framework for supervised clustering over tasks, features or samples and provided two robust algorithms for solving the associated optimization problems. Results on synthetic and realistic text datasets suggest that our method is competitive against standard regression and classification methods, while having the additional benefit of providing clusters of tasks, features or samples. Similarly as in compressed sensing, optimization is made on a union of subspaces, hence in analogy with RIP conditions for iterative hard thresholding [15], it would be interesting to see under which assumptions our algorithms can recover the optimal solution to the supervised clustering problem."
    }, {
      "heading" : "6. APPENDIX",
      "text" : "We give here detailed computations of the function G corresponding to the convex relaxation defined in 3.3 in all settings.\nWe always suppose ZTZ inversible i.e.∀q, sq 6= 0 (there is no empty cluster). For any integer p, we let 1p ∈ Rp be the vector whose coordinates are all ones, Ip the identity matrix in Rp×p, Γp = 1p1 T p\np and Πp = Ip − Γp the corresponding centering matrix.\nFor all settings input samples are represented by the matrix X = (x1, ...,xn)T ∈ Rn×d and their respective labels by y = (y1, ....yn) ∈ Y . For regression problems Y = R. Classification problems are casted into the multitask setting, each class corresponding to a task. We denote by yk = (yk1 , ..., y k n) the vector of binary labels corresponding to the class k ∈ [[1,K]] and Y = (y1, ...,yK).\n6.1. Minimization in C for Soft Clustering Problems . For soft supervised clustering problems, the minimization in the variable C for W,Z fixed can be made without assumptions on the specific loss L. We let (δ,m) be the dimensions of the predictors of interest, such thatW ∈ Rδ×m,C ∈ Rδ×Q andZ ∈ {0, 1}m×Q. We denote by λW , λB, λM the weights associated respectively to the within, between and mean penalties. Minimization in C is then given by\nH(W,Z) := min C∈Rδ×Q λW 2 ‖W − CZT ‖2F + λB 2 Tr(CZTΠmZC T ) + λM 2 Tr(CZTΓmZC T )\n= min C∈Rδ×Q λW 2 ‖W‖2F + 1 2 Tr ( C ( (λW + λB)Z TZ + (λM − λB)ZTΓmZ ) CT ) − λW Tr(CTWZ)\n= λW 2 Tr(WW T )− λ 2 W 2 Tr ( WZ ( (λW + λB)ZZ T + (λM − λB)ZTΓmZ )−1 ZTW ) .\nLet s = ZT1m ∈ RQ be the vector whose coordinates sq are the sizes of the clusters, denoting by s 1 2\nthe vector whose coordinates are √ sq, and by s − 1 2 the vector whose coordinates are 1√\nsq , we can derive the\ninversion in the precedent formula,\nJ−1 = ( (λW + λB)ZZ T + (λM − λB)ZTΓmZ )−1\n= ( (λW + λB)diag(s) + (λM − λB) ssT\nm\n)−1\n= 1\nλW + λB diag(s− 1 2 )\n IQ +\nλM − λB λW + λB s 1 2 s 1 2 T m\n  −1\ndiag(s− 1 2 )\n= 1\nλW + λB diag(s− 1 2 )\n IQ −\nλM − λB λW + λM s 1 2 s 1 2 T m\n diag(s− 12 )\n= 1 λW + λB (ZTZ)−1 − λM − λB (λW + λM )(λW + λB) 1Q1 T Q m\n= 1\nλW + λB ((ZTZ)−1 −\n1Q1 T Q\nm ) +\n1\nλW + λM\n1Q1 T Q\nm ,\nwhere we used that p = s 1 2 s 1 2 T m = s 1 2 s 1 2 T\n‖s 1 2 ‖22 is a projector and therefore (I + αp)−1 = I − αα+1p, added to the fact that diag(s) = ZTZ.\nNow introducing the equivalence matrix M = Z(ZTZ)−1ZT , and using that Z 1Q1\nT Q\nm Z T = Γm, we\nfinally obtain\nH(W,Z) = λW 2 Tr(WW T )− λ 2 W 2 Tr\n( W ( 1\nλW + λB (M − Γm) +\n1\nλW + λM Γm\n) W T )\n= λW 2\nTr ( W (Im − α(M − Γm)− βΓm)W T ) ,\nwhere α = λWλW+λB and β = λW λW+λM .\nDenoting P = Im − α(M − Γm)− βΓm, one can also use Kronecker’s formula :\nH(W,Z) = λW 2 Vec(W )T (P ⊗ Iδ) Vec(W ).\n6.2. Clustered Multitask Learning . We derive here the computation of G when clustering tasks. We restrict ourselves to the case of multiclass setting cast as a multitask problem such that each task shares the same input data. GivenK classes, we denote byW = (w1, ...,wK) ∈ Rd×K the matrix of linear predictors. Using a squared loss l(f(x), y) = 12(y − f(x))2 the empirical loss is given by :\nL(W ) = 1\n2n\nn∑\ni=1\nK∑\nk=1\n(yki −wTk xi)2.\nUsing Kronecker’s product formula, we get\nL(W ) = 1\n2n\nK∑\nk=1\nwTk X TXwk−\n1\nn\nK∑\nk=1\nwTk X T yk +\n1\n2n ‖y ‖22\n= 1\n2n\nK∑\nk=1\neTk W TXTXW ek−\n1 n Tr(WXTY ) + 1 2n ‖y ‖22\n= 1\n2n Vec(W )T (IK ⊗XTX) Vec(W )−\n1 n Vec(W )T Vec(XTY ) + 1 2n ‖y ‖22.\nUsing the expression found by minimizing in C the regularization penalty, we get an expression for G :\nG(M) = min W L(W ) +H(W,Z)\n= min W\n1\n2n Vec(W )T (IK ⊗XTX + P ⊗ Id) Vec(W )−\n1 n Vec(W )T Vec(XTY ) + 1 2n ‖y ‖22\n= − 1 2n\nVec(XTY )T ( IK ⊗XTX + λWnP ⊗ Id )−1 Vec(XTY ) + 1\n2n ‖y ‖22\nDenote (v1, ...,vd) ∈ Rd×d, (λ1, ..., λd) ∈ Rd and (u1, ...,uS) ∈ RS×S , (µ1, ..., µS) ∈ RS the eigenvectors and corresponding eigenvalues respectively of matricesXTX and P = (IK−α(M −ΓK)+βΓK). The eigenvectors and corresponding eigenvalues of IK ⊗ XTX + λWnP ⊗ Id are (ui ⊗ vj)i∈[[1,n]]\nj∈[[1,d]] and\n(λWnµi + λj)i∈[[1,n]] j∈[[1,d]] . The inversion in the expression of G is then given by\nJ−1 = ( IK ⊗XTX + λWnP ⊗ Id )−1 = n∑\ni=1\nd∑\nj=1\n1\nλWnµi + λj uiu\nT i ⊗ vjvTj .\nThen we note that the set of eigenvectors of P can be decomposed into three sets. Indeed the matrices IK −M ,M − ΓK and ΓK are orthogonal projectors on orthogonal subspaces spanning the entire space. Denote by IW , IB , IM the sets of eigenvectors corresponding respectively to IK −M , M − ΓK and ΓK , their corresponding eigenvalues in P can easily be computed and we obtain\nP = IK −M + (1− α)(M − ΓK) + (1− β)ΓK = ∑\ni∈IW\nuiu T i + (1− α)\n∑\ni∈IW\nuiu T i + (1− β)\n∑\ni∈IM\nuiu T i .\nThis decomposition can be used for the inversion :\nJ−1 = ∑\ni∈IW\nd∑\nj=1\n1\nλWn+ λj uiu\nT i ⊗ vjvTj\n+ ∑\ni∈IB\nd∑\nj=1\n1\nλWn(1− α) + λj uiu\nT i ⊗ vjvTj\n+ ∑\ni∈IW\nd∑\nj=1\n1\nλWn(1− β) + λj uiu\nT i ⊗ vjvTj\n= (IK −M)⊗ (XTX + nλW Id)−1 + (M − ΓK)⊗ (XTX + nλW (1− α)Id)−1 + ΓK ⊗ (XTX + nλW (1− β)Id)−1.\nFinally G can be simplified using properties of the Kronecker product\nG(M) =− 1 2n\nVec(XTY )T ( (IK −M)⊗ (XTX + nλW Id)−1 ) Vec(XTY )\n− 1 2n\nVec(XTY )T ( (M − ΓK)⊗ (XTX + nλW (1− α)Id)−1 ) Vec(XTY )\n− 1 2n\nVec(XTY )T ( ΓK ⊗ (XTX + nλW (1− β)Id)−1 ) Vec(XTY ) + 1\n2n ‖Y ‖22\n=− 1 2n Tr(Y TX(XTX + nλW Id) −1XTY (IK −M))\n− 1 2n Tr(Y TX(XTX + nλW (1− α)Id)−1XTY (M − ΓK)) − 1 2n Tr(Y TX(XTX + nλW (1− β)Id)−1XTY ΓK) + 1 2n ‖Y ‖2F .\nTherfore G is a linear function of M whose gradient is given by\n∇G(M) = 1 2n Y TX\n( (XTX + nλW Id) −1 − (XTX + nλW (1− α)Id)−1 ) XTY.\nAs 1 ≥ α ≥ 0, we get that −∇G(M) 0. For a fixed M , W is given using precedent computations and Kronecker’s formula by\nWM = (X TX + nλW Id) −1XTY (IK −M) + (XTX + nλW (1− α)Id)−1XTY (M − ΓK) + (XTX + nλW (1− β)Id)−1XTY ΓK .\n6.3. Learning with clustered features. For more generality we cast the problem of learning clustered features in the multiclass learning setting. We restrict here to the soft supervised clustering problem as the hard one has already be done. We keep notations introduced in section 6.2 though we now have W =\n(w1, ...,wK) T ∈ RK×d. The empirical loss is given by\nL(W ) = 1\n2n\nn∑\ni=1\nK∑\nk=1\n(yki −wTk xi)2.\n= 1\n2n\nK∑\nk=1\nwTk X TXwk−\n1\nn\nK∑\nk=1\nwTk X T yk +\n1\n2n ‖Y ‖2F\n= 1\n2n\nK∑\nk=1\nTr(W TWXTX)− 1 n Tr(WXTY ) + 1 2n ‖Y ‖2F\nAdding the regularization penalty and minimizing in C we obtain\nG(M) = min W\n1\n2n\nK∑\nk=1\nTr ( W TW (XTX + λWnP ) ) − 1 n Tr(WXTY ) + 1 2n ‖Y ‖22\n= 1\n2n ‖Y ‖22 −\n1\n2n\n( Y TX(XTX + λWnP ) −1XTY )\n= 1 2n Tr ( Y Y T (In −X(XTX + λWnP )−1XT )\n= 1\n2n Tr\n( Y Y T (In + 1\nλWn XP−1XT )−1\n)\nAs detailed before, the inverse of P can be found analytically by observing that it is composed of orthogonal projectors on orthogonal subspaces spanning the entire space. Hence we have P−1 = Id −M + 11−α(M − Γd) + 1 1−βΓd. We now get\nG(M) = 1\n2n Tr\n( Y Y T (Id + 1\nnλB X(M − Γd)XT +\n1\nnλM XΓdX\nT )\n)\nNote that we still have −∇G(M) 0. For a fixed M , W is given analitically by\nWM = Y TX ( XTX + λWn(Id − α(M − Γd)− βΓd) )−1 .\n6.4. Learning multiple predictors. In the setting of learning multiple predictors, we denote by Cq the set of points whose best linear predictor is wq, having therefore C1, ..., CQ a partition of [[1, n]]. We let as above sq be the cardinal of the set Cq, Xq ∈ Rsq×d is the matrix whose columns are the points belonging to the cluster q, and yq is the column vector of labels corresponding to cluster q. We use a squared loss l(f(x), y) = 12(y − f(x))2.\nFor C1, ..., CQ fixed (or equivalently Z fixed), we can computeG using the Sherman-Woodbury-Morrison formula as\nG(M) = min w1,...,wQ\n1\n2n\nQ∑\nq=1\n∑\ni∈Cq\n(yi −wTq xi)2 + µ\n2\nQ∑\nq=1\nsq‖wq ‖22\n=\nQ∑\nq=1\n1\n2n yTq\n( 1\nsqµn XqX\nT q + In )−1 yq .\nWe define E ∈ Rn×n the permutation matrix permuting order of points such that\nE y =   yC1\n... yCQ\n  EX =   X1\n... XQ\n  .\nWe denote for q ∈ [[1, Q]], Rq = ∑∑q−1 p=1 sp≤i≤ ∑q p=1 sp eie T i ortohgonal projectors on the ordered sets of points belonging to cluster q such that   X1X T 1 0 0 0 . . . 0\n0 0 XQX T Q\n  = diag(EXXTET ) = Q∑\nq=1\nRqEXX TETRq.\nThus we get\nG(M) = 1\n2n yT ET\n  Q∑\nq=1\n1\nsqµn RqEXX\nTETRq + In\n  −1\nE y\n= 1\n2n yT\n  Q∑\nq=1\n1\nsqµn ETRqEXX TETRqE + In\n  −1\ny .\nThen we notice that ETRqE = diag(Zq) i.e. the projector on the set of points belonging to cluster q, and that ∑Q q=1 1 sq diag(ZQ)XX T diag(Zq) = M ◦XXT , where ◦ denotes the Hadamard product. Hence we finally get\nG(M) = 1 2n yT ( 1 µn XXT ◦M + In )−1 y .\nIts gradient is given by\n2n∇G(M) = − 1 µn\nXXT ◦ ( (In + 1\nµn XXT ◦M)−1 y yT (In +\n1\nµn XXT ◦M)−1\n) ,\nfor which we have −∇G(M) 0. For a fixed Z, denoting by Xq = ZTq X the set of points belonging to cluster q, the linear predictors wq for each cluster of points are given by\nwq = (nµsqId +X T q Xq) −1XTq yq .\nAcknowledgements. AA is at CNRS, at the Département d’Informatique at École Normale Supérieure, 23 avenue d’Italie, 75013 Paris, France. INRIA, Sierra project-team, PSL Research University. The authors would like to acknowledge support from a starting grant from the European Research Council (ERC project SIPA), an AMX fellowship, the MSR-Inria Joint Centre, as well as support from the chaire Économie des nouvelles données, the data science joint research initiative with the fonds AXA pour la recherche and a gift from Société Générale Cross Asset Quantitative Research."
    }, {
      "heading" : "INRIA - SIERRA PROJECT TEAM & D.I., UMR 8548,",
      "text" : ""
    }, {
      "heading" : "ÉCOLE NORMALE SUPÉRIEURE, PARIS, FRANCE.",
      "text" : "E-mail address: vincent.roulet@inria.fr"
    }, {
      "heading" : "C.M.A.P., ÉCOLE POLYTECHNIQUE, UMR CNRS 7641",
      "text" : "E-mail address: fajwel.fogel@cmap.polytechnique.fr\nCNRS & D.I., UMR 8548, ÉCOLE NORMALE SUPÉRIEURE, PARIS, FRANCE. E-mail address: aspremon@ens.fr"
    }, {
      "heading" : "INRIA - SIERRA PROJECT TEAM & D.I., UMR 8548,",
      "text" : ""
    }, {
      "heading" : "ÉCOLE NORMALE SUPÉRIEURE, PARIS, FRANCE.",
      "text" : "E-mail address: francis.bach@inria.fr"
    } ],
    "references" : [ {
      "title" : "Convex multi-task feature learning",
      "author" : [ "Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "Clustered multi-task learning: A convex formulation",
      "author" : [ "Laurent Jacob", "Jean philippe Vert", "Francis R. Bach" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Convex learning of multiple tasks and their structure",
      "author" : [ "Carlo Ciliberto", "Tomaso Poggio", "Lorenzo Rosasco" ],
      "venue" : "arXiv preprint arXiv:1504.03101,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with oscar",
      "author" : [ "Howard D. Bondell", "Brian J. Reich" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "A divisive information theoretic feature clustering algorithm for text classification",
      "author" : [ "Inderjit S. Dhillon", "Subramanyam Mallela", "Rahul Kumar" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "A fuzzy self-constructing feature clustering algorithm for text classification",
      "author" : [ "Jung-Yi Jiang", "Ren-Jia Liou", "Shie-Jue Lee" ],
      "venue" : "Knowledge and Data Engineering, IEEE Transactions on,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Efficiently enforcing diversity in multi-output structured prediction",
      "author" : [ "Abner Guzman-Rivera", "Pushmeet Kohli", "Dhruv Batra", "Rob Rutenbar" ],
      "venue" : "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Large-scale learning for image classification",
      "author" : [ "Zaid Harchaoui" ],
      "venue" : "http://www.di.ens.fr/ willow/events/cvml2013/materials/slides/thursday/harch_cvml13.pdf,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "k-means++: The advantages of careful seeding",
      "author" : [ "David Arthur", "Sergei Vassilvitskii" ],
      "venue" : "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "A note on cluster analysis and dynamic programming",
      "author" : [ "Richard Bellman" ],
      "venue" : "Mathematical Biosciences,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1973
    }, {
      "title" : "Ckmeans. 1d. dp: optimal k-means clustering in one dimension by dynamic programming",
      "author" : [ "Haizhou Wang", "Mingzhou Song" ],
      "venue" : "The R Journal,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "An algorithm for quadratic programming",
      "author" : [ "Marguerite Frank", "Philip Wolfe" ],
      "venue" : "Naval research logistics quarterly,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1956
    }, {
      "title" : "Revisiting frank-wolfe: Projection-free sparse convex optimization",
      "author" : [ "Martin Jaggi" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Bo Pang", "Lillian Lee" ],
      "venue" : "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2005
    }, {
      "title" : "Iterative hard thresholding for compressed sensing",
      "author" : [ "Thomas Blumensath", "Mike E. Davies" ],
      "venue" : "Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This problem has been well studied in the multi-task learning literature [1, 2, 3].",
      "startOffset" : 73,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "This problem has been well studied in the multi-task learning literature [1, 2, 3].",
      "startOffset" : 73,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "This problem has been well studied in the multi-task learning literature [1, 2, 3].",
      "startOffset" : 73,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "Similarly, when there exists some groups of highly correlated features, reducing dimensionality by assigning the same weights to some groups of features can be beneficial both in terms of prediction and interpretation [4].",
      "startOffset" : 218,
      "endOffset" : 221
    }, {
      "referenceID" : 4,
      "context" : "This often occurs in text classification, where it is natural to group together words having the same meaning for a given task [5, 6].",
      "startOffset" : 127,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "This often occurs in text classification, where it is natural to group together words having the same meaning for a given task [5, 6].",
      "startOffset" : 127,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "Finally, in some settings, it can be valuable to cluster sample points, with each cluster having its own distinct prediction function [7].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "Here, we present a unified and flexible framework for supervised clustering over the “data cube” of either tasks, features or sample points (a representation introduced by [8]).",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 8,
      "context" : "While the original optimization problem is non-convex, we show that the core nonconvexity is concentrated in a subproblem similar to k-means, which we solve using classical approximation techniques [9].",
      "startOffset" : 198,
      "endOffset" : 201
    }, {
      "referenceID" : 9,
      "context" : "In the particular case of feature clustering for regression, the k-means steps are performed in dimension one, and can therefore be solved exactly by dynamic programming [10, 11].",
      "startOffset" : 170,
      "endOffset" : 178
    }, {
      "referenceID" : 10,
      "context" : "In the particular case of feature clustering for regression, the k-means steps are performed in dimension one, and can therefore be solved exactly by dynamic programming [10, 11].",
      "startOffset" : 170,
      "endOffset" : 178
    }, {
      "referenceID" : 11,
      "context" : "Our formulation is then an explicit convex relaxation which can be solved efficiently using the conditional gradient algorithm [12, 13].",
      "startOffset" : 127,
      "endOffset" : 135
    }, {
      "referenceID" : 12,
      "context" : "Our formulation is then an explicit convex relaxation which can be solved efficiently using the conditional gradient algorithm [12, 13].",
      "startOffset" : 127,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "As in [2], the clustering penalty ΩSC(W,V ) can be decomposed into three separable terms as follows (see Figure 1 for an illustration).",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 3,
      "context" : "We present the linear regression case [4], which can be extended to logistic regression and classification.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "Given a set of K supervised tasks like regression or binary classification, multitask learning aims at jointly solving these tasks, hoping that each task can benefit from the information given by other tasks [1, 2, 3].",
      "startOffset" : 208,
      "endOffset" : 217
    }, {
      "referenceID" : 1,
      "context" : "Given a set of K supervised tasks like regression or binary classification, multitask learning aims at jointly solving these tasks, hoping that each task can benefit from the information given by other tasks [1, 2, 3].",
      "startOffset" : 208,
      "endOffset" : 217
    }, {
      "referenceID" : 2,
      "context" : "Given a set of K supervised tasks like regression or binary classification, multitask learning aims at jointly solving these tasks, hoping that each task can benefit from the information given by other tasks [1, 2, 3].",
      "startOffset" : 208,
      "endOffset" : 217
    }, {
      "referenceID" : 8,
      "context" : "Although it is a non-convex problem, k-means++ gives general approximation bounds on its solution [9].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 11,
      "context" : "conditional gradient, [12, 13]).",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "conditional gradient, [12, 13]).",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "When clustering features, we optimize over the coefficients associated with each feature and the k-means step can be performed exactly using dynamic programming [10, 11].",
      "startOffset" : 161,
      "endOffset" : 169
    }, {
      "referenceID" : 10,
      "context" : "When clustering features, we optimize over the coefficients associated with each feature and the k-means step can be performed exactly using dynamic programming [10, 11].",
      "startOffset" : 161,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : "In fact, in this particular case, the k-means subproblem is one-dimensional and can be solved exactly using dynamic programming [10, 11].",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 10,
      "context" : "In fact, in this particular case, the k-means subproblem is one-dimensional and can be solved exactly using dynamic programming [10, 11].",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "We compare in Table 3 the proposed algorithms to OSCAR [4], Ridge, Lasso and Ridge followed by k-means on the weights (using associated centroids as predictors).",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "In Table 4, we compare our approach to other classical regularizations such as the Frobenius norm and the trace norm, as implemented in [3], using either a ridge or a logistic loss.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 13,
      "context" : "We use the publicly available dataset introduced in [14] which contains movie reviews paired with star ratings.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "Similarly as in compressed sensing, optimization is made on a union of subspaces, hence in analogy with RIP conditions for iterative hard thresholding [15], it would be interesting to see under which assumptions our algorithms can recover the optimal solution to the supervised clustering problem.",
      "startOffset" : 151,
      "endOffset" : 155
    } ],
    "year" : 2015,
    "abstractText" : "We study a supervised clustering problem seeking to cluster either features, tasks or sample points using losses extracted from supervised learning problems. We formulate a unified optimization problem handling these three settings and derive algorithms whose core iteration complexity is concentrated in a k-means clustering step, which can be approximated efficiently. We test our methods on both artificial and realistic data sets extracted from movie reviews and 20NewsGroup.",
    "creator" : "LaTeX with hyperref package"
  }
}