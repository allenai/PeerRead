{
  "name" : "1506.01057.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Hierarchical Neural Autoencoder for Paragraphs and Documents",
    "authors" : [ "Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky" ],
    "emails" : [ "jurafsky@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse\n1Code for models described in this paper are available at www.stanford.edu/˜jiweil/.\n2To appear as conference paper at ACL 2015.\n(Barzilay and Lapata, 2008; Barzilay and Lee, 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text.\nRecent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; 0; Luong et al., 2014) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to form meanings that they wish to express.\nCould these models be extended to deal with generation of larger structures like paragraphs or even entire documents? In standard sequenceto-sequence generation tasks, an input sequence is mapped to a vector embedding that represents the sequence, and then to an output string of words. Multi-text generation tasks like summarization could work in a similar way: the system reads a collection of input sentences, and is then asked to generate meaningful texts with certain properties (such as—for summarization— being succinct and conclusive). Just as the local semantic and syntactic compositionally of words\nar X\niv :1\n50 6.\n01 05\n7v 1\n[ cs\n.C L\n] 2\nJ un\n2 01\n5\ncan be captured by LSTM models, can the compositionally of discourse releations of higher-level text units (e.g., clauses, sentences, paragraphs, and documents) be captured in a similar way, with clues about how text units connect with each another stored in the neural compositional matrices?\nIn this paper we explore a first step toward this task of neural natural language generation. We focus on the component task of training a paragraph (document)-to-paragraph (document) autoencoder to reconstruct the input text sequence from a compressed vector representation from a deep learning model. We develop hierarchical LSTM models that arranges tokens, sentences and paragraphs in a hierarchical structure, with different levels of LSTMs capturing compositionality at the tokentoken and sentence-to-sentence levels.\nWe offer in the following section to a brief description of sequence-to-sequence LSTM models. The proposed hierarchical LSTM models are then described in Section 3, followed by experimental results in Section 4, and then a brief conclusion."
    }, {
      "heading" : "2 Long-Short Term Memory (LSTM)",
      "text" : "In this section we give a quick overview of LSTM models. LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x1, x2, ..., xnX}, an LSTM associates each timestep with an input, memory and output gate, respectively denoted as it, ft and ot, the status which are given by:\nit = σ(Wi · et + Vi · ht−1) ft = σ(Wf · et + Vf · ht−1) ot = σ(Wo · et + Vo · ht−1)\n(1)\nFor notations, we disambiguate e and h where et denote the vector for individual text unite (e.g., word or sentence) at time step t while ht denotes the vector computed by LSTM model at time t by combining et and ht−1. σ denotes the sigmoid function. The vector representation ht for each time-step t is given by:\nlt = tanh(Wl · ext + Vl · ht−1) mt = ft ·mt−1 + it · lt ht = ot ·mt\n(2)\nIn sequence-to-sequence generation tasks, each input X is paired with a sequence of outputs to predict: Y = {y1, y2, ..., ynY }. An LSTM defines\na distribution over outputs and sequentially predicts tokens using a softmax function:\nP (Y |X) = ∏\nt∈[1,ny ]\np(yt|x1, x2, ..., xt, y1, y2, ..., yt−1)\n= ∏\nt∈[1,ny ]\nexp(f(ht−1, eyt))∑ y′ exp(f(ht−1, ey′))\n(3) f(ht−1, eyt) denotes the activation function between eh−1 and eyt , where ht−1 is the representation outputted from the LSTM at time t− 1. Note that each sentence ends up with a special end-ofsentence symbol <end>. Commonly, the input and output use two different LSTMs with different sets of convolutional parameters for capturing different compositional patterns.\nIn the decoding procedure, the algorithm terminates when an <end> token is predicted. At each timestep, either a greedy approach or beam search can be adopted for word prediction. Greedy search selects the token with the largest conditional probability, the embedding of which is then combined with preceding output for next step token prediction. For beam search, (Sutskever et al., 2014) discovered that a beam size of 2 suffices to provide most of benefits of beam search."
    }, {
      "heading" : "3 Paragraph Autoencoder",
      "text" : "In this section, we introduce our proposed hierarchical LSTM model for the autoencoder."
    }, {
      "heading" : "3.1 Notation",
      "text" : "Let D denote a paragraph or a document, which is comprised of a sequence of ND sentences, D = {s1, s2, ..., sND , endD}. An additional ”endD” token is appended to each document. Each sentence s is comprised of a sequence of tokens s = {w1, w2, ..., wNs} where Ns denotes the length of the sentence, each sentence ending with an “ends” token. The word w is associated with a K-dimensional embedding ew, ew = {e1w, e2w, ..., eKw }. Let V denote vocabulary size. Each sentence s is associated with a Kdimensional representation es.\nAn autoencoder is a neural model where output units are directly connected with or identical to input units. Typically, inputs are compressed into a representation using neural models (encoding), which is then used to reconstruct it back (decoding). For a paragraph autoencoder, both the input\nX and output Y are the same document D. The autoencoder first compresses D into a vector representation eD and then reconstructs D based on eD.\nFor simplicity, we define LSTM(ht−1, et) to be the LSTM operation on vectors ht−1 and et to achieve ht as in Equ.1 and 2. For clarification, we first describe the following notations used in encoder and decoder:\n• hwt and hst denote hidden vectors from LSTM models, the subscripts of which indicate timestep t, the superscripts of which indicate operations at word level (w) or sequence level (s). hst (encode) specifies encoding stage and hst (decode) specifies decoding stage.\n• ewt and est denotes word-level and sentencelevel embedding for word and sentence at position t in terms of its residing sentence or document."
    }, {
      "heading" : "3.2 Model 1: Standard LSTM",
      "text" : "The whole input and output are treated as one sequence of tokens. Following Sutskever et al. (2014) and 0), we trained an autoencoder that first maps input documents into vector representations from a LSTMencode and then reconstructs inputs by predicting tokens within the document sequentially from a LSTMdecode. Two separate LSTMs are implemented for encoding and decoding with no sentence structures considered. Illustration is shown in Figure 1."
    }, {
      "heading" : "3.3 Model 2: Hierarchical LSTM",
      "text" : "The hierarchical model draws on the intuition that just as the juxtaposition of words creates a joint meaning of a sentence, the juxtaposition of sentences also creates a joint meaning of a paragraph or a document.\nEncoder We first obtain representation vectors at the sentence level by putting one layer of LSTM (denoted as LSTMwordencode) on top of its containing words:\nhwt (encode) = LSTM word encode(e w t , h w t−1(encode))\n(4) The vector output at the ending time-step is used to represent the entire sentence as\nes = h w ends\nTo build representation eD for the current document/paragraph D, another layer of LSTM (denoted as LSTM sentenceencode ) is placed on top of all sentences, computing representations sequentially for each timestep:\nhst (encode) = LSTM sentence encode (e s t , h s t−1(encode))\n(5) Representation esendD computed at the final time step is used to represent the entire document: eD = h\ns endD . Thus one LSTM operates at the token level, leading to the acquisition of sentence-level representations that are then used as inputs into the second LSTM that acquires document-level representations, in a hierarchical structure.\nDecoder As with encoding, the decoding algorithm operates on a hierarchical structure with two layers of LSTMs. LSTM outputs at sentence level for time step t are obtained by:\nhst (decode) = LSTM sentence decode (e s t , h s t−1(decode))\n(6) The initial time step hs0(d) = eD, the end-to-end output from the encoding procedure. hst (d) is used as the original input into LSTMworddecode for subsequently predicting tokens within sentence t + 1. LSTMworddecode predicts tokens at each position sequentially, the embedding of which is then combined with earlier hidden vectors for the next timestep prediction until the ends token is predicted. The procedure can be summarized as follows:\nhwt (decode) = LSTM sentence decode (e w t , h w t−1(decode))\n(7) p(w|·) = softmax(ew, hwt−1(decode)) (8) During decoding, LSTMworddecode generates each word token w sequentially and combines it with earlier LSTM-outputted hidden vectors. The LSTM hidden vector computed at the final time step is used to represent the current sentence.\nThis is passed to LSTM sentencedecode , combined with hst for the acquisition of ht+1, and outputted to the next time step in sentence decoding.\nFor each timestep t, LSTM sentencedecode has to first decide whether decoding should proceed or come to a full stop: an additional binary softmax function is therefore added. We associate each timestep with an additional “endD” or “notendD” token, which is represented as a vector representation eendD and enotendD . Decoding terminates when the endD token is generated by LSTM sentencedecode . Details are shown in Figure 2."
    }, {
      "heading" : "3.4 Model 3: Hierarchical LSTM with Attention",
      "text" : "Attention models adopt a look-back strategy by linking the current decoding stage with input sentences in an attempt to consider which part of the\ninput is most responsible for the current decoding state. This attention version of hierarchical model is inspired by similar work in image caption generation and machine translation (Xu et al., 2015; 0).\nLet H = {hs1(e), hs2(e), ..., hsN (e)} be the\ncollection of sentence-level hidden vectors for each sentence from the inputs, outputted from LSTMSentenceencode . Each element in H contains information about input sequences with a strong focus on the parts surrounding each specific sentence (time-step). During decoding, attention models would first link the current-step decoding information, i.e., hst (decode) which is outputted from LSTM sentencedecode with each of the input sentences i ∈ [1, N ], characterized by a strength indicator vi:\nvi = U T f(W1 · hst (decode) +W2 · hsi (encode))\n(9) W1,W2 ∈ RK×K , U ∈ RK×1. vi is then normalized:\nai = exp(vi)∑ i′ exp(v ′ i)\n(10)\nThe attention vector is then created by averaging weights over all input sentences:\nct = ∑\ni∈[1,ND]\naih s i (encode) (11)\nDecoding of the current step is then based on the vector rt, by combining the attention vector ct and hst (decode)\nrt = f(W3 · hst (d) +W2 · ct) (12)\nwhere W3,W4 ∈ RK×K . rt is then used for word predicting as in the vanilla version of the hierarchical model."
    }, {
      "heading" : "3.5 Training and Testing",
      "text" : "Parameters are estimated by maximizing likelihood of outputs given inputs, similar to standard sequence-to-sequence models. A softmax function is adopted for predicting each token within output documents, the error of which is first backpropagated through LSTMworddecode to sentences, then through LSTM sentencedecode to document representation eD, and last through LSTM sentenceencode and LSTMwordencode to inputs. Stochastic gradient descent with minibatches is adopted.\nFor testing, we adopt a greedy strategy with no beam search. For a given document D, eD is first obtained given already learned LSTMencode parameters and word embeddings. Then in decoding, LSTM sentencedecode computes embeddings at each sentence-level time-step, which is first fed into the binary classifier to decide whether sentence decoding terminates and then into LSTMworddecode for word decoding."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We implement the proposed autoencoder on two datasets, a highly domain specific dataset consisting of hotel reviews and a general dataset extracted from Wkipedia.\nHotel Reviews We use a subset of hotel reviews crawled from TripAdvisor. We consider only reviews consisting sentences ranging from 50 to 250 words; the model has problems dealing with extremely long sentences, as we will discuss later. We keep a vocabulary set consisting of the 25,000 most frequent words. A special “<unk>” token is used to denote all the remaining less frequent tokens. Reviews that consist of more than 2 percent of unknown words are discarded. Our training dataset is comprised of roughly 340,000 reviews; the testing set is comprised of 40,000 reviews. Dataset details are shown in Table 1.\nWikipedia We extracted paragraphs from Wikipedia corpus that meet the aforementioned length requirements. We keep a top frequent vocabulary list of 120,000 words. Paragraphs with larger than 4 percent of unknown words are discarded. The training dataset is comprised of roughly 500,000 paragraphs and testing contains roughly 50,000."
    }, {
      "heading" : "4.2 Training Details and Implementation",
      "text" : "Previous research has shown that deep LSTMs work better than shallow ones for sequence-tosequence tasks (Vinyals et al., 2014; Sutskever et al., 2014). We adopt a LSTM structure with four layer for encoding and four layer for decoding, each of which is comprised of a different set of parameters. Each LSTM layer consists of 1,000 hidden neurons and the dimensionality of word embeddings is set to 1,000. Other training details are given below, some of which follow Sutskever et al. (2014).\n• Input documents are reversed. • LSTM parameters and word embeddings are\ninitialized from a uniform distribution between [-0.08, 0.08]. • Stochastic gradient decent is implemented\nwithout momentum using a fixed learning rate of 0.1. We stated halving the learning rate every half epoch after 5 epochs. We trained our models for a total of 7 epochs. • Batch size is set to 32 (32 documents). • Decoding algorithm allows generating at\nmost 1.5 times the number of words in inputs. • 0.2 dropout rate. • Gradient clipping is adopted by scaling gra-\ndients when the norm exceeded a threshold of 5.\nOur implementation on a single GPU3 processes a speed of approximately 600-1,200 tokens per second. We trained our models for a total of 7 iterations, which roughly takes about 2 weeks to run for standard sequence-to-sequence models and 3 weeks for hierarchical models on hotel dataset and 3-5 weeks for Wikipedia dataset."
    }, {
      "heading" : "4.3 Evaluations",
      "text" : "We need to measure the closeness of the output (candidate) to the input (reference). We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al., 2002).\nROUGE is a recall-oriented measure widely used in the summarization literature. It measures the n-gram recall between the candidate text and the reference text(s). In this work, we only have one reference document (the input document) and ROUGE score is therefore given by:\nROUGEn =\n∑ gramn∈input\ncountmatch(gramn)∑ gramn∈input\ncount(gramn) (13)\nwhere countmatch denotes the number of n-grams co-occurring in the input and output. We report ROUGE-1, 2 and W (based on weighted longest common subsequence).\nBLEU Purely measuring recall will inappropriately reward long outputs. BLEU is designed to address such an issue by emphasizing precision. n-gram precision scores for our situation are given\n3Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores.\nby:\nprecisionn =\n∑ gramn∈output\ncountmatch(gramn)∑ gramn∈output\ncount(gramn) (14)\nBLEU then combines the average logarithm of precision scores with exceeded length penalization. For details, see Papineni et al. (2002).\nCoherence Evaluation Neither BLEU nor ROUGE attempts to evaluate true coherence. There is no generally accepted and readily available coherence evaluation metric.4 Because of the difficulty of developing a universal coherence evaluation metric, we proposed here only a tailored metric specific to our case. Based on the assumption that human-generated texts (i.e., input documents in our tasks) are coherent (Barzilay and Lapata, 2008), we compare generated outputs with input documents in terms of how much original text order is preserved.\nWe develop a grid evaluation metric similar to the entity transition algorithms in (Barzilay and Lee, 2004; Lapata and Barzilay, 2005). The key idea of Barzilay and Lapata’s models is to first identify grammatical roles (i.e., object and subject) that entities play and then model the transition probability over entities and roles across sentences. We represent each sentence as a featurevector consisting of verbs and nouns in the sentence. Next we align sentences from output documents to input sentences based on sentence-tosentence F1 scores (precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors. Note that multiple output sentences can be matched to one input sentence. Assume that sentence sioutput is aligned with sentence si\n′ input, where i and i ′ denote position index for a output sentence and its aligned input. The penalization score L is then given by:\nL = 2\nNoutput · (Noutput − 1) × ∑\ni∈[1,Noutput−1] ∑ j∈[i+1,Noutput] |(j − i)− (j′ − i′)|\n(15) 4Wolf and Gibson (2005) and Lin et al. (2011) proposed metrics based on discourse relations, but these are hard to apply widely since identifying discourse relations is a difficult problem. Indeed sophisticated coherence evaluation metrics are seldom adopted in real-world applications, and summarization researchers tend to use simple approximations like number of overlapped tokens or topic distribution similarity (e.g., (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-Tür, 2011)).\nEqu. 15 can be interpreted as follows: (j − i) denotes the distance in terms of position index between two outputted sentences indexed by j and i, and (j′ − i′) denotes the distance between their mirrors in inputs. As we wish to penalize the degree of permutation in terms of text order, we penalize the absolute difference between the two computed distances. This metric is also relevant to the overall performance of prediction and recall: an irrelevant output will be aligned to a random input, thus being heavily penalized. The deficiency of the proposed metric is that it concerns itself only with a semantic perspective on coherence, barely considering syntactical issues."
    }, {
      "heading" : "4.4 Results",
      "text" : "A summary of our experimental results is given in Table 3. We observe better performances for the hotel-review dataset than the open domain Wikipedia dataset, for the intuitive reason that documents and sentences are written in a more fixed format and easy to predict for hotel reviews.\nThe hierarchical model that considers sentencelevel structure outperforms standard sequenceto-sequence models. Attention models at the sentence level introduce performance boost over vanilla hierarchical models.\nWith respect to the coherence evaluation, the original sentence order is mostly preserved: the hierarchical model with attention achieves L = 1.57\non the hotel-review dataset, equivalent to the fact that the relative position of two input sentences are permuted by an average degree of 1.57. Even for the Wikipedia dataset where more poor-quality sentences are observed, the original text order can still be adequately maintained with L = 2.04."
    }, {
      "heading" : "5 Discussion and Future Work",
      "text" : "In this paper, we extended recent sequence-tosequence LSTM models to the task of multisentence generation. We trained an autoencoder to see how well LSTM models can reconstruct input documents of many sentences. We find that the proposed hierarchical LSTM models can partially preserve the semantic and syntactic integrity of multi-text units and generate meaningful and grammatical sentences in coherent order. Our model performs better than standard sequence-tosequence models which do not consider the intrinsic hierarchical discourse structure of texts.\nWhile our work on auto-encoding for larger texts is only a preliminary effort toward allowing neural models to deal with discourse, it nonetheless suggests that neural models are capable of encoding complex clues about how coherent texts are connected .\nThe performance on this autoencoder task could certainly also benefit from more sophisticated neural models. For example one extension might align the sentence currently being generated with the original input sentence (similar to sequenceto-sequence translation in (0)), and later transform the original task to sentence-to-sentence generation. However our long-term goal here is not on perfecting this basic multi-text generation scenario of reconstructing input documents, but rather on extending it to more important applications.\nThat is, the autoencoder described in this work, where input sequenceX is identical to output Y , is only the most basic instance of the family of doc-\nument (paragraph)-to-document (paragraph) generation tasks. We hope the ideas proposed in this paper can play some role in enabling such more sophisticated generation tasks like summarization, where the inputs are original documents and outputs are summaries or question answering, where inputs are questions and outputs are the actual wording of answers. Sophisticated generation tasks like summarization or dialogue systems could extend this paradigm, and could themselves benefit from task-specific adaptations. In summarization, sentences to generate at each timestep might be pre-pointed to or pre-aligned to specific aspects, topics, or pieces of texts to be summarized. Dialogue systems could incorporate information about the user or the time course of the dialogue. In any case, we look forward to more sophisticated applications of neural models to the important task of natural language generation."
    }, {
      "heading" : "6 Acknowledgement",
      "text" : "The authors want to thank Gabor Angeli, Sam Bowman, Percy Liang and other members of the Stanford NLP group for insightful comments and suggestion. We also thank the three anonymous ACL reviewers for helpful comments. This work is supported by Enlight Foundation Graduate Fellowship, and a gift from Bloomberg L.P, which we gratefully acknowledge."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Modeling local coherence: An entity-based approach",
      "author" : [ "Regina Barzilay", "Mirella Lapata." ],
      "venue" : "Computational Linguistics, 34(1):1–34.",
      "citeRegEx" : "Barzilay and Lapata.,? 2008",
      "shortCiteRegEx" : "Barzilay and Lapata.",
      "year" : 2008
    }, {
      "title" : "Discovery of topically coherent sentences for extractive summarization",
      "author" : [ "Asli Celikyilmaz", "Dilek Hakkani-Tür." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,",
      "citeRegEx" : "Celikyilmaz and Hakkani.Tür.,? 2011",
      "shortCiteRegEx" : "Celikyilmaz and Hakkani.Tür.",
      "year" : 2011
    }, {
      "title" : "Coreference-inspired coherence modeling",
      "author" : [ "Micha Elsner", "Eugene Charniak." ],
      "venue" : "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages",
      "citeRegEx" : "Elsner and Charniak.,? 2008",
      "shortCiteRegEx" : "Elsner and Charniak.",
      "year" : 2008
    }, {
      "title" : "Textlevel discourse parsing with rich linguistic features",
      "author" : [ "Vanessa Wei Feng", "Graeme Hirst." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 60–68. Association",
      "citeRegEx" : "Feng and Hirst.,? 2012",
      "shortCiteRegEx" : "Feng and Hirst.",
      "year" : 2012
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves." ],
      "venue" : "arXiv preprint arXiv:1308.0850.",
      "citeRegEx" : "Graves.,? 2013",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Hilda: a discourse parser using support vector machine classification",
      "author" : [ "Hugo Hernault", "Helmut Prendinger", "Mitsuru Ishizuka" ],
      "venue" : "Dialogue & Discourse,",
      "citeRegEx" : "Hernault et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hernault et al\\.",
      "year" : 2010
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Representation learning for text-level discourse parsing",
      "author" : [ "Yangfeng Ji", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 13–24.",
      "citeRegEx" : "Ji and Eisenstein.,? 2014",
      "shortCiteRegEx" : "Ji and Eisenstein.",
      "year" : 2014
    }, {
      "title" : "Automatic evaluation of text coherence: Models and representations",
      "author" : [ "Mirella Lapata", "Regina Barzilay." ],
      "venue" : "IJCAI, volume 5, pages 1085–1090.",
      "citeRegEx" : "Lapata and Barzilay.,? 2005",
      "shortCiteRegEx" : "Lapata and Barzilay.",
      "year" : 2005
    }, {
      "title" : "Discourse relations and defeasible knowledge",
      "author" : [ "Alex Lascarides", "Nicholas Asher." ],
      "venue" : "Proceedings of the 29th annual meeting on Association for Computational Linguistics, pages 55–62. Association for Computational Linguistics.",
      "citeRegEx" : "Lascarides and Asher.,? 1991",
      "shortCiteRegEx" : "Lascarides and Asher.",
      "year" : 1991
    }, {
      "title" : "Generating discourse structures for written texts",
      "author" : [ "Huong LeThanh", "Geetha Abeysinghe", "Christian Huyck." ],
      "venue" : "Proceedings of the 20th international conference on Computational Linguistics, page 329. Association for Computational Linguis-",
      "citeRegEx" : "LeThanh et al\\.,? 2004",
      "shortCiteRegEx" : "LeThanh et al\\.",
      "year" : 2004
    }, {
      "title" : "A model of coherence based on distributed sentence representation",
      "author" : [ "Jiwei Li", "Eduard Hovy" ],
      "venue" : null,
      "citeRegEx" : "Li and Hovy.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li and Hovy.",
      "year" : 2014
    }, {
      "title" : "The nlp engine: A universal turing machine for nlp",
      "author" : [ "Jiwei Li", "Eduard Hovy." ],
      "venue" : "arXiv preprint arXiv:1503.00168.",
      "citeRegEx" : "Li and Hovy.,? 2015",
      "shortCiteRegEx" : "Li and Hovy.",
      "year" : 2015
    }, {
      "title" : "Recursive deep models for discourse parsing",
      "author" : [ "Jiwei Li", "Rumeng Li", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2061–2069.",
      "citeRegEx" : "Li et al\\.,? 2014",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Automatic evaluation of summaries using n-gram cooccurrence statistics",
      "author" : [ "Chin-Yew Lin", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Hu-",
      "citeRegEx" : "Lin and Hovy.,? 2003",
      "shortCiteRegEx" : "Lin and Hovy.",
      "year" : 2003
    }, {
      "title" : "Automatically evaluating text coherence using discourse relations",
      "author" : [ "Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume",
      "citeRegEx" : "Lin et al\\.,? 2011",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2011
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Addressing the rare word problem in neural machine translation",
      "author" : [ "Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba." ],
      "venue" : "arXiv preprint arXiv:1410.8206.",
      "citeRegEx" : "Luong et al\\.,? 2014",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2014
    }, {
      "title" : "Rhetorical structure theory: Toward a functional theory of text organization",
      "author" : [ "William C Mann", "Sandra A Thompson." ],
      "venue" : "Text, 8(3):243–281.",
      "citeRegEx" : "Mann and Thompson.,? 1988",
      "shortCiteRegEx" : "Mann and Thompson.",
      "year" : 1988
    }, {
      "title" : "The rhetorical parsing of unrestricted texts: A surface-based approach",
      "author" : [ "Daniel Marcu." ],
      "venue" : "Computational linguistics, 26(3):395–448.",
      "citeRegEx" : "Marcu.,? 2000",
      "shortCiteRegEx" : "Marcu.",
      "year" : 2000
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Overcoming the curse of sentence length for neural machine translation using automatic segmentation",
      "author" : [ "Jean Pouget-Abadie", "Dzmitry Bahdanau", "Bart van Merriënboer", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.1257.",
      "citeRegEx" : "Pouget.Abadie et al\\.,? 2014",
      "shortCiteRegEx" : "Pouget.Abadie et al\\.",
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton." ],
      "venue" : "arXiv preprint arXiv:1412.7449.",
      "citeRegEx" : "Vinyals et al\\.,? 2014",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2014
    }, {
      "title" : "Representing discourse coherence: A corpus-based study",
      "author" : [ "Florian Wolf", "Edward Gibson." ],
      "venue" : "Computational Linguistics, 31(2):249–287.",
      "citeRegEx" : "Wolf and Gibson.,? 2005",
      "shortCiteRegEx" : "Wolf and Gibson.",
      "year" : 2005
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1502.03044.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Timeline generation through evolutionary trans-temporal summarization",
      "author" : [ "Rui Yan", "Liang Kong", "Congrui Huang", "Xiaojun Wan", "Xiaoming Li", "Yan Zhang." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Yan et al\\.,? 2011a",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2011
    }, {
      "title" : "Evolutionary timeline summarization: a balanced optimization framework via iterative substitution",
      "author" : [ "Rui Yan", "Xiaojun Wan", "Jahna Otterbacher", "Liang Kong", "Xiaoming Li", "Yan Zhang." ],
      "venue" : "Proceedings of the 34th international ACM SIGIR con-",
      "citeRegEx" : "Yan et al\\.,? 2011b",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al.",
      "startOffset" : 120,
      "endOffset" : 145
    }, {
      "referenceID" : 10,
      "context" : "A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al.",
      "startOffset" : 181,
      "endOffset" : 209
    }, {
      "referenceID" : 7,
      "context" : "Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in",
      "startOffset" : 19,
      "endOffset" : 53
    }, {
      "referenceID" : 23,
      "context" : "sequence generation tasks like machine translation (Sutskever et al., 2014; 0; Luong et al., 2014) or parsing (Vinyals et al.",
      "startOffset" : 51,
      "endOffset" : 98
    }, {
      "referenceID" : 18,
      "context" : "sequence generation tasks like machine translation (Sutskever et al., 2014; 0; Luong et al., 2014) or parsing (Vinyals et al.",
      "startOffset" : 51,
      "endOffset" : 98
    }, {
      "referenceID" : 24,
      "context" : ", 2014) or parsing (Vinyals et al., 2014).",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 7,
      "context" : "LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x1, x2, .",
      "startOffset" : 12,
      "endOffset" : 46
    }, {
      "referenceID" : 23,
      "context" : "For beam search, (Sutskever et al., 2014) discovered that a beam size of 2 suffices to provide",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 24,
      "context" : "Previous research has shown that deep LSTMs work better than shallow ones for sequence-tosequence tasks (Vinyals et al., 2014; Sutskever et al., 2014).",
      "startOffset" : 104,
      "endOffset" : 150
    }, {
      "referenceID" : 23,
      "context" : "Previous research has shown that deep LSTMs work better than shallow ones for sequence-tosequence tasks (Vinyals et al., 2014; Sutskever et al., 2014).",
      "startOffset" : 104,
      "endOffset" : 150
    }, {
      "referenceID" : 23,
      "context" : ", 2014; Sutskever et al., 2014). We adopt a LSTM structure with four layer for encoding and four layer for decoding, each of which is comprised of a different set of parameters. Each LSTM layer consists of 1,000 hidden neurons and the dimensionality of word embeddings is set to 1,000. Other training details are given below, some of which follow Sutskever et al. (2014).",
      "startOffset" : 8,
      "endOffset" : 371
    }, {
      "referenceID" : 17,
      "context" : "We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al.",
      "startOffset" : 54,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al.",
      "startOffset" : 54,
      "endOffset" : 85
    }, {
      "referenceID" : 21,
      "context" : "We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al., 2002).",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 21,
      "context" : "For details, see Papineni et al. (2002).",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : ", input documents in our tasks) are coherent (Barzilay and Lapata, 2008), we compare generated outputs with input documents in terms of how much",
      "startOffset" : 45,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "We develop a grid evaluation metric similar to the entity transition algorithms in (Barzilay and Lee, 2004; Lapata and Barzilay, 2005).",
      "startOffset" : 83,
      "endOffset" : 134
    }, {
      "referenceID" : 28,
      "context" : ", (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-Tür, 2011)).",
      "startOffset" : 2,
      "endOffset" : 75
    }, {
      "referenceID" : 27,
      "context" : ", (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-Tür, 2011)).",
      "startOffset" : 2,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : ", (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-Tür, 2011)).",
      "startOffset" : 2,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "Wolf and Gibson (2005) and Lin et al. (2011) proposed metrics based on discourse relations, but these are hard to apply widely since identifying discourse relations is a difficult problem.",
      "startOffset" : 27,
      "endOffset" : 45
    } ],
    "year" : 2015,
    "abstractText" : "Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Longshort term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1 2.",
    "creator" : "TeX"
  }
}