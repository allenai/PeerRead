{
  "name" : "1503.05034.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "genCNN: A Convolutional Architecture for Word Sequence Prediction",
    "authors" : [ "Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Wenbin Jiang", "Qun Liu" ],
    "emails" : [ "wangmingxuan@ict.ac.cn", "jiangwenbin@ict.ac.cn", "liuqun@ict.ac.cn", "Lu.Zhengdong@huawei.com", "HangLi.HL@huawei.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Both language modeling (Wu and Khudanpur, 2003; Mikolov et al., 2010; Bengio et al., 2003) and text generation (Axelrod et al., 2011) boil down to modeling the conditional probability of a word given the proceeding words. Previously, it is mostly done through purely memory-based approaches, such as n-grams, which cannot deal with long sequences and has to use some heuristics (called smoothing) for rare ones. Another family of methods are based on distributed representations of words, which is usually tied with a neural-network (NN) architecture for estimating the conditional probabilities of words.\nTwo categories of neural networks have been used for language modeling: 1) recurrent neural networks (RNN), and 2) feedfoward network (FFN): • The RNN-based models, including its variants like LSTM, enjoy more popularity, mainly due\nto their flexible structures for processing word sequences of arbitrary lengths, and their recent empirical success(Sutskever et al., 2014; Graves, 2013). We however argue that RNNs, with their power built on the recursive use of a relatively simple computation units, are forced to make greedy summarization of the history and consequently not efficient on modeling word sequences, which clearly have a bottom-up structures. • The FFN-based models, on the other hand, avoid this difficulty by directly taking the history as\ninput. However the FFNs are fully-connected networks, rendering them inefficient on capturing local structures of languages. Moreover their “rigid” architectures make it futile to handle the great variety of patterns in long range correlations of words. We propose a novel convolutional neural network architecture, named genCNN, for efficiently combining local and long range structures of language with the purpose of modeling conditional probabilities. genCNN can be directly used in generating a word sequence (i.e., text generation) or evaluating\nar X\niv :1\n50 3.\n05 03\n4v 1\n[ cs\n.C L\n] 1\n7 M\nar 2\n01 5\nthe likelihood of a word sequence (i.e., language modeling). We also show the empirical superiority of genCNN on both tasks over traditional n-grams and its RNN and FFN counterparts.\nNotations: We use V to denote the vocabulary, et (∈ {1, · · · , |V|}) to denote the tth word in a sequence e1:T def = [e1, · · · , eT ], and e(n)t if the sequence itself is further indexed by n."
    }, {
      "heading" : "2 Overview",
      "text" : "As shown in Figure 1, genCNN is overall recursive, consisting of CNN-based processing units of two types:\n• αCNN as the “front-end”, dealing with the history that is closest to the prediction;\n• βCNNs (which can repeat), in charge of more “ancient” history.\nTogether, genCNN takes history e1:t of arbitrary length to predict the next word et+1 with probability\np(et+1 |e1:t; Θ̄), (1)\nbased on a representation φ(e1:t; Θ̄) produced by the CNN, and a |V|-class soft-max:\np(et+1|e1:t; Θ̄) ∝ eµ > et+1 φ(e1:t)+bet+1 . (2)\ngenCNN is fully tailored for modeling the sequential structure in natural language, notably different from conventional CNN (Lawrence et al., 1997; Hu et al., 2014) in 1) its specifically designed weightssharing strategy (in αCNN), 2) its gating design, and 3) certainly its recursive architectures. Also distinct from RNN, genCNN gains most of its processing power from the heavy-duty processing units (i.e.,αCNN and βCNNs), which follow a bottom-up information flow and yet can adequately capture the temporal structure in word sequence with its convolutional-gating architecture.\n3 genCNN: Architecture\nWe start with discussing the convolutional architecture of αCNN as a stand-alone sentence model, and then proceed to the recursive structure. After that we give a comparative analysis on the mechanism of genCNN. αCNN, just like a normal CNN, has fixed architecture with predefined maximum words (denoted as Lα). History shorter than Lα will filled with zero paddings, and history longer than that will be folded to feed to βCNN after it, as will be elaborated in Section 3.3. Similar to most other CNNs, αCNN alternates between convolution layers and pooling layers, and finally a fully connected layer to reach the representation before soft-max, as illustrated by Figure 2. Unlike the toyish example in Figure 2,\nin practice we use a larger and deeper αCNN with Lα = 30 or 40, and two or three convolution layers (see Section 4.1).\nIn Section 3.1 we will introduce the hybrid design of convolution in genCNN for capturing structures of different nature in word sequence prediction. In Section 3.2, we will discuss the design of gating mechanism."
    }, {
      "heading" : "3.1 αCNN: Convolution",
      "text" : "Different from conventional CNN, the weights of convolution units in αCNN is only partially shared. More specifically, in the convolution units there are two types feature-maps: TIME-FLOW and the TIME-ARROW, illustrated respectively with the unfilled nodes and filled nodes in Figure 2. The parameters for TIME-FLOW are shared among different convolution units, while for TIME-ARROW the parameters are location-dependent. Intuitively, TIME-FLOW acts more like a conventional CNN (e.g., that in (Hu et al., 2014)), aiming to understand the overall temporal structure in the word sequences; TIME-ARROW, on the other hand, works more like a traditional NN-based language model (Vaswani et al., 2013; Bengio et al., 2003): with its location-dependent parameters, it focuses on capturing the prediction task and the direction of time.\nFor sentence input x={x1, · · · ,xT }, the feature-map of type-f on Layer-` is if f ∈ TIME-FLOW:\nz (`,f) i (x) = σ(w (`,f) TF ẑ (`−1) i + b (`,f) TF ), (3)\nif f ∈ TIME-ARROW: z\n(`,f) i (x) = σ(w (`,f,i) TA ẑ (`−1) i + b (`,f,i) TA ), (4)\nwhere\n• z(`,f)i (x) gives the output of feature-map of type-f for location i in Layer-`;\n• σ(·) is the activation function, e.g., Sigmoid or Relu (Dahl et al., 2013)\n• (w(`,f)TF ,b (`,f) TF ) denotes the location-independent parameters for f ∈ TIME-FLOW on Layer-`,\nwhile (w(`,f,i)TA , b (`,f,i) TA ) stands for that for f ∈TIME-ARROW and location i on Layer-`;\n• ẑ(`−1)i denotes the segment of Layer-`−1 for the convolution at location i , while\nẑ (0) i def = [x>i , x > i+1, · · · , x>i+k1−1] >\nconcatenates the vectors for k1 words from sentence input x."
    }, {
      "heading" : "3.2 Gating Network",
      "text" : "Previous CNNs, including those for NLP tasks (Hu et al., 2014; Kalchbrenner et al., 2014), take a straightforward convolution-pooling strategy, in which the “fusion” decisions (e.g., selecting the largest one in max-pooling) are made based on the values of feature-maps. This is essentially a soft template matching (Lawrence et al., 1997), which works for tasks like classification, but undesired for maintaining the composition functionality of convolution. In this paper, we propose to use separate gating networks to release the scoring duty from the convolution, and let it focus on composition. Similar idea has been proposed by (Socher et al., 2011) for recursive neural networks on parsing tasks, but never been combined with a convolutional architecture.\nSuppose we have convolution feature-maps on Layer-` and gating (with window size = 2) on Layer`+1. For the jth gating window (2j−1, 2j), we merge ẑ(`−1)2j−1 and ẑ (`−1) 2j as the input (denoted as z̄ (`) j ) for gating network, as illustrated in Figure 3. We use a separate gate for each feature-map, but follow a different parametrization strategy for TIME-FLOW and TIME-ARROW. With window size = 2, the gating is binary, we use a logistic regressor to determine the weights of two candidates. For f∈TIME-ARROW, with location-dependent w(`,f,j)gate , the normalized weight for left side is\ng (`+1,f) j = 1/(1 + e\n−w(`,f,j)gate z̄ (`) j ),\nwhile for For f∈TIME-FLOW, the parameters for the corresponding gating network, denoted as w(`,f)gate , are shared. The gated feature map is then a weighted sum to feature-maps from the two windows:\nz (`+1,f) j = g (`+1,f) j z (`,f) 2j−1 + (1− g (`+1,f) j )z (`,f) 2j . (5)\nWe find that this gating strategy works significantly better than direct pooling over feature-maps, and also slightly better than a hard gate version of Equation (5)."
    }, {
      "heading" : "3.3 Recursive Architecture",
      "text" : "As suggested early on in Section 2 and Figure 1, we use extra CNNs with conventional weight-sharing, named βCNN, to summarize the history out of scope of αCNN. More specifically, the output of βCNN (with the same dimension of word-embedding) is put before the first word as the input to the αCNN, as illustrated in Figure 4. Different from αCNN, βCNN is designed just to summarize the history, with weight shared across its convolution units. In a sense, βCNN has only TIME-FLOW feature-maps. All βCNN are identical and recursively aligned, enabling genCNN to handle sentences with arbitrary length. We put a special switch after each βCNN to turn it off (replacing a pading vector shown as “/” in Figure 4) when there is no history assigned to it. As the result, when the history is shorter than Lα, the recursive structure reduces to αCNN.\nIn practice, 90+% sentences can be modeled by αCNN with Lα = 40 and 99+% sentences can be contained with one extra βCNN. Our experiment shows that this recursive strategy yields better\nestimate of conditional density than neglecting the out-of-scope history (Section 6.1.2). In practice, we found that a larger (greater Lα) and deeper αCNN works better than small αCNN and more recursion of βCNN, which is consistent with our intuition that the bottom-up convolutional architecture is well suited for modeling the sequence."
    }, {
      "heading" : "3.4 Analysis",
      "text" : ""
    }, {
      "heading" : "3.4.1 TIME-FLOW vs. TIME-ARROW",
      "text" : "Both conceptually and systemically, genCNN gives two interweaved treatments of word history. With the globally-shared parameters in the convolution units, TIME-FLOW summarizes what has been said. The hierarchical convolution+gating architecture in TIME-FLOW enables it to model the composition in language, yielding representation of segments at different intermediate layers. TIME-FLOW is aware of the sequential direction, inherited from the space-awareness of CNN, but it is not sensitive enough about the prediction task, due to the uniform weights in the convolution.\nOn the other hand, TIME-ARROW, living in location-dependent parameters of convolution units, acts like an arrow pin-pointing the prediction task. TIME-ARROW has predictive power all by itself, but it concentrates on capturing the direction of time and consequently short on modelling the longrange dependency.\nTIME-FLOW and TIME-ARROW have to work together for optimal performance in predicting what is going to be said. This intuition has been empirically verified, as our experiments have demonstrated that TIME-FLOW or TIME-ARROW alone perform inferiorly. One can imagine, through the layerby-layer convolution and gating, the TIME-ARROW gradually picks the most relevant part from the representation of TIME-FLOW for the prediction task, even if that part is long distance ahead.\n3.4.2 genCNN vs. RNN-LM Different from RNNs, which recursively applies a relatively simple processing units, genCNN gains its ability on sequence modeling mostly from its flexible and powerful bottom-up and convolution architecture. genCNN takes the “uncompressed” history, therefore avoids\n• the difficulty in finding the representation for history (i.e., unfinished sentences), especially those end in the middle of a chunk (e.g.,“the cat sat on the”),\n• the damping effort in RNN when the history-summarizing hidden states are updated at each time, which renders the long term memory rather difficult.\nBoth drawbacks can only be partially ameliorated with complicated design of gates (Hochreiter and Schmidhuber, 1997) and or more heavy processing units (essentially a fully connected DNN) (Sutskever et al., 2014).\n4 genCNN: Training\nThe parameters of a genCNN Θ̄ consists of the parameters for CNN Θnn, word-embedding Θembed, and the parameters for soft-max Θsoftmax. All the parameters are jointly learned by maximizing the likelihood of observed sentences. Formally the log-likelihood of sentence Sn ( def= [e(n)1 , e (n) 2 , · · · , e (n) Tn ]) is\nlog p(Sn; Θ̄) = Tn∑ t=1 log p(e (n) t |e (n) 1:t−1; Θ̄),\nwhich can be trivially split into Tn training instances during the optimization, in contrast to the training of RNN that requires unfolding through time due to the temporal-dependency of the hidden states."
    }, {
      "heading" : "4.1 Implementation Details",
      "text" : "Architectures: In all of our experiments (Section 5 and 6) we set the maximum words for αCNN to be 30 and that for βCNN to be 20. αCNN have two convolution layers (both containing TIMEFLOW and TIME-ARROW convolution) and two gating layers, followed by a fully connected layer (400 dimension) and then a soft-max layer. The numbers of feature-maps for TIME-FLOW are respectively 150 (1st convolution layer) and 100 (2nd convolution layer), while TIME-ARROW has the same feature-maps. βCNN is relatively simple, with two convolution layer containing only TIME-FLOW with 150 feature-maps, two gating layers and a fully connected layer. We use ReLU as the activation function for convolution layers and switch to Sigmoid for fully connected layers. We use word embedding with dimension 100.\nSoft-max: Calculating a full soft-max is expensive since it has to enumerate all the words in vocabulary (in our case 40K words) in the denominator. Here we take a simple hierarchical approximation of it, following (Bahdanau et al., 2014). Basically we group the words into 200 clusters (indexed by cm), and factorize (in an approximate sense) the conditional probability of a word p(et|e1:t−1; Θ̄) into the probability of its cluster and the probability of et given its cluster\np(cm|e1:t−1; Θ̄) p(et|cm; Θsoftmax).\nWe found that this simple heuristic can speed-up the optimization by 5 times with only slight loss of accuracy.\nOptimization: We use stochastic gradient descent with mini-batch (size 500) for optimization, aided further by AdaGrad (Duchi et al., 2011). For initialization, we use Word2Vec (Mikolov et al., 2013) for the starting state of the word-embeddings (trained on the same dataset as the main task), and set all the other parameters by randomly sampling from uniform distribution in [−0.1, 0.1]. The optimization is done mainly on a Tesla K40 GPU, which takes about 2 days for the training on a dataset containing 1M sentences."
    }, {
      "heading" : "5 Experiments: Sentence Generation",
      "text" : "In this experiment, we randomly generate sentences by recurrently sampling\ne?t+1 ∼ p(et+1|e1:t; Θ̄),\nand put the newly generated word into history, until EOS (end-of-sentence) is generated. We consider generating two types of sentences: 1) the plain sentences, and 2) sentences with dependency parsing, which will be covered respectively in Section 5.1 and 5.2."
    }, {
      "heading" : "5.1 Natural Sentences",
      "text" : "We train genCNN on Wiki data with 112M words for one week, with some representative examples randomly generated given in Table 1 (upper and middle blocks). We try two settings, by asking genCNN to 1) finish a sentence started by human (upper block), or 2) generate a sentence from the beginning (middle block), or It is fairly clear that most of the time genCNN can generate sentences that are syntactically grammatical and semantically meaningful. More specifically, most of the sentences can be aligned to a parse tree with reasonable structure. It is also worth noting that quotation marks (‘‘ and ’’) are always generated in pairs and in the correct order, even across a relatively long distance, as exemplified by the first sentence in the upper block."
    }, {
      "heading" : "5.2 Sentences with Dependency Tags",
      "text" : "For training, we first parse(Klein and Manning, 2002) the English sentences and feed sequences with dependency tags as follows\n( I ? like ( red ? apple ) )\nto genCNN, where 1) each paired parentheses contain a subtree, and 2) the symbol “?” indicates that the word next to it is the dependency head in the corresponding sub-tree. Some representative examples generated by genCNN are given in Table 1 (bottom block). As it suggests, genCNN is fairly accurate on respecting the rules of parentheses, and probably more remarkably, it can get the dependency tree head correct most of the time."
    }, {
      "heading" : "6 Experiments: Language Modeling",
      "text" : "We evaluate our model as a language model in terms of both perplexity (Brown et al., 1992) and its efficacy in re-ranking the n-best candidates from state-of-the-art models in statistical machine translation, both with comparison to the following competitor language models.\nCompetitor Models we compare genCNN to the following competitor models\n• 5-gram: We use SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 5-gram language model with modified Kneser-Ney smoothing;\n• FFN-LM: The neural language model based on feedfoward network (Vaswani et al., 2013). We vary the input window-size from 5 to 20, while the performance stops improving after window size 20;\n• RNN: we use the implementation1 of RNN-based language model with hidden size 600 for optimal performance of it;\n• LSTM: we use the code in Groundhog2, but vary the hyper-parameters, including the depth and word-embedding dimension, for best performance. LSTM (Hochreiter and Schmidhuber, 1997) is widely considered to be the state-of-the-art for sequence modeling."
    }, {
      "heading" : "6.1 Perplexity",
      "text" : "We test the performance of genCNN on PENN TREEBANK and FBIS, two public datasets with different sizes.\n6.1.1 On PENN TREEBANK Although a relatively small dataset 3, PENN TREEBANK is widely used as a language modelling benchmark (Graves, 2013; Mikolov et al., 2010). It has 930, 000 words in training set, 74, 000 words in validation set, and 82, 000 words in test set. We use exactly the same settings as in (Mikolov et al., 2010), with a 10, 000-words vocabulary (all out-of-vocabulary words are replaced with unknown) and end-of-sentence token (EOS). In addition to the conventional testing strategy where the models are kept unchanged during testing, Mikolov et al. (2010) proposes to also update the parameters in an online fashion when seeing test sentences. This new way of testing, named “dynamic evaluation”, is also adopted by Graves (2013).\nFrom Table 2, genCNN manages to give perplexity superior in both metrics, with about 25 point reduction over the widely used 5-gram, and over 10 point reduction from LSTM, the state-of-the-art and the second-best performer. We defer the comparison of genCNN variants to next experiment on a larger dataset (FBIS), since PENN TREEBANK is too small for evaluating some of the differences between them."
    }, {
      "heading" : "6.1.2 On FBIS",
      "text" : "The FBIS corpus (LDC2003E14) is relatively large, with 22.5K sentences and 8.6M English words. The validation set is NIST MT06 and test set is NIST MT08. For training the neural network, we limit the vocabulary to the most frequent 40,000 words, covering∼99.4% of the corpus. Similar to the first experiment, all out-of-vocabulary words are replaced with unknown and the EOS token is counted in the sequence loss.\n1http://rnnlm.org/ 2https://github.com/lisa-groundhog/GroundHog 3http://www.fit.vutbr.cz/∼imikolov/rnnlm/simple-examples.tgz\nFrom Table 3 (upper block), genCNN clearly wins again in the comparison to competitors, with over 25 point margin over LSTM (in its optimal setting), the second best performer. Interestingly genCNN outperforms its variants also quite significantly (bottom block): 1) with only TIME-ARROW (same number of feature-maps), the performance deteriorates considerably for losing the ability of capturing long range correlation reliably; 2) with only TIME-FLOW the performance gets even worse, for partially losing the sensitivity to the prediction task. It is quite remarkable that, although αCNN (with Lα = 30) can achieve good results, the recursive structure in full genCNN can further decrease the perplexity by over 3 points, indicating that genCNN can benefit from modeling the dependency over range as long as 30 words."
    }, {
      "heading" : "6.2 Re-ranking for Machine Translation",
      "text" : "In this experiment, we re-rank the 1000-best English translation candidates for Chinese sentences generated by statistical machine translation (SMT) system, and compare it with other language models in the same setting.\nSMT setup The baseline hierarchical phrase-based SMT system ( Chines→ English) was built using Moses, a widely accepted state-of-the-art, with default settings. The bilingual training data is from NIST MT2012 constrained track, with reduced size of 1.1M sentence pairs using selection strategy in (Axelrod et al., 2011). The baseline use conventional 5-gram language model (LM), estimated with modified Kneser-Ney smoothing (Chen and Goodman, 1996) on the English side of the 329Mword Xinhua portion of English Gigaword(LDC2011T07). We also try FFN-LM, as a much stronger language model in decoding. The weights of all the features are tuned via MERT (Och and Ney, 2002) on NIST MT05, and tested on NIST MT06 and MT08. Case-insensitive NIST BLEU4 is used in evaluation.\nRe-ranking with genCNN significantly improves the quality of the final translation. Indeed, it can increase the BLEU score by over 1.33 point over Moses baseline on average. This boosting force barely slacks up on translation with a enhanced language model in decoding: genCNN re-ranker still achieves 1.29 point improvement on top of Moses with FFN-LM, which is 1.76 point over the Moses (default setting). To see the significance of this improvement, the state-of-the-art Neural Network Joint Model (Devlin et al., 2014) usually brings less than one point increase on this task.\n4ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl"
    }, {
      "heading" : "7 Related Work",
      "text" : "In addition to the long thread of work on neural network based language model (Auli et al., 2013; Mikolov et al., 2010; Graves, 2013; Bengio et al., 2003; Vaswani et al., 2013), our work is also related to the effort on modeling long range dependency in word sequence prediction(Wu and Khudanpur, 2003). Different from those work on hand-crafting features for incorporating long range dependency, our model can elegantly assimilate relevant information in an unified way, in both long and short range, with the bottom-up information flow and convolutional architecture.\nCNN has been widely used in computer vision and speech (Lawrence et al., 1997; Krizhevsky et al., 2012; LeCun and Bengio, 1995; Abdel-Hamid et al., 2012), and lately in sentence representation(Kalchbrenner and Blunsom, 2013), matching(Hu et al., 2014) and classification(Kalchbrenner et al., 2014). To our best knowledge, it is the first time this is used in word sequence prediction. Modelwise the previous work that is closest to genCNN is the convolution model for predicting moves in the Go game (Maddison et al., 2014), which, when applied recurrently, essentially generates a sequence. Different from the conventional CNN taken in (Maddison et al., 2014), genCNN has architectures designed for modeling the composition in natural language and the temporal structure of word sequence."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We propose a convolutional architecture for natural language generation and modeling. Our extensive experiments on sentence generation, perplexity, and n-best re-ranking for machine translation show that our model can significantly improve upon state-of-the-arts."
    } ],
    "references" : [ {
      "title" : "Hui Jiang",
      "author" : [ "Ossama Abdel-Hamid", "Abdel-rahman Mohamed" ],
      "venue" : "and Gerald Penn.",
      "citeRegEx" : "Abdel.Hamid et al.2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Chris Quirk",
      "author" : [ "Michael Auli", "Michel Galley" ],
      "venue" : "and Geoffrey Zweig.",
      "citeRegEx" : "Auli et al.2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Xiaodong He",
      "author" : [ "Amittai Axelrod" ],
      "venue" : "and Jianfeng Gao.",
      "citeRegEx" : "Axelrod et al.2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Kyunghyun Cho",
      "author" : [ "Dzmitry Bahdanau" ],
      "venue" : "and Yoshua Bengio.",
      "citeRegEx" : "Bahdanau et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Pascal Vincent",
      "author" : [ "Yoshua Bengio", "Rjean Ducharme" ],
      "venue" : "and Christian Jauvin.",
      "citeRegEx" : "Bengio et al.2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "and Jennifer C",
      "author" : [ "Peter F. Brown", "Vincent J. Della Pietra", "Robert L. Mercer", "Stephen A. Della Pietra" ],
      "venue" : "Lai.",
      "citeRegEx" : "Brown et al.1992",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "An empirical study of smoothing techniques for language modeling",
      "author" : [ "Chen", "Goodman1996] Stanley F Chen", "Joshua Goodman" ],
      "venue" : "In Proceedings of the 34th annual meeting on Association for Computational Linguistics,",
      "citeRegEx" : "Chen et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 1996
    }, {
      "title" : "and Geoffrey E",
      "author" : [ "George E Dahl", "Tara N Sainath" ],
      "venue" : "Hinton.",
      "citeRegEx" : "Dahl et al.2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Richard Schwartz",
      "author" : [ "Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar" ],
      "venue" : "and John Makhoul.",
      "citeRegEx" : "Devlin et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Elad Hazan",
      "author" : [ "John Duchi" ],
      "venue" : "and Yoram Singer.",
      "citeRegEx" : "Duchi et al.2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850",
      "author" : [ "Alex Graves" ],
      "venue" : null,
      "citeRegEx" : "Graves.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Hang Li",
      "author" : [ "Baotian Hu", "Zhengdong Lu" ],
      "venue" : "and Qingcai Chen.",
      "citeRegEx" : "Hu et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom" ],
      "venue" : "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2013
    }, {
      "title" : "Edward Grefenstette",
      "author" : [ "Nal Kalchbrenner" ],
      "venue" : "and Phil Blunsom.",
      "citeRegEx" : "Kalchbrenner et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fast exact inference with a factored model for natural language parsing",
      "author" : [ "Klein", "Manning2002] Dan Klein", "Christopher D Manning" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Klein et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2002
    }, {
      "title" : "Ilya Sutskever",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : "and Geoffrey E Hinton.",
      "citeRegEx" : "Krizhevsky et al.2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Ah Chung Tsoi",
      "author" : [ "Steve Lawrence", "C Lee Giles" ],
      "venue" : "and Andrew D Back.",
      "citeRegEx" : "Lawrence et al.1997",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Convolutional networks for images, speech, and time series",
      "author" : [ "LeCun", "Bengio1995] Yann LeCun", "Yoshua Bengio" ],
      "venue" : "The handbook of brain theory and neural networks,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1995
    }, {
      "title" : "Ilya Sutskever",
      "author" : [ "Chris J. Maddison", "Aja Huang" ],
      "venue" : "and David Silver.",
      "citeRegEx" : "Maddison et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Jan Cernocky",
      "author" : [ "Tomas Mikolov", "Martin Karafit", "Lukas Burget" ],
      "venue" : "and Sanjeev Khudanpur.",
      "citeRegEx" : "Mikolov et al.2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Greg Corrado",
      "author" : [ "Tomas Mikolov", "Kai Chen" ],
      "venue" : "and Jeffrey Dean.",
      "citeRegEx" : "Mikolov et al.2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Discriminative training and maximum entropy models for statistical machine translation",
      "author" : [ "Och", "Ney2002] Franz Josef Och", "Hermann Ney" ],
      "venue" : "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,",
      "citeRegEx" : "Och et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Och et al\\.",
      "year" : 2002
    }, {
      "title" : "and Christopher D",
      "author" : [ "Richard Socher", "Cliff C. Lin", "Andrew Y. Ng" ],
      "venue" : "Manning.",
      "citeRegEx" : "Socher et al.2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Srilm-an extensible language modeling toolkit",
      "author" : [ "Andreas Stolcke" ],
      "venue" : "In Proceedings of the international conference on spoken language processing,",
      "citeRegEx" : "Stolcke,? \\Q2002\\E",
      "shortCiteRegEx" : "Stolcke",
      "year" : 2002
    }, {
      "title" : "Oriol Vinyals",
      "author" : [ "Ilya Sutskever" ],
      "venue" : "and Quoc V Le.",
      "citeRegEx" : "Sutskever et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Victoria Fossum",
      "author" : [ "Ashish Vaswani", "Yinggong Zhao" ],
      "venue" : "and David Chiang.",
      "citeRegEx" : "Vaswani et al.2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Maximum entropy language modeling with non-local dependencies",
      "author" : [ "Wu", "Khudanpur2003] Jun Wu", "Sanjeev Khudanpur" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We propose a novel convolutional architecture, named genCNN, for word sequence prediction. Different from previous work on neural network-based language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy specifically designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and n-best re-ranking in machine translation show that genCNN outperforms the state-of-the-arts with big margins.",
    "creator" : "LaTeX with hyperref package"
  }
}