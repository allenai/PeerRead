{
  "name" : "1502.05615.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Forgetting and consolidation for incremental and cumulative knowledge acquisition systems",
    "authors" : [ "Fernando Mart́ınez-Plumed", "Cèsar Ferri", "José Hernández-Orallo" ],
    "emails" : [ "fmartinez@dsic.upv.es", "cferri@dsic.upv.es", "jorallo@dsic.upv.es", "mramirez@dsic.upv.es" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Cognitive abilities, forgetting, consolidation, lifelong machine learning, knowledge acquisition, declarative learning, MML."
    }, {
      "heading" : "1 Introduction",
      "text" : "Machine learning and other data analysis techniques are becoming crucial for many applications where we want to turn (big) data into knowledge. However, any conception of knowledge discovery that aims at generating more insightful results must overhaul the whole process with an incremental, developmental perspective. The view cannot longer be a transformation from data to knowledge, but a transformation of knowledge (plus data) into new\nar X\niv :1\n50 2.\n05 61\n5v 1\n[ cs\n.A I]\n1 9\nknowledge. As a result, properly representing, revising, evaluating, organising and retrieving previous knowledge is crucial in this quest for more complex, insightful, powerful and ultimately cognitive approaches to make knowledge discovery an incremental process.\nKnowledge acquisition1, understood as an automated process of abstracting knowledge from facts and other knowledge, cannot be understood as a naive accumulation of what is being learned. It should be checked whether new learned knowledge can be redundant, irrelevant or inconsistent with old one, and whether it may be built upon previously acquired knowledge. From our point of view, knowledge acquisition systems should be developed for this purpose. This lead us to one of the well-known constraints for AI systems: The StabilityPlasticity dilemma [1]. The basic idea is that an AI system must be capable of learning new things (plasticity) without losing previously learned concepts (stability). This has been a designing principle mainly investigated within the perspective of neural computation over the last thirty years. Some of the proposed solutions include: (a) dual-memory systems simulating the presence of short and long-term memory [3, 4, 5], and (b) cognitive architectures such as the Adaptive Resonance Theory (ART) [6] emulating how the brain processes information. In both cases, catastrophic forgetting 2 of previously learned information was thereby effectively overcome, however, those approaches are only able to gain new knowledge (forgetting is not allowed) without proper management of existing knowledge, thus taking away versatility and efficiency to the proposals.\nFrom our point of view the above principle should point the way to a more general principle which also applies to general AI systems for knowledge acquisition. It could be used to define “truly” intelligent systems (a) able to support incrementally knowledge acquisition without the need to be discarded and retrained repeatedly (which is not cost-effective), (b) where the inductive and deductive reasoning algorithms are integrated for such a goal and guided by knowledge evaluation metrics, and, finally, (c) able to focus on what is relevant knowledge (or dually to discard what is not) by the use of cognitive mechanisms that simplify the learning of new knowledge. Following those requirements, below we overview some prior work in the area of knowledge acquisition.\nOver the last decades, there has been an extensive work on growing knowledge bases from discovered patterns and rules. We find this in different areas, including expert systems, machine learning, cognitive science, nonmonotonic logic, information systems and inductive (logic) programming. For instance, Lifelong Machine Learning (LML)[7] is concerned with the persistent and cumulative nature of learning, namely: (a) capable of retaining and using prior knowledge, and (b) capable of acquiring new knowledge over a series of prediction tasks. Similarly, Transfer Learning [8] and multitask learning [9, 10] take a similar perspective, where it is more explicit that the process is task-oriented, and knowledge and its structure does not always play a central role in these systems. ELLA (Efficient Lifelong Learning Algorithm) [11] and NELL (Never-Ending Language Learner) [12] are two more recent approaches to LML, which are able to integrate many capabilities. However, it is not easy to export or derive general principles from these works to analyse a knowledge base and\n1In expert systems, the term knowledge acquisition is usually understood as the incorporation of expert knowledge into the system. In this paper, we use the term knowledge acquisition as the process of discovering new knowledge from facts and integrating it with the existing knowledge.\n2Phenomenon by which neural networks completely forget previously learned information when exposed to new one.\nhelp in a general incremental knowledge discovery process. Other related topics are concept drift and theory revision [13, 14, 15], where some rules are replaced by new rules that are consistent with new experience. This is similar to the approach in nonmonotonic and approximate reasoning, and probabilistic or stochastic logic representations. The areas of inductive logic programming [16, 17] or general inductive programming [18, 19] have seen several approaches for incremental [20] or cumulative systems [21].\nA crucial aspect relies on theory and knowledge evaluation. When the theory or hypothesis is considered as a whole and separated from the evidence, we have many well-founded proposal, such as the MML principle [2, 22] or the similar (but posterior) MDL principle [23, 24]. However, for knowledge integration and consolidation it is necessary to assess each part of the theory independently, where different parts of the theory can have different degrees of validity, probability or reinforcement [25, 26]. However, there is still a separation between knowledge and evidence. It would be meaningful to provide a fully integration of knowledge and evidence into a hierarchical assessment structure from very specific and ground facts to more abstract rules. The perspective of a network or hierarchy of nodes that get support from other nodes is more common in the area of link analysis in web graphs such as the HITS algorithm [27], PageRank [28] or SALSA [29], or in infometrics.\nFinally, knowledge acquisition has much to learn from the study of human cognition [30, 31, 32, 33, 34]. We can fully realise the benefits of knowledge acquisition by paying attention to the cognitive factors that simplify the learning and processing of the knowledge which make the resulting models coherent, efficient, credible, easy to use and understandable [35]. In particular, there is a characteristic feature of intelligence that is essential for knowledge development: forgetting. Meanwhile human memory has a positive connotation linked with performance, forgetting is often associated with negative terms as a state where memory does not work properly. Memory and forgetting are two complementary faces of the same biological process (synaptic plasticity), being the latter the one of the human mind’s selective activities which allow us to abstract concepts. It could be said that, without forgetting, memory would be completely useless. The absence of forgetting was masterly described by Jose Luis Borges in his tale “Funes, the Memorious” (1942): “To think is to forget a difference, to generalise, to abstract. In the overly replete world of Funes, there were nothing but details”. Clearly, remembering absolutely everything prevents from having abstract thought (the process of generalisation), given that induction and deduction rely on this ability. Therefore, in AI systems, forgetting should play an important role when acquiring knowledge. Forget has multiple shades of meaning in AI systems: it can refer to a complete and irreversible elimination of significant old knowledge while learning new one; or it can denote that new learned knowledge is not always kept in the working memory but abstractly encoded by identifying their relation to abstract concepts already present in the knowledge base. The first meaning clearly refers to those AI systems that are booted up for solving individual problems, whereas the latter definition is the desired one: forgetting should exist in knowledge bases and learning systems to avoid possible information overflow and redundancy, and in order to preserve and strengthen important or frequently used rules and remove (or forget) useless ones.\nThe ability to focus on what to discard what is not relevant is becoming more relevant not only in cognitive science and neuroscience [36], but also in artificial intelligence (e.g.,\nreasoning, planning, decision making). The notion of forgetting, also known as variable elimination, has been widely investigated in the context of classical logic (propositional and first-order logic) [37, 38, 39] and developed under the notion of logical equivalence, that is, logically equivalent formulas (theories) will remain equivalent after forgetting the same set of propositional variables or literals. A similar approach but for reasoning from inconsistent propositional bases is proposed in [39]. Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases. Forgetting (abstracting from) actions in planning has been also investigated in [47]. Finally, in [48] is proposed a forgetting mechanism for an online learning algorithm to learn sequential data with timelines able to gradually expel the outdated data that could become a possible source of misleading information.\nFrom our point of view, forgetting in a knowledge base is closely linked to the previous concept of theory and knowledge evaluation. Therefore, inspired by the MML principle, the informativeness of a piece of knowledge (in terms of usefulness or the opposite concept, irrelevance) can be assessed quantitatively only by its relationship between complexity and compression. This lead us to an easy and general concept of forgetting where as much information as possible from the original knowledge is preserved, thus setting aside tasks such as the preservation of logical equivalences or the satisfaction of semantic properties between theories.\nClosely related with the above concept we found memory consolidation, namely, the neurological process of converting information from short-term memory into long-term memory. Some studies about episodic memory in humans [49, 50] claim that memory traces in the hippocampus are not permanent and are occasionally transferred to neocortical areas in the brain through a consolidation processes. This consolidation process refers to the idea that memories continue to strengthen after they have been formed in the human brain and seems a primary factor underpinning memory and forgetting in knowledge bases and learning systems. Notwithstanding a single recent cognitive model of memory ascribes too much importance to consolidation procedures [51], we consider that not only forgetting must be a prevalent operation in knowledge acquisition, but also consolidating is crucial as well for promoting efficient memory storage.\nGiven the above overview, we see that it is not easy to develop a new knowledge discovery system that is meant to be cumulative. In fact, this research started when developing our system gErl [52, 53]. We were looking on a proper foundation for detailed knowledge assessment metrics and criteria for forgetting. The need of making general principles available for our system and other systems motivated the current work.\nIn this work we take a most general approach by considering that we start with an off-theshelf inductive engine (e.g., a rule learner, an inductive logic programming (ILP) system [16, 17] or an inductive programming (IP) system [18, 19]) and an off-the-shelf deductive engine (e.g., a coverage checker, an automated deduction system or a declarative programming language) and, over them, we build an long-life knowledge discovery system (see Figure 1).\nFor this purpose, several issues have to be addressed:\n1. The inductive engine can generate many possible hypotheses and patterns. Once brought to working memory we require metrics to evaluate how these hypotheses behave and how they are related in the context of previous knowledge. Additionally, at any time new evidence can be added as rules to the working space.\n2. As working memory and computational time are limited, we need a forgetting criterion to discard some rules which are considered irrelevant in terms of informativeness.\n3. The deductive engine checks the coverage of each hypothesis independently, using the background or consolidated knowledge as auxiliary rules, but not other working rules. As a result, only when new knowledge is consolidated we can use it for new problems or for more difficult examples of the same problem. This means that deduction is “modulo the background knowledge”. In other words, working hypotheses must be able to use consolidated knowledge but not other working rules.\n4. The promotion of rules into consolidated knowledge must avoid unnecessarily large knowledge bases and the consolidation of rules that are useless, too preliminary or inconsistent. This means that rules must promoted and demoted. Also, if the knowledge base becomes too large, finding the appropriate pieces of knowledge for new tasks will be less efficient. This means that rules must promoted and demoted to keep a powerful, but still manageable knowledge base.\nThe idea of coverage graph is used as the basis for structuring knowledge and is delegated to the deductive engine. The generation of new rules is delegated to the inductive engine. The\ncrucial part is the definition of appropriate metrics to guide the way knowledge develops. For this purpose, the MML principle is used as a sound theoretical ground for the metrics.\nThe paper is organised as follows. Section 2 introduces the notion of coverage graph, which is our setting for a knowledge base. Over this coverage graph, we are able to introduce an adaptation of the MML principle and related metrics in section 3. Section 4 deals with knowledge structuring, how rules are forgot, promoted and demoted. We include several experiments where we illustrate how knowledge consolidation and forgetting works in section 5. Finally, section 6 closes the paper with the contributions and some future work."
    }, {
      "heading" : "2 Coverage graph",
      "text" : "We consider that ‘rules’ are used for expressing examples, hypotheses and background knowledge. Rules are denoted as e where class(e) = c, c ∈ C and C is the set of classes, such as {false, true}. The set of all possible rules is denoted by R, where W ⊂ R is the working space or memory, and K ⊂ R is the background or consolidated knowledge base.\nRules are presented as vertexes or nodes V (and we refer them indistinctly) in a directed acyclic graph G(V,A) we call coverage graph (which is the DAG representation of a specific working space), because the directed edges A represent the coverage relation between the different rules as determined by the deductive engine. We say that a rule ρa is covered by another rule ρb if (K ∪ ρb) |= ρa. The precise understanding of the semantic consequence operator will depend on the rule representation language used and the deductive engine. Hence, if there is an edge a = (µ, ν) (or µ → ν), then ν is said to be directly covered by µ using K3.\nThe set of ancestors and successors of a node ν are defined as anc(ν) = {µ|µ → ν} and suc(µ) = {ν|µ → ν} (respectively). Also, we distinguish two subsets of nodes: leaves, nodes without successors (|suc(ν)| = 0), where the set of leaves ν of class c is denoted as leavesc; and roots, nodes without ancestors (|anc(ν)| = 0).\nFigure 2 shows an example of Coverage Graph of a well-known ILP problem [16]: the family relationship. In this problem, the task is to define the target relation daughter(X, Y ), which states that person X is daughter of person Y . W consists of three positive examples (rules 1, 2 and 5), two negative ones (rules 3 and 4), and seven selected rules that try to generalise and solve the problem (Table 1 right), whereas K is composed of the relations female and parent (Table 1 left). Note that the rules in K have not been included in the graph for clarity, although they belong to the initial “consolidated knowledge”."
    }, {
      "heading" : "3 Basic Metrics for Discovered Knowledge Assessment",
      "text" : "In order to select and arrange the set of rules in the working space, various measures of usefulness, relevance and consistency have to be derived from the coverage graph. Based on the idea that the relevance or usefulness of a rule can be stated by the relationship between\n3For simplicity, the coverage graphs do not include the edges for the transitive closure of the covering relation, i.e., if a node µ covers nodes ν and γ, but ν also covers γ, only the edges µ → ν and ν → γ are included in the graph.\nits own complexity and the complexity of the rules it covers, a general criterion such as the Minimum Message Length [2] (MML) can be used as a starting criterion from which to derive new metrics."
    }, {
      "heading" : "3.1 Minimum Message Length",
      "text" : "The Minimum Message Length is one of the most popular selection criterion in inductive inference (for a formal justification and its relation to Kolmogorov complexity and the related MDL principle, see [54, 55, 22]). It provides an interpretation of the Occam’s Razor principle: the model generating the shortest overall message (composed by the model and the evidence concisely encoded using it) is more likely to be correct. This message can be re-stated in a Bayesian form [2] with the length of the first part of the message (the model) and the length of the second part (evidence covered). The Bayesian theorem, which is the primary concern of Bayesian inference, is shown in equation 1:\nP (H|E) = P (H) · P (E|H) P (E) = P (H ∩ E) P (E)\n(1)\nwhere P (H) is the prior probability of the modelH, P (E|H) is the likelihood, and P (E) is the probability of the evidence E. An information-theoretic interpretation of MML is that a given evidence E of probability P (E) can be coded by a message of length L(E) = −log2(P (E)) [56]. Therefore, taking the negative logarithm of the expression 1 and according to the MML philosophy, the length of a hypothesis H given a fixed evidence E (L(H|E) is defined as the sum of three simple heuristics: a complexity-based heuristic (which measures the complexity of H), a coverage heuristic (which measures how much extra information is necessary to express the evidence given the hypothesis H) and the length of the evidence (L(E)) which equal for all competing hypotheses:\nL(H|E) = L(H) + L(E|H)− L(E) (2)\nBy minimising equation 2 we maximise the posterior probability. This involves searching for the model that gives the shortest message.\nApart from its connection with Kolmogorov complexity and Solomonoff induction [57], which gives additional support for its use, the MML principle (and the similar MDL principle) has been successfully applied in many areas of machine learning, AI and cognitive science. However, to our knowledge, the MML principle has always been applied to select between hypotheses with respect to some given evidence. In our case, we have a coverage graph where rules cover other rules, so they become H and E at the same time. In a way, what we need is a hierarchical MML application, with this in mind the MML principle can be adapted to be used in our approach with the following considerations: instead of measuring the length of a hypothesis H given fixed evidence E, what we want to measure is the length of each rule ρ in W with respect to the rest of rules in W (which includes examples and hypotheses) because ρ can model not only examples, but also other rules. Therefore, L(ρ|W ) is defined as the sum of the length of ρ (L(ρ)), and the length necessary to express the rules in {W −ρ} not modelled by ρ (L(W |ρ)), minus the length of the total rules in W (L(W )). Formally:\nL(ρ|W ) = L(ρ) + L(W |ρ)− L(W ) (3)\nApparently, it just seems a notational change wrt. Eq. 2. This is only true for the first term, which is estimated in the same way as the original MML principle. The term L(ρ) can be defined in different ways depending on the rule representation language. For instance, if we are using logical or functional rules (as in the family example), we could use the following approximation. Given Σ a set of mΣ functor symbols of arity ≥ 0, and X a set of mX variables, we could define the length of a rule ρ containing nΣ functors and nX variables as\nL(ρ) , mΣ log2(nΣ + 1)\n+ mX 2 log2(nX + 1) (4)\nNote that we promote variables over constants or functors. Table 2 shows the length in bits and the class for the rules in the graph of Figure 2."
    }, {
      "heading" : "3.2 MML goes hierarchical: Support",
      "text" : "Following with the equation 3, we are going to reunderstand the terms L(W |ρ)−L(w) to be adapted to coverage graphs and multiclass settings. Roughly speaking, these terms capture the “net profit” of the rules both in terms of support or coverage (length in bits of the rules covered). More formally, we define the support of a rule ρ ∈ W as:\nS(ρ,W ) , L(ρ)− L(ρ|W ) = L(W )− L(W |ρ) (5)\nwhere L(W ) − L(W |ρ) represents the coverage of a rule ρ expressed in bits, that is, the length of all the rules in W minus the length of the rules not covered by ρ. Therefore, the support of a rule ρ represents the length of the rules it covers:\nS(ρ,W ) = ∑ ν:ρ|=ν L(ν) (6)\nleading to an alternative expression for L(ρ|W ) (eq. 3) in terms of support:\nL(ρ|W ) = −S(ρ,W ) + L(ρ) (7) which establishes that maximising S(ρ,W ) and minimising L(ρ) we minimise L(ρ|W ) which involves searching for the rule ρ that covers the maximum number of rules and has the lowest length.\nThe following step is to adapt eq. 7 to be used in coverage graphs that does not explicitly include the edges for the transitivity of the coverage relation. In order to consider the upwards propagation, only the leaves will have an initial support value which is equal to its length in bits, and the rest of nodes will distribute it recursively by propagating this support. Thus, the new support (S ′(ρ,W )) adapted to work on coverage graphs is defined as:\nS ′(ρ,W ) , L(ρ) if ρ ∈ leaves∑ ν∈suc(ρ) S ′(ν,W ) otherwise (8)\nIn order to avoid the scenario where the less grounded (upper) nodes get higher and higher support values, the support measure is required to satisfy a conservative condition. This property is somehow related to the law of conservation of energy, implying that at any node in a coverage graph, the sum of the total support flowing into that node is equal to the sum of the total support flowing out of that node.\nNow, to make S ′ conservative we need to divide the support coming from the outcoming of a specific node ν by |anc(ν)| in order to equally distribute the support of ν between all of its ancestors.\nTherefore, the new formula used to calculate the support of a rule (Ṡ(ρ,W )) is defined to be equal to:\nṠ(ρ,W ) , L(ρ) if ρ ∈ leaves∑ ν∈suc(ρ) Ṡ(ν,W ) |anc(ν)| otherwise\n(9)\nand leading to an expression for L(ρ|W ) (7) in terms of this conservative support:\nL̇(ρ|W ) = −Ṡ(ρ,W ) + L(ρ) (10) Equation 9 now accomplishes the mandatory conservative condition which could be stated such as the support of a node (which depends on its successors) has to be always entirely allocated in its ancestors together with the support inherited from other covered nodes (see Figure 3).\nThis implies (but not vice versa) that the total sum of the support in the leaves in the coverage graph is equal to the total sum of the support at the root nodes. Namely:∑\nµ∈leaves\nṠ(µ,W ) = ∑\nν∈roots\nṠ(ν,W ) (11)\nFor each leaf in the coverage graph we have n different paths whereby the support flows upwards to root nodes. Whenever a path is forked (an ancestor is found), the support is always divided by the number of the outcoming paths, having the ancestors an equally part of the support and thus having the roots a proportion of the original support of the leaves transitively covered by them. Therefore, if we assume that the total support at the roots is different from the total support at the leaves, it means that an external transfer of support (which comes from or goes to other sources) has happened. However, accordingly to eq. 9, this is not possible and, therefore, the total sum of the support at the roots always remains constant and equal to the total support at the leaf nodes (see Figure 3).\nExample Viewed through the example in Figure 3 and accordingly to the equation 9 we have that the support of the root nodes is\nṠ(d,W ) = Ṡ(e,W ) = Ṡ(x,W )\n3 , Ṡ(f,W ) =\nṠ(x,W )\n3 + Ṡ(Y,W ),\nwhere\nṠ(x,W ) = Ṡ(a,W ) + Ṡ(b,W )\n2 , Ṡ(y,W ) =\nṠ(b,W )\n2 + Ṡ(c),\nand\nṠ(a,W ) = L(a), Ṡ(b,W ) = L(b), Ṡ(c,W ) = L(c)\nthus making the following equations true (accordingly to the formula 9):\nṠ(a,W ) + Ṡ(b,W )\n2︸ ︷︷ ︸ Ṡ(x,W ) − Ṡ(d,W )︸ ︷︷ ︸ Ṡ(x,W ) 3 − Ṡ(e,W )︸ ︷︷ ︸ Ṡ(x,W ) 3 − (Ṡ(f,W )− Ṡ(y,W ) 2 )︸ ︷︷ ︸ Ṡ(x,W )\n3\n= 0\nṠ(c,W ) + Ṡ(b,W )\n2︸ ︷︷ ︸ Ṡ(y,W )\n− (Ṡ(f,W )− Ṡ(x,W ) 3\n)︸ ︷︷ ︸ Ṡ(y,W ) = 0\nand, also, being the total support at leaf nodes (L(a)+L(b)+L(c)) equal to the total support at root nodes (equation 11):\nṠ(d,W ) + Ṡ(e,W ) + Ṡ(f,W ) = ( Ṡ(x,W )\n3 ) + (\nṠ(x,W )\n3 ) + (\nṠ(x,W )\n3 + Ṡ(y,W ))\n= S(x,W ) + Ṡ(y,W )\n= (Ṡ(a,W ) + Ṡ(b,W )\n2 ) + (\nṠ(b,W )\n2 + Ṡ(c,W ))\n= Ṡ(a,W ) + Ṡ(b,W ) + Ṡ(c,W )\nFinally, we need to take into account that, since the working space W can accommodate examples of different classes, we need our metric to distinguish between them and, hence, there are as many support values for each node as many different classes there are in the working space, each one holding the conservative property and formally defined to be equal to:\nṠc(ρ,W ) , L(ρ), if ρ ∈ leavesc∑ ν∈suc(ρ) Ṡc(ν,W ) |anc(ν)| otherwise\n(12)\nwith eq. 10 being defined for classes as follow:\n− L̇c(ρ|W ) = Ṡc(ρ,W )− L(ρ) (13)\nThe value of L̇c is interpreted as the hierarchical version of the MML principle, with L̇c being\nthe lower the better (and obviously −L̇c the higher the better). Following with the Family example, Table 3 shows the support and the negative form of L(ρ|W ) (for each class) of the rules in the graph in Figure 2."
    }, {
      "heading" : "ID L(ρ) class Ṡ+ Ṡ− −L̇+ −L̇−",
      "text" : ""
    }, {
      "heading" : "3.3 Optimality",
      "text" : "By using the support as the sole criterion to rank the rules in W is useful provided there are only rules belonging to one class. However, when there are more than one class inW , we need to consider the purity or confidence of the rules. In the same spirit of the MML principle, we define the optimality as the difference between the cost of coding a rule following equation 13 for a specific class and the cost of coding the exceptions, i.e.,: the support of the rules covered that belong to the other classes. We use a factor β indicating the relevance of rules being as pure as possible. Formally:\noptc(ρ,W ) , −β · L̇c(ρ|W )− (1− β) · ∑ c′∈C c′ ̸=c Ṡc′(ρ,W ) (14)\nleading to a generic optimality of a rule as:\nopt(ρ,W ) , max c∈C (optc(ρ,W )) (15)\nFollowing with the Family example, Table 4 shows the optimality values per class (the generic optimality in bold) for the rules in the graph of Figure 2 using β = 0.5. According to these values, rule 110 is the most significant rule, as it can be easily viewed in the coverage graph because it covers all the positive examples and no negative one."
    }, {
      "heading" : "4 Structuring knowledge: forgetting, promotion and",
      "text" : "demotion\nIn our setting, rules are repeatedly generated by the inductive engine and added to the working space W . As an answer to the possible never-ending growth of W , it is necessary to have mechanisms for forgetting or revising useless pieces of acquired knowledge. Using the metrics we have just introduced, we need a mechanism to discard those rules that are not useful, are inconsistent or do not get enough support."
    }, {
      "heading" : "4.1 Forgetting mechanism",
      "text" : "The optimality of a rule ρ is a core metric to determine its usefulness, but it is also important to see whether ρ could be considered superfluous because it is covered (transitive or directly) by another rule of higher optimality. If it is the case, ρ is mostly redundant and it could be discarded safely. This idea leads to the following definition for the permanence of a rule:\npermc(ρ,W ) , optc(ρ)−max(0,max ν:ν|=ρ optc(ν)) (16)\nwith a generic permanence:\nperm(ρ,W ) , max c∈C (permc(ρ,W )) (17)\nThe lower the value of permanence a rule has, the higher the odds it has to be forgotten. When we perform a forgetting step, the coverage graph is affected and coverages are also affected. In order to keep as much information about the past support, each rule is provided with a trace of its old support. In cognitive systems this is associated to notions such as the\npreservation of belief and trust even if we forget the particular cases that gave support to a given statement. Therefore, the forgetting mechanism will work as follows:\n1. If a non-leaf node is selected to be forgotten, the support of its successors has to be redistributed among their ancestors and the ancestors of the forgotten node (see Figure 4 (left)).\n2. In case there is a forgetting step that removes a leaf node, its support has to be equally distributed among the rules that cover it which inherit it as their “residual” support value associated to each class c (resc) (see Figure 4 (right)).\nHence, the equation 12 is modified to include the residual:\nS̊c(ρ|W ) , L(ρ) if ρ ∈ leavescresc + ∑ v∈suc(ρ) S̊c(v,W ) |anc(v)| otherwise\n(18)\nwhere resc is initially set as 0. For each forgetting step, the support of forgotten nodes is distributed among the outcoming nodes increasing their resc value, but if the last forgetting step removes a node without ancestor nor successors and a non-zero resc, this value cannot be distributed and, therefore, is lost. These results in a decrease of the total support of the graph: although the support will remain conservative, the total amount will be lower than the total support of the coverage graph before the forgetting steps. Consequently, in the end some rules may have an under-estimated support value in terms of how many rules (of different classes) cover (see Figure 5).\nµ∈leaves\nS̊(µ,W ) = L(A)) has been reduced at the last step = 4 (\n∑\nµ∈leaves\nS̊(µ,W ) = L(A)\n2\n)\ndue to the forgetting mechanism.\nIn order to clarify how this mechanism works we illustrate this with the Family example. Figure 6 shows the evolution of the coverage graph in Figure 2 and its measures (see Table 6) through nine consecutive forgetting steps, where the rule with lowest permanence is forgotten in each step (shown with a grey square). For instance, in step 1, we see that rule number 59 is redundant because it is covered by a more significant rule (with ID 110), and it has the lowest value of permanence (see Table 5 (step 1)). Thus, rule 59 is forgotten, the coverage graph is redrawn (see Figure 6 (step 2)) and the metrics are recalculated if necessary (see Table 5 (step 2)). In step 2 (and other steps where a leaf node is deleted), its support is distributed equally among its ancestors and this distributed support becomes part of their residual or intrinsic support (resc).\nIn this example, we have forgotten one rule at a time, but the actual pace and number of rules to forget can be tuned to the purpose of the system."
    }, {
      "heading" : "4.2 Consolidated knowledge: promotion and demotion",
      "text" : "Finally, some of the rules with good indicators in the working space have to be eventually promoted to consolidated knowledge (or belief). This has to be a careful process, as the consolidated knowledge will be used by the deductive engine to calculate coverage. This means that an inconsistent rule that is promoted to the consolidated knowledge may have important consequences on the behaviour of the system.\nThe promotion function can be tuned for the application, but a general choice is to use a threshold θp on the optimality to consolidate or promote a rule to a belief status in B.\nWhen a rule is promoted to consolidated knowledge, it cannot be target of the forgetting mechanism and, hence, be forgotten. It may happen that this rule can be eventually removed from the consolidated knowledge. Therefore, the promotion system is mirrored by a demotion system, with the use of another threshold θd. The original background knowledge (B0) cannot be demoted (and forgotten).\nIn the example in Figure 6, we have established θp equal to the average optimality of all the rules in the working space. Then, in step 1, all the rules that exceed this average value will be consolidated to the background base (rules 110 and 73). Any rule that is consolidated cannot be target of the forgetting mechanism until it is demoted to the working space again (in the example, we have considered a demoting threshold θd equal to θp). Thus, in Table 5 (step 5), rule 73 has the lowest permanence value (perm(73) = −6.037) but 35 (perm(35) = −4.642) is forgotten instead, because the former is a consolidated rule."
    }, {
      "heading" : "5 Experiments",
      "text" : "As mentioned in section 1, one of the issues in many cognitive systems (especially connexionistic, either artificial or biological) is the Stability-Plasticity dilemma. We claim that our approach is able to address this issue in a long-life learning process. For this purpose, we have conducted an experimental evaluation to explore the following questions: (a) is it possible to gradually generate a large repository of consolidated knowledge assessing the usefulness of the rules? (b) is our approach able to forget or revise the existing knowledge in order to generate a rich and reusable knowledge base? and (c) how are the process and the resulting knowledge structure understood in terms of cognitive systems that must discover and develop knowledge incrementally? We want to illustrate these features in one single domain. The ultimate goal of these experiments is to see whether the framework is general enough to work with off-the-shelf inductive and deductive engines, to better understand how the metrics and procedures work, and finding whether they may require some tuning or\nimprovement to the framework before addressing other problems."
    }, {
      "heading" : "5.1 Methodology",
      "text" : "We will focus on the problem of learning the rules of chess by observation. In particular, we focus on learning a model of legal moves of different pieces from a set of legal and illegal move examples (extracted from [58]). In our framework, the legal moves are the positive examples and the illegal moves the negative ones (so we have two classes). Each example represents a move of a specific piece on an empty board. Therefore, a move is represented by a triple from the domain Piece × Pos × Pos, where the second and third components represent, respectively, the piece’s initial position and its destination on a chessboard. Positions are represented by a tuple from the domain File × Rank where files (a-h) stand for columns and ranks (1-8) stand for rows. For instance, Figure 7 illustrates all the possible moves of a knight from a specific initial position (K) to several other positions (K ′). We will use a Prolog notation (as in the example in the previous section).\nThe only background predicate used is the absolute difference, diff(X,Y), that calculates the distance between X and Y , where both X and Y can be ranks or files (see Table 6).\nThe challenge we would like to face is knowledge discovery and acquisition in a progressive way from examples provided incrementally. A random set of chess moves from all chess pieces in the game except the pawn (rook, bishop, knight, queen and king) is given. This includes positive and negative examples (28 and 12 examples respectively). We also consider that an inductive engine is generating rules during the whole process (according to the working space and using the consolidated knowledge as background knowledge) and they are arriving to the system in a random order as well. In our case, we have taken the rules generated by the ILP system Progol [59] (60 in total). How many examples and rules are given for each step of the system is defined following a geometric distribution. Formally, the probability that k examples (and similarly for rules) are given is Pr(X = k) = (1 − p)k−1 · p where k is 1, 2, 3, . . . and p is the probability of success (we set it to 0.5). In order to better mimic a situation where the inductive engine can produce rules it has already generated (as otherwise we would need to keep trace of all this), it is more realistic to use this distribution with replacement. Similarly, as the same move can appear repeatedly in chess, we have also considered replacement for the set of examples.\nIn this experiment, we have set the consolidation criterion with a threshold of optimality\ngreater than the average of the optimality value of the rules in W (provided that it is above the average optimality of the evidence), namely,\nopt(ρ,W ) > max(0,\n∑ ν∈W opt(ν,W )\n|W | ) (19)\nFurthermore, since we want the consolidated knowledge to represent legal chess moves, we have set the β parameter equal to 0.1 in equation 14 with the aim of penalising those rules that are not pure."
    }, {
      "heading" : "5.2 Consolidation without forgetting",
      "text" : "In a first experiment we try to show what would happen without applying the forgetting mechanism and check whether the MML-based measures work successfully for knowledge acquisition: are the final consolidated knowledge useful to solve the problem given the evidence? Figure 8 shows the evolution of the learning process during 500 steps. As no rules are forgotten, the rule population (dashed brown line) reaches its maximum value (100) and it stagnates ignoring any new evidence which arrives to the system (because they are already placed in W ) from step 180 onwards. In this case we have assumed that all the evidence of the chess problem can be allocated in W , however it could be the case that all knowledge of a problem will not fit into W (memory restrictions) thus collapsing with no improvement. The same applies to both the average optimality of all rules (dashed blue line) and the consolidated ones (dashed green line) which, since no more new rules are allocated into W , no further learning or knowledge improvement can take place. Table 7 shows the consolidated rules at step 500 where we can see that they almost represent all the legal chess moves (only two movements of the knight are missing in this set) and there is only one rule (x20) which, despite representing a legal move, does not completely generalise the movement of the piece (king). This is a good result as the working space is large enough to accommodate all these rules (and many other less significant rules). See Table 12 in Appendix 7 for all the rules in W at step 500 and Figure 11 for their coverage relations. The conclusion we can draw from these results is that the metrics used to measure the usefulness of the rules provide a guarantee of promoting those rules that, having the maximum compression, best describe the problem."
    }, {
      "heading" : "5.3 Consolidation with forgetting",
      "text" : "After that, we repeat the same experiment, but using the forgetting mechanism. This tries to represent a situation where we have bounded resources, in this case a more limited working space, so it is necessary to forget rules in order to allocate new ones. What we want to show is that if our approach is able to find a solution to a certain problem without the use of the forgetting mechanism, a suitable (and possibly better) solution to the problem should exist having bounded resources and by using the forgetting mechanism. In order to do that, we have executed several configurations with varying maximum number of rules in the working space (|W | ∈ {(20, 30, 40, 50, 60, 70, 80, 90)}) and every time the limit is exceeded the forgetting process is launched, forgetting up to 25%, 50% or 75% of the most meaningless\nrules (those with the lowest perm value). Each different configuration has been launched 10 times, hence, there are 240 executions in total.\nTable 8 is a Heat map showing, for each possible configuration (|W | × forgetting(%)) how many times a specific rule appears in the consolidated knowledge in 10 repetitions, from white (0 times), light yellow (1 time) to dark green (10 times). Rules that are not represented in the Heat Map is because they have not been consolidated at any time. Knowing that the consolidated rules by the first experiment (Table 7) are those represented in the bottom row (|W | = 100), it is easy to see that not only the set of consolidated rules almost always includes the reference solution (even with very limited resources), but also the forgetting criterion allows the system to include those rules that perfectly generalise the moves of the king (rules in bold). The rest of rules included in the consolidated set in each experiment also generalise different movements of the pieces and, in some cases, they could disappear from this set by using a more restrictive consolidation criterion (i.e., by using the average of the optimality plus n times its standard deviation). See Table 13 in Appendix 7 for all the rules in W at step 500.\nIn order to compare both experiments, Figure 9 shows the evolution of the system during 500 steps for one representative setting of the 24 configurations (maximum number of rules\nequals to 60 and up to 50% of rules forgotten in each forgetting step). Now, the variations in the amount of consolidated rules (dotted black line) and rules in the working space (dashed brown line) allow us to observe how the forgetting mechanism works (every 30 steps approximately). Table 9 presents the consolidated rules at the final step (500). In this case, this set perfectly generalises all the legal moves of all the chess pieces. The system has reached a stable situation in which the number of consolidated rules (dotted black line) remains almost constant from step 250. The average optimality of both the consolidated rules (dashed green line) and all the rules (dashed blue line) have an increasing trend due to the distribution with replacement used to populate the working space. The appearance of new rules in the system or the execution of the forgetting mechanism mainly affect the average optimality of W (dashed blue line): every time it runs, the working space is cleaned of useless rules which strongly affects the metrics of the rules in W (and to a lesser extent to the consolidated set of rules (green line)) that have to be recalculated. Compared with the former experiment, the number of rules in W has been reduced (with one order of magnitude (10x) speedup in execution) obtaining a better set of consolidated knowledge: it includes all the rules that solve the chess problem, including the two legal moves of the knight, rules k22 and k24, which were missing from the consolidated knowledge in the first experiment."
    }, {
      "heading" : "5.4 Incremental knowledge acquisition",
      "text" : "Finally, one last experiment tries to show the capability of our approach for the incremental learning of new knowledge from previously consolidated concepts. This experiment is divided in two phases: in the first one we have only taken rules and examples of moves of the rook and bishop chess pieces (15 and 30 rules respectively) providing the system with them in the same way as in the previous experiment. The consolidation criterion has not been changed, but the maximum number of rules in the working space has been established to 15 (in order to allow the forgetting mechanism to work) and the percentage of meaningless rules that are\nforgotten for each forgetting process up to 25%, due to the smaller size of the working set. In Table 10 we can see the set of consolidated rules after 100 steps. This set contains the rules that perfectly generalise all the legal moves of the rook and the bishop. In the first 100 steps of Figure 10 we can see how the forgetting and consolidation mechanisms work. This time, due to the lower maximum number of rules allowed in the working space, the lower percentage of rules forgotten and the geometric distribution used to provide the rules, the forgetting mechanism runs here every few steps, showing non-constant sawtooth-like wave ramps for the number of rules in the working system (dashed brown). However, the number of consolidated rules remains constant almost from step 45 to the end of this stage (100).\nIn the second phase, we provided the system with a new set of rules and examples (10 and 20 rules respectively) only representing moves of the queen chess piece. Apart from using the background knowledge that is provided initially, it should also be possible at this point to use the previously learned moves of the rook and the bishop in order to express the moves of the queen. This is what the inductive engine can take advantage of. Table 11 shows the set of consolidated rules which contains the previously consolidated rules that generalise the legal moves of the rook and bishop, and a new set of rules that represents the legal moves of the queen. This latter set includes a pair of rules (q29 and q25) that use the rook and bishop rules and represent all the possible moves of the queen piece: q25 which covers both the\nhorizontal and vertical moves of the queen; and q29 which covers the diagonal movement. The second half of Figure 10 (from step 100) shows how the forgetting mechanism runs even more frequently than previously (dashed brown) due to the increment of consolidated rules (that cannot be targeted by forgetting). Again, the number of consolidated rules (dotted black line) remains constant most of the time (from step 140 to step 200)."
    }, {
      "heading" : "5.5 Discussion",
      "text" : "In an effort to facilitate an understanding of whether our approach is able to effectively and incrementally grow a knowledge base by using appropriate evaluation metrics and useful cognitive abilities for addressing the knowledge acquired, we have performed some experiments over a well-known scientific domain, the chess problem. As we have said, the ultimate goal is not to validate the approach but to provide some insight into both its generality, efficiency and the much-needed use of forgetting and consolidation cognitive procedures in incremental and developmental approaches for knowledge discovery. In order to shed some light on these aspects, we will refer to the questions raised at the beginning of this section.\nFrom the above experiments, we see that the repository of rules can be well structured and ranked by the metrics and the system consolidates those rules that are appropriate, therefore responding affirmatively to the first question (a). Regarding question (b), we also see that a moderate limitation of working space with forgetting is even capable to improve the identification of the rules to be consolidated, and, what is better, prevents the system for stagnating or collapsing in situations where we have bounded resources. Finally, in connection with question (c), we see the behaviour in an incremental setting, where the knowledge can be used in new tasks; one of the principles of developmental cognition.\nConsequently, the proposed approach for knowledge acquisition is a favourable compromise to the stability-plasticity dilemma, which is characterised as:\n• Too much plasticity will result in previously learned knowledge being constantly forgotten. However, the promotion and demotion mechanisms together with the evaluation metrics rank and structure the knowledge allocated in the working space avoiding useful knowledge losses.\n• Too much stability will impede the efficient coding of new learnt knowledge. However, the forgetting mechanism also together with the evaluation metrics is in charge of removing those meaningless and redundant knowledge."
    }, {
      "heading" : "6 Conclusions",
      "text" : "Learning a set of rules from data is nowadays a well-known problem for which many approaches exist. However, the use of background knowledge and the consolidation of new knowledge is one of the conspicuous problems in the understanding and creation of cognitive systems, and the management of more long-life knowledge discovery systems. The organisation of complex knowledge structures in terms of coverage graphs allows a straightforward and principled approach to knowledge acquisition, consolidation (promotion), revision (demotion) and forgetting. All this can be applied and analysed at a meta-level, with the use of off-the-shelf deductive and inductive engines. This modularity, and the ability of dealing with declarative knowledge bases opens up a range of applications in knowledge discovery, developmental cognition, expert systems and other intelligent systems that are meant to have a non-ephimeral life.\nThe main contributions of this work are: (1) The first extension of the MML principle to a knowledge network (in the form of coverage graph). While the MML principle has a\nBayesian inspiration, the metrics are more flexible than actual probabilities, stauncher when pieces of the working space are removed, and can be combined into metrics for different processes. (2) We show that the development of a formal epistemology to support knowledge discovery, in terms of how the knowledge can be acquired and justified, supports a constructive and developmental way to define appropriate knowledge acquisition processes. In particular, we have seen how cognitive procedures as the forgetting criterion are not only necessary when the working space is finite but it can even be beneficial in our setting. (3) Our approach is parametrisable to other cognitive or intelligent systems, as it works at a meta-level and is independent of the actual deductive and inductive mechanisms that are used underneath. (4) The nonmonoticity problem of knowledge acquisition and revision is approached in a more lightweight and robust way, and the system can cope with redundancy and even inconsistency without heavy conflict resolutions or complex semantic artifacts. (5) The problem of catastrophic forgetting and, thus, The Stability-Plasticity dilemma has been effectively overcome when acquiring knowledge allowing to our approach not only gain new knowledge, but also addressing it efficiently. (6) Its adaptive an off-the-shelf characteristics allow to feed our approach on dynamic data in real time, or near real time.\nGiven the flexibility of the approach we consider many avenues of future work. We plan to apply the setting to some other applications, by using the same or other deductive and inductive engines, and keep on with the integration into our learning system gErl [52]. Furthermore, it is also of interest the application of the principles used (MML evaluations and cognitive mechanisms) in other kind of AI systems such as decision support systems in order to help them make better decisions based on the best available data. Finally, two further desirable characteristics for our approach are also likely to be part of our future research: (a) interactiveness, namely, the ability to find an additional (human or not) source input if a problem statement is ambiguous or incomplete; (b) contextuality, in terms identify, understand and extract contextual elements such as syntax, semantics, domain , time, location, goal, . . . , which may be useful to move beyond the current knowledge acquisition systems."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work has been partially supported by the EU (FEDER) and the Spanish MINECO under grants TIN 2010-21062-C02-02, TIN 2013-45732-C4-1-P and FPI-ME grant BES-2011045099, and by Generalitat Valenciana PROMETEO2011/052.\nPart of this work is under consideration at Pattern Recognition Letters."
    }, {
      "heading" : "7 Appendix",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "The art of adaptive pattern recognition by a selforganizing neural network",
      "author" : [ "G. Carpenter", "S. Grossberg" ],
      "venue" : "Computer, vol. 21, no. 3, pp. 77–88, March 1988.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "An information measure for classification",
      "author" : [ "C.S. Wallace", "D.M. Boulton" ],
      "venue" : "The Computer Journal, vol. 11, no. 2, pp. 185–194, 1968. 27",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1968
    }, {
      "title" : "Catastrophic forgetting, rehearsal and pseudorehearsal",
      "author" : [ "A. Robins" ],
      "venue" : "Connection Science, vol. 7, pp. 123–146, 1995.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Pseudo-recurrent connectionist networks: An approach to the ”sensitivity-stability” dilemma",
      "author" : [ "R.M. French" ],
      "venue" : "Connection Science, vol. 9, pp. 353–379, 1997.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Avoiding catastrophic forgetting by coupling two reverberating neural networks",
      "author" : [ "B. Ans", "S. Rousset" ],
      "venue" : "Comptes Rendus de l’Acadmie des Sciences - Series {III} - Sciences de la Vie, vol. 320, no. 12, pp. 989 – 997, 1997.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Adaptive resonance theory: How a brain learns to consciously attend, learn, and recognize a changing world",
      "author" : [ "S. Grossberg" ],
      "venue" : "Neural Netw., vol. 37, pp. 1–47, Jan. 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Is learning the n-th thing any easier than learning the first",
      "author" : [ "S. Thrun" ],
      "venue" : "Advances in Neural Information Processing Systems, vol. 8, 1996, pp. 640–646.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "S.J. Pan", "Q. Yang" ],
      "venue" : "IEEE Trans. on Knowl. and Data Eng., vol. 22, no. 10, pp. 1345–1359, Oct. 2010.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A model of inductive bias learning",
      "author" : [ "J. Baxter" ],
      "venue" : "Journal of Artificial Intelligence Research, vol. 12, pp. 149–198, 2000.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Multitask learning: A knowledge-based source of inductive bias",
      "author" : [ "R. Caruana" ],
      "venue" : "Proceedings of the Tenth International Conference on Machine Learning. Morgan Kaufmann, 1993, pp. 41–48.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Ella: An efficient lifelong learning algorithm",
      "author" : [ "E. Eaton", "P.L. Ruvolo" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning (ICML-13), vol. 28, 2013, pp. 507–515.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Toward an architecture for never-ending language learning",
      "author" : [ "A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E.R. Hruschka Jr.", "T.M. Mitchell" ],
      "venue" : "Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI 2010), 2010.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Lessons from theory revision applied to constructive induction",
      "author" : [ "L.A. Rendell" ],
      "venue" : "Machine Learning Proceedings 1995: Proceedings of the Twelfth International Conference on Machine Learning, Tahoe City, California, July 9-12 1995, vol. 51. Morgan Kaufmann, 1995, p. 185.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Probabilistic first-order theory revision from examples",
      "author" : [ "A. Paes", "K. Revoredo", "G. Zaverucha", "V.S. Costa" ],
      "venue" : "Inductive Logic Programming. Springer, 2005, pp. 295–311.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A survey on concept drift adaptation",
      "author" : [ "J. Gama", "I. Žliobaitė", "A. Bifet", "M. Pechenizkiy", "A. Bouchachia" ],
      "venue" : "ACM Computing Surveys (CSUR), vol. 46, no. 4, p. 44, 2014.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Inductive logic programming: Theory and methods",
      "author" : [ "S.H. Muggleton", "L. De Raedt" ],
      "venue" : "The Journal of Logic Programming, vol. 19, pp. 629–679, 1994. 28",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Scientific knowledge discovery using inductive logic programming",
      "author" : [ "S.H. Muggleton" ],
      "venue" : "Communications of the ACM, vol. 42, no. 11, pp. 42–46, 1999.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "An introduction to inductive programming",
      "author" : [ "P. Flener", "U. Schmid" ],
      "venue" : "Artificial Intelligence Review, vol. 29, no. 1, pp. 45–62, 2008.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Inductive programming meets the real world",
      "author" : [ "S. Gulwani", "J. Hernández-Orallo", "E. Kitzelmann", "S.H. Muggleton", "U. Schmid", "B. Zorn" ],
      "venue" : "2014.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Incremental learning of functional logic programs",
      "author" : [ "C. Ferri-Ramı́rez", "J. Hernández-Orallo", "M.J. Ramı́rez-Quintana" ],
      "venue" : "Functional and Logic Programming. Springer, 2001, pp. 233–247.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Cumulative learning in the lambda calculus",
      "author" : [ "R. Henderson" ],
      "venue" : "Ph.D. dissertation, Imperial College London, 2014.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Statistical and Inductive Inference by Minimum Message Length (Information Science and Statistics)",
      "author" : [ "C. Wallace" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2005
    }, {
      "title" : "Hypothesis selection and testing by the mdl principle",
      "author" : [ "J. Rissanen" ],
      "venue" : "The Computer Journal, vol. 42, no. 4, pp. 260–269, 1999.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Advances in minimum description length: Theory and applications",
      "author" : [ "P.D. Grünwald", "I.J. Myung", "M.A. Pitt" ],
      "venue" : "MIT press,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2005
    }, {
      "title" : "Constructive reinforcement learning",
      "author" : [ "J. Hernández-Orallo" ],
      "venue" : "International Journal of Intelligent Systems, vol. 15, no. 3, pp. 241–264, 2000.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Explanatory and creative alternatives to the MDL priciple",
      "author" : [ "J. Hernández-Orallo", "I. Garćıa-Varea" ],
      "venue" : "Foundations of Science, vol. 5, no. 2, pp. 185–207, 2000.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Authoritative sources in a hyperlinked environment",
      "author" : [ "J.M. Kleinberg" ],
      "venue" : "J. ACM, vol. 46, no. 5, pp. 604–632, Sep. 1999.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "The anatomy of a large-scale hypertextual web search engine",
      "author" : [ "S. Brin", "L. Page" ],
      "venue" : "Comput. Netw. ISDN Syst., vol. 30, no. 1-7, pp. 107–117, Apr. 1998.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "The stochastic approach for link-structure analysis (salsa) and the tkc effect",
      "author" : [ "R. Lempel", "S. Moran" ],
      "venue" : "Comput. Netw., vol. 33, no. 1-6, pp. 387–401, Jun. 2000.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A fuzzy cognitive structure for pattern recognition",
      "author" : [ "W. Pedrycz" ],
      "venue" : "Pattern Recognition Letters, vol. 9, no. 5, pp. 305 – 313, 1989.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Learning to learn: From smart machines to intelligent machines",
      "author" : [ "B. Raducanu", "J. Vitri" ],
      "venue" : "Pattern Recognition Letters, vol. 29, no. 8, pp. 1024 – 1032, 2008, pattern Recognition in Interdisciplinary Perception and Intelligence {PRintPerclntel}.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Modeling human color categorization",
      "author" : [ "E. van den Broek", "T. Schouten", "P. Kisters" ],
      "venue" : "Pattern Recognition Letters, vol. 29, no. 8, pp. 1136 – 1144, 2008, pattern Recognition in Interdisciplinary Perception and Intelligence {PRintPerclntel}. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0167865507002759 29",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Common-sense reasoning for human action recognition",
      "author" : [ "J.M. del Rincn", "M.J. Santofimia", "J.-C. Nebel" ],
      "venue" : "Pattern Recognition Letters, vol. 34, no. 15, pp. 1849 – 1860, 2013, smart Approaches for Human Action Recognition. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0167865512003509",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1849
    }, {
      "title" : "From visual patterns to semantic description: A cognitive approach using artificial curiosity as the foundation",
      "author" : [ "D.M. Ramk", "K. Madani", "C. Sabourin" ],
      "venue" : "Pattern Recognition Letters, vol. 34, no. 14, pp. 1577 – 1588, 2013, innovative Knowledge Based Techniques in Pattern Recognition.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Knowledge discovery from data?",
      "author" : [ "M.J. Pazzani" ],
      "venue" : "Intelligent systems and their applications, IEEE,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2000
    }, {
      "title" : "Concept cells: the building blocks of declarative memory functions",
      "author" : [ "R.Q. Quiroga" ],
      "venue" : "Nature Reviews Neuroscience, vol. 13, pp. 587–597, 2012.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Forget it!",
      "author" : [ "F. Lin", "R. Reiter" ],
      "venue" : "Proceedings of the AAAI Fall Symposium on Relevance,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1994
    }, {
      "title" : "Propositional independence: Formula-variable independence and forgetting",
      "author" : [ "J. Lang", "P. Liberatore" ],
      "venue" : "Journal of Artificial Intelligence Research, vol. 18, p. 2003, 2003.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Reasoning under inconsistency: A forgetting-based approach",
      "author" : [ "J. Lang", "P. Marquis" ],
      "venue" : "Artif. Intell., vol. 174, no. 12-13, pp. 799–823, Aug. 2010.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Solving logic program conflict through strong and weak forgettings",
      "author" : [ "Y. Zhang", "N.Y. Foo" ],
      "venue" : "Artificial Intelligence, vol. 170, no. 89, pp. 739 – 778, 2006.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Semantic forgetting in answer set programming",
      "author" : [ "T. Eiter", "K. Wang" ],
      "venue" : "Artificial Intelligence, vol. 172, no. 14, pp. 1644 – 1672, 2008.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Forgetting in logic programs under strong equivalence.",
      "author" : [ "Y. Wang", "Y. Zhang", "Y. Zhou", "M. Zhang" ],
      "venue" : "in KR,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2012
    }, {
      "title" : "Knowledge forgetting: Properties and applications",
      "author" : [ "Y. Zhang", "Y. Zhou" ],
      "venue" : "Artificial Intelligence, vol. 173, no. 1617, pp. 1525 – 1537, 2009.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Variable forgetting in reasoning about knowledge",
      "author" : [ "K. Su", "A. Sattar", "G. Lv", "Y. Zhang" ],
      "venue" : "Journal of Artificial Intelligence Research, vol. 35, no. 2, p. 677, 2009.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On the progression of knowledge in the situation calculus",
      "author" : [ "Y. Liu", "X. Wen" ],
      "venue" : "IJCAI Proceedings-International Joint Conference on Artificial Intelligence, vol. 22, no. 1, 2011, p. 976.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Forgetting for knowledge bases in dl-lite",
      "author" : [ "Z. Wang", "K. Wang", "R. Topor", "J. Pan" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence, vol. 58, no. 1-2, pp. 117–151, 2010.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Forgetting actions in domain descriptions",
      "author" : [ "E. Erdem", "P. Ferraris" ],
      "venue" : "Proc. of the 22nd AAAI Conference on Artificial Intelligence, 2007, pp. 409–414. 30",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Online sequential extreme learning machine with forgetting mechanism",
      "author" : [ "J. Zhao", "Z. Wang", "D.S. Park" ],
      "venue" : "Neurocomputing, vol. 87, pp. 79–89, 2012.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Biological grounding of recruitment learning and vicinal algorithms in longterm potentiation",
      "author" : [ "L. Shastri" ],
      "venue" : "Emergent Neural Computational Architectures Based on Neuroscience, ser. Lecture Notes in Computer Science, S. Wermter, J. Austin, and D. Willshaw, Eds. Springer Berlin Heidelberg, 2001, vol. 2036, pp. 348–367.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Handbook of behavioral neuroscience",
      "author" : [ "L.N. Ekrem Dere", "Alexander Easton", "J.P. Huston" ],
      "venue" : "Handbook of Episodic Memory, ser. Handbook of Behavioral Neuroscience. Elsevier, 2008, vol. 18.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning with configurable operators and RL-based heuristics",
      "author" : [ "F. Mart́ınez-Plumed", "C. Ferri", "J. Hernández-Orallo", "M. Ramı́rez-Quintana" ],
      "venue" : "New Frontiers in Mining Complex Patterns, ser. LNCS, 2013, vol. 7765, pp. 1–16.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A knowledge growth and consolidation framework for lifelong machine learning systems",
      "author" : [ "F. Martınez-Plumed", "C. Ferri", "J. Hernández-Orallo", "M.J. Ramırez-Quintana" ],
      "venue" : "Proceedings of the 13th International Conference on Machine Learning and Applications. IEEE, 2014, pp. 111–116.",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "An Introduction to Kolmogorov Complexity and Its Applications, 3rd ed",
      "author" : [ "M. Li", "P.M. Vitányi" ],
      "venue" : null,
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2008
    }, {
      "title" : "Refinements of MDL and MML coding",
      "author" : [ "C.S. Wallace", "D.L. Dowe" ],
      "venue" : "Comput. J., vol. 42, no. 4, pp. 330–337, 1999.",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A mathematical theory of communication",
      "author" : [ "C.E. Shannon" ],
      "venue" : "Bell system technical journal, vol. 27, 1948.",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 1948
    }, {
      "title" : "An experimental comparison of human and machine learning formalisms",
      "author" : [ "S.H. Muggleton", "M. Bain", "J. Hayes-Michie", "D. Michie" ],
      "venue" : "In Proc. of 6th International Workshop on Machine Learning. Morgan Kaufmann, 1989, pp. 113–118.",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 1989
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We present an incremental, long-life view of knowledge acquisition which tries to improve task after task by determining what to keep, what to consolidate and what to forget, overcoming The Stability-Plasticity dilemma [1].",
      "startOffset" : 219,
      "endOffset" : 222
    }, {
      "referenceID" : 1,
      "context" : "In order to do that, we rate rules by introducing several metrics through the first adaptation, to our knowledge, of the Minimum Message Length (MML) principle [2] to a coverage graph, a hierarchical assessment structure which treats evidence and rules in a unified way.",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : "This lead us to one of the well-known constraints for AI systems: The StabilityPlasticity dilemma [1].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "Some of the proposed solutions include: (a) dual-memory systems simulating the presence of short and long-term memory [3, 4, 5], and (b) cognitive architectures such as the Adaptive Resonance Theory (ART) [6] emulating how the brain processes information.",
      "startOffset" : 118,
      "endOffset" : 127
    }, {
      "referenceID" : 3,
      "context" : "Some of the proposed solutions include: (a) dual-memory systems simulating the presence of short and long-term memory [3, 4, 5], and (b) cognitive architectures such as the Adaptive Resonance Theory (ART) [6] emulating how the brain processes information.",
      "startOffset" : 118,
      "endOffset" : 127
    }, {
      "referenceID" : 4,
      "context" : "Some of the proposed solutions include: (a) dual-memory systems simulating the presence of short and long-term memory [3, 4, 5], and (b) cognitive architectures such as the Adaptive Resonance Theory (ART) [6] emulating how the brain processes information.",
      "startOffset" : 118,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "Some of the proposed solutions include: (a) dual-memory systems simulating the presence of short and long-term memory [3, 4, 5], and (b) cognitive architectures such as the Adaptive Resonance Theory (ART) [6] emulating how the brain processes information.",
      "startOffset" : 205,
      "endOffset" : 208
    }, {
      "referenceID" : 6,
      "context" : "For instance, Lifelong Machine Learning (LML)[7] is concerned with the persistent and cumulative nature of learning, namely: (a) capable of retaining and using prior knowledge, and (b) capable of acquiring new knowledge over a series of prediction tasks.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "Similarly, Transfer Learning [8] and multitask learning [9, 10] take a similar perspective, where it is more explicit that the process is task-oriented, and knowledge and its structure does not always play a central role in these systems.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "Similarly, Transfer Learning [8] and multitask learning [9, 10] take a similar perspective, where it is more explicit that the process is task-oriented, and knowledge and its structure does not always play a central role in these systems.",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "Similarly, Transfer Learning [8] and multitask learning [9, 10] take a similar perspective, where it is more explicit that the process is task-oriented, and knowledge and its structure does not always play a central role in these systems.",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "ELLA (Efficient Lifelong Learning Algorithm) [11] and NELL (Never-Ending Language Learner) [12] are two more recent approaches to LML, which are able to integrate many capabilities.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "ELLA (Efficient Lifelong Learning Algorithm) [11] and NELL (Never-Ending Language Learner) [12] are two more recent approaches to LML, which are able to integrate many capabilities.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 12,
      "context" : "Other related topics are concept drift and theory revision [13, 14, 15], where some rules are replaced by new rules that are consistent with new experience.",
      "startOffset" : 59,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "Other related topics are concept drift and theory revision [13, 14, 15], where some rules are replaced by new rules that are consistent with new experience.",
      "startOffset" : 59,
      "endOffset" : 71
    }, {
      "referenceID" : 14,
      "context" : "Other related topics are concept drift and theory revision [13, 14, 15], where some rules are replaced by new rules that are consistent with new experience.",
      "startOffset" : 59,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "The areas of inductive logic programming [16, 17] or general inductive programming [18, 19] have seen several approaches for incremental [20] or cumulative systems [21].",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 16,
      "context" : "The areas of inductive logic programming [16, 17] or general inductive programming [18, 19] have seen several approaches for incremental [20] or cumulative systems [21].",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : "The areas of inductive logic programming [16, 17] or general inductive programming [18, 19] have seen several approaches for incremental [20] or cumulative systems [21].",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 18,
      "context" : "The areas of inductive logic programming [16, 17] or general inductive programming [18, 19] have seen several approaches for incremental [20] or cumulative systems [21].",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "The areas of inductive logic programming [16, 17] or general inductive programming [18, 19] have seen several approaches for incremental [20] or cumulative systems [21].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 20,
      "context" : "The areas of inductive logic programming [16, 17] or general inductive programming [18, 19] have seen several approaches for incremental [20] or cumulative systems [21].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 1,
      "context" : "When the theory or hypothesis is considered as a whole and separated from the evidence, we have many well-founded proposal, such as the MML principle [2, 22] or the similar (but posterior) MDL principle [23, 24].",
      "startOffset" : 150,
      "endOffset" : 157
    }, {
      "referenceID" : 21,
      "context" : "When the theory or hypothesis is considered as a whole and separated from the evidence, we have many well-founded proposal, such as the MML principle [2, 22] or the similar (but posterior) MDL principle [23, 24].",
      "startOffset" : 150,
      "endOffset" : 157
    }, {
      "referenceID" : 22,
      "context" : "When the theory or hypothesis is considered as a whole and separated from the evidence, we have many well-founded proposal, such as the MML principle [2, 22] or the similar (but posterior) MDL principle [23, 24].",
      "startOffset" : 203,
      "endOffset" : 211
    }, {
      "referenceID" : 23,
      "context" : "When the theory or hypothesis is considered as a whole and separated from the evidence, we have many well-founded proposal, such as the MML principle [2, 22] or the similar (but posterior) MDL principle [23, 24].",
      "startOffset" : 203,
      "endOffset" : 211
    }, {
      "referenceID" : 24,
      "context" : "However, for knowledge integration and consolidation it is necessary to assess each part of the theory independently, where different parts of the theory can have different degrees of validity, probability or reinforcement [25, 26].",
      "startOffset" : 223,
      "endOffset" : 231
    }, {
      "referenceID" : 25,
      "context" : "However, for knowledge integration and consolidation it is necessary to assess each part of the theory independently, where different parts of the theory can have different degrees of validity, probability or reinforcement [25, 26].",
      "startOffset" : 223,
      "endOffset" : 231
    }, {
      "referenceID" : 26,
      "context" : "The perspective of a network or hierarchy of nodes that get support from other nodes is more common in the area of link analysis in web graphs such as the HITS algorithm [27], PageRank [28] or SALSA [29], or in infometrics.",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 27,
      "context" : "The perspective of a network or hierarchy of nodes that get support from other nodes is more common in the area of link analysis in web graphs such as the HITS algorithm [27], PageRank [28] or SALSA [29], or in infometrics.",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 28,
      "context" : "The perspective of a network or hierarchy of nodes that get support from other nodes is more common in the area of link analysis in web graphs such as the HITS algorithm [27], PageRank [28] or SALSA [29], or in infometrics.",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 29,
      "context" : "Finally, knowledge acquisition has much to learn from the study of human cognition [30, 31, 32, 33, 34].",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 30,
      "context" : "Finally, knowledge acquisition has much to learn from the study of human cognition [30, 31, 32, 33, 34].",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 31,
      "context" : "Finally, knowledge acquisition has much to learn from the study of human cognition [30, 31, 32, 33, 34].",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 32,
      "context" : "Finally, knowledge acquisition has much to learn from the study of human cognition [30, 31, 32, 33, 34].",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 33,
      "context" : "Finally, knowledge acquisition has much to learn from the study of human cognition [30, 31, 32, 33, 34].",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 34,
      "context" : "We can fully realise the benefits of knowledge acquisition by paying attention to the cognitive factors that simplify the learning and processing of the knowledge which make the resulting models coherent, efficient, credible, easy to use and understandable [35].",
      "startOffset" : 257,
      "endOffset" : 261
    }, {
      "referenceID" : 35,
      "context" : "The ability to focus on what to discard what is not relevant is becoming more relevant not only in cognitive science and neuroscience [36], but also in artificial intelligence (e.",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 36,
      "context" : "The notion of forgetting, also known as variable elimination, has been widely investigated in the context of classical logic (propositional and first-order logic) [37, 38, 39] and developed under the notion of logical equivalence, that is, logically equivalent formulas (theories) will remain equivalent after forgetting the same set of propositional variables or literals.",
      "startOffset" : 163,
      "endOffset" : 175
    }, {
      "referenceID" : 37,
      "context" : "The notion of forgetting, also known as variable elimination, has been widely investigated in the context of classical logic (propositional and first-order logic) [37, 38, 39] and developed under the notion of logical equivalence, that is, logically equivalent formulas (theories) will remain equivalent after forgetting the same set of propositional variables or literals.",
      "startOffset" : 163,
      "endOffset" : 175
    }, {
      "referenceID" : 38,
      "context" : "The notion of forgetting, also known as variable elimination, has been widely investigated in the context of classical logic (propositional and first-order logic) [37, 38, 39] and developed under the notion of logical equivalence, that is, logically equivalent formulas (theories) will remain equivalent after forgetting the same set of propositional variables or literals.",
      "startOffset" : 163,
      "endOffset" : 175
    }, {
      "referenceID" : 38,
      "context" : "A similar approach but for reasoning from inconsistent propositional bases is proposed in [39].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 39,
      "context" : "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.",
      "startOffset" : 145,
      "endOffset" : 157
    }, {
      "referenceID" : 40,
      "context" : "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.",
      "startOffset" : 145,
      "endOffset" : 157
    }, {
      "referenceID" : 41,
      "context" : "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.",
      "startOffset" : 145,
      "endOffset" : 157
    }, {
      "referenceID" : 42,
      "context" : "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.",
      "startOffset" : 274,
      "endOffset" : 286
    }, {
      "referenceID" : 43,
      "context" : "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.",
      "startOffset" : 274,
      "endOffset" : 286
    }, {
      "referenceID" : 44,
      "context" : "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.",
      "startOffset" : 274,
      "endOffset" : 286
    }, {
      "referenceID" : 45,
      "context" : "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.",
      "startOffset" : 394,
      "endOffset" : 398
    }, {
      "referenceID" : 46,
      "context" : "Forgetting (abstracting from) actions in planning has been also investigated in [47].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 47,
      "context" : "Finally, in [48] is proposed a forgetting mechanism for an online learning algorithm to learn sequential data with timelines able to gradually expel the outdated data that could become a possible source of misleading information.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 48,
      "context" : "Some studies about episodic memory in humans [49, 50] claim that memory traces in the hippocampus are not permanent and are occasionally transferred to neocortical areas in the brain through a consolidation processes.",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 49,
      "context" : "Some studies about episodic memory in humans [49, 50] claim that memory traces in the hippocampus are not permanent and are occasionally transferred to neocortical areas in the brain through a consolidation processes.",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 50,
      "context" : "In fact, this research started when developing our system gErl [52, 53].",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 51,
      "context" : "In fact, this research started when developing our system gErl [52, 53].",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : ", a rule learner, an inductive logic programming (ILP) system [16, 17] or an inductive programming (IP) system [18, 19]) and an off-the-shelf deductive engine (e.",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : ", a rule learner, an inductive logic programming (ILP) system [16, 17] or an inductive programming (IP) system [18, 19]) and an off-the-shelf deductive engine (e.",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 17,
      "context" : ", a rule learner, an inductive logic programming (ILP) system [16, 17] or an inductive programming (IP) system [18, 19]) and an off-the-shelf deductive engine (e.",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 18,
      "context" : ", a rule learner, an inductive logic programming (ILP) system [16, 17] or an inductive programming (IP) system [18, 19]) and an off-the-shelf deductive engine (e.",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 15,
      "context" : "Figure 2 shows an example of Coverage Graph of a well-known ILP problem [16]: the family relationship.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "its own complexity and the complexity of the rules it covers, a general criterion such as the Minimum Message Length [2] (MML) can be used as a starting criterion from which to derive new metrics.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 52,
      "context" : "The Minimum Message Length is one of the most popular selection criterion in inductive inference (for a formal justification and its relation to Kolmogorov complexity and the related MDL principle, see [54, 55, 22]).",
      "startOffset" : 202,
      "endOffset" : 214
    }, {
      "referenceID" : 53,
      "context" : "The Minimum Message Length is one of the most popular selection criterion in inductive inference (for a formal justification and its relation to Kolmogorov complexity and the related MDL principle, see [54, 55, 22]).",
      "startOffset" : 202,
      "endOffset" : 214
    }, {
      "referenceID" : 21,
      "context" : "The Minimum Message Length is one of the most popular selection criterion in inductive inference (for a formal justification and its relation to Kolmogorov complexity and the related MDL principle, see [54, 55, 22]).",
      "startOffset" : 202,
      "endOffset" : 214
    }, {
      "referenceID" : 1,
      "context" : "This message can be re-stated in a Bayesian form [2] with the length of the first part of the message (the model) and the length of the second part (evidence covered).",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 54,
      "context" : "An information-theoretic interpretation of MML is that a given evidence E of probability P (E) can be coded by a message of length L(E) = −log2(P (E)) [56].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 55,
      "context" : "In particular, we focus on learning a model of legal moves of different pieces from a set of legal and illegal move examples (extracted from [58]).",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 50,
      "context" : "We plan to apply the setting to some other applications, by using the same or other deductive and inductive engines, and keep on with the integration into our learning system gErl [52].",
      "startOffset" : 180,
      "endOffset" : 184
    } ],
    "year" : 2015,
    "abstractText" : "The application of cognitive mechanisms to support knowledge acquisition is, from our point of view, crucial for making the resulting models coherent, efficient, credible, easy to use and understandable. In particular, there are two characteristic features of intelligence that are essential for knowledge development: forgetting and consolidation. Both plays an important role in knowledge bases and learning systems to avoid possible information overflow and redundancy, and in order to preserve and strengthen important or frequently used rules and remove (or forget) useless ones. We present an incremental, long-life view of knowledge acquisition which tries to improve task after task by determining what to keep, what to consolidate and what to forget, overcoming The Stability-Plasticity dilemma [1]. In order to do that, we rate rules by introducing several metrics through the first adaptation, to our knowledge, of the Minimum Message Length (MML) principle [2] to a coverage graph, a hierarchical assessment structure which treats evidence and rules in a unified way. The metrics are not only used to forget some of the worst rules, but also to set a consolidation process to promote those selected rules to the knowledge base, which is also mirrored by a demotion system. We evaluate the framework with a series of tasks in a chess rule learning domain.",
    "creator" : "LaTeX with hyperref package"
  }
}