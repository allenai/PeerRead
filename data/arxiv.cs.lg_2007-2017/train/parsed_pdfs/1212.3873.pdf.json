{
  "name" : "1212.3873.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Markov Decision Processes for Model Checking",
    "authors" : [ "© H. Mao", "Y. Chen", "M. Jaeger", "T. D. Nielsen", "K. G. Larsen", "Hua Mao", "Yingke Chen", "Manfred Jaeger", "Thomas D. Nielsen", "Kim G. Larsen", "Brian Nielsen" ],
    "emails" : [ "@cs.aau.dk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "U. Fahrenberg, A. Legay and C. Thrane: Quantities in Formal Methods (QFM 2012) EPTCS 103, 2012, pp. 49–63, doi:10.4204/EPTCS.103.6\nc© H. Mao, Y. Chen, M. Jaeger, T. D. Nielsen, K. G. Larsen, & B. Nielsen This work is licensed under the Creative Commons Attribution License.\nLearning Markov Decision Processes for Model Checking\nHua Mao, Yingke Chen, Manfred Jaeger, Thomas D. Nielsen, Kim G. Larsen, and Brian Nielsen Department of Computer Science\nAalborg University Denmark\n[huamao,ykchen,jaeger,tdn,kgl,bnielsen] @cs.aau.dk\nConstructing an accurate system model for formal model verification can be both resource demanding and time-consuming. To alleviate this shortcoming, algorithms have been proposed for automatically learning system models based on observed system behaviors. In this paper we extend the algorithm on learning probabilistic automata to reactive systems, where the observed system behavior is in the form of alternating sequences of inputs and outputs. We propose an algorithm for automatically learning a deterministic labeled Markov decision process model from the observed behavior of a reactive system. The proposed learning algorithm is adapted from algorithms for learning deterministic probabilistic finite automata, and extended to include both probabilistic and nondeterministic transitions. The algorithm is empirically analyzed and evaluated by learning system models of slot machines. The evaluation is performed by analyzing the probabilistic linear temporal logic properties of the system as well as by analyzing the schedulers, in particular the optimal schedulers, induced by the learned models."
    }, {
      "heading" : "1 Introduction",
      "text" : "Model checking is successfully used in many areas to check a formal system model against a specification given by a logical expression. However, to construct an accurate model of an industrial system is usually difficult and time consuming. The difficulty of model construction, or system modeling, is regarded by industry as a challenge to adopt other powerful model-driven development (MDD) techniques and tools as well. Meanwhile, the necessary accurate, updated and detailed documentations rarely exist for legacy software or 3rd party components. Therefore, we consider system model learning techniques [12–14, 16], which can automatically construct or learn an accurate high-level model from observations of a given black-box embedded system component. Afterwards, given a learned and explicitly represented model, model checking and other MDD techniques can be applied with other existing component models.\nFor learning non-probabilistic system models, Angluin’s approaches [2] has been well developed and implemented [1, 12, 14]. However, a disadvantage of those system models is that complex systems are often only partially observable via their interactions with the user. Even worse, the observation is often not noise-free. Compared with deterministic models, probabilistic models are more feasible to model a complicated real system and its physical components, unpredictable user interactions and the usage of randomized algorithms. In this paper, we focus on probabilistic models. Sen et al. [16] adapted the algorithm from [5] for learning Markov chain models in purpose of verification. In [13], a learning approach related to [16] is developed, and strong theoretical and experimental consistency results are established. Considering a limited situation that the target system is not fully under control and only a single observation sequence is available, the algorithm for learning variable order Markov chains [15] is developed to verify stationary system properties on the learned models [6].\nIn Markov chains, probabilistic choices may serve to model and quantify possible outcomes of randomized actions or the interface between a system and its environment. This, nevertheless, requires abundant statistical experiments to obtain adequate distributions to model the average behavior of the environment. It is a natural choice to model by nondeterminism a system which is open for interaction from environment, system properties then need to be guaranteed for all potential environments [17]. Therefore, Markov decision processes (MDPs), which exhibit both nondeterministic and probabilistic behavior, are widely used for modeling reactive systems [3]. In this paper, we adapted the algorithm for learning deterministic probabilistic finite automata to include nondeterministic actions. Particularly, we learn deterministic labeled Markov decision processes (DLMDPs), where input actions are chosen nondeterministically and outputs given inputs are determined probabilistically, from the observed input and output behavior of a reactive system. This leads to another motivation of the learning purposes. For large systems, we may be interested in only one component, and it receives certain inputs from the environment or other components. Then the learner can output a model which is the representation of this component.\nBesides model learning, statistical model checking (SMC) [11, 20] techniques can also be used to analyze black-box systems. Statistical model-checking uses hypothesis testing based on sampling runs of a system that allows the user to check to a desired level of confidence whether a given logical property holds with a given (minimum) probability. Unfortunately, this technique is not well suited to MDPs since the presence of nondeterminism making running for sample paths is not well defined [4] without an extra scheduler. Moreover, the model output by the model learning approach can be used to other properties without re-sampling, as well as being used for other MDD tasks.\nThe main contribution of this paper is the development of IOALERGIA algorithm for learning DLMDP, which is obtained as an adaptation of the previous ALERGIA [5] algorithm. In order to demonstrate the applicability, the new algorithm is applied to learning models for slot machines from observed system behavior, which is in the form of alternating sequences of inputs and outputs. The evaluation is performed by analyzing and comparing probabilistic linear time properties in the learned model and the known generating model, as well as maximal expected reward and optimal schedulers.\nThis paper is structured as follows: section 2 contains background material. Section 3 describes the procedure of generating learning data, while section 4 describes IOALERGIA algorithm. Section 5 demonstrate its applicability through a case study concerning slot machine. Section 6 concludes the paper."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Labeled Markov Decision Processes",
      "text" : "Definition 1 (LMDP) A labeled Markov decision processes (LMDP) is a tuple M = (Q,ΣI,ΣO,π,τ ,L)\n• Q is a finite set of states,\n• ΣI is a finite input alphabet, and ΣO is a finite output alphabet,\n• π : Q → [0,1] is an initial probability distribution such that ∑q∈Q π(q) = 1,\n• τ : Q×ΣI ×Q → [0,1] is the transition probability function such that for all q ∈ Q and all α ∈ ΣI , ∑q′∈Q τ(q,α ,q′) = 1, or ∑q′∈Q τ(q,α ,q′) = 0,\n• L : Q → ΣO is a labeling function.\nAn input α ∈ ΣI is enabled in state q ∈ Q if and only if ∑q′∈Q τ(q,α ,q′) = 1. Let Act(q) denote the set of enabled actions in q.\nDefinition 2 (DLMDP) A LMDP is deterministic if\n• There exists a state qs ∈ Q with π(qs) = 1, • For all q∈Q, α ∈ΣI and σ ∈ΣO, there exists at most one q′ ∈Q with L(q′)=σ and τ(q,α ,q′)> 0. We then also write τ(q,α ,σ) instead of τ(q,α ,q′)."
    }, {
      "heading" : "2.2 Strings",
      "text" : "Let ΣO(ΣIΣO)∗ and ΣO(ΣIΣO)ω denote the set of all finite, respectively infinite strings of alternative input and output symbols. For a finite string s = σ0α1σ1 . . .αnσn, αi ∈ ΣI and σi ∈ ΣO, the set of all its prefixes is defined as:\nprefix(s) = {σ0α1σ1 . . .αkσk | 0 ≤ k ≤ n,k ∈N}\nFor a set of strings S, prefix(S) denotes the set of all prefixes of strings s∈ S. We assume an lexicographic ordering on ΣO(ΣIΣO)∗.\nIn a DLMDP there is a tight connection between strings and states: given an observed string s there is a unique state q that the LMDP must be in. Conversely, every state q is associated with the set strings(q) of all strings that lead from the start state to q. We therefore use symbols q for states and s for strings to some extent interchangeably: s can also denote the state in a DLMDP reached by the string s. The association of strings with states, on the other hand, is not one-to-one. We can still identify q with the lexicographically minimal s ∈ strings(q), and may use q also to denote this string."
    }, {
      "heading" : "2.3 Scheduler",
      "text" : "A scheduler [3] for a MDP M is a function S : Q+ → ΣI such that S(q0q1 . . .qn) ∈ Act(qn) for all q0,q1, . . . ,qn ∈ Q+. The scheduler chooses in any state q one action α ∈ ΣI , and induces a Markov chain, i.e., the behavior of an MDP M under the decisions of scheduler S can be formalized by a Markov chain MS [3, Section 10.6].\nA labeled Markov chain (LMC) MS of an LMDP M induced by a scheduler S defines a probability measure PMS on (ΣO)ω which is the basis for associating probabilities with events in the LMC MS. The probability of a string s = σ0σ1 . . .σn,σ ∈ ΣO defined by MS is:\nPMS(s) = n\n∏ i=1 τS(σ0σ1 . . .σi−1,σi)\nwhere τS is the transition probability function of MS."
    }, {
      "heading" : "2.4 Probabilistic LTL",
      "text" : "Linear time temporal logic (LTL) over ΣO is defined as usual by the syntax\nϕ ::= a | ϕ1 ∧ϕ2 | ¬ϕ | ©ϕ | ϕ1Uϕ2 a ∈ ΣO\nFor better readability, we also use the derived temporal operators (always) and ♦ (eventually). Let ϕ be an LTL formula. For s=σ0σ1 . . .∈ (ΣO)ω , s[ j . . .] =σ jσ j+1σ j+2 . . . is the suffix of s starting with the ( j)th symbol σ j. Then the LTL semantics for infinite words over ΣO are as follows:\n• s |= true\n• s |= σ , iff σ = σ0 • s |= ϕ1 ∧ϕ1, iff s |= ϕ1 and s |= ϕ2 • s |= ¬ ϕ , iff s 2 ϕ\n• s |= © ϕ , iff s[1 . . .] |= ϕ\n• s |= ϕ1Uϕ2, iff ∃ j ≥ 0. s[ j . . .] |= ϕ2 and s[i . . .] |= ϕ1, for all 0 ≤ i < j\nThe syntax of probabilistic LTL (PLTL) is:\nφ ::= P⊲⊳r(ϕ) (⊲⊳ ∈≥, ≤, =; r ∈ [0,1]; ϕ ∈ LTL)\nA labeled Markov decision process M satisfies the PLTL formula P⊲⊳r(ϕ) iff PMS(ϕ)⊲⊳ r for all schedulers of M, where PMS is the probability distribution defined by the LMC induced by a scheduler S of M, and PMS(ϕ) is short for PMS(s|s |= ϕ ,s ∈ (ΣO)ω)\nThe quantitative analysis of an MDP M against specification ϕ amounts to establishing the lower and upper bounds that can be guaranteed, when ranging over all schedulers. This corresponds to computing\nPmaxM (ϕ) = sup S PMS(ϕ) and P min M (ϕ) = inf S PMS(ϕ)\nwhere the infimum and the supremum are taken over all schedulers for M."
    }, {
      "heading" : "3 Data Generation",
      "text" : "The data we learn from is generated by observing the running reactive system. From the system we can observe input actions which determine probability distributions over successor states, and outputs which are labels of successor states. The learning algorithm requires that all nondeterministic choices are resolved by a fair scheduler S which means each input action will be chosen infinitely often. We assume that the input and output will be observed alternately, and every observation sequence starts from the label of the initial state, and ends in a state, i.e. σ0α1σ1 . . .αnσn,with αi ∈ ΣI and σi ∈ ΣO.\nUsually, enabled and disabled actions for states in a black-box system are unknown. Therefore, we allow that all actions can be chosen on each state of the system. For enabled actions, the system will transit to other states, and the input and the corresponding label of the successor state will be collected. For disabled actions, the system will stay in the same state but give a special error message. Through this setting, enabled and disabled inputs could be distinguished. Furthermore, we denote the prompted error by err, thus the output alphabet ΣO is extended to ΣO ∪{err}. Due to the memoryless scheduler, the same disabled input on the same state could be chosen more than once, and the statistic information about err will be found necessary in the following compatibility test.\nAfter all nondeterministic choices have been resolved, let Sω1 ,S ω 2 , . . . be an independent family of PMS-distributed random variables (with values in ΣO(ΣIΣO)ω ), and L1,L2, . . . be an independent family of integer-valued random variables, such that the Li are also independent of the Sωi . We assume that we observe the finite observation sequences Si := σ0α1σ1 . . .αLiσLi , i.e., the first Li symbols of Sωi . Thus, we observe the independent run of the system for a period of time that is determined independently of the observed behavior (in particular, the observation does not automatically end when the system enters a deadlock or failure state – such a situation would rather lead to repeated deadlock or failure observations in the final part of the sequence). We assume that the Li are unbounded, i.e. P(Li > k)> 0 for all k ∈ N.\nThis will be satisfied by a geometric distribution for the Li. For some models, there exists a uniquely labeled absorbing state which can be identified by its observation (e.g., a failure state which can not recover). When prior knowledge is available, observations can be stopped when the model reaches that state.\nFinally, we denote with S[n] = S1, . . . ,Sn the sample consisting of the first n observations."
    }, {
      "heading" : "4 Learning",
      "text" : "IOALERGIA for learning DLMDP consists of two phases. Firstly, represent the data as I/O frequency prefix tree acceptor (IOFPTA) where common prefixes are combined together. Then, do compatibility test on the tree following lexicographical order. If two states are compatible which requires that the next state distributions given the same input are compatible, they and their successor states will be merged correspondingly.\n4.1 IOFPTA\nThe input and output frequency prefix tree acceptor IOFPTA is constructed as a representation of the set of strings S which captures the behavior of the reactive system under observation. Since in DLMDP, same sequences will lead to the same state, then in IOFPTA common prefixes are merged together and result in a tree shaped automaton. Each node in the tree is labeled by an output symbol σ ∈ ΣO, and each edge is labeled by an input action α ∈ ΣI . Every path from the root to a node corresponds to a string s ∈ prefix(S). The node s is associated with the frequency function f (s,α ,σ) (α ∈ ΣI , σ ∈ ΣO) which is the number of strings in S with the prefix sασ , and f (s,α) = ∑σ∈ΣO f (s,α ,σ). From one node in IOFPTA, given an input action and an output symbol, the next state can be uniquely determined. An IOFPTA can be transformed to DLMDP by normalizing frequencies f (s,α , ·) to τ(s,α , ·). As assumed in data generation phase, when the scheduler chooses a disabled input on a state in LMDP, the model will stay in the current state, and output the symbol err. We are going to take the special meaning of the err symbol into account in the IOFPTA construction. Specifically, s and sαerr would lead to the same state from the root state. We will take the special treatment for the err symbol, but there is no difference between it and other symbols in learning. A new node labeled by err will not be created as a successor node or we can say that the err nodes are folded up.\nExample 1 IOFPTA\nin node labeled by C when we meet the input β (which is drawn and linked by dash lines in Fig.1(b)). Then for each node the incoming frequencies are not equivalent to the outgoing frequencies.\n4.2 IOALERGIA\nIOALERGIA algorithm, is an adapted version of the ALERGIA algorithm [5, 8]. As seen in Example 1, the same state in generating LMDP could be reached by more than one sequences through running, which will create more than one node in the IOFPTA. The basic idea of this learning algorithm is to approximate the generating model by grouping together the nodes in IOFPTA which can be mapped to the same state in the generating model. The partition which is introduced by grouping nodes will be inferred by pairwise testing. The compatibility of two nodes is tested by comparing distributions defined by nondeterministic choices, and recursively testing on successor nodes. If two nodes in the tree pass the compatibility test which means they can be mapped to the same state in the generating model, then they will be merged, as well as their successor nodes.\nAlgorithm 1 IOALERGIA Input: : A dataset S and a parameter ε ∈ (0,1]; Output: : A DLMDP A;\n1: T,A ← IOFPTA(S); 2: RED ← qAs ; 3: BLUE ←{q | q = qAs ασ ,α ∈ ΣI ,σ ∈ ΣO,qAs ασ ∈ prefix(S)}; /* immediate successor states */ 4: while BLUE 6= /0 do 5: qb ← lexicographically minimal q ∈ BLUE; 6: merged ← false; 7: for qr ∈ RED /* in lexicographic order */ do 8: if Compatible(T,qr,qb,ε) then 9: A ← Merge(A,qr,qb);\n10: merged ← true; 11: end if 12: end for 13: if !merged then 14: RED ← RED∪{qb}; 15: end if 16: BLUE ← BLUE \\{qb}∪{q = qrασ | α ∈ ΣI ,σ ∈ ΣO,q ∈ prefix(S),qr ∈ RED,q /∈ RED} ; 17: end while 18: return makeDLMDP(A); /* normalize */\nIn the learning algorithm, firstly, two IOFPTAs T and A are constructed as the representation of the dataset S (line 1 of the Algorithm 1). The IOFPTA T is kept as a data representation from which relevant statistics are retrieved during the execution of the algorithm. The IOFPTA A is iteratively transformed by merging nodes which have passed the compatibility test. All compatibility is tested on T , and the reason for this is that it has a clear interpretation as empirical probabilities defined by the data S. Following the terminology from [8], Algorithm 1 maintains two sets of states: RED states, which have already been determined as representative states of partitions and will be included in the final output DLMDP, and BLUE states which are going to be tested. Initially, RED contains only the initial state while BLUE contains the immediate successor states of the initial state. During iterations, the lexicographically minimal node qb in BLUE will be chosen. If there exists a state qr in RED which is\ncompatible with qb, then qb and its successor nodes are going to be merged into qr and its corresponding successor states. If qb is not compatible with any state in RED, it will be included in RED. At the end of each iteration, BLUE is going to be updated as the margin between RED and the remaining states, in the other word, the set of states which are immediate successor states of RED but not included in it. After merging all possible compatible nodes in the tree, the frequencies in A are going to be normalized by the Algorithm 1 (line 18). Then a DLMDP is constructed."
    }, {
      "heading" : "4.3 Compatibility Test",
      "text" : "Algorithm 2 demonstrates the compatibility test. It will return true if two nodes are compatible, i.e., the distance of distributions for every action is within the Hoeffding bound [19], Algorithm 3, parameterized by ε . Formally, two nodes qr and qb are ε-compatible (1 ≥ ε > 0), if it holds that:\n1. L(qr) = L(qb)\n2. Hoeffding( f (qr,α ,σ), f (qr,α), f (qb,α ,σ), f (qb,α),ε) is TRUE, for all α ∈ ΣI and σ ∈ ΣO .\n3. Nodes qrασ and qbασ are ε-compatible, for all α ∈ ΣI , and σ ∈ ΣO\nCondition 1) requires two nodes in the tree to have the same label. Condition 2) defines the compatibility between each outgoing transition with the same input action respectively from state qr and qb. The last condition requires the compatibility to be recursively satisfied for every pair of successors of qr and qb. If two nodes in IOFPTA are compatible, then distributions for all input actions should pass the compatibility test.\nIn the original ALERGIA algorithm, termination probabilities of two nodes are compared, while not in Algorithm 2. The reason is that the termination probability is not included in the definition of DLMDP. In Algorithm 3, the distance of two empirical probabilities are compared with the Hoeffding bound. If there is few, even none, statistical evidence to support their difference, the distance is small. In particular, two nodes are compatible, if there is no evidence against that. The err information is used to discriminate two nodes which have different enabled actions. For example, there are q1 and q2, and input action α is only enabled on q1. For q1, f (q1,α ,σ)> 0,σ 6= err and f (q1,α ,err) = 0, while f (q2,α) = f (q2,α ,err)> 0. Comparing the empirical probability distribution over ΣO including err, q1 and q2 can not be compatible."
    }, {
      "heading" : "4.4 Merge states",
      "text" : "If two states qr and qb are compatible, qb will be merged to qr. The Merge procedure (line 9 of the Algorithm 1) follows the same way as described in [8]: firstly, the (unique) transition leading to qb from its predecessor node q′ ( f A(q′,α ,qb)> 0) is re-directed to qr by setting f A(q′,α ,qr)← f A(q′,α ,qb) and f A(q′,α ,qb) = 0. Then, successor nodes of qb will be recursively folded to the corresponding successor nodes of qr.\nExample 2 Merge States Fig. 2 shows the procedure that the node qb (shadowed) will be merge to the node qr (shadowed double circle). In (a), the transition from the node q′ to qb firstly redirected to qr. In (b), transitions from qb to three successor nodes labeled with A, B and C, will be folded into the corresponding successor nodes of qr, respectively. (c) illustrates the result after merge.\nAlgorithm 2 Compatible Input: : IOFPTA T , nodes qr and qb, ε ∈ (0,1] Output: : true if qr and qb are compatible\n1: if L(qr) 6= L(qb) then 2: return false 3: end if 4: for α ∈ ΣI do 5: for σ ∈ ΣO do 6: if !Hoeffding( f T (qr,α ,σ), f T (qr,α) f T (qb,α ,σ), f T (qb,α),ε) then 7: return false 8: end if 9: if !Compatible(T,qrασ ,qbασ ,ε) then\n10: return false 11: end if 12: end for 13: end for 14: return true\nAlgorithm 3 Hoeffding Input: : f1,n1, f2,n2,ε ∈ (0,1] Output: : true if f1/n1 and f2/n2 are sufficiently close\n1: if n1 == 0 or n2 == 0 then 2: return true 3: end if 4: return | f1n1 − f2 n2 |< ( √ 1 n1 + √ 1 n2 ) · √ 1 2 ln 2 ε"
    }, {
      "heading" : "4.5 Discussion",
      "text" : "The algorithm takes a set S of sample sequences and a parameter ε as inputs. Here ε is used to bound the type-I error, which is the probability of wrongly rejecting a correct compatibility hypothesis. Smaller\nvalues of ε lead to loose Hoeffding bounds and making IOALERGIA output a smaller model. For any particular finite sample size we try to tune the choice of ε so as to obtain the best approximation to the real model. In order to do this we run IOALERGIA with different ε values, and evaluate the learned model using the Bayesian Information Criterion (BIC) score. This score combines the likelihood of a model with a term penalizing model complexity. Concretely, the BIC score of a DLMDP A given data S is defined as\nBIC(A | S) := log(PA(S))−1/2 |A | log(N)\nwhere |A |=|Q | · |ΣI | · |ΣO | is the number of free parameters in the model. N is the number symbols in the data. Using a golden section search [18, Section E.1.1] we systematically search for an ε value maximizing the BIC score of the learned model. Our algorithm is implemented in Matlab and is available for download at http://mi.cs.aau.dk/code/ioalergia.\nA convergence analysis, similar to the analysis in [7, 13] for deterministic Markov chain models, can be obtained for IOALERGIA: first, one can show that in the large sample limit, IOALERGIA will identify up to bisimulation equivalence the structure of the true model from which the data was sampled; the structure of a model refers to all of its components, except the probability values of transitions. Second, the parameters in the learned model will converge to the corresponding parameter values in the true model. As a slight refinement of Theorem 2 in [13], one then obtains that for any LTL formula ϕ :\nP( lim n→∞ PmaxAn (ϕ) = PmaxM (ϕ)) = 1,and P( limn→∞ P min An (ϕ) = PminM (ϕ)) = 1;\nwhere An is the DLMDP returned by IOALERGIA on data S[n]. As also observed in [13], similar results do not carry over to PCTL formulas."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we are going to show the applicability of the IOALERGIA algorithm using a case study based on the slot machine [9]. The slot machine we considered has 3 reels, named as reel-1, reel-2 and reel-3, and each reel contains 5 different symbols: lemon, grape, cherry, bar and, apple. The slot machine will return a prize based on the combination of symbols on those 3 reels. The prizes for different configurations are shown in Table 1(a). We extend the basic gambling machine as follows: at each round the player can choose one of the reels to spin, and other reels will be kept. The player starts with paying 1 coin for first 3 spins, and afterwards each extra spin costs 1 additional coin. Each reel must be spun at least once, and the player can quit the game only if all reels have been spun. The behavior of the slot machine contains both probabilistic and nondeterministic aspects. Specifically, the symbol show for each reel is probabilistic, but the choice of which reel to spin is nondeterministic.\nIn the following parts of this section, the algorithm will be applied for learning deterministic and nondeterministic models for different number of spins. A memoryless and random scheduler with a uniform distribution over all input actions, that modeling the fair requirement, is used in the data generation procedure. For experiment, we analyze the behavior of learned models by comparing them with known generating models in terms of maximal and minimal probabilities of winning a specific reward as well as the maximal expected reward in general. These probabilities and rewards are all computed by PRISM [10]. We will also analyze the accuracy that the optimal action in the learned model given symbols on reels and number of times the reels have been spun.\nTable 1: Prize\nreel-1 reel-2 reel-3 Prize\nbar bar bar 10 cherry cherry cherry 10 grapes grapes grapes 10\n? bar bar 5 cherry ? cherry 5 grapes grapes ? 5\n? ? bar 2 ? ? cherry 1\nTable 2: Summery of slot machines\nDeterministic\nSlot Machine\nNondeterministic\nSlot Machine\nN |Q| |Tran| |Q| |Tran|\n4 437 4021 510 4959\n6 867 10721 1012 13291\n8 1297 17421 1514 21623\n10 1727 24121 2016 29955"
    }, {
      "heading" : "5.1 Learning models from Deterministic systems",
      "text" : "We implemented the slot machine in PRISM. The distribution for 3 reels showing different symbols are (0.2,0.2,0.1,0.3,0.2), (0.2,0.1,0.3,0.2,0.2), and (0.2,0.3,0.2,0.1,0.2), respectively. In this model, there are 4 actions: spin reel-1 (sp1), spin reel-2 (sp2), spin reel-3 (sp3), and get the prize (pay), thus ΣI = {sp1,sp2,sp3, pay}. Every state is labeled by the combination of states on the 3 reels and the number of times the reels have been spun. We also attached reward variables to the states which are labeled by prize. Table 2 shows statistics for models with various number of spins. Here, N (N ≥ 3) is the number of spins, |Q| is the number of states, and |Tran| is the number of transitions.\nThe generating model is a deterministic LMDP. The results of applying the learning algorithm for different data sets are produced by the generating model are summarized in Table 3: |S| is the number of symbols in the dataset (×103), |Seq| is the number of sequences in the dataset; |IOFPTA| is the number of nodes in the IOFPTA; Time is the learning time (in seconds), including the time for constructing IOFPTA and the average time for each iteration performed by the golden section search (typically the golden section search terminated after 14 to 19 iterations); ‘ε range’ is the interval (identified using the golden section search) for ε for which a BIC-optimal DLMDP is learned, |Q| is the number of states in the learned model.\nFig. 3(a) and (b) show the maximal and minimal probabilities of eventually getting different prizes using the Pmax(♦L coins) and Pmin(♦L coins), where L∈ {0,1,2,5,10} (on both generating model and learned models, for N = 4,6,8,10). As the size of dataset increases, the learned models provide better approximations of the maximal and minimal probabilities defined for the generating models. Using PRISM, the maximal expected reward for one gamble (Rmax(♦ stop)) can be computed. In Fig. 3(c), for various initially bought spin chances, the maximal expected rewards for the learned models (dashed lines) are all approaching the ones for the generating models as the sizes of the datasets increase.\nThe optimal action which reel to spin next for a specific configuration of the reels, can also be accurately preserved by learned models. For example, given that there are three apples on reels and we only have 1 spin left, the best choice is to spin the 3rd reel since taking any other action will not produce a prize. We consider the 125 configurations where every reel has been spun once. Given a specific configuration Ci, the optimal action in the learned model and the generating model are denoted as Act li and Actgi , respectively. We define a criterion which interpret the accuracy of optimal actions inferred by the learned model against the generating model as follows:\nAcc = ∑125i=1 Pmax(Ci) · |Act li ∩Act g i |\n|Act li |\nWhere, Pmax(Ci) is the maximal probability of reaching configuration Ci. As shown in Fig. 3 (d), by increasing the size of dataset, the learned models have almost the same optimal actions as the generating models. Even with very limited data amount, accuracies for optimal actions in learned models are always greater than 25%, which is the probability of randomly choosing an optimal action."
    }, {
      "heading" : "5.2 Learning models from Nondeterministic systems",
      "text" : "In order to make the slot machine more interesting, we increase the prize for three bars but reduce the probability of getting that. This is done by adding another bar on reel-2, two bars, denoted as b1 and b2, that are indistinguishable, but have different mechanical characteristics. The probability for these two bars depend on the symbols on other two reels.\nThe distributions for all reels are shown in Table 4(a) and Table 4(b). Since reels are no longer independent, we name refer to machine as hooked slot machine. In this machine, the probability of getting 3 bars is decreased, but the reward for getting 3 bars is 20 coins. Every other configuration has the same prize as the previous game. After this modification, the generating model becomes nondeterministic, and its statistics listed in Table 2.\nmodels. The learning results are summarized in Table 5, where each column has the same meaning as in Table 3. Given sufficient dtat, we observed that learned models have the same number of states as the deterministic models of the previous slot machine, thus the states introduced by the extra symbol on reel-2 was not get identified. The reason is that states labeled by b1 on reel-2 and b2 on reel-2 are mixed and generally observed as bar on reel-2.\nFig. 4 shows maximal and minimal probabilities for getting different prizes, maximal rewards from the initial state and the accuracy of the optimal action. Given adequate data, learned deterministic models provide good approximations for nondeterministic generating models in terms of maximal probability, minimal probability and the maximal expected reward. On the other hand, the accuracy of choosing optimal action in next step is no longer as good as before. Nevertheless, the suggestion given by learned model is still better than random choice (which has 25% accuracy) in most cases.\nThe generating model is a nondeterministic LMDP, so there is no guarantee that the learned model\npreserves all PLTL properties . For example, suppose there are two bars after two spins, and corresponding to the configurations ‘bar, bar, not-spun’ (C1), ‘bar, not-spun, bar’ (C2), and ‘not-spun, bar, bar’ (C3). From these configurations, we can calculate the maximal probability of getting 3 bars after next spin (see Table 6). The maximal probability in the generating model for different N are the same since there is still one reel is that has not been spun. We can observe that conditional probabilities in learned models are quite different from the ones in generating models."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we have proposed the IOALERGIA algorithm for learning deterministic labeled Markov processes (DLMDPs). Given sequences of alternating input and output symbols, the algorithm can automatically construct a model, for the reactive system under observation, and we have similar convergence result of the IOALERGIA algorithm as given in [13] for deterministic Markov chain models. The algorithm is empirically analyzed using a case study based on slot machines. The learning results are evaluated by comparing in terms of PLTL properties and maximal expected rewards of both the learned model with the known generating models as well as the accuracy of optimal actions derived from the learned models.\nCompared to the learning algorithm for deterministic automata [14], further research is required to make the learning algorithm that suitable for routine use. In addition to empirically demonstrating the learned model is a good approximation, measuring the distance between the learned model and the generating model will be part of our future work. For compositional systems, this learning approach\ncould be extended to learn models for each individual component from the observed interaction among components. Moreover, the approach for learning DLMDP could be refined by active learning techniques that take advantage of interactive data acquisition."
    } ],
    "references" : [ {
      "title" : "Learning I/O Automata",
      "author" : [ "Fides Aarts", "Frits W. Vaandrager" ],
      "venue" : "In: CONCUR,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Learning regular sets from queries and counterexamples",
      "author" : [ "D. Angluin" ],
      "venue" : "Information and Computation",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1987
    }, {
      "title" : "Principles of model checking",
      "author" : [ "Christel Baier", "Joost-Pieter Katoen" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Partial Order Methods for",
      "author" : [ "J. Bogdoll", "L.M.F. Fioriti", "A. Hartmanns", "H. Hermanns" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Learning Stochastic Regular Grammars by Means of a State Merging Method",
      "author" : [ "R.C. Carrasco", "J. Oncina" ],
      "venue" : "In: ICGI,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1994
    }, {
      "title" : "Learning Markov Models for Stationary System Behaviors",
      "author" : [ "Y. Chen", "H. Mao", "M. Jaeger", "T.D. Nielsen", "K.G. Larsen", "B. Nielsen" ],
      "venue" : "In: NFM,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Identification in the Limit with Probability One of Stochastic Deterministic Finite Automata",
      "author" : [ "Colin de la Higuera", "Franck Thollard" ],
      "venue" : "ICGI, pp. 141–156",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2000
    }, {
      "title" : "Grammatical Inference — Learning Automata and Grammars",
      "author" : [ "Colin de la Higuera" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Probabilistic UML Statecharts for Specification and Verification a Case Study",
      "author" : [ "D.N. Jansen" ],
      "venue" : "Critical Systems Development with UML – Proc. of the UML’02 workshop,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2002
    }, {
      "title" : "Statistical Model Checking: An Overview",
      "author" : [ "A. Legay", "B. Delahaye", "S. Bensalem" ],
      "venue" : "RV, pp. 122–135",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Learning Meets Verification",
      "author" : [ "Martin Leucker" ],
      "venue" : "FMCO, pp. 127–151",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Learning Probabilistic Automata for Model Checking",
      "author" : [ "H. Mao", "Y. Chen", "M. Jaeger", "T.D. Nielsen", "K.G. Larsen", "B. Nielsen" ],
      "venue" : "http://doi",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "LearnLib: A Library for Automata Learning and Experimentation",
      "author" : [ "H. Raffelt", "B. Steffen" ],
      "venue" : "In: FASE,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length",
      "author" : [ "D. Ron", "Y. Singer", "N. Tishby" ],
      "venue" : "Machine Learning 25(2-3),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1996
    }, {
      "title" : "Learning Continuous Time Markov Chains from Sample Executions. In:  QEST, pp. 146–155",
      "author" : [ "K. Sen", "M. Viswanathan", "G. Agha" ],
      "venue" : "Available at http://doi.ieeecomputersociety",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2004
    }, {
      "title" : "An Introduction to Probabilistic Automata",
      "author" : [ "Mariëlle Stoelinga" ],
      "venue" : "Bulletin of the EATCS",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2002
    }, {
      "title" : "Introduction to Data Mining. Addison Wesley",
      "author" : [ "P.-N. Tan", "M. Steinbach", "V. Kumar" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Probability Inequalities for Sums of Bounded Random Variables",
      "author" : [ "H. Wassily" ],
      "venue" : "Journal of the American Statistical Association",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1963
    }, {
      "title" : "Probabilistic Verification of Discrete Event Systems Using Acceptance Sampling",
      "author" : [ "H.L.S. Younes", "R.G. Simmons" ],
      "venue" : "CAV, pp. 223–235. Available at http://dx.doi.org/10",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Therefore, we consider system model learning techniques [12–14, 16], which can automatically construct or learn an accurate high-level model from observations of a given black-box embedded system component.",
      "startOffset" : 56,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "Therefore, we consider system model learning techniques [12–14, 16], which can automatically construct or learn an accurate high-level model from observations of a given black-box embedded system component.",
      "startOffset" : 56,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "Therefore, we consider system model learning techniques [12–14, 16], which can automatically construct or learn an accurate high-level model from observations of a given black-box embedded system component.",
      "startOffset" : 56,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "Therefore, we consider system model learning techniques [12–14, 16], which can automatically construct or learn an accurate high-level model from observations of a given black-box embedded system component.",
      "startOffset" : 56,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "For learning non-probabilistic system models, Angluin’s approaches [2] has been well developed and implemented [1, 12, 14].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "For learning non-probabilistic system models, Angluin’s approaches [2] has been well developed and implemented [1, 12, 14].",
      "startOffset" : 111,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : "For learning non-probabilistic system models, Angluin’s approaches [2] has been well developed and implemented [1, 12, 14].",
      "startOffset" : 111,
      "endOffset" : 122
    }, {
      "referenceID" : 12,
      "context" : "For learning non-probabilistic system models, Angluin’s approaches [2] has been well developed and implemented [1, 12, 14].",
      "startOffset" : 111,
      "endOffset" : 122
    }, {
      "referenceID" : 14,
      "context" : "[16] adapted the algorithm from [5] for learning Markov chain models in purpose of verification.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[16] adapted the algorithm from [5] for learning Markov chain models in purpose of verification.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "In [13], a learning approach related to [16] is developed, and strong theoretical and experimental consistency results are established.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "In [13], a learning approach related to [16] is developed, and strong theoretical and experimental consistency results are established.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "Considering a limited situation that the target system is not fully under control and only a single observation sequence is available, the algorithm for learning variable order Markov chains [15] is developed to verify stationary system properties on the learned models [6].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 5,
      "context" : "Considering a limited situation that the target system is not fully under control and only a single observation sequence is available, the algorithm for learning variable order Markov chains [15] is developed to verify stationary system properties on the learned models [6].",
      "startOffset" : 270,
      "endOffset" : 273
    }, {
      "referenceID" : 15,
      "context" : "It is a natural choice to model by nondeterminism a system which is open for interaction from environment, system properties then need to be guaranteed for all potential environments [17].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 2,
      "context" : "Therefore, Markov decision processes (MDPs), which exhibit both nondeterministic and probabilistic behavior, are widely used for modeling reactive systems [3].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 9,
      "context" : "Besides model learning, statistical model checking (SMC) [11, 20] techniques can also be used to analyze black-box systems.",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 18,
      "context" : "Besides model learning, statistical model checking (SMC) [11, 20] techniques can also be used to analyze black-box systems.",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "Unfortunately, this technique is not well suited to MDPs since the presence of nondeterminism making running for sample paths is not well defined [4] without an extra scheduler.",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 4,
      "context" : "The main contribution of this paper is the development of IOALERGIA algorithm for learning DLMDP, which is obtained as an adaptation of the previous ALERGIA [5] algorithm.",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "• ΣI is a finite input alphabet, and ΣO is a finite output alphabet, • π : Q → [0,1] is an initial probability distribution such that ∑q∈Q π(q) = 1, • τ : Q×ΣI ×Q → [0,1] is the transition probability function such that for all q ∈ Q and all α ∈ ΣI , ∑q′∈Q τ(q,α ,q′) = 1, or ∑q′∈Q τ(q,α ,q′) = 0, • L : Q → ΣO is a labeling function.",
      "startOffset" : 79,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "• ΣI is a finite input alphabet, and ΣO is a finite output alphabet, • π : Q → [0,1] is an initial probability distribution such that ∑q∈Q π(q) = 1, • τ : Q×ΣI ×Q → [0,1] is the transition probability function such that for all q ∈ Q and all α ∈ ΣI , ∑q′∈Q τ(q,α ,q′) = 1, or ∑q′∈Q τ(q,α ,q′) = 0, • L : Q → ΣO is a labeling function.",
      "startOffset" : 165,
      "endOffset" : 170
    }, {
      "referenceID" : 2,
      "context" : "3 Scheduler A scheduler [3] for a MDP M is a function S : Q+ → ΣI such that S(q0q1 .",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "φ ::= P⊲⊳r(φ) (⊲⊳ ∈≥, ≤, =; r ∈ [0,1]; φ ∈ LTL)",
      "startOffset" : 32,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "IOALERGIA algorithm, is an adapted version of the ALERGIA algorithm [5, 8].",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "IOALERGIA algorithm, is an adapted version of the ALERGIA algorithm [5, 8].",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "Following the terminology from [8], Algorithm 1 maintains two sets of states: RED states, which have already been determined as representative states of partitions and will be included in the final output DLMDP, and BLUE states which are going to be tested.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 17,
      "context" : ", the distance of distributions for every action is within the Hoeffding bound [19], Algorithm 3, parameterized by ε .",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "The Merge procedure (line 9 of the Algorithm 1) follows the same way as described in [8]: firstly, the (unique) transition leading to qb from its predecessor node q′ ( f A(q′,α ,qb)> 0) is re-directed to qr by setting f A(q′,α ,qr)← f A(q′,α ,qb) and f A(q′,α ,qb) = 0.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "A convergence analysis, similar to the analysis in [7, 13] for deterministic Markov chain models, can be obtained for IOALERGIA: first, one can show that in the large sample limit, IOALERGIA will identify up to bisimulation equivalence the structure of the true model from which the data was sampled; the structure of a model refers to all of its components, except the probability values of transitions.",
      "startOffset" : 51,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "A convergence analysis, similar to the analysis in [7, 13] for deterministic Markov chain models, can be obtained for IOALERGIA: first, one can show that in the large sample limit, IOALERGIA will identify up to bisimulation equivalence the structure of the true model from which the data was sampled; the structure of a model refers to all of its components, except the probability values of transitions.",
      "startOffset" : 51,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "As a slight refinement of Theorem 2 in [13], one then obtains that for any LTL formula φ :",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "As also observed in [13], similar results do not carry over to PCTL formulas.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 8,
      "context" : "In this section, we are going to show the applicability of the IOALERGIA algorithm using a case study based on the slot machine [9].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 11,
      "context" : "Given sequences of alternating input and output symbols, the algorithm can automatically construct a model, for the reactive system under observation, and we have similar convergence result of the IOALERGIA algorithm as given in [13] for deterministic Markov chain models.",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 12,
      "context" : "Compared to the learning algorithm for deterministic automata [14], further research is required to make the learning algorithm that suitable for routine use.",
      "startOffset" : 62,
      "endOffset" : 66
    } ],
    "year" : 2012,
    "abstractText" : "Constructing an accurate system model for formal model verification can be both resource demanding and time-consuming. To alleviate this shortcoming, algorithms have been proposed for automatically learning system models based on observed system behaviors. In this paper we extend the algorithm on learning probabilistic automata to reactive systems, where the observed system behavior is in the form of alternating sequences of inputs and outputs. We propose an algorithm for automatically learning a deterministic labeled Markov decision process model from the observed behavior of a reactive system. The proposed learning algorithm is adapted from algorithms for learning deterministic probabilistic finite automata, and extended to include both probabilistic and nondeterministic transitions. The algorithm is empirically analyzed and evaluated by learning system models of slot machines. The evaluation is performed by analyzing the probabilistic linear temporal logic properties of the system as well as by analyzing the schedulers, in particular the optimal schedulers, induced by the learned models.",
    "creator" : "LaTeX with hyperref package"
  }
}