{"conference": "ICLR 2017 conference submission", "title": "Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification", "abstract": "Recurrent neural networks have shown their ability to construct sentence or paragraph representations. Variants such as LSTM overcome the problem of vanishing gradients to some degree, thus being able to model long-time dependency. Still, these recurrent based models lack the ability of capturing complex semantic compositions. To address this problem, we propose a model which can incorporate local contexts with the guide of global context attention. Both the local and global contexts are obtained through LSTM networks. The working procedure of this model is just like how we human beings read a text and then answer a related question. Empirical studies show that the proposed model can achieve state of the art on some benchmark datasets. Attention visualization also verifies our intuition. Meanwhile, this model does not need pretrained embeddings to get good results.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "This paper presents a hierarchical attention-based method for document classification. \nThe main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). \nA simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). \nExperiments on three datasets are presented, however the results are mostly not state-of-the-art.\n\nI think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. \nWhy is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? \nI noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets?\nYou should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)\nI also suggest removing phrases such as \"Learning to Understand\" when presenting their model.\nOverall, I think that this submission is a better fit for the workshop.\n\nMinor comments:\n- gloal -> global\n- Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The consensus amongst reviewers' was that this paper, incorporating global context into classification, is not ready for publication. It provides no novelty over similar methods. The evaluation did not convince most of the reviewers. The paper seems peppered with unjustified and (as rather bluntly, but accurately, put by one reviewer) unscientific claims. Disappointingly, the authors did not respond to pre-review questions. Perhaps more understandably, they did not respond to the uniformly negative reviews of their paper to defend it. I see no reason to diverge from the reviewers' recommendation, and advocate rejection of this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "below acceptance threshold", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors did not bother responding or fixing any of the pre-review comments. Hence I repeat here:\n\nPlease do not make incredibly unscientific statements like this one:\n\"The working procedure of this model is just like how we human beings read a text and then answer a related question. \"\nReally, \"humans beings\" have an LSTM like model to read a text? Can you cite an actual neuroscience paper for such a claim? The answer is no, so please delete such statements from future drafts.\n\nGenerally, your experiments are about simple classification and the methods you're competing against are simple models like NB-SVM. So I would change the title, abstract ad introduction accordingly and not attempt hyperbole like \"Learning to Understand\" in the title.\n\nLastly, your attention level approach seems similar to dynamic memory networks by Kumar et al. they also have experiments for sentiment and it would be interesting to understand the differences to your model and compare to them.\n\nOther reviewers included further missing related work and fitting this paper into the context of current literature.\nGiven that no efforts were made to fix the pre-review questions and feedback, I doubt this will become ready in time for publication.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "24 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "official review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes to enhance the attention mechanism for sentiment classification by using global context computed by a Bi-LSTM. The proposed models outperform many existing models in the literature on 3 sentiment analysis datasets. \n\nThe key idea of using Bi-LSTM to compute global context for attention is actually not novel, as proposed several times in the literature, e.g., Luong et al (2015) and Shen & Lee (2016). Especially, Luong et al (2015) already proposed to combine global context with local context for attention.\n\nRegarding to the experiments, of course it would be nice if the model can work well without the need of tricks like dropout or pre-trained word embeddings. However, it would be even better if the model can work well using those tricks. The authors should show results of the models using those tricks and compare them to the results in the literature.  \n\n\nRef:\nLuong et al. Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "feedback", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a hierarchical attention-based method for document classification. \nThe main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). \nA simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). \nExperiments on three datasets are presented, however the results are mostly not state-of-the-art.\n\nI think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. \nWhy is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? \nI noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets?\nYou should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)\nI also suggest removing phrases such as \"Learning to Understand\" when presenting their model.\nOverall, I think that this submission is a better fit for the workshop.\n\nMinor comments:\n- gloal -> global\n- Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "Do you have more experimental results?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "feedback", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "01 Dec 2016", "TITLE": "prereview questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}, {"IS_META_REVIEW": true, "comments": "This paper presents a hierarchical attention-based method for document classification. \nThe main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). \nA simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). \nExperiments on three datasets are presented, however the results are mostly not state-of-the-art.\n\nI think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. \nWhy is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? \nI noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets?\nYou should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)\nI also suggest removing phrases such as \"Learning to Understand\" when presenting their model.\nOverall, I think that this submission is a better fit for the workshop.\n\nMinor comments:\n- gloal -> global\n- Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The consensus amongst reviewers' was that this paper, incorporating global context into classification, is not ready for publication. It provides no novelty over similar methods. The evaluation did not convince most of the reviewers. The paper seems peppered with unjustified and (as rather bluntly, but accurately, put by one reviewer) unscientific claims. Disappointingly, the authors did not respond to pre-review questions. Perhaps more understandably, they did not respond to the uniformly negative reviews of their paper to defend it. I see no reason to diverge from the reviewers' recommendation, and advocate rejection of this paper.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"TITLE": "below acceptance threshold", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "The authors did not bother responding or fixing any of the pre-review comments. Hence I repeat here:\n\nPlease do not make incredibly unscientific statements like this one:\n\"The working procedure of this model is just like how we human beings read a text and then answer a related question. \"\nReally, \"humans beings\" have an LSTM like model to read a text? Can you cite an actual neuroscience paper for such a claim? The answer is no, so please delete such statements from future drafts.\n\nGenerally, your experiments are about simple classification and the methods you're competing against are simple models like NB-SVM. So I would change the title, abstract ad introduction accordingly and not attempt hyperbole like \"Learning to Understand\" in the title.\n\nLastly, your attention level approach seems similar to dynamic memory networks by Kumar et al. they also have experiments for sentiment and it would be interesting to understand the differences to your model and compare to them.\n\nOther reviewers included further missing related work and fitting this paper into the context of current literature.\nGiven that no efforts were made to fix the pre-review questions and feedback, I doubt this will become ready in time for publication.", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "24 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "official review", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper proposes to enhance the attention mechanism for sentiment classification by using global context computed by a Bi-LSTM. The proposed models outperform many existing models in the literature on 3 sentiment analysis datasets. \n\nThe key idea of using Bi-LSTM to compute global context for attention is actually not novel, as proposed several times in the literature, e.g., Luong et al (2015) and Shen & Lee (2016). Especially, Luong et al (2015) already proposed to combine global context with local context for attention.\n\nRegarding to the experiments, of course it would be nice if the model can work well without the need of tricks like dropout or pre-trained word embeddings. However, it would be even better if the model can work well using those tricks. The authors should show results of the models using those tricks and compare them to the results in the literature.  \n\n\nRef:\nLuong et al. Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015", "IS_META_REVIEW": false, "RECOMMENDATION": 3, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "feedback", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "This paper presents a hierarchical attention-based method for document classification. \nThe main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). \nA simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). \nExperiments on three datasets are presented, however the results are mostly not state-of-the-art.\n\nI think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. \nWhy is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? \nI noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets?\nYou should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)\nI also suggest removing phrases such as \"Learning to Understand\" when presenting their model.\nOverall, I think that this submission is a better fit for the workshop.\n\nMinor comments:\n- gloal -> global\n- Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"DATE": "02 Dec 2016", "TITLE": "Do you have more experimental results?", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2"}, {"DATE": "02 Dec 2016", "TITLE": "feedback", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3"}, {"DATE": "01 Dec 2016", "TITLE": "prereview questions", "IS_META_REVIEW": false, "comments": "", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1"}], "authors": "Zhigang Yuan, Yuting Hu, Yongfeng Huang", "accepted": false, "id": "692"}