{
  "name" : "1212.3493.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sentence Compression in Spanish driven by Discourse Segmentation and Language Models",
    "authors" : [ "Alejandro Molina", "Juan-Manuel Torres-Moreno", "Iria da Cunha", "Eric SanJuan", "Gerardo Sierra" ],
    "emails" : [ "alejandro.molina@univ-avignon.fr", "juan-manuel.torres@univ-avignon.fr", "eric.sanjuan@univ-avignon.fr", "iria.dacunha@upf.edu", "gerardo.sierra@iingen.unam.mx" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n21 2.\n34 93\nv2 [\ncs .C\nL ]"
    }, {
      "heading" : "1 Introduction",
      "text" : "Automatic Text Summarization (ATS) is indispensable to cope with ever increasing volumes of valuable information. An abstract is by far the most concrete and most recognized kind of text condensation [1]. Sentences extraction allows to generate summaries by extraction sentences [10, 5, 24].\nSentence compression can be used to improve extract summarization [6, 13]. Previous works suggest that sentence segmentation could be helpful in sentence compression generation [18].\nIn this work we present an new automatic sentence compression generation approach. First sentences are segmented using a discourse segmenter and then, compression candidates are generated. Finally, the best candidate i.e., the most grammatical one, is selected based on its probability as a sequence in a Language Model.\nWe organized the rest of the paper as follows. Firstly, in section §2 we recall the mains concepts of sentence compression. Then, we present in §3 our compression candidates generation approach. Compression candidates evaluation is introduced in §4. Experimental results are showed in §5. Finally, section §6 presents conclusions and future work."
    }, {
      "heading" : "2 Sentence compression",
      "text" : "Sentence compression can be considered as a summarization at the sentence level. Sentence compression task is defined as follows:\n“Consider an input sentence as a sequence of n words W = (w1, w2, ..., wn). An algorithm may drop any subset of these words. The words that remain (order unchanged) form a compression” [7].\nThere are interesting algorithms to determine the removal of words in a sentence but humans tend also to delete long phrases in an abstract [15].\nRecent studies have found good results by concentrating on clauses, instead of isolated words. In [19] an algorithm first divides sentences into clauses prior to any elimination and then, compression candidates are scored based on Latent Semantic Analysis proposed in [4]. However, no component to mitigate grammaticality issues is included in this algorithm [19]. Although the results of this last work are in general good, in some cases the main subject of the sentence is removed. The authors attempted to solve this issue by including features in a machine learning approach [20].\nAs an alternative to clauses, some studies explore discourse structures to tackle the sentence compression task. Discourse chunking [18] is an alternative to discourse parsing, thereby, showing a direct application to sentence compression. The authors of this last work plausibly argued that, while discourse parsing at document-level stills poses a significant challenge, sentence-level discourse chunking could represent an alternative in languages with limited full discourse parsing tools. In addition, some sentence-level discourse models have shown accuracies comparable to human performance [17]."
    }, {
      "heading" : "3 Compression Candidates Generation",
      "text" : "In this work, we use a sentence-level discourse segmentation approach. Formally, “Discourse segmentation is the process of decomposing discourse into Elementary Discourse Units (EDUs), which may be simple sentences or clauses in a complex sentence, and from which discourse trees are constructed” [22]. Discourse segmentation is only the first stage for discourse parsing (the others are detection of rhetorical relations and building of discourse trees). However, we can consider segmentation at the sentence level in order to identify segments to be eliminated in the sentence compression task. This decomposition of a sentence into EDUs using only local information is called shallow discourse segmentation. In [14], the authors use a discourse segmenter in order to segment sentences in spanish. The discourse segmenter is described in [3] and is based in the Rhetorical Structure framework [11].\nWe propose that compression candidates be generated by deleting some discourse segments from the original sentence. Let be a sentence S the sequence of its k discourse segments: S = (s1, s2, ..., sk). A candidate, CCi, is a subsequence of S that preserves the original order of the segments. The original sentence always form a candidate, i.e., CC0 = S, this is convenient because sometimes there is no shorter grammatical version of the sentence, especially in short sentences that conform one single EDU. Since we do not consider the empty subsequence as a candidate, there are 2k−1 candidates. Furthermore, since we rarely have more than 5 discourse segments in a sentence, usually we create between 1 and 31 candidates, this, dramatically reduces the solution space given that k << n. The compression candidates are constructed using a O(2k) binary counter. In Example 1 we show all the candidates associated to a sentence extracted from our corpus.\nExample 1. CC0:[Además ella participó ese mismo año en el concierto en tributo a Freddie Mercury,][hablando acerca de la prevención necesaria][para combatir el SIDA.]1 CC1:[hablando acerca de la prevención necesaria][para combatir el SIDA.] CC2:[Además ella participó ese mismo año en el concierto en tributo a Freddie\n1English translation: [Also she participated that year in the concert in tribute to Freddie Mercury, ][talking about prevention needed][to fight AIDS.]\nMercury,][para combatir el SIDA.] CC3:[para combatir el SIDA.] CC4:[Además ella participó ese mismo año en el concierto en tributo a Freddie Mercury,][hablando acerca de la prevención necesaria] CC5:[hablando acerca de la prevención necesaria] CC6:[Además ella participó ese mismo año en el concierto en tributo a Freddie Mercury,]"
    }, {
      "heading" : "4 Compression Candidates Scoring with Language Model",
      "text" : "A Language Model (LM) estimates the probability distribution of natural language. Statistical language modeling [2, 12] is a technique widely used to assign a probability to a sequence of words. We assume that good compression candidates must have a high probability as sequences in a LM. In general, for a sentence W = (w1, w2, ..., wn), the probability of W is:\nP (wn1 ) = P (w1)P (w2|w1)P (w3|w 2 1)\n...P (wn|w n−1 1 )(1)\nWhere wba = (wa, ..., wb). The probabilities in a LM are estimated counting sequences from a corpus. Even though we will never be able to get enough data to compute the statistics for all possible sentences, we can base our estimations using big corpora and interpolation methods. In our experiments we use a big corpus with 1T words2 to get the sequences counts and a LM interpolation based on Jelinek-Mercer smoothing [2]:\nPinterp(wi|w i−1 i−n+1) =\nλ w i−1\ni−n+1\nPmax likelihood(wi|w i−1 i−n+1) +\n(1 − λwi−1 i−n+1\n)Pinterp(wi|w i−1 i−n+2)(2)\nIn equation (2) the maximum likelihood estimate of a sequence is interpolated with the smoothed lower-order distribution.\nFor a given candidate,\n(3) CCi = (s1, ..., sk) = (w1, ..., wn),\nwe assign a LM based score (Scoreg) based on its probability as in equation (4). For our experiments we use the Language Modeling Toolkit SRILM3 [21].\n(4) Scoreg(CCi) = Pinterp(CCi)\nn"
    }, {
      "heading" : "5 Experimental Results",
      "text" : "For the experiments, two annotators were required to compress each sentence following the instructions in [14]. The corpus contains four sub-corpora: Wikipedia sections, brief news,\n2LDC Catalog No.: LDC2009T25 ISBN: 1-58563-525-1 3Avaliable at http://www-speech.sri.com/projects/srilm/download.html\nscientific abstracts and short stories. Each sub-corpus has 20 texts composed of no more than 50 phrases each one (1939 tokens). We have randomly selected eight documents for evaluation, two of each sub-corpus.\nWe have generated abstract summaries selecting the best compression candidate of each sentence considering two different approaches:\n1. All system: Selecting the best scored candidate for each sentence.\n2. First system: Selecting the best candidate from those that include the first segment.\nFor comparison we have created Random system: a baseline system which applies a random compression. Random system eliminates some words of a given sentence at the same rate of human annotators.\nAfter compressions, three judges (different from annotators) read the eight summaries. The judges do not know the source of the final summary. They mark each sentence in the final summary if they found it grammatically incorrect in the context. In addition, they evaluate the global coherence of summaries after compressions. Coherence of summaries is scored with a categorical variable: a value of -1 is assigned for incoherent summaries, 0 if some coherences are found and +1 for coherent productions. The Compression Rate (CR) is defined as the proportion of content eliminated from the original document. It says how much of the content was eliminated.\nIn Table 1, we compare our systems and the two human annotators. The results confirm that scoring compressions before producing a summary improves the text quality in summaries with compressed sentences. It is very surprising that, for Human2, the number of compressions judged grammatically incorrect is greater than that of our systems. May be Human2 misunderstood the compression instructions. However, considering as limits the results of Human1 and Random baseline system we consider that the proportion of bad formed sentences is very low. We confirmed our initial intuition that preserving the first segment tends to save the main subject in most of the cases. The introduction of this simple heuristic in the First system improves the grammar quality of productions.\nTable 2 shows the result of comparing our systems using the two human summaries as references. We wanted to evaluate the content quality of summaries with the ROUGE package [9]. ROUGE is used to evaluate summaries because some results show that it correlates well with human judgements [8]. Results in Table 2 are opposite to what was expected. We assumed that Random system would have worst results with respect to First system. Looking at the judgments of coherence and grammar, made by humans, in Table 1, we expected the same positions of the systems. However, in Table 2 we see that the best value of ROUGE, using human-made summaries with compressed sentences as references, is for the Random system. Other than that, again, First system overcomes All system.\nAs an alternative, we compare the divergence of texts with respect to the original uncompressed text using the FRESA package4 [23, 16, 25]. The FRESA score FM asses the summaries qualities. Lower values of FM means significant difference whit respect to the original text (i.e. more radical compressions). The results of divergence tests of summaries is showed in Table 3. Results in Table 3 are interesting. Considering FM values, we see that First system is found more related to Human1 than Human2 by FRESA and Human2 is closer to Random system performance. These values are congruent with the coherence and grammar judgments showed in Table 1. The FM value of All system suggests that it is the most aggressive approach.\n4http://lia.univ-avignon.fr/fileadmin/axes/TALNE\nFor all tables we use the following notation about the sources: All system=all compression candidates, First system=candidates including the first segment, Random system=random compression (baseline), Human1,2=human compressions."
    }, {
      "heading" : "6 Conclusions and future work",
      "text" : "In this work we have introduced the concept of Sentence Compression driven by Discourse Segmentation and Language Models. We have found that using Probabilistic Language Models can be helpful for evaluation of compressions candidates. The results in Spanish presented in this paper are very encouraging. We believe that this approach is independent enough of the language to be transposed into other languages such as English or French. In future work we aim to improve the score (4) adding content restrictions.\nEvaluation of compressed sentences and summaries with compressions is still a challenge in languages other than English that do not have reference corpora. We think that more studies\nare necessary in order to evaluate if ROUGE or FRESA are good methods for compressed text evaluations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partially supported by Consejo Nacional de Ciencia y Tecnología (CONACYT) México, grant number 211963."
    } ],
    "references" : [ {
      "title" : "An empirical study of smoothing techniques for language modeling",
      "author" : [ "S.F. Chen", "J. Goodman" ],
      "venue" : "Computer Speech & Language,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1999
    }, {
      "title" : "Discourse segmentation for spanish based on shallow parsing",
      "author" : [ "Iria da Cunha", "Eric SanJuan", "Juan-Manuel Torres-Moreno", "Marina Lloberes", "Irene Castellón" ],
      "venue" : "Advances in Artificial Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Indexing by Latent Semantic Analysis",
      "author" : [ "S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman" ],
      "venue" : "Journal of the American Society for Information Science,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1990
    }, {
      "title" : "New Methods in Automatic Extraction",
      "author" : [ "H.P. Edmundson" ],
      "venue" : "Journal of the Association for Computing Machinery,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1969
    }, {
      "title" : "Statistics-based summarization – step one: Sentence compression",
      "author" : [ "Kevin Knight", "Daniel Marcu" ],
      "venue" : "In Proceedings of the 17th National Conference on Artificial Intelligence and 12th Conference on Innovative Applications of Artificial Intelligence,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2000
    }, {
      "title" : "Summarization beyond sentence extraction: a probabilistic approach to sentence compression",
      "author" : [ "Kevin Knight", "Daniel Marcu" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2002
    }, {
      "title" : "ROUGE: A Package for Automatic Evaluation of Summaries",
      "author" : [ "Chin-Yew Lin" ],
      "venue" : "Proceedings of the Workshop Text Summarization Branches Out (ACL’04),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "Automatic Evaluation of Summaries using N-gram Co-occurrence Statistics",
      "author" : [ "Chin-Yew Lin", "Eduard Hovy" ],
      "venue" : "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL’03),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2003
    }, {
      "title" : "The Automatic Creation of Literature Abstracts",
      "author" : [ "H.P. Luhn" ],
      "venue" : "IBM Journal of Research and Development,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1958
    }, {
      "title" : "Rhetorical Structure Theory: A Theory of Text Organization",
      "author" : [ "W.C. Mann", "S.A. Thompson" ],
      "venue" : "Information Sciences Institute, Marina del Rey,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1987
    }, {
      "title" : "Foundations of Statistical Natural Language Processing",
      "author" : [ "Christopher D. Manning", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1999
    }, {
      "title" : "Discourse Segmentation for Sentence Compression",
      "author" : [ "Alejandro Molina", "Torres-Moreno Juan-Manuel", "Eric SanJuan Eric", "Iria da Cunha", "Gerardo Sierra", "Patricia Velázquez-Morales" ],
      "venue" : "In Proceedings of the Mexican International Conference on Artificial Intelligence (MICAI’11),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Automatic evaluation of linguistic quality in multidocument summarization",
      "author" : [ "Emily Pitler", "Annie Louis", "Ani Nenkova" ],
      "venue" : "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Multilingual summarization evaluation without human models",
      "author" : [ "Horacio Saggion", "Juan-Manuel Torres-Moreno", "Iria da Cunha", "Eric SanJuan" ],
      "venue" : "In Proceedings of the 23rd International Conference on Computational Linguistics: Posters (COLING’10),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Sentence level discourse parsing using syntactic and lexical information",
      "author" : [ "Radu Soricut", "Daniel Marcu" ],
      "venue" : "In HLT-NAACL,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2003
    }, {
      "title" : "Discourse chunking and its application to sentence compression",
      "author" : [ "Caroline Sporleder", "Mireille Lapata" ],
      "venue" : "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2005
    }, {
      "title" : "Sentence Compression for the LSA-based Summarizer, pages 141—148",
      "author" : [ "Josef Steinberger", "Karel Jezek" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Knowledge-poor multilingual sentence compression",
      "author" : [ "Josef Steinberger", "Roman Tesar" ],
      "venue" : "In 7th Conference on Language Engineering",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2007
    }, {
      "title" : "Srilm – an extensible language modeling toolkit",
      "author" : [ "A. Stolcke" ],
      "venue" : "In Intl. Conf. on Spoken Language Processing,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2002
    }, {
      "title" : "A syntactic and lexical-based discourse segmenter",
      "author" : [ "Milan Tofiloski", "Julian Brooke", "Maite Taboada" ],
      "venue" : "In ACL-IJCNLP,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2009
    }, {
      "title" : "Fresa: a framework for evaluating summaries",
      "author" : [ "Juan-Manuel Torres-Moreno" ],
      "venue" : "automatically. rapport technique, Université d’Avignon et des Pays de Vaucluse, Avignon, France,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Résumé automatique de documents: une approche statistique",
      "author" : [ "Juan-Manuel Torres-Moreno" ],
      "venue" : "Hermès-Lavoisier, Paris,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Summary Evaluation With and Without References",
      "author" : [ "Juan-Manuel Torres-Moreno", "Horacio Saggion", "Iria da Cunha", "Eric SanJuan" ],
      "venue" : "Polibits: Research journal on Computer science and computer engineering with applications,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Sentences extraction allows to generate summaries by extraction sentences [10, 5, 24].",
      "startOffset" : 74,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "Sentences extraction allows to generate summaries by extraction sentences [10, 5, 24].",
      "startOffset" : 74,
      "endOffset" : 85
    }, {
      "referenceID" : 21,
      "context" : "Sentences extraction allows to generate summaries by extraction sentences [10, 5, 24].",
      "startOffset" : 74,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "Sentence compression can be used to improve extract summarization [6, 13].",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 15,
      "context" : "Previous works suggest that sentence segmentation could be helpful in sentence compression generation [18].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : "The words that remain (order unchanged) form a compression” [7].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "There are interesting algorithms to determine the removal of words in a sentence but humans tend also to delete long phrases in an abstract [15].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : "In [19] an algorithm first divides sentences into clauses prior to any elimination and then, compression candidates are scored based on Latent Semantic Analysis proposed in [4].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 2,
      "context" : "In [19] an algorithm first divides sentences into clauses prior to any elimination and then, compression candidates are scored based on Latent Semantic Analysis proposed in [4].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 16,
      "context" : "However, no component to mitigate grammaticality issues is included in this algorithm [19].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "The authors attempted to solve this issue by including features in a machine learning approach [20].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "Discourse chunking [18] is an alternative to discourse parsing, thereby, showing a direct application to sentence compression.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "In addition, some sentence-level discourse models have shown accuracies comparable to human performance [17].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 19,
      "context" : "Formally, “Discourse segmentation is the process of decomposing discourse into Elementary Discourse Units (EDUs), which may be simple sentences or clauses in a complex sentence, and from which discourse trees are constructed” [22].",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 11,
      "context" : "In [14], the authors use a discourse segmenter in order to segment sentences in spanish.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "The discourse segmenter is described in [3] and is based in the Rhetorical Structure framework [11].",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : "The discourse segmenter is described in [3] and is based in the Rhetorical Structure framework [11].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "Statistical language modeling [2, 12] is a technique widely used to assign a probability to a sequence of words.",
      "startOffset" : 30,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "Statistical language modeling [2, 12] is a technique widely used to assign a probability to a sequence of words.",
      "startOffset" : 30,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "In our experiments we use a big corpus with 1T words to get the sequences counts and a LM interpolation based on Jelinek-Mercer smoothing [2]:",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 18,
      "context" : "For our experiments we use the Language Modeling Toolkit SRILM [21].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "For the experiments, two annotators were required to compress each sentence following the instructions in [14].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "We wanted to evaluate the content quality of summaries with the ROUGE package [9].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : "ROUGE is used to evaluate summaries because some results show that it correlates well with human judgements [8].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 20,
      "context" : "As an alternative, we compare the divergence of texts with respect to the original uncompressed text using the FRESA package [23, 16, 25].",
      "startOffset" : 125,
      "endOffset" : 137
    }, {
      "referenceID" : 13,
      "context" : "As an alternative, we compare the divergence of texts with respect to the original uncompressed text using the FRESA package [23, 16, 25].",
      "startOffset" : 125,
      "endOffset" : 137
    }, {
      "referenceID" : 22,
      "context" : "As an alternative, we compare the divergence of texts with respect to the original uncompressed text using the FRESA package [23, 16, 25].",
      "startOffset" : 125,
      "endOffset" : 137
    } ],
    "year" : 2012,
    "abstractText" : "Previous works demonstrated that Automatic Text Summarization (ATS) by sentences extraction may be improved using sentence compression. In this work we present a sentence compressions approach guided by level-sentence discourse segmentation and probabilistic language models (LM). The results presented here show that the proposed solution is able to generate coherent summaries with grammatical compressed sentences. The approach is simple enough to be transposed into other languages.",
    "creator" : "easychair.cls-3.0"
  }
}