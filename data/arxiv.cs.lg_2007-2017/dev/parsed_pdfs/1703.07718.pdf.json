{
  "name" : "1703.07718.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Independently Controllable Features",
    "authors" : [ "Emmanuel Bengio", "Valentin Thomas" ],
    "emails" : [ "ebengi@cs.mcgill.ca", "valentin.thomas@epfl.ch", "jpineau@cs.mcgill.ca", "dprecup@cs.mcgill.ca", "yoshua.bengio@umontreal.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: representation learning, controllable features"
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors gratefully acknowledge financial support for this work by the Samsung Advanced Institute of Technology (SAIT), the Canadian Institute For Advanced Research (CIFAR), as well as the Natural Sciences and Engineering Research Council of Canada (NSERC).\nar X\niv :1\n70 3.\n07 71\n8v 1\n[ cs\n.L G\n] 2\n2 M\nar 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "Whether in static or dynamic environments, decision making for real world problems is often confronted with the hard challenge of finding a “good” representation of the problem. In the context of supervised or semi-supervised learning, it has been argued (Bengio, 2009) that good representations separate out underlying explanatory factors, which may be causes of the observed data. In such problems, feature learning often involves mechanisms such as autoencoders (Hinton & Salakhutdinov, 2006), which find latent features that explain the observed data. In interactive environments, the temporal dependency between successive observations creates a new opportunity to notice structure in data which may not be apparent using only observational studies. The need to experiment in order to discover causal structures has already been well explored in psychology (e.g. Gopnik & Wellman (in press)). In reinforcement learning, several approaches explore mechanisms that push the internal representations of learned models to be “good” in the sense that they provide better control (see Sec. 4).\nWe propose and explore a more direct mechanism, which explicitly links an agent’s control over its environment with its internal feature representations. Specifically, we hypothesize that some of the factors explaining variations in the data correspond to aspects of the world which can be controlled by the agent. For example, an object could be pushed around or picked up independently of others. In such a case, our approach aims to extract object features from the raw data while learning a policy that controls precisely these features of the data. In Sec. 2 we explain this mechanism and show experimental results in its simplest instantiation. In Sec. 3 we discuss how this principle could be applied more generally, and what are the research challenges that emerge."
    }, {
      "heading" : "2 Independently controllable features",
      "text" : "To make the above intuitions concrete, assume that there are factors of variation underlying the observations coming from an interactive environment that are “independently controllable”. That is, for each of these factors of variation, there exists a policy which will modify that factor only, and not the others. For example, the object behind a set of pixels could be acted on independently from other objects, which would explain variations in its pose and scale when we move it around. The object in this case is a “factor of variation”. What makes discovering and mapping such factors into features tricky is that the factors are not explicitly observed. Our goal is to learn these factors, which we call independently controllable features, along with policies that control them. While these may seem strong assumptions about the nature of the environment, our point of view is that they are similar to regularizers meant to make a difficult learning problem better constrained.\nThere are many possible ways to express the desire to learn independently controllable features as an objective. Section 2.2 proposes such an objective for a simple scenario. Section 2.3 illustrates the effect of this objective when all the features of the environment are simple and controllable by the agent. Section 2.4 explores a slightly harder scenario in which there is redundancy and policies are learned through a reinforcement learning algorithm."
    }, {
      "heading" : "2.1 Autoencoders",
      "text" : "Our approach builds on the familiar framework of autoencoders (Hinton & Salakhutdinov, 2006), which are defined as a pair of function approximators f, g with parameters θ such that f : X → H maps the input space to some latent space H , and g : H → X maps back to the input space X ⊂ Rd. Autoencoders are trained to minimize the discrepancy between x and g(f(x)), a.k.a. the reconstruction error, e.g.,:\nmin θ\n1 2‖x− g(f(x))‖ 2 2\nWe call f(x) = h ∈ H ⊂ Rn the latent feature representation of x, with n features. It is common to assume that n d. This causes f and g to perform dimensionality reduction of X , i.e. compression, since there is a dimension bottleneck through which information about the input data must pass. Often, this bottleneck forces the optimization procedure to uncover principal factors of variation of the data on which they are trained. However, this does not necessarily imply that the different dimensions of h = f(x) are individually meaningful. In fact, note that for any bijective function r, we could obtain the same reconstruction error by replacing f by r ◦ f and g by r−1 ◦ g, so we should not expect any form of disentangling of the factors of variation unless some additional constraints or penalties are imposed on h. This motivates the approach we are about to present. Specifically, we will look for policies which can separately influence one of the dimensions of h, and we will prefer representations which make such policies possible."
    }, {
      "heading" : "2.2 Policy Selectivity",
      "text" : "Consider the following simple scenario: we train an autoencoder f, g producing n latent features, fk, k = 1, . . . n. In tandem with these features we train n policies, denoted πk. Autoencoders can learn relatively arbitrary feature representations, but we would like these features to correspond to controllable factors in the learner’s environment. Specifically, we would like policy πk to cause a change only in fk and not in any other features. We think of fk and πk as a feature-policy pair.\nIn order to quantify the change in fk when actions are taken according to πk, we define the selectivity of a feature as:\nsel(s, a, k) = Es′∼Pa ss′ [ |fk(s′)− fk(s)|∑ k′ |fk′(s′)− fk′(s)| ] (1)\nwhere s,s′ are successive raw state representations (e.g. pixels), a is the action, Pass′ is the environment’s transition distribution from s to s′ under action a. The normalization by the change in all features means that the selectivity of fk is maximal when only that single feature changes as a result of some action.\nBy having an objective that maximizes selectivity and minimizes the autoencoder objective, we can ensure that the features learned can both reconstruct the data and recover independently controllable factors. Hence, we define the following objective, which can be minimized via stochastic gradient descent:\nEs[ 12 ||s− g(f(s))|| 2 2]︸ ︷︷ ︸\nreconstruction error\n− λ ∑ k Es[ ∑ a\nπk(a|s) log sel(s, a, k)]︸ ︷︷ ︸ disentanglement objective\n(2)\nHere one can think of log sel(s, a, k) as the reward signal Rk(s, a) of a control problem, and the expected reward Ea∼πk [Rk] is maximized by finding the optimal set of policies πk.\nNote that it is also possible to have directed selectivity: by not taking the absolute value of the numerator of (1) (and replacing log sel with log(1+sel) in (2)), the policies must learn to increase the learned latent feature rather than simply change it. This may be useful if the policy to gradually increase a feature is distinct from the policy that decreases it."
    }, {
      "heading" : "2.3 A first toy problem",
      "text" : "Consider the simple environment described in Figure 1(a): the agent sees a 2 × 2 square of adjacent cells in the environment, and has 4 actions that move it up, down, left or right. An autoencoder with directed selectivity (see Figure 1(c,d)) learns latent features that map to the (x, y) position of the square in the input space, without ever having explicitly access to these values, and while reconstructing the input properly. An autoencoder without selectivity also reconstructs the input properly but without learning these two latent (x, y) features explicitly.\nIn this setting f , g and π share some of their parameters. We use the following architecture: f has two 16×3×3 ReLU convolutional layers, followed by a fully connected ReLU layer of 32 units, and a tanh layer of n = 4 features; g is the transpose architecture of f ; πk is a softmax policy over 4 actions, computed from the output of the ReLU fully connected layer."
    }, {
      "heading" : "2.4 A slightly harder toy problem",
      "text" : "In the next experiments, we aim to generalize the model above in a slightly more complex environment. Instead of having f and πk parametrized by the same parameters, we now introduce a different set of parameters for each policy and for the encoder, so that each policy can be learned separately.\nAdditionally, we use a richer action space, in which we added a “move down and to the right” action as well as an “increase/decrease color of square” action. Note also that the first two actions (”move down”) are redundant.\nThus, in this setting, f, g and πk are all parametrized by different variables, as follows: • f(s) = f(s;Wf ) a neural net with a tanh output activation • g(h) = g(h;Wg) a neural net with a ReLU output activation • πk(a|s) = π(a|s; θk) so that πk(·|s) = softmax(θk · s)\nAlgorithm 1 Training an autoencoder with disentangled factors 1: for t = 1..T do 2: Sample s 3: Wf ←Wf − ηf∇Wf [ 12 ||s− g(f(s))|| 2 2]\n4: Wg ←Wg − ηg∇Wg [ 12 ||s− g(f(s))|| 2 2] 5: for k = 1..K do 6: Wf ←Wf + ηf λ ∇WfEa∼πk(·|s)[log sel(s, a, k)] 7: θk ← θk + ηk λ ∇θkEa∼πk(·|s)[log sel(s, a, k)]\nThe gradients on lines 3 and 4 are computed exactly via backpropagation. In our experiments, gradients on lines 6 and 7 are also computed by backpropagation, but in a more general case in which the environment also provides a reward rt (so the total reward to maximize becomes rt+ log selk), they can be estimated via Monte-Carlo. Note that, in any case,∇θkEa∼πk(·|s)[log sel(s, a, k)] can be computed with the REINFORCE (Williams, 1992) estimator:\n∇θkEa∼πk(·|s)[log sel(s, a, k)] = Ea∼πk(·|s)[log sel(s, a, k) · ∇θk log πk(a|s)]"
    }, {
      "heading" : "3 Scaling to general environments: controllability and the binding problem",
      "text" : "In the previous section we used problems in which the environment is made of a static set of objects. In this case, if the objective posited in section 2.2 is learned correctly, we can assume that feature k of the representation can unambiguously refer to some controllable property of some specific object in the environment. For example, the agent’s world might contain only a red circle and a green rectangle, which are only affected bu the actions of the agent (they do not move on their own) and we only change the positions and colours of these objects from one trial to the next. Hence, a specific feature fk can learn to unambiguously refer to the position or the colour of one of these two objects.\nIn reality, environments are stochastic, and the set of objects in a given scene is drawn from some distribution. The number of objects may vary and their types may be different. It then becomes less obvious how feature k could refer in a clear way to some feature of one of the objects in a particular scene. If we have instances of objects of different types, some addressing or naming scheme is required to refer to the particular objects (instances) present in the scene, so as to match the policy with a particular attribute of a\nparticular object to selectively modify. In our simple environments, this task was trivial because we could simply use the integer k to achieve this coordination between the policy πk and the representation elements (feature fk(s)). In the more general case, this is not possible, which gives rise to a representational problem.\nThis is connected to the binding problem in neuro-cognitive science: how to represent a set of objects, each having different attributes, so that we don’t confuse, for example, the set {red circle, blue square} with {red square, blue circle}. It is not enough to have a red-detector feature and a blue-detector feature, a square-detector feature and a circle-detector feature. The binding problem has seen some attention in the representation learning literature (Minin et al., 2012; Greff et al., 2016), but still remains mostly unsolved. Jointly considering this problem and larning controllable features may prove fruitful.\nThese ideas may also lead to interesting ways of performing exploration. How do humans choose with which object to play? We are attracted to objects which we do not know yet (i.e., if and how we can control them). The RL exploration process could be driven by a notion of controllability, predicting the interestingness of objects in a scene and choosing features and associated policies with which to attempt control them. Such ideas have only been explored briefly in the literature, e.g. Ratitch & Precup (2003)"
    }, {
      "heading" : "4 Discussion",
      "text" : "There is a large body of work on learning features in RL focusing on indirectly learning good internal representations. In Jaderberg et al. (2016), agents learn off-policy to control their pixel inputs, forcing them to learn features that, intuitively, help control the environment (at least at the pixel level). Oh et al. (2015) propose models that learn to predict the future, conditioned on action sequences, which push the agent to capture temporal features. There are many more works that go in this direction, such as (deep) successor feature representations (Dayan, 1993; Kulkarni et al., 2016) or the options framework (Sutton et al., 1999; Precup, 2000) when used in conjunction with neural networks (Bacon et al., 2016).\nOur approach is similar in spirit to the Horde architecture (Sutton, 2011). In that scenarion, agents learn policies that maximize specific inputs. The predictions for all these policies then become features for the agent. Our objective is defined specifically in the context of autoencoders. Unlike recent work on the predictron (David Silver, 2017), our approach is not focused on solving a planning task, and the goal is simply to learn how the world works."
    } ],
    "references" : [ {
      "title" : "The option-critic architecture",
      "author" : [ "Bacon", "Pierre-Luc", "Harb", "Jean", "Precup", "Doina" ],
      "venue" : "arXiv preprint arXiv:1609.05140,",
      "citeRegEx" : "Bacon et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bacon et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning deep architectures for AI",
      "author" : [ "Bengio", "Yoshua" ],
      "venue" : "Now Publishers,",
      "citeRegEx" : "Bengio and Yoshua.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio and Yoshua.",
      "year" : 2009
    }, {
      "title" : "Improving generalization for temporal difference learning: The successor representation",
      "author" : [ "Dayan", "Peter" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Dayan and Peter.,? \\Q1993\\E",
      "shortCiteRegEx" : "Dayan and Peter.",
      "year" : 1993
    }, {
      "title" : "Tagger: Deep unsupervised perceptual grouping",
      "author" : [ "Greff", "Klaus", "Rasmus", "Antti", "Berglund", "Mathias", "Hao", "Tele", "Valpola", "Harri", "Schmidhuber", "Juergen" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Greff et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Greff et al\\.",
      "year" : 2016
    }, {
      "title" : "Reducing the dimensionality of data with neural networks. science",
      "author" : [ "Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Reinforcement learning with unsupervised auxiliary tasks",
      "author" : [ "Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray" ],
      "venue" : "arXiv preprint arXiv:1611.05397,",
      "citeRegEx" : "Jaderberg et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jaderberg et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep successor reinforcement learning",
      "author" : [ "Kulkarni", "Tejas D", "Saeedi", "Ardavan", "Gautam", "Simanta", "Gershman", "Samuel J" ],
      "venue" : "arXiv preprint arXiv:1606.02396,",
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2016
    }, {
      "title" : "Complex valued artificial recurrent neural network as a novel approach to model the perceptual binding problem",
      "author" : [ "Minin", "Alexey", "Knoll", "Alois", "Zimmermann", "Hans-Georg", "AG Siemens", "Siemens", "LLC" ],
      "venue" : "In ESANN. Citeseer,",
      "citeRegEx" : "Minin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Minin et al\\.",
      "year" : 2012
    }, {
      "title" : "Action-conditional video prediction using deep networks in atari games",
      "author" : [ "Oh", "Junhyuk", "Guo", "Xiaoxiao", "Lee", "Honglak", "Lewis", "Richard L", "Singh", "Satinder" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Oh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2015
    }, {
      "title" : "Temporal abstraction in reinforcement learning",
      "author" : [ "Precup", "Doina" ],
      "venue" : null,
      "citeRegEx" : "Precup and Doina.,? \\Q2000\\E",
      "shortCiteRegEx" : "Precup and Doina.",
      "year" : 2000
    }, {
      "title" : "Using mdp characteristics to guide exploration in reinforcement learning",
      "author" : [ "B. Ratitch", "D. Precup" ],
      "venue" : "In ECML, pp",
      "citeRegEx" : "Ratitch and Precup,? \\Q2003\\E",
      "shortCiteRegEx" : "Ratitch and Precup",
      "year" : 2003
    }, {
      "title" : "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
      "author" : [ "R.S. Sutton" ],
      "venue" : "In AAMAS,",
      "citeRegEx" : "Sutton and Precup.D.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sutton and Precup.D.",
      "year" : 2011
    }, {
      "title" : "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Williams", "Ronald J" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Williams and J.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams and J.",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "The binding problem has seen some attention in the representation learning literature (Minin et al., 2012; Greff et al., 2016), but still remains mostly unsolved.",
      "startOffset" : 86,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "The binding problem has seen some attention in the representation learning literature (Minin et al., 2012; Greff et al., 2016), but still remains mostly unsolved.",
      "startOffset" : 86,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : ", 2012; Greff et al., 2016), but still remains mostly unsolved. Jointly considering this problem and larning controllable features may prove fruitful. These ideas may also lead to interesting ways of performing exploration. How do humans choose with which object to play? We are attracted to objects which we do not know yet (i.e., if and how we can control them). The RL exploration process could be driven by a notion of controllability, predicting the interestingness of objects in a scene and choosing features and associated policies with which to attempt control them. Such ideas have only been explored briefly in the literature, e.g. Ratitch & Precup (2003)",
      "startOffset" : 8,
      "endOffset" : 666
    } ],
    "year" : 2017,
    "abstractText" : "Finding features that disentangle the different causes of variation in real data is a difficult task, that has nonetheless received considerable attention in static domains like natural images. Interactive environments, in which an agent can deliberately take actions, offer an opportunity to tackle this task better, because the agent can experiment with different actions and observe their effects. We introduce the idea that in interactive environments, latent factors that control the variation in observed data can be identified by figuring out what the agent can control. We propose a naive method to find factors that explain or measure the effect of the actions of a learner, and test it in illustrative experiments.",
    "creator" : "LaTeX with hyperref package"
  }
}