{
  "name" : "1503.03974.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Roberto Posenato" ],
    "emails" : [ "carlo.comin@unitn.it", "romeo.rizzi}@univr.it" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 3.\n03 97\n4v 3\n[ cs\n.D S]\n2 2\nM ar\n2 01\nKeywords Simple Temporal Networks · Temporal Consistency · Hypergraphs · Pseudo-polynomial Time Algorithms · Mean Payoff Games · Disjunctive Temporal Problems · Workflows"
    }, {
      "heading" : "1 Introduction",
      "text" : "In many areas of Artificial Intelligence (AI), including planning, scheduling and workflow management systems, the representation and management of quantita-\nCarlo Comin Department of Mathematics, University of Trento, Trento, Italy and LIGM, Université ParisEst in Marne-la-Vallée, Paris, France. Supported by Department of Computer Science, University of Verona under PhD grant “Computational Mathematics and Biology” on a co-tutelle agreement with Université Paris-Est in Marne-la-Vallée. E-mail: carlo.comin@unitn.it\nRoberto Posenato, Romeo Rizzi Department of Computer Science, University of Verona, Verona, Italy E-mail: {roberto.posenato, romeo.rizzi}@univr.it\ntive temporal aspects is of crucial importance [4, 10, 11, 20, 37, 43]. Examples of possible quantitative temporal aspects are: constraints on the earliest start time and latest end time of activities, constraints over the minimum and maximum temporal distance between activities, etc.\nIn many cases these constraints can be represented as an instance of a Simple Temporal Network (STN) [18], a directed weighted graph where each node represents a time-point variable (timepoint), usually corresponding to the beginning or the end of an activity, and each arc specifies a binary constraint on the scheduling times to be assigned to its endpoints. In [18], each arc is labeled with a closed interval of real values: for example, the labeled arc u [x,y] −→ v encodes the binary constraint x ≤ v − u ≤ y over its endpoints u and v. A more uniform and elementary representation of an STN is provided by its distance graph1 [18], a graph having the same set of nodes as the original one, but where each arc u [x,y] −→ v is replaced by two arcs, each labeled with a single real value: arc u y −→ v to express the constraint v − u ≤ y, and arc v −x −→ u to express the constraint u − v ≤ −x, i.e., x ≤ v − u. An STN is said to be consistent if it is possible to assign a real value to each timepoint so that all temporal constraints are satisfied. The consistency property can be verified by searching for negative cycles in the distance graph and it is well known that the consistency check and the determination of the earliest/latest value for each timepoint can be done in polynomial time [18].\nHowever, STNs do not allow the expression of constraints like “trigger off an event exactly δ min after the occurrence of the last of its predecessors”, which are a quite natural constraints to represent synchronization events in a process aware information system plan/workflow schema [25]. This is because in STNs, and in some of their natural extensions, (1) it is not possible to represent a single constraint involving more than two timepoints and (2) all constraints have to be satisfied in order to have the network consistent. On the contrary, the above constraint about a synchronization event can be represented as a set of distance constraints, each involving a different pair of timepoints, that is considered satisfied when at least one of set components is satisfied. In order to represent and analyze disjunctive constraints like the above one, it is then necessary to consider models like Disjunctive Temporal Problem (DTP) [44] where a constraint is a set of disjunctive difference constraints over the timepoints. The drawback of such model is that the consistency check problem is NP-complete [44].\n1.1 Contribution\nIn this article we propose to generalize STN to Hyper Temporal Network (HyTN), which allows also the expression of constraints like the above one regarding synchronization events, but where the consistency check is amenable of effective solution algorithms.\nMoreover, we show an interesting link between the consistency check of HyTNs and resolution in Mean Payoff Games (MPG), a family of perfect information\n1 Distance graph is also called constraint graph by other authors [17]. Moreover, Bellman [3] was the first to describe the relation between shortest paths and difference constraints in a constraint graph.\ngames played on graphs by two opponents [21], for which some pseudo-polynomial time algorithms for determining winning strategies are known [7, 47].\nA preliminary version of this article appeared in the proceedings of TIME symposium [15]. Here we extend the presentation as follows: (1) the definition of HyTN has been extended in order to allow the presence of two kinds of hyperarcs; (2) the motivating example section has been revised to show how the new kind of hyperarc can be used; (3) some further issues and pertinent properties about HyTN have been introduced and proved; (4) several proofs have been expanded and clarified; (5) the experimental analysis of the consistency check algorithm has been improved considering more recent algorithms [7] for finding winning strategies for MPGs. This has improved the performances dramatically.\nOrganization The remainder of the article is organized as follows. In Section 2 we present a motivating example from the domain of the workflow-based process management to bring out HyTNs. Section 3 introduces some definitions and wellknown results for STNs and introduces some definitions about hypergraphs. The generalization of STNs into HyTNs and the definition of consistency problem for HyTNs are presented in Section 4. In Section 5 we recall the main facts and results about Mean Payoff Games which are useful for the following sections. Section 6 presents the investigation into the link between the HyTN consistency problem and Mean Payoff Games deriving pseudo-polynomial time algorithms for checking the consistency of HyTNs and computing feasible schedules whenever they exist. Some empirical evaluations of the proposed algorithms are reported in Section 7. In Section 8, some related works are presented and discussed with respect to our approach. Section 9 summarizes the main facts brought to light in this article and presents a possible future development of the work we are currently carrying on."
    }, {
      "heading" : "2 Motivating Examples",
      "text" : "In the introduction we have briefly recalled a kind of constraint that cannot be expressed within STNs. In this section, we describe in more detail two examples of temporal constraints that cannot be fully described in an STN in order to introduce and motivate the new expressive capability of our model. As a further motivation, at the end of the section we also spotlight how this new capability has been recently exploited to check the consistency of Conditional Simple Temporal Networks (CSTNs) [45] in a more efficient way.\nLet us consider an example in the domain of the workflow-based process management, a domain concerned with the coordination and control of business processes using information technology. A workflow is a representation of a business process as the coordinated execution of activities by human or automatic executors (agents). A Workflow management system (WfMS) is a software system that supports the automatic execution of workflows [25]. In a WfMS, the management of temporal aspects is a critical component and in the literature there are many proposals on how to extend a workflow in order to represent and manage temporal constraints of a business process [4, 8, 10–13, 19, 20, 23]. In particular, in [4,8,13,19,20] authors show how to represent and manage some kinds of temporal constraints using specific algorithms, while in [10–12,23] authors show how it is\npossible to represent and manage a wider class of temporal constraints exploiting models like Time Petri Nets [35] or STNs/STNUs [36].\nIn this paper we consider an excerpt of the conceptual temporal model proposed by Combi et al. [12], where the specification of a temporal workflow is given by a workflow schema, a directed graph (also called workflow graph) where nodes correspond to activities and arcs represent control flows that define activity dependencies on the order of execution. Both nodes and arcs may be associated to temporal ranges to specify temporal constraints. There are two different types of activity: tasks and connectors. Tasks represent elementary work units that will be executed by external agents. Each task is graphically represented by a box containing a name and a temporal range that specifies the allowed temporal span for its execution. Connectors represent internal activities executed by the WfMS to achieve a correct and coordinated execution of tasks. They are graphically represented by diamonds and, as with tasks, each of them has a temporal range that gives the temporal span allowed to the WfMS for executing it. Every arc has a temporal property that gives the allowed times that can be spent by the WfMS for possibly delaying the consideration of the next activity after the end of the previous one. There are different kinds of connector that allow one to modify a control flow. Split connectors are nodes with one incoming arc and two or more outgoing arcs: after the execution of the predecessor, (possibly) several successors have to be considered for the execution. The set of nodes that can start their execution is given by the kind of split connector. A split connector can be: Parallel, Alternative or Conditional. Join connectors are nodes with two or more incoming arcs and one outgoing arc only. The types of activities considered in [12] are a subset of the possible activities specified by the Workflow Management Coalition [1, 25].\nFig. 1 shows a simple workflow schema where the Parallel connector 1 splits the flow into three parallel flows of execution (one for the sequence of tasks T1 and T2, one for task T3, and one for task T4 and T5) that have to be joined (synchronized) by the AND join connector 2 before continuing the execution; all temporal ranges are in minutes.\nLet us consider the connector 2; according to the recommendations from the Workflow Management Coalition (WfMC) [25] and the temporal specification from [12], the execution of this connector requires to wait all incoming flows and, after the last incoming flow, to wait a time according to the connector temporal range before following the outgoing arc. In other words, the incoming flows can arrive at different instants but only when the last one arrives, the connector has to be activated in order to continue with the execution.\nCombi et al. [12] proposed a method to translate workflow schemata to STNs/ STNUs [36] in order to analyze and validate all temporal aspects in a rigorous way. As already noted in [9] and [10], such translation cannot specifically represent the behavior of an AND join connector, because the kind of constraints in an STN/ STNU is limited. Therefore, in [10], the authors proposed an adjustment of the translation of an AND join connector introducing for each incoming arc of the connector a buffer node connected with some determined new arcs and assuming a reasonable but fixed execution algorithm for the STN. In more detail, let us consider Fig. 2 that depicts the representation of workflow of Fig. 1 by means of an STN following partially the method described in [10] (without loss of generality, here we convert task constraints as STN arcs instead of STNU contingent ones because we are interested only in the AND join conversion). Each activity of the workflow is represented by two STN nodes, one to represent the begin timepoint, Bi, one for the end one, Ei, and temporal ranges in the workflow are represented by STN arc labels. Regarding the translation of the AND join node 2, nodes representing the task endings on parallel flows,ET2 , ET3 , and ET5 , are connected to buffer nodes b1, b2, and b3 that allow the parallel flows to complete their execution following only their temporal constraints. Then, b1, b2, and b3 are connected to node B\n2 (which represents the begin instant of the AND join connector) by temporal constraints [0, t1], [0, t2] and [0, t3], where the values t1, t2, and t3 are determined during the workflow-to-STN conversion as explained in [10].\nNow, let us consider a possible execution scenario. If b1, b2, and b3 occur all together at instant 20, then, following the proposed temporal semantics [12], the only possible instant value for B\n2 must be 20 while the updated STN allows any value in the range [20, 20 + min{t1, t2, t3}]. In [10], the authors showed that the right value is always the lower bound of such extended range and, therefore, it is sufficient to adopt an early execution strategy in order to choose the right value for timepoint B\n2\n.\nIn other words, the proposed translation has two drawbacks: (1) it requires some preliminary computations for determining t1, t2, and t3 values, and (2) the resulting STN admits some solutions that are not admissible by the semantics of the AND join connector.\nTo specifically represent the behavior of an AND join connector with respect to its predecessor time points without auxiliary conditions or analysis, it is necessary\nto introduce a new kind of constraint based on hyperarcs, as shown in Fig. 3. In the figure the multi-tail hyperarc A consists of three dashed arcs—called components— and replaces the arcs from bi (i = 1, 2, 3) to B\n2 of Fig. 2. We say that a multi-tail hyperarc is satisfied if at least one of its components is satisfied. In Fig. 3 dashed arcs define the hyperarc A that is satisfied if B\n2 is 0 distant from at least one time point among b1, b2, and b3. Since B\n2 is constrained to occur at the same instant or after each time point b1, b2, and b3 by the arcs between B\n2 and bi, i = 1, 2, 3, the result is that to satisfy A it is necessary that B\n2 occurs at the same instant of the last time point among b1, b2, and b3, as required originally. In more general, a multi-tail hyperarc is defined as a set of distance constraints (components) between some time points and a common end point.\nThe use of hyperarcs allows also the representation of temporal aspect of other advanced connectors as, for example, the Structured Discriminator [1]. The Structured Discriminator connector provides a means of merging two or more distinct flows in a workflow instance into a single subsequent. In particular, it triggers the subsequent flow as soon as the the first incoming flow arrives. The arrival of other incoming flows thereafter have no effect on the subsequent flow. As such, the Structured Discriminator provides a mechanism for progressing the execution of a process once the first of a series of concurrent tasks has completed and according to the connector temporal range. Fig. 4a depicts an excerpt of a workflow schema containing a structured discriminator connector, D, that joins three parallel flows.\nAt the best of our knowledge, currently there are no proposals for the representation of temporal constraints of a discriminator connector in any temporal workflow model or process-aware information system [32]. Even exploiting the methodology proposed in [10], it is easy to verify that it is not possible to represent such connector as an STN because in a consistent STN all constraints have to be satisfied while here it is necessary to allow the possibility that only one constraint of a set has to be satisfied in order to specifically represent a discriminator connector. A possible way for specifically managing a discriminator connector consists in following the approach suggested by [10] for representing activities and considering a variant of the multi-tail hyperarc, called multi-head hyperarc, for representing its temporal constraint, as depicted in Fig. 4b. In the figure there is a multi-head\nhyperarc A that connects the node representing the beginning instant of the discriminator activity to all nodes representing the end instant of the activities that precede the considered discriminator and are directly connected to it. In general a multi-head hyperarc is defined as a set of distance constraints (components) between one time point and some end points. We say that a multi-head hyperarc is satisfied if at least one of its components is satisfied. In Fig. 4b, dashed arcs define the hyperarc A that is satisfied if B\nD is at least 2 distant from ET1 or 1 distant\nfrom ET2 or 5 distant from ET3 . It is sufficient that one of such previous nodes is executed and that the delay represented in the corresponding connecting arc is passed to execute B\nD\n, as required by the structured discriminator connector\nsemantics.\nHyTNs are not only suitable for better representing temporal constraints originating from temporal workflow, but also for better representing more general temporal constraint networks like Conditional Simple Temporal Network [45].\nA Conditional Simple Temporal Network (CSTN) is an enriched graph for representing and reasoning about temporal constraints in domains where some constraints may apply only in certain condition settings (scenarios). Each condition in a CSTN is represented by a propositional letter whose truth value is observed in real time as the outcome of the execution of an observation timepoint. An execution strategy for a CSTN has to determine an execution time for each time-point guaranteeing that all temporal constrains that are significant in the resulting scenario are satisfied. An execution strategy can be dynamic in that its execution decisions can react to the information obtained from such observations. The Conditional Simple Temporal Problem (CSTP) consists in determining whether a given CSTN admits a dynamic execution strategy for any possible combination of propositional outcomes happens to be observed over time. If such a strategy exists, the CSTN is said to be dynamically consistent (DC).\nTsamardinos et al. [45] solved the CSTP by first encoding it as a meta-level Disjunctive Temporal Problem (DTP), then feeding it to an off-the-shelf DTP solver. Although of theoretical interest, this approach is not practical because the\nCSTP-to-DTP encoding has exponential size, and the DTP solver itself runs in exponential time. To our knowledge, this approach has never been empirically evaluated [26].\nIn [16], Comin and Rizzi proposed a novel representation of CSTNs in terms of HyTNs allowing the determination of the first singly exponential-time algorithm for checking the dynamic consistency of Conditional Simple Temporal Networks. More precisely, a CSTN instance is represented as a suitable HyTN where each possible scenario is represented and connected to other scenarios in an appropriate way and, then, such HyTN instance is solved in pseudo-polynomial time by the algorithms analyzed in the present paper.\nIn summary, HyTNs allow the representation of temporal constraints that are more general of those represented in STNs [18], because they allow disjunctions involving more than two time points, but less general than those represented in DTPs [44] because all disjunctions related to a multi-head(tail) hyperarcs have to contain a common variable. Such kind of STN generalization not only allows the compact representation of some common temporal constraints in the domains like the workflow-based process management but also allows the determination of new interesting algorithm for checking dynamic-consistency in richer models like CSTN."
    }, {
      "heading" : "3 Background and Notation",
      "text" : "In this section, we introduce some definitions, notations and well-know results about graphs and conservative graphs; moreover, we recall the relation between the consistency property of STNs and the conservative property of weighted graphs.\nWe consider graphs that are directed and weighted on the arcs. Thus, if G = (V,A) is a graph, then every arc a ∈ A is a triple (ta, ha, wa) where ta ∈ V is the tail of a, ha ∈ V is the head of a, and wa ∈ R is the weight of a. Moreover, since we use graphs to represent distance constraints, they do not need to have either loops (unary constraints are meaningless) or parallel arcs (two parallel constraints represent two different distance constraints between the same pair of node: only the most restrictive is meaningful). We also use the notations h(a) for ha, t(a) for ta, and w(a) or w(ta, ha) for wa, when it helps. The order and size of a graph G = (V,A) are denoted by n , |V | and m , |A|, respectively. The size is a good measure for the encoding length of G.\nA cycle of G is a set of arcs C ⊆ A cyclically sequenced as a0, a1, . . . , aℓ−1 so that h(ai) = t(aj) if and only if j = (i+ 1) mod ℓ; it is called a negative cycle if w(C) ≤ 0, where w(C) stands for ∑\ne∈C we. A graph is called conservative when it contains no negative cycle.\nA potential is a function p : V 7→ R. The reduced weight of an arc a = (u, v, wa) with respect to a potential p is defined as wpa , wa − pv + pu. A potential p of G = (V,A) is called feasible if wpa ≥ 0 for every a ∈ A. Notice that, for any cycle C, wp(C) = w(C). Therefore, the existence of a feasible potential implies that the graph is conservative as w(C) = wp(C) ≥ 0 for every cycle C.\nThe Bellman-Ford algorithm [17] can be used to produce in O(nm) time:\n– either a proof that G is conservative in the form of a feasible potential function; – or a proof that G is not conservative in the form of a negative cycle C in G.\nWhen the graph is conservative, the smallest weight of a walk between two nodes is well defined, and, fixed a root node r in G, the potentials returned by the Bellman-Ford algorithm are, for each node v, the smallest weight of a walk from r to v. Moreover, if all the arc weights are integral, then these potentials are integral as well. Therefore, the Bellman-Ford algorithm provides a proof to the following theorem.\nTheorem 1 ([3, 17, 22]). A graph admits a feasible potential if and only if it is conservative. Moreover, when all arc weights are integral, the feasible potential is an integral function.\nAn STN can be viewed as a weighted graph whose nodes are timepoints that must be placed on the real line and whose arcs express mutual constraints on the allocations of their end points. An STN G = (V,A) is called consistent if it admits a feasible scheduling, i.e., a scheduling s : V 7→ R such that\ns(v) ≤ s(u) + w(u, v) ∀ arc (u, v) of G.\nCorollary 1 ([3,17,18]). An STN G is consistent if and only if G is conservative.\nProof. A feasible scheduling is just a feasible potential. Therefore, this corollary is just a restatement of Theorem 1.\nIn this paper, we also deal with directed weighted hypergraphs.\nDefinition 1 (Hypergraph). A hypergraph H is a pair (V,A), where V is the set of nodes, and A is the set of hyperarcs. Each hyperarc A ∈ A is either a multi-head or a multi-tail hyperarc.\nA multi-head hyperarc A = (tA, HA, wA) has a distinguished node tA, called the tail of A, and a nonempty set HA ⊆ V \\ {tA} containing the heads of A; to each head v ∈ HA is associated a weight wA(v) ∈ R. Fig. 5a depicts a possible representation of a multi-head hyperarc: the tail is connected to each head by a dashed arc labeled by the name of the hyperarc and the weight associated to the considered head.\nA multi-tail hyperarc A = (TA, hA, wA) has a distinguished node hA, called the head of A, and a nonempty set TA ⊆ V \\ {hA} containing the tails of A; to each tail v ∈ TA is associated a weight wA(v) ∈ R. Fig. 5b depicts a possible representation of a multi-tail hyperarc: the head is connected to each tail by a dotted arc labeled by the name of the hyperarc and the weight associated to the considered tail.\nThe cardinality of a hyperarc A ∈ A is given by |A| , |HA∪{tA}| if A is multihead, and |A| , |TA ∪ {hA}| if A is multi-tail; if |A| = 2, then A = (u, v, w) is a standard arc. The order and size of a hypergraph (V,A) are denoted by n , |V | and m , ∑\nA∈A |A|, respectively."
    }, {
      "heading" : "4 Hyper Temporal Networks and Consistency Property",
      "text" : "We introduce now Hyper Temporal Networks (HyTNs), a strict generalization of STNs to partially overcome the limitation of allowing only conjunctions of constraints. Compared to STN distance graphs, which they naturally extend, HyTNs allow a greater flexibility in the definition of temporal constraints.\nA HyTN is a directed weighted hypergraphH = (V,A) where a node represents a time point variable (timepoint), and a multi-head/multi-tail hyperarc represents a set of temporal distance constraints between the tail/head and the heads/tails, respectively.\nFor example, the multi-tail hyperarc A = (TA, B 2 , wA) in Fig. 3, where TA = {b1, b2, b3} and wA(bi) = 0 for i = 1, 2, 3, stands for the set of distance constraints {B\n2 − bi ≤ 0 | i = 1, 2, 3}. In general, we say that a hyperarc is satisfied when at least one of its distance constraints is satisfied. Then, we say that a HyTN is consistent when it is possible to assign a value to each time-point variable so that all of its hyperarcs are satisfied.\nMore formally, in the HyTN framework the consistency problem is defined as the following decision problem.\nDefinition 2 (General-HyTN-Consistency). Given a HyTN H = (V,A), decide whether there exists a scheduling s : V → R such that, for every hyperarc A ∈ A, the following holds:\n– if A = (t, h,w) is a standard arc, then\ns(h)− s(t) ≤ w;\n– if A = (tA, HA, wA) is a multi-head hyperarc, then\ns(tA) ≥ min v∈HA {s(v)− wA(v)};\n– if A = (TA, hA, wA) is a multi-tail hyperarc, then\ns(hA) ≤ max v∈TA {s(v) + wA(v)}.\nAny such scheduling s : V → R is called feasible. A HyTN that admits at least one feasible scheduling is called consistent.\nComparing the consistency of HyTNs with the consistency of STNs, the most important aspect of novelty is that, while in a distance graph of a STN each arc represents a distance constraint and all such constraints have to be satisfied by a feasible schedule, in a HyTN each hyperarc represents a set of one or more distance constraints and a feasible scheduling has to satisfy at least one such distance constraints for each hyperarc.\nLet us show some interesting properties about the consistency problem for HyTNs.\nThe first interesting property is that any integral-weighted HyTN admits an integral feasible schedule when it is consistent, as proved in the following lemma.\nLemma 1. Let H = (V,A) be an integral-weighted and consistent HyTN . Then H admits an integral feasible scheduling s : V → {−T,−T+1, . . . , T −1, T}, where T = ∑\nA∈A,v∈V |wA(v)|.\nProof. Since H is consistent, then there exists a feasible scheduling s̃ : V → R. The idea in this proof is to project the HyTN H over a conservative graph and then, in that setting, to exploit the integrality properties of potentials as stated in Theorem 1. However, this projection is asked to resolve the non-determinism contained in the disjunctive nature of the hyperarcs; in order to sort out such non-determinism, the projection is built considering the given feasible scheduling s̃ as follows.\nFor each hyperarc A ∈ A, a weighted directed arc eA is defined as follows:\n– if A = (u, v, w) is a standard arc, then eA , (u, v, w). Note that s̃(v) ≤ s̃(u)+w follows by the feasibility of s̃; – if A = (tA, HA, wA) is a multi-head hyperarc, then\neA , (tA, vA, wA(v)) where vA = arg min v∈HA {s̃(v)− wA(v)}.\nHere, s̃(vA) ≤ s̃(tA) + wA(v) follows by the feasibility of s̃; – if A = (TA, hA, wA) is a multi-tail hyperarc, then\neA , (vA, hA, wA(v)) where vA = arg max v∈TA {s̃(v) + wA(v)}.\nHere, s̃(hA) ≤ s̃(vA) + wA(v) follows by the feasibility of s̃.\nNow, a weighted directed graph G = (V,E) with E , {eA | A ∈ A} is defined. G is integral-weighted and conservative graph since it admits s̃ as a potential function. Therefore, G admits an integral potential function s : V → {−T,−T + 1, . . . , T − 1, T}. Indeed, such a function s is obtained by applying the BellmanFord algorithm on G. To conclude, we observe that s is also an integral feasible scheduling for H.\nThe following theorem states thatGeneral-HyTN-Consistency is NP-complete.\nTheorem 2. General-HyTN-Consistency is an NP-complete problem even if input instances H = (V,A) are restricted to satisfy wA(·) ∈ {−1, 0, 1} and |HA|, |TA| ≤ 2 for every A ∈ A.\nProof. If H = (V,A) is integral-weighted and consistent, then it admits an integral feasible scheduling s : V → {−T, . . . , T} by Lemma 1. Moreover, any such feasible scheduling can be verified in polynomial time with respect to the size of the input; hence, General-HyTN-Consistency is in NP.\nTo show that the problem is NP-hard, we describe a reduction from 3-SAT. Let us consider a boolean 3-CNF formula with n ≥ 1 variables and m ≥ 1\nclauses:\nϕ(x1, . . . , xn) = m ∧\ni=1\n(αi ∨ βi ∨ γi)\nwhere Ci = (αi ∨ βi ∨ γi) is the i-th clause of ϕ and each αi, βi, γi ∈ {xj , xj | 1 ≤ j ≤ n} is either a positive or a negative literal.\nWe associate to ϕ a HyTN Hϕ = (V,A), where each boolean variable xi occurring in ϕ is represented by two nodes, xi and xi. V also contains node z that represents the reference initial node for the HyTN Hϕ, i.e., the first node that has to be executed. For each pair xi and xi, Hϕ contains a pair of hyperarc constraints as depicted in Fig. 6a: one with multi-head {xi, xi} and tail in z and the other multi-tail {xi, xi} and head in z. If Hϕ is consistent, the pair of hyperarcs associated to x,¬x assures that Hϕ admits a feasible scheduling s such that s(xi) and s(xi) are coherently set with values in {0, 1} (see Lemma 1). In this way, s is forced to encode a truth assignment on the xi’s. The HyTN Hϕ contains also a node Cj for each clause Cj of ϕ; each node Cj is connected by a multi-tail hyperarc with head in Cj and tails over the literals occurring in Cj and by two standard and opposite arcs with node z as displayed in Fig. 6b. Such setting of arcs assures that if Hϕ admits a feasible scheduling s, then s assigns value 1 at least to one of the node representing the literals connected with the hyperarc.\nMore formally, Hϕ = (V,A) is defined as follows:\n– V = {z} ∪ {xi | 1 ≤ i ≤ n} ∪ {xi | 1 ≤ i ≤ n} ∪ {Cj | 1 ≤ j ≤ m}; – A =\n⋃n i=1 Vari ∪ ⋃m j=1 Claj , where:\n– Vari = { (z, xi, 1), (xi, z, 0), (z, xi, 1), (xi, z, 0),\n({xi, xi}, z, [w(xi), w(xi)] = [−1,−1]), (z, {xi, xi}, [w(xi), w(xi)] = [0, 0]) } .\nThis defines the variable gadget for xi as depicted in Fig. 6a;\n– Claj = { (z, Cj , 1), (Cj , z,−1),\n({αj , βj , γj}, Cj , [w(αj), w(βj), w(γj)] = [0, 0, 0]) } .\nThis defines the clause gadget for clause Cj = (αi ∨ βi ∨ γi) as depicted in Fig. 6b.\nNotice that |V | = 1 + 2n + m = O(m+ n) and mA = 8n + 5m = O(m+ n); therefore, the transformation is linearly bounded in time and space.\nWe next show that ϕ is satisfiable if and only if Hϕ is consistent.\nAny truth assignment ν : {x1, . . . , xn} → {true, false} satisfying ϕ can be translated into a feasible scheduling s : V → Z of Hϕ as follows. For node z, let s(z) = 0, and let s(Cj) = 1 for each j = 1, . . . ,m; then, for each i = 1, . . . , n, let s(xi) = 1 and s(xi) = 0 if the truth value of xi, ν(xi), is true, otherwise let s(xi) = 0 and s(xi) = 1. It is easy to verify that, using this scheduling s, all the constraints comprising each single gadget are satisfied and, therefore, the network is consistent.\nVice versa, assume that Hϕ is consistent. Then, it admits an integral feasible scheduling s by Lemma 1. After the translation s(v) = s(v)− s(z), we can assume that s(z) = 0. Hence, s(Cj) = 1 for each j = 1, . . . ,m, as enforced by the two standard arcs incident at Cj in the clause gadget, and {s(xi), s(xi)} = {0, 1} for each i = 1, . . . , n, as enforced by the constraints comprising the variable gadgets. Therefore, the feasible scheduling s can be translated into a truth assignment ν : {x1, . . . , xn} → {true, false} defined by ν(xi) = true if s(xi) = 1 (and s(xi) = 0); ν(xi) = false if s(xi) = 0 (and s(xi) = 1) for every i = 1, . . . , n.\nTo conclude, we observe that any hyperarc A ∈ A of Hϕ has weights wA(·) ∈ {−1, 0, 1} and size |A| ≤ 3. Since any hyperarc with three heads (tails) can be replaced by two hyperarcs each having at most two heads (tails), the consistency problem remains NP-Complete even if wA(·) ∈ {−1, 0, 1} and |A| ≤ 2 for every A ∈ A.\nTheorem 2 motivates the study of consistency problems on hypergraphs having either only multi-head or only multi-tail hyperarcs. In the former case, the consistency problem is called Head-HyTN-Consistency, while in the latter it is called Tail-HyTN-Consistency. In the following theorem we observe that the two problems are inter-reducible, i.e., we can solve consistency for any one of the two models in f(m,n,W ) time whenever we have a f(m,n,W ) time procedure for solving consistency for the other one.\nTheorem 3. Head-HyTN-Consistency and Tail-HyTN-Consistency are inter-reducible by means of log-space, linear-time, local-replacement reductions.\nProof. We show the reduction frommulti-tail to multi-head hypergraphs; the other direction is symmetric. Informally, what we will do is to reverse all the arcs (so that what was multi-tail becomes multi-head), and, contextually, we invert the time-axis (to account for the inversion of the direction of all arcs).\nLet H = (V,A) be a multi-tail hypergraph, we associate to H a multi-head hypergraph H′ = (V,A′) by reversing all multi-tail hyperarcs. Formally, we define\nA′ = {(v, S,w) | (S, v, w) ∈ A}.\nFor example, a standard arc (t, h, w) ∈ A is transformed into a reversed standard arc (h, t, w) in A′ while a hyperarc with two weighted tails t1 and t2 becomes a hyperarc having t1 and t2 as its two weighted heads.\nNow, H is consistent if and only if H′ is consistent. To prove it, we note that each scheduling s for H can be associated, with a flip of the time direction, to\nthe scheduling s′ , −s. Then, it holds that s is feasible for H if and only if s′ is feasible for H′. Indeed, s satisfies the constraint represented by an hyperarc A = (TA, hA, wA) ∈ A, namely\ns(hA) ≤ max v∈TA {s(v) + wA(v)},\nor, equivalently\n−s(hA) ≥ min v∈TA {−s(v)− wA(v)},\nif and only if s′ (that is, −s) satisfies the constraint represented by the reversed hyperarc A′ = (hA, TA, wA), namely\ns′(hA) ≥ min v∈TA {s′(v)− wA′(v)}.\nIn the remainder of this work we shall adopt the multi-head hypergraph as our reference model. Hence, when considering hypergraphs and HyTNs, we will be implicitly referring to multi-head hyperarcs. Notably, we consider the following specialized notion of consistency for HyTNs.\nDefinition 3 (HyTN-Consistency). Given a (multi-head) HyTN H = (V,A), decide whether there exists a scheduling s : V → R such that:\ns(tA) ≥ min v∈HA {s(v)− wA(v)} ∀A ∈ A. (1)\nRemark 1. Notice that this notion of consistency for HyTNs is a strict generalization of STN one. In general, the feasible schedules of an STN are the solutions of a linear system and, therefore, they form a convex polytope. Since an STN may be viewed as a HyTN, the space of feasible schedules of an STN can always be described as the space of feasible schedules of a HyTN. The converse is not true because feasible schedules for a HyTN do not form a convex polytope. Let us consider, for example, a HyTN of just three nodes x1, x2, x3 and one single hyperarc with heads {x1, x2} and tail x3 expressing the constraint x3 ≥ min{x1, x2}; (0, 2, 2) and (−2, 0, 2) are both admissible schedules, but (1, 1, 0) = 12(0, 2, 2)− 1 2 (−2, 0, 2) is not an admissible schedule. In conclusion, the STN model is a special case of the Linear Programming paradigm, whereas the HyTN model is not.\nIn the rest of this section, we extend the characterization of STN consistency recalled in Section 3 to HyTNs.\nDefinition 4 (Reduced Slack Value wpA(v)). With reference to a potential p : V → R, we define, for every arc A ∈ A and every v ∈ HA, the reduced slack value wpA(v) as wA(v) + p(tA)− p(v) and the reduced slack w p A as\nwpA , max{w p A(v) | v ∈ HA}.\nA potential p is said to be feasible if and only if wpA ≥ 0 for every A ∈ A.\nAgain, as it was the case for STNs, a mapping f : V → R is a feasible potential if and only if it is a feasible schedule. In order to better characterize feasible schedules, we introduce a notion of negative cycle.\nDefinition 5 (Negative Cycle). Given a multi-head HyTN H = (V,A), a cycle is a pair (S,C) with S ⊆ V and C ⊆ A such that:\n1. S = ⋃\nA∈C(HA ∪ {tA}) and S 6= ∅; 2. ∀v ∈ S there exists an unique A ∈ C such that tA = v.\nMoreover, we let a(v) denote the unique arc A ∈ C with tA = v as required in previous item 2. Every infinite path in a cycle (S,C) contains, at least, one finite cyclic sequence vi, vi+1, . . . , vi+p, where vi+p = vi is the only repeated node in the sequence. A cycle (S,C) is negative if for any finite cyclic sequence v1, v2, . . . , vp, it holds that\np−1 ∑\nt=1\nwa(vt)(vt+1) < 0.\nThere are two results about negative cycles as stated in the following lemmas.\nLemma 2. A HyTN with a negative cycle admits no feasible schedule.\nProof. By contraposition. Let H be a consistent HyTN and let p be a feasible potential for H. Also, let (S,C) be any cycle of H; we will show that (S,C) is not negative. For every A ∈ C, let hA be the head of A with maximum reduced slack value:\nhA , arg max v∈HA\n{wpA(v)}.\nLet us consider the infinite path in (S,C) built choosing, at each node vt, ha(vt) as the following node. As already seen, such a path contains at least one finite cyclic sequence vh, vh+1, . . . , vk with vk = vh. The sum of weights of the finite cyclic sequence is given by\nk−1 ∑\nt=h\nwa(vt)(vt+1) =\nk−1 ∑\nt=h\nwp a(vt) (vt+1)\nfor every potential p; since p is feasible, all terms of the last sum are non-negative. It follows that (S, C) is not negative.\nAt first sight, it may appear that checking whether (S,C) is a negative cycle might take exponential time since one should check a possibly exponential number of cyclic sequences. The next lemma shows instead that it is possible to check the presence of negative cycle in polynomial time.\nLemma 3. Let (S,C) be a cycle in a HyTN. Then checking whether (S,C) is a negative cycle can be done in polynomial time.\nProof. Consider the weighted graph G = (S,∪t∈SAt) where each hyperarc a(t), for every t ∈ S, is transformed into a set of standard arcs as follows:\na(t) ❀ At , {(t, v,−wa(t)(v)) | v ∈ Ha(t))}, ∀ t ∈ S.\nNotice that G is thus an STN. Checking whether (S,C) is a negative cycle amounts to check whether all cycles in G have strictly positive weight. To do this, firstly, a potential function π for G is determined by Bellman-Ford algorithm. If the algorithm returns a negative cycle instead of π, then there is no negative cycle in (S,C) and the check ends.\nOtherwise, since w(C) = wπ(C) ≥ 0 for every cycle C of G, it is necessary to verify that no cycle in G has wπ(C) = 0. This check can be done by verifying the acyclicity of the subgraph of G comprising only arcs a of G with wπ(a) = 0. The check that a graph is acyclic can be done in linear time by a depth first visit [17].\nA hypergraph H is called conservative when it contains no negative cycle. In the next sections we will provide a pseudo-polynomial time algorithm that always returns either a feasible scheduling or a negative cycle, thus extending the validity of the classical good-characterization of STN consistency to general HyTN consistency. Here, we anticipate the statement of the main result in order to complete this general introduction of HyTNs.\nTheorem 4. A HyTN H is consistent if and only if it is conservative. Moreover, when all weights are integral, then H admits an integral scheduling if and only if it is conservative.\nProof. If H is consistent, then it is conservative by Lemma 2. If H is not consistent, then there is a negative cycle as shown in Theorem 8-(3). The existence of an integral scheduling when all weights are integral is guaranteed by Lemma 1."
    }, {
      "heading" : "5 Mean Payoff Games",
      "text" : "In this section, we propose an introduction to Mean Payoff Games (MPGs) tailored to the needs of the present work. MPGs represent a well-studied model for representing some kinds of two-player dynamics and we will show in Section 6 that there is a substantial equivalence between the MPG and the HyTN model, which will allow us to exploit some important algorithmic and structural results.\nAn MPG is a weighted directed graph G = (V0 ∪· V1, E) whose node set V is partitioned into two disjoint sets V0 and V1, where, for p = 0, 1, the nodes in Vp are those under control of Player p. Even with these graphs we have no loops and no parallel arcs. It is also assumed that every node has at least one outgoing arc. Notice that, in general, (V0, V1) does not need to be a bipartiton of G, i.e., E may contain arcs with both endpoints in V0, or with both endpoints in V1.\nEach play starts with a pebble placed at some node v0 ∈ V0 ∪· V1 and consists in a sequence of moves. Move t begins with the pebble placed in node vt−1 and is played by the Player p such that vt−1 ∈ Vp: the player chooses any arc e ∈ E with tail te = vt−1 and moves the pebble along e; at the end of the move the pebble is in node vt = he. The game ends as soon as vt = vt′ for some t > t\n′, i.e., when the pebble comes back to an already visited node vt′ . At this point, the pebble has traversed a cyclic sequence of arcs et′+1, . . . , et and Player 0 “pays” to Player 1 the average weight of the visited cycle:\n1\nt− t′\nt ∑\ni=t′+1\nw(ei).\nIf this amount is negative, then Player 0 wins the game, otherwise the winner is Player 1.\nA strategy for Player p is a mapping that, given all the previous visited nodes and the current node, returns which node has to be visited in the next move; a strategy is said to be positional (or memoryless) if it depends only on the current position vt and does not take into account all the previous history. If s ∈ V0 ∪ V1 and Player p has a strategy leading him to win any possible play starting at v0 = s, then we say that s is a winning start position for Player p. We denote by Wp the set of winning start positions for Player p. A winning strategy for Player p leads Player p to win every play started from any node in Wp. Since these finite games are zero-sum, i.e., what won by a player is what lost by the other one, then they admit a game value ν: for each start position s ∈ V of the game, there exists a νs ∈ R such that Player 0 has a strategy ensuring payoff at most νs, while Player 1 has a strategy ensuring payoff at least νs.\nIt is worthwhile to consider an infinite variant of the model, in which the game does not stop, and continues for an infinite number of steps. In this model, Player 1 wants to maximize the limit inferior of the average weight:\nlim inf n→∞\n1\nn\nn ∑\nt=1\nw(vt−1, vt)\nSymmetrically, Player 0 wants to minimize the limit superior of the same average weight:\nlim sup n→∞\n1\nn\nn ∑\nt=1\nw(vt−1, vt)\nIn their Determinacy Theorem, Ehrenfeucht and Mycielski [21] proved that any infinite game admits a value ν∞, and that this value equals the one of the finite counterpart game on every start position, i.e., ν∞s = νs for every s ∈ V0 ∪ V1. Moreover, they proved the existence of positional strategies which are optimal for both variants of the model: when Player p limits himself to an optimal strategy πp, i.e., when, for every v ∈ Vp, he disregards all arcs with tail in v except the one with head in πp(v), then he will secure himself the optimal payoff ν in every play, finite or infinite, however the adversary plays. The graph Gπp obtained from G by dropping all arcs with tail in Vp not prescribed by πp is called the projection of the game G on πp, and is a solitaire game whose value can be easily computed by means of a simple variant of Bellman-Ford algorithm. Therefore, the Ehrenfeucht and Mycielski’s results are already sufficient for determining a simple exponential time algorithm computing the node values and the two optimal positional strategies in an MPG: the algorithm consists in evaluating each possible strategy for one of the two players as a solitaire game for determining the optimal one. In the literature there are many local search algorithms that explore this space in a more efficient way [5, 6, 41, 42] and some of them have been proven to be practically efficient in many settings by experiments [6, 41]. Moreover, the global optimization problem of computing the best strategies for one player, according to a given metric, has been shown to have the property that every local optimum is also a global one for many complete metrics [5].\nAs another line of research, Zwick and Paterson [47] proposed pseudopolynomial time algorithms for computing values of games, as well as positional optimal strategies. In particular, they considered the following four algorithmic problems:\n1. MPG-Decision(ν, s): given a real number ν and a start position s, decide whether νs ≥ ν; 2. MPG-Threshold(T ): given a real number T , determine for which nodes s ∈ V it holds that νs ≥ T ; 3. MPG-Values: compute the optimal values νs for all s ∈ V . 4. MPG-Synthesis: assuming νs ≥ 0 (νs < 0) for every s ∈ V , synthesize a\npositional winning strategy for Player 1 (Player 0);\nand they proved the following theorem:\nTheorem 5 ([47]). Let G = (V,E) be a mean payoff game. Assume all weights are integers and let W = maxe∈E |w(e)|. Then the following hold:\n1. MPG-Threshold(T ) can be solved in time O(|V |2|E|W ) when T ∈ Z, whereas it can be solved in time O(|V |3|E|W ) when T ∈ R; 2. MPG-Values can be solved in time O(|V |3|E|W ); 3. MPG-Synthesis can be solved in time O(|V |4|E| log(|E|/|V |)W ).\nThen, they observed that MPG-Decision is the basic decision problem for MPGs in the sense that several natural questions for MPGs, like evaluating the value νs for every node s or constructing the optimal positional strategies, may all be Turing-reduced to it. They also pointed out that the existential results of Ehrenfeucht and Mycielski [21] already implies that MPG-Decision ∈ NP ∩ co-NP and asked whether there might exist a strongly polynomial time decision procedure. Proving the existence of such algorithm is an open problem [7]. Finally, they showed how to reduce mean payoff games to other important families of games on graphs, like discounted payoff games and simple stochastic games.\nThe complexity status of MPG-Decision has been since updated by proving that it lays in UP ∩ co-UP by Jurdziński in [27].\nIn recent years, some other interesting results have been proven. Notably, in 2007 Lifshits, Pavlov [33] proposed a potential theory for MPGs and in 2011 Brim et al. [7] obtained faster algorithms exploiting results obtained in the the fields of Energy Games and energy progress measures, which are intimately related to the potentials studied in [33].\nTheir algorithmic results are summarized in the following theorem.\nTheorem 6 ([7]). For MPGs in which all weights are integers and for T ∈ Z, the Value Iteration Algorithm [7] solves MPG-Threshold(T ) and MPG-Synthesis in time O(|V | |E|W ), where W = maxe∈E |w(e)|.\nWe remark that both the algorithm of Paterson and Zwick [47] and the Value Iteration Algorithm [7] prescribe well defined procedures even if the weights on the arcs are real values. What is lost in running these algorithms on real weights is only the pseudo-polynomial upper bound on their running time.\nFor our purposes, the family of pseudo-polynomial algorithms for MPGs is the best option. Indeed, in most of temporal workflow graphs all weights are expressed by integers of relatively small magnitude with respect to the intrinsic temporal granularity of the considered workflow. For example, in a workflow containing temporal distance constraints of days, the commonly adopted temporal granularity is the “minute” (m) and, therefore, all weights can be assumed to be less than 104 as order of magnitude. In such circumstances, Brim’s algorithm offers the guarantee to terminate within short computation times. For these reasons we opted for\nintegrating the procedures of Zwick and Paterson, as well as the faster procedures of Brim et al. [7], in order to efficiently solve instances of HyTN-Consistency and compute feasible schedules.\nFurthermore, as will be discovered in the experimental section, if these algorithms are suitably adapted—so as to allow them to terminate earlier as soon as certain evidences of inconsistency have been collected—then their observed behavior outperforms by orders of magnitude what predicted by their theoretical pseudo-polynomial bounds even on input instances containing very large integer values.\nBased on these findings, we think that these pseudo-polynomial algorithms are to be considered (and probably adopted) even for solving HyTN instances where weights are floating point values whose magnitudes may differ in a significant way. In case the running time results to be unacceptable for a real application, one could then consider the possibility to round the weights to integer values. This rounding would clearly require special care: a very accurate approximation might lead to very high computation times while a gross approximation might not represent the original instance in a correct way."
    }, {
      "heading" : "6 The Reductions",
      "text" : "This section presents the direct connection and the computational equivalence between MPG-Threshold and HyTN-Consistency. The equivalence is formally proven by offering one reduction in each direction.\nThe reduction of HyTN-Consistency to MPG-Threshold allows to apply, in the context of HyTNs, any of the algorithms known for MPGs, included the exponential and subexponential ones.\nVice versa, in consideration of the fact that the MPG-Decision ? ∈ P question is an open problem [5, 7, 27, 41, 47], the reduction of MPG-Decision to HyTNConsistency confirms that HyTN-Consistency offers an algorithmically more ambitious and mathematically steeper generalization of STN-Consistency (see also Remark 1). Moreover, the reduction gives a further evidence that, within STNs, a new algorithmic approach is necessary in order to manage temporal aspects of event like the synchronization one presented in the Introduction.\nLet us start considering the first reduction.\nTheorem 7. There exists a log-space2, linear-time, local-replacement3 reduction from HyTN-Consistency to MPG-Threshold.\nSince this reduction plays a main role in the algorithmic solutions proposed in this paper, we firstly describe how it works and, secondly, we prove its correctness by means of two lemmas, Lemma 4 and Lemma 5.\nThe reduction goes as follows. Let H = (V,A) be a HyTN. We assume that every v ∈ V is the tail of some arc A ∈ A. This assumption is not a restriction since, if H contains a sink node v, i.e., a node v with no arc A ∈ A having tail in it, then H is consistent if and only if so is Hv, the HyTN obtained from H by removing node v and every hyperarc\n2 A strong and basic-form of reduction introduced by Papadimitriou in [38]. 3 A restricted kind of Karp reduction introduced in [24].\nhaving v as an head. Indeed, any feasible scheduling s : V 7→ R for H, once projected onto V \\ {v}, gives a feasible scheduling for Hv since every constraint involving v has been dropped and no constraint has been added; conversely, any feasible scheduling s for Hv can be easily extended to a feasible scheduling s for H by exploiting the property of v being a sink node: it is sufficient to set s(v) , min{s(tA)− wA(v) | A ∈ A, v ∈ HA}.\nNow, let us consider a mean payoff game GH = (V0∪· V1, E) where: (1) V0 = V , V1 = A, nodes in V0 are colored by black while nodes in V1 are colored by white, and (2) for each A ∈ A, the following weighted arcs are added to E:\n– an arc of weight 0 from the black node ta to the white node A, i.e., arc (tA, A, 0); – for each head node h ∈ HA, an arc of weight wA(h) from the white node A to\nthe black node h, i.e., arc (A, h,wA(h)).\nIn short, GH = (V0 ∪· V1,A), with V0 = V , V1 = A, E = {(tA, A, 0) | A ∈ A} ∪ {(A,h,wA(h)) | A ∈ A, h ∈ HA}. Fig. 7 depicts how a hyperarc is transformed into a MPG subnetwork while Fig. 8 reports a pseudocode for the whole construction process, i.e., Algorithm 1.\nGH has |V |+ |A| nodes and O(m) arcs and can be constructed in linear time. Moreover, GH is a bipartite graph with bipartition (V0, V1) and it has been obtained from H by a simple local replacement rule: replace every hyperarc A ∈ A by a claw subgraph as depicted in Fig. 7. For each single object, it is necessary only to manage a constant number of indexes, each of them having a polynomial size; thus the reduction is log-space. Fig. 9 depicts an MPG obtained applying the reduction to the motivating example HyTN depicted in Fig. 3; we remark that\nthe MPG depicted in Fig. 9 has been obtained by considering the (equivalent) multi-head HyTN transformation of the multi-tail HyTN shown in Fig. 3, indeed, Theorem 3 allows us to consider both the multi-head or the (equivalent) multi-tail HyTN without concerns with respect to the consistency checking problem.\nNow, let us introduce the formal proof of Theorem 7 by the following two lemmas.\nLemma 4. If H is consistent then every node of GH is a winning start position for Player 1.\nProof. Since H is consistent, there exists a feasible scheduling s : V → R such that, for each hyperarc A ∈ A, the reduced slack weight is non-negative wsA ≥ 0. Consider the following positional strategy π1 for Player 1: for each A ∈ V1,\nπ1(A) = arg min h∈HA {s(h)− wA(h)}.\nWe claim that π1 ensures Player 1 the win, wherever node the game starts from and however Player 0 moves. In order to show this, we prove that the projection graph Gπ1 is conservative exhibiting a feasible potential p. Let p : V0 ∪ V1 → R be defined as follows:\np(v) ,\n{\ns(v) if v ∈ V0, s(t(v)) if v ∈ V1. (2)\nNow, let a = (u, v, w) be any arc of Gπ1 : Case 1: if v ∈ V1, then v is a hyperarc of H with t(v) = u and w = 0; therefore, p(v) = s(t(v)) = s(u) = p(u) since u ∈ V0. Then w\np(u, v) = w−p(v)+p(u) = 0 ≥ 0 follows;\nCase 2: if v ∈ V0, then u ∈ V1 and w = wu(v). Moreover, v = π1(u), which implies that v = argminh∈Hu{s(h) − wu(h)}. Therefore, recalling that w s u ≥ 0, i.e., s(t(u)) ≥ minh∈Hu{s(h)− wu(h)}:\np(u) = s(t(u)) ≥ min h∈Hu {s(h)− wu(h)} = s(v)− wu(v) = p(v)− w.\nHence, wp(u, v) = w − p(v) + p(u) ≥ 0. In conclusion, Gπ1 is conservative. Therefore, the positional strategy π1 certifies that any node of G is a winning start position for Player 1.\nLemma 5. If every node of GH is a winning start position for Player 1 then H is a consistent HyTN.\nProof. If every node is a winning start position for Player 1, then there exists a positional strategy π1 which is everywhere winning for Player 1. Notice that Gπ1 must be conservative since Player 0 can clearly win any play starting from a node located on a negative cycle. Let p : V0∪V1 → R be a feasible potential for Gπ1 . We claim that the restriction of p onto V0 is a feasible scheduling for H. Indeed, for any hyperarc A of H, (tA, A, 0) is an arc of Gπ1 , whence p(A) ≤ p(tA). Moreover, (A,π1(A),wA(π1(A))) is also an arc of Gπ1 , whence p(π1(A)) ≤ p(A)+wA(π1(A)). Since π1(A) ∈ HA, then the following holds:\np(tA) ≥ p(A) ≥ p(π1(A))− wA(π1(A))\n≥ min h∈HA {s(h)− wA(h)}.\nHence, the restriction of p onto V0 is a feasible scheduling for H. Thus, H is consistent.\nIn Fig. 10 the values under the nodes represent a feasible potential for the projection of the MPG depicted in Fig. 9. By Lemma 5, the restriction of such a feasible potential on the black nodes is also a feasible scheduling for the corresponding HyTN depicted in Fig. 3. Now, we have all the necessary results to prove the following theorem.\nTheorem 8. Let H = (V,A) be an integral-weighted HyTN, m = ∑\nA∈A |A|, and W = maxA∈A{maxh∈A |wA(h)|} the maximal weight value present in H. The following propositions hold:\n1. There exists an O((|V |+ |A|)mW ) pseudo-polynomial time algorithm deciding HyTN-Consistency for H; 2. There exists an O((|V |+|A|)mW ) pseudo-polynomial time algorithm such that, given on input any consistent HyTN H, it returns as output a feasible scheduling s : VH → Z of H; 3. There exists an O((|V |+|A|)mW ) pseudo-polynomial time algorithm such that, given on input any not-consistent HyTN H, it returns as output a negative cycle (S,C) of H.\nProof. 1. The decision algorithm is sketched in Fig. 11. It takes in input a HyTN H = (V,A) and, in line 1, constructs the corresponding MPG GH as described in Theorem 7. This first step takes O(m) time and yields a graph with |V |+ |A| nodes and O(m) arcs. Then, in line 2, the instance of MPG-Threshold with\nT = 0 on graph GH is solved in O((|V |+ |A|)mW ) time by the Value Iteration Algorithm (see Theorem 6). The output consists in a partition of GH nodes into two sets: W1 = {v ∈ V ∪ A | νv ≥ 0} and W0 = {v ∈ V ∪ A | νv < 0}. If W0 is empty, then H is consistent by Lemma 5, otherwise it is not consistent by Lemma 4. 2. In case W0 is empty, a feasible scheduling is obtained as shown in Algorithm 3. First, in line 2, the algorithm computes a positional winning strategy π1 for Player 1. This takes O((|V | + |A|)mW ) time by Theorem 6. Next, in line 3, it builds the graph Gπ1 which is conservative since π1 is a positional winning strategy for Player 1. Then, in lines 4-5, it adds a new node s to V1 and a new arc ev = (s, v, 0) for each node v ∈ V0 in Gπ1 . Let G ′ π1 = (V0 ∪· (V1 ∪ {s}), E ′)\nAlgorithm 3: computeAFeasibleSchedule(H)\n// a consistent HyTN H = (V,A) 1 G ← makeACorrespondingGame(H); // See Algorithm 1 2 π1 ← MPG-Synthesis (G); // Compute a positional winning strategy for Player 1; see Theorem 6 3 Gπ1 ← compute the subgraph of G induced by π1;\n// Recall Gπ1 = (V0 ∪· V1, E), where V0 = V and V1 = A. 4 s ← a new node; // s 6∈ V0 ∪ V1 5 Add s to V1 and add an arc (s, v, 0) for each v ∈ V0; 6 p ← Bellman-Ford(Gπ1 , s); // compute a potential function p\nOutput: the restriction of p onto V\nRemark 2. In Theorem 8 Item 2), a set of feasible potentials may be obtained without executing the Bellman-Ford algorithm. Actually, if the partition (W0,W1) is computed by the Value Iteration Algorithm [7], then a feasible scheduling for H can be directly derived from the progress measure computed within the algorithm. In more detail, let G = (V0 ∪· V1, E) be an MPG weighted by w : E → Z. An energy progress measure is a function f : V0 ∪ V1 → N ∪ {+∞} such that: if v ∈ V0, then for every (v, v\n′, w) ∈ E it holds f(v) ≥ f(v′) − w; otherwise, v ∈ V1 and there exists (v, v′, w) ∈ E such that f(v) ≥ f(v′) − w. An energy progress\nmeasure f : V0 ∪V1 → N∪ {+∞} such that 0 ≤ f(v) < +∞ for every v ∈ V0 ∪ V1 is provided by the resolution algorithm of Theorem 6 in time O((|V |+ |A|)mW ).\nThe progress measure f is already a feasible scheduling for H: in fact, for every hyperarc A ∈ A, it holds (tA, A, 0) ∈ E and (A, v, wA(v)) ∈ E, for every v ∈ HA; combining these two last facts, it follows that:\nf(tA) ≥ f(A) ≥ min v∈HA {f(v)− wA(v)},\ni.e., f is a scheduling satisfying all constrains A ∈ A. This allow us to employ the algorithm depicted in Fig. 14 instead of the one depicted in Fig. 12 in the case that W1 = V .\nThe computational equivalence between MPG-Decision problem and HyTNConsistency can be now determined by showing that also MPG-Decision can be reduced to HyTN-Consistency.\nTheorem 9. There exists a log-space, linear-time, local-replacement reduction from MPG-Decision to HyTN-Consistency.\nProof. Let G = (V0 ∪· V1, E) be an MPG. For each node u ∈ V0 ∪ V1, let NG(u) denote the outgoing neighborhood of u in G, i.e., NG(u) , {v ∈ V0 ∪ V1 | (u, v) ∈ E}.\nA corresponding HyTN H = (V,A), where V = V0 ∪· V1, is constructed from G as follows. For every u ∈ V1, a hyperarc Au ∈ A is added to H, where:\nAu , (u,NG(u), wAu),\nwith weight wAu(v) , w(u, v) for every v ∈ NG(u). Moreover, for every u ∈ V0 and every v ∈ NG(u), a hyperarc Auv ∈ A is added to H, where:\nAuv , (u, v, w(u, v)).\nThis construction requires a log-space and linear-time computation. Now, we firstly prove that if H is consistent then every node of G is a winning start position for Player 1. Indeed, let s : V → R be a feasible scheduling for H. Thus, wsA ≥ 0 for every hyperarc A ∈ A. Notice that, by construction, for each u ∈ V1 there exists a unique hyperarc Au ∈ A with tail tAu = u; moreover, it holds that HAu , NG(u). Hence, for each u ∈ V1, we can define a positional strategy π1 for Player 1 as follows:\nπ1(u) , arg min h∈HAu {s(h)− wAu(h)}.\nNow, consider the potential function p : V → R defined as: p(u) , s(u) for every u ∈ V . We argue that p is a feasible potential for Gπ1 .\nIn fact, let a = (u, v, w) ∈ E be any arc of G:\nCase 1: Assume that u ∈ V0. Then, by construction, a = Auv. Hence, from p , s and the feasibility of s, we have:\np(u) = s(u) ≥ minh∈HAuv {s(h)− wAuv (h)} = s(v)− wAuv (v) = p(v)− w\nHence, wp(u, v) = w − p(v) + p(u) ≥ 0; Case 2: Assume that u ∈ V1. Then, by construction, A = (u,NG(u), wA), where\nwA(z) = w(u, z) for every z ∈ NG(u); moreover, notice that v = π1(u) ∈ NG(u) = HA. Hence, from p , s, the feasibility of s, and the definition of π1, we have:\np(u) = s(u) ≥ minh∈Hu{s(h)− wAu(h)} = s(π1(u))− wAu(π1(u)) = s(v)− wAu(v) = p(v)− w\nHence, wp(u, v) = w − p(v)− p(u) ≥ 0.\nThus, Gπ1 is conservative. This implies that every node of G is a winning start for Player 1.\nSecondly, we prove that if every node of G is a winning start position for Player 1, then H is consistent.\nLet π1 be a positional winning strategy for Player 1. It follows that Gπ1 is conservative and, therefore, it admits a feasible potential p : V → R. Now, consider the scheduling function s : V → R for H defined as: s(u) , p(u) for every u ∈ V . We argue that s is a feasible scheduling of H.\nIn fact, let A = (tA, HA, wA) ∈ A be any hyperarc of H:\nCase 1: assume tA ∈ V0. Then, by construction, A = (u, v, w) for some v ∈ NG(u), w ∈ R and u = tA. Hence, from s , p and the feasibility of p, we have:\ns(tA) = p(u) ≥ p(v)− w = s(v)− wA(v) = minh∈HA{s(h)− wA(h)}\nHence, s satisfies A, i.e., wsA ≥ 0 ;\nCase 2: assume tA ∈ V1. Then, by construction, A = (u,NG(u), wA) for u = tA and wA(v) = w(u, v) ∈ R for every v ∈ NG(u); moreover, if v , π1(u), then v ∈ NG(u) = HA. Hence, from s , p and the feasibility of p, we have:\ns(tA) = p(u) ≥ p(v)− w = s(v)− wA(v) ≥ minh∈HA{s(h)− wA(h)}\nHence, s satisfies A, i.e., wsA ≥ 0.\nThis proves that s satisfies every hyperarc A ∈ A. Then s is a feasible scheduling of H, which is thus consistent."
    }, {
      "heading" : "7 Computational Experiments",
      "text" : "This section describes our empirical evaluation of the proposed consistency checking algorithms to evaluate the performances and the general applicability of the proposed HyTN model. Both Algorithm 3 and Algorithm 4 consist of one single call to Algorithm 2, plus some extra computation of lower asymptotic complexity. Since the cost of these further computations was confirmed to be practically negligible in some preliminary experiments, we report on the results of our experimental investigations only for Algorithm 2.\nAll algorithms and procedures employed in this empirical evaluation have been implemented in C/C++ and executed on a Linux machine having the following characteristics:\n– 2 CPU AMD Opteron 4334; – 64GB RAM; – Ubuntu server 14.04.1 Operating System.\nThe source code and all HyTNs used in the experiments are freely available [14]. The main goal of this empirical evaluation was to determine the average computation time of Algorithm 2, with respect to randomly-generated HyTNs following different criteria, in order to give an idea of the practical behavior of the algorithm. According to Theorem 8, the worst-case time complexity of Algorithm 2 is O((n + m′)mW ), where n = |V |, m′ = |A|, m = ∑\nA∈A |A|, and W = maxA∈A{maxh∈A |wA(h)|}. Hence, we implemented different experiments with respect to the parameters n,m′,m, and W . Here we propose a summary of the obtained results presenting a brief report about four tests, Test 1, Test 2, Test 3 and Test 4.\nIn Test 1 the average computation time was determined for different HyTN orders n to emphasize the practical computation time dependency on n. In Test 2 the average computation time was determined for different HyTN maximal edgeweights W to understand how much the practical computation time is dependent on W . In Test 3 we investigated how some execution times affect the value of the standard deviation, with the goal to determine how many instances require a significant greater computation time with respect to the average time of a data set. Finally, in Test 4 the average computation time was determined with respect to different values of the number of possible strategies of Player 1 ∏\nA∈A |HA| in order to give an idea about the possible practical relation between execution time and number of possible strategies.\nThe generation of random HyTN instances was carried out exploiting two generators. The first generator was the random workflow schema generator provided by ATAPIS toolset [31]: it produces random workflow graphs according to different input parameters that allow to control the minimal and maximal number of activities, probability for having parallel branches, the minimal and maximal probability of inter-task temporal constraints, etc. on the generated graphs. We verified that this tool allows the determination of graphs that are not only a closer approximation to real-world examples, but also more difficult to check than those generated at random without particular criteria.\nWe generated benchmarks as follows:\n1. First, temporal workflow graphs were generated by fixing the probability for parallel branches to 10% and maximal value for each activity duration or delay between activities to a value W , where W was chosen accordingly to the test type; 2. Then, each workflow graph was translated into an equivalent HyTN H by the simple transformation algorithm exemplified in Section 2.\nIt is worth noting that different random workflow graphs all having the same number of activities may translate into HyTNs having different orders n because the original workflow graphs may have different number of connector nodes. Considering the transformation algorithm exemplified in Section 2, it is easy to verify that a workflow with N activities can translate into a CSTN having between 2N + 2 nodes (when the workflow is a simple sequence) and 5N + 2 nodes (when the workflow is a sequence of groups of two parallel activities).\nATAPIS toolset has been designed to generate graphs with strongly connected components (Andreas Lanz, personal communication, October 6, 2015). In particular, it has been optimized for small graphs with up to 50 activities. This design choice was motivated by the widely accepted seven process modeling guidelines [34] which suggests to always “decompose a (workflow) model with more than 50 elements (activities)”. Therefore, we used the tool for generating random workflow graphs with 100 activities at maximum and, consequently, obtaining HyTNs having 502 nodes at most.\nIn Table 1 we report the orders of the smallest and the largest HyTN determined from each set of random generated workflow graphs having N activities for N ∈ {10, 20, . . . , 100}.\nIn order to study the scalability of the algorithm with respect to the number of nodes, we had to rely on a second generator of random HyTN graphs. Our choice has been to use the randomgame procedure of pgsolver suite [39], that can produce parity games instances for any given number of nodes. In particular, we exploited randomgame in the following way:\n1. First, randomgame was used to generate random directed graphs with outdegree fixed to 3; 2. Then, the resulting graphs were translated into MPGs by weighting each arc with an integer randomly chosen in the interval [−W,W ], where W was chosen accordingly to the test type; 3. Finally, each MPG G was translated into a HyTN HG by the reduction algorithm of Theorem 9. During the translation from MPG to HyTN, only 10% of the hyperarcs were maintained having multiple heads, while 90% of hyperarcs were transformed into standard arcs. This 10%-rule stems from the fact that we are considering workflow based applications where the percentage of (multiheaded) hyperarcs is less than 10% compared to standard arcs in general.\nWith such settings, the resulting HyTNs are characterized by m,m′ ∈ Θ(n).\nBefore presenting the summary of results, it is worthwhile to present some implementation choices about Algorithm 2 that we had to adopt. The core of the algorithm consists of calls to algorithms makeACorrespondingGame(H), that transforms the given HyTN H into a MPG GH, and solveMPG-Threshold(GH , 0) (Value Iteration algorithm), that determines for which game nodes s it holds that vs ≥ 0. The makeACorrespondingGame() implementation didn’t require significant choices thanks to the simple structure of the algorithm. On the contrary, in the implementation of solveMPG-Threshold() we introduced some further ideas in order to speed-up the algorithm and avoid unnecessary computations. In particular, it is not necessary for solveMPG-Threshold() to continue to determine other potential value vs′ as soon as it determines a value vs < 0: at this point we can already conclude that the network is not consistent and, with a lower computational cost, we can yield a generalized negative circuit assessing this fact (Lemmas 4 and 5).\nMoreover, we verified that there is an important data structure in the original Value Iteration algorithm, a queue, that is not further specified by the authors and that different implementations of it affect the performance of the algorithm. Therefore, we decided to verify whether solveMPG-Threshold() performance could be appreciably improved adding a suitable stopping criterion and a proper queue implementation. Table 2 reports the obtained results, mean execution time µ and its standard deviations σ, determined running the following different versions of\nsolveMPG-Threshold() on the same data set of 103 not consistent HyTNs4 having |V | = 106 and W = 103:\n1. FIFO Queue: the original queue is implemented as a FIFO queue; 2. LIFO Queue: the original queue is implemented as a a LIFO queue (stack); 3. LIFO Queue+Stopping Criterion: the queue is implemented as stack and\nthe computation is halted either when all potential values are stable or when any of them is negative; 4. Max-Priority Queue: the original queue is implemented as a Fibonacci’s heap.\nThe results show that, in general, solveMPG-Threshold() performance becomes better if the original queue is implemented as a stack and, in particular, a further improvement can be obtained if the stopping criterion is also considered. Nevertheless, such improvements can only partially reduce the statistics variability of the running time, as it is shown in the following experimental results.\nAs mentioned above, the goal of Test 1 was to determine the average computation time of Algorithm 2 implementation for different values of n to study the practical computation time dependency on such parameter.\nThe instances in Test 1 come from the randomgame generator, except those for the first row of the table in Fig. 15a which have been built by the ATAPIS workflow random generator. In particular, for each n ∈ {1 ·105, 2 ·105, . . . , 10 ·105}, 1600 HyTN instances with maximum weight W := 1000 and unknown consistency state were generated by randomgame, whereas 1600 HyTNs of unknown consistency state and order n around 400 were generated by ATAPIS. The results of the test are summarized in Fig. 15, where each execution mean time is depicted as a point with a vertical bar representing its confidence interval determined according to its standard deviation.\nThe depicted function interpolating the mean values shows that the practical performance of the algorithms is definitely better than the theoretical worst-case bound of O((n + m′)mW ); in our case this last is O(n2) since in the generated data sets W is constant and m,m′ ∈ Θ(n). Fig. 15c depicts the interpolating function of experimental execution times and, in red, the function n2/1010 as a reasonable surrogate for the worst-case execution time. The comparison shows that the algorithm performs very well in real case executions.\nHowever, since the standard deviation observed in the experiment is not negligible, we further investigated the behavior of the algorithm and we discovered that there is a correlation between the execution time of the algorithm and the consistency state of the input HyTN. Therefore, µ and σ were recalculated considering two kind of HyTN sets: one having all consistent HyTNs, and the other having all not consistent HyTNs.\nFig. 16 depicts average execution times obtained in Test 1 calculated considering samples of either all consistent or all not consistent HyTNs obtained from workflow graphs. Fig. 17 offers the same view but for HyTNs obtained from MPG graphs. In general, the mean execution times for consistent HyTNs are smaller than the corresponding ones for not consistent HyTNs; furthermore, they also exhibit a negligible standard deviation. However, for samples of consistent HyTNs\n4 We considered not consistent HyTNs because they practically required more time to be solved.\nobtained from workflows, the standard deviation is not negligible even for samples with size N = 20. Part of the reasons for this behavior is given by the structure of the data sets: in each data set HyTNs can differ a lot with respect to their order and, therefore, they may require very different execution times. For example, the data set relating to workflow graphs with 20 activities contains HyTNs with order in range [48, 94]. Since the number of activities is usually considered as main parameter in workflow community, we wanted to maintain such structure of data set and experiment results to emphasize the dependency of execution time with respect to such number.\nOn the other side, for consistent HyTNs determined from MPGs, the observed standard deviation σ is always less that the 10% of the average execution time µ with 99% level of confidence, while for not consistent HyTNs it has not been possible to determine any confidence level because the observed standard deviation σ resulted to be always high due to some hard instances.\nEven though procedure solveMPG-Threshold() could require up to Θ(W ) updates according to the theoretical worst-case bound, our experiments suggest that, in practice, some kind of dependency of the running time on W is appreciable only for a few MPG games, all associated to not consistent HyTN instances.\nThe goal of Test 2 was to determine the average computation time of Algorithm 2 for different values of W , in order to understand how much the practical\ncomputation time is dependent on W . Therefore, we considered three possible edge weight ranges, [102, 103], [105, 106], and [108, 109], and for each of them two data sets have been built using the randomgame generator, one comprising only consistent HyTNs, and the other only not consistent ones. Each data set comprised of 800 HyTNs instances having |V | = 105 nodes, m,m′ ∈ Θ(n) and edge weights in the corresponding weight range. Fig. 18 depicts the results on these six data sets. Applying the worst-case analysis for these data sets, it results that the time complexity should be O(W ) since n, m and m′ are constants. On the contrary, the determined interpolation functions representing the experimental execution times do not show any clear dependence on W . This result suggests that, in practice, uniform random weighted instances are decided very quickly with respect to the magnitude of their weights and that the algorithm does not seem to exhibit the worst-case pseudo-polynomial behavior predicted in the asymptotic analysis. Moreover, the average execution times for each data set comprising only consistent HyTNs are less than those for the corresponding data of only not consistent HyTNs. Only for consistent HyTNs data sets the standard deviation was below 7% than the average execution time with a confidence of 99%.\nIn order to better understand how some execution times affect the value of the standard deviation, we conducted a third experiment, Test 3, with the goal to visualize the distribution of the instances with computation times significantly above the average. Procedure solveMPG-Threshold() has been executed on 103 randomly generated not consistent HyTNs, each having order n = 106 and W ≈ 103. The determined running times are depicted in Fig. 19a: most of the instances are decided very quickly, i.e., in a time between 0 and 10 seconds, while a smaller portion of the HyTNs required a time between 10 and 500 seconds. In more details, in repeated tests we verified that, approximately, 1% of the HyTN instances required a time between 50 and 100 seconds to be decided, 0.4% required a time between 100 and 500 seconds, and, finally, only 0.1% required more than 500 seconds. These results are shown in Fig. 19b.\nSuch behavior has been confirmed in other tests with different graph orders and maximum edge weight values. In several experiments we conducted, we observed that the maximum execution time of the algorithm keeps growing as we enlarge the size of the dataset. This explains why the standard deviation can’t be reduced. If we could characterize such hard instances in general, we would be making a major progress in understanding the computational complexity of MPGs. We didn’t find any pattern or property that characterizes the found hard instance. Here we can only show a simple family of HyTNs instances in which the execution time grows\nlinearly with W . The family is given by just one single HyTN graph where only W changes, as depicted in Fig. 20a. The corresponding MPG is shown in Fig. 20b and provides a clear example where Brim’s Value Iteration algorithm [7] performs poorly. It is worth noting that in the context of MPGs this example can be reduced down to 6 nodes.\nFinally, in order to show how much the running time is dependent on the number of different positional strategies of one player, in Test 4 the average computation time has been calculated with respect to different values of the product of the heads of hyperarcs (i.e., ∏\nA∈A |HA|) in a HyTN. In particular,\nfor each Π ∈ {1015, 1030, . . . , 1065}, 2500 HyTNs instances (V,A) each having |V | = 50 nodes, ∏\nA∈A |HA| ≈ Π, and W = 10 3 have been generated by means\nof randomgame generator. The results of the evaluation are depicted in Fig. 21, where Π values are drawn in logarithmic scale. Analyzing the diagram in the figure it is possible to say that, experimentally, the average execution time increases only logarithmically with respect to the number of different positional strategies of one player. This results is quite interesting because, considering the HyTN in Fig. 20a, it is evident that the time for checking a HyTN is more dependent on the edge weight magnitude than on the number of different positional strategies of one player."
    }, {
      "heading" : "8 Related Work",
      "text" : "In the literature there are some extension proposals of the STN model to augment the capability to represent temporal constraints.\nIn the STN seminal paper [18], Dechter et al. firstly proposed to consider the Temporal Constraint Satisfaction Problem (TCSP). A binary constraint in a TCSP is represented using a set of intervals rather than a single interval as in an STN. In particular, a binary constraint Cij = {[a1, b1], [a2, b2], . . . , [al, bl]} between time points xi and xj represents the disjunction a1 ≤ xj − xi ≤ b1 ∨ a2 ≤ xj − xi ≤ b2 ∨ al ≤ xj − xi ≤ bl. The problem of verifying consistency of a TCSP is NPcomplete as the same authors showed in the paper; hence, they finally propose to consider STNs as a tractable simplified model.\nA similar kind of generalization considering disjunction of temporal distance constraints was proposed by Stergiou and Koubarakis [44] defining the Disjunctive Temporal Problem (DTP). A DTP consists of a set of variablesX = {x1, x2, . . . , xn} having continuous domains and representing time points and a set of disjunctive difference constraints between the time points in the form: a1 ≤ xi1 − xj1 ≤ b1 ∨ a2 ≤ xi2 − xj2 ≤ b2 ∨ . . .∨ ak ≤ xik − xjk ≤ bk; where xi1 , xj1 , . . . , xik , xjk are time points from X and a1, b1, . . . , ak, bk are real numbers. A DTP is consistent if there exists an instantiation of variables X to real numbers satisfying all the constraints. Since DTPs are a generalization of TCSPs, also for DTPs the consistency check problem is NP-complete. In [44] the authors presented some of the theoretical results characterizing the possible backtracking algorithms that solve the consistency problem in terms of search nodes visited and consistency checks performed.\nIn 2005, Kumar proposed to consider a restricted class of DTP in order to maintain some of the expressive power of DTPs but, at the same time, allowing an efficient consistency check. In particular, in [40], RDTPs (restricted DTPs) is defined as a disjunctive temporal problem where a constraint is one of the following types: (Type 1) (l ≤ xi − xj ≤ u), (Type 2) (l1 ≤ xi ≤ u1) ∨ (l2 ≤ xi ≤ u2) . . . (lj ≤ xi ≤ uj), (Type3) (l1 ≤ xi ≤ u1) ∨ (l2 ≤ xj ≤ u2), where xi and xj represent a timepoint variable, and li, ui real values. An RDTP instance can be solved in strongly polynomial-time deterministic algorithm transforming it into a binary Constraint Satisfiability Problem (CSP) over meta variables representing constraints of Type 2 or Type 3 and, then, showing that such binary constraints are also connected row-convex (CRC) constraints, and, then, exploiting the properties of CRC constraints. An instantiation of a consistency check algorithm for RDTPs that further exploits the structure of CRC constraints has a running time complexity of O((|TP2| + |TP3|) 3d2max + (|TP2| + |TP3|) 2(NM + d2max)), where TP2 is the set of Type 2 constraints, TP3 is the set of Type 3 ones, dmax is the maximum number of disjuncts in any constraint, and N/M is the number of the nodes/arcs of the instance, respectively. In the same paper, Kumar presented also a simpler and faster, but randomized, algorithm for the same class RDTP.\nAn attempt to model some aspects of STNs similar to those addressed by HyTNs was lead in [2], where fun-in and fun-out subgraphs much resembling our multi-tail and multi-head hyperarcs were considered. However, since the problem 1-in-3-SAT is NP-complete even when all the literals comprising the clauses are positive, it readily follows that their models lead to NP-complete problems even\nwhen fun-out subgraphs (or fun-in subgraphs) are banned. As such, the opportunity for tractability spotlighted in this paper is missed in those models.\nAnother approach to extend STN is represented by the proposal of Khatib et al. [28, 29]. They introduced the characterization of hard and soft constraints. STNs are able to model just hard temporal constraints, i.e., they can represent instances where all constraints have to be satisfied, and that the solutions of a constraint are all equally satisfying. However, such assumption can be too much restrictive in some real-life scenarios. For example, it may be that some solutions are preferred with respect to others and, hence, the main problem is to find a way to satisfy them optimally, according to the preferences specified. To address these kind of problems, in [28] the authors introduced a framework in which each temporal constraint is associated with a preference function specifying the preference for each distance or duration; a soft simple temporal constraint is a 4-tuple 〈(X,Y ), I, A, f〉 consisting of (1) an ordered pair of variables (X,Y ) over the integers, called the scope of the constraint; (2) an interval I = [a, b], where a and b are integers such that a ≤ b; (3) a set of preferences A; (4) a preference function f , where f : [a, b] 7→ A is a mapping of the elements belonging to interval I into preference values, taken from set A. An assignment vx and vy to the variablesX and Y is said to satisfy the constraint 〈(X,Y ), I, A, f〉 if and only if a ≤ vy − vx. In such a case, the preference associated to the assignment by the constraint is f(vy −vx). Using soft simple temporal constraint, a new model of temporal constraint network has been introduce: the Simple Temporal Problem with Preferences (STPP). In general, each solution of a STPP has a global preference value, obtained by combining in a suitable way the preference levels at which the solution satisfies the constraints. The optimal solutions of an STPP are those solutions which are not dominated by any other solution in terms of global preference. It was shown in [28] that, in general, STPPs belongs to the class of NP-hard problems. When the preference functions are semi-convex and some other side conditions are observed, then the problem to find an optimal solutions of an STPP is tractable [29].\nFinally, another kind of possible extension is represented by the use of 6= operator instead of ≤ in the binary constrains of STNs. Koubarakis [30] showed that if in a STN temporal constraints are used together with disequations in the form x−y 6= r, where r is a real constant, then the problem of deciding consistency is still tractable. This extension does not allow the specification of alternative constraints but it is interesting because it allows to exclude some solutions maintaining the consistency problem tractable."
    }, {
      "heading" : "9 Conclusions and Future Work",
      "text" : "In the literature, there are different frameworks and approaches aimed to extend the STN model allowing the representation of disjunctive temporal constraints [18, 44], but at cost of an exponential-time consistency check procedure. The only extension with a polynomial time consistency check procedure we are aware of is the one of Kumar [40] mentioned in Section 8.\nIn this paper, we proposed a novel extension, called Hyper Temporal Network (HyTN), where it is possible to represent a new kind of disjunctive constraint, hyper constraint, and to check the consistency of a network in pseudo-polynomial time. A hyper constraint is a suitable set of STN distance constraints and it is sat-\nisfied if at least one distance constraint is satisfied. There could be two kinds of hyperarc: multi-head and multi-tail. In a multi-head hyperarc, its distance constraints are between a common source timepoint and different destination timepoints. In a multi-tail hyperarc, its distance constraints are between different source timepoints and a common destination timepoint.\nA HyTN is said consistent if it is possible to determine an assignment for all its timepoints such that all hyperarcs are satisfied. The computational complexity of the consistency problem of a HyTN is NP-complete when instances contain both kinds of hyperarc.\nOn instances containing either only multi-tail hyperarcs, or only multi-head hyperarcs, the consistency problem can be solved by reducing it, in a very efficient way, to the search of a winning strategy in an equivalent Mean Payoff Game (MPG), and exploiting the known winning-strategy search algorithms for MPGs.\nMoreover, we presented an empirical analysis of the efficiency of the resulting consistency check algorithm. The empirical analysis shows that the proposed algorithm can be effectively used in real cases and confirms the general robustness of our approach.\nAs future work we are investigating the frontier of practical efficient consistency checking for possible generalizations of the HyTN model as, for example, those including contingent constraints [46] or conditional ones [45]."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Simple Temporal Networks (STNs) provide a powerful and general tool for representing conjunctions of maximum delay constraints over ordered pairs of temporal variables. In this paper we introduce Hyper Temporal Networks (HyTNs), a strict generalization of STNs, to overcome the limitation of considering only conjunctions of constraints but maintaining a practical efficiency in the consistency check of the instances. In a Hyper Temporal Network a single temporal hyperarc constraint may be defined as a set of two or more maximum delay constraints which is satisfied when at least one of these delay constraints is satisfied. HyTNs are meant as a light generalization of STNs offering an interesting compromise. On one side, there exist practical pseudo-polynomial time algorithms for checking consistency and computing feasible schedules for HyTNs. On the other side, HyTNs offer a more powerful model accommodating natural constraints that cannot be expressed by STNs like “Trigger off exactly δ min before (after) the occurrence of the first (last) event in a set.”, which are used to represent synchronization events in some process aware information systems/workflow models proposed in the literature.",
    "creator" : "LaTeX with hyperref package"
  }
}